<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3452 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3452</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3452</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-80.html">extraction-schema-80</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <p><strong>Paper ID:</strong> paper-211533462</p>
                <p><strong>Paper Title:</strong> Exploring Geometric Feature Hyper-Space in Data to Learn Representations of Abstract Concepts</p>
                <p><strong>Paper Abstract:</strong> : The term concept has been a prominent part of investigations in psychology and neurobiology where, mostly, it is mathematically or theoretically represented. Concepts are also studied in the computational domain through their symbolic, distributed and hybrid representations. The majority of these approaches focused on addressing concrete concepts notion, but the view of the abstract concept is rarely explored. Moreover, most computational approaches have a predefined structure or configurations. The proposed method, Regulated Activation Network (RAN), has an evolving topology and learns representations of abstract concepts by exploiting the geometrical view of concepts, without supervision. In the article, first, a Toy-data problem was used to demonstrate the RANs modeling. Secondly, we demonstrate the liberty of concept identifier choice in RANs modeling and deep hierarchy generation using the IRIS dataset. Thirdly, data from the IoT’s human activity recognition problem is used to show automatic identification of alike classes as abstract concepts. The evaluation of RAN with eight UCI benchmarks and the comparisons with five Machine Learning models establishes the RANs credibility as a classifier. The classification operation also proved the RANs hypothesis of abstract concept representation. The experiments demonstrate the RANs ability to simulate psychological processes (like concept creation and learning) and carry out effective classification irrespective of training data size.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3452.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3452.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conceptual Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conceptual Spaces: The Geometry of Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A geometrical functional theory that represents concepts as regions in a multidimensional quality space where dimensions correspond to perceptual or cognitive quality dimensions; similarity is given by geometric distance and atomic concepts are convex regions with prototypes at central points.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conceptual Spaces: The Geometry of Thought</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>conceptual spaces (geometric representation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are represented as points or convex regions in a multi-dimensional feature (quality) space whose axes correspond to interpretable perceptual/cognitive dimensions; similarity is a function of geometric distance and category prototypes correspond to central (typical) points of convex regions.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Explains similarity judgments and prototype effects (typicality) via geometric distances; example domains include color space (hue, saturation, brightness) and clustering of multivariate data as convex regions; used in RAN to identify clusters (CRDPs) and form abstract concept nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Relies on the assumption that cognitive categories form convex regions and that appropriate quality dimensions are available; the paper notes conceptual-spaces-style methods are less suitable for non-convex data and that images are not ideal candidates without an appropriate feature transform.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Contrasted with symbolic (amodal) and connectionist representations: conceptual spaces provide an interpretable, spatial/ geometric account that can bridge prototype effects and distributed feature structure; RAN explicitly builds on conceptual spaces while combining symbolic node labels and distributed activations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How to choose or learn the relevant quality dimensions for complex or abstract concepts; handling non-convex categories (the paper states RAN currently models convex groups only and non-convex modeling is ongoing work); applicability to some data types (e.g., raw images) is limited without transformation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3452.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3452.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cognitive representations of semantic categories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A categorization-level functional theory holding that categories are organized around prototypical exemplars that are more central/representative and that membership and typicality are graded.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cognitive representations of semantic categories</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Conceptual categories are represented by prototypical central tendencies (averages) of their members; items are categorized based on similarity to the prototype rather than strict membership rules.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Accounts for typicality/graded membership effects (e.g., robins judged more representative of 'bird' than penguins); in this paper, cluster centers (CRDPs) are treated as prototypes used to form abstract concept representatives and inter-layer weights.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Prototype representations may not capture cases where category knowledge depends on stored exemplars or structured relations; the paper notes abstract concepts are debated and harder to capture with approaches developed mainly for concrete concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Placed in contrast with exemplar or symbolic accounts implicitly: conceptual-spaces + prototype interpretation drives RAN's choice of CRDPs for abstract node centers; RAN uses prototype-like centers within a geometric clustering framework rather than pure symbolic rules or exhaustive exemplar storage.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How prototype-based accounts extend to abstract concepts that depend on contextual, affective, or metaphorical structure; whether prototype centers suffice for complex or non-convex categories (RAN's convexity assumption limits generalization).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3452.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3452.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spreading Activation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A spreading activation theory of memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional model of associative memory in which activation of one concept node spreads along weighted associative links to related nodes, with decaying activation over distance and time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A spreading activation theory of memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>spreading activation (associative network)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Long-term memory is modeled as a network of nodes connected by weighted links; activating one node propagates activation to connected nodes according to link strengths and decay, making related concepts more available for processing.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Explains associative phenomena such as semantic priming, priming effects, and retrieval dynamics; the paper cites priming, false memory and general associative memory work and uses spreading-activation as the inspiration for RAN's Upward Activation Propagation to compute node activations and degrees of confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Spreading activation requires choices about link weights and decay and may oversimplify complex contextual modulation; the paper does not present neural-level evidence here and notes decay and weakening over levels as an inherent characteristic.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Used complementarily with conceptual spaces in RAN: geometric distances determine similarity and are then propagated in a spreading-activation-like fashion to obtain activations; contrasts with purely symbolic rule application where no graded spreading occurs.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How to set link weights and decay dynamics to match brain data; integration with geometric representational formats for abstract concepts requires design choices (e.g., normalization and similarity translation functions used in RAN).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3452.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3452.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic representation / ACT-R</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ACT-R: A theory of higher level cognition and its relation to visual attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic functional architecture where knowledge is represented as discrete symbols and rules operate over those symbols to produce cognition; meaning of symbols is internal to the symbolic system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ACT-R: A theory of higher level cognition and its relation to visual attention</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>symbolic (amodal) representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Conceptual knowledge is represented as symbols (tokens, feature lists, frames) manipulated by symbolic rules; symbols derive meaning from relationships to other symbols rather than direct grounding in sensory dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Symbolic models (e.g., ACT-R) have been successful in modeling high-level cognitive tasks such as language processing, attention, and decision-making where rule-like behavior is observed.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>The paper highlights that symbolic representations are amodal and their symbols 'have sense only regarding other symbols'—a grounding problem; symbolic models can be rigid, require predefined structure, and lack adaptability exhibited by connectionist systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Contrasted with connectionist and hybrid approaches: symbolic approaches emphasize explicit rules and interpretability, whereas connectionist models emphasize learning from data; RAN claims to unify symbolic identifiers (nodes) with distributed/spatial representations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How to ground symbolic representations in perceptual data and how symbolic models handle flexible graded similarity and prototype phenomena without hybridization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3452.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3452.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Connectionist / Distributed representations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Biologically based computational models of high-level cognition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Functional models in which information is represented by patterns of activation across distributed networks; learning occurs by adjusting connection weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Biologically based computational models of high-level cognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>connectionist (distributed) representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are encoded as distributed patterns of activation across many units, with knowledge stored in the weights; representations are graded and learned from data via local learning rules.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Connectionist models (e.g., RBMs, autoencoders, deep nets) adapt to data and explain generalization, perceptual classification and many cognitive phenomena; the paper cites successes in classification, perception, and recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Connectionist weights provide limited explanatory insight into symbolic-like cognitive processes; networks often have fixed topologies and require supervision or large data; paper notes many approaches have predefined structure limiting flexibility for abstract-concept modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Contrasted with symbolic models (interpretable rules) and conceptual spaces (geometric interpretable dimensions); RAN positions itself as unifying distributed (graph topology), symbolic (named nodes), and spatial (feature-space coordinates) aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How to extract interpretable, symbolic-level conceptual structure from distributed weights; how to evolve topology dynamically to capture emergent abstract concepts (RAN addresses evolving topology as a design goal).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3452.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3452.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid models / CLARION</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning in reactive sequential decision tasks: The CLARION model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid functional architecture combining symbolic and connectionist components to model both explicit rule-based and implicit associative knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning in reactive sequential decision tasks: The CLARION model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>hybrid (symbolic + connectionist) representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Conceptual knowledge is represented with both symbolic structures for explicit rule-like knowledge and distributed subsymbolic components for associative/graded knowledge, allowing the model to simulate a wider range of cognitive phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Hybrid frameworks (CLARION) have been used to simulate tasks in cognitive and social psychology by combining strengths of symbolic and connectionist approaches; the paper cites CLARION as an example of hybrid methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Hybrid models can be complex to design and may still rely on predefined architectures; the paper criticizes many methods for requiring fixed topologies or supervision and presents RAN as an evolving-topology hybrid alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Compared to pure symbolic or pure connectionist models: hybrid approaches aim to capture both explicit and implicit knowledge; RAN is presented as a hybrid that additionally features an evolving graph topology and geometric/spatial grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How to design hybrid systems that can autonomously grow topology and form abstract categories without strong supervision; how to reconcile the representational formats across levels.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3452.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3452.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAN (Regulated Activation Networks)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised, hybrid, graph-based model that represents concepts as spatially grounded nodes (points/weights in a conceptual space) and forms higher-level abstract concept nodes by clustering and upward activation propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>hybrid spatial-symbolic-distributed representation (RAN)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concrete instances are points in an n-dimensional conceptual space; clusters (CRDPs) form prototypes that are instantiated as symbolic nodes in a graph; inter-layer weights store prototype coordinates and upward spreading activation (similarity→activation) provides graded membership/DoC for abstract concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Empirical evaluations on Toy-data, IRIS, UCIHAR and eight UCI benchmarks show RAN can form abstract nodes corresponding to class labels, produce high classification metrics (precision/recall/F1/AUC) and simulate prototype and spreading-activation phenomena; RAN reproduces prototype centrality (CRDP) and graded DoC activations.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>RAN is currently limited to data forming convex clusters and to input features normalized to [0,1]; it underperforms on raw image tasks and cannot yet model non-convex concept structure (stated as ongoing work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>RAN explicitly unifies aspects of conceptual spaces (geometric prototypes), spreading activation (graded propagation), symbolic nodes (concept labels) and distributed graph topology; compared against purely symbolic, purely connectionist, and other classifiers (RBM, MLP, KNN, LR, SGD), RAN is positioned as biologically inspired and dynamic.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>Extending RAN to handle non-convex concept regions and richer modalities (images) is an open research direction; how RAN's functional-level mechanisms map to neural data (brain systems for abstract vs concrete concepts) remains to be explored.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conceptual Spaces: The Geometry of Thought <em>(Rating: 2)</em></li>
                <li>A spreading activation theory of memory <em>(Rating: 2)</em></li>
                <li>Cognitive representations of semantic categories <em>(Rating: 2)</em></li>
                <li>ACT-R: A theory of higher level cognition and its relation to visual attention <em>(Rating: 1)</em></li>
                <li>Learning in reactive sequential decision tasks: The CLARION model <em>(Rating: 1)</em></li>
                <li>The representation of abstract words: Why emotion matters <em>(Rating: 1)</em></li>
                <li>Distinct brain systems for processing concrete and abstract concepts <em>(Rating: 1)</em></li>
                <li>Perceiving abstract concepts via evolving computational cognitive modeling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3452",
    "paper_id": "paper-211533462",
    "extraction_schema_id": "extraction-schema-80",
    "extracted_data": [
        {
            "name_short": "Conceptual Spaces",
            "name_full": "Conceptual Spaces: The Geometry of Thought",
            "brief_description": "A geometrical functional theory that represents concepts as regions in a multidimensional quality space where dimensions correspond to perceptual or cognitive quality dimensions; similarity is given by geometric distance and atomic concepts are convex regions with prototypes at central points.",
            "citation_title": "Conceptual Spaces: The Geometry of Thought",
            "mention_or_use": "use",
            "theory_name": "conceptual spaces (geometric representation)",
            "theory_description": "Concepts are represented as points or convex regions in a multi-dimensional feature (quality) space whose axes correspond to interpretable perceptual/cognitive dimensions; similarity is a function of geometric distance and category prototypes correspond to central (typical) points of convex regions.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Explains similarity judgments and prototype effects (typicality) via geometric distances; example domains include color space (hue, saturation, brightness) and clustering of multivariate data as convex regions; used in RAN to identify clusters (CRDPs) and form abstract concept nodes.",
            "counter_evidence_or_challenges": "Relies on the assumption that cognitive categories form convex regions and that appropriate quality dimensions are available; the paper notes conceptual-spaces-style methods are less suitable for non-convex data and that images are not ideal candidates without an appropriate feature transform.",
            "comparison_to_other_theories": "Contrasted with symbolic (amodal) and connectionist representations: conceptual spaces provide an interpretable, spatial/ geometric account that can bridge prototype effects and distributed feature structure; RAN explicitly builds on conceptual spaces while combining symbolic node labels and distributed activations.",
            "notable_limitations_or_open_questions": "How to choose or learn the relevant quality dimensions for complex or abstract concepts; handling non-convex categories (the paper states RAN currently models convex groups only and non-convex modeling is ongoing work); applicability to some data types (e.g., raw images) is limited without transformation.",
            "uuid": "e3452.0"
        },
        {
            "name_short": "Prototype theory",
            "name_full": "Cognitive representations of semantic categories",
            "brief_description": "A categorization-level functional theory holding that categories are organized around prototypical exemplars that are more central/representative and that membership and typicality are graded.",
            "citation_title": "Cognitive representations of semantic categories",
            "mention_or_use": "use",
            "theory_name": "prototype theory",
            "theory_description": "Conceptual categories are represented by prototypical central tendencies (averages) of their members; items are categorized based on similarity to the prototype rather than strict membership rules.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Accounts for typicality/graded membership effects (e.g., robins judged more representative of 'bird' than penguins); in this paper, cluster centers (CRDPs) are treated as prototypes used to form abstract concept representatives and inter-layer weights.",
            "counter_evidence_or_challenges": "Prototype representations may not capture cases where category knowledge depends on stored exemplars or structured relations; the paper notes abstract concepts are debated and harder to capture with approaches developed mainly for concrete concepts.",
            "comparison_to_other_theories": "Placed in contrast with exemplar or symbolic accounts implicitly: conceptual-spaces + prototype interpretation drives RAN's choice of CRDPs for abstract node centers; RAN uses prototype-like centers within a geometric clustering framework rather than pure symbolic rules or exhaustive exemplar storage.",
            "notable_limitations_or_open_questions": "How prototype-based accounts extend to abstract concepts that depend on contextual, affective, or metaphorical structure; whether prototype centers suffice for complex or non-convex categories (RAN's convexity assumption limits generalization).",
            "uuid": "e3452.1"
        },
        {
            "name_short": "Spreading Activation",
            "name_full": "A spreading activation theory of memory",
            "brief_description": "A functional model of associative memory in which activation of one concept node spreads along weighted associative links to related nodes, with decaying activation over distance and time.",
            "citation_title": "A spreading activation theory of memory",
            "mention_or_use": "use",
            "theory_name": "spreading activation (associative network)",
            "theory_description": "Long-term memory is modeled as a network of nodes connected by weighted links; activating one node propagates activation to connected nodes according to link strengths and decay, making related concepts more available for processing.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Explains associative phenomena such as semantic priming, priming effects, and retrieval dynamics; the paper cites priming, false memory and general associative memory work and uses spreading-activation as the inspiration for RAN's Upward Activation Propagation to compute node activations and degrees of confidence.",
            "counter_evidence_or_challenges": "Spreading activation requires choices about link weights and decay and may oversimplify complex contextual modulation; the paper does not present neural-level evidence here and notes decay and weakening over levels as an inherent characteristic.",
            "comparison_to_other_theories": "Used complementarily with conceptual spaces in RAN: geometric distances determine similarity and are then propagated in a spreading-activation-like fashion to obtain activations; contrasts with purely symbolic rule application where no graded spreading occurs.",
            "notable_limitations_or_open_questions": "How to set link weights and decay dynamics to match brain data; integration with geometric representational formats for abstract concepts requires design choices (e.g., normalization and similarity translation functions used in RAN).",
            "uuid": "e3452.2"
        },
        {
            "name_short": "Symbolic representation / ACT-R",
            "name_full": "ACT-R: A theory of higher level cognition and its relation to visual attention",
            "brief_description": "A symbolic functional architecture where knowledge is represented as discrete symbols and rules operate over those symbols to produce cognition; meaning of symbols is internal to the symbolic system.",
            "citation_title": "ACT-R: A theory of higher level cognition and its relation to visual attention",
            "mention_or_use": "mention",
            "theory_name": "symbolic (amodal) representation",
            "theory_description": "Conceptual knowledge is represented as symbols (tokens, feature lists, frames) manipulated by symbolic rules; symbols derive meaning from relationships to other symbols rather than direct grounding in sensory dimensions.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Symbolic models (e.g., ACT-R) have been successful in modeling high-level cognitive tasks such as language processing, attention, and decision-making where rule-like behavior is observed.",
            "counter_evidence_or_challenges": "The paper highlights that symbolic representations are amodal and their symbols 'have sense only regarding other symbols'—a grounding problem; symbolic models can be rigid, require predefined structure, and lack adaptability exhibited by connectionist systems.",
            "comparison_to_other_theories": "Contrasted with connectionist and hybrid approaches: symbolic approaches emphasize explicit rules and interpretability, whereas connectionist models emphasize learning from data; RAN claims to unify symbolic identifiers (nodes) with distributed/spatial representations.",
            "notable_limitations_or_open_questions": "How to ground symbolic representations in perceptual data and how symbolic models handle flexible graded similarity and prototype phenomena without hybridization.",
            "uuid": "e3452.3"
        },
        {
            "name_short": "Connectionist / Distributed representations",
            "name_full": "Biologically based computational models of high-level cognition",
            "brief_description": "Functional models in which information is represented by patterns of activation across distributed networks; learning occurs by adjusting connection weights.",
            "citation_title": "Biologically based computational models of high-level cognition",
            "mention_or_use": "mention",
            "theory_name": "connectionist (distributed) representation",
            "theory_description": "Concepts are encoded as distributed patterns of activation across many units, with knowledge stored in the weights; representations are graded and learned from data via local learning rules.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Connectionist models (e.g., RBMs, autoencoders, deep nets) adapt to data and explain generalization, perceptual classification and many cognitive phenomena; the paper cites successes in classification, perception, and recognition.",
            "counter_evidence_or_challenges": "Connectionist weights provide limited explanatory insight into symbolic-like cognitive processes; networks often have fixed topologies and require supervision or large data; paper notes many approaches have predefined structure limiting flexibility for abstract-concept modeling.",
            "comparison_to_other_theories": "Contrasted with symbolic models (interpretable rules) and conceptual spaces (geometric interpretable dimensions); RAN positions itself as unifying distributed (graph topology), symbolic (named nodes), and spatial (feature-space coordinates) aspects.",
            "notable_limitations_or_open_questions": "How to extract interpretable, symbolic-level conceptual structure from distributed weights; how to evolve topology dynamically to capture emergent abstract concepts (RAN addresses evolving topology as a design goal).",
            "uuid": "e3452.4"
        },
        {
            "name_short": "Hybrid models / CLARION",
            "name_full": "Learning in reactive sequential decision tasks: The CLARION model",
            "brief_description": "A hybrid functional architecture combining symbolic and connectionist components to model both explicit rule-based and implicit associative knowledge.",
            "citation_title": "Learning in reactive sequential decision tasks: The CLARION model",
            "mention_or_use": "mention",
            "theory_name": "hybrid (symbolic + connectionist) representation",
            "theory_description": "Conceptual knowledge is represented with both symbolic structures for explicit rule-like knowledge and distributed subsymbolic components for associative/graded knowledge, allowing the model to simulate a wider range of cognitive phenomena.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Hybrid frameworks (CLARION) have been used to simulate tasks in cognitive and social psychology by combining strengths of symbolic and connectionist approaches; the paper cites CLARION as an example of hybrid methodology.",
            "counter_evidence_or_challenges": "Hybrid models can be complex to design and may still rely on predefined architectures; the paper criticizes many methods for requiring fixed topologies or supervision and presents RAN as an evolving-topology hybrid alternative.",
            "comparison_to_other_theories": "Compared to pure symbolic or pure connectionist models: hybrid approaches aim to capture both explicit and implicit knowledge; RAN is presented as a hybrid that additionally features an evolving graph topology and geometric/spatial grounding.",
            "notable_limitations_or_open_questions": "How to design hybrid systems that can autonomously grow topology and form abstract categories without strong supervision; how to reconcile the representational formats across levels.",
            "uuid": "e3452.5"
        },
        {
            "name_short": "RAN (Regulated Activation Networks)",
            "name_full": "here",
            "brief_description": "An unsupervised, hybrid, graph-based model that represents concepts as spatially grounded nodes (points/weights in a conceptual space) and forms higher-level abstract concept nodes by clustering and upward activation propagation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_name": "hybrid spatial-symbolic-distributed representation (RAN)",
            "theory_description": "Concrete instances are points in an n-dimensional conceptual space; clusters (CRDPs) form prototypes that are instantiated as symbolic nodes in a graph; inter-layer weights store prototype coordinates and upward spreading activation (similarity→activation) provides graded membership/DoC for abstract concepts.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Empirical evaluations on Toy-data, IRIS, UCIHAR and eight UCI benchmarks show RAN can form abstract nodes corresponding to class labels, produce high classification metrics (precision/recall/F1/AUC) and simulate prototype and spreading-activation phenomena; RAN reproduces prototype centrality (CRDP) and graded DoC activations.",
            "counter_evidence_or_challenges": "RAN is currently limited to data forming convex clusters and to input features normalized to [0,1]; it underperforms on raw image tasks and cannot yet model non-convex concept structure (stated as ongoing work).",
            "comparison_to_other_theories": "RAN explicitly unifies aspects of conceptual spaces (geometric prototypes), spreading activation (graded propagation), symbolic nodes (concept labels) and distributed graph topology; compared against purely symbolic, purely connectionist, and other classifiers (RBM, MLP, KNN, LR, SGD), RAN is positioned as biologically inspired and dynamic.",
            "notable_limitations_or_open_questions": "Extending RAN to handle non-convex concept regions and richer modalities (images) is an open research direction; how RAN's functional-level mechanisms map to neural data (brain systems for abstract vs concrete concepts) remains to be explored.",
            "uuid": "e3452.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Conceptual Spaces: The Geometry of Thought",
            "rating": 2,
            "sanitized_title": "conceptual_spaces_the_geometry_of_thought"
        },
        {
            "paper_title": "A spreading activation theory of memory",
            "rating": 2,
            "sanitized_title": "a_spreading_activation_theory_of_memory"
        },
        {
            "paper_title": "Cognitive representations of semantic categories",
            "rating": 2,
            "sanitized_title": "cognitive_representations_of_semantic_categories"
        },
        {
            "paper_title": "ACT-R: A theory of higher level cognition and its relation to visual attention",
            "rating": 1,
            "sanitized_title": "actr_a_theory_of_higher_level_cognition_and_its_relation_to_visual_attention"
        },
        {
            "paper_title": "Learning in reactive sequential decision tasks: The CLARION model",
            "rating": 1,
            "sanitized_title": "learning_in_reactive_sequential_decision_tasks_the_clarion_model"
        },
        {
            "paper_title": "The representation of abstract words: Why emotion matters",
            "rating": 1,
            "sanitized_title": "the_representation_of_abstract_words_why_emotion_matters"
        },
        {
            "paper_title": "Distinct brain systems for processing concrete and abstract concepts",
            "rating": 1,
            "sanitized_title": "distinct_brain_systems_for_processing_concrete_and_abstract_concepts"
        },
        {
            "paper_title": "Perceiving abstract concepts via evolving computational cognitive modeling",
            "rating": 1,
            "sanitized_title": "perceiving_abstract_concepts_via_evolving_computational_cognitive_modeling"
        }
    ],
    "cost": 0.01408975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring Geometric Feature Hyper-Space in Data to Learn Representations of Abstract Concepts
14 March 2020</p>
<p>Rahul Sharma 0000-0002-0699-6940
Department of Informatics Engineering
University of Coimbra
3030-290CoimbraPortugal</p>
<p>Bernardete Ribeiro bribeiro@dei.uc.pt 0000-0002-0699-6940
Department of Informatics Engineering
University of Coimbra
3030-290CoimbraPortugal</p>
<p>Miguel Alexandre 
Department of Informatics Engineering
University of Coimbra
3030-290CoimbraPortugal</p>
<p>Pinto ampinto@dei.uc.pt 
Department of Informatics Engineering
University of Coimbra
3030-290CoimbraPortugal</p>
<p>F Amílcar Cardoso 
Department of Informatics Engineering
University of Coimbra
3030-290CoimbraPortugal</p>
<p>Exploring Geometric Feature Hyper-Space in Data to Learn Representations of Abstract Concepts
14 March 20206F6FED0213D840D7F419FDFF55C4097D10.3390/app10061994Received: 29 January 2020; Accepted: 10 March 2020 ;unsupervised machine learninghierarchical learningcomputational representationcomputational cognitive modelingcontextual modelingclassificationIoT data modeling
The term concept has been a prominent part of investigations in psychology and neurobiology where, mostly, it is mathematically or theoretically represented.Concepts are also studied in the computational domain through their symbolic, distributed and hybrid representations.The majority of these approaches focused on addressing concrete concepts notion, but the view of the abstract concept is rarely explored.Moreover, most computational approaches have a predefined structure or configurations.The proposed method, Regulated Activation Network (RAN), has an evolving topology and learns representations of abstract concepts by exploiting the geometrical view of concepts, without supervision.In the article, first, a Toy-data problem was used to demonstrate the RANs modeling.Secondly, we demonstrate the liberty of concept identifier choice in RANs modeling and deep hierarchy generation using the IRIS dataset.Thirdly, data from the IoT's human activity recognition problem is used to show automatic identification of alike classes as abstract concepts.The evaluation of RAN with eight UCI benchmarks and the comparisons with five Machine Learning models establishes the RANs credibility as a classifier.The classification operation also proved the RANs hypothesis of abstract concept representation.The experiments demonstrate the RANs ability to simulate psychological processes (like concept creation and learning) and carry out effective classification irrespective of training data size.</p>
<p>Introduction</p>
<p>Concepts are of great value to humans because they are one of the building blocks of our recognition process.They enable us to perform cognitive functions such as classification which is fundamental in decision making and also capacitate us for contextual comprehension.The term concept has a lot to say about itself.Anything can be seen as a concept, whether it is a living being, or a thing, or an idea.An individual concept is referred to as a concrete concept (or feature) whereas a generalized form of a set of concepts (or features) can be perceived as an abstract concept.The denomination concept immediately coins the need to understand its representations.There are several conceptual representation theoretical frameworks [1] like modality-specific, localist-distributed, experience-dependent [2].Such frameworks not only helps us to understand the various cognitive processes in humans but also the psychological ones, like creativity.Each theory has a way to represent concrete concepts through perception (or recognition), action, emotion, and introspection, but the notion of abstract concepts is debatable [1].Abstract concepts are largely studied in psychology, and there are attempts to study them by the computational linguistics research community for Natural Language Processing (NLP) [3].However, the representation aspect of abstract concepts is still a challenge.In this article, we address this issue of representation of abstract concepts computationally by simulating and studying the formation of convex abstract concepts.</p>
<p>Computational models provide us algorithmic specificity, conceptual clarity, and precision.Besides, they empower us to perform simulations that can either be useful to test and validate psychological theories or to generate new hypotheses about how the mind works-this has turned them into an indispensable tool to study the human brain.The literature [4][5][6] shows that this ambitious goal is not out of reach of computational cognitive modeling.Furthermore, these types of computational tools with the ability to capture cognitive phenomena also has the potential to simulate and study some mental states and processes such as those linked to creativity [7].</p>
<p>Several computational modeling techniques (or tools) simulate cognitive states and represent concepts at symbolic and connectionist levels.Symbols represent information at a symbolic level.Rules are defined to manipulate symbols.Within a symbolic representation, the meaning is internal to the description itself; symbols have sense only regarding other symbols, and not regarding any real-world objects or phenomena they may represent.Adaptive Control of Thought-Rational (ACT-R) [8] is an example of symbolic approaches, with contributions in, almost, all fields of AI (such as language processing, perception, attention, decision making, etc.).At the connectionist level, information is represented by the dynamics over densely connected networks of primitive units.A particular strength of connectionist networks is their ability to adapt their behavior according to observed data.The weights among the units of a distributed network represent the learned behavior, they offer limited explanatory insights into the process, being modeled.Bioinspired Artificial Neural Networks (ANN) such as Restricted Boltzmann Machine (RBM) [9], autoencoders [10], and deep neural networks [11] are some excellent examples of connectionist approaches with a significant contribution toward classification, perception, and recognition.</p>
<p>A third way constitutes a hybrid view of connectionist, and symbolic methods.Connectionist Learning with Adaptive Rule Induction Online (CLARION) [12] is a methodology that is hybrid, and capable of simulating scenarios related to cognitive and social psychology.All these methodologies either require a predefined structure or have a fixed topology that imposes a limitation of having supervision, and inflexibility while modeling the concepts.Some techniques exhibit dynamic and evolving behavior while performing computational operations, such as evolving neural networks by using their genotype-phenotype mapping of cells [13].The proposed model emulates the behavior of the dynamic creation of abstract concepts by evolving the computational model upon identifying different groups in the data.</p>
<p>This article proposes a computational method named Regulated Activation Network (RAN) which unifies the virtues of symbolic, distributed, and spatial representations to represent concepts (both concrete and abstract).RAN has a graph-based topology hence it is distributed, every node in the graph (network) identifies an entity, therefore it is symbolic, and every node (or entity) is viewed in an n-dimensional feature space, hence it is also spatial.The spatial view of concepts as points in multidimensional geometric feature space (see Figure 1 for six-dimensional view of concepts) is inspired by the theory of conceptual spaces [14].The RAN's modeling has an evolving topology that enables it to build a model depicting a hierarchy of concepts.The geometrical associations among concepts aid in determining the convex abstract concepts.Further, the representatives (nodes) of the abstract concepts form a new layer dynamically, where each node acts as a convex abstract concept representative for the underlying category.Symbolically, the concepts at (relatively) lower levels in the hierarchy are identified as concrete concepts and the concepts at (relatively) higher levels are seen as abstract concepts.The model generation process with RAN and the three cognitive functions (i.e., concept creation, learning and activation propagation) are simulated using a Toy-data problem.The deep hierarchy generation, automatic generic concept modeling simulations are performed using two University of California Irvine (UCI) benchmarks: IRIS data; and IoT data from smartphone sensors.</p>
<p>The application of RAN as a classifier is reported along with the proof of concept of classification using eight UCI benchmark datasets.</p>
<p>The generated models were evaluated using metrics precision, recall, F1-score, accuracy, and Receiver Operating Characteristic (ROC) curve analysis.The article also reports the RANs classification and feature comparison with five machine learning techniques, Multilayer Perceptron (MLP) [15], Logistic Regression (LR) [16], K Nearest Neighbors (K-NN) [17], Stochastic Gradient Descent (SGD) [18] and Restrict Boltzmann Machine [9] pipelined with Logistic Regression (RBM+).</p>
<p>The article is organized in the following order; Section 2 puts forward the work closely related to abstract concept representation and models with evolving topology.Section 3 describes the background associated with principles, theories, and motivations for RAN modeling.RANs methodology is detailed using Toy-data in Section 4. Section 5 shows the experiments with two datasets acquired from UCI machine learning repository to exhibit (1) flexibility in choosing a suitable concept identifier, (2) building a deep hierarchy of abstract concepts, (3) automatic association of input-labels to their respective abstract concept nodes.Section 6 provides RAN comparisons with five classifiers and proof of concept with eight benchmark datasets.At last, Section 7 summarizes and concludes the article with remarks over ongoing and future work.</p>
<p>Related Work</p>
<p>Abstract concepts are of immense value because they help in developing unique abilities in humans such as relative recognition and effective decision-making.In medical science, there have been significant efforts to study abstract concepts with the help of technology.One such example is MRI (Magnetic Resonance Imaging) , which is being used to inspect the sections of the brain involved in abstract concept identification [19,20].Research in psychology has also reported investigations over abstract concepts, like probing the role of emotional content in processing and representing abstract concepts [21].</p>
<p>There has been a notable contribution from cognitive, and psycholinguists in studying languages through abstract concept modeling and representations.Internally representing abstract concepts via amodal symbols like a feature list, and frames [22,23] is among the preliminary research work in linguistics.The association and context were also established, to relating abstract and Concrete words [22].Some research reveals that we internally recognize metaphors as abstract concepts [24].Besides theoretical methods, computational approaches are playing a vital role in comprehending and representing abstract concepts.Research in NLP addresses computational learning, comprehension and processing of human-understandable language, and its components.An interesting article published a work about the representation of abstract, and concrete concepts in daily written language using a text-based multimodal architecture of NLP [3].Other than NLP, semantic networks are also used to study semantic similarity among abstract, and concrete nouns (of Greek, and English) [25] with the aid of network-based Distributed Semantic Model [26].</p>
<p>Though the aforementioned computational approaches contribute toward abstract concept modeling and representation, they have a fixed topology (i.e., the modeling process begins with a fixed structure and configuration).In connectionist computational modeling, there have been efforts to develop models that evolve.Artificial Neural Networks Adaptation: Evolutionary Learning Of Neural Optimal Running Abilities (ANNA ELEONORA) [27] demonstrated a way to grow neural networks with the aid of parallel genetic algorithms.NeuroEvolution of Augmenting Topologies (NEAT) [28] is another work that reported evolving neural network modeling, showing how nodes and weights are added to the model when new features emerge as part of the existing population and CoDeepNEAT [29] is the most recent member of such evolving models.Markov Brains [30] also belongs to the family of evolving neural networks which uses binary variables and arbitrary logic to implement deterministic or probabilistic finite state machines.They have been used to investigate behaviors, character recognition and game theory.This article communicates an approach which is not only hybrid but also has an evolving topology.The RANs modeling learns the representation of the convex abstract concepts dynamically, hence makes it an evolving topology.RANs approach is a connectionist, and each newly created node corresponds to an abstract concept symbolically, thus portraying its hybrid characteristics.</p>
<p>Background</p>
<p>This section provides information about the principles and methodologies related to RANs modeling.It highlights the significance of each approach, along with their applicability in RANs modeling.</p>
<p>Principles of Regulated Activation Networks</p>
<p>The tenets of RANs modeling presented in [31], state that the model should be topologically connectionist and intend to represent and simulate the dynamic cognitive state of an agent.In the first version RAN [31] the authors implemented a single-layer version of the model where each node had a lateral connection to its same-layer companions.It had a simple learning and reasoning mechanisms, but these showed to be sufficient to simulate several known cognitive phenomena such as the Priming [32], the False Memory [33,34].</p>
<p>Two principles of Regulated Activation Networks inspired our proposal.First, the model should be dynamic, and this is achieved by dynamically creating layers (deep representations) of concepts.Second, the model must be capable of learning and creating an abstract representation of concepts.This is obtained by viewing associations among the concepts (at the same level) in n-dimensional geometric space, and learning relationship between the newly created abstract concepts, and input level concepts.</p>
<p>Conceptual Spaces</p>
<p>Conceptual Spaces Theory [14] is one of the cognitive approaches that form the basis of RANs modeling.This theory views the concepts as regions within a multi-dimensional space, with the data features representing the dimensions.The similarity among the concepts can be identified based upon the geometrical distance between the objects.The conceptual spaces thus serve as a natural way or tool to capture the similarity relationships among concepts, or objects.Under this setting, one data instance corresponds to a single point in the space.Formally we can say, the quality dimensions, i.e., a set of D 1 , .....,D n , forms the conceptual space S. A point in S is represented by a vector v = d 1 , ....., d n , where {1,....n} are the indexes of the dimensions.Atomic concepts are convex regions-a convex region C having point x that falls between points x 1 ∈ C and x 2 ∈ C also belongs to C. The quality dimension is the basic requirement for conceptual spaces [35].An example is a color space with the dimensions Hue, Saturation, and Brightness.Each quality dimension has a geometrical structure.For example, Hue is circular, whereas brightness and saturation correspond with finite linear scales (see Figure 2).The theory of conceptual spaces also addresses prototype theory of categorization [37][38][39].The main idea of prototype theory is that within a category of objects, like those instantiating a concept, certain members are judged to be more representative of the group than others.For example, robins are judged to be more representative of the category "bird" than are ravens, penguins, and emus.If convex regions of conceptual space describes concepts, then prototype effect is, indeed, expected, i.e., the most likely central position of a convex region describes an abstract concept.For example, if color concepts in a convex region identified as subsets of the color space, then the central points of these regions would be the most prototypical examples of the color.</p>
<p>Clustering is a suitable way of identifying and learning atomic convex concepts in conceptual spaces.There are several clustering techniques, like hierarchical clustering, subspace clustering [40], partitioning relocation clustering, density-based clustering, grid-based clustering and many more.Many are frequently used in the statistical and scientific analysis of data [41,42], and in machine learning for the identification of concepts/features [43].On the other hand, the creation of a hierarchy of sub/super-concepts is a way to represent more abstract concepts and their taxonomic-like relations.Deep learning techniques [44][45][46][47][48] found in the literature can also be used to create deep hierarchical representations, but usually do not interpret data as points in conceptual spaces.In the proposed approach, the clustering techniques enable us to identify categories of concepts in a conceptual space thus laying the foundation to form a layer of abstract representation of concepts.</p>
<p>Spreading Activation</p>
<p>Spreading Activation is a theory of memory [49] based on Collins and Quillian's computer model [50] which has been widely used for the cognitive modeling of human associative memory and in other domains such as information retrieval [51].It intends to capture the information representation and how it is processing.According to the theory, long-term Memory is represented by nodes and associative links between them, forming a semantic network of concepts.The links characterized by a weight denotes the associative or semantic relation between the concepts.The model assumes activating one concept implies the spreading of activation to related nodes, making those memory areas more available for further cognitive processing.This activation decays over time as it spreads, which can occur through multiple levels [52], and the further it gets the weaker it becomes.That is usually modeled using a decaying factor for activation.The method of spreading activation has been central in many cognitive models due to its tractability and resemblance of interrelated groups of neurons in the human brain [53].This theory of Spreading Activation inspires the activation propagation mechanism in our proposal to propagate (spread) activation in the upward direction, i.e., from the input-to-abstract layer in the network.The method has its significance, i.e., in the creation of the network, and in understanding the created abstract concepts.</p>
<p>Abstract Concept Modeling with RANs</p>
<p>The proposed approach models convex abstract concepts through four core steps (i.e., Concept Identification, Concept Creation, Interlayer Learning and Upward Activation Propagation), along with one optional step (i.e., Abstract Concept Labeling).The RAN's methodology is explained using a Toy-data problem. Figure 3 shows the plot of Toy-data displaying the Cluster Representative Data Points (CRDPs) for all five classes of Toy-data (the importance of CRDP is detailed in Section 4.2).The objective of this experiment is to show how RANs build a hierarchical representation dynamically and simulate cognitive process of concept creation, learning, and activation propagation.For this experiment, it was hypothesized that the created abstract concepts symbolically represents the 5 classes of Toy-data.Classification operations were performed to prove the hypothesis which is reported at the end of this section.The notations used to describe the RAN's methodology are listed in Table 1.</p>
<p>Assumptions and Boundaries</p>
<p>The necessary boundary related to input data is, data value should be between "0' and "1" (both inclusive), this limitation has its inspiration from biological neurons.A value "0" indicates neuron (or node) is inactive, whereas "1" shows the neuron is highly active.The model is, by design, applicable only to multidimensional data sets where each feature takes A real value between 0 and 1-It works as well for discrete data sets where the variables take either 0 or 1 values.If the user data is in a different format, the user must define the transformation and inverse transformation of the data.The following are a few possibilities of such alterations for some of the most common kinds of data:</p>
<p>•</p>
<p>If a variable in the input data is categorical, e.g., blue; green; red, transform the data using One Hot Coding technique.</p>
<p>•</p>
<p>If a variable in the input data is numerical, bounded within a minimum and a maximum value it can be normalized into [0, 1], e.g., via value−min max−min ;</p>
<p>The user must implement these and the inverse transformation functions to interpret the results obtained from our model.Since our technique is designed to work with multi-variate data-sets, where each data value is a point in conceptual space, we assume that the data being used is compatible with the requirements.Though images are a form of multivariate data, pictures are not ideal candidates to be interpreted as points in conceptual spaces, (discussed in Section 3.2).For this reason, our approach will, most probably, underperform on image processing tasks against other models that are, individually, designed for these kinds of data, such as deep representations built with Convolutional Networks [47,54,55]; our technique is preferably suitable for understanding and simulating cognitive processes like abstract concept Identification.The version of RAN in this article can model data that consists of convex groups of data points, therefore, the model does not perform well well with the complex data having non-convex groups of data points.Modeling non-convex concepts is one of the ongoing research in RAN's modeling and out of the scope of this article but readers who are interested in knowing more can refer to the published research work [56].</p>
<p>To use the RANs approach provide the data to the model with an additional header stacked over the data.The size of the header is the same as the dimension of the input data vector, and each header element holds the largest value of their corresponding input data attribute.See Appendix A.1 for elaboration.</p>
<p>Step 1: Concept Identification (CI) Process</p>
<p>Concept identification is the first step in RANs modeling.The objective of the CI procedure is to appropriately identify each instance within the data as a distinguished member of various underlying convex groups.This is realized by categorizing the input data based upon their geometrical relationship, i.e., distance, conforming to the theory of conceptual spaces (see Section 3.2).Here, we also recognize data points that are the most probable representative of each identified group, complying with the prototype theory (see Section 3.2).These identified data points are termed as Cluster Representative Data Points (CRDP) and are used in Step 3 for learning the relationship between two adjacent layers (see Section 4.4).</p>
<p>The process of CI instantiates after preprocessing the input data.Initially, an input layer is formed, with dimension equal to the size of the input data feature vector.Step 1 in Figure 4 shows the Layer-0 with two nodes which is like the magnitude of the input vector of 2-Dimensional Toy-data.At Layer-0, clustering methods are used to determine geometrical relation among the several input data instances and identify the underlying categories within the data.Thus, K-means [57] clustering algorithm is applied to Toy-data to identify five classes (Class-1, ..., Class-5) by assigning a value 5 to 'K' in K-means clustering algorithm (Note: The 'K' value in the K-means algorithm is to provided manually but in unlabeled datasets, the best value of 'K' can be determined using the Elbow method).Figure 3 shows the plot of 2-D data points obtained after performing concept identification operation using the K-means algorithm.Figure 3   Any clustering algorithm can act as a concept Identifier in RANs modeling if it suffices two basic requirements.First, the algorithm can determine convex categories based upon their geometric relationship among the data instances.Second, the algorithm recognizes CRDPs of all the identified clusters.This flexibility of choosing a suitable method for the concept Identification process in RANs modeling is demonstrated by a separate experiment using Affinity propagation [58] clustering algorithm, in Section 5.1.</p>
<p>Step 2: Concept Creation (CC) Process</p>
<p>Concept creation is a cognitive process to create a representation of a newly identified concept.In RAN's modeling, this cognitive process is simulated by creating a new layer of concepts dynamically.Each constituent node in the new layer symbolically acts as an abstract representation of their respective categories identified in the CI process.The Step-2 in Figure 4 shows the newly created layer (Layer-1), that has five nodes (N 1 , ..., N 5 ), corresponding to five classes (see Figure 3), identified in CI operation with Toy-data.</p>
<p>Besides abstract representation of underlying categories, the activation of nodes in newly created layer discloses the degree of confidence (DoC) (Calculating DoC of a node is explained in detail with upward activation propagation operation).indicating the certainty of identification of a class by its representative node in the new layer (for a given input data instance).For example, if a node (say N 1 ) gets an activation of 0.85, it can be stated that with a confidence of 85% the input data belongs to the category being represented by node N 1 .Thus, for all input data instances, the obtained feature, value pair of abstract-node, Activation-value at new layer adds more meaning.For instance, in Figure 4, Step-2, at Layer-0 input vector is [0.1, 0.21] it signifies that the dimensions S 1 and S 2 has activation 0.1, and 0.21 respectively.For the, aforementioned, input vector, [0.13, 0.32, 0.89, 0.16, 0.05] vector of activation is observed at all nodes (N 1 , ..., N 5 ) respectively, at Layer-1.The observed activation vector itself describes that the input data belongs to Class-4 with a DoC of 89%.</p>
<p>Step 3: Inter-Layer Learning (ILL) Process</p>
<p>Learning is an important cognitive process it acts as a relationship to associate concepts.In RANs modeling, learning is simulated by an assignment operation.The developed Inter-Layer Learning procedure also fulfills the second objective of RANs modeling (mentioned in Section 3.1).As aforestated in Section 4.3 that each node in the new layer is an abstract representative of categories identified in the CI process, thus we learn association among the two-layer such that it substantiates the abstract representation by the nodes at the new layer.Since CRDPs (see Section 4.2) are the most apparent choice as an abstract representative of a cluster (and adhere to the inspiration from prototype theory); consequently, the CRDPs learned as an association between the two layers.</p>
<p>Equation (1) shows the general learning in the form of a matrix, where W is the learned Inter-Layer Weight (ILW) between node j at new layer (i.e., Layer-1 in Figure 4) and node i at input layer (i.e., Layer-0).The set of ILWs, from one node j at new layer to all input nodes i, are the values of CRDP of j th cluster center (i.e., C j ) identified in CI process.For instance, cluster center C 1 (see Figure 3) forms the weight vector [W 1,1 , W 1,2 , W 1,3 and W 1,4 ] (ILWs shown by 2 yellow lines in Step 3 Figure 4) between the node N 1 at Layer-1 and all four input nodes S 1 and S 2 at Layer-0.
W =        W 1,1 , W 1,2 , . . . , W 1,n a . . . W k,1 , W k,2 , . . . , W k,n a . . . W n A ,1 , W n A ,2 , . . . , W n A ,n a        =        C 1 . . . C k . . . C n A        (1)
where j = 1, 2, ..., n A , and i = 1, 2, ..., n a .</p>
<p>The distance between the learned weight vector of one node j (at Layer-1) and activation of all input nodes S 1 and S 2 (at Layer-0), is used to determine how strongly the input vector represents the node N j at new layer.Thus, it enables us to identify the convex abstract concepts for the input instance (elaborated in Section 4.5).</p>
<p>Step 4: Upwards Activation Propagation (UAP) Process</p>
<p>This upward activation propagation is a geometric reasoning operation, i.e., a non-linear projection of an i-dimensional input data vector a i , into a j-dimensional output vector A j (see Step 4 in Figure 4).The UAP operation is carried out in two stages, in the first stage the geometric distance operation takes place, and in the second stage, geometric distance is translated to establish a similarity relation.</p>
<p>Geometric Distance Function (GDF)-Stage 1</p>
<p>In the first phase of the UAP mechanism we determine the geometrical distance between the learned weight vectors (see Equation (1)) and an input instance a i .The numerator of Equation (2) shows a function to calculate the Euclidean distance between the j th weight vector and input vector a i .The denominator of Equation (2) shows the relation that normalizes (in RANs modeling the activation values are, by definition, real values in the [0, 1] interval -in an n-dimensional space the maximal possible euclidean distance between any two points is ∑ n i=1 (a i − 0) 2 = √ n, where a i = 1 the calculated distance between [0, 1].
d j = ∑ n a i=1 (W j,i − a i ) 2 √ n a(2)
and consequently, j normalized Euclidean distances d j are obtained between all j weight vectors and input instance a i .</p>
<p>Similarity Translation Function (STF)-Stage 2</p>
<p>In the second phase the calculated normalized distance is transformed to obtain a similarity relation such that following requirements are fulfilled:
• f (d = 0) = 1, i.e., when distance is 0 similarity is 100%. • f (d = 1) = 0 i.e., when distance is 1 similarity is 0%. • f (d = x
) is continuous, monotonous, and differentiable in the [0, 1] interval.
f (x) = (1 − 3 √ x) 2(3)
In RANs modeling Equation ( 3) is used as the Similarity Translation Function to determine the similarity relation of the previously calculated distance.The non-linearity of STF is depicted in Figure 5, indicating that the similarity value reduces drastically when the normalized Euclidean distance is larger than 0.05 (or 5% dissimilar).The first three steps generate the RANs model (see Figure 4), later, in the fourth step, this model is used via UAP operation by propagating the input activation (a i ) upward and obtaining activation (A j ) at convex abstract concept layer (inspired by the theory of spreading activation see Section 3.3).Algorithm 1 describes the Upward Activation Propagation operation, showing how the inputs and interlayer learning weights W are used to calculate similarity relation to generating output activation at each abstract concept representative nodes.The activation A j in newly created nodes N j also indicates the degree of confidence (DoC) of the identification of a class by its representative node in the new layer (for a given input data instance).For instance, in Figure 4, Step-2, at Layer-0 input vector is [0.1, 0.21] it signifies that the dimensions S 1 and S 2 has activation 0.1 and 0.21 respectively.For the, aforementioned, input vector, a [0.13, 0.32, 0.89, 0.22, 0.01] vector of activation is observed at all nodes (N 1 , N2,N 3 , N4 and N 5 ) respectively, at Layer-1.The observed activation vector itself describes that the input data belongs to Class-3 (Versicolor) with a DoC of 89%.</p>
<p>RANs Proof of Hypothesis and Complexity</p>
<p>At the beginning of this Section 4 it was hypothesized that nodes in the newly created layer symbolically represent abstract concepts of the five classes (Class-1, Class-2, Class-3, Class-4, and Class-5) of Toy-data.This hypothesis can be proven through classification operation using the RAN model generated with Toy-data.The classification experiment setup consists of 30 iterations of an experiment.Each experiment consist of 9 Research Design (RD)(see Table A3 in Appendix A.2), where, in every RD a 10-fold cross-validation procedure was applied.To carry out the evaluation operation True-labels, and Test-labels are determined via Abstract Concept Labeling (ACL) operation of RANs (see Appendix A.3 for ACL's description).Further, these labels were used to form a multi-class confusion matrix for the 3 classes of IRIS data and with the aid of this confusion matrix, 4 metrics (i.e., Precision, Recall, F1-Score, and Accuracy) were calculated.</p>
<p>Algorithm 1 Upwards Activation Propagation algorithm</p>
<p>Input: Vector [a 1 , a 2 , ..., a n a ] as input at layer l.</p>
<p>Output: New activation vector
[A 1 , A 2 , ..., A n A ] in layer l + 1.
for Each node A j in layer l + 1 do Calculate Normalized Euclidean Distance:
d j = ∑ na i=1 (W j,i −a i ) 2
√ n a Transform d j through STF Equation (3):
A j = f (d 2 j ) end for Where: i = [1, 2, ...., n a ]. j = [1, 2, ...., n A ].</p>
<p>W j,i is ILW see Equation (1).</p>
<p>Multi-class Receiver Operating Characteristics (ROC) curves were also plotted for the five classes to support the classification experiment with Toy-data.The binary labels corresponding to the True-labels (obtained via ACL operation) were obtained using the method node-wise binary transformation of input True-label (see Appendix A.5). Further, the confidence scores for the binary vectors were calculated using the node-wise confidence-score calculation method (described in Appendix A.5).</p>
<p>Table 2 not only shows the RAN's comparison with the other 5 classifiers but also that RAN indeed performed well in the classification process with a performance of 99% (ca.) for all classification metrics.Figure 6 shows the ROC-AUC analysis of RANs model with Toy-data, in this graph one can see that an average AUC for all the five classes is 99% (ca.).These results show the ability of RAN's modeling to identify the abstract concept where the three nodes (N 1 , N 2 , N 3 , N 4 and N 5 ) in Layer-1 symbolically represents the 5 classes (Class-1, Class-2, Class-3, Class-4, and Class-5) of Toy-data, respectively, as abstract concepts, hence proves the hypothesis.In RAN's algorithm the four operations have different complexities: (1) the concept identification process is expressed as O( f (n)) where f (n) is the complexity of the concept identifier (or clustering algorithm); (1) the concept creation has complexity of O(k) where k is the number of clusters; (3) the inter layer learning also has complexity of O(k) because it is an assignment operation and is equal to number of identified cluster centers; (4) the upward activation operation has the complexity of O(n) when n is the number data instances.The overall complexity of the RAN's modeling for creating a single layer is expressed by Equation (4).
T(n) = O(max {O( f (n)), O(n)})(4)f (n) = O(n (k+2/p) )(5)
where: k is number of clusters; p is number of features.</p>
<p>The time complexity of the K-means algorithm is given by Equation ( 5) and when K-means is chosen to be the concept identifier is, therefore, the T(n) = O(n (k+2/p) ).Table 3 lists the time complexities of all the algorithms used in this article including the RAN's time complexities with both K-means and Affinity Propagation algorithms.In Table 3 the complexities of K-means and Affinity Propagation algorithms, in fact, are the complexities of the RAN's modeling because their complexities are greater than O(n).</p>
<p>Behavioral Demonstration of RANs</p>
<p>This section exhibits two distinct aspects of RANs modeling via separate experiments.Both investigations present a different view of RANs methodology, highlighting the capabilities of the RANs approach.</p>
<p>Experiment with IRIS Dataset</p>
<p>There are two objectives of this probe, first is to demonstrate flexibility in choosing an appropriate methodology for concept Identification operation in RANs modeling (see Section 4.2).The second is to show how RANs modeling can be used to build a deep hierarchy of convex abstract concepts dynamically.This experiment uses affinity propagation [58] clustering algorithm as a concept identifier to support the claim of independence in selecting a suitable clustering method for CI process in RANs modeling.Unlike the K-means algorithm (used to describe the RANs methodology in Section 4), with the affinity propagation algorithm, the number of clusters within the data need not be known beforehand.Furthermore, affinity propagation conforms to the basic requirements (see Section 4.2) for being a concept identifier in RANs modeling.</p>
<p>The second prospect of this experiment is to illustrate the dynamic topology of the RAN's approach where the network grows to form several layers representing convex abstract concepts.For this demonstration, an algorithm is developed, named Concept Hierarchy Creation (CHC) algorithm (see Algorithm 2).The CHC algorithm streamlines all four steps of RANs modeling (i.e., CI, CC, ILL and UAP) and uses these steps iteratively to build a hierarchy of convex abstract concepts as described through Algorithm 2. This experiment was also conducted using the IRIS dataset obtained from the UCI machine learning repository [60].In the CHC algorithm the affinity propagation clustering algorithm was initialized with the following parameters: ( 1 Input layer-0 was created, with four nodes (equal to the dimension of IRIS data), and the RANs hierarchy generation was carried out according to Algorithm 2. The model obtained from the CHC process is depicted by Figure 7, the model was initialized to grow six layers deep.Therefore, hierarchy augmentation terminates at Layer-5, with Layer-5 identified as most abstract layer consisting of three nodes acting as abstract representatives of three categories of flowers of IRIS dataset.To evaluate the obtained RANs model, True-labels, and Test-labels were retrieved using an abstract concept labeling procedure (see Appendix A.3).A confusion matrix (see Figure 8a) was generated using the True and Test labels.With the aid of the confusion matrix, Precision, Recall, F1-Score, and Accuracy were calculated to evaluate the model.The model performed quite decently with an observed accuracy of 93.33 (ca.), the results of precision, recall, and F1-Score are reported in Table 4.</p>
<p>The ROC curve analysis of the RANs model, as shown in Figure 8b, displays the various operating characteristic and the observed Area Under the Curve for all the classes of IRIS data.</p>
<p>In this experiment, it is worth mentioning the application of RANs modeling for data dimension transformation and data visualization.In Figure 7 we can observe that the dimension of Layer-0 is four, whereas the size of the other layers either expands or reduces when the network grows.This dimension transformation operation helps address the issue of the cures of dimensionality.Besides, the transformed data can be plotted to extract useful information from the data.</p>
<p>Experiment with Human Activity Recognition Data</p>
<p>This experiment aims to show the ability of the RAN's approach to building the representation of generic concepts.The experiment uses UCIHAR [61] dataset for home activity recognition using the smartphone, obtained from the UCI machine learning repository.The data captured six activities: walking, walking_upstairs, walking_downstairs, sitting, standing, and laying.The hypothesis of this experiment is that the labels walking, walking_upstairs, walking_downstairs are identified by an abstract concept (say) mobile and the other three labels sitting, standing, and laying by abstract concept (say) immobile.In this experiment also classification operation can be used to prove the hypothesis.</p>
<p>The UCIHAR dataset was normalized and a header was attached.In CHC algorithm K-means is chosen as a concept identifier and the parameter desired-depth was set to 1 so that model has only two layers.The K-means was configured with K = 2 because the model was hypothesized to have 2 abstract concepts at Layer-1.Having fulfilled the initialization part of the CHC algorithm modeling is performed, generating a two-layered model as depicted in Figure 9.In Figure 9 Layer-0 shows input-layer and Layer-1 corresponds to the abstract concept layer where both nodes (N 1 , and N 2 ) represents either of the two abstract concepts (i.e., mobile and immobile abstract concepts).</p>
<p>Among captured six activities (walking, walking_upstairs, walking_downstairs,sitting, standing and laying), walking, walking_upstairs, and walking_downstairs are the actions of motion, whereas the remaining three represent static states.Based upon these two facts, we expect that one of the abstract nodes in Layer-1 conjointly represents walking, walking_upstairs and walking_downstairs as one class.The other node in Layer-1 stages the other three categories (i.e., sitting, standing and laying) together.Upon performing the labeling of nodes at Layer-1 through ACL procedure (see Appendix A.3 for ACL process elaboration), it was observed that walking, walking_upstairs, and walking_downstairs classes were mapped to one node of Layer-1.Whereas, the labels sitting, standing and laying traced to the other node in Layer-1.Interestingly, this outcome commensurate with the expectations from this experiment and shows the RANs capability to identify abstract concepts in an unsupervised manner naturally.The True-label and Test-label obtained through ACL operation were used to form the confusion matrix, which is later referred to calculate precision, recall, F1-score, and accuracy for evaluating the generated model.Node-wise binary labels and confidence scores were determined (as described in Appendix A.5) for both abstract nodes at Layer-1. Figure 10 shows the Area Under the Curve (AUC) observed during the ROC curve analysis of all 10-folds in different research designs.With both these evaluations it is deduced that, apart from building the representation of abstract concepts, the model generated with RANs performed satisfactorily.The RANs modeling was compared with five different types of approaches based upon their classification operation.To carry out the comparative study it was essential to transform the six labels into binary labels, because RANs modeling was identifying two abstract concepts, and its performance was measured based upon them.Thus, with these five approaches, the Labels of the dataset were merged to form two groups, i.e., walking, walking_upstairs, and walking_downstairs in Class-1, and sitting, standing, and laying in Class-2.Later the modeling was performed followed by validation and evaluation.Table 5 displays the comparison of all five approaches with RANs modeling.It is observed that RANs approach is competent to these five techniques, with an added advantage of being an unsupervised approach, and ability to build representations of abstract concepts.</p>
<p>RANs Applicability and Observations</p>
<p>This section highlights the scope of RANs modeling as a classifier with respect to distinct domains.To support this ambit of RANs usability, experimental results are reported using eight datasets concerning different areas.A comparative study was also carried out using these datasets to match RANs classification ability with five different classifiers.Table A5 in Appendix A. 5 shows configurations of all the models for all the experiments.Table A4 in Appendix A. 4 provide the details about the all the datasets used in this article.</p>
<p>Among the eight datasets (Appendix A.4 lists the description of all the datasets used in the article), the Mice Protein [62], Mammographic Mass [63], Breast Cancer 569 and 669 [64,65] data pertain to the medical field, Glass Identification [66] data representing forensic science, Credit Approval [67] represents economic data, Iris [68] is a botanical data set, and Wine Recognition [69] is a data set for chemical composition analysis.The experiments performed with these datasets were akin to the investigations done with Toy-data (in Section 4), and UCIHAR data (in Section 5.2), i.e., K-means algorithm used as concept identifier, where 'K' is the number of class labels of each dataset, the hierarchy is set to have a depth of two layers (one Input and one abstract concept layer).For every dataset, models were generated using thirty iterations in nine Research Designs (RDs) (refer the Table A3 in Appendix A.2).In every RD 10-Fold cross-validation was applied to determine the performance of the models.An aggregate of precision, recall, F1-Score, and accuracy of all folds in all RDs was calculated for all the datasets, as shown in Figure 11a.From the Figure 11a it can be observed that with Mice Protein data RANs scores 99.99% (ca.) for all evaluation metric, whereas for Iris, Glass Identification, Breast Cancer, and Wine Recognitions the observations were convincing, i.e., above 89.00%(ca.).In all the folds of nine RDs ROC curves were also plotted for each class label of the eight datasets, the mean AUC for each class of the datasets is shown in Figure 11b.The evaluation metrics and ROC-AUC analysis (Figure 11a,b respectively) display the RAN's capability in machine learning tasks with different kind of datasets.</p>
<p>The same procedure was applied to obtain average Precision, Recall, F1-Score and Accuracy for all the datasets with five other classifiers (i.e., RBM+, KNN, LR, MLP, and SGD).Table 6 shows the overall comparison.It is worth noting that being dynamic and unsupervised RANs modeling performed quite satisfactorily especially with Mice Protein data, where it outperformed SGD and RBM+, was found competent with LR, KNN and MLP classifiers.Figure 12 shows four graphs depicting RANs performance with different benchmark data sets.These graphs display an important aspect of RANs modeling and its performance behavior when evaluated to different research design Figure 12.The precision, recall, F1-Score, and accuracy trajectories of Human Activity Recognition (HAR), Breast Cancer 669 (BC1), Toy-data (TD) and Mice Protein (MP) Data is almost straight.The evaluation plots of Glass Identification (GI), Wine Recognition (WR), Mammographic Mass (MM), Breast cancer 569 (BC2) and Mice Protein (MP) datasets show a minimal decline in observations w.r.t RD-1 and RD-9 Research Design.On the contrary, results from IRIS Data (ID) and Credit Approval (CA) dataset depicted a higher value while comparing the evaluation of RD-1 with RD-9 Research Designs of these data sets.Principally, the results of all four metrics of evaluation obtained similar results (with marginal variation) irrespective of the Test and Train data ratio.This is a notable observation because it shows that the RAN's approach obtains a satisfactory result even when trained with a small amount of data.Besides classification comparison, the RAN's modeling is compared with the five classifiers based upon seven features: (1) Whether the modeling in graph-based; (2) whether the modeling has a dynamic topology; (3) and (4) whether modeling can reduce or expand the dimension of the data;</p>
<p>(5) whether modeling can perform classification; and (7) whether modeling is biologically inspired or not.Table 7 details this comparative study.It can be observed from this table that RAN is closely related to the models that are biologically inspired i.e., RBM and MLP.</p>
<p>Conclusions and Future Work</p>
<p>To comprehend and reasoning for emotions, ideas, etc., it is evident to understand abstract concepts because they are perceived differently from concrete concepts.There have been notable efforts to study Concrete concepts (features like walking or ingredients), but progress in investigating abstract concepts (generic features such as is-moving or recipe) is relatively less.This article proposes an unsupervised computational modeling approach, named Regulated Activation Networks (RANs), that has an evolving topology and learns a representation of abstract concepts.The RAN's methodology was exemplified through a UCI's IRIS dataset, yielding a satisfactory performance evaluation of 95% (ca.) for precision, recall, F1-Score and accuracy metrics, along with an average AUC of 99% (ca.) for all the three classes in the dataset.These evaluation result not only showed the classification capability of RANs but also proved the hypothesis of the experiment i.e., the three newly created nodes in the Layer-1 symbolically represent the three classes of IRIS data as abstract concepts.</p>
<p>Another experiment with IRIS data displayed the characteristic of RAN's deep hierarchy generation and independence in choosing the concept Identifier.With the aid of the Concept Hierarchy Creation algorithm (proposed in Section 5.1), the evolving nature of RAN's modeling is shown using the Affinity Propagation clustering algorithm (as an alternate concept Identifier instead of the K-means algorithm as used in modeling with a Toy-data problem).With the generated model it was shown that the model dynamically grew to a depth of six layers and performed with Precision of 94.44% (ca.), Recall of 93.33% (ca.), F1-Score of 93.26% (ca.) and Accuracy of 93.33% (ca.), along with an observed AUC of 100% (ca.), 92% (ca.) and 94% (ca.) for the three classes of data.This experiment also highlights the application of RANs modeling in data dimension transformation and data visualization.</p>
<p>Modeling with UCI's IoT based Home Activity Recognition (UCIHAR) smartphone sensor dataset exhibited the RAN's behavior of natural identification of generic concepts.The experiment hypothesize that six data labels (activity of walking, walking_upstairs, walking_downstairs, sitting, standing and laying) of the dataset are to be identified as mobile (walking, walking_upstairs and walking_downstairs) and immobile (sitting, standing and laying) abstract concepts.This hypothesis was also proven using classification operation, where, the evaluation of the model shown a performance of 99.85% (ca.) for all four metrics and AUC of 99.9% (ca.) for both abstract concepts.The experiment also demonstrates how RAN can be used to model the data from the IoT domain in an unsupervised manner.</p>
<p>The proof of concept of RAN's modeling as a Machine Learning classifier was also provided with eight UCI benchmarks.It was identified that RAN's approach performed satisfactorily displaying the best outcome of 98.9% (ca.) with Mice Protein dataset (for all metrics).The comparison of RAN's modeling with five classifiers substantiated the effectiveness of the proposed methodology.We also observed that the RAN's performance remained similar irrespective of the size of train data.RAN was also compared with the five classifiers based upon its features and it was observed that RAN was similar to bio-inspired models.The model presented in this article is capable of modeling data that is convex which limits the RAN's performance with non-convex (or complex) datasets.As future work, we intend to improve RANs modeling that can capture the non-convexity in the data and enhance the performance of the model with complex datasets.</p>
<p>Figure 1 .
1
Figure 1.A universe of concepts in six-dimensional feature hyper-space.The ovals in the diagram depict individual concepts.Each individual concept is described by their defining six-dimensions.The cluster of concepts shows the groups formed by similar concepts represented by a convex cluster of concepts, and the cluster centers depicts the most generic concept of the cluster.</p>
<p>Figure 2 .
2
Figure 2. The color space [36].</p>
<p>Figure 3 .
3
Figure 3. Plot of Toy-data, a 2-D artificially generated data.The plot shows five classes along with their cluster centers.</p>
<p>also displays the centroids (C 1 , ..., C 5 ) of all the clusters, recognized as CRDPs of all five classes and will be used in Inter-Layer Learning (ILL) in Step 3 (see Section 4.4).</p>
<p>Figure 4 .
4
Figure 4. Steps in model generation with Regulated Activation Networks.</p>
<p>Figure 5 .
5
Figure 5. Plot of Similarity Translation Function with respect to varying input values in range [0, 1].</p>
<p>Figure 6 .
6
Figure 6.Area Under Curve for five classes of Toy-data for nine Research Designs (RD) of varying Test and Train data sizes.</p>
<p>Algorithm 2
2
) damping_factor (DF) = 0.94 for layers below level 3, DF = 0.9679 for the layers at level 3 and above; (2) convergence_iteration = 15; (3) max_iteration = 1000.Concept Hierarchy Creation algorithm Input: Multi-variate data with values between [0,1].Output: Set of layers of concepts-concept hierarchy.Initialization: Create input layer layer-0 having dimension equal to that of input data.Set Current-layer-size CLS = i, dimension of input-data vector.Set Layer-count L = 0. Set Desired-depth = 6.Select Clustering algorithm and initialize.Set current-data = input-data.repeat Run clustering algorithm on current-data to identify set of cluster centers C. Create a new-layer above current-layer, with no nodes.for each cluster center C j ∈ C do Create new node j in new layer l+1.for each node i in current-layer do Create a new weighted connection W c j ,i between c j and i such that W c j ,i is the coordinate of c along the i dimension.end for end for Set new-data = empty data set.for each datum in current-data do Inject datum in current-layer Propagate activation from current-layer to new-layer using algorithm 1. Add activation pattern produced in new-layer to new-data.end for Set L = L + 1. Set CLS = number of clusters in current-layer.Set current-data = new-data.Set current-layer = new-layer.until CLS = 1 OR Desired-depth = L.</p>
<p>Figure 8 .
8
Figure 7.The model generated with 90% stratified IRIS data using Concept Hierarchy Creation (CHC) algorithm.Layer-0 is created while initializing the CHC algorithm.The algorithm grew to a Desired-depth of six Layers (including input Layer-0), and in each iteration of the CHC algorithm a new layer is created dynamically and the interlayer weights (ILW) are learned between the existing layer and a newly created layer above it.</p>
<p>Figure 9 .
9
Figure 9. Model generated with RANs approach.Nodes N 1 and N 1 at Layer-1 represents either of the two abstract concepts, i.e., mobile and immobile.Each node at Layer-0 represents individual dimensions of input data vector.</p>
<p>Figure 10 .
10
Figure 10.Area Under Curve observed during ROC curve analysis of UCIHAR data to determine operational points of two abstract concepts (i.e., Mobile and Immobile) for all nine Research Designs (RD).</p>
<p>Figure 11 .
11
Figure 11.RANs performance with eight datasets using Precision, Recall, F1-Score and Accuracy along with ROC-AUC analysis with Eight benchmark datasets [Mice Protein (MP), Breast Cancer 669 (BC1), Breast Cancer 569 (BC2), Credit Approval (CA), IRIS data (ID), Mamographic Mass (MM), Wine Recognition (WR) and Glass Identification (GI)].(b) shows the plot of percentage AUC for classes 1 to 8.For each dataset class labels of the graph is serially mapped as: Mice protein (c-CS-s [Class-1], c-CS-m [Class-2], c-SC-s [Class-3], c-SC-m [Class-4], t-CS-s [Class-5], t-CS-m [Class-6], t-SC-s [Class-7] and t-SC-m [Class-8]); Mammographic Mass (Benign [Class-1] and Malignant [Class-2]); Credit Approval (Postitive [Class-1] and Negative [Class-2]); IRIS) (Setosa [Class-1], Versicolar [Class-2] and Verginica [Class-3]); Breast Cancer 569 (Benign [Class-1] and Malignant [Class-2]); Breast Cancer 669 (Benign [Class-1] and Malignant [Class-2]), Wine Recognition (Class-1, Class-2 and Class-3) Glass Identification (Window Glass [Class-1] and Non-Window Glass [Class-2]).(a) RANs performance with eight different datasets depicting RANs appositeness with data belonging to distinct domains; (b) Observed Area Under Curve (AUC) while performing ROC curve analysis for RANs model generated with eight different datasets.</p>
<p>Figure 12 .
12
Figure 12.RANs evaluation metric (precision, recall, F1-Score and accuracy) value behavior with respect to varying test and train data ratio over ten datasets [Mice Protein (MP), Breast Cancer 669 (BC1), Breast Cancer 569 (BC2), Credit Approval (CA), IRIS data (ID), Mamographic Mass (MM), Human Activity Recognition (HAR), Toy-data(TD), Wine Recognition (WR) and Glass Identification (GI)] (a) Precision; (b) Recall; (c) F1-Score; (d) Accuracy.</p>
<p>Table 1 .
1
Notations.
NotationDescriptionWInter-Layer weight matrixAOutput ActivationaInput Activationn
a Number of elements in input vector at Layer l n A Number of elements in output vector at Layer l + 1 l l'th Layer representative d Normalized Euclidean distance C Cluster center or Centroids i, j, k Variables to represent node index for input-level, abstract-level, and arbitrary node index in either of the levels, respectively t Iterator variable f (x) Transfer function to obtain similarity relation</p>
<p>Table 2 .
2
RAN's Comparative Study for Toy-data.
Model Precision (%) Recall (%)F1-Score (%) Accuracy (%)RBM90.87 ± 01.26 85.25 ± 2.61 82.34 ± 3.8585.25 ± 2.61K-NN99.96 ± 00.08 99.95 ± 0.11 99.94 ± 0.1299.95 ± 0.11LR99.65 ± 00.07 99.64 ± 0.07 99.64 ± 0.0799.64 ± 0.07MLP95.62 ± 11.18 96.82 ± 7.56 96.02 ± 9.9596.82 ± 7.56RANs99.12 ± 00.09 99.12 ± 0.09 99.12 ± 0.0999.12 ± 0.09SGD96.00 ± 02.81 95.25 ± 2.8694.57 ±3.7695.25 ± 2.86</p>
<p>Table 3 .
3
Time Complexities of Models used in the Article.
AlgorithmTime ComplexityDescriptionSourceK-meansO(n k+2/p )n: n_samples; k: n_clusters; p: n_features[59]Affinity PropagationO(n 2 )n: n_samples[59]MLPO(n • [59]RBMO(d 2 )d: max(n_components, n_features)[59]KNNO(m • n • i)m: n_components; n: n_samples; i: min(m, n)[59]LRO(n • m 2 )n: n_samples; m: n_features[59]n: n_samples; k: n_iterations;SGDO(k • n • p)p: the average number of non-zero[59]attributes per sample
m • h k • o • i)n: n_samples; m: features; k: no. of hidden layers; h: number of hidden neurons o: output neuron; i: no. of iterations</p>
<p>Table 4 .
4
Evaluation of RANs Model generated through IRIS data.
ClassPrecision (%) Recall (%) F1-Score (%) SupportSetosa1001001005Versicolour83.3310090.915Virginica1008088.895Avg/Total94.4493.3393.2615</p>
<p>Table 5 .
5
RAN's Comparative Study for UCIHAR dataset.
Model Precision (%) Recall (%)F1-Score (%) Accuracy (%)RBM99.68 ± 0.1499.68 ± 0.1499.68 ±0.1499.68 ± 0.14K-NN99.96 ± 0.0299.96 ± 0.02 99.96 ± 0.0299.96 ± 0.02LR99.97 ± 0.0299.97 ± 0.02 99.97 ± 0.0299.97 ± 0.02MLP99.96 ± 0.0299.96 ± 0.02 99.96 ± 0.0299.96 ± 0.02RANs99.85 ± 0.0199.85 ± 0.01 99.85 ± 0.0199.85 ± 0.01SGD99.98 ± 0.0199.98 ± 0.01 99.98 ± 0.0199.98 ± 0.01</p>
<p>Table 6 .
6
RANs comparison with eight datasets belonging to different domains.
DataAlgoPrecision (%)Recall (%)F1-Score (%) Accuracy (%) DataAlgoPrecision (%)Recall (%)F1-Score (%) Accuracy (%)RBM+43.45 ±44.07 53.50 ± 38.23 45.46 ± 43.36 53.50 ± 38.23RBM+93.60 ± 2.6993.51 ± 2.7793.46 ± 2.8693.51 ± 2.77MiceProteinKNN LR MLP RAN98.63 ± 3.97 98.99 ± 1.94 98.54 ± 2.19 99.98 ± 0.0698.34 ± 4.84 98.28 ± 3.38 98.23 ± 2.71 99.97 ± 0.0698.07 ± 5.65 98.14 ± 3.71 97.83 ± 3.34 99.89 ± 0.0698.34 ± 4.84 98.28 ± 3.38 98.23 ± 2.71 99.97 ± 0.06BreastCancer 569KNN LR MLP RAN99.80 ± 0.59 99.89 ± 0.07 98.67 ± 0.94 93.17 ± 0.3699.79 ± 0.62 99.89 ± 0.07 98.65 ± 0.96 92.97 ± 0.3699.78 ± 0.63 99.89 ± 0.07 98.64 ± 0.96 92.87 ± 0.4299.79 ± 0.62 99.89 ± 0.07 99.89 ± 0.07 92.97 ± 0.36SGD99.11 ± 1.8498.84 ± 2.4698.68 ± 2.8198.84 ± 2.46SGD99.87 ± 0.1399.85 ± 0.1899.83 ± 0.2099.85 ± 0.18RBM+95.72 ± 3.6295.34 ± 4.6095.13 ± 5.1695.34 ± 4.60RBM+76.44 ±12.50 75.63 ±12.98 74.04 ±14.5975.63 ±12.98BreastCancer 669KNN LR MLP RAN99.46 ± 0.88 99.16 ± 0.17 98.96 ± 0.76 95.18 ± 0.2599.44 ± 0.93 99.14 ± 0.17 98.95 ± 0.76 95.15 ± 0.2499.43 ± 0.94 99.15 ± 0.17 98.95 ± 0.77 95.11 ± 0.2599.44 ± 0.93 99.14 ± 0.17 98.95 ± 0.76 95.15 ± 0.24CreditApprovalKNN LR MLP RAN95.48 ± 0.16 95.06 ± 0.38 98.02 ± 1.32 80.67 ± 1.3795.46 ± 0.17 95.04 ± 0.39 98.00 ± 1.34 79.58 ± 1.0595.46 ± 0.17 95.04 ± 0.39 97.99 ± 1.34 79.66 ± 1.1395.46 ± 0.17 95.04 ± 0.39 98.00 ± 1.34 79.58 ± 1.05SGD99.88 ± 0.1699.88 ± 0.1699.18 ± 0.1699.88 ± 0.16SGD99.77 ± 0.3999.75 ± 0.4099.75 ± 0.4099.75 ± 0.40RBM+82.58 ±10.2984.19 ± 4.9080.61 ± 8.4284.19 ± 4.90RBM+84.85 ±16.54 85.18 ±14.98 82.42 ±20.3085.18 ±14.98GlassIdentificationKNN LR MLP RAN94.08 ±12.12 99.52 ± 0.18 93.78 ± 1.40 90.07 ± 0.4395.97 ± 7.32 99.49 ± 0.18 93.28 ± 1.52 89.18 ± 1.2394.82 ±10.59 99.49 ± 0.18 92.85 ± 1.64 89.32 ± 1.1095.97 ± 7.32 99.49 ± 0.18 93.28 ± 1.52 89.18 ± 1.23MamographicMassKNN LR MLP RAN99.65 ± 0.88 99.41 ± 0.30 98.91 ± 2.11 80.28 ± 0.1899.64 ± 0.89 99.40 ± 0.30 98.79 ± 2.35 79.20 ± 0.2399.64 ± 0.89 99.40 ± 0.30 98.79 ± 2.35 79.08 ± 0.2499.64 ± 0.89 99.40 ± 0.30 98.79 ± 2.35 79.20 ± 0.23SGD97.95 ± 0.6697.87 ± 0.6997.82 ± 0.7097.87 ± 0.69SGD99.96 ± 0.0399.94 ± 0.0799.93 ± 0.0999.94 ± 0.07RBM+79.81 ±11.9177.41 ±11.8870.66 ±16.2877.41 ±11.88RBM+56.00 ±25.66 67.05 ±16.91 59.07 ±21.9167.05 ±16.91IRISKNN LR MLP RAN90.41 ±28.77 97.38 ± 4.15 97.31 ± 0.71 95.43 ± 0.6792.80 ±21.61 96.64 ± 5.65 96.86 ± 1.13 95.02 ± 0.9491.00 ±27.01 96.45 ± 6.12 96.81 ± 1.21 94.98 ± 0.9892.80 ±21.61 96.64 ± 5.65 96.86 ± 1.13 95.02 ± 0.94WineRecognitionKNN LR MLP RAN90.74 ±26.00 92.88 ±19.48 91.14 ±24.70 94.14 ± 1.55 93.13 ± 1.82 93.00 ± 1.92 97.44 ± 0.51 97.33 ± 0.59 97.32 ± 0.59 94.87 ± 0.91 94.34 ± 1.00 94.29 ± 1.0192.88 ±19.48 93.13 ± 1.82 97.33 ± 0.59 94.34 ± 1.00SGD94.47 ± 6.4094.46 ± 5.2093.31 ± 6.7894.46 ± 5.20SGD98.13 ± 0.7097.91 ± 0.7597.91 ± 0.7697.91 ± 0.75</p>
<p>Table 7 .
7
Feature based comparative study of RANs with five modeling techniques.
Features\ModelsRBMK-NN LRMLPRANs SGDGraph-BasedYesNoNoYesYesNoDynamic TopologyNoNoNoNoYesNoDimension ReductionYesYesNoYesYesNoDimension ExpansionMay beNoNo May beYesNoUnisupervisedYesNoNoNoYesNoSupports ClassificationYesYesYesYesYesYesBio-inspiredYesNoNoYesYesNo</p>
<p>Table A2 .
A2
Input Data Format for implemented RANs Modeling.
HeaderH-1 H-2.............. H-nD-1D-2..............D-nDataD-1D-2 ............... D-nInstances......
Funding:The work presented in this paper was partially carried out in the scope of the SOCIALITE Project (PTDC/EEI-SCR/2072/2014), co-financed by COMPETE 2020, Portugal 2020-Operational Program for Competitiveness and Internationalization (POCI), European Union's ERDF (European Regional Development Fund), and the Portuguese Foundation for Science and Technology (FCT).Author Contributions: R.S. performed state of the art, developed and implemented the methodology, carried out data selection and methodology validation, and prepared the original draft of the article.B.R. supervised the research work performed the formal analysis, review and editing, took care of funding.A.M.P. conceived the study plan and methodology, supervised the investigation, methodology development and implementation.F.A.C. supervised the research work, performed formal analysis, review and edition, managed funding.All authors have read and agreed to the published version of the manuscript.Conflicts of Interest:The authors declare no conflict of interest.AbbreviationsThe abbreviations used in this manuscript: This section provides links to download the data and python script used to perform RANs modeling experiments, mentioned in this article.The data and script folders can be downloaded from the web URL mentioned in TableA1.The data folder contains many files and the direct path to the files are provided in the TableA1.Similarly, the script folder RAN_V2.0 also contains many folders where Folder RAN consist of the python scripts.The folder Observations is for storing the outcome of the experiments, at the beginning of each experiment the empty folder in directory empty_passes_for_Experiment_Observations must be copied into the Observation directory.The python script related to RANs modeling is in folder RAN, the description is mentioned in the TableA1.The implemented RANs modeling tool in python takes input data in a specific format (shown in TableA2).Besides the data, the inputs require a header as the first row stacked over the original data.Each header element, [H − 1, H − 2, ......., H − n], is the Maximum value possible for their respective column (feature, or dimension).It is assumed that the minimum value of the column is zero, if it is not then the data must be transformed between zero and the maximum positive value as described in Section 4.1.Various experiments, reported in this article, were conducted with several datasets, using six modeling techniques including the proposed methodology i.e., RANs modeling.TableA5in Appendix A.5shows configurations of all the models for all the experiments.The experiments were carried out using python programing language, and implementations of Restricted Boltzmann Machine pipelined with Logistic Regression (RBM+), Logistic Regression (LR), K-Nearest Neighbor (K-NN), Multilayer Perceptron (MLP), and Stochastic Gradient Descent (SGD) models of Scikit-learn library[59].It is to be noted that experiments with RBM were carried out, pipelined with the LR algorithm using the default configuration of its implementation in scikit-learn library.The TableA3lists the nine Research Designs (RD) used in the experiments of this article.In every RD the ratio of the Train and Test data is varied to capture the ability of the classifier being inspected. .This method is optional and useful when the input data is labeled.With this mechanism, we associate an identifier to every Abstract concept node N j .Having generated the RANs model with CI, then trough CC, ILL, input data is sorted label-wise, and perform UAP operation.The propagated data is inspected class-wise, and label node N j with a class-name for which it got the maximum count of the highest activation.For example, suppose input data for class-X has 100 instances, after inspecting the propagated data, it is observed that node N 1 received highest activation 74-times, whereas, with remaining 26 cases other nodes experienced maximum activation, therefore, we recognize node N 1 as representative of class-X.True-Labels are identified by mapping each class of the input instance directly to its respective node representative Observed-Labels are obtained by propagating every test-instance through UAP operation, inspecting which Abstract node received the highest activation for that data-unit, and label it with the class represented by that node.True-Labels and Observed-Labels are used to validate the model's performance.This study is carried out by two processes, first the input true-labels are transformed into a separate vector of binary labels, individually for all Abstract nodes (i.e., 1 for class c1, 0 for all other classes), second, calculating the confidence score for each instance of the input data (or test-data).Both processes are described as follows:ACLAppendix A.4. Dataset Description1Node-wise binary transformation of True-Labels: For example, suppose there are three classes (c1, c2, c3) represented by three abstract nodes (n1, n2, and n3) in RANs model at Layer-1, and let true-label be [c1, c2, c2, c1, c2, c3, c3] for 7 test instances, then for node n1 label will be [1, 0, 0, 1, 0, 0, 0] where 1 represents class c1, and 0 depicts others (i.e., c2, and c3). 2Node-wise confidence-score calculation: This is calculated by averaging activation-value and confidence-indicator of activation for an input instance at an Abstract node.Activation-value is an individual activation of an activation vector obtained by propagating up the data using UAP mechanism of RANs whereas, confidence-indicator is calculated by min-max normalization operation of activation vector.For example, after UAP operation each node (n1, n2, and n3) receives activation [0.89, 0.34, 0.11] (a vector of activation), and confidence-indicator is min-max ([0.89, 0.34, 0.11]) = [1.0,0.29, 0.0].and the confidence-score for nodes n1 = (0.89 + 1.0)/2.0= 0.95, n2 = (0.34 + 0.29)/2.0= 0.32, and n3 = (0.11 + 0.11)/2.0= 0.05.
Conceptual representations in mind and brain: Theoretical developments, current evidence and future directions. M Kiefer, F Pulvermüller, Cortex. 482012</p>
<p>. P Xiao, H Toivonen, O Gross, A Cardoso, J A Correia, P Machado, P Martins, H G Oliveira, R Sharma, A M Pinto, Conceptual representations for computational concept creation. ACM Comput. Surv. 522019</p>
<p>Learning abstract concept embeddings from multi-modal data: Since you probably can't see what I mean. F Hill, A Korhonen, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarOctober 2014</p>
<p>Cognition and control in schizophrenia: A computational model of dopamine and prefrontal function. T Braver, D Barch, J Cohen, Biol. Psychiatry. 461999</p>
<p>Biologically based computational models of high-level cognition. R C O'reilly, Science. 3142006</p>
<p>Computational models of schizophrenia and dopamine modulation in the prefrontal cortex. E Rolls, M Loh, G Deco, G Winterer, Nat. Rev. Neurosci. 92008</p>
<p>Mental illness, suicide and creativity: 40-Year prospective total population study. S Kyaga, M Landén, M Boman, C M Hultman, N Långström, P Lichtenstein, J. Psychiatr. Res. 472013</p>
<p>ACT-R: A theory of higher level cognition and its relation to visual attention. J R Anderson, M Matessa, C Lebiere, Hum. Comput. Interact. 121997</p>
<p>A practical guide to training restricted boltzmann machines. Momentum. G Hinton, 2010926</p>
<p>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. P Vincent, H Larochelle, I Lajoie, Y Bengio, P A Manzagol, J. Mach. Learn. Res. 112010</p>
<p>A unified architecture for natural language processing: Deep neural networks with multitask learning. R Collobert, J Weston, Proceedings of the 25th International Conference on Machine Learning. the 25th International Conference on Machine LearningHelsinki, Finland; New York, NY, USAACM5-9 July 2008. 2008</p>
<p>Learning in reactive sequential decision tasks: The CLARION model. R Sun, T Peterson, Proceedings of the IEEE International Conference on Neural Networks. the IEEE International Conference on Neural NetworksWashington, DC, USAJune 19962</p>
<p>Automatic definition of modular neural networks. F Gruau, Adapt. Behav. 31994</p>
<p>Conceptual Spaces: The Geometry of Thought. P Gärdenfors, 2004MIT PressCambridge, MA, USA</p>
<p>Learning Internal Representations by Error Propagation. D E Rumelhart, G E Hinton, R J Williams, 1985San Diego, CA, USACalifornia Univ San Diego La Jolla Inst for Cognitive ScienceTechnical Report</p>
<p>Statistical Models: Theory and Practice. D A Freedman, 2009Cambridge University PressCambridge, UKChapter 7</p>
<p>An introduction to kernel and nearest-neighbor nonparametric regression. N S Altman, Am. Stat. 461992</p>
<p>Solving large scale linear prediction problems using stochastic gradient descent algorithms. T Zhang, Proceedings of the Twenty-First International Conference on Machine Learning. the Twenty-First International Conference on Machine LearningBanff, AB, Canada; New York, NY, USAACM4-8 July 2004. 2004116</p>
<p>Distinct brain systems for processing concrete and abstract concepts. J R Binder, C F Westbury, K A Mckiernan, E T Possing, D A Medler, J. Cogn. Neurosci. 172005</p>
<p>Distinguishing abstract from concrete concepts in supramodal brain begions. C Gao, L B Baucom, J Kim, J Wang, D H Wedell, S V Shinkareva, Neuropsychologia. 1312019</p>
<p>The representation of abstract words: Why emotion matters. S T Kousta, G Vigliocco, D P Vinson, M Andrews, E Del Campo, J. Exp. Psychology Gen. 2011</p>
<p>Situating abstract concepts. L W Barsalou, K Wiemer-Hastings, Grounding Cognition: The Role of Perception and Action in Memory,Language, and Thought. New York, NY, USACambridge University Press2005</p>
<p>Embodied cognition and abstract concepts: Do concept empiricists leave anything out?. G Löhr, Philos. Psychol. 322019</p>
<p>Why many concepts are metaphorical. R W Gibbs, Jr, Cognition. 611996</p>
<p>Semantic similarity computation for abstract and concrete nouns using network-based distributional semantic models. E Iosif, A Potamianos, M Giannoudaki, K Zervanou, Proceedings of the 10th International Conference on Computational Semantics. the 10th International Conference on Computational SemanticsPotsdam, Germany19-22 March 2013</p>
<p>Network-Based Distributional Semantic Models. E Iosif, 2013Chania, GreeceTechnical University of CretePh.D. Thesis</p>
<p>Genetic evolution of the topology and weight distribution of neural networks. V Maniezzo, IEEE Trans. Neural Netw. 51994</p>
<p>Evolving neural networks through augmenting topologies. K O Stanley, R Miikkulainen, Evol. Comput. 102002</p>
<p>Evolving deep neural networks. R Miikkulainen, J Liang, E Meyerson, A Rawal, D Fink, O Francon, B Raju, H Shahrzad, A Navruzyan, N Duffy, Artificial Intelligence in the Age of Neural Networks and Brain Computing. Amsterdam, The NetherlandsElsevier2019</p>
<p>A Hintze, J A Edlund, R S Olson, D B Knoester, J Schossau, L Albantakis, A Tehrani-Saleh, P Kvam, L Sheneman, H Goldsby, arXiv:1709.05601Markov Brains: A technical introduction. 2017</p>
<p>Principles of Regulated Activation Networks. A M Pinto, L Barroso, Graph-Based Representation and Reasoning. Berlin/Heidelberg, GermanySpringer2014</p>
<p>Perceptual enhancement: persistent effects of an experience. L Jacoby, J. Exp. Psychol. Learn. Mem. Cogn. 91983</p>
<p>Effects of varying modality, surface features, and retention interval on priming in word-fragment completion. H Roediger, T Blaxton, Mem. Cogn. 151987</p>
<p>Creating false memories: Remembering words not presented in lists. H Roediger, K Mcdermott, J. Exp. Psychol. Learn. Mem. Cogn. 211995</p>
<p>Conceptual spaces as a framework for knowledge representation. P Gärdenfors, Mind Matter. 22004</p>
<p>Color naming: A mapping in the IMCS of common color terms. Scandi. L Sivik, C Taft, J. Psychol. 351994</p>
<p>Cognitive representations of semantic categories. E Rosch, J. Exp. Psychol. Gen. 1041921975</p>
<p>Categorization of natural objects. C B Mervis, E Rosch, Ann. Rev. Psychol. 321981</p>
<p>Prototype classification and logical classification: The two systems. E Rosch, New Trends in Conceptual Representation: Challenges to Piaget's Theory. New Jersey, NJ, USALawrence Erlbaum Associates1983</p>
<p>Evaluating subspace clustering algorithms. L Parsons, Proceedings of the Workshop on Clustering High Dimensional Data and its Applications, SIAM International Conference on Data Mining. the Workshop on Clustering High Dimensional Data and its Applications, SIAM International Conference on Data MiningBuena Vista, FL, USALake24 April 2004</p>
<p>. D Unsupervised Livingstone, Learning, </p>
<p>. John Wiley, &amp; Sons, Ltd , 2009Hoboken, NJ, USA</p>
<p>BitCube: Clustering and statistical analysis for XML documents. J Yoon, V Raghavan, V Chakilam, J. Intell. Inf. Syst. 172001</p>
<p>Identifying objects using cluster and concept analysis. A Van Deursen, T Kuipers, Proceedings of the 21st International Conference on Software Engineering. the 21st International Conference on Software EngineeringLos Angeles, CA, USA; Piscataway, NJ, USAACM21-28 May 1999. 1999</p>
<p>Deep learning. Y Lecun, Y Bengio, G Hinton, Nature. 5212015</p>
<p>Deep Learning in Neural Networks: An Overview. J Schmidhuber, Neural Netw. 612015</p>
<p>Learning deep architectures for AI. Y Bengio, Trends Mach. Learn. 22009</p>
<p>Understanding deep architectures using a recursive convolutional network. D Eigen, J Rolfe, R Fergus, Y Lecun, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)Banff, AB, CanadaApril 2014</p>
<p>Representation Learning: A Review and New Perspectives. Y Bengio, A Courville, P Vincent, IEEE Trans. Patt. Anal. Mach. Intell. 352013</p>
<p>A spreading activation theory of memory. J Anderson, J. Verb. Learn. Verb. Behav. 221983</p>
<p>Retrieval time from semantic memory. A Collins, M Quillian, J. Verb. Learn. Verb. Behav. 81969</p>
<p>Application of spreading activation techniques in information retrieval. F Crestani, Artif. Intell. Rev. 111997</p>
<p>Depth of spreading activation revisited: Semantic mediated priming occurs in lexical decisions. T Mcnamara, J Altarriba, J. Mem. Lang. 271988</p>
<p>Spreading activation and arousal of false memories. H Roediger, D Balota, J Watson, </p>
<p>The Nature of Remembering: Essays in Honor of Robert G. Crowder; American Psychological Association. Washington, DC, USA2001</p>
<p>Learning Convolutional Feature Hierachies for Visual Recognition. K Kavukcuoglu, P Sermanet, Y Boureau, K Gregor, M Mathieu, Y Lecun, Advances in Neural Information Processing Systems (NIPS 2010). </p>
<p>Neural Information Processing Systems Foundation, Inc.: South Lake. Tahoe, NV, USA201023</p>
<p>OverFeat: Integrated recognition, localization and detection using convolutional networks. P Sermanet, D Eigen, X Zhang, M Mathieu, R Fergus, Y Lecun, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)Banff, AB, CanadaApril 2014</p>
<p>Perceiving abstract concepts via evolving computational cognitive modeling. R Sharma, B Ribeiro, A M Pinto, F A Cardoso, Proceedings of the IEEE 2018 International Joint Conference on Neural Networks (IJCNN). the IEEE 2018 International Joint Conference on Neural Networks (IJCNN)Rio de Janeiro, Brasil8-13 July 2018</p>
<p>Algorithm AS 136: A K-means clustering algorithm. J A Hartigan, M A Wong, J. R. Stat. Soc. Ser. C Appl. Stat. 281979</p>
<p>Clustering by passing messages between data points. B J Frey, D Dueck, Science. 3152007</p>
<p>Scikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J. Mach. Learn. Res. 122011</p>
<p>Machine Learning Repository. M Lichman, Uci, 2013. 13 March 2020</p>
<p>A public domain dataset for human activity recognition using smartphones. D Anguita, A Ghio, L Oneto, X Parra, J L Reyes-Ortiz, Proceedings of the 21th International European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning. the 21th International European Symposium on Artificial Neural Networks, Computational Intelligence and Machine LearningBruges, BelgiumApril 2013</p>
<p>Self-organizing feature maps identify proteins critical to learning in a mouse model of down syndrome. C Higuera, K J Gardiner, K J Cios, PLoS ONE. 10e01291262015</p>
<p>The prediction of breast cancer biopsy outcomes using two CAD approaches that both emphasize an intelligible decision process. M Elter, R Schulz-Wendtland, T Wittenberg, Med. Phys. 342007</p>
<p>Nuclear feature extraction for breast tumor diagnosis. W N Street, W H Wolberg, O L Mangasarian, Biomedical Image Processing and Biomedical Visualization; International Society for Optics and Photonics. Bellingham, WA, USA1993. 1905</p>
<p>Robust linear programming discrimination of two linearly inseparable sets. K P Bennett, O L Mangasarian, Optim. Methods Softw. 11992</p>
<p>Rule Induction in Forensic Science. I W Evett, E J Spiehler, 1987UKCentral Research Establishment, Home Office Forensic Science Service: BirminghamTechnical Report</p>
<p>Simplifying decision trees. J R Quinlan, Int. J. Hum. Comput. Stud. 511999</p>
<p>The use of multiple measurements in taxonomic problems. R A Fisher, Ann. Hum. Gen. 71936</p>
<p>PARVUS: An extendable package of programs for data exploration, classification and correlation. M Forina, R Leardi, C Armanino, S Lanteri, P Conti, P Princi, J. Chemom. 41988</p>
<p>This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license. Licensee MDPI. are available from the authors. c 2020 by the authors</p>            </div>
        </div>

    </div>
</body>
</html>
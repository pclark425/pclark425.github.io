<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5802 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5802</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5802</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-d03a9b2a0e090cc9fd2ba0a457ecea35372f1018</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d03a9b2a0e090cc9fd2ba0a457ecea35372f1018" target="_blank">Demystifying Prompts in Language Models via Perplexity Estimation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Over a wide range of tasks, it is shown that the lower the perplexity of the prompt is, the better the prompts are able to perform the task and a new empirical hypothesis is established: the performance of a prompt is coupled with the extent to which the model is familiar with the language it contains.</p>
                <p><strong>Paper Abstract:</strong> Language models can be prompted to perform a wide variety of zero- and few-shot learning problems. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens or how to pick the best prompts. In this work, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is coupled with the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that the lower the perplexity of the prompt is, the better the prompt is able to perform the task. As a result, we devise a method for creating prompts: (1) automatically extend a small seed set of manually written prompts by paraphrasing using GPT3 and backtranslation and (2) choose the lowest perplexity prompts to get significant gains in performance.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5802.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5802.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-Perplexity Predictor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Perplexity as Predictor of Prompt Quality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's central empirical finding: for a wide range of tasks and autoregressive LMs, lower perplexity of a (task-related) prompt under the model correlates with better downstream performance (higher label log-probability / accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT, Bloom</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B, 30B, 175B (OPT); 176B (Bloom)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple: classification tasks, antonym prediction, word-level translation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot evaluation on (i) classification datasets (AG News, Newspop, GLUE Cola, IMDB, DBpedia, Emotion, Tweet Offensive), (ii) antonym prediction (WordNet-derived), and (iii) word-level translation (NorthEuraLex) in multiple languages.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural-language prompts (human-written seed prompts expanded via paraphrasing/backtranslation); prompts include the input and list choices for classification tasks ("Choices: ... Answer:"). Perplexity of full prompt (including input, excluding label) averaged over 1,000 examples is used as the predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Correlation between prompt perplexity and model performance: e.g., for AG News with OPT 175B, perplexity-vs-accuracy Pearson = -0.77, Spearman = -0.81 (both highly significant). Word-level translation correlations (OPT 175B) per language: e.g., Spanish Pearson = -0.47, Spearman = -0.61 (all languages show consistent negative correlations, Table 6). Many other tasks show negative, often significant correlations (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Quantitative effect reported as correlation coefficients (Pearson/Spearman). Example extremes: AG News perplexity-accuracy Pearson -0.77 (OPT175B); many tasks show Pearson in range approx. -0.15 to -0.66 across models/tasks (see Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>lower prompt perplexity associated with improved performance</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Hypothesis: prompts with lower perplexity are more familiar to the model (i.e., phrases or closely related phrases occurred more in pretraining), so the model better understands/exploits the prompt to extract relevant information; perplexity is used as a proxy for 'familiarity' / expectedness under the pretrained LM.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Not universal: some tasks/models show weak or non-significant correlations (e.g., GLUE Cola often weak), and some models/tasks yield positive or small correlations in particular settings (see Table 5). Also the relationship structure varies by task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Prompts in Language Models via Perplexity Estimation', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5802.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5802.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPELL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selecting Prompts by Estimating LM Likelihood (SPELL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical method that expands a small seed prompt set and ranks prompts by model perplexity, selecting the lowest-perplexity prompts to reduce variance and improve average zero-shot accuracy without using labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT, Bloom</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>OPT: 1.3B, 30B, 175B; Bloom: 176B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same classification and prediction tasks as analyzed (AG News, Newspop, GLUE Cola, IMDB, DBpedia, Emotion, Tweet Offensive, antonyms, word-level translation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot classification and word/antonym prediction using human-readable prompts; SPELL chooses k lowest-perplexity prompts (k=3 in experiments) from an expanded pool.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Generate expanded prompt pool (manual seed -> GPT-3 paraphrases -> backtranslation), compute prompt perplexity averaged on 1,000 task examples, rank prompts, pick lowest-k prompts for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against using the manually-created seed prompts (baseline manual prompt set).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy improvements when using the 3 lowest-perplexity prompts vs manual prompts: OPT average +1.8 percentage points; Bloom average +2.3 percentage points (across classification tasks). Example per-task deltas (OPT): GLUE Cola +3.1, Newspop +10.2, AG News +6.5; Bloom: GLUE Cola +3.6, Newspop +10.0, but AG News -12.5 (task-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Manual prompts average accuracy reported per task; SPELL (low-ppl 3 prompts) average accuracy reported per task (Table near Section 7). See example deltas above.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Average improvement: +1.8% (OPT), +2.3% (Bloom). Reduced variability: standard deviation across the 3 lowest-perplexity prompts (OPT avg) 5.07 vs manual prompts 6.86; Bloom 2.6 vs manual 7.47.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved on average (and more stable), though some tasks show negative deltas for specific model/task pairs</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Selecting low-perplexity prompts tends to pick prompts the model is more familiar with, yielding more consistent and often higher zero-shot performance; requires only unlabeled inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>AG News with Bloom showed decreased performance when using SPELL in that instance (Bloom: low-ppl 51.0 vs manual 63.5, delta -12.5), indicating that model-specific differences mean SPELL is not uniformly beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Prompts in Language Models via Perplexity Estimation', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5802.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5802.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Choice-List Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicitly Listing Label Choices in Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For classification tasks, the authors appended an explicit 'Choices: X, Y, Z. Answer:' clause to prompts to define the label space in zero-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT, Bloom</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B, 30B, 175B (OPT); 176B (Bloom)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification tasks (AG News, Newspop, GLUE Cola, IMDB, DBpedia, Emotion, Tweet Offensive)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot classification where the prompt follows the input and includes explicit enumeration of possible class labels.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt format: [input] [instruction/question]. Choices: label1, label2, label3. Answer: (model scores each listed label).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report that adding the choices helps accuracy in their zero-shot setting; accuracy numbers in Table 5 are computed with this prompting style (e.g., AG News OPT175B average accuracies reported are with choices present).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (authors state it helps accuracy and reduces surface-form ambiguity)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Explicitly listing labels likely helps because (1) the model has no demonstrations to learn label set from, and (2) it reduces surface-form competition by constraining possible outputs to the desired label tokens (reference to Holtzman et al. 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Prompts in Language Models via Perplexity Estimation', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5802.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5802.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quotation-Formatting (WLT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of Quotation Marks Around Source/Target Words in Word-Level Translation Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>In word-level translation prompts, including quotation marks around the source word consistently produced lower perplexity and higher label scores than prompts without quotes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT, Bloom</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>OPT 175B (analysis shown), Bloom 176B (also analyzed)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Word-level translation (NorthEuraLex) across multiple languages</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an English source word and a target language, prompt asks model to produce the translated word (zero-shot). Prompts differ in surface formatting such as use of quotation marks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompts examples: with quotes: 'The word for "dog" in French is "' vs without quotes: 'The word dog in French is '.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparison between prompts with quotation marks vs without quotation marks for the same translation task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Prompts with quotation marks show both lower perplexity and higher correct-label score; within the quotation-mark cluster, correlations between perplexity and score remain negative and statistically significant (range roughly -0.28 to -0.38 reported for OPT 175B). Table 12 shows one low-perplexity example: 'The word for "dog" in French is "' perplexity 7.73.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Cluster separation was clear in scatter plots; within the quoted prompts cluster, negative correlations in range ≈ -0.28 to -0.38 (OPT 175B). Exact pointwise score deltas are visual and tabulated qualitatively in plots and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (quotation marks improved expectedness and label score)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Quotation marks likely make the prompt more closely match natural pretraining text patterns (e.g., quoted lexical items), increasing familiarity and lowering perplexity, which in turn improves model scoring of the correct label.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Prompts without quotation marks form a separate cluster with generally weaker performance and weaker correlations; no case shown where quotes decreased performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Prompts in Language Models via Perplexity Estimation', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5802.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5802.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-Length Null Result</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Length vs Performance Analysis (Null Finding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors tested prompt length as an alternative predictor and found weak positive correlations that were mostly not statistically significant, i.e., length alone does not explain prompt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT, Bloom</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (same models as main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same suite of classification and word/antonym tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Analysis comparing prompt length to downstream performance to test whether simple length is a confounding factor for the perplexity-performance relationship.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Measure of prompt length (token count) vs task performance across prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Correlation of prompt length with performance: weak positive correlations, almost all not statistically significant (authors report this as a negative finding in footnote).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Weak positive correlations; not statistically significant (no meaningful effect size reported).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no reliable effect (null result)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Perplexity is a better predictor than raw prompt length because length does not meaningfully capture the model's familiarity with phrasing; authors explicitly note length yields weak and mostly insignificant correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>This is itself a reported null result: prompt length does not reliably predict performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Prompts in Language Models via Perplexity Estimation', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5802.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5802.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Expansion (Paraphrase + Backtranslation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Prompt Expansion via GPT-3 Paraphrasing and Backtranslation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Method to expand a small manual seed set of prompts using GPT-3 paraphrase generation (several meta-prompts) followed by backtranslation (NLLB) into English to yield diverse prompt variants (~100 prompts per task).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (used to generate paraphrases); evaluated prompts on OPT and Bloom</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3 text-davinci-002 (paraphrasing model); evaluated LMs: OPT 1.3B/30B/175B, Bloom 176B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used to generate prompts for the same classification/word/antonym tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate many paraphrases of seed prompts to form a large candidate prompt pool; these prompts are then ranked by perplexity under target model(s).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Paraphrase generation meta-prompts (7 variants) fed to GPT-3, then backtranslation through 8-16 languages via NLLB to expand set; enforce task-specific constraints for word-prediction tasks (e.g., include source word/language name).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared analyses performed before and after expansion; also manual inspection / filtering of noisy prompts vs original set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Expansion produced dozens–~100 prompts per task (Table 3: e.g., AG News: Step0=4, Step1=23, Step2=108). Using expanded pool + SPELL yields the reported correlations and accuracy improvements (see SPELL).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Enables finding prompts with substantially different perplexities and corresponding performance; manual filtering of top-10% highest-perplexity prompts did not materially change correlations (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>useful / no detrimental effect to core correlation findings</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Expansion increases prompt diversity making it more likely to find low-perplexity (familiar) prompts; automatic paraphrases are high-quality and not the source of the observed perplexity-performance correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Authors inspected the 10% highest-perplexity prompts and found them to be valid prompts; filtering out noisy prompts did not alter the strong correlations, arguing against noise-driven artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Prompts in Language Models via Perplexity Estimation', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5802.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5802.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Manual Wording Sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity to Small Wording Changes in Manual Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstration that small changes in prompt wording can yield large differences in accuracy; e.g., four human-written AG News prompts produce accuracies ranging from 40.9% to 71.2% with OPT 175B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AG News (news topic classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot classification of news articles into topics using single-sentence prompts that ask the model for a label.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Different manually-authored question phrasings appended after the input, e.g., 'What is this piece of news regarding?' vs 'What is the most accurate label for this news article?'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct comparison across four alternative human-written prompts (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracies (OPT 175B): 'What is this piece of news regarding?' = 40.9%; 'What is this article about?' = 52.4%; 'What is the best way to describe this article?' = 68.2%; 'What is the most accurate label for this news article?' = 71.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large differences up to ∼30.3 percentage points between worst and best manual prompts for the same model/task (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format (wording) can greatly improve or reduce accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Wording changes alter prompt familiarity/perplexity and thus the model's ability to map from input to correct label; this motivates perplexity-based selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Prompts in Language Models via Perplexity Estimation', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>Surface form competition: Why the highest probability answer isn't always right <em>(Rating: 2)</em></li>
                <li>Autoprompt: Eliciting knowledge from language models with automatically generated prompts <em>(Rating: 2)</em></li>
                <li>Making pre-trained language models better few-shot learners <em>(Rating: 2)</em></li>
                <li>Reframing instructional prompts to gptk's language <em>(Rating: 2)</em></li>
                <li>Promptsource: An integrated development environment and repository for natural language prompts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5802",
    "paper_id": "paper-d03a9b2a0e090cc9fd2ba0a457ecea35372f1018",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Prompt-Perplexity Predictor",
            "name_full": "Prompt Perplexity as Predictor of Prompt Quality",
            "brief_description": "The paper's central empirical finding: for a wide range of tasks and autoregressive LMs, lower perplexity of a (task-related) prompt under the model correlates with better downstream performance (higher label log-probability / accuracy).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OPT, Bloom",
            "model_size": "1.3B, 30B, 175B (OPT); 176B (Bloom)",
            "task_name": "Multiple: classification tasks, antonym prediction, word-level translation",
            "task_description": "Zero-shot evaluation on (i) classification datasets (AG News, Newspop, GLUE Cola, IMDB, DBpedia, Emotion, Tweet Offensive), (ii) antonym prediction (WordNet-derived), and (iii) word-level translation (NorthEuraLex) in multiple languages.",
            "problem_format": "Zero-shot natural-language prompts (human-written seed prompts expanded via paraphrasing/backtranslation); prompts include the input and list choices for classification tasks (\"Choices: ... Answer:\"). Perplexity of full prompt (including input, excluding label) averaged over 1,000 examples is used as the predictor.",
            "comparison_format": null,
            "performance": "Correlation between prompt perplexity and model performance: e.g., for AG News with OPT 175B, perplexity-vs-accuracy Pearson = -0.77, Spearman = -0.81 (both highly significant). Word-level translation correlations (OPT 175B) per language: e.g., Spanish Pearson = -0.47, Spearman = -0.61 (all languages show consistent negative correlations, Table 6). Many other tasks show negative, often significant correlations (Table 5).",
            "performance_comparison": null,
            "format_effect_size": "Quantitative effect reported as correlation coefficients (Pearson/Spearman). Example extremes: AG News perplexity-accuracy Pearson -0.77 (OPT175B); many tasks show Pearson in range approx. -0.15 to -0.66 across models/tasks (see Table 5).",
            "format_effect_direction": "lower prompt perplexity associated with improved performance",
            "explanation_or_hypothesis": "Hypothesis: prompts with lower perplexity are more familiar to the model (i.e., phrases or closely related phrases occurred more in pretraining), so the model better understands/exploits the prompt to extract relevant information; perplexity is used as a proxy for 'familiarity' / expectedness under the pretrained LM.",
            "counterexample_or_null_result": "Not universal: some tasks/models show weak or non-significant correlations (e.g., GLUE Cola often weak), and some models/tasks yield positive or small correlations in particular settings (see Table 5). Also the relationship structure varies by task.",
            "uuid": "e5802.0",
            "source_info": {
                "paper_title": "Demystifying Prompts in Language Models via Perplexity Estimation",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "SPELL",
            "name_full": "Selecting Prompts by Estimating LM Likelihood (SPELL)",
            "brief_description": "A practical method that expands a small seed prompt set and ranks prompts by model perplexity, selecting the lowest-perplexity prompts to reduce variance and improve average zero-shot accuracy without using labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OPT, Bloom",
            "model_size": "OPT: 1.3B, 30B, 175B; Bloom: 176B",
            "task_name": "Same classification and prediction tasks as analyzed (AG News, Newspop, GLUE Cola, IMDB, DBpedia, Emotion, Tweet Offensive, antonyms, word-level translation)",
            "task_description": "Zero-shot classification and word/antonym prediction using human-readable prompts; SPELL chooses k lowest-perplexity prompts (k=3 in experiments) from an expanded pool.",
            "problem_format": "Generate expanded prompt pool (manual seed -&gt; GPT-3 paraphrases -&gt; backtranslation), compute prompt perplexity averaged on 1,000 task examples, rank prompts, pick lowest-k prompts for evaluation.",
            "comparison_format": "Compared against using the manually-created seed prompts (baseline manual prompt set).",
            "performance": "Average accuracy improvements when using the 3 lowest-perplexity prompts vs manual prompts: OPT average +1.8 percentage points; Bloom average +2.3 percentage points (across classification tasks). Example per-task deltas (OPT): GLUE Cola +3.1, Newspop +10.2, AG News +6.5; Bloom: GLUE Cola +3.6, Newspop +10.0, but AG News -12.5 (task-dependent).",
            "performance_comparison": "Manual prompts average accuracy reported per task; SPELL (low-ppl 3 prompts) average accuracy reported per task (Table near Section 7). See example deltas above.",
            "format_effect_size": "Average improvement: +1.8% (OPT), +2.3% (Bloom). Reduced variability: standard deviation across the 3 lowest-perplexity prompts (OPT avg) 5.07 vs manual prompts 6.86; Bloom 2.6 vs manual 7.47.",
            "format_effect_direction": "improved on average (and more stable), though some tasks show negative deltas for specific model/task pairs",
            "explanation_or_hypothesis": "Selecting low-perplexity prompts tends to pick prompts the model is more familiar with, yielding more consistent and often higher zero-shot performance; requires only unlabeled inputs.",
            "counterexample_or_null_result": "AG News with Bloom showed decreased performance when using SPELL in that instance (Bloom: low-ppl 51.0 vs manual 63.5, delta -12.5), indicating that model-specific differences mean SPELL is not uniformly beneficial.",
            "uuid": "e5802.1",
            "source_info": {
                "paper_title": "Demystifying Prompts in Language Models via Perplexity Estimation",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Choice-List Prompting",
            "name_full": "Explicitly Listing Label Choices in Prompt",
            "brief_description": "For classification tasks, the authors appended an explicit 'Choices: X, Y, Z. Answer:' clause to prompts to define the label space in zero-shot prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OPT, Bloom",
            "model_size": "1.3B, 30B, 175B (OPT); 176B (Bloom)",
            "task_name": "Classification tasks (AG News, Newspop, GLUE Cola, IMDB, DBpedia, Emotion, Tweet Offensive)",
            "task_description": "Zero-shot classification where the prompt follows the input and includes explicit enumeration of possible class labels.",
            "problem_format": "Prompt format: [input] [instruction/question]. Choices: label1, label2, label3. Answer: (model scores each listed label).",
            "comparison_format": null,
            "performance": "Authors report that adding the choices helps accuracy in their zero-shot setting; accuracy numbers in Table 5 are computed with this prompting style (e.g., AG News OPT175B average accuracies reported are with choices present).",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (authors state it helps accuracy and reduces surface-form ambiguity)",
            "explanation_or_hypothesis": "Explicitly listing labels likely helps because (1) the model has no demonstrations to learn label set from, and (2) it reduces surface-form competition by constraining possible outputs to the desired label tokens (reference to Holtzman et al. 2021).",
            "counterexample_or_null_result": null,
            "uuid": "e5802.2",
            "source_info": {
                "paper_title": "Demystifying Prompts in Language Models via Perplexity Estimation",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Quotation-Formatting (WLT)",
            "name_full": "Use of Quotation Marks Around Source/Target Words in Word-Level Translation Prompts",
            "brief_description": "In word-level translation prompts, including quotation marks around the source word consistently produced lower perplexity and higher label scores than prompts without quotes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OPT, Bloom",
            "model_size": "OPT 175B (analysis shown), Bloom 176B (also analyzed)",
            "task_name": "Word-level translation (NorthEuraLex) across multiple languages",
            "task_description": "Given an English source word and a target language, prompt asks model to produce the translated word (zero-shot). Prompts differ in surface formatting such as use of quotation marks.",
            "problem_format": "Prompts examples: with quotes: 'The word for \"dog\" in French is \"' vs without quotes: 'The word dog in French is '.",
            "comparison_format": "Comparison between prompts with quotation marks vs without quotation marks for the same translation task.",
            "performance": "Prompts with quotation marks show both lower perplexity and higher correct-label score; within the quotation-mark cluster, correlations between perplexity and score remain negative and statistically significant (range roughly -0.28 to -0.38 reported for OPT 175B). Table 12 shows one low-perplexity example: 'The word for \"dog\" in French is \"' perplexity 7.73.",
            "performance_comparison": null,
            "format_effect_size": "Cluster separation was clear in scatter plots; within the quoted prompts cluster, negative correlations in range ≈ -0.28 to -0.38 (OPT 175B). Exact pointwise score deltas are visual and tabulated qualitatively in plots and tables.",
            "format_effect_direction": "improved (quotation marks improved expectedness and label score)",
            "explanation_or_hypothesis": "Quotation marks likely make the prompt more closely match natural pretraining text patterns (e.g., quoted lexical items), increasing familiarity and lowering perplexity, which in turn improves model scoring of the correct label.",
            "counterexample_or_null_result": "Prompts without quotation marks form a separate cluster with generally weaker performance and weaker correlations; no case shown where quotes decreased performance.",
            "uuid": "e5802.3",
            "source_info": {
                "paper_title": "Demystifying Prompts in Language Models via Perplexity Estimation",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Prompt-Length Null Result",
            "name_full": "Prompt Length vs Performance Analysis (Null Finding)",
            "brief_description": "Authors tested prompt length as an alternative predictor and found weak positive correlations that were mostly not statistically significant, i.e., length alone does not explain prompt performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OPT, Bloom",
            "model_size": "various (same models as main experiments)",
            "task_name": "Same suite of classification and word/antonym tasks",
            "task_description": "Analysis comparing prompt length to downstream performance to test whether simple length is a confounding factor for the perplexity-performance relationship.",
            "problem_format": "Measure of prompt length (token count) vs task performance across prompts.",
            "comparison_format": null,
            "performance": "Correlation of prompt length with performance: weak positive correlations, almost all not statistically significant (authors report this as a negative finding in footnote).",
            "performance_comparison": null,
            "format_effect_size": "Weak positive correlations; not statistically significant (no meaningful effect size reported).",
            "format_effect_direction": "no reliable effect (null result)",
            "explanation_or_hypothesis": "Perplexity is a better predictor than raw prompt length because length does not meaningfully capture the model's familiarity with phrasing; authors explicitly note length yields weak and mostly insignificant correlations.",
            "counterexample_or_null_result": "This is itself a reported null result: prompt length does not reliably predict performance.",
            "uuid": "e5802.4",
            "source_info": {
                "paper_title": "Demystifying Prompts in Language Models via Perplexity Estimation",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Prompt Expansion (Paraphrase + Backtranslation)",
            "name_full": "Automatic Prompt Expansion via GPT-3 Paraphrasing and Backtranslation",
            "brief_description": "Method to expand a small manual seed set of prompts using GPT-3 paraphrase generation (several meta-prompts) followed by backtranslation (NLLB) into English to yield diverse prompt variants (~100 prompts per task).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (used to generate paraphrases); evaluated prompts on OPT and Bloom",
            "model_size": "GPT-3 text-davinci-002 (paraphrasing model); evaluated LMs: OPT 1.3B/30B/175B, Bloom 176B",
            "task_name": "Used to generate prompts for the same classification/word/antonym tasks",
            "task_description": "Generate many paraphrases of seed prompts to form a large candidate prompt pool; these prompts are then ranked by perplexity under target model(s).",
            "problem_format": "Paraphrase generation meta-prompts (7 variants) fed to GPT-3, then backtranslation through 8-16 languages via NLLB to expand set; enforce task-specific constraints for word-prediction tasks (e.g., include source word/language name).",
            "comparison_format": "Compared analyses performed before and after expansion; also manual inspection / filtering of noisy prompts vs original set.",
            "performance": "Expansion produced dozens–~100 prompts per task (Table 3: e.g., AG News: Step0=4, Step1=23, Step2=108). Using expanded pool + SPELL yields the reported correlations and accuracy improvements (see SPELL).",
            "performance_comparison": null,
            "format_effect_size": "Enables finding prompts with substantially different perplexities and corresponding performance; manual filtering of top-10% highest-perplexity prompts did not materially change correlations (Table 8).",
            "format_effect_direction": "useful / no detrimental effect to core correlation findings",
            "explanation_or_hypothesis": "Expansion increases prompt diversity making it more likely to find low-perplexity (familiar) prompts; automatic paraphrases are high-quality and not the source of the observed perplexity-performance correlations.",
            "counterexample_or_null_result": "Authors inspected the 10% highest-perplexity prompts and found them to be valid prompts; filtering out noisy prompts did not alter the strong correlations, arguing against noise-driven artifacts.",
            "uuid": "e5802.5",
            "source_info": {
                "paper_title": "Demystifying Prompts in Language Models via Perplexity Estimation",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Manual Wording Sensitivity",
            "name_full": "Sensitivity to Small Wording Changes in Manual Prompts",
            "brief_description": "Demonstration that small changes in prompt wording can yield large differences in accuracy; e.g., four human-written AG News prompts produce accuracies ranging from 40.9% to 71.2% with OPT 175B.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OPT",
            "model_size": "175B",
            "task_name": "AG News (news topic classification)",
            "task_description": "Zero-shot classification of news articles into topics using single-sentence prompts that ask the model for a label.",
            "problem_format": "Different manually-authored question phrasings appended after the input, e.g., 'What is this piece of news regarding?' vs 'What is the most accurate label for this news article?'.",
            "comparison_format": "Direct comparison across four alternative human-written prompts (Table 1).",
            "performance": "Reported accuracies (OPT 175B): 'What is this piece of news regarding?' = 40.9%; 'What is this article about?' = 52.4%; 'What is the best way to describe this article?' = 68.2%; 'What is the most accurate label for this news article?' = 71.2%.",
            "performance_comparison": null,
            "format_effect_size": "Large differences up to ∼30.3 percentage points between worst and best manual prompts for the same model/task (Table 1).",
            "format_effect_direction": "format (wording) can greatly improve or reduce accuracy",
            "explanation_or_hypothesis": "Wording changes alter prompt familiarity/perplexity and thus the model's ability to map from input to correct label; this motivates perplexity-based selection.",
            "uuid": "e5802.6",
            "source_info": {
                "paper_title": "Demystifying Prompts in Language Models via Perplexity Estimation",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2
        },
        {
            "paper_title": "Surface form competition: Why the highest probability answer isn't always right",
            "rating": 2
        },
        {
            "paper_title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
            "rating": 2
        },
        {
            "paper_title": "Making pre-trained language models better few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Reframing instructional prompts to gptk's language",
            "rating": 2
        },
        {
            "paper_title": "Promptsource: An integrated development environment and repository for natural language prompts",
            "rating": 1
        }
    ],
    "cost": 0.01444125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Demystifying Prompts in Language Models via Perplexity Estimation</h1>
<p>Hila Gonen ${ }^{1,2}$ Srini Iyer ${ }^{2}$ Terra Blevins ${ }^{1}$ Noah A. Smith ${ }^{1,3}$ Luke Zettlemoyer ${ }^{1,2}$<br>${ }^{1}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington<br>${ }^{2}$ Meta AI Research ${ }^{3}$ Allen Institute for Artificial Intelligence<br>hilagnn@gmail.com<br>sviyer@meta.com<br>{blvns, nasmith, lsz}@cs.washington.edu</p>
<h4>Abstract</h4>
<p>Language models can be prompted to perform a wide variety of tasks with zero- and few-shot incontext learning. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens. In this paper, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is predicted by the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that the lower the perplexity of the prompt, the better it is able to perform the task, when considering reasonable prompts that are related to it. As part of our analysis, we also devise a method to automatically extend a small seed set of manually written prompts by paraphrasing with GPT3 and backtranslation. This larger set allows us to verify that perplexity is a strong predictor of the success of a prompt and we show that the lowest perplexity prompts are consistently effective.</p>
<h2>1 Introduction</h2>
<p>Language models can be prompted to perform a wide range of zero- and few-shot learning tasks (Brown et al., 2020; Schick and Schütze, 2020). However, there is significant variance in the performance of seemingly similar prompts (Chen et al., 2022): for AG News (Zhang et al., 2015), we find an over 30 point accuracy gap between different manually curated prompts (see Table 1) on OPT 175B (Zhang et al., 2022). Despite efforts to improve prompt engineering (Shin et al., 2020; Li and Liang, 2021; Gao et al., 2021), it is still challenging to develop high-quality prompts for new tasks, and little is known about why this phenomenon occurs.</p>
<p>We are interested in understanding what makes some prompts better than others, and using this understanding to create better prompts for given tasks and models. We hypothesize that the lower the perplexity of a prompt is, the better its performance
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Accuracy vs. perplexity for the AG News dataset with OPT 175B. The $x$ axis is in log scale. Each point stands for a different prompt.
on the task will be, when considering reasonable prompts that are related to the task. This is based on the intuition that the more frequently the prompt (or very similar phrases) appears in the training data, the more the model is familiar with it and is able to perform the described task. We refrain from using the training data directly as it is often unavailable, expensive to search due to its size, and hard to use for approximate matching of similar prompts. Instead, we focus on the perplexity of the prompt as a proxy for its occurrences in the data.</p>
<p>To enable more complete analysis, we automatically expand the set of manually created prompts for the task by paraphrasing, resulting in a much larger and diverse set of prompts. We focus on prompts in English that reasonably describe the task for two reasons: (a) our main motivation is to understand what lies under the variance of performance in this type of prompt; (b) we aim to devise a useful method for creating prompts that are consistently effective, that could be easily adopted and interpreted by future, potentially non-expert users.</p>
<p>We show empirically that our hypothesis holds across a diverse set of tasks (including classification and word prediction), models, and model</p>
<p>sizes, providing us some insights about the underlying mechanism of prompting (see Figure 1). As a result, we devise a method, SPELL (Selecting Prompts by Estimating LM Likelihood), for creating prompts in an informed manner. We show that using SPELL to choose prompts results in less variability in performance as well as in accuracy gains ( 1.8 accuracy points with OPT and 2.3 accuracy points with Bloom on average). Importantly, our method does not require labels at all, only a small sample of inputs for the task.</p>
<p>Our contributions can be summarized as follows: (a) we formalize the notion that better familiarity of the model with the prompt correlates with better performance (Section 2); (b) we automatically elaborate a given set of seed prompts using paraphrasing (Section 3); (c) we establish experimentally the hypothesis that lower perplexity of the prompt correlates well with better performance (Section 5); (d) we devise a method to create a more consistent set of prompts, that also improve results even with no labels for the task (Section 7).</p>
<h2>2 Why are prompts not all created equal?</h2>
<p>Despite the popularity of prompting as a method for using language models (Shin et al., 2020; Li and Liang, 2021; Gao et al., 2021), the cause for the different behavior of various prompts remains unclear so far. Table 1 shows four example prompts for a news topic classification task (AG News) and their respective accuracies when used to prompt OPT 175B (Zhang et al., 2022). The accuracy gap between the different prompts is not trivial, and it is not possible to predict from the prompts alone.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt</th>
<th style="text-align: right;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">What is this piece of news regarding?</td>
<td style="text-align: right;">40.9</td>
</tr>
<tr>
<td style="text-align: left;">What is this article about?</td>
<td style="text-align: right;">52.4</td>
</tr>
<tr>
<td style="text-align: left;">What is the best way to describe this article?</td>
<td style="text-align: right;">68.2</td>
</tr>
<tr>
<td style="text-align: left;">What is the most accurate label for this news article?</td>
<td style="text-align: right;">71.2</td>
</tr>
</tbody>
</table>
<p>Table 1: Example prompts for the task AG News (news classification) that vary considerably in accuracy.</p>
<p>We propose that the more frequently a prompt appears in some variation in the data, the better it works for the task. The intuition behind this is that a sequence that is more expected by the model is more likely to aid the model to extract the relevant information. However, this premise is hard to measure accurately: most language models use huge amounts of training data (e.g., OPT uses a corpus of roughly 180B tokens, and Bloom uses
roughly 366B tokens), and in addition, this training data is not always publicly available (e.g., GPT3; Brown et al. 2020). Our initial attempts to estimate exact-match occurrences of prompts in the data resulted in very sparse counts, which led us to look for a softer formalization. ${ }^{1}$</p>
<p>Instead of considering the training data directly, we propose to focus on the perplexity of the prompt as a proxy for its occurrences in some form in the data - essentially indicating to what extent the model expects this prompt. This perplexity-based framing helps to avoid the challenge of exact match in the data, and takes into account variations of the prompt that the model is also exposed to and might be influenced by. In addition, it helps overcome the challenges mentioned above as it requires neither access to the pretraining data (which is not always publicly available for LMs) nor matching over huge amounts of text.</p>
<p>Hypothesis: lower perplexity correlates with better performance. We hypothesize that on average, lower-perplexity prompts perform better. We are interested in establishing this hypothesis by experimentally showing a significant negative correlation between the perplexity of the prompt and its performance on the task, across a diverse set of tasks and models.</p>
<p>We define the perplexity of the prompt as the perplexity of the full prompt sequence, including the input itself, and without the label, averaged over 1,000 examples (see Section 4 for details). The input is a part of the prompt in the case of the word prediction tasks by design (e.g., "The opposite of the word good is"). Inclusion of the task input as part of the prompt for classification tasks as well is intentional: we want to ground the prompt to the task (without the input, we are testing the hypothesis that lower perplexity prompts across all tasks work better on every task). The label is not considered a part of the prompt and is not taken into consideration when computing the prompt. In practice, this also results in a huge advantage of our method, SPELL (Section 7), which aims to find better prompts-it does not require any labels.</p>
<p>For performance measures, we use the loglikelihood score assigned by the model to the correct label given that prompt. We choose this metric</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>over accuracy as it gives a more fine-grained distinction between prompts and because accuracy can be unstable, as explained in more detail in Section 4. For classification tasks, we also report correlation with accuracy, which is the main evaluation metric for this type of task.</p>
<h2>3 Automatic Expansion of Seed Prompts</h2>
<p>We are interested in expanding our pool of prompts in order to: (a) have a more diverse set of prompts, making it more likely to find a better prompt for our task, and (b) support better analysis to validate our prompt quality hypothesis. In this section, we describe our method for automatically expanding a seed set of manually created prompts using paraphrasing.</p>
<p>Step 0: Creating a seed set of manually-written prompts We first write/collect a small set of human written prompts that describe the task. For classification tasks we assume that the input appears before the prompt, with no choices appearing as part of the prompt (to help in smooth paraphrasing of the prompt itself).</p>
<p>Step 1: Paraphrasing with GPT3 We use the text-davinci-002 version of GPT3 (Brown et al., 2020) to generate paraphrases for each of the manual prompts in our seed set. We prompt it with a meta-prompt for paraphrasing to generate variations of one of our seed prompts. An example of such a meta-prompt is: Write a paraphrase for the following sentence: <seed prompt> Paraphrase:. The 7 meta-prompts used in this step are listed in Table 2.</p>
<p>We choose GPT3 as our paraphrasing model because of its well-documented generation abilities. This is also to ensure that there is a separation between the model we use to create the prompts and the models we use to rank them (OPT and Bloom, see Section 4 for details), to avoid confounding the experimental setup.</p>
<p>Step 2: Paraphrasing using backtranslation Our second step takes as input the paraphrases from GPT3 (in addition to the seed set of prompts) and translates them into different languages and back into English to get additional prompt paraphrases (Wieting et al., 2017). We use a set of 8 languages available in the NLLB translation model (Costajussà et al., 2022) that are relatively high resource
and close to English, ${ }^{2}$ to reduce the risk of noise. Since we aim to get about 100 prompts per task, we add 8 additional languages ${ }^{3}$ in the case where the basic 8 languages yielded too few alternatives. For word prediction tasks, we use the sequence of the created prompt up to the index of the label, not including the label, for example: The word "dog" in French is ". Depending on the task, we enforce the existence of specific words (e.g., the name of the language, and the source word, in word-level translation) or enforce the prompt to be a question.</p>
<p>Examples and Statistics Table 4 lists all 4 manually created prompts we use for the AG News task (news classification), alongside a few sampled prompts created automatically using our method. As was typically the case, we are able to get prompts that are rather different in phrasing and structure from those included in the seed set.</p>
<p>The statistics of the prompts in the manually created seed set (Step 0) as well as the prompts after Step 1 and Step 2 for each task (see Section 4.1 for details about the tasks) are detailed in Table 3.</p>
<h2>4 Experimental Setup</h2>
<h3>4.1 Models, Tasks and Datasets</h3>
<p>We study four auto-regressive models: OPT (Zhang et al., 2022) of different sizes (1.3B, 30B, 175B parameters), all trained mainly on English, ${ }^{4}$ and Bloom (176B parameters; Luccioni et al. 2022), which is trained on 46 natural languages and 13 programming languages. We experiment with two types of tasks: word prediction tasks and classification tasks, as detailed below.</p>
<p>Word Prediction Tasks The first task in this category is word-level translation. Given a source word in English and a target language, we expect the model to predict the correct translation. For this task we use NorthEuraLex ${ }^{5}$ (Dellert et al., 2019), a lexical database providing translations of 1016 words into 107 languages. We experiment with 9 languages that use the Latin script. For Bloom, we use 5 additional languages that do not use the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta prompts</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Write a paraphrase for the following sentence: <seed-prompt> Paraphrase:</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"><seed-prompt> Paraphrase:</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Write a likely paraphrase of the text: <seed-prompt> Paraphrase:</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Write a sentence similar to the following one: <seed-prompt> Paraphrase:</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Paraphrase the following sentence: <seed-prompt> Paraphrase:</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Write a variation of this sentence: <seed-prompt></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">How would you say the following sentence in a different way? <seed-prompt></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Meta prompts used in Step 1 of our method for paraphrasing using GPT3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: right;"># Step 0</th>
<th style="text-align: right;"># Step 1</th>
<th style="text-align: right;"># Step 2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Word-Level Translation</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">118</td>
</tr>
<tr>
<td style="text-align: left;">Antonyms</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">85</td>
<td style="text-align: right;">176</td>
</tr>
<tr>
<td style="text-align: left;">GLUE Cola</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">27</td>
<td style="text-align: right;">144</td>
</tr>
<tr>
<td style="text-align: left;">Newspop</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">43</td>
<td style="text-align: right;">119</td>
</tr>
<tr>
<td style="text-align: left;">AG News</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">23</td>
<td style="text-align: right;">108</td>
</tr>
<tr>
<td style="text-align: left;">IMDB</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">178</td>
</tr>
<tr>
<td style="text-align: left;">DBpedia</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">23</td>
<td style="text-align: right;">103</td>
</tr>
<tr>
<td style="text-align: left;">Emotion</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">94</td>
</tr>
<tr>
<td style="text-align: left;">Tweet Offensive</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">119</td>
</tr>
</tbody>
</table>
<p>Table 3: Number of prompts for the different tasks: prompts after step 0 (creating prompts manually), prompts after step 1 (GPT3 paraphrasing), and prompts after step 2 (backtranslation).</p>
<p>Latin script (since Bloom is multilingual). Note that only 5 of the languages we experiment with are officially covered by Bloom. ${ }^{6}$</p>
<p>We also consider antonym prediction where, given a word, the model is expected to predict its antonym. For this task, we use data from Kaggle, ${ }^{7}$ which is based on WordNet (Miller, 1995). We choose 1,000 word pairs at random.</p>
<p>Classification Tasks We choose classification tasks from Huggingface Datasets, ${ }^{8}$ with an attempt to have a set of diverse tasks that use relatively short inputs, with some prompts available in PromptSource (Bach et al., 2022): ${ }^{9}$ (a) GLUE Cola (grammaticality; Warstadt et al. 2018); (b) Newspop (news classification; Moniz and Torgo 2018); (c) AG News (news classification; Zhang et al. 2015); (d) IMDB (movie review classification; Maas et al. 2011); (e) DBpedia (topic classification; Lehmann et al. 2015); (f) Emotion (classification to emo-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tions; Saravia et al. 2018); (g) Tweet Offensive (classification to offensive vs. not offensive tweets; Barbieri et al. 2020). We use 1,000 random examples from each dataset.</p>
<p>The full set of manual prompts is listed in Section A in the Appendix. In these tasks, the prompt follows the input, and at the end of each prompt we add the choices of classes (i.e., we provide the possible labels explicitly in the prompt by listing the possible answers as defined by the dataset itself.): "Choices: X, Y, Z. Answer:" as we find it helps in terms of accuracy. Defining the label space likely helps in our zero-shot setting because there are no previous demonstrations from which the model can learn the possible classes. Additionally, adding class options to the prompt helps to reduce the effect of the surface form competition (Holtzman et al., 2021). The option of generating the answer and comparing it with the gold label was not reasonable here, since we cannot expect the model to generate the exact label as the first choice often enough.</p>
<h3>4.2 Implementation Details</h3>
<p>In all experiments we evaluate zero-shot performance. To avoid noise when computing perplexity, we instantiate the prompts with 1,000 examples of the dataset, compute the perplexity of the prompt with each example, and calculate the average across all instantiated prompts.</p>
<p>To estimate the performance of the prompt, we look at two measures: (a) the language model score (log probability) of the correct label, averaged across 1,000 examples; (b) the accuracy on the task, computed over the 1,000 examples. To compute accuracy, for each example we score all classes and choose the highest ranking class as the prediction of the model. The score of a label of multiple tokens is defined by the sum of the token</p>
<table>
<thead>
<tr>
<th>All Manually Created Prompts</th>
<th>Examples of Similar Automatically Created Prompts</th>
</tr>
</thead>
<tbody>
<tr>
<td>What label best describes this news article?</td>
<td>What’s the most accurate label for this news article?</td>
</tr>
<tr>
<td>What is this piece of news regarding?</td>
<td>What does this piece of news concern?</td>
</tr>
<tr>
<td>Which newspaper section would this article likely appear in?</td>
<td>In what section of the newspaper could this article be published?</td>
</tr>
<tr>
<td>What topic is this news article about?</td>
<td>What category does this article fall into?</td>
</tr>
</tbody>
</table>
<p>Table 4: Prompts for the task AG News (news classification): the manually created prompts and a sample of automatically created prompts using our method.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Task</th>
<th>Perplexity-score corr.</th>
<th></th>
<th>Perplexity-acc corr.</th>
<th></th>
<th>Avg Acc</th>
<th>Acc 50%</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Pearson</td>
<td>Spearman</td>
<td>Pearson</td>
<td>Spearman</td>
<td></td>
<td></td>
</tr>
<tr>
<td>OPT 175B</td>
<td>Antonyms</td>
<td>**-0.41</td>
<td>**-0.53</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>GLUE Cola</td>
<td>-0.15</td>
<td>-0.14</td>
<td>-0.04</td>
<td>-0.02</td>
<td>47.7</td>
<td>57.1</td>
</tr>
<tr>
<td></td>
<td>Newspop</td>
<td>*-0.24</td>
<td>**-0.26</td>
<td>*-0.20</td>
<td>-0.18</td>
<td>66.4</td>
<td>72.9</td>
</tr>
<tr>
<td></td>
<td>AG News</td>
<td>**-0.63</td>
<td>**-0.68</td>
<td>**-0.77</td>
<td>**-0.81</td>
<td>57.5</td>
<td>68.7</td>
</tr>
<tr>
<td></td>
<td>IMDB</td>
<td>**0.35</td>
<td>**0.40</td>
<td>0.14</td>
<td>*0.20</td>
<td>86.2</td>
<td>91.0</td>
</tr>
<tr>
<td></td>
<td>DBpedia</td>
<td>**-0.50</td>
<td>**-0.44</td>
<td>**-0.51</td>
<td>**-0.42</td>
<td>46.7</td>
<td>55.2</td>
</tr>
<tr>
<td></td>
<td>Emotion</td>
<td>-0.14</td>
<td>-0.19</td>
<td>**-0.30</td>
<td>**-0.32</td>
<td>16.4</td>
<td>23.0</td>
</tr>
<tr>
<td></td>
<td>Tweet Offensive</td>
<td>*-0.19</td>
<td>0.07</td>
<td>0.18</td>
<td>*0.23</td>
<td>51.3</td>
<td>55.8</td>
</tr>
<tr>
<td>Bloom 176B</td>
<td>Antonyms</td>
<td>**-0.37</td>
<td>**-0.23</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>GLUE Cola</td>
<td>0.07</td>
<td>0.11</td>
<td>**-0.25</td>
<td>**-0.26</td>
<td>55.5</td>
<td>65.6</td>
</tr>
<tr>
<td></td>
<td>Newspop</td>
<td>**-0.50</td>
<td>**-0.42</td>
<td>**-0.59</td>
<td>**-0.51</td>
<td>78.9</td>
<td>87.8</td>
</tr>
<tr>
<td></td>
<td>AG News</td>
<td>**-0.62</td>
<td>**-0.54</td>
<td>**-0.44</td>
<td>**-0.44</td>
<td>50.3</td>
<td>59.4</td>
</tr>
<tr>
<td></td>
<td>IMDB</td>
<td>0.04</td>
<td>0.09</td>
<td>-0.08</td>
<td>-0.14</td>
<td>89.3</td>
<td>92.2</td>
</tr>
<tr>
<td></td>
<td>DBpedia</td>
<td>**-0.47</td>
<td>*-0.27</td>
<td>**-0.35</td>
<td>*-0.21</td>
<td>27.2</td>
<td>33.4</td>
</tr>
<tr>
<td></td>
<td>Emotion</td>
<td>**-0.33</td>
<td>**-0.42</td>
<td>**-0.48</td>
<td>**-0.55</td>
<td>29.3</td>
<td>31.7</td>
</tr>
<tr>
<td></td>
<td>Tweet Offensive</td>
<td>0.14</td>
<td>*0.24</td>
<td>*-0.20</td>
<td>-0.03</td>
<td>41.6</td>
<td>46.2</td>
</tr>
<tr>
<td>OPT 30B</td>
<td>Antonyms</td>
<td>**-0.54</td>
<td>**-0.70</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>GLUE Cola</td>
<td>-0.05</td>
<td>0.03</td>
<td>-0.13</td>
<td>0.02</td>
<td>32.2</td>
<td>35.5</td>
</tr>
<tr>
<td></td>
<td>Newspop</td>
<td>*-0.23</td>
<td>*-0.25</td>
<td>*-0.18</td>
<td>-0.12</td>
<td>60.3</td>
<td>66.6</td>
</tr>
<tr>
<td></td>
<td>AG News</td>
<td>**-0.66</td>
<td>**-0.71</td>
<td>**-0.81</td>
<td>**-0.80</td>
<td>49.3</td>
<td>60.7</td>
</tr>
<tr>
<td></td>
<td>IMDB</td>
<td>-0.06</td>
<td>*0.17</td>
<td>0.04</td>
<td>**0.22</td>
<td>81.6</td>
<td>86.1</td>
</tr>
<tr>
<td></td>
<td>DBpedia</td>
<td>**-0.41</td>
<td>**-0.34</td>
<td>*-0.21</td>
<td>*-0.25</td>
<td>35.9</td>
<td>42.4</td>
</tr>
<tr>
<td></td>
<td>Emotion</td>
<td>0.00</td>
<td>-0.03</td>
<td>0.18</td>
<td>0.13</td>
<td>12.3</td>
<td>16.2</td>
</tr>
<tr>
<td></td>
<td>Tweet Offensive</td>
<td>**-0.44</td>
<td>**-0.39</td>
<td>-0.11</td>
<td>-0.05</td>
<td>54.6</td>
<td>60.2</td>
</tr>
<tr>
<td>OPT 1.3B</td>
<td>Antonyms</td>
<td>**-0.45</td>
<td>**-0.53</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>GLUE Cola</td>
<td>**-0.39</td>
<td>**-0.36</td>
<td>-0.09</td>
<td>*-0.19</td>
<td>60.3</td>
<td>65.9</td>
</tr>
<tr>
<td></td>
<td>Newspop</td>
<td>**0.33</td>
<td>*0.21</td>
<td>-0.07</td>
<td>-0.07</td>
<td>37.6</td>
<td>40.3</td>
</tr>
<tr>
<td></td>
<td>AG News</td>
<td>**-0.33</td>
<td>**-0.29</td>
<td>**-0.56</td>
<td>**-0.49</td>
<td>31.9</td>
<td>37.6</td>
</tr>
<tr>
<td></td>
<td>IMDB</td>
<td>-0.11</td>
<td>-0.07</td>
<td>**0.24</td>
<td>**0.22</td>
<td>86.0</td>
<td>89.1</td>
</tr>
<tr>
<td></td>
<td>DBpedia</td>
<td>-0.16</td>
<td>-0.14</td>
<td>-0.02</td>
<td>-0.01</td>
<td>8.7</td>
<td>9.2</td>
</tr>
<tr>
<td></td>
<td>Emotion</td>
<td>0.08</td>
<td>0.08</td>
<td>**-0.29</td>
<td>**-0.30</td>
<td>7.0</td>
<td>9.1</td>
</tr>
<tr>
<td></td>
<td>Tweet Offensive</td>
<td>**-0.42</td>
<td>**-0.35</td>
<td>**-0.50</td>
<td>**-0.38</td>
<td>58.6</td>
<td>62.6</td>
</tr>
</tbody>
</table>
<p>Table 5: Correlation results for the different tasks, with OPT (different sizes) and Bloom. Correlations with $p&lt;0.05$ are marked with <em>. Correlations with $p&lt;0.00625$ (according to Bonferroni correction for multiple hypotheses) are marked with </em>*. Dark and light blue colored cells stand for negative correlations $&lt;-0.2$ and $&gt;-0.2$, respectively. Dark and light orange colored cells stand for positive correlations $&gt;0.2$ and $&lt;0.2$, respectively. Average accuracy across all prompts and average accuracy of best 50% prompts are also reported for reference (Avg Acc and Acc 50%, respectively).
scores.
For the word prediction tasks we only report scores, since accuracy in general is less stable, suffers more from the surface form competition (Holtzman et al., 2021), and is usually quite low for these tasks in our setting (the chances the model will generate an exact match of the label are low). Hence, the score of the correct label gives a better estimate of the actual performance of the model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Lang</th>
<th style="text-align: center;">OPT 175B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Bloom 176B</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pearson</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pearson</td>
<td style="text-align: center;">Spear.</td>
</tr>
<tr>
<td style="text-align: center;">ita</td>
<td style="text-align: center;">$-0.44$</td>
<td style="text-align: center;">$-0.57$</td>
<td style="text-align: center;">$-0.37$</td>
<td style="text-align: center;">$-0.63$</td>
</tr>
<tr>
<td style="text-align: center;">spa</td>
<td style="text-align: center;">$-0.47$</td>
<td style="text-align: center;">$-0.61$</td>
<td style="text-align: center;">$-0.51$</td>
<td style="text-align: center;">$-0.66$</td>
</tr>
<tr>
<td style="text-align: center;">cat</td>
<td style="text-align: center;">$-0.47$</td>
<td style="text-align: center;">$-0.58$</td>
<td style="text-align: center;">$-0.24$</td>
<td style="text-align: center;">$-0.31$</td>
</tr>
<tr>
<td style="text-align: center;">fra</td>
<td style="text-align: center;">$-0.48$</td>
<td style="text-align: center;">$-0.57$</td>
<td style="text-align: center;">$-0.48$</td>
<td style="text-align: center;">$-0.64$</td>
</tr>
<tr>
<td style="text-align: center;">deu</td>
<td style="text-align: center;">$-0.44$</td>
<td style="text-align: center;">$-0.60$</td>
<td style="text-align: center;">$-0.46$</td>
<td style="text-align: center;">$-0.65$</td>
</tr>
<tr>
<td style="text-align: center;">fin</td>
<td style="text-align: center;">$-0.44$</td>
<td style="text-align: center;">$-0.62$</td>
<td style="text-align: center;">$-0.34$</td>
<td style="text-align: center;">$-0.56$</td>
</tr>
<tr>
<td style="text-align: center;">por</td>
<td style="text-align: center;">$-0.45$</td>
<td style="text-align: center;">$-0.62$</td>
<td style="text-align: center;">$-0.46$</td>
<td style="text-align: center;">$-0.61$</td>
</tr>
<tr>
<td style="text-align: center;">eus</td>
<td style="text-align: center;">$-0.47$</td>
<td style="text-align: center;">$-0.61$</td>
<td style="text-align: center;">$-0.45$</td>
<td style="text-align: center;">$-0.61$</td>
</tr>
<tr>
<td style="text-align: center;">tur</td>
<td style="text-align: center;">$-0.44$</td>
<td style="text-align: center;">$-0.62$</td>
<td style="text-align: center;">$-0.33$</td>
<td style="text-align: center;">$-0.62$</td>
</tr>
<tr>
<td style="text-align: center;">jpn</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$-0.33$</td>
<td style="text-align: center;">$-0.26$</td>
</tr>
<tr>
<td style="text-align: center;">arb</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$-0.36$</td>
<td style="text-align: center;">$-0.47$</td>
</tr>
<tr>
<td style="text-align: center;">rus</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$-0.54$</td>
<td style="text-align: center;">$-0.69$</td>
</tr>
<tr>
<td style="text-align: center;">kor</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$-0.42$</td>
<td style="text-align: center;">$-0.58$</td>
</tr>
<tr>
<td style="text-align: center;">ell</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$-0.40$</td>
<td style="text-align: center;">$-0.51$</td>
</tr>
</tbody>
</table>
<p>Table 6: Correlation results for word-level translation, with OPT 175B and Bloom 176B. All correlations are statistically significant also according to Bonferroni correction for multiple hypotheses for OPT ( $p&lt;0.0055$ ). Same for Bloom ( $p&lt;0.00357$ ), except for Catalan (Pearson) and Japanese (Spearman).</p>
<h2>5 Results</h2>
<h2>Classification Tasks and Antonym Prediction</h2>
<p>Table 5 depicts the Pearson and Spearman correlation results on the classification tasks and the antonym task, with both OPT 175B and Bloom (two upper blocks). We see that most correlations are negative and statistically significant, as we expect. This validates our hypothesis and shows that in the majority of tasks we indeed get a strong correlation between low perplexity of the prompt and better performance on the task. ${ }^{10}$ For each task we also report the average accuracy.</p>
<p>Word-Level Translation The results of the wordlevel translation task are reported in Table 6. Here the correlations are extremely consistent across all languages and across models, with statistical significance for all languages except for Catalan and Japanese (in Bloom).</p>
<p>Results across Different Model Sizes We repeat the same experiment with the OPT models of sizes 1.3B and 30B, to investigate whether these correlations are also consistent across model sizes or whether this is a phenomenon we should expect only in large language models. Table 5 (two lower blocks) shows these results for all classification tasks and antonym prediction. We do see that in</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>general the trend appears to be the same in the smaller models as well; however, the correlations seem to be slightly weaker. We hypothesize that this might be due to the overall lower performance of these smaller models, making the performance results we use for correlation less stable and reliable. For word-level translation, however, all correlations with the 30B and 1.3B models are similar to those with the 175B model, and are all statistically significant (also after Bonferroni correction for multiple hypotheses).</p>
<h2>6 Analysis</h2>
<p>Next, we further explore the observed relationship between model perplexity and prompt performance. Despite the consistently high correlation between these two factors, the structure of this relationship varies across tasks (Section 6.1). Additionally, we find that the automatically added prompts are highquality and not a significant source of noise (Section 6.2), and that the best prompts selected by our approach vary across models (Section 6.3).</p>
<h3>6.1 Visualizing the Relationship between Perplexity and Performance</h3>
<p>To visualize the correlations we get between the perplexity and the performance of the prompts across the different settings, we plot a few examples for different tasks and languages. Figures 1 and 2 show some of the results for selected tasks, as detailed in the captions. The negative trend of the correlation is clearly visible in all plots. Interestingly, the structure of the plots for word-level translation are very similar across all the language pairs, suggesting that prompts get consistent perplexity and performance across languages (possibly at different scales). Indeed, the intersection of the 10 lowest perplexity prompts between any two different languages is 8.6 and 8.4 on average (for OPT 175B and Bloom, respectively), which is extremely high. This is not very surprising since we know that the only differences between the prompts in the different languages are the names of the target languages (e.g., The word for "dog" in French is "). Additionally, the intersection of 10 prompts with the highest label score between any two different languages is 7 and 6.5 on average (for OPT 175B and Bloom, respectively).</p>
<p>A notable finding that appears in the word-level translation plots is the clear separation between prompts that include or do not include quotation</p>
<p>marks for the label (usually aligns with whether the prompt uses quotation marks for the source word) – three example prompts appear on the plot. Prompts with quotation marks for the words tend to have both lower perplexity and better performance, consistently. We further analyze the results for OPT 175B within clusters (with/without quotations marks). In the cluster with quotation marks, we get negative correlations (in the range of –0.28 to –0.38) that are statistically significant for almost all languages. The correlations within the other cluster are weaker and less significant (this is expected given the overall lower performance of that cluster).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Score of correct label vs. perplexity for the word-level translation task in French with OPT 175B. The <em>x</em> axis is in log scale. The blue points stand for prompts with quotation marks for the words, while the yellow points are of prompts without quotation marks.</p>
<h3>6.2 Effect of Noisy Prompts</h3>
<p>We expect our automatic method for expanding the set of prompts to also introduce some noise. Though our focus is on the lower perplexity prompts, since we want to benefit from this analysis and be able to devise a method for creating better prompts, we do want to make sure that this potential noise is not the cause for the strong correlations we get. In other words, one might claim that some noisy prompts have particularly high perplexity and also perform badly, thus, supporting our hypothesis in an undesirable and uncontrolled manner.</p>
<p>We turn to inspect the 10% highest perplexity prompts in the different tasks and find subjectively that they are not noisy, and are usually valid prompts for the tasks. The 5 highest perplexity prompts for the GLUE Cola task are listed in Table 7 as an example.</p>
<table>
<thead>
<tr>
<th>prompt</th>
<th>ppl</th>
</tr>
</thead>
<tbody>
<tr>
<td>Is this example correct English usage?</td>
<td>25.79</td>
</tr>
<tr>
<td>Is this example using English correctly?</td>
<td>25.46</td>
</tr>
<tr>
<td>Is this example correct English?</td>
<td>25.33</td>
</tr>
<tr>
<td>Is this the example in correct English?</td>
<td>25.00</td>
</tr>
<tr>
<td>Is English in this example correct?</td>
<td>24.90</td>
</tr>
</tbody>
</table>
<p>Table 7: Example of the 5 highest perplexity prompts for GLUE Cola, using OPT 175B.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Lang</th>
<th>Before filtering</th>
<th>After filtering</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Pearson</td>
<td>Spearman</td>
</tr>
<tr>
<td>AG News</td>
<td>-</td>
<td>-0.63</td>
<td>-0.68</td>
</tr>
<tr>
<td></td>
<td>ita</td>
<td>-0.44</td>
<td>-0.58</td>
</tr>
<tr>
<td></td>
<td>spa</td>
<td>-0.47</td>
<td>-0.61</td>
</tr>
<tr>
<td></td>
<td>cat</td>
<td>-0.45</td>
<td>-0.57</td>
</tr>
<tr>
<td></td>
<td>fra</td>
<td>-0.47</td>
<td>-0.57</td>
</tr>
<tr>
<td>WLT</td>
<td>deu</td>
<td>-0.43</td>
<td>-0.60</td>
</tr>
<tr>
<td></td>
<td>fin</td>
<td>-0.41</td>
<td>-0.60</td>
</tr>
<tr>
<td></td>
<td>por</td>
<td>-0.43</td>
<td>-0.61</td>
</tr>
<tr>
<td></td>
<td>eus</td>
<td>-0.45</td>
<td>-0.60</td>
</tr>
<tr>
<td></td>
<td>tur</td>
<td>-0.43</td>
<td>-0.61</td>
</tr>
</tbody>
</table>
<p>Table 8: Correlations before and after filtering out noisy prompts, with AG News and Word-Level Translation (WLT).</p>
<p>As a sanity check, we choose two tasks: word-level translation and AG News, manually filter out the noisy prompts, and compute the correlations again. The annotation is done by external annotators (NLP researchers) that were presented with the tasks and asked to label whether the prompt is reasonable to use for the task. The new correlations with OPT 175B are reported in Table 8. We find that all correlations remain strong and statistically significant when noise is manually removed from the analysis. We get the same trends with Bloom as well.</p>
<h3>6.3 Best Performing Prompts</h3>
<p>Table 9 lists the 5 lowest perplexity prompts for the task of antonym prediction, as an example. Similar lists for the rest of the tasks are listed in Section B in the Appendix.</p>
<p>A closer look at the lowest perplexity prompts reveals that the intersection of 10 lowest perplex-</p>
<table>
<thead>
<tr>
<th>prompt</th>
<th>ppl</th>
</tr>
</thead>
<tbody>
<tr>
<td>The following two words are antonyms: "good" and "</td>
<td>10.24</td>
</tr>
<tr>
<td>The antonym of the word "good" is "</td>
<td>10.32</td>
</tr>
<tr>
<td>The word that has the opposite meaning of the word "good" is "</td>
<td>10.43</td>
</tr>
<tr>
<td>The word "good" is the antithesis of the word "</td>
<td>10.85</td>
</tr>
<tr>
<td>The word "good" is the opposite of the word "</td>
<td>11.15</td>
</tr>
</tbody>
</table>
<p>Table 9: Lowest perplexity prompts for the antonym prediction task, using OPT 175B.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">OPT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Bloom</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">low-ppl</td>
<td style="text-align: center;">manual</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">low-ppl</td>
<td style="text-align: center;">manual</td>
<td style="text-align: center;">$\Delta$</td>
</tr>
<tr>
<td style="text-align: center;">GLUE Cola</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">3.6</td>
</tr>
<tr>
<td style="text-align: center;">Newspop</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">AG News</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">$-12.5$</td>
</tr>
<tr>
<td style="text-align: center;">IMDB</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">2.5</td>
</tr>
<tr>
<td style="text-align: center;">DBpedia</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">$-5.7$</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;">Emotion</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">$-1.1$</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">3.6</td>
</tr>
<tr>
<td style="text-align: center;">Tweet Offensive</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">$-2.3$</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">7.8</td>
</tr>
</tbody>
</table>
<p>ity prompts between OPT 175B and Bloom is 7.1 on average, across the classification tasks. When looking at the 10 highest accuracy prompts across models we get an average intersection of 3.1 across the classification tasks.</p>
<h2>7 SPELL: Selecting Prompts by Estimating LM Likelihood</h2>
<p>The primary contribution of this work is the analysis of the relationship between prompt perplexity and downstream task performance (Section 5). As one potential application of our findings, we also present a new method, SPELL, for generating and selecting consistently effective prompts.</p>
<p>Assuming a fixed computational budget for finding effective prompts for a given task, and that the search space might be quite large, we devise the following straightforward procedure:</p>
<ol>
<li>Obtain a small set of manually created prompts for the task.</li>
<li>Expand the set of prompts with automatic paraphrasing using a LM (e.g., GPT3) and backtranslation (see Section 3).</li>
<li>Rank the list of prompts by perplexity (averaged on a representative sample of task inputs, e.g., 1,000 ).</li>
<li>Choose the $k$ (e.g., 3) lowest perplexity prompts.</li>
</ol>
<p>Using this algorithm, we show empirically that it is best to prioritize experimenting with the lowest perplexity prompts, as they are more stable (exhibit less variation in performance) and perform better than manual prompts on average. This method also does not require any labels for the task, and is applicable to any task, also by non-experts, given example inputs only.</p>
<h3>7.1 Empirical Validation of SPELL</h3>
<p>To show the effectiveness of our method, we report the results we get using SPELL across the different tasks. In Table 10 we report the average accuracy with the manual prompts compared to the average accuracy with the 3 lowest-perplexity prompts, for both OPT 175B and Bloom. Indeed, in most cases, the average accuracy using the 3 lowest perplexity prompts outperforms the average accuracy of the manual prompts, with an average of 1.8 accuracy points across tasks with OPT and 2.3 accuracy</p>
<p>Table 10: The average accuracy with the manual prompts (manual) compared to the average accuracy with the 3 lowest-perplexity prompts (low-ppl), for both OPT 175B and Bloom, across tasks.
points with Bloom, demonstrating the effectiveness of our method.</p>
<p>The variability in accuracy of the 3 lowest perplexity prompts is also much lower than that of the manually created prompts: with OPT 175B, the average standard deviation within the 3 lowest perplexity prompts (across tasks) is 5.07 , vs. 6.86 for the manual prompts, and with Bloom the gap is much bigger, with an average of 2.6 for the 3 lowest perplexity prompts vs. 7.47 for the manual ones. ${ }^{11}$ This further shows that SPELL is more stable and reliable compared to using an arbitrary set of manually created prompts. SPELL sets the stage for further development in this direction, and serves as an initial indication of the benefits of involving perplexity estimation in the process of generating effective prompts.</p>
<h2>8 Related Work</h2>
<p>Relation between performance and training data Previous work looking directly into the relation between the training data and the performance is limited. Razeghi et al. (2022) study numeric deduction tasks, and examine the correlations between the model performance on specific test instances and the frequency of terms from those instances in the pretraining data. They find that the models are more accurate on instances whose terms are more prevalent in the training data. Additionally, Han and Tsvetkov (2022) propose a method to effectively identify a very small subset of pretraining data that directly supports the model in performing a specific task. Elazar et al. (2022) use causal inference to measure the effect of pretraining data statistics on factual knowledge performance, and</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Kandpal et al. (2022) show correlational and causal relationships between accuracy and relevant document count (from training data) for QA datasets.</p>
<p>Prompt tuning and analysis There is a very rich line of work trying to find prompts automatically. Shin et al. (2020) present an automated method to create discrete prompts for a diverse set of tasks, based on a gradient-guided search, and they demonstrate their method on masked LMs. Other work also focuses on discrete prompts, aiming to improve zero-shot performance (Gao et al., 2021; Le Scao and Rush, 2021; Deng et al., 2022; Shi et al., 2022), or trains continuous prompts (Li and Liang, 2021; Lester et al., 2021; Qin and Eisner, 2021).</p>
<p>On top of works that suggest a variety of methods for creating better prompts, some work also analyzes those prompts to try and get some insights about them: Khashabi et al. (2022a) find that model performance is highly sensitive to small changes in wordings and Khashabi et al. (2022b) point to a surprising disconnect between continuous and discrete prompts.</p>
<h2>9 Conclusion</h2>
<p>We investigate the phenomenon where some prompts perform better than others despite appearing similar to the human users of LMs. Specifically, we hypothesize that the perplexity of a prompt under a given LM is closely tied to its task performance. We test this theory on a large number of tasks and autoregressive LMs, and the resulting correlation study validates our hypothesis. Further analysis of this relationship demonstrates that the best prompts differ across models, highlighting the importance of model-specific analysis, and that the underlying structure of the relationship between perplexity and performance varies across tasks.</p>
<p>In light of these findings, we then propose a method, SPELL, to help users find wellperforming prompts for new tasks. Empirical validation of the proposed procedure shows that SPELL generates effective prompts with low variability in performance, and produces small gains of 1.8 (2.3) accuracy points with OPT (Bloom) over manual prompts. We therefore conclude that SPELL provides a general and interpretable approach for applying LMs to new tasks while requiring minimal human effort, and no labels.</p>
<h2>Limitations</h2>
<p>Searching for human-readable prompts We limit our search space to human-readable prompts that are fluent and accurately describe the task at hand, as we are primarily motivated in understanding why some relevant prompts work better than others. We do this by using manually created prompts and their automatically created paraphrases. Our findings may not hold when the possible prompt space is expanded to include any token sequence; we leave this direction to future work.</p>
<p>Generality of our analysis and of the SPELL method We perform our analysis on and build our method around specific models, namely OPT and Bloom. Additionally, our study is limited to the specific tasks we experiment with and to English. It is possible that our analysis and SPELL method do not generalize to other pretrained models or tasks; however, we consider models of various sizes and from different sources, and a wide range of tasks to mitigate this risk.</p>
<h2>Acknowledgements</h2>
<p>We thank Alisa Liu and Orevaoghene Ahia for their help in annotating noisy prompts. We also thank the reviewers for their valuable comments on the paper.</p>
<h2>References</h2>
<p>Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush. 2022. Promptsource: An integrated development environment and repository for natural language prompts.</p>
<p>Francesco Barbieri, Jose Camacho-Collados, Luis Espinosa-Anke, and Leonardo Neves. 2020. TweetEval:Unified Benchmark and Comparative Evaluation for Tweet Classification. In Proceedings of Findings of EMNLP.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, and He He. 2022. On the relation between sensitivity and accuracy in in-context learning. arXiv e-prints, pages arXiv-2209.</p>
<p>Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.</p>
<p>Johannes Dellert, Thora Daneyko, Alla Münch, Alina Ladygina, Armin Buch, Natalie Clarius, Ilja Grigorjew, Mohamed Balabel, Hizniye Isabella Boga, Zalina Baysarova, et al. 2019. Northeuralex: a widecoverage lexical database of northern eurasia. Language Resources and Evaluation, pages 1-29.</p>
<p>Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. 2022. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548.</p>
<p>Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schütze, and Yoav Goldberg. 2022. Measuring causal effects of data statistics on language model'sfactual'predictions. arXiv preprint arXiv:2207.14251.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816-3830.</p>
<p>Xiaochuang Han and Yulia Tsvetkov. 2022. Orca: Interpreting prompted language models via locating supporting data evidence in the ocean of pretraining data. arXiv preprint arXiv:2205.12600.</p>
<p>Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability answer isn't always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7038-7051.</p>
<p>Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022. Large language models struggle to learn long-tail knowledge. arXiv preprint arXiv:2211.08411.</p>
<p>Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2022a. Reframing instructional prompts to gptk's language. In Findings of the Association for Computational Linguistics: ACL 2022, pages 589-612.</p>
<p>Daniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui Qin, Kyle Richardson, Sean Welleck, Hannaneh Hajishirzi, Tushar Khot, Ashish Sabharwal, Sameer</p>
<p>Singh, and Yejin Choi. 2022b. Prompt waywardness: The curious case of discretized interpretation of continuous prompts. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</p>
<p>Teven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.</p>
<p>Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Sören Auer, et al. 2015. Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia. Semantic web, 6(2):167-195.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045-3059.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 45824597.</p>
<p>Alexandra Sasha Luccioni, Sylvain Viguier, and AnneLaure Ligozat. 2022. Estimating the carbon footprint of bloom, a 176b parameter language model. arXiv preprint arXiv:2211.02001.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150, Portland, Oregon, USA. Association for Computational Linguistics.</p>
<p>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39-41.
N. Moniz and L. Torgo. 2018. Multi-source social feedback of online news feeds. ArXiv, abs/1801.07055.</p>
<p>Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5203-5212.</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206.</p>
<p>Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. 2018. CARER: Contextualized affect representations for emotion recognition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3687-3697, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Timo Schick and Hinrich Schütze. 2020. It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.</p>
<p>Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke Zettlemoyer. 2022. Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too? arXiv preprint arXiv:2212.10539.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980.</p>
<p>Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471.</p>
<p>John Wieting, Jonathan Mallinson, and Kevin Gimpel. 2017. Learning paraphrastic sentence embeddings from back-translated bitext. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 274-285.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In NIPS.</p>
<h1>A Manually Created Prompts</h1>
<p>Table 11 lists the manually created prompts we use for the different tasks. We manually add, remove and edit prompts for some of these tasks, to make them fit for our setting. For example, the following prompt for AG News, taken from Promptsource, does not fit our setting: Would you recommend the following article to a politician, an athlete, business executive, or a scientist?</p>
<h2>B Lowest Perplexity Prompts</h2>
<p>Table 12 lists the 5 lowest perplexity prompts for each task, using OPT 175B.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Manual Prompts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Antonyms</td>
<td style="text-align: center;">The antonym of the word "good" is " <br> The opposite meaning of the word "good" is " <br> "Good" is the opposite of " <br> "Good" is the negation of " <br> The following are opposites of each other: "good" and " <br> The word "good" contradicts the word " <br> The antonym of the word good is <br> The opposite meaning of the word good is <br> Good is the opposite of <br> Good is the negation of <br> The following are opposites of each other: good and <br> The word good contradicts the word</td>
</tr>
<tr>
<td style="text-align: center;">GLUE Cola</td>
<td style="text-align: center;">Does the this sentence make sense and use correct English? <br> Is this example grammatically correct and sensible? <br> Does this sentence make sense and is it grammatically correct? <br> Does this example use correct English?</td>
</tr>
<tr>
<td style="text-align: center;">Newspop</td>
<td style="text-align: center;">What is the article about? <br> What is this news about? <br> What is the topic of this news piece? <br> What does this article discuss? <br> What is the topic of this sentence? <br> What category does the article belong to? <br> Pick one category for this news piece. <br> Pick the category that fits the text. <br> The article refers to which category? <br> What topic does the article belong to? <br> What category fits this article? <br> What topic does this news piece belong to? <br> Choose the correct category for this article.</td>
</tr>
<tr>
<td style="text-align: center;">AG News</td>
<td style="text-align: center;">What label best describes this news article? <br> What is this piece of news regarding? <br> Which newspaper section would this article likely appear in? <br> What topic is this news article about?</td>
</tr>
<tr>
<td style="text-align: center;">IMDB</td>
<td style="text-align: center;">This movie review expresses what sentiment? <br> Did the reviewer find this movie good or bad? <br> Is this review positive or negative? <br> How does the viewer feel about the movie? <br> What sentiment does the writer express for the movie? <br> What sentiment is expressed for the movie? <br> What is the sentiment expressed in this text? <br> Did the reviewer enjoy the movie? <br> What is the sentiment expressed by the reviewer for the movie? <br> How does the reviewer feel about the movie?</td>
</tr>
<tr>
<td style="text-align: center;">DBpedia</td>
<td style="text-align: center;">What category does the paragraph belong to? <br> Pick one category for the text. <br> Pick the category that fits the text. <br> The text refers to which category? <br> What category does the title belong to? <br> What category fits this text? <br> What topic does this text belong to? <br> Choose the correct category for the text.</td>
</tr>
<tr>
<td style="text-align: center;">Emotion</td>
<td style="text-align: center;">What is the emotion expressed in this message? <br> What emotion does this message express? <br> How will you feel about the message? <br> What emotion does the writer express for the message?</td>
</tr>
<tr>
<td style="text-align: center;">Tweet Offensive</td>
<td style="text-align: center;">Is this tweet offensive? <br> Can the tweet be removed for being offensive? <br> Is the author's tweet offensive? <br> Task: Identify if the tweet or text is offensive. <br> Is this an offensive tweet?</td>
</tr>
<tr>
<td style="text-align: center;">Word-Level Translation</td>
<td style="text-align: center;">The translation of the word "dog" to French is " <br> The translation of the word dog to French is <br> The word "dog" in French is " <br> "dog" (In French: " <br> Translate the word dog into French: <br> The translation of dog to French is <br> "dog" (French: " <br> The word dog in French is <br> Translate the word "dog" into French: " <br> dog (In French: <br> dog (French: <br> The translation of "dog" to French is "</td>
</tr>
</tbody>
</table>
<p>Table 11: The set of manually created prompts for each task.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Lowest Perplexity Prompts</th>
<th style="text-align: center;">Perplexity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Antonyms</td>
<td style="text-align: center;">The following two words are antonyms: "good" and "</td>
<td style="text-align: center;">10.24</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The antonym of the word "good" is "</td>
<td style="text-align: center;">10.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The word that has the opposite meaning of the word "good" is "</td>
<td style="text-align: center;">10.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The word "good" is the antithesis of the word "</td>
<td style="text-align: center;">10.85</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The word "good" is the opposite of the word "</td>
<td style="text-align: center;">11.15</td>
</tr>
<tr>
<td style="text-align: center;">GLUE Cola</td>
<td style="text-align: center;">Is this an example of the proper use of the English language?</td>
<td style="text-align: center;">11.63</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Does the sentence make sense and does it follow the rules of grammar?</td>
<td style="text-align: center;">11.76</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Is this sentence an example of the correct use of the English language?</td>
<td style="text-align: center;">12.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Does this sentence make sense and is it grammatically correct?</td>
<td style="text-align: center;">12.15</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Is this sentence grammatically correct and does it make sense?</td>
<td style="text-align: center;">12.68</td>
</tr>
<tr>
<td style="text-align: center;">Newspop</td>
<td style="text-align: center;">What is the main subject of the article?</td>
<td style="text-align: center;">10.01</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What is the main topic of the article?</td>
<td style="text-align: center;">10.01</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What is the subject matter of the article?</td>
<td style="text-align: center;">10.17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What is the subject of the article?</td>
<td style="text-align: center;">10.21</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What is the main idea of this article?</td>
<td style="text-align: center;">10.21</td>
</tr>
<tr>
<td style="text-align: center;">AG News</td>
<td style="text-align: center;">In what section of the newspaper would you expect to find this article?</td>
<td style="text-align: center;">7.51</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">In which section of the newspaper would you expect to find this article?</td>
<td style="text-align: center;">7.52</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">In which section of the newspaper would this article be most likely to appear?</td>
<td style="text-align: center;">7.60</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">In what section of the newspaper do you expect to find this article?</td>
<td style="text-align: center;">7.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">In what section of the newspaper would this article most likely appear?</td>
<td style="text-align: center;">7.87</td>
</tr>
<tr>
<td style="text-align: center;">IMDB</td>
<td style="text-align: center;">What is the opinion of the review? Is it positive or negative?</td>
<td style="text-align: center;">7.19</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Is this a positive or negative review?</td>
<td style="text-align: center;">7.31</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What do you think of the movie?</td>
<td style="text-align: center;">7.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What do you think of the film?</td>
<td style="text-align: center;">7.35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Is that a positive or a negative?</td>
<td style="text-align: center;">7.35</td>
</tr>
<tr>
<td style="text-align: center;">DBpedia</td>
<td style="text-align: center;">What is the category to which the text refers?</td>
<td style="text-align: center;">8.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What is the subject of the text?</td>
<td style="text-align: center;">9.15</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What category does the title belong to?</td>
<td style="text-align: center;">9.18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Which category does the text refer to?</td>
<td style="text-align: center;">9.19</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What is the subject of this text?</td>
<td style="text-align: center;">9.20</td>
</tr>
<tr>
<td style="text-align: center;">Emotion</td>
<td style="text-align: center;">How do you feel when you hear this message?</td>
<td style="text-align: center;">12.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What is the writer's emotional reaction to this news?</td>
<td style="text-align: center;">13.18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">What is the emotion expressed in this message?</td>
<td style="text-align: center;">13.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">How does this message make you feel?</td>
<td style="text-align: center;">13.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">How do you feel about this message?</td>
<td style="text-align: center;">13.50</td>
</tr>
<tr>
<td style="text-align: center;">Tweet Offensive</td>
<td style="text-align: center;">If someone said this to you, would you be offended?</td>
<td style="text-align: center;">13.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">If someone said that to you, would you be offended?</td>
<td style="text-align: center;">13.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Would you be offended if someone said that to you?</td>
<td style="text-align: center;">13.73</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Would it offend you if someone said that to you?</td>
<td style="text-align: center;">14.79</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">If someone told you that, would you be offended?</td>
<td style="text-align: center;">14.93</td>
</tr>
<tr>
<td style="text-align: center;">Word-Level Translation</td>
<td style="text-align: center;">The word for "dog" in French is "</td>
<td style="text-align: center;">7.73</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The French word for "dog" is "</td>
<td style="text-align: center;">8.16</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The French translation of the word "dog" is "</td>
<td style="text-align: center;">8.24</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The translation of the word "dog" in French is "</td>
<td style="text-align: center;">8.35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The translation of the word "dog" into French is "</td>
<td style="text-align: center;">8.91</td>
</tr>
</tbody>
</table>
<p>Table 12: The 5 lowest perplexity prompts for each task, using OPT 175B.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ We also calculate the standard deviation when using the same amount of low-perplexity prompts as in the manual prompts set for each task and get averages of 6.32 and 3.78 for OPT 175B and Bloom, respectively.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
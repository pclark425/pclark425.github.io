<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2198 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2198</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2198</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-58.html">extraction-schema-58</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <p><strong>Paper ID:</strong> paper-276575610</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.16069v1.pdf" target="_blank">Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents</a></p>
                <p><strong>Paper Abstract:</strong> Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results. Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge. To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability. To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects. Compared to the strongest baseline tested, we achieve a 3.4$\times$ improvement in correctly answering experimental questions. Curie is open-sourced at https://github.com/Just-Curieous/Curie.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2198.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2198.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curie</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curie AI agent framework for rigorous experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-agent framework that injects rigor into automated experimentation via an Experimental Rigor Engine composed of intra-agent validators, inter-agent control, and a structured experiment knowledge store to produce reproducible computational experiments and audited results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Curie (Experimental Rigor Engine + Architect + Technicians)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / AI-driven scientific experimentation (software/ML evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Curie validates experiments primarily via computational checks: (1) Intra-ARM modular validators that perform stepwise validation (Experimental Setup Validator checking alignment with plan, variables, I/O, placeholders; Execution Validator running setups in a clean environment and detecting errors), (2) Inter-ARM control enforcing methodical state transitions and scheduling to avoid out-of-order execution, and (3) an Experiment Knowledge Module that enforces structured reads/writes, provenance (time-machine), and tiered write access. Evaluation against ground truth uses computational reproduction of results drawn from published papers and open-source benchmarks; the framework also employs an LLM-based judge for automated verification and a manual expert check for semantic alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Reported benchmark metrics (aggregated across 46 tasks, averaged over 5 trials each): Curie weighted average - Experiment Design: 97.9%, Execution Setup: 78.1%, Implementation Alignment: 73.4%, Conclusion Correctness: 36.1%. Curie achieves a 3.4× improvement in correctly answering experimental questions versus the strongest baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Sufficiency is defined by: (1) reproducible and runnable experimental setup (code executes reliably), (2) faithful implementation aligned with the experimental plan, (3) structured documentation and provenance to reconstruct states, and (4) agreement of conclusions with ground-truth published results or official benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Not discussed as a substitute for physical experiments in this paper; Curie focuses on running real computational experiments and reproducibility rather than simulation-as-proxy for physical processes.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>No physics/chemistry simulation failures discussed; paper reports failures of baseline agents to produce executable, aligned code (syntax, logic, unresolved dependencies) which Curie's validators detect and mitigate.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Statistical robustness via executing each task in 5 independent trials and reporting averaged metrics; LLM judge assessments are cross-checked on a subset against expert annotations to measure agreement (numeric agreement rates not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Multiple safeguards: Setup validator checks for placeholders and hardcoded/mock values; tiered write access + validation before commit prevents agents from corrupting records; the knowledge module records provenance (time machine) to detect inconsistent or hallucinated edits.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Paper discusses resource-aware scheduling (partitioning, prioritization, and agent availability) and notes that some tasks (e.g., cloud/long-running experiments) are resource/time intensive; no concrete monetary cost metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Limitations explicitly acknowledged: (1) manual assessment still required for implementation alignment because automated judges struggle to detect semantic mismatches in code, (2) domain-specific adaptations are needed for disciplines with different experimental modalities (e.g., long-term biological studies), and (3) conclusion accuracy remains constrained by earlier pipeline errors.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Paper argues that structured documentation, provenance, and systematic validation increase scientific credibility and auditability; supports this with empirical gains in benchmark metrics and case studies reproducing/extending published findings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Compared to two baselines (OpenHands and Microsoft Magentic). Curie outperforms them across metrics (weighted averages above). Example: Curie Conclusion Correctness 36.1% vs OpenHands 10.5% and Magentic 2.3%; Curie achieves an overall 3.4× improvement on experimental questions vs the strongest baseline.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2198.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2198.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Intra-ARM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intra-Agent Rigor Module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A module inside Curie that enforces reliability within individual agents by running extensible, modular validators at each experimental step to detect and correct errors early (before they cascade).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Intra-ARM (modular validators)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / AI-driven scientific experimentation (software/ML evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Provides continuous, stage-wise validation using a suite of validators. Two described validators: Experimental Setup Validator (verifies plan alignment, variables, I/O arguments, placeholders/hardcoded values, and documentation of intermediate steps/expected results) and Execution Validator (executes in a clean environment, checks for error-free runs, logs errors for debugging, and runs reproducibility checks by executing workflows multiple times). Validators are extensible to incorporate additional checks.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Indirect: credited with improving Curie's execution/setup and alignment metrics; paper-level reported Curie execution setup weighted average 78.1% (domain breakdowns available in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Validator policies explicitly check that setup aligns to research question and variable definitions and that execution runs error-free and reproducibly; these checks constitute sufficient validation for computational experiments in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Not applicable; Intra-ARM focuses on real execution and reproducibility rather than substituting simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Not applicable (no physics simulations described).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Reproducibility checks by running workflows multiple times and detecting anomalies or hidden dependencies; results aggregated over multiple trials (5 trials per task) at benchmark level.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Setup Validator detects placeholders and hardcoded/mocked values; validators log inconsistent or missing values as errors to prevent fabricated results from being accepted.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Execution Validator runs multiple times for reproducibility which increases compute/time; no absolute cost figures given. Curie uses scheduler to manage resource constraints (e.g., GPU availability) to limit runtime costs.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Validators may detect syntactic/structural issues but can miss subtle semantic mismatches between intended methodology and implementation (necessitating manual expert review for alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>By surfacing and logging intermediate errors and ensuring stepwise correctness, Intra-ARM increases reproducibility and traceability, thereby improving credibility of automated experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>No explicit gold-standard comparison; credited qualitatively and quantitatively (via Curie's improved execution/alignment metrics) against baselines lacking continuous validators.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2198.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2198.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experimental Setup Validator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experimental Setup Validator (Intra-ARM component)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Validator that inspects proposed experiment setups for methodological soundness and logical consistency before execution, ensuring alignment with the experimental plan and checking I/O handling, placeholders, and documentation of intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Experimental Setup Validator</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / AI-driven experimental workflows</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Policy-driven checks applied to technician-generated setup code/configs: confirms alignment to research question and variables (independent, dependent, constants), analyzes I/O argument handling to detect missing or hardcoded values, ensures intermediate steps and expected outputs are documented, and flags incomplete variables or placeholders.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>No per-validator numeric success rate reported; credited as a major factor in Curie's higher execution/setup and alignment scores (e.g., Curie Exec Setup weighted avg 78.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Treats reproducible, documented, and parameterized setups (no hardcoded placeholders) as standard for sufficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Flags anomalies and missing pieces rather than numeric error bars; reproducibility later quantified via repeated execution.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Explicit checks for placeholders and hardcoded/mock values to prevent fabricated or non-generalizable setups.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Adds verification overhead prior to execution but prevents wasted time on invalid runs; no quantitative cost estimates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Can miss high-level semantic mismatches between intended experimental methodology and implementation; paper notes manual inspection is sometimes required to detect such issues.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Pre-execution validation reduces downstream failures and increases confidence in reported computational results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>No direct comparison to an external gold-standard validator; presented as improvement over baseline agents that lack such systematic setup checks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2198.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2198.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Execution Validator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Execution Validator (Intra-ARM component)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A validator that runs experiment setups in a controlled, clean environment to ensure error-free execution and reproducibility, logging errors and re-running workflows to detect non-determinism or hidden dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Execution Validator</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Reproducible software experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Executes the prepared experiment setup inside an isolated/clean environment, checking for (1) error-free runs (detailed error logs for debugging), (2) reproducibility by re-running workflows multiple times and comparing outputs to detect anomalies or hidden dependencies, and (3) alignment of produced results with the experimental plan and predefined quality standards.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Domain-specific execution rates reported in Table 2 for Curie: LLM Reasoning Exec 83.3%, Vector DB Exec 71.7%, Cloud Comp Exec 92.7%, ML Training Exec 66.7%; weighted Exec avg 78.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Execution is considered valid when code runs without errors in a clean environment and gives consistent outputs across repeated runs.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Paper reports many baseline failures (syntax errors, logic mistakes, unresolved dependencies) detected by this validator; no physics-simulation failures since domain is computational.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Reproducibility checks via multiple runs; aggregated performance reported across 5 independent trials per task at benchmark level.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Detects fabricated outputs when code depends on hardcoded or mocked data that cause inconsistent or non-reproducible outputs across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Multiple runs increase compute/time; paper notes long-running experiments (e.g., cloud tasks) require scheduling and resource-aware execution to manage cost/time.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Does not guarantee semantic correctness of implementation relative to intended methodology; thus manual semantic review is required in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>High-quality, repeatable execution logs and error traces improve reproducibility and trustworthiness of computational experiment results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Execution Validator produced higher execution success rates for Curie relative to baselines (see Table 2), but no comparison to an external industry gold-standard harness is provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2198.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2198.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experiment Knowledge Module</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experiment Knowledge Module (structured knowledge & provenance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured experimental knowledge store that enforces consistent reads/writes, records a DAG-like time-machine of plan changes, and enforces tiered write access to improve interpretability, provenance, and detect inconsistent or hallucinated agent updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Experiment Knowledge Module</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Computational reproducibility & provenance</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Converts free-form plans into enriched structured records with metadata (setups, execution status, results), maintains a time-stamped change history to reconstruct past states, and enforces tiered write permissions. All writes are validated for semantic integrity (e.g., file path validity) and are rejected with actionable error messages if malformed. The module reduces reliance on LLM memory and prevents inconsistent/hallucinated updates.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>No direct numeric success rates; credited qualitatively with improving Curie's design and conclusion performance by enabling traceability and structured analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Provenance, time-stamped records, and role-based write permissions are treated as necessary for interpretability and reproducibility in computational experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not a numeric UQ mechanism; uncertainty reduced via provenance and versioning which enable isolation and rollback of problematic contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Yes — tiered write access, semantic validation of writes, and provenance history help detect and prevent fabricated, hallucinated, or inconsistent experimental records.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Adds metadata and validation overhead to each write; paper does not quantify extra time but emphasizes benefits for large-scale experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Cannot by itself detect semantic correctness of experimental code/results; remains complementary to execution validators and human review.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Structured provenance and auditable change history are argued to increase community acceptance by making experiments auditable and reproducible.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>No explicit comparison to external provenance systems; presented as improvement over approaches that rely solely on LLM memory or unconstrained logs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2198.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2198.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Judge + Expert Cross-check</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Judge (automated verifier) with manual expert cross-check</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated LLM judge is used to verify experiment design, execution setup, and conclusion correctness against ground truth, while a human expert manually assesses implementation alignment on a subset to ensure semantic fidelity and to calibrate the judge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLM judge (Zheng et al., 2023) combined with manual expert verification</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Evaluation of AI-driven experiments</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The paper uses a system prompt driven LLM judge to produce pass/fail binary evaluations for Experiment Design, Execution Setup, Implementation Alignment, and Conclusion Correctness when ground truth is available. Because automated judges struggle with semantic code alignment, implementation alignment is manually assessed by experts; a subset of LLM judge outputs is cross-checked against expert annotations to measure agreement and refine the judge prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>No numeric agreement rate provided; paper states that the LLM judge is used for straightforward verification tasks and that a subset of its assessments was cross-checked with experts to measure agreement and refine the judge's system prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Automated pass/fail decisions from the judge are acceptable for straightforward checks (design/setup/conclusion) but manual review is required for nuanced semantic alignment of code and methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Judge outputs are binary for each metric; statistical robustness achieved at the task level by averaging results across 5 independent trials per task. Agreement statistics with experts are mentioned but not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Limited — automated judge helps detect incorrect conclusions or non-reproducible outputs, but paper emphasizes the need for manual inspection to detect subtle fabricated or semantically incorrect implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Automated judging scales efficiently; manual expert cross-check introduces human time cost for semantic verification. No numeric time/costs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Automated LLM judge is insufficient to detect semantic discrepancies in code (hence manual review is used); numeric agreement rates between judge and experts are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Using an automated judge plus expert checks is presented as a pragmatic tradeoff to enable scalable verification while preserving credibility through human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Not directly compared to a canonical gold-standard human-only evaluation; presented as a hybrid scalable alternative with manual spot-checks to preserve quality.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Let's verify step by step. H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, arXiv:2305.20050 <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>The hallmark effect: Supporting provenance and transparent use of large language models in writing with interactive visualization <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 1)</em></li>
                <li>Agent laboratory: Using llm agents as research assistants <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2198",
    "paper_id": "paper-276575610",
    "extraction_schema_id": "extraction-schema-58",
    "extracted_data": [
        {
            "name_short": "Curie",
            "name_full": "Curie AI agent framework for rigorous experimentation",
            "brief_description": "An LLM-agent framework that injects rigor into automated experimentation via an Experimental Rigor Engine composed of intra-agent validators, inter-agent control, and a structured experiment knowledge store to produce reproducible computational experiments and audited results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Curie (Experimental Rigor Engine + Architect + Technicians)",
            "scientific_domain": "Computer Science / AI-driven scientific experimentation (software/ML evaluation)",
            "validation_type": "computational validation",
            "validation_description": "Curie validates experiments primarily via computational checks: (1) Intra-ARM modular validators that perform stepwise validation (Experimental Setup Validator checking alignment with plan, variables, I/O, placeholders; Execution Validator running setups in a clean environment and detecting errors), (2) Inter-ARM control enforcing methodical state transitions and scheduling to avoid out-of-order execution, and (3) an Experiment Knowledge Module that enforces structured reads/writes, provenance (time-machine), and tiered write access. Evaluation against ground truth uses computational reproduction of results drawn from published papers and open-source benchmarks; the framework also employs an LLM-based judge for automated verification and a manual expert check for semantic alignment.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": null,
            "validation_success_rate": "Reported benchmark metrics (aggregated across 46 tasks, averaged over 5 trials each): Curie weighted average - Experiment Design: 97.9%, Execution Setup: 78.1%, Implementation Alignment: 73.4%, Conclusion Correctness: 36.1%. Curie achieves a 3.4× improvement in correctly answering experimental questions versus the strongest baseline.",
            "domain_validation_standards": "Sufficiency is defined by: (1) reproducible and runnable experimental setup (code executes reliably), (2) faithful implementation aligned with the experimental plan, (3) structured documentation and provenance to reconstruct states, and (4) agreement of conclusions with ground-truth published results or official benchmarks.",
            "when_simulation_sufficient": "Not discussed as a substitute for physical experiments in this paper; Curie focuses on running real computational experiments and reproducibility rather than simulation-as-proxy for physical processes.",
            "simulation_failures": "No physics/chemistry simulation failures discussed; paper reports failures of baseline agents to produce executable, aligned code (syntax, logic, unresolved dependencies) which Curie's validators detect and mitigate.",
            "uncertainty_quantification": "Statistical robustness via executing each task in 5 independent trials and reporting averaged metrics; LLM judge assessments are cross-checked on a subset against expert annotations to measure agreement (numeric agreement rates not reported).",
            "fabrication_detection": "Multiple safeguards: Setup validator checks for placeholders and hardcoded/mock values; tiered write access + validation before commit prevents agents from corrupting records; the knowledge module records provenance (time machine) to detect inconsistent or hallucinated edits.",
            "validation_cost_time": "Paper discusses resource-aware scheduling (partitioning, prioritization, and agent availability) and notes that some tasks (e.g., cloud/long-running experiments) are resource/time intensive; no concrete monetary cost metrics provided.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Limitations explicitly acknowledged: (1) manual assessment still required for implementation alignment because automated judges struggle to detect semantic mismatches in code, (2) domain-specific adaptations are needed for disciplines with different experimental modalities (e.g., long-term biological studies), and (3) conclusion accuracy remains constrained by earlier pipeline errors.",
            "acceptance_credibility": "Paper argues that structured documentation, provenance, and systematic validation increase scientific credibility and auditability; supports this with empirical gains in benchmark metrics and case studies reproducing/extending published findings.",
            "comparison_to_gold_standard": "Compared to two baselines (OpenHands and Microsoft Magentic). Curie outperforms them across metrics (weighted averages above). Example: Curie Conclusion Correctness 36.1% vs OpenHands 10.5% and Magentic 2.3%; Curie achieves an overall 3.4× improvement on experimental questions vs the strongest baseline.",
            "uuid": "e2198.0"
        },
        {
            "name_short": "Intra-ARM",
            "name_full": "Intra-Agent Rigor Module",
            "brief_description": "A module inside Curie that enforces reliability within individual agents by running extensible, modular validators at each experimental step to detect and correct errors early (before they cascade).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Intra-ARM (modular validators)",
            "scientific_domain": "Computer Science / AI-driven scientific experimentation (software/ML evaluation)",
            "validation_type": "computational validation",
            "validation_description": "Provides continuous, stage-wise validation using a suite of validators. Two described validators: Experimental Setup Validator (verifies plan alignment, variables, I/O arguments, placeholders/hardcoded values, and documentation of intermediate steps/expected results) and Execution Validator (executes in a clean environment, checks for error-free runs, logs errors for debugging, and runs reproducibility checks by executing workflows multiple times). Validators are extensible to incorporate additional checks.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": null,
            "validation_success_rate": "Indirect: credited with improving Curie's execution/setup and alignment metrics; paper-level reported Curie execution setup weighted average 78.1% (domain breakdowns available in Table 2).",
            "domain_validation_standards": "Validator policies explicitly check that setup aligns to research question and variable definitions and that execution runs error-free and reproducibly; these checks constitute sufficient validation for computational experiments in the benchmark.",
            "when_simulation_sufficient": "Not applicable; Intra-ARM focuses on real execution and reproducibility rather than substituting simulation.",
            "simulation_failures": "Not applicable (no physics simulations described).",
            "uncertainty_quantification": "Reproducibility checks by running workflows multiple times and detecting anomalies or hidden dependencies; results aggregated over multiple trials (5 trials per task) at benchmark level.",
            "fabrication_detection": "Setup Validator detects placeholders and hardcoded/mocked values; validators log inconsistent or missing values as errors to prevent fabricated results from being accepted.",
            "validation_cost_time": "Execution Validator runs multiple times for reproducibility which increases compute/time; no absolute cost figures given. Curie uses scheduler to manage resource constraints (e.g., GPU availability) to limit runtime costs.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Validators may detect syntactic/structural issues but can miss subtle semantic mismatches between intended methodology and implementation (necessitating manual expert review for alignment).",
            "acceptance_credibility": "By surfacing and logging intermediate errors and ensuring stepwise correctness, Intra-ARM increases reproducibility and traceability, thereby improving credibility of automated experiments.",
            "comparison_to_gold_standard": "No explicit gold-standard comparison; credited qualitatively and quantitatively (via Curie's improved execution/alignment metrics) against baselines lacking continuous validators.",
            "uuid": "e2198.1"
        },
        {
            "name_short": "Experimental Setup Validator",
            "name_full": "Experimental Setup Validator (Intra-ARM component)",
            "brief_description": "Validator that inspects proposed experiment setups for methodological soundness and logical consistency before execution, ensuring alignment with the experimental plan and checking I/O handling, placeholders, and documentation of intermediate steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Experimental Setup Validator",
            "scientific_domain": "Computer Science / AI-driven experimental workflows",
            "validation_type": "computational validation",
            "validation_description": "Policy-driven checks applied to technician-generated setup code/configs: confirms alignment to research question and variables (independent, dependent, constants), analyzes I/O argument handling to detect missing or hardcoded values, ensures intermediate steps and expected outputs are documented, and flags incomplete variables or placeholders.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": null,
            "validation_success_rate": "No per-validator numeric success rate reported; credited as a major factor in Curie's higher execution/setup and alignment scores (e.g., Curie Exec Setup weighted avg 78.1%).",
            "domain_validation_standards": "Treats reproducible, documented, and parameterized setups (no hardcoded placeholders) as standard for sufficiency.",
            "when_simulation_sufficient": null,
            "simulation_failures": null,
            "uncertainty_quantification": "Flags anomalies and missing pieces rather than numeric error bars; reproducibility later quantified via repeated execution.",
            "fabrication_detection": "Explicit checks for placeholders and hardcoded/mock values to prevent fabricated or non-generalizable setups.",
            "validation_cost_time": "Adds verification overhead prior to execution but prevents wasted time on invalid runs; no quantitative cost estimates provided.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Can miss high-level semantic mismatches between intended experimental methodology and implementation; paper notes manual inspection is sometimes required to detect such issues.",
            "acceptance_credibility": "Pre-execution validation reduces downstream failures and increases confidence in reported computational results.",
            "comparison_to_gold_standard": "No direct comparison to an external gold-standard validator; presented as improvement over baseline agents that lack such systematic setup checks.",
            "uuid": "e2198.2"
        },
        {
            "name_short": "Execution Validator",
            "name_full": "Execution Validator (Intra-ARM component)",
            "brief_description": "A validator that runs experiment setups in a controlled, clean environment to ensure error-free execution and reproducibility, logging errors and re-running workflows to detect non-determinism or hidden dependencies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Execution Validator",
            "scientific_domain": "Computer Science / Reproducible software experimentation",
            "validation_type": "computational validation",
            "validation_description": "Executes the prepared experiment setup inside an isolated/clean environment, checking for (1) error-free runs (detailed error logs for debugging), (2) reproducibility by re-running workflows multiple times and comparing outputs to detect anomalies or hidden dependencies, and (3) alignment of produced results with the experimental plan and predefined quality standards.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": null,
            "validation_success_rate": "Domain-specific execution rates reported in Table 2 for Curie: LLM Reasoning Exec 83.3%, Vector DB Exec 71.7%, Cloud Comp Exec 92.7%, ML Training Exec 66.7%; weighted Exec avg 78.1%.",
            "domain_validation_standards": "Execution is considered valid when code runs without errors in a clean environment and gives consistent outputs across repeated runs.",
            "when_simulation_sufficient": null,
            "simulation_failures": "Paper reports many baseline failures (syntax errors, logic mistakes, unresolved dependencies) detected by this validator; no physics-simulation failures since domain is computational.",
            "uncertainty_quantification": "Reproducibility checks via multiple runs; aggregated performance reported across 5 independent trials per task at benchmark level.",
            "fabrication_detection": "Detects fabricated outputs when code depends on hardcoded or mocked data that cause inconsistent or non-reproducible outputs across runs.",
            "validation_cost_time": "Multiple runs increase compute/time; paper notes long-running experiments (e.g., cloud tasks) require scheduling and resource-aware execution to manage cost/time.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Does not guarantee semantic correctness of implementation relative to intended methodology; thus manual semantic review is required in some cases.",
            "acceptance_credibility": "High-quality, repeatable execution logs and error traces improve reproducibility and trustworthiness of computational experiment results.",
            "comparison_to_gold_standard": "Execution Validator produced higher execution success rates for Curie relative to baselines (see Table 2), but no comparison to an external industry gold-standard harness is provided.",
            "uuid": "e2198.3"
        },
        {
            "name_short": "Experiment Knowledge Module",
            "name_full": "Experiment Knowledge Module (structured knowledge & provenance)",
            "brief_description": "A structured experimental knowledge store that enforces consistent reads/writes, records a DAG-like time-machine of plan changes, and enforces tiered write access to improve interpretability, provenance, and detect inconsistent or hallucinated agent updates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Experiment Knowledge Module",
            "scientific_domain": "Computer Science / Computational reproducibility & provenance",
            "validation_type": "other",
            "validation_description": "Converts free-form plans into enriched structured records with metadata (setups, execution status, results), maintains a time-stamped change history to reconstruct past states, and enforces tiered write permissions. All writes are validated for semantic integrity (e.g., file path validity) and are rejected with actionable error messages if malformed. The module reduces reliance on LLM memory and prevents inconsistent/hallucinated updates.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": null,
            "validation_success_rate": "No direct numeric success rates; credited qualitatively with improving Curie's design and conclusion performance by enabling traceability and structured analysis.",
            "domain_validation_standards": "Provenance, time-stamped records, and role-based write permissions are treated as necessary for interpretability and reproducibility in computational experiments.",
            "when_simulation_sufficient": null,
            "simulation_failures": null,
            "uncertainty_quantification": "Not a numeric UQ mechanism; uncertainty reduced via provenance and versioning which enable isolation and rollback of problematic contributions.",
            "fabrication_detection": "Yes — tiered write access, semantic validation of writes, and provenance history help detect and prevent fabricated, hallucinated, or inconsistent experimental records.",
            "validation_cost_time": "Adds metadata and validation overhead to each write; paper does not quantify extra time but emphasizes benefits for large-scale experiments.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Cannot by itself detect semantic correctness of experimental code/results; remains complementary to execution validators and human review.",
            "acceptance_credibility": "Structured provenance and auditable change history are argued to increase community acceptance by making experiments auditable and reproducible.",
            "comparison_to_gold_standard": "No explicit comparison to external provenance systems; presented as improvement over approaches that rely solely on LLM memory or unconstrained logs.",
            "uuid": "e2198.4"
        },
        {
            "name_short": "LLM Judge + Expert Cross-check",
            "name_full": "LLM-based Judge (automated verifier) with manual expert cross-check",
            "brief_description": "An automated LLM judge is used to verify experiment design, execution setup, and conclusion correctness against ground truth, while a human expert manually assesses implementation alignment on a subset to ensure semantic fidelity and to calibrate the judge.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_or_method_name": "LLM judge (Zheng et al., 2023) combined with manual expert verification",
            "scientific_domain": "Computer Science / Evaluation of AI-driven experiments",
            "validation_type": "hybrid",
            "validation_description": "The paper uses a system prompt driven LLM judge to produce pass/fail binary evaluations for Experiment Design, Execution Setup, Implementation Alignment, and Conclusion Correctness when ground truth is available. Because automated judges struggle with semantic code alignment, implementation alignment is manually assessed by experts; a subset of LLM judge outputs is cross-checked against expert annotations to measure agreement and refine the judge prompts.",
            "simulation_fidelity": null,
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": null,
            "validation_success_rate": "No numeric agreement rate provided; paper states that the LLM judge is used for straightforward verification tasks and that a subset of its assessments was cross-checked with experts to measure agreement and refine the judge's system prompt.",
            "domain_validation_standards": "Automated pass/fail decisions from the judge are acceptable for straightforward checks (design/setup/conclusion) but manual review is required for nuanced semantic alignment of code and methodology.",
            "when_simulation_sufficient": null,
            "simulation_failures": null,
            "uncertainty_quantification": "Judge outputs are binary for each metric; statistical robustness achieved at the task level by averaging results across 5 independent trials per task. Agreement statistics with experts are mentioned but not quantified.",
            "fabrication_detection": "Limited — automated judge helps detect incorrect conclusions or non-reproducible outputs, but paper emphasizes the need for manual inspection to detect subtle fabricated or semantically incorrect implementations.",
            "validation_cost_time": "Automated judging scales efficiently; manual expert cross-check introduces human time cost for semantic verification. No numeric time/costs reported.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Automated LLM judge is insufficient to detect semantic discrepancies in code (hence manual review is used); numeric agreement rates between judge and experts are not provided.",
            "acceptance_credibility": "Using an automated judge plus expert checks is presented as a pragmatic tradeoff to enable scalable verification while preserving credibility through human oversight.",
            "comparison_to_gold_standard": "Not directly compared to a canonical gold-standard human-only evaluation; presented as a hybrid scalable alternative with manual spot-checks to preserve quality.",
            "uuid": "e2198.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Let's verify step by step. H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, arXiv:2305.20050",
            "rating": 2
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "The hallmark effect: Supporting provenance and transparent use of large language models in writing with interactive visualization",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 1
        },
        {
            "paper_title": "Agent laboratory: Using llm agents as research assistants",
            "rating": 1
        }
    ],
    "cost": 0.01669375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents
26 Feb 2025</p>
<p>Patrick Tser 
Jern Kon 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Jiachen Liu 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Qiuyi Ding 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Yiming Qiu 
Zhenning Yang 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Yibo Huang 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Jayanth Srinivasa 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Cisco Systems</p>
<p>Myungjin Lee 
Cisco Systems</p>
<p>Mosharaf Chowdhury 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Ang Chen 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents
26 Feb 2025B6A409D7B1A744479206BB7AA7280F28arXiv:2502.16069v2[cs.AI]
Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results.Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge.To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability.To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects.Compared to the strongest baseline tested, we achieve a 3.4× improvement in correctly answering experimental questions.Curie is open-sourced at https: //github.com/Just-Curieous/Curie.</p>
<p>Introduction</p>
<p>Scientific research drives human progress, advancing medicine, technology, and our understanding of the universe.At the heart of this endeavor lies experimentation-a disciplined intellectual pursuit that transforms human curiosity, expressed through bold hypotheses, into verifiable knowledge.Experimentation thrives on creativity, as new ideas fuel discovery.Yet it also depends on rigor-ensuring that research is methodologically sound and its findings are trustworthy (Armour et al., 2009;Gill &amp; Gill, 2020).If science isn't rigorous, it's reckless (Hofseth, 2018).</p>
<p>In recent years, numerous works (Zhang et al., 2024b;Kramer et al., 2023;Lu et al., 2024) leveraging large language models (LLMs) to automate scientific research have emerged ( §2.3).These solutions typically rely on ad-hoc prompt-based methods to mimic scientific workflows, which are prone to hallucination.While effective for creative tasks such as literature review and brainstorming, these approaches remain limited in their ability to support rigorous experimentation, a largely unexplored capability.</p>
<p>More specifically, rigorous experimentation ( §2.2) involves a methodical procedure that includes formulating hypotheses, designing experiments, executing controlled trials, and analyzing results.Achieving reliability at every step is essential to ensure that the results are accurate, reproducible, and scientifically meaningful.Finally, all procedures and results must be documented in a well-structured and interpretable manner, facilitating verification, reproducibility, and collaboration across the scientific community.</p>
<p>To meet these requirements, we propose Curie, an AI agent framework representing the first step toward rigorous and automated experimentation ( §3).As shown in Fig. 1, Curie takes an experimental question and relevant context (e.g., domain-specific knowledge or starter code) as input.The Architect Agent generates high-level experimental plans, coordinates the process, and reflects on findings to guide subsequent steps.Working in unison, our Technician Agents focus on carefully implementing and executing Curie can help researchers validate, expand, and critique existing research on the benefits of repeated sampling in LLM reasoning (Brown et al., 2024).The first panel (Original Finding) presents a result from the original paper.The second panel (Reproduce) has Curie confirming this finding through rigorous experimentation.The third panel (Extend) has Curie exploring the impact of sampling temperature on repeated sampling.The final panel (Challenge) shows Curie identifying a limitation in the original methodology, suggesting an avenue for future research.controlled experiments following these plans.</p>
<p>At the core of Curie, the Experimental Rigor Engine preserves agent creativity while embedding rigor seamlessly throughout the experimentation process.This is achieved via three key modules: (1) The Intra-Agent Rigor Module safeguards reliability within individual agents by enforcing a set of extensible rigor policies (e.g., validating that experiment plans align with objectives and setups are reproducible).(2) The Inter-Agent Rigor Module maintains methodical control over agent coordination, ensuring correct task transitions and efficient task scheduling.(3) Finally, the Experiment Knowledge Module enhances interpretability by maintaining well-structured documentation, enabling seamless collaboration in large-scale experiments.</p>
<p>Though our architecture suggests applications across various disciplines, this paper focuses on addressing research problems in computer science by leveraging existing LLMfriendly interfaces for computer access (Anthropic, 2024;Yang et al., 2024).To evaluate Curie, we introduce an Experimentation Benchmark comprising 46 tasks of varying complexity across multiple domains within computer science ( §4).We derive these questions directly from influential research papers and widely adopted open-source projects, in order to reflect real-world challenges and practical significance.As shown in Fig. 2, Curie enables researchers to reproduce, extend, and challenge existing research through rigorous experimentation.</p>
<p>We benchmarked Curie ( §5) against several state-ofthe-art agents: OpenHands (Wang et al., 2024c) (a topperforming coding agent on SWE-Bench (Jimenez et al., 2023)), and Microsoft Magentic (Fourney et al., 2024) (a state-of-the-art generalist multi-agent system).Our empirical findings show that Curie achieves a 3.4× improvement in correctly answering experimental questions, compared to the strongest baseline tested, among other aspects.These results underscore Curie's ability to automate complex and rigorous experimentation tasks, making it a promising step toward accelerating scientific research.</p>
<p>Background</p>
<p>Science Experimentation</p>
<p>Scientific experimentation often starts with researchers posing testable hypotheses based on their past results, domain knowledge, and intuition.This experimentation process then unfolds across three key stages: (1) Experimental Design, where researchers plan the controlled experiment by identifying variables, selecting methodologies, and outlining procedures to enhance reproducibility and validity.(2) Experiment Execution, where researchers set up the complex experiment environments and iteratively explore vast search spaces, and (3) Data Documentation and Analysis, where researchers systematically gather data, apply analytical techniques, and extract insights to validate or refine their hypotheses.This process is iterative, as insights gained from data analysis often lead to the refinement of hypotheses, leading to subsequent rounds of these three steps.</p>
<p>Rigor in Experimentation</p>
<p>Rigor is essential in scientific research, ensuring systematic, precise, and reliable findings (Armour et al., 2009).If science isn't rigorous, it's reckless.(Hofseth, 2018).More precisely, experimental rigor is grounded in three core principles (Gill &amp; Gill, 2020):</p>
<p>Methodical Procedure: Experimentation must adhere to a principled and systematic methodology throughout all aforementioned stages, from hypothesis formulation to data documentation.Such a structured procedure ensures that no critical procedures are overlooked or performed incompletely, thereby preserving the integrity of the research.</p>
<p>Reliability: Every stage in the experimental pipeline-such as experiment design and environment setup-needs to be reliable and reproducible so that any final findings rest on solid ground.For instance, it encompasses correct variable identification, controlled experimental design, and rigorous code verification.By meticulously verifying each stage, reliability minimizes the risk of cascading errors, thereby ensuring that the results are trustworthy.</p>
<p>Interpretability: All processes and outcomes need to be clearly documented in a consistent manner.This makes it easier for researchers or agents to replicate experiments, understand results, and extend research.</p>
<p>Related Work</p>
<p>AI Agents for Science.Prior work has leveraged AI to accelerate scientific discovery (Berens et al., 2023;Kitano, 2021), focusing on various stages of the research lifecycle, including literature reviews (Agarwal et al., 2024;Tyser et al., 2024), brainstorming ideas (Gu &amp; Krenn, 2024;Bran et al., 2024), hypothesis generation (Sourati &amp; Evans, 2023;Zhou et al., 2024;Wang et al., 2024a;Qi et al., 2024) and data analysis (Hong et al., 2024a;Chen et al., 2024).While these efforts works on various aspects of the scientific lifecycle, experimentation-a critical, rigor-intensive step-remains underexplored.</p>
<p>Existing agents for end-to-end scientific research (Schmidgall et al., 2025;Lu et al., 2024;Yuan et al., 2025;Ghafarollahi &amp; Buehler, 2024) rely on ad-hoc prompts to guide predefined workflows, from idea generation to paper writing.Their open-sourced frameworks often require ex-perimental code to follow constrained, framework-specific formats, adding overhead and hindering their usability.These solutions mimic experimentation processes using multi-agent systems but lack systematic enforcement of a methodical procedure, reliability, and interpretability.Without these core principles, such agents struggle to deliver meaningful and reproducible results, limiting their practical utility in real-world scientific research.</p>
<p>AI Agent Task Benchmarks.A wide range of benchmarks have been developed to assess the capabilities of AI agents across diverse domains.Existing benchmarks primarily focus on logical reasoning (Cobbe et al., 2021;Hendrycks et al., 2021a;Bang et al., 2023), problem-solving (Hendrycks et al., 2021b;Frieder et al., 2023;Wang et al., 2024b;Sun et al., 2024a;Chevalier et al., 2024), knowledge retrieval tasks (Sun et al., 2024b) and machine learning training (Huang et al., 2024;Zhang et al., 2023;2024a).These benchmarks evaluate agents on well-defined tasks that typically have clear, deterministic solutions.</p>
<p>In contrast, our benchmark focuses on experimentation, which requires a more rigorous and systematic approach beyond problem-solving.Experimental tasks require iterative hypothesis refinement, complex experiment setup and execution, and robust result interpretation.Our benchmark captures these challenges by evaluating AI systems on real-world experimentation tasks derived from influential research papers and widely adopted open-source projects.</p>
<p>Curie: Rigorous Experimentation</p>
<p>Architectural Overview</p>
<p>As shown in Fig. 3, Curie is composed of two types of LLM-based agents (an Architect Agent and a host of Tech- nician Agents), sandwiched between them is our main innovation, the Experimental Rigor Engine that injects rigor throughout the experimental process.</p>
<p>High-level workflow.Given an experimental question, our Architect will 1 designs high-level experimental plans (e.g., defining hypotheses, variables), completing its turn.Our Inter-Agent Rigor Module (Inter-ARM ) will A intercept and enforce methodical procedure.Since the plan is new, it is broken into smaller partitions for finer-grained execution.Inter-ARM applies control flow policies to determine the next step for each partition.In this case, it decides go through the B the Intra-Agent Rigor Module (Intra-ARM ) validation, which enhances reliability by verifying partition integrity (e.g., assessing relevance to the experimental question).Similarly, Inter-ARM repeats this process based on the validation results, eventually C forwarding the partition to a Technician to 2 set up the controlled experiment.The remaining steps are omitted for brevity, but at a high level, every agent action follows the same structured workflow: A interception by Inter-ARM, B validation by Intra-ARM, and C forwarding to the next appropriate agent.Finally, all of the above components will make use of our Experiment Knowledge Module for storing and tracking experimental progress, providing interpretability.For example, the Architect stores refined experimental plans in a structured, metadata-enriched format, making them easier to analyze, track, and validate over time.</p>
<p>Intra-Agent Rigor Module -Reliability</p>
<p>Large-scale and long-running experiments involve complex, interdependent steps where early-stage errors can propagate and compromise final results.This is especially critical to LLM-based experimentation since: (1) LLM-based agents are prone to hallucination, and (2) experimental processes are inherently exploratory, requiring iterative refinements to hypotheses, setups, and designs in response to new or unexpected findings.Despite this, existing works (Lu et al., 2024;Schmidgall et al., 2025) largely overlook the need for  continuous validation throughout the experimental process.A naive approach is to perform end-to-end validation only after an experiment concludes.However, this lacks the ability to backtrack to intermediate stages, preventing error isolation and correction, and forcing researchers to either discard progress or rerun the entire experiment-an inefficient and costly approach.To address this, we introduce Intra-ARM, a validation module that verifies the assigned tasks of our Architect and Technicians step by step, improving reliability and reproducibility to align with the overarching experimental objectives.Inspired by process supervision (Lightman et al., 2023), Intra-ARM utilizes modular validation, where a suite of validators continuously verifies each stage of the experiment (Fig. 3), so that errors can be proactively detected and addressed early.Moreover, Intra-ARM 's validators are extensible, allowing new ones to be incorporated as needed.We focus on two key validators here for brevity:</p>
<p>Experimental Setup Validator.This component (Fig. 4) verifies that the experimental setup by our technicians aligns with the plan before execution, ensuring methodological soundness and logical consistency.Each enforced policy checks alignment within a specific part of the experiment setup.This includes (Fig. 5a): (1) confirming the setup aligns with the experimental plan, including the research question and all specified variables (independent, dependent, and constant).( 2) Analyzing all procedures for correct handling of input/output arguments; and detecting placeholders, hardcoded values, or incomplete variables to ensure meaningful results.(3) Checking that the setup documents all intermediate steps and expected results, including any identified issues for future analysis.</p>
<p>Execution Validator.Once the setup passes the experimental setup validator, this validator enhances reproducibility by executing it in a controlled and clean environment to detect and resolve potential errors, a sample of which is illustrated in Fig. 5b.(1) Error-Free Execution: The setup is executed in a clean environment, verifying that it operates without errors.Any encountered errors are logged in detail, providing actionable feedback for debugging and iterative refinement.(2) Reproducibility Checks: The workflow is also run multiple times to enhance consistency in outputs and detect anomalies or hidden dependencies.Finally, the results are validated to ensure alignment with the experimental plan and compliance with predefined quality standards.</p>
<p>Inter-Agent Rigor Module -Methodical Control</p>
<p>Experimental processes must follow a methodical precedure ( §2.2) while balancing resource constraints (e.g., GPU availability), and experiment priorities.Traditional agentic conversational patterns (AutoGen, 2024)-such as naive LLM-based coordination, sequential, or round-robin execution-are thus ill-suited for such a workflow.To ensure task coordination and optimize resource efficiency, Inter-ARM enables seamless collaboration between our Architect, Technicians and Intra-ARM through three key functions (illustrated in Fig. 6).We discuss each in turn.</p>
<p>Fine-grained Plan Partitioning.Inter-ARM first breaks down new complex experimental plans generated by the Architect into smaller, independent partitions: defined as a distinct subset of independent variable values within the plan.By creating smaller, self-contained tasks, this facilitates modular execution and enables parallelization, making experimentation more scalable.In addition, this enables our Architect to track intermediate progress and results, making real-time decisions as new insights emerge (e.g., reprioritizing partitions by updating their execution priority).</p>
<p>Control Flow Enforcement.This component ensures that transitions between our Architect, Technicians, and Intra-ARM follow a logical sequence aligned with the experimentation lifecycle.This is critical to maintaining consistent, error-free progress.Without structured coordination, tasks may be executed out of order or without necessary dependencies, leading to wasted effort and erroneous conclusions.For instance, it prevents Technicians from directly executing experiment setups before validation by Intra-ARM 's setup validator, to reduce the risk of erroneous data propagation.This is done in two steps: (1) State Evaluation: First, it evaluates the current state of each partition (within an experimental plan) that has been modified by any given agent, e.g., a Technician who produced experimental results and recorded its progress via the Experiment Knowledge Module.(2) Permissible State Transitions: Based on the current state of the partition(s), this component produces a set of allowed state transitions for the given partition, e.g., newly produced experimental results for a given partition need to be validated by Intra-ARM first.It also gathers relevant context that would be useful if the transition were to be executed.This state transition information will be consumed by our scheduler (defined below).Partition Scheduling.Executing large-scale experiments can be resource-intensive and time-consuming, requiring careful scheduling and prioritization of tasks to improve efficiency.Our scheduler currently utilizes three key parameters for partition scheduling: (1) partition execution priorities set by our Architect, (2) allowed partition state transitions, and (3) the availability of our agents (that may be busy handling other partitions).Overall, this adaptive scheduling strategy enables large-scale experimentation by improving resource efficiency while adhering to methodical experimental procedures.</p>
<p>Experiment Knowledge Module -Interpretability</p>
<p>Interpretability is fundamental to experimentation-not only for scientific accountability but also for effective experiment management.Specifically, all other components within Curie require this for real-time visibility, enabling informed decision-making, efficient troubleshooting, and adaptability as new insights emerge.A naive approach would be to delegate experimental knowledge management entirely to LLM-based agents.However, LLMs alone are ill-suited for this task for two reasons: (1) Inconsistent Reads: LLMs have inconsistent recall and are prone to forgetting (Xu et al., 2024).Without a structured and verifiable record of experimental progress, they may retrieve outdated, irrelevant, or hallucinated information, leading to misinterpretations, flawed conclusions, and compounding errors over time.(2) Inconsistent Writes: LLMs tend to hallucinate, particularly when managing large-scale experimental data.This lack of structured control risks corrupting experimental records, propagating inaccuracies, and ultimately compromising the integrity of the experimentation process.Unlike databases, LLMs do not inherently track provenance (Hoque et al., 2024), making it difficult to reconstruct how conclusions were reached.We address these two challenges in turn:</p>
<p>Structured Knowledge Reads.This mechanism organizes experimental progress in a structured format.The process begins by restructuring new experimental plans that were written by our Architect into an enriched format with critical metadata-such as setups, execution status, and results.Subsequent modifications to any part of the plan are recorded as a time machine (Fig. 7) for experimental progression, maintaining a structured, DAG-like history of changes.This historical record captures hypotheses tested, variable changes, and the reasoning behind key decisions.By preserving this evolution, Curie can reconstruct past states, trace decision rationales, and diagnose issues with greater precision.Tiered Write Access.To maintain experimental integrity and minimize the risk of errors, the interface enforces a tiered write access policy that restricts and validates updates made to the experimental plan.This ensures that our other components can only modify the portions of the plan they are responsible for, while all changes undergo rigorous validation.Our LLM-based Architect and Technicians are granted fine-grained write permissions tailored to their roles.For example, Technicians are permitted to append experimental results to their assigned partitions but cannot modify unrelated sections of the plan.Similarly, architects have broader write access, including the ability to create or remove entire partitions, but their modifications are still constrained to specific attributes, such as updating variable values or marking partitions for re-execution.Every write operation is validated before being committed to the knowledge bank.This process ensures proper structuring of inputs and enforces semantic integrity (e.g., that result file paths are valid).If errors are detected, the system returns concise error messages, enabling agents to quickly identify and resolve issues.Through this, Curie enhances robustness and error resistance in collaboration.</p>
<p>Experimentation Benchmark</p>
<p>We design a novel benchmark to stress test Curie's ability to automate experiments while enforcing rigor in the face Investigates strategies for scaling test-time computation in LLMs, focusing on balancing accuracy, latency, and cost.</p>
<p>Research papers: (Brown et al., 2024), (Jin et al., 2024).</p>
<p>Vector Indexing 6 6 3</p>
<p>Examines efficient vector indexing methods for similarity search, analyzing its trade-offs in retrieval recall, memory, and latency.</p>
<p>Open-source project: Faiss (Douze et al.,</p>
<p>Experiment-Centric Task Design</p>
<p>Instead of treating tasks as isolated problems with fixed solutions, we structure each task as a full experimental process.This means that tasks require hypothesis formation, iterative refinement, and rigorous validation, mirroring real-world experiment workflows rather than one-shot problem-solving.</p>
<p>The process begins with distilling high-level contributions from research papers (e.g., theoretical insights or empirical findings), or core system behaviors from open-source projects (e.g., the interplay between configuration parameters and performance).These insights are then translated into testable questions framed with explicit configurations, metrics, and expected outcomes.Ground truth data is derived from published results or official benchmarks provided by open-source projects.We use these findings to design tasks with three key components:</p>
<ol>
<li>Experiment Formulation: Each task specifies the (a) Experiment Question (e.g., optimizing performance, identifying relationships); (b) Practical constraints (e.g., resource budgets); (c) High-level Setup Requirements -Contextual details such as datasets, and experimental environments.This framing ensures that tasks are open-ended, requiring iterative exploration rather than one-shot solutions.3. Ground Truth: This is defined in two key areas: (a) Experimental Design: Does the agent correctly formulate the experiment, identifying relevant variables and methodologies?(b) Result Analysis: Does the agent correctly interpret findings, and justify its conclusions?We outline the expected outcomes or acceptable solution ranges.</li>
</ol>
<p>Experimental Context</p>
<p>Experimental Complexity</p>
<p>Experimental research varies in complexity across different dimensions.Our benchmark reflects this by structuring tasks into a hierarchical framework, assessing an agent's ability to handle increasingly sophisticated experimentation tasks.Unlike standard benchmarks that classify tasks by a single difficulty metric (e.g., easy, medium, hard), ours structures complexity along experiment-driven dimensions (detailed definitions in App.A):</p>
<p>1).Design Complexity: The complexity of structuring an experiment (e.g., requiring hypothesis refinement), including defining the scope of exploration, selecting key variables, and structuring parameter spaces-ranging from discrete to continuous and from sparse to dense configurations.</p>
<p>2). Experiment Setup Complexity:</p>
<p>The difficulty of initializing and configuring the experimental environment, from simple predefined setups to intricate dependencies requiring multi-step configuration.</p>
<p>3).Relationship Complexity: The interactions between variables and outcomes, from simple linear dependencies to complex non-monotonic relationships.</p>
<p>4). Experiment Goal Complexity:</p>
<p>The number of compet-</p>
<p>Evaluation</p>
<p>We evaluate Curie using our experimentation benchmark, which consists of 46 research tasks spanning varying complexity levels across four key domains ( §4).To enhance statistical robustness, each task is executed independently for five trials for each of our baselines (below) and Curie, and we report the average performance across these trials.Apart from our main results described in §5.1, our evaluation includes our case studies (Fig. 2 and App.B), and additional results (App.C).</p>
<p>Baselines.We compare Curie with two state-of-theart AI agents as our baselines: OpenHands (Wang et al., 2024c), a top-performing coding agent, and Microsoft Magentic (Fourney et al., 2024), a generalist multi-agent system.These baselines were selected because our benchmark primarily focuses on coding-related tasks within computer science, where both models demonstrate strong performance, with the expectation that Magentic, as a generalist multiagent system, may be able to generalize to experimental tasks too.To ensure fairness, each baseline is provided with a detailed system prompt instructing them to act as a professional experimenter (see App. E.1).All baselines and Curie utilize GPT-4o as the underlying LLM.</p>
<p>Performance Metrics.We assess performance using four key metrics, each evaluated as a binary score per task, ensuring rigor at every stage of the experimentation process:</p>
<ol>
<li>
<p>Experiment Design -Ability to structure the high-level experiment plan to address the research question.</p>
</li>
<li>
<p>Execution Setup -Ensuring that the generated code (experiment setup) is executable and produces consistent results across multiple runs.</p>
</li>
<li>
<p>Implementation Alignment -Faithfulness of the experimental setup with the proposed plan.</p>
</li>
</ol>
<p>Conclusion Correctness -Accuracy in reflecting the</p>
<p>ground truth answer to the experimental question.</p>
<p>Evaluator.We employ an LLM judge (Zheng et al., 2023) for straightforward verification such as checking design, setup and conclusion, where the ground truth is provided.However, we manually assess the implementation alignment, as detecting semantic discrepancies between the intended methodology and code is non-trivial.To ensure accuracy, we also verify the LLM judge's assessments by cross-checking a subset of its evaluations against expert annotations, measuring agreement rates, and refining the judge system prompt.Details of the evaluation prompts are provided in App.E.2.This hybrid evaluation approach enables reliable and scalable assessment of experimentation performance.</p>
<p>Benchmark Performance</p>
<p>Table 2 shows aggregated success rates across all performance metrics and benchmark task domains.</p>
<p>Performance Breakdown By Metric.Across all four metrics, Curie consistently outperforms the baselines, demonstrating the benefits of our Experimental Rigor Engine in improving experimentation performance.(i) For experiment design correctness, all frameworks perform well since the current tasks are relatively straightforward and do not require iterative refinement.However, for more complex research tasks, Curie holds an advantage by dynamically refining hypotheses based on intermediate observations, whereas baselines rely on static planning.Our experimental knowledge module further enhances performance by improving recall and adaptation.(ii) For execution setup and implementation alignment, Curie demonstrates higher reliability, as Intra-ARM proactively validates and corrects execution steps, while Inter-ARM guarantees that we follow methodical task transitions.This results in particularly strong execution setup performance, from 66.7% to 92.7%.Open-Hands (with 32.4% and 40.2%), as a coding-specialized agent, outperforms Magentic in this aspect.However, it still struggles with incomplete or erroneous setups, including getting stuck in loops, syntax errors, logic mistakes, and unresolved dependencies-leading to execution failures in Curie outperforms the others consistently, with performance generally dropping as complexity increases.</p>
<p>complex environments.Magentic, in particular, performs poorly in locating the correct files in the task starter file and handling script input/output.(iii) Finally, for conclusion correctness, its accuracy is largely constrained by earlier errors, as conclusions rely on the correctness of experimental results.However, Curie maintains a strong lead due to its Experiment Knowledge Module, which systematically documents experimental results for structured data analysis.This enables Curie to achieve a significantly higher conclusion score of 36.1%, compared to 10.5% for OpenHands and 2.3% for Magentic.While Magentic demonstrates relatively decent alignment, it struggles to translate this into meaningful conclusions because of previous cascading errors.</p>
<p>Performance Breakdown By Domain.Across all four task domains, Curie consistently outperforms the baselines, demonstrating Curie's ability to adapt to different research domains.(i) First, for LLM reasoning tasks, Curie performed exceptionally well, achieving the highest conclusion accuracy at 44.9%.OpenHands had its best performance in this category (14.2%), while Magentic attained its only non-zero score of 6.7%.We attribute this to the inherent intuitiveness of conclusions for our tasks in this domain.(ii) For Vector DB tasks, both OpenHands and Magentic achieved their highest alignment scores-52.3%and 63.6%, respectively-likely due to the familiarity of the task.Alignment was also easier given the availability of well-established open-source benchmarks and shorter execution runs, which provided faster feedback.(iii) For Cloud Computing tasks, Curie outperformed OpenHands significantly in all aspects (e.g., 6.5× the conclusion accuracy).This is because these tasks often involve long-running experiments, which requires robust execution tracking and dynamical experimentation workflows adjustment based on partial results.(iv) Finally, for ML Training tasks, all agents underperformed in alignment and execution as the detailed environment setup instructions are not provided for these tasks.Despite this, Curie can figure out the correct setup by reflection and refinement, achieving a 7.3× higher conclusion accuracy than OpenHands.</p>
<p>Performance Breakdown by Complexity.Next, we analyze how each framework performs as we increase difficulty within each complexity dimension.Fig. 8 reports the aggregated performance score, computed as the average across all four evaluation metrics.We observe that increasing complexity difficulties across all dimensions correlates with a decline in performance across all agents.However, the rate of degradation varies across complexity types and agent architectures.Notably, Magentic consistently underperforms across all complexity levels, highlighting the robustness of our complexity-based difficulty scaling in distinguishing agent capabilities.Further, we observe a sublinear decline in performance as task complexity increases, suggesting that our hardest tasks could be made even more challenging.Despite this, our current results demonstrate Curie's capabilities, supported by our case studies.Exploring the limit of experimentation difficulty and its impact on model performance remains an open direction for future work.</p>
<p>In summary, our findings underscore the importance of rigorous evaluation across all stages of the experimentation process, shedding light on each framework's strengths and limitations under varying complexity conditions.</p>
<p>Conclusion and Future Work</p>
<p>We introduced Curie, an AI agent framework designed to automate and enhance the rigor of scientific experimentation.Central to its design is the Experimental Rigor Engine, which enforces methodical control, reliability, and interpretability.To assess Curie's effectiveness, we developed a new Experimentation Benchmark featuring real-world research-level challenges.Our empirical evaluation, comparing Curie against state-of-the-art AI agents, demonstrated its capability to automate rigorous experimentation.</p>
<p>We hope Curie inspires further advancements toward fully autonomous and rigorous experimentation in the era of AI agent-driven scientific research.Several open research challenges remain: For instance, adapting Curie for interdisciplinary research requires accommodating domainspecific methodologies, uncertainty control, and extended time scales, such as long-term biological studies (Hilty et al., 2021).Moreover, enabling knowledge reuse (Wang et al., 2024d) across experiments could enhance efficiency and further accelerate discovery.</p>
<p>Impact Statement</p>
<p>We introduce Curie, an AI agent framework designed to ensure methodical control, execution reliability, and structured knowledge management throughout the experimentation lifecycle.We introduce a novel experimentation benchmark, spanning four key domains in computer science, to evaluate the reliability and effectiveness of AI agents in conducting scientific research.Our empirical results demonstrate that Curie achieves higher conclusion accuracy and execution reliability, significantly outperforming state-ofthe-art AI agents.</p>
<p>Curie has broad implications across multiple scientific disciplines, including machine learning, cloud computing, and database systems, where rigorous experimentation is essential.Beyond computer science, our framework has the potential to accelerate research in materials science, physics, and biomedical research, where complex experimental setups and iterative hypothesis testing are critical for discovery.By automating experimental workflows with built-in validation, Curie can enhance research productivity, reduce human error, and facilitate large-scale scientific exploration.</p>
<p>Ensuring transparency, fairness, and reproducibility in AIdriven scientific research is paramount.Curie explicitly enforces structured documentation and interpretability, making experimental processes auditable and traceable.However, over-reliance on AI for scientific discovery raises concerns regarding bias in automated decision-making and the need for human oversight.We advocate for hybrid human-AI collaboration, where AI assists researchers rather than replacing critical scientific judgment.</p>
<p>Curie lays the foundation for trustworthy AI-driven scientific experimentation, opening avenues for self-improving agents that refine methodologies through continual learning.Future research could explore domain-specific adaptations, enabling AI to automate rigorous experimentation in disciplines such as drug discovery, materials engineering, and high-energy physics.By bridging AI and the scientific method, Curie has the potential to shape the next generation of AI-powered research methodologies, driving scientific discovery at an unprecedented scale.In Fig. 9b, the objective of this experiment is to examine the relationship between task complexity and the optimal length of reasoning chains in large language models (LLMs).The experiment maintains constant variables, including the model (gpt-4o-mini), the method (auto cot), and the environment setup (OpenAI credentials and a Conda environment).The independent variable is the number of reasoning steps, controlled through different demo files, while the dependent variable is the model's accuracy, as reported in the log files.The experiment consists of a control group and experimental groups.The control group uses the gsm8k 1 demo file with a single reasoning step to establish a baseline accuracy.The experimental groups involve testing gsm8k with reasoning steps from gsm8k 2 and gsm8k 3, and last letters with reasoning steps ranging from last letters 1 to last letters 10.The results will help determine whether task complexity influences the optimal number of reasoning steps required for maximizing accuracy in LLMs.</p>
<p>Curie extends the scope by analyzing how task complexity relates to the optimal length of reasoning chains.This study differentiates between problem types (e.g., logical inference and mathematical operations) and systematically evaluates the effect of reasoning step count within different datasets (gsm8k and last letters).By introducing controlled experimental conditions, Curie enables a more detailed exploration of how task complexity interacts with reasoning steps to optimize model performance.</p>
<p>C. Extended Evaluation: Fine-grained Performance Breakdown by Individual Metrics</p>
<p>We detail fine-grained breakdowns for each of our performance metrics mentioned in §5.Here we observe the general trend that increasing complexity across all dimensions causes reductions in average metric scores, as shown in Fig. 10, Fig. 11 and Fig. 12, respectively.In particular, we observe that conclusion scores are most heavily affected as complexity increases across dimensions, reaching 0% on many occasions for Magentic in particular.For design complexity on the other hand, we observe that we're able to maintain a relatively high average score across all baselines and Curie, but this tapers down as the difficulty increases across dimensions.-Did you identify a clear, correct hypothesis?-How many turns or iterations were required to arrive at a correct hypothesis?</p>
<ol>
<li>Experimental Setup:</li>
</ol>
<p>-Is the experimental setup reproducible, usable, and interpretable?-Does it meet the rigor required by the scientific method?</p>
<ol>
<li>Results Generation:</li>
</ol>
<p>-Are the results actually produced through experimentation?-Are the results accurate and sufficient to justify your conclusions?</p>
<p>Conclusion Derivation:</p>
<p>-Are the conclusions correct and logically derived from the results?-Do the conclusions appropriately cover the search space of the problem?</p>
<p>Workflow Design:</p>
<p>-Is the experimental workflow cohesive and callable as a single program?-Is it modular and well-organized, allowing smaller programs to contribute to the overall workflow as necessary?</p>
<p>Expectations for Your Behavior:</p>
<p>-Think like a scientist.Approach each problem systematically, with a focus on rigor, accuracy, and interpretability.</p>
<p>-Produce experiments and results that can be scrutinized, reproduced, and used by others.</p>
<p>-Justify your steps and decisions clearly, and ensure your results align with the problem's requirements.</p>
<p>-Your success depends on delivering usable, rigorous, and interpretable experimental workflows that solve the given questions effectively.</p>
<p>-</p>
<p>Figure 1 .
1
Figure 1.Curie overview.</p>
<p>Figure 2 .
2
Figure 2. Case Study.Curie can help researchers validate, expand, and critique existing research on the benefits of repeated sampling in LLM reasoning (Brown et al., 2024).The first panel (Original Finding) presents a result from the original paper.The second panel (Reproduce) has Curie confirming this finding through rigorous experimentation.The third panel (Extend) has Curie exploring the impact of sampling temperature on repeated sampling.The final panel (Challenge) shows Curie identifying a limitation in the original methodology, suggesting an avenue for future research.</p>
<p>Figure 3 .
3
Figure3.Curie workflow with an example task in LLM reasoning.The Architect is responsible for designing high-level plans and reflects on the new findings.The Technician is responsible for implementing and executing the experiments based on the plans.Whenever an agent completes its action (step 1 , 2 , 3 , 4 , 5 ), the Experimental Rigor Engine (steps A ⇀ B ⇀ C ) validates the action, determines next steps, assigns tasks and maintains interpretable experimental progress, ensuring rigor throughout the entire process.</p>
<p>Figure 4 .
4
Figure 4. Intra-ARM setup validation high-level workflow.</p>
<p>(a) Example errors that can be captured by the setup validator.(b) Example errors that can be captured by the execution validator.</p>
<p>Figure 5 .
5
Figure 5. Errors detected by two of Intra-ARM 's many validators.</p>
<p>Figure 6 .
6
Figure 6.Simplified Inter-ARM workflow with a partition state snapshot.Partition, control flow, and scheduling policies are customizable.</p>
<p>Figure 7 .
7
Figure 7. Simplified partial snapshot of an example Time Machine.</p>
<p>:</p>
<p>To ensure agents correctly interpret and execute tasks, the benchmark provides detailed context for each question.This includes: (a) Domain Knowl-edge -Background information essential for interpreting the problem.(b) Starter Code &amp; Tools -Predefined scaffolding to simulate real-world research workflows.</p>
<p>Figure 9 .
9
Figure 9. Case studies on LLM reasoning tasks.</p>
<p>Figure 12 .
12
Figure12.Average design scores across different complexity dimensions at varying difficulty levels for Curie, OpenHands, and Magentic.Curie outperforms the others consistently, with performance generally dropping as complexity increases.</p>
<p>Table 1 .
1
Experimentation benchmark overview.
DomainComplexity Dist. Easy Med. HardDescriptionSourcesLLM Reasoning457</p>
<p>Table 2 .
2
Main benchmark results in terms of four metrics introduced in §5.We aggregate and average the success rate among all tasks within each domain.The final row presents the weighted average, computed based on the number of tasks in each domain.One Des.Exec.Align.Con.Des.Exec.Align.Con.Des.Exec.Align.Con.
Curie Microsoft Magentic-LLM Reason. OpenHands 98.3 83.3 76.7 44.9 86.7 24.6 36.7 14.2 72.0 9.3 146.7Vector DB97.871.777.225.6 85.0 48.352.311.7 85.06.463.60.0Cloud Comp. 100.0 92.796.932.3 96.9 25.249.25.095.06.333.80.0ML Training95.266.739.341.7 63.1 24.316.75.790.02.925.70.0Weighted Avg. 97.978.173.436.1 83.6 32.440.210.5 82.96.835.22.3ing objectives and trade-offs involved, from single-metricoptimization to multi-objective balancing under constraints.</p>
<p>Average scores across different complexity dimensions at varying difficulty levels for Curie, OpenHands, and Magentic.
90Average score (%)25 50 75CurieOpenHandsMagentic0Easy Medium HardEasy Medium HardEasy Medium HardEasy Medium HardEasy Medium HardFigure 8.</p>
<p>Average alignment scores across different complexity dimensions at varying difficulty levels for Curie, OpenHands, and Magentic.Curie outperforms the others consistently, with performance generally dropping as complexity increases.Average conclusion scores across different complexity dimensions at varying difficulty levels for Curie, OpenHands, and Magentic.Curie outperforms the others consistently, with performance generally dropping as complexity increases.
100Average score (%)25 50 75CurieOpenHandsMagentic0Easy Medium HardEasy Medium HardEasy Medium HardEasy Medium HardEasy Medium Hard0 100 Figure 10. Easy Medium Hard 25 50 75 Average score (%)Easy Medium Hard CurieEasy Medium Hard OpenHandsEasy Medium Hard MagenticEasy Medium Hard0 100 Figure 11. Easy Medium Hard 25 50 75 Average score (%)Easy Medium Hard CurieEasy Medium Hard OpenHandsEasy Medium Hard MagenticEasy Medium Hard</p>
<p>Make sure you provide a reproducible experimental workflow (i.e., verify that it is runnable multiple times to produce acceptable results) that can be callable through a single program; name it experimental_workflow.shReminder:Yourrole is to conduct actual experiments and generate real results, no simulations, placeholders, or unverified assumptions are allowed.E.2.LLM Judge System Prompt[System Prompt] You are an strict Experimentation Agent Verifier, responsible for evaluating whether an experimentation agent correctly conducted an experiment based on the experimentation question.You are provided with an experiment log chunk, the original experimentation question, and the ground truth (only contains the conclusion).Your assessment should focus on: 1. Experiment Design -Did the agent structure the correct high-level plan to address the experimentation question?It does not need to write implementation code or execute the plan.2.Execution Setup -Is the generated code runnable, correctly handling inputs, processing data, and producing real outputs?Is the whole experimental workflow generated for reproducibility?3.Implementation Alignment-Is the code properly aligned with the experimentation design and accurately implementing the intended methodology?Ensure: Legitimate handling of inputs and outputs.No hardcoded or mock data.4.Conclusion Correctness -Is the conclusion acceptable by the ground truth?Analyze the provided chunked Log File, and provide a structured evaluation based on the criteria below: Analyze this log chunk and provide your evaluation in the specified JSON format.
Response Format<em> Overall Verdict: Correct / Incorrect</em> Detailed Assessment:<em> Experiment Design: [Pass/Fail]</em> Execution Setup: [Pass/Fail]<em> Implementation Alignment : [Pass/Fail]</em> Conclusion Correctness: [Pass/Fail]* Explanation: [Concisely explanation about the failure reasons, no reasonneeded if the step is missing]"""user_prompt = f"""&gt; Original Experimentation Question:{question}&gt; Ground Truth:{ground_truth}&gt; Log Chunk:{log_chunk}
A. Curie Benchmark Complexity ExplanationWe describe in detail our complexity level definitions in Table.3.B. Case Studies for CurieWe provide two example case studies for LLM reasoning tasks that Curie was able to extend from the paper The Impact of Reasoning Step Length on Large Language Models(Jin et al., 2024).In Fig.9a, the objective of this experiment is to examine whether different models exhibit varying accuracy levels based on the number of reasoning steps.The experiment maintains constant variables, including the dataset (last letters), the method (auto cot), and the evaluation metric (accuracy).The independent variables include the model type (gpt-4o-mini vs. gpt-4o) and the number of reasoning steps(1,2,3,4,5,6,10), while the dependent variable is the model's accuracy.The experiment consists of a control group and experimental groups.The control group uses gpt-4o-mini with a single reasoning step to establish a baseline accuracy.The experimental groups involve testing gpt-4o-mini with reasoning steps ranging from 2 to 10 and gpt-4o with reasoning steps from 1 to 10.The results will help determine whether reasoning step variations impact accuracy differently across models.Curie extends the original investigation by examining whether different LLMs exhibit varying accuracy using GPT-4o and GPT-4o-mini.What is the best AWS EC2 instance type within the c5 family (instances listed below) for running an e-commerce web application serving 500 concurrent requests to its add to cart function?Do not terminate until you identify the best instance type concretely.Easy Medium Easy Medium MediumWhat is the best AWS EC2 instance type within the c5 family (instances listed below) for running an e-commerce web application serving 500 concurrent requests to its add to cart function, aiming to minimise cost while maintaining a 99th percentile latency below 150ms?Do not terminate until you identify the best instance type concretely.Easy Easy Medium Hard MediumWhat is the best AWS EC2 instance type within the c5 family (instances listed below) for running an e-commerce web application serving 500 concurrent requests to its add to cart function, aiming to minimise cost while maintaining a 99th percentile latency below 150ms?Do not terminate until you identify the best instance type concretely.Easy Medium Medium Medium MediumWhat is the best AWS EC2 instance type within the c5 and t3 families (instances listed below) for running an e-commerce web application serving 500 concurrent requests to its add to cart function, aiming to minimise cost while maintaining a 99th percentile latency below 150ms?Do not terminate until you identify the best instance type concretely.
S Agarwal, I H Laradji, L Charlin, C Pal, Litllm, arXiv:2402.01788A toolkit for scientific literature review. 2024arXiv preprinta new claude 3.5 sonnet, and claude 3.5 haiku. 2024.</p>
<p>Using context to build rigor: Application to two hermeneutic phenomenological studies. M Armour, S L Rivaux, H Bell, 10.1177/1473325008100424Qualitative Social Work. 1473-325081Mar 2009</p>
<p>Conversation patterns. Autogen , 2024</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, Q V Do, Y Xu, P Fung, 2023</p>
<p>Ai for science: An emerging agenda. P Berens, K Cranmer, N D Lawrence, U Von Luxburg, J Montgomery, 2023</p>
<p>Knowledge graph extraction from total synthesis documents. A M Bran, Z Jončev, P Schwaller, Proceedings of the 1st Workshop on Language+ Molecules (L+ M 2024). the 1st Workshop on Language+ Molecules (L+ M 2024)2024</p>
<p>Large language monkeys: Scaling inference compute with repeated sampling. B Brown, J Juravsky, R Ehrlich, R Clark, Q V Le, C Ré, A Mirhoseini, arXiv:2407.217872024arXiv preprint</p>
<p>Z Chen, S Chen, Y Ning, Q Zhang, B Wang, B Yu, Y Li, Z Liao, C Wei, Z Lu, arXiv:2410.05080Toward rigorous assessment of language agents for data-driven scientific discovery. 2024arXiv preprint</p>
<p>Language models as science tutors. A Chevalier, J Geng, A Wettig, H Chen, S Mizera, T Annala, M J Aragon, A R Fanlo, S Frieder, S Machado, A Prabhakar, E Thieu, J T Wang, Z Wang, X Wu, M Xia, W Xia, J Yu, J.-J Zhu, Z J Ren, S Arora, D Chen, 2024</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, 2021</p>
<p>The faiss library. M Douze, A Guzhva, C Deng, J Johnson, G Szilvasy, P.-E Mazaré, M Lomeli, L Hosseini, H Jégou, 2024</p>
<p>Magentic-one: A generalist multi-agent system for solving complex tasks. A Fourney, G Bansal, H Mozannar, C Tan, E Salinas, F Niedtner, G Proebsting, G Bassman, J Gerrits, J Alber, arXiv:2411.044682024arXiv preprint</p>
<p>Mathematical capabilities of chatgpt. S Frieder, L Pinchetti, R.-R Griffiths, T Salvatori, T Lukasiewicz, P Petersen, J Berner, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Automating scientific discovery through multi-agent intelligent graph reasoning. A Ghafarollahi, M J Buehler, Sciagents, 2024</p>
<p>Informing Science: The International Journal of an Emerging Transdiscipline. T Gill, T Gill, 10.28945/4528232020What is research rigor? lessons for a transdiscipline</p>
<p>Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models. X Gu, M Krenn, arXiv:2405.170442024arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, 2021a</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, 2021b</p>
<p>Plant growth: the what, the how, and the why. J Hilty, B Muller, F Pantin, S Leuzinger, 10.1111/nph.17610New Phytologist. 23212021</p>
<p>Getting rigorous with scientific rigor. L J Hofseth, Carcinogenesis. 391January 2018</p>
<p>Data interpreter: An llm agent for data science. S Hong, Y Lin, B Liu, B Liu, B Wu, C Zhang, C Wei, D Li, J Chen, J Zhang, arXiv:2402.186792024aarXiv preprint</p>
<p>Meta programming for a multi-agent collaborative framework. S Hong, M Zhuge, J Chen, X Zheng, Y Cheng, J Wang, C Zhang, Z Wang, S K S Yau, Z Lin, L Zhou, C Ran, L Xiao, C Wu, J Schmidhuber, Metagpt, The Twelfth International Conference on Learning Representations. 2024b</p>
<p>The hallmark effect: Supporting provenance and transparent use of large language models in writing with interactive visualization. M N Hoque, T Mashiat, B Ghai, C D Shelton, F Chevalier, K Kraus, N Elmqvist, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Q Huang, J Vora, P Liang, J Leskovec, 2024</p>
<p>C E Jimenez, J Yang, A Wettig, S Yao, K Pei, O Press, K Narasimhan, arXiv:2310.06770Swe-bench: Can language models resolve real-world github issues?. 2023arXiv preprint</p>
<p>Nobel turing challenge: creating the engine for scientific discovery. M Jin, Q Yu, D Shu, H Zhao, W Hua, Y Meng, Y Zhang, M Du, 10.1038/s41540-021-00189-3arXiv:2401.04925Systems Biology and Applications. 2056-718971292024. Jun 2021arXiv preprintThe impact of reasoning step length on large language models</p>
<p>Automated scientific discovery: From equation discovery to autonomous discovery systems. S Kramer, M Cerrato, S Džeroski, R King, 2023</p>
<p>Let's verify step by step. H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, arXiv:2305.200502023arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Large language models as biomedical hypothesis generators: A comprehensive evaluation. B Qi, K Zhang, K Tian, H Li, Z.-R Chen, S Zeng, E Hua, H Jinfang, B Zhou, 2024</p>
<p>S Schmidgall, Y Su, Z Wang, X Sun, J Wu, X Yu, J Liu, Z Liu, E Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025arXiv preprint</p>
<p>Accelerating science with humanaware artificial intelligence. J Sourati, J A Evans, Nature human behaviour. 7102023</p>
<p>A multi-level large language model evaluation benchmark for scientific research. L Sun, Y Han, Z Zhao, D Ma, Z Shen, B Chen, L Chen, K Yu, Scieval, 10.1609/aaai.v38i17.29872Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 2024a38</p>
<p>W Sun, L Yan, X Ma, S Wang, P Ren, Z Chen, D Yin, Z Ren, Is chatgpt good at search? investigating large language models as re-ranking agents. 2024b</p>
<p>Ai-driven review systems: Evaluating llms in scalable and bias-aware academic reviews. K Tyser, B Segev, G Longhitano, X.-Y Zhang, Z Meeks, J Lee, U Garg, N Belsten, A Shporer, M Udell, D Te'eni, I Drori, 2024</p>
<p>R Wang, E Zelikman, G Poesia, Y Pu, N Haber, N D Goodman, Hypothesis search: Inductive reasoning with language models. 2024a</p>
<p>Evaluating college-level scientific problem-solving abilities of large language models. X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, Scibench, 2024b</p>
<p>X Wang, B Li, Y Song, F F Xu, X Tang, M Zhuge, J Pan, Y Song, B Li, J Singh, arXiv:2407.16741An open platform for ai software developers as generalist agents. 2024carXiv preprint</p>
<p>Agent workflow memory. Z Z Wang, J Mao, D Fried, G Neubig, 2024d</p>
<p>R Xu, Z Qi, Z Guo, C Wang, H Wang, Y Zhang, W Xu, arXiv:2403.08319Knowledge conflicts for llms: A survey. 2024arXiv preprint</p>
<p>J Yang, C E Jimenez, A Wettig, K Lieret, S Yao, K Narasimhan, O Press, arXiv:2405.15793Swe-agent: Agent-computer interfaces enable automated software engineering. 2024arXiv preprint</p>
<p>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. J Yuan, X Yan, B Shi, T Chen, W Ouyang, B Zhang, L Bai, Y Qiao, B Zhou, 2025</p>
<p>Unleashing the power of large language models in solving machine learning tasks. L Zhang, Y Zhang, K Ren, D Li, Y Yang, Mlcopilot, 2024a</p>
<p>Automl-gpt: Automatic machine learning with gpt. S Zhang, C Gong, L Wu, X Liu, M Zhou, 2023</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Y Zhang, X Chen, B Jin, S Wang, S Ji, W Wang, J Han, arXiv:2406.108332024barXiv preprint</p>
<p>Judging llm-as-ajudge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, Advances in Neural Information Processing Systems. 202336</p>
<p>Hypothesis generation with large language models. Y Zhou, H Liu, T Srivastava, H Mei, C Tan, 10.18653/v1/2024.nlp4science-1.10Proceedings of the 1st Workshop on NLP for Science (NLP4Science). the 1st Workshop on NLP for Science (NLP4Science)Association for Computational Linguistics2024</p>            </div>
        </div>

    </div>
</body>
</html>
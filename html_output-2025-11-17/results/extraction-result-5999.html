<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5999 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5999</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5999</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-267627936</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.06853v3.pdf" target="_blank">History, development, and principles of large language models: an introductory survey</a></p>
                <p><strong>Paper Abstract:</strong> Language models serve as a cornerstone in natural language processing, utilizing mathematical methods to generalize language laws and knowledge for prediction and generation. Over extensive research spanning decades, language modeling has progressed from initial statistical language models to the contemporary landscape of large language models (LLMs). Notably, the swift evolution of LLMs has reached the ability to process, understand, and generate human-level text. Nevertheless, despite the significant advantages that LLMs offer in improving both work and personal lives, the limited understanding among general practitioners about the background and principles of these models hampers their full potential. Notably, most LLM reviews focus on specific aspects and utilize specialized language, posing a challenge for practitioners lacking relevant background knowledge. In light of this, this survey aims to present a comprehensible overview of LLMs to assist a broader audience. It strives to facilitate a comprehensive understanding by exploring the historical background of language models and tracing their evolution over time. The survey further investigates the factors influencing the development of LLMs, emphasizing key contributions. Additionally, it concentrates on elucidating the underlying principles of LLMs, equipping audiences with essential theoretical knowledge. The survey also highlights the limitations of existing work and points out promising future directions.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5999.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5999.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-targeted large language model referenced in the survey that was developed to represent and generate scientific knowledge; cited in the paper's reference list as a model for science.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM explicitly positioned for scientific content and knowledge (referenced in the survey); intended to ingest scientific text and produce knowledge-grounded outputs for science-related queries.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Galactica (the model itself) — specific internal model variants or sizes are not described in this survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Not specified in this survey; the paper only references the model title and does not provide details on number/collection of scientific papers ingested.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Not specified in this survey (the survey only cites the model; no method details such as summarization, KG construction, or chain-of-thought synthesis are provided).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this survey (presumed: scientific summaries/answers, but the survey does not detail outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Not specified in this survey (only the model is listed in references; no quantitative/qualitative results are reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not specified in this survey (the survey does not report Galactica's failure modes or limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Not specified in this survey (no direct comparisons provided in the survey text).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'History, development, and principles of large language models: an introductory survey', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5999.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5999.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebGPT: Browser-assisted question-answering with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based system (referenced) that augments language models with browser access and human feedback to improve factual question answering and grounding in sources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>WebGPT: Browser-assisted question-answering with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A browser-augmented question-answering system that combines retrieval from live web sources with human feedback to produce better grounded answers (as indicated by the referenced paper title and listing in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this survey (original WebGPT work used OpenAI models, but the survey text does not state the exact LLM versions).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Not specified in this survey; implied inputs are natural-language questions and web pages retrieved by the system, but the survey provides no counts or corpus details.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Retrieval-augmented generation with human feedback (inferred from the paper title quoted in the survey: 'browser-assisted question-answering with human feedback'); the survey itself does not describe steps or pipelines in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Answers to user questions grounded in retrieved web sources (the survey mentions WebGPT only by title/reference and does not provide concrete output examples).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not specified in this survey (no evaluation metrics or procedures for WebGPT are detailed in the survey text).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Not specified in this survey (the survey lists the reference but reports no quantitative WebGPT outcomes).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not specified in this survey (the survey does not enumerate WebGPT limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'History, development, and principles of large language models: an introductory survey', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5999.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5999.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeLilegal</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeLilegal (website/tool referenced in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A legal-domain system mentioned in the survey that integrates external law databases to enhance retrieval and reduce hallucination in legal LLM responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeLilegal</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A tool/platform (referenced via a web link in the survey) described as integrating external legal databases to improve retrieval and reduce hallucination for legal queries.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this survey (the survey only notes the system by name/link; no underlying LLM or version is given).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Not specified in this survey (presumably legal queries and legal-text corpora, but the survey does not provide specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Integration of external knowledge bases / retrieval augmentation to ground answers and reduce hallucination (survey explicitly states it integrates external law databases to enhance retrieval and minimize hallucination).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this survey (implied: improved legal answers/drafts/retrieval, but the survey does not define exact outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Not specified in this survey (no quantitative or qualitative outcomes are provided).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not specified in this survey beyond the general legal-domain concerns (the survey mentions hallucination as a domain-wide issue that such systems attempt to mitigate).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Not specified in this survey (only discussed as an advancement addressing hallucination via external DB integration).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'History, development, and principles of large language models: an introductory survey', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5999.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5999.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatLaw</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatLaw: Open-source legal large language model with integrated external knowledge bases</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source legal LLM referenced in the survey that integrates external knowledge bases to support legal tasks and reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatLaw: Open-source legal large language model with integrated external knowledge bases</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatLaw</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An open-source legal-domain LLM that incorporates external knowledge bases for improved retrieval and grounding of legal answers (cited in the survey's references).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this survey (the survey references the ChatLaw paper but does not enumerate the model sizes or base LLM variants).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Not specified in this survey (details about the number of legal documents or queries processed are not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Retrieval-augmented generation via integration with external legal knowledge bases (survey references the design but does not detail the distillation pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this survey (presumed legal answers and drafting assistance, but the survey does not provide explicit output descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not specified in this survey (no evaluation methodology or metrics for ChatLaw are described here).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Not specified in this survey (the survey only lists the reference).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not specified in this survey beyond general concerns about hallucination and timeliness of legal research noted in the legal section.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Not specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'History, development, and principles of large language models: an introductory survey', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5999.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5999.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Med-PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Med-PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A medical-specialized large language model referenced in the survey that attains near-expert levels on biomedical question-answering benchmarks (reported close to human expert proficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards expert-level medical question answering with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Med-PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A medical-domain LLM (cited) developed for medical question answering and alignment, reported in the survey to achieve proficiency close to human experts on medical benchmarks in a short time frame.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Med-PaLM2 (the survey references this medical LLM; specific backbone/version details are not provided in the survey text).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Evaluated on MedQA (United States Medical Licensing Examination-style questions); the survey cites MedQA as the benchmark dataset used in reporting performance.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Not fully specified in this survey; the survey reports that medical models like Med-PaLM2 are built and aligned for medical QA (likely involving fine-tuning and alignment), but detailed distillation pipeline is not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Answers to medical exam–style questions / medical QA; survey reports pass-level performance on MedQA-like tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluation on MedQA (USMLE-style question answering) and comparisons to human passing thresholds; the survey notes human-passing levels and proximity to expert performance.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Survey states that ChatGPT attains human passing levels on MedQA and that Med-PaLM2 achieves proficiency levels close to human experts in under six months; no numeric metrics beyond these qualitative statements are provided in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>MedQA (USMLE-style questions) is explicitly mentioned in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Survey highlights general LLM issues relevant to medical models: hallucination risk, need for careful alignment and high-quality human feedback (RLHF), and the high cost/time of obtaining professional annotator data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Survey compares performance qualitatively to ChatGPT and GPT-4 (noting ChatGPT's human-passing performance and Med-PaLM2's near-expert proficiency), but does not provide granular comparative metrics in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'History, development, and principles of large language models: an introductory survey', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Galactica: A large language model for science <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 2)</em></li>
                <li>Towards expert-level medical question answering with large language models <em>(Rating: 2)</em></li>
                <li>ChatLaw: Open-source legal large language model with integrated external knowledge bases <em>(Rating: 2)</em></li>
                <li>Pythia: A suite for analyzing large language models across training and scaling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5999",
    "paper_id": "paper-267627936",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [
        {
            "name_short": "Galactica",
            "name_full": "Galactica: A large language model for science",
            "brief_description": "A domain-targeted large language model referenced in the survey that was developed to represent and generate scientific knowledge; cited in the paper's reference list as a model for science.",
            "citation_title": "Galactica: A large language model for science",
            "mention_or_use": "mention",
            "system_name": "Galactica",
            "system_description": "An LLM explicitly positioned for scientific content and knowledge (referenced in the survey); intended to ingest scientific text and produce knowledge-grounded outputs for science-related queries.",
            "llm_model_used": "Galactica (the model itself) — specific internal model variants or sizes are not described in this survey text.",
            "input_type_and_size": "Not specified in this survey; the paper only references the model title and does not provide details on number/collection of scientific papers ingested.",
            "distillation_approach": "Not specified in this survey (the survey only cites the model; no method details such as summarization, KG construction, or chain-of-thought synthesis are provided).",
            "output_type": "Not specified in this survey (presumed: scientific summaries/answers, but the survey does not detail outputs).",
            "evaluation_methods": "Not specified in this survey.",
            "results": "Not specified in this survey (only the model is listed in references; no quantitative/qualitative results are reported here).",
            "datasets_or_benchmarks": "Not specified in this survey.",
            "challenges_or_limitations": "Not specified in this survey (the survey does not report Galactica's failure modes or limitations).",
            "comparisons_to_other_methods": "Not specified in this survey (no direct comparisons provided in the survey text).",
            "uuid": "e5999.0",
            "source_info": {
                "paper_title": "History, development, and principles of large language models: an introductory survey",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "WebGPT",
            "name_full": "WebGPT: Browser-assisted question-answering with human feedback",
            "brief_description": "An LLM-based system (referenced) that augments language models with browser access and human feedback to improve factual question answering and grounding in sources.",
            "citation_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "mention_or_use": "mention",
            "system_name": "WebGPT",
            "system_description": "A browser-augmented question-answering system that combines retrieval from live web sources with human feedback to produce better grounded answers (as indicated by the referenced paper title and listing in the survey).",
            "llm_model_used": "Not specified in this survey (original WebGPT work used OpenAI models, but the survey text does not state the exact LLM versions).",
            "input_type_and_size": "Not specified in this survey; implied inputs are natural-language questions and web pages retrieved by the system, but the survey provides no counts or corpus details.",
            "distillation_approach": "Retrieval-augmented generation with human feedback (inferred from the paper title quoted in the survey: 'browser-assisted question-answering with human feedback'); the survey itself does not describe steps or pipelines in detail.",
            "output_type": "Answers to user questions grounded in retrieved web sources (the survey mentions WebGPT only by title/reference and does not provide concrete output examples).",
            "evaluation_methods": "Not specified in this survey (no evaluation metrics or procedures for WebGPT are detailed in the survey text).",
            "results": "Not specified in this survey (the survey lists the reference but reports no quantitative WebGPT outcomes).",
            "datasets_or_benchmarks": "Not specified in this survey.",
            "challenges_or_limitations": "Not specified in this survey (the survey does not enumerate WebGPT limitations).",
            "comparisons_to_other_methods": "Not specified in this survey.",
            "uuid": "e5999.1",
            "source_info": {
                "paper_title": "History, development, and principles of large language models: an introductory survey",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DeLilegal",
            "name_full": "DeLilegal (website/tool referenced in survey)",
            "brief_description": "A legal-domain system mentioned in the survey that integrates external law databases to enhance retrieval and reduce hallucination in legal LLM responses.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "DeLilegal",
            "system_description": "A tool/platform (referenced via a web link in the survey) described as integrating external legal databases to improve retrieval and reduce hallucination for legal queries.",
            "llm_model_used": "Not specified in this survey (the survey only notes the system by name/link; no underlying LLM or version is given).",
            "input_type_and_size": "Not specified in this survey (presumably legal queries and legal-text corpora, but the survey does not provide specifics).",
            "distillation_approach": "Integration of external knowledge bases / retrieval augmentation to ground answers and reduce hallucination (survey explicitly states it integrates external law databases to enhance retrieval and minimize hallucination).",
            "output_type": "Not specified in this survey (implied: improved legal answers/drafts/retrieval, but the survey does not define exact outputs).",
            "evaluation_methods": "Not specified in this survey.",
            "results": "Not specified in this survey (no quantitative or qualitative outcomes are provided).",
            "datasets_or_benchmarks": "Not specified in this survey.",
            "challenges_or_limitations": "Not specified in this survey beyond the general legal-domain concerns (the survey mentions hallucination as a domain-wide issue that such systems attempt to mitigate).",
            "comparisons_to_other_methods": "Not specified in this survey (only discussed as an advancement addressing hallucination via external DB integration).",
            "uuid": "e5999.2",
            "source_info": {
                "paper_title": "History, development, and principles of large language models: an introductory survey",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ChatLaw",
            "name_full": "ChatLaw: Open-source legal large language model with integrated external knowledge bases",
            "brief_description": "An open-source legal LLM referenced in the survey that integrates external knowledge bases to support legal tasks and reduce hallucination.",
            "citation_title": "ChatLaw: Open-source legal large language model with integrated external knowledge bases",
            "mention_or_use": "mention",
            "system_name": "ChatLaw",
            "system_description": "An open-source legal-domain LLM that incorporates external knowledge bases for improved retrieval and grounding of legal answers (cited in the survey's references).",
            "llm_model_used": "Not specified in this survey (the survey references the ChatLaw paper but does not enumerate the model sizes or base LLM variants).",
            "input_type_and_size": "Not specified in this survey (details about the number of legal documents or queries processed are not provided).",
            "distillation_approach": "Retrieval-augmented generation via integration with external legal knowledge bases (survey references the design but does not detail the distillation pipeline).",
            "output_type": "Not specified in this survey (presumed legal answers and drafting assistance, but the survey does not provide explicit output descriptions).",
            "evaluation_methods": "Not specified in this survey (no evaluation methodology or metrics for ChatLaw are described here).",
            "results": "Not specified in this survey (the survey only lists the reference).",
            "datasets_or_benchmarks": "Not specified in this survey.",
            "challenges_or_limitations": "Not specified in this survey beyond general concerns about hallucination and timeliness of legal research noted in the legal section.",
            "comparisons_to_other_methods": "Not specified in this survey.",
            "uuid": "e5999.3",
            "source_info": {
                "paper_title": "History, development, and principles of large language models: an introductory survey",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Med-PaLM2",
            "name_full": "Med-PaLM2",
            "brief_description": "A medical-specialized large language model referenced in the survey that attains near-expert levels on biomedical question-answering benchmarks (reported close to human expert proficiency).",
            "citation_title": "Towards expert-level medical question answering with large language models",
            "mention_or_use": "mention",
            "system_name": "Med-PaLM2",
            "system_description": "A medical-domain LLM (cited) developed for medical question answering and alignment, reported in the survey to achieve proficiency close to human experts on medical benchmarks in a short time frame.",
            "llm_model_used": "Med-PaLM2 (the survey references this medical LLM; specific backbone/version details are not provided in the survey text).",
            "input_type_and_size": "Evaluated on MedQA (United States Medical Licensing Examination-style questions); the survey cites MedQA as the benchmark dataset used in reporting performance.",
            "distillation_approach": "Not fully specified in this survey; the survey reports that medical models like Med-PaLM2 are built and aligned for medical QA (likely involving fine-tuning and alignment), but detailed distillation pipeline is not provided here.",
            "output_type": "Answers to medical exam–style questions / medical QA; survey reports pass-level performance on MedQA-like tasks.",
            "evaluation_methods": "Evaluation on MedQA (USMLE-style question answering) and comparisons to human passing thresholds; the survey notes human-passing levels and proximity to expert performance.",
            "results": "Survey states that ChatGPT attains human passing levels on MedQA and that Med-PaLM2 achieves proficiency levels close to human experts in under six months; no numeric metrics beyond these qualitative statements are provided in the survey text.",
            "datasets_or_benchmarks": "MedQA (USMLE-style questions) is explicitly mentioned in the survey.",
            "challenges_or_limitations": "Survey highlights general LLM issues relevant to medical models: hallucination risk, need for careful alignment and high-quality human feedback (RLHF), and the high cost/time of obtaining professional annotator data.",
            "comparisons_to_other_methods": "Survey compares performance qualitatively to ChatGPT and GPT-4 (noting ChatGPT's human-passing performance and Med-PaLM2's near-expert proficiency), but does not provide granular comparative metrics in the survey text.",
            "uuid": "e5999.4",
            "source_info": {
                "paper_title": "History, development, and principles of large language models: an introductory survey",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 2,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 2,
            "sanitized_title": "webgpt_browserassisted_questionanswering_with_human_feedback"
        },
        {
            "paper_title": "Towards expert-level medical question answering with large language models",
            "rating": 2,
            "sanitized_title": "towards_expertlevel_medical_question_answering_with_large_language_models"
        },
        {
            "paper_title": "ChatLaw: Open-source legal large language model with integrated external knowledge bases",
            "rating": 2,
            "sanitized_title": "chatlaw_opensource_legal_large_language_model_with_integrated_external_knowledge_bases"
        },
        {
            "paper_title": "Pythia: A suite for analyzing large language models across training and scaling",
            "rating": 1,
            "sanitized_title": "pythia_a_suite_for_analyzing_large_language_models_across_training_and_scaling"
        }
    ],
    "cost": 0.0174935,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>History, Development, and Principles of Large Language Models-An Introductory Survey
23 Sep 2024</p>
<p>Zichong Wang ziwang@fiu.edu 
Florida International University
11200 SW 8th Street33199MiamiFLUSA</p>
<p>Zhibo Chu 
Florida International University
11200 SW 8th Street33199MiamiFLUSA</p>
<p>Thang Viet Doan 
Florida International University
11200 SW 8th Street33199MiamiFLUSA</p>
<p>Shiwen Ni 
Shenzhen Institutes of Advanced Technology
Chinese Academy of Sciences
Shenzhen, GuangdongChina</p>
<p>Min Yang 
Shenzhen Institutes of Advanced Technology
Chinese Academy of Sciences
Shenzhen, GuangdongChina</p>
<p>Wenbin Zhang wenbin.zhang@fiu.edu 
Florida International University
11200 SW 8th Street33199MiamiFLUSA</p>
<p>History, Development, and Principles of Large Language Models-An Introductory Survey
23 Sep 20240BDDF69AFBD1E083A09A970AF4FBA074arXiv:2402.06853v3[cs.CL]Large Language ModelNatural Language ProcessingArtificial IntelligenceLanguage Model
Language models serve as a cornerstone in natural language processing (NLP), utilizing mathematical methods to generalize language laws and knowledge for prediction and generation.Over extensive research spanning decades, language modeling has progressed from initial statistical language models (SLMs) to the contemporary landscape of large language models (LLMs).Notably, the swift evolution of LLMs has reached the ability to process, understand, and generate human-level text.Nevertheless, despite the significant advantages that LLMs offer in improving both work and personal lives, the limited understanding among general practitioners about the background and principles of these models hampers their full potential.Notably, most LLM reviews focus on specific aspects and utilize specialized language, posing a challenge for practitioners lacking relevant background knowledge.In light of this, this survey aims to present a comprehensible overview of LLMs to assist a broader audience.It strives to facilitate a comprehensive understanding by exploring the historical background of language models and tracing their evolution over time.The survey further investigates the factors influencing the development of LLMs, emphasizing key contributions.Additionally, it concentrates on elucidating the underlying principles of LLMs, equipping audiences with essential theoretical knowledge.The survey also highlights the limitations of existing work and points out promising future directions.</p>
<p>Introduction</p>
<p>Language stands as humanity's most potent tool, enabling the expression of thoughts and emotions and facilitating communication with others [1,2].However, machines lack the intrinsic ability to comprehend and communicate in human language, necessitating powerful Artificial Intelligence (AI) algorithms to acquire such capabilities.The ultimate aim of AI research is the creation of machines capable of reading, writing, and conversing like humans [3][4][5].Natural language processing (NLP) emerges as the AI branch dedicated to realizing this goal.At the core of NLP are language models (LMs), tasked with predicting or generating the probability or likelihood of linguistic units (e.g., words, phrases, sentences) based on context.The evolutionary stages of LMs unfold chronologically from the initial statistical language models (SLMs) to subsequent neural language models (NLMs), progressing to pre-trained language models (PLMs), and reaching the current state of large language models (LLMs).Specifically, SLMs employ simple probability distributions to model word sequences, while NLMs utilize neural networks to grasp complex patterns and representations of language.PLMs leverage large-scale corpora and self-supervised learning to capture general linguistic knowledge, and LLMs extend PLMs by incorporating massive data, computation, and algorithms, resulting in more expressive, sophisticated, and adaptable LMs [6,7].</p>
<p>In recent years, significant advancements in NLP have been achieved with LLMs, such as the generative pre-trained transformer (GPT) series of models [8][9][10][11].Extensively trained on textual data, these models demonstrate the ability to generate human-level texts and perform language-based tasks with exceptional precision.Despite these advancements having greatly benefited various aspects of work and life, LLMs might not be widely understood by practitioners, particularly those without a background in NLP.To provide practitioners with a fundamental understanding of LLMs, this review introduces it across six dimensions: history, development, principles, applications, drawbacks and future directions.This survey distinguishes itself from other reviews on PLMs or LLMs [4,[12][13][14], which often concentrate on specific aspects using technical language; this survey aims to elucidate LLMs and their principles in a more accessible manner, intending to maximize its full potential.</p>
<p>To achieve a comprehensive survey of LLMs, we employed a meticulous review process guided by a structured search strategy.We searched databases such as IEEE Xplore, ACM Digital Library, Scopus, and Google Scholar using keywords including "large language models", "large language models survey", and "large language models for X " where X includes software engineering, drug discovery, finance, medical, legal, and education.Our review focused on publications from major conferences and journals about NLP and LLMs' applications, including EMNLP [15][16][17][18], ACL [19][20][21][22], ICLR [23][24][25][26][27][28][29][30], ICML [31][32][33][34], ICSE [35,36], ICAIF [37,38], ICDM [39][40][41], IJCAI [42][43][44], AAAI [45][46][47], ECAI [48][49][50], BEA [51], Financial Innovation [52], PLOS Digital Health [53,54], Applied Sciences [55], Interspeech [56][57][58], Journal of Machine Learning Research [59][60][61][62], Journal of AI [63], etc., with an emphasis on studies from the past decade.Additionally, we reviewed references from selected papers and cross-checked our findings against existing LLMs surveys [4, [64][65][66].Our process involved identifying relevant studies and applying screening criteria based on LLMs relevance, citation count, and the quality of the publication venue.Papers with minimal contributions or lower relevance were excluded.The remaining studies were then evaluated for methodological rigor and clarity of conclusions.Key data from these studies were extracted and synthesized to highlight trends, gaps, and emerging themes in LLM research.This synthesis offers a comprehensive overview of current advancements, ongoing challenges, and future research directions, providing valuable insights for both academic and industrial applications.</p>
<p>The subsequent sections of this review are structured as follows: Section 2 details the history of LLMs and the contributing factors behind their rapid growth, covering the evolution of LMs, increased data diversity, computational advancements, and algorithmic innovations.Section 3 provides an overview of the principles of LLMs, using the GPT family of models as an accessible example to enhance understanding of LLMs' principles.Additionally, we present a comparison of state-of-the-art models across various types to assist readers in selecting the most suitable model for their specific applications.Moving on, Section 4 explores the wide-ranging applications of LLMs across various industries and professional domains.Section 5 critically examines the drawbacks of current state-of-the-art LLMs, and finally, Section 6 concludes the survey.</p>
<p>HISTORY AND DEVELOPMENT OF LARGE LANGUAGE MODELS</p>
<p>In this section, we will explore the history of the LMs and analyze the developmental dynamics and influences shaping LLMs.</p>
<p>History of LLMs</p>
<p>To better understand LLMs, this section will follow the developmental stages of LMs and introduce SLMs, NLMs, PLMs, and LLMs. Figure 1 provides a visual map of the history of the development of the LMs.• Statistical Language Models: SLMs [58,67,68] originated in the 1990s as mathematical models addressing contextually relevant properties of natural language from a probabilistic statistical perspective.The essence of statistical language modeling lies in ascertaining the probability of a sentence occurring within a text.Considering S as the sentence "I am very happy", P (ω i ) signifies the probability of the i-th word in the sentence: ω 1 as "I", ω 2 as "am", ω 3 as "very", and ω 4 as "happy".Now, the objective is to ascertain the likelihood of S appearing in the text, denoted as P (S):
P (S) = P (ω 1 , ω 2 , ω 3 , ω 4 ) = P (I, am, very, happy)(1)
To calculate this probability, the conditional probability can be employed:
P (I,
where P (I) represents the probability of the word "I" appearing and P (am|I) stands for the probability of "am" appearing given that "I" has appeared.When we multiply P (am|I) by P (I), it fulfills the condition of "I" appearing in P (am|I), resulting in the probability of "I am" appearing together as P (I, am) = P (I) • P (am|I).Now, the question arises: how do we calculate the conditional probability of the occurrence of each word?The answer lies in Maximum Likelihood Estimation, enabling us to estimate probabilities by substituting them with frequencies when the sample size is sufficiently large, given by:
P (w i |w 1 w 2 • • • w i−1 ) = P (w 1 • • • w i−1 w i ) P (w 1 w 2 • • • w i−1 ) = C(w 1 w 2 • • • w i ) C(w 1 w 2 • • • w i−1 )(3)
where C(•) represents the count of occurrences of the subsequence in the training set.Using this formula, we can calculate the likelihood of each word as the i-th word given the preceding i − 1 words.Then, we select the i-th word by choosing the word associated with the highest probability.</p>
<p>The previous discussion assumes that the n-th word is related to the initial n − 1 words, consistent with our intuition and commonly referred to as the n-gram model.In order to efficiently compute conditional probabilities, it is necessary to pre-compute and save C(X) required for the conditional probability computation, where X is a sentence of length n.The number of possible sentences X grows exponentially with the size of the vocabulary.For instance, with 1000 different words, there exist 1000 n potential sequences of length n.However, excessively large values of n pose storage limitations.Typically, n is confined to 2 or 3, causing each word to relate to only its first 1 or 2 preceding words, ultimately leading to a reduction in the model's accuracy.</p>
<p>• Neural Language Models: NLMs [56,57,69] leverage neural networks to predict the probabilities of subsequent words within sequences.They effectively handle longer sequences and mitigate the limitations associated with small n in SLMs.Before delving into neural networks, let's grasp the concept of word vectors.Humans effortlessly comprehend word meanings.For instance, "cat" and "dog" share closer semantic connections than "cat" and "tree" since they both represent animals.But how does a computer accomplish this?Computers operate using binary code -0s and 1s -so human language needs translation into binary.Word vectors can accomplish this, which are numerical representations of human language, where each word corresponds to a distinct vector.These vectors usually possess fixed dimensions and can simulate word relationships through the angles between them.An intriguing example is the angle between the word vectors for "cat" and "dog" which is smaller than the angle between the word vector for "cat" and "tree".Another illustrative example is "M essi − Argentina + P ortugal = Cristiano Ronaldo".Word2Vec [70] is a widely recognized tool for computing word vectors.Its function involves converting words into dense numerical representations, making words with similar semantics closer together in the vector space.The efficacy of Word2Vec is notably influenced by its utilization of neural networks, which mirrors the structure and functionality of the human brain, comprising interconnected simple units known as neurons organized into different layers.Figure 2 provides a visualization of a simple NLM structure, composed of input, hidden, and output layers.Within this structure, each word vector x t ∈ R m , the matrix W xh ∈ R m(n−1)×h , and the matrix W ho ∈ R h×|V | , where h signifies the number of neurons in the hidden layer, and V is the vocabulary containing all words recognized by the model.NLMs operate akin to the n-gram concept, assuming that the probability of each word depends only on its previous n-1 words.The first layer-the input layer-concatenates the word vectors of n-1 words, forming a vector X ∈ R m×(n−1) .Subsequently, the second layer-the hidden layer-derives the output H by applying a non-linear activation function, like sigmoid or tanh, to the matrix product of X and W xh .Following this, the third layer-the output layer-aims to forecast the subsequent word based on the hidden layer's output.With |V | neurons within the output layer, the resulting output vector O ∈ R |V | is computed by multiplying H and W ho .This resultant vector then undergoes the Softmax function, producing a vector O ′ containing the probability value assigned to each word.The Softmax function, concerning the output of the i-th neuron, is defined as follows:
O ′ i = Softmax (O ′ i ) = exp (O i ) |V | j=1 exp (O j ) ,(4)
where O ′ i denotes the output value of the i-th node, and |V | represents the count of output nodes, corresponding to the classification categories.Utilizing the Softmax function allows the transformation of output values from multiclass classification into a probability distribution, ensuring a sum of 1 within the range [0, 1].</p>
<p>• Pre-trained Language Models: PLMs undergo initial training using an extensive volume of unlabeled text, enabling them to grasp fundamental language structures such as vocabulary, syntax, semantics, and logic -a phase termed pre-training.Subsequently, this comprehensive LM can be applied to various NLP tasks like machine translation, text summarization, and question-answering systems.To optimize its performance, models need to be trained a second time on a smaller dataset customized for a specific downstream task -a phase known as fine-tuning.This is the "pre-training and fine-tuning" learning paradigm.We can use a visual example to understand the "pre-training and fine-tuning", as follows: in martial arts novels, a person who wants to become a martial arts master needs to have a solid foundation of internal martial arts, which can be acquired by training on a large variety of techniques.</p>
<p>Then, the person can learn a specific skill, such as a sword or a palm strike, and master it quickly and effectively by applying internal martial arts principles.A large number of studies on PLMs have been built on this paradigm, which introduces different architectures [61,71] (e.g., GPT-2 [9] and Bert [71]).• Large Language Models: LLMs are trained on massive text corpora with tens of billions (or more) of parameters, such as GPT-3 [10], GPT-4 [11], PaLM [60], and LLaMA [72].The goal of LLMs is to enable machines to understand human commands and adherence to human values.Their hallmark lies in the consolidation of two stages: initial pre-training on a vast general-purpose corpus followed by alignment with human values, rather than transitioning to a different domain.LLMs exhibit remarkable adaptability compared to PLMs, transitioning from specialized to general-purpose models.The substantial increase in model size, dataset volume, and computational prowess has resulted in significant enhancements across various tasks and unveiled remarkable capabilities absent in smaller models.</p>
<p>For example, GPT-3 has the capability to leverage contextual information, a functionality that GPT-2 [9] lacks.This means that when GPT-3 is prompted with task-related examples, it utilizes them to enhance its problem-solving capabilities.The number of parameters for LLMs typically exceeds a hundred billion, and the training data is usually in the range of a few hundred GB to a few TB.A concrete example is that there are multiple versions of the GPT-2 model, with the largest version having 1.5 billion parameters and using 40GB of text data for training.In contrast, the largest version of the GPT-3 model has 175 billion parameters and uses 570GB of text data for training.This example illustrates the significant discrepancy between LLMs and PLMs concerning parameter count and training data volume.</p>
<p>Factors Propelling Large Language Models</p>
<p>In the past few years, various factors have played a significant role in the swift advancement of LLMs:</p>
<p>• Data Diversity: Data diversity has emerged as a crucial catalyst for the development of LLMs.Recent years have witnessed a surge in the availability and diversity of extensive internet-based data sources, furnishing these models with training data containing rich linguistic and worldly insights.The quality and origins of this data significantly influence the efficacy and capacity of LLMs.Earlier studies trained language models on a single domain of text, such as news articles [73], Wikipedia [30], or fiction books [74].However, the proliferation of vast and heterogeneous text corpora across the web-including blogs, social media, forums, and reviews, among others-has spurred researchers to pivot towards training models on multi-domain texts.This shift enhances the model's aptitude for generalization and multitasking.An exemplary instance is Meta AI's open-source large language model, LLaMA [72],</p>
<p>trained on publicly accessible datasets like CommonCrawl1 , C4 [62], Github2 , Wikipedia3 , Books [75], ArXiv4 , and StackExchange5 .• Computational Advancement: LLMs are neural network models with a large number of parameters.</p>
<p>With the progression of deep learning (DL) technology in recent years, the scale and effectiveness of LLMs have notably escalated.Simultaneously, they present serious challenges due to their substantial computational demands [76].Computational power signifies a computer system's capability to execute computational tasks, often quantified in terms of floating-point operations per second.According to OpenAI calculations6 , since 2012, the amount of computation used for global AI training has shown exponential growth, doubling every 3.43 months on average, and now the amount of computation has expanded 300,000 times, far outpacing the rate of computational power growth.Fortunately, hardware innovators like Nvidia continually invent specialized devices such as GPUs, TPUs, and NPUs with amplified computational prowess, LLMs can be trained quickly.For instance, the launch of GPT-3 by OpenAI in May 2020 [10] underscores this reliance on robust hardware.Training GPT-3 on a single NVIDIA Tesla V100 GPU would require an estimated 355 years.Conversely, leveraging 1024×A100 GPUs, researchers estimated the potential to train GPT-3 in as little as 34 days.This exemplifies the indispensability of potent hardware in facilitating the development and training of LLMs.• Algorithmic Innovation: Algorithmic innovation stands as the pivotal driving force behind LLMs, shaping their structure and functionality.From the initial rule-and statistics-based approach to the later DL-based approach, the algorithms of LMs have continuously evolved and improved.Presently, all LLMs are based on the transformer architecture [77], which employs a self-attention mechanism (see discussion in Section 3).This architecture offers distinct advantages over traditional recurrent [57] and convolutional neural networks [78], enabling fully parallel computation and adeptly capturing long-distance dependencies.Moreover, there exist numerous iterations and enhancements of the transformer, such as Transformer-XL [79], XLNet [80], ALBERT [81], each targeting specific aspects for optimization.These variations aim to enhance the attention mechanism, refine pre-training objectives, and curtail parameter counts, thereby advancing the transformer architecture in diverse dimensions.</p>
<p>PRINCIPLES AND TAXONOMY OF LARGE LANGUAGE MODELS</p>
<p>This section strives to clarify the fundamental principles and taxonomy of LLMs, intending to provide general practitioners with a more profound understanding of the operational mechanisms employed by these models and help readers choose the most suitable LLMs for their specific applications.</p>
<p>Fundamental to LLMs is the utilization of deep neural networks to grasp the statistical patterns and semantic nuances of language, enabling the understanding and generation of language.The GPT series, a notable example among these models, incorporates the transformer architecture [77] and utilizes autoregression to predict consecutive words.The following discussion will concentrate on the GPT-3 [10] as an exemplar, providing insights into multiple facets that elucidate the principles of LLMs, as illustrated in Figure 3.</p>
<p>• Input: The GPT model architecture takes a sequence of symbol representations (x 1 , . . ., x n ) as input.This sequence comprises n words, also known as tokens, with GPT-3 explicitly defining the input sequence length as 2048 words.• Encoding: The significance of encoding words into vectors is heightened by the fact that the machine learning model operates on numeric inputs.Within the input embedding layer, the initial step involves constructing a comprehensive vocabulary that encompasses all words and assigning each word a unique position within this established vocabulary.For instance, "a" could be designated as 0, "an" as 1, and so forth.In the case of GPT-3, its vocabulary encompasses 50,257 words.Following this, every word can be transformed into a one-hot vector of 50,257 dimensions, where only a single element has the value of one while the rest are zeros.For example, "a" would be represented as [1,0,0,0,. . .], and "an" as [0,1,0,0,0,. . .].This process leads to the encoding of the input sequence into a matrix denoted as
I E ∈ R 2048×50257 .
• Embedding: The encoding process above transforms every input word into a one-hot vector of 50,257 dimensions.However, this vector exhibits high sparsity, primarily consisting of zeros, leading to significant space inefficiency.To address this drawback, the subsequent step in the input embedding layer employs an embedding matrix, denoted as W E , to condense these 50,257-dimensional input vectors into shorter numeric vectors of length n (n is set to 12,288 for GPT-3).This method condenses information about word meanings into a more compact space, effectively reducing the vector's length.Specifically, the transformation is represented as:
X W ordEmbedding = I E × W E , where X W ordEmbedding ∈ R 2048×12288 , I E ∈ R 2048×50257 and W E ∈ R 50257×12288 .
• Positional Encoding: The position and order of elements in a sequence are very important for understanding and generating sequences.Demonstrated through the sentences, "He is a good person and does not do bad things" and "He is a bad person and does not do good things", a slight alteration in word order -the transition from "good" to "bad" -results in a significant change in the sentence's semantic meaning.Within the architectural framework of the GPT model, the positional encoding layer serves as a complement to the input embedding layer.This augmentation facilitates the transformer layer in comprehending both the positional and sequential cues within the data.GPT-3 employs the following implementation for positional encodings:
P E(pos, 2i) = sin pos 10000 2i/d model (5) P E(pos, 2i+1) = cos pos 10000 2i/d model (6)
where pos is the position of the current word within the sentence (e.g., in the sentence "A survey of Large Language Model", pos A =0, pos Large =3), d model is the length of the embedding vector 12288 and i is the dimension.However, why delve into such intricate computations for positional encodings?Let's explore a more straightforward approach to conceptualize it.Initially, considering a text of length T , the most basic approach of positional encoding is counting.This method utilizes [0, 1, 2...T − 1] as positional encodings for each word in the text.For instance, the positional encodings for the 2-th word would be [1, 1, ..., 1], aligning in length with the embedding vector.However, this encoding strategy presents a notable drawback: within longer sentences, subsequent words hold considerably larger positional encoding values compared to the positional encodings of the initial words.As a result, this disparity inevitably introduces numerical biases in the merged features when combined with word embeddings.To address this issue, normalization becomes a viable consideration.The most straightforward normalization method involves dividing directly by T , resulting in positional encodings such as [0, 1 T , . . ., T −1 T ] (e.g., the 2nd word's positional encodings = [ 1 T , 1 T , . . ., 1 T ]).While this method restricts all positional encodings to the [0,1] range, it introduces a significant inconsistency: the positional values are contingent on the text's length, causing the positional distinctions between adjacent words in shorter texts to differ from those in longer texts.As illustrated in Figure 4, the positional encodings discrepancy between adjacent word positions in sentence ① is 0.5, while in sentence ②, neighboring word positions are coded with a difference of 0.125.Consequently, neither of these methods proves to be satisfactory.To address this issue, Google devised an alternative approach involving trigonometric functions [77].As a result, a positional encoding matrix denoted as X P ositionalEncoding ∈ R 2048×12288 .Fig. 4: Incorrect position encoding.</p>
<p>• Input Matrix: The input sequence (x 1 , . . ., x n ) undergoes processing through both the input embedding layer and the positional embedding layer to generate the input matrix.Specifically, input matrix X = embedding matrix X W ordEmbedding + positional encoding matrix X P ositionalEncoding , where X ∈ R 2048×12288 , X W ordEmbedding ∈ R 2048×12288 and X P ositionalEncoding ∈ R 2048×12288 .• Masked Multi-Head Self-Attention: Masked multi-head self-attention involves combining a masking technique and a multi-head mechanism within the framework of self-attention.The following discussion commences with self-attention before delving into masked multi-head self-attention for enhanced comprehension.</p>
<p>Self-Attention: Attention is a crucial cognitive function for human beings, allowing us to concentrate on significant aspects within a complex environment.Within the transformer layer, self-attention mirrors this cognitive functionality by forecasting which input words warrant attention for every output in the sequence, subsequently assessing their significance.For instance, when analyzing the input "I am very" and predicting the output "happy", the relative contributions of the three words "I am very" to the output "happy" might be quantified as 0.7, 0.1, and 0.2, respectively.The specific steps are as follows:</p>
<p>Step 1: Initiation commences with the creation of three projection matrices W q ∈ R 12288×128 , W k ∈ R 12288×128 , and W v ∈ R 12288×128 .These matrices are subsequently multiplied with the input matrix X ∈ R 2048×12288 to obtain three different matrices Q, K, and V , which respectively represent the query, key, and value.</p>
<p>Step 2: Following this, a matrix is derived using the formula Sof tmax (Q × K T ).This matrix then multiplied with V to obtain the output Y , represented as
Y = Sof tmax (Q × K T ) × V , where Q ∈ R 2048×128 , K T ∈ R 128×2048 , V ∈ R 2048×128
, and Y ∈ R 2048×128 .An illustrative analogy can be drawn to searching for a song within a music software -here, Q denotes the queried song's name, K encompasses all song names within the software database, and V includes the corresponding audio data of all songs in the software.Thus, Y signifies the audio data of the sought-after song.</p>
<p>Masked Self-Attention: Masked self-attention is a modified version of self-attention designed to prevent the model from accessing future words during the prediction of the next word by incorporating a masking technique.For example, with the corpus "abcde", we can train GPT to predict the next word four times by feeding in "a", "ab", "abc" and "abcd" separately.However, this involves segmenting and inputting the sentence four times.The mask technique trick streamlines this process, enabling the model to receive the entire sentence at once and generate an output for each word.The challenge Fig. 5: Self-Attention and Musk Self-Attention: a normal self-attention block allows a position to attend to all words to its right, while masked self-attention prevents a position from attending to words that come after it in the sequence.arises from the model peeking at subsequent words, which necessitates the use of masking.When predicting the next word of "a" GPT will only see "a", and when predicting the next word of "ab", it will only see "ab".It's essential to differentiate between self-attention (utilized in BERT [71]) and masked self-attention (utilized in GPT-3 [10]).Figure 5 provides a visual illustration.</p>
<p>Multi-Head Self-Attention: Multi-head self-attention subdivides the self-attention mechanism into h independent heads to compute in parallel (for GPT-3, h is 96).Each head focuses on different aspects and connections within the sequence, concatenating their outcomes afterward.This approach facilitates the model to collectively concentrate on information derived from diverse representation sub-spaces, thereby enhancing its capacity for expression.Each head possesses its distinctive projection matrix, each with dimensions d = d model /h = 12288/96 = 128.The outcome from each head is symbolized as Y s , and when these individual head outputs are combined through concatenation (96Y s = 12288 = 96 × 128), the resultant amalgamation is denoted as Y.</p>
<p>• Feedforward Neural Net: In addition to masked multi-head self-attention sub-layers, each of the transformer layers in the GPT model architecture contains a fully connected feedforward neural net.This neural network serves as a mechanism designed to non-linearly transform its inputs, thereby enhancing the model's overall expressive capabilities.• Add &amp; Norm: The add &amp; norm modules are incorporated into each transformer layer on the GPT model architecture, where "add" represents the skip connection [82] and "norm" represents layer normalization [83].The skip connection helps alleviate the issue of gradient vanishing in deep models, enabling the transformer layer to be stacked to a considerable depth.Additionally, layer normalization serves as a regularization strategy to prevent network overfitting.It calculates the mean and variance on each sample independently, without considering other data, thereby mitigating the impact of varying batch sizes.• Decoding &amp; Output: After 12 transformer layers, the input matrix X ∈ R 2048×12288 transforms into an information-rich matrix Y ∈ R 2048×12288 , representing the final representation of the input sequence.</p>
<p>To achieve the desired result, we employ Sof tmax(Y • W ), where W ∈ R 12288×50257 , with 50,257 as the vocabulary size.This process generates probabilities assigned to each word in the vocabulary, enabling the selection of the most likely output as GPT's prediction for each word.</p>
<p>To help readers choose the most suitable model for their specific applications, we further organize LLMs based on their structure.Specifically, we classify LLMs into three types: encoder-only, encoder-decoder, and decoder-only models.This taxonomy is based on the transformer architecture, the backbone of LLMs, comprising core components like the encoder, decoder, and their variations, which together enable their capabilities [77].Specifically, encoders are designed to process and interpret input sequences, while decoders generate output sequences based on this interpretation [84].In some models, both components are integrated into an encoder-decoder framework to handle a diverse set of tasks effectively [33].Conversely, other models focus on specific applications by using either the encoder alone for understanding tasks or the decoder alone for text generation.The detail information about each type of models is as outlined below:</p>
<p>• Encoder-only models, such as BERT [71], primarily focus on understanding and interpreting input data rather than generating it.They excel in tasks that require strong contextual understanding, such as sentiment analysis, named entity recognition, and text classification.These models are highly effective for analyzing text but are less suitable for text generation tasks due to their limited generation capabilities and computational intensity when handling large texts.• Encoder-decoder LLMs, like T5 [62], combine the strengths of both encoders and decoders, making them versatile and powerful for a range of applications.These models are particularly effective for tasks that require both text understanding and generation, such as machine translation, text summarization, and question answering.The encoder-decoder architecture provides a unified approach to these tasks, allowing for a comprehensive understanding and generation of text.However, the complexity of this dual architecture can result in more challenging training processes and slower inference times compared to other model types.• Decoder-only LLMs, such as GPT-3 [10], are optimized for generating text, making them ideal for content creation, conversational AI, and creative writing.These models are known for their excellent text-generation capabilities and are effective in few-shot learning scenarios, where they can generate coherent and contextually relevant responses from minimal input.However, decoder-only models can face limitations in understanding context deeply and are often resource-intensive, requiring significant computational power to generate high-quality text outputs.</p>
<p>With the proposed taxonomy, table 1 provides a comparison of state-of-the-art LLMs across different types, with models in each category organized in ascending order of parameter size.This structure enables a systematic examination of key aspects such as architecture, parameter count, provider, release date, open-source status, tunability status, subsequently fine-tuning applied for the model (Adaptation), pretraining data scale, and evaluation methods in their original papers.Analyzing these elements together provides a thorough understanding of the models' capabilities and differences.Specifically, the type of each model indicates its architecture, whereas the parameter size reflects both the model's complexity and the resources required for fine-tuning.Next, the provider describes the model's development context and goals, and the release date places the model within the timeline of technological advancements.Furthermore, open-source status indicates the model's accessibility for future research and adaptation, whereas tunability status indicates whether the model has been customized after release.Last but not least, the pre-training data scale shows the model's exposure to various types of information, whereas the evaluation methods assess its performance against benchmarks.As an example, Codex [85], a decoderonly model with 12 billion parameters created by OpenAI and released in July 2021, was pre-trained on 100 billion tokens and supports in-context learning.While it is customizable and open-source, Codex does not incorporate more advanced techniques such as chain-of-thought methods or reinforcement learning with human feedback, which limits its ability to handle certain complex tasks.</p>
<p>APPLICATIONS OF LARGE LANGUAGE MODELS</p>
<p>LLMs have been instrumental across diverse domains, and this section aims to illustrate exemplary applications of LLMs in various fields.</p>
<p>Software Engineering: LLMs have demonstrated remarkable capabilities across various software engineering tasks such as defect prediction [35], code review [114], code generation [19], and analysis of software-related questions [115][116][117].These models significantly aid in improving code quality, debugging, and reducing human involvement in many aspects of software development.For instance, Codex [85] is a pioneering model in this domain, utilizing a large generative pre-trained model with up to 12 billion parameters to aid developers by generating code, suggesting optimizations, and identifying and correcting errors.Building on this foundation, several other models have been developed to address various facets of  [91], which specializes in generating code for programming competitions, and Meta's InCoder [23] and Code Llama [118], as well as Salesforce's CodeRL [119] and CodeGen [24,101].The BigCode project has introduced Star-Coder [119], and OpenAI has made significant strides with its GPT [11] and ChatGPT series, enhanced through Reinforcement Learning from Human Feedback (RLHF).These developments expand the scope and potential of automated code generation, setting a new standard in software engineering that offers both opportunities and challenges for the industry.</p>
<p>Drug Discovery: The field of drug discovery is characterized by substantial challenges and elevated costs.The utilization of computational methodologies, such as AI, DL, and quantum mechanical techniques, holds the promise of accelerating the identification of potential drug candidates and predicting their properties [120,121].Among recent advancements in this arena, the integration of LLMs, such as ChatGPT, emerges as a valuable addition, adept at generating and analyzing natural language texts [122].</p>
<p>LLMs contribute significantly to the initial phase of drug discovery, particularly in identifying suitable drug targets for diverse diseases [123,124].They offer preliminary insights into the structure and functionality of protein-based drug targets, subject to further validation by alternative methodologies.Furthermore, LLMs play a pivotal role in subsequent drug discovery steps, aiding in the design and screening of small molecules capable of interacting with these drug targets [122].</p>
<p>Finance: LLMs are driving significant advancements in the finance industry, with significant financial applications including trading and portfolio management [125], financial risk modeling [126], financial text mining [38,52], and financial advisory and customer services [127].AI integration in financial advisory and customer services is an emerging and rapidly expanding field.According to [128], AIdriven chatbots already provide more than 37% of supporting functions in various e-service scenarios.Within the financial field, these chatbots are embraced as cost-effective alternatives to human customer service, a trend underscored in the "Chatbots in Consumer Finance" report 7 .With the advent of LLMs, more and more tasks that were previously considered difficult to handle have become possible, further expanding the potential applications of AI in the financial industry [37].Notably, there are already some specialized LLMs like BloombergGPT [129], a 50-billion-parameter model excelling in financial tasks while maintaining strong performance in general LM benchmarks.</p>
<p>Medical:</p>
<p>LLMs are now at the forefront of medical AI with immense potential to improve the efficiency and effectiveness of clinical, educational, and research work [130,131].One example is the performance of LLMs on the MedQA dataset [55], which serves as a widely used biomedical question-answering dataset consisting of the United States Medical Licensing Examination (USMLE)-style questions.In this dataset, ChatGPT attains human passing levels and [53].Additionally, in a span of fewer than six months, Med-PaLM2 achieves proficiency levels close to human experts [132].The strong performance of GPT-4 [11] and Med-PaLM2 in medical tests suggests that LLMs may be useful teaching tools for students currently attaining a lower level in such tests [130].GPT-4's meta-prompt feature allows users to explicitly describe the desired role for the chatbot to take on during conversation; a useful example is a "Socratic tutor mode"8 , which encourages students to think for themselves by pitching questions at decreasing levels of difficulty until students can work out solutions to the fuller question at hand.This mode can foster critical thinking and problem-solving skills among students [130].ChatGPT demonstrates superior performance in tasks not necessitating specialized knowledge or when such expertise is available in user prompts [133].For example, consider discharge summaries as a case in point -they entail repetitive tasks involving information interpretation and compression, often requiring minimal specialized knowledge and user prompts [72].</p>
<p>Legal: LLMs exhibit robust capabilities across various legal tasks, particularly in case prediction and generating legal texts.For instance, ChatGPT achieved a 50.3% accuracy rate on NCBE MBE practice exams, significantly surpassing the 25% random guess level, and achieved passing scores in both Evidence and Torts [134].The integration of ChatGPT holds the potential to reshape the entire legal industry.An Australian law firm experimented with ChatGPT to draft a statement of claim based on the 1992 Mabo case, yielding results akin to those of a first-year lawyer 9 .[135] investigation explored the possibility that ChatGPT could replace litigators by evaluating its drafting and research capabilities.While recommending ChatGPT as a complement to litigators rather than a complete replacement, this suggestion stemmed from ChatGPT's limitations in meeting the precise accuracy and timeliness demands of legal texts, especially in ensuring continuous access to updated legal research.More recently, advancements such as DeLilegal10 addressed this issue by integrating with external law databases to enhance retrieval and minimize hallucination.Meanwhile, LLMs in the legal field have emerged in recent months such as ChatLaw [79].</p>
<p>Education: With the rapid advancement of AI technology, its applications in education are expanding, fundamentally transforming teaching and learning methods.Researchers have recognized the remarkable capabilities of LLMs, such as ChatGPT, and have explored their substantial potential to impact various educational settings.These models can be used in diverse ways, such as simulating students to help train teachers [136][137][138] and acting as virtual instructors to provide personalized instruction to students [16,139,140].LLMs can also function as personal tutors, offering real-time feedback [141] and tailored evaluations and suggestions based on individual learning progress [63,142].Furthermore, they can enhance the overall learning experience by improving student engagement and promoting greater autonomy in learning [51,143].In addition to supporting individualized learning, LLMs can address broader educational challenges, such as the low teacher-student ratio, by supplementing human instructors and providing additional support to students [144].The integration of LLMs into education presents a unique opportunity to personalize and scale educational resources, making quality education more accessible to a broader range of learners.</p>
<p>DRAWBACKS AND FUTURE DIRECTIONS OF LARGE LANGUAGE MODELS</p>
<p>While LLMs offer significant advantages in improving both professional and personal aspects of life, it is essential to acknowledge their accompanying drawbacks.In the subsequent discussion, we will identify and explore some of these limitations inherent in current LLMs, which concurrently provide insights into potential directions for future advancements.</p>
<p>Privacy: LLMs are widely used across various sectors, but they also present significant privacy concerns [65,145].One major issue is the risk of data leakage, where LLMs may inadvertently memorize and reproduce sensitive or personal information from their training datasets, potentially leading to privacy breaches.This risk is particularly severe in domains like healthcare and finance, where exposing private data can have serious consequences [146].Additionally, the training datasets used for LLMs are often not fully anonymized, meaning personal identifiers and other sensitive information could be embedded within the model [147].This increases the risk of violating privacy regulations, such as the General Data Protection Regulation [148].Recent developments have further complicated the privacy landscape, as tools like FraudGPT and WormGPT have emerged.These tools, designed for cybercrime, highlight the misuse potential of LLMs by generating fraudulent emails, suggesting malicious link placements, and facilitating cyberattacks.FraudGPT [149,150] and WormGPT [151] were trained on datasets focused on malware and fraud, enabling cybercriminals to conduct sophisticated attacks such as Business Email Compromise (BEC).This illustrates the urgent need for robust privacy safeguards and regulatory frameworks to prevent the misuse of LLMs and protect sensitive information from being exploited maliciously.</p>
<p>Fairness: LLMs are widely employed for decision-making across various domains, impacting individuals and society.However, these decisions might exhibit bias against marginalized populations [152][153][154][155][156].</p>
<p>For instance, when utilized in screening resumes for programming positions, LLMs might exhibit a bias favoring male candidates, demonstrating evident gender discrimination [157][158][159][160][161].This bias stems from the training of LLMs on extensive and unstructured Internet data, where they inherit stereotypes, misrepresentations, derogatory language, and exclusionary behaviors.These aspects disproportionately affect already disadvantaged and marginalized groups [15,22,162].The commonly employed strategy to mitigate bias in LLMs is to remove biased data from the training dataset.However, this strategy does not entirely eliminate bias and often diminishes the effectiveness of language modeling [163].Saxena et al. [152] discussed that the root causes of these problems lie not only in technology but also in socially acceptable definitions of fairness and meaningful interventions to ensure the long-term wellbeing of all groups.Researchers should consider the needs of disadvantaged groups from the outset and design these technologies proactively, rather than simply reacting to the biases present in their designed systems [40,164,165].The documented bias in LLMs, as highlighted by studies like [54,166,167], underscores the imperative for future efforts in this direction.</p>
<p>Safety: LLMs have diverse applications in industries such as finance and healthcare.However, these applications also present significant security challenges.LLMs may generate outputs that diverge from the provided context, user input, or factual knowledge, a phenomenon referred to as "hallucination" [21,66].This undermines their reliability in real-world scenarios, emphasizing the vital necessity for precision, particularly in critical domains like medicine [168,169].As a primary way to avoid these problems, well-aligned LLMs can be developed by including humans in the training loop and using reinforcement learning from human feedback (RLHF) [75,109].To improve model safety, it is also important to include safety-related cues in the RLHF process, as shown in GPT-4 [11].However, RLHF relies heavily on high-quality human feedback data from professional annotators, which is costly and time-consuming.As a result, enhancing the RLHF framework is essential to alleviate the burden on human annotators.Additionally, exploring more efficient annotation methods with assured data quality, such as LLMs, can assist in the annotation process and enhance overall safety.</p>
<p>Intellectual Property: LLMs possess the capability to create content resembling human-generated material, potentially encroaching upon users' intellectual property rights.These models rely on training and optimizing using users' textual data, which may encompass personal information, proprietary knowledge, patented technology, and more.Notably, AI-generated code exemplifies this issue.Code-generation models trained on open-source repositories can produce programs resembling existing ones, potentially disregarding relevant licenses [34].For instance, legal action has been taken against Microsoft, GitHub, and OpenAI, alleging copyright infringement due to Copilot reproducing licensed code without adherence to licensing terms 11 12 .Additionally, there are concerns about potential copyright infringements with LLM-generated images and texts [170,171], as highlighted in recent lawsuits by a group of novelists against OpenAI in July 2023 for allegedly using their books without permission to train models [172], and by The New York Times against OpenAI and Microsoft in December 2023 for scraping articles to train their generative models without consent [173].Consequently, there's a pressing need for heightened attention and regulation concerning intellectual property rights concerning LLMs.This is essential to safeguard the legitimate rights and interests of original creators, users, and the public while fostering and respecting the innovation and advancement of LLMs technology [174].</p>
<p>CONCLUSIONS</p>
<p>In recent times, Large Language Models (LLMs) have garnered significant attention from the fields of science, industry, society, and beyond.However, despite the substantial advantages that LLMs offer in enhancing both professional and personal aspects of life, a limited understanding among general practitioners regarding the background and principles of these models impedes their full potential.In light of this, the survey at hand systematically delves into the evolution of LLMs, presenting key concepts and techniques for a comprehensive understanding of these models.It introduces a detailed taxonomy of LLMs and compares state-of-the-art models to assist readers in selecting the most suitable model for their specific needs.Additionally, the survey highlights the limitations of current LLMs and identifies promising areas for future research.Notably, setting itself apart from other surveys on LLMs, this study prioritizes empowering practitioners, irrespective of their background knowledge, to ensure their proficient utilization in both scientific research and daily tasks.</p>
<p>Fig. 1 :
1
Fig. 1: History and development of language models.</p>
<p>am, very, happy) = P (I) • P (am | I) • P (very | I, am) • P (happy | I, am, very)</p>
<p>Fig. 2 :
2
Fig. 2: Neural language model.</p>
<p>Fig. 3 :
3
Fig. 3: The GPT-3 model architecture.Note that only two transformer layers are illustrated for simplicity and illustrative purposes, and the actual model consists of 12 transformer layers.</p>
<p>Table 1 :
1
Statistics of large language models in recent years.IT: instruction tuning.RLHF: reinforcement learning with human feedback.ICL: in-context learning.COT: chain-of-thought
TypeModelSizeProviderReleaseOpen-sourcedTuna-bilityAdaptation IT RLHFPre-train Data scaleEvaluation ICL CoTALBERT [81]0.012Google AI02/2020--16GB--DeBERTa [29]0.1Microsoft06/2020--80GB--Encoder-ELECTRA [86]0.11Google AI03/2020-----onlyBERT [71]0.11Google AI10/2018--3.3B words--RoBERTa [87]0.125Meta AI07/2019--160GB--XLM-RoBERTa [88]0.27Meta AI07/2020--2.5TB--ERNIE 3.0 [89]10Baidu07/2021x--300B tokens-T5 [62]11Google AI10/2019--1T tokens-T0 [26]11BigScience10/2021---Flan-T5 [59]11Google AI10/2022--UL2 [27]20Google AI05/2022--1T tokensAlexaTM [90]20Amazon08/2022x--1.3T tokensAlphaCode [91]41Google AI02/2022xx--967B tokens--Anthropic [92]52Anthropic12/2021x--400B tokens-NLLB [93]54.5Meta AI07/2022----Encoder-decoderLLaMa [72] FLAN [25] Sparrow [94]65 67 70Meta AI Google AI Google AI02/2023 09/2021 09/2022x x----1.4T tokens ----Chinchilla [95]70Google AI03/2022x--1.4T tokens-OPT-OML [96]175Meta AI12/2022x--Gopher [97]280Google AI12/2021x--300B tokens-PaLM [60]540Google AI04/2022--780B tokensU-PaLM [18]540Google AI10/2022xx---GLaM [31]1200Google AI12/2021x--280B tokens-Codex [85]12OpenAI07/2021xx--100B tokens-Pythia [32]12Eleuther AI04/2023x--300B tokens-CodeGeeX [98]13Tsinghua U.09/2022--850B tokens-Skywork [99]13Kunlun Inc10/2023--3.2T tokens-QWEN [100]14Alibaba09/20233T tokens-CodeGen2 [101]16Salesforce AI 05/2023--400B tokens-GPT-NeoX-20EleutherAI04/2022x--825GB-20B [102]Gemini 1.5 [103]20Google AI02/2024x----LlaMa 2 [72]70Meta AI07/20232T tokens-LlaMa 3 [104]70Meta AI04/202415T tokens-HyperCLOVA [17]82Naiver09/2021x--300B tokens-Decoder-onlyGalactia [105] GLM [28] LaMDA [106]120 130 137Meta AI Tsinghua U. Google AI11/2022 10/2022 01/2022x------106B tokens 400B tokens 768B tokens--Jurassic-1 [107]178AI21 Labs08/2021x--300B tokens-GPT-3 [10]175OpenAI05/2020xx--300B tokens-WebGPT [108]175OpenAI12/2021xx----InstructGPT [109]175OpenAI03/2022xx--BLOOM [110]176BigScience11/2022--366B tokens-BLOOMZ [20]176BigScience11/2022x---Llama 3.1 [111]405Meta AI06/202415T tokens-GPT-4 [11]450OpenAI03/2023x1.76TMT-NLG [112]530Microsoft01/2022x--967B tokens-Gemma [113]2700Google AI02/2024--6T tokens-coding and software development. Notable examples include DeepMind's AlphaCode
https://commoncrawl.org/get-started
https://github.com/
https://huggingface.co/datasets/wikipedia
https://arxiv.org/
https://stackexchange.com/
https://openai.com/research/ai-and-compute
https://www.consumerfinance.gov/data-research/research-reports/chatbots-in-consumer-finance/chatbots-in-consumer-finance
https://sites.highlands.edu/tutorial-center/tutor-resources/online-tutor-training/module-4/the-socratic-method/
How ChatGPT and other new AI tools are being used by lawyers, architects and coders -ABC News
https://data.delilegal.com/lawQuestion
AcknowledgementThis work was supported in part by the National Science Foundation (NSF) under Grant No. 2245895.
The language instinct: How the mind creates. Language. S Pinker, 1994Harper CollinsNew York</p>
<p>Computing machinery and intelligence. A M Turing, H Geirsson, M Losonsky, Artificial Intelligence: Critical Concepts. 2236192000</p>
<p>so what if chatgpt wrote it?" multidisciplinary perspectives on opportunities, challenges and implications of generative conversational ai for research, practice and policy. Y K Dwivedi, N Kshetri, L Hughes, E L Slade, A Jeyaraj, A K Kar, A M Baabdullah, A Koohang, V Raghavan, M Ahuja, International Journal of Information Management. 711026422023</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 202311arXiv preprint</p>
<p>Rethinking learning rate tuning in the era of large language models. H Jin, W Wei, X Wang, W Zhang, Y Wu, 2023IEEE</p>
<p>How does gpt obtain its ability? tracing emergent abilities of language models to their sources. Y Fu, H Peng, T Khot, 2022Yao Fu's Notion</p>
<p>Emergent abilities of large language models. J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, Transactions on Machine Learning Research. 2022</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 332020</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, ACM Computing Surveys. 5592023</p>
<p>Pre-trained models: Past, present and future. X Han, Z Zhang, N Ding, Y Gu, X Liu, Y Huo, J Qiu, Y Yao, A Zhang, L Zhang, AI Open. 22021</p>
<p>Talking about large language models. M Shanahan, Communications of the ACM. 6722024</p>
<p>Documenting large webtext corpora: A case study on the colossal clean crawled corpus. J Dodge, M Sap, A Marasović, W Agnew, G Ilharco, D Groeneveld, M Mitchell, M Gardner, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Class: A design framework for building intelligent tutoring systems based on learning science principles. S Sonkar, N Liu, D Mallick, R Baraniuk, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers. B Kim, H Kim, S.-W Lee, G Lee, D Kwak, J D Hyeon, S Park, S Kim, S Kim, D Seo, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Transcending scaling laws with 0.1% extra compute. Y Tay, J Wei, H Chung, V Tran, D So, S Shakeri, X Garcia, S Zheng, J Rao, A Chowdhery, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Unified pre-training for program understanding and generation. W Ahmad, S Chakraborty, B Ray, K Chang, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Crosslingual generalization through multitask finetuning. N Muennighoff, T Wang, L Sutawika, A Roberts, S Biderman, T Le Scao, M S Bari, S Shen, Z X Yong, H Schoelkopf, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterLong Papers20231</p>
<p>Societal biases in language generation: Progress and challenges. E Sheng, K.-W Chang, P Natarajan, N Peng, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Incoder: A generative model for code infilling and synthesis. D Fried, A Aghajanyan, J Lin, S Wang, E Wallace, F Shi, R Zhong, S Yih, L Zettlemoyer, M Lewis, The Eleventh International Conference on Learning Representations. </p>
<p>Codegen: An open large language model for code with multi-turn program synthesis. E Nijkamp, B Pang, H Hayashi, L Tu, H Wang, Y Zhou, S Savarese, C Xiong, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, International Conference on Learning Representations. 2021</p>
<p>Multitask prompted training enables zero-shot task generalization. V Sanh, A Webson, C Raffel, S H Bach, L Sutawika, Z Alyafeai, A Chaffin, A Stiegler, T Le Scao, A Raja, ICLR 2022-Tenth International Conference on Learning Representations. 2022</p>
<p>Ul2: Unifying language learning paradigms. Y Tay, M Dehghani, V Q Tran, X Garcia, J Wei, X Wang, H W Chung, D Bahri, T Schuster, S Zheng, The Eleventh International Conference on Learning Representations. </p>
<p>Glm-130b: An open bilingual pre-trained model. A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, The Eleventh International Conference on Learning Representations. </p>
<p>Deberta: Decoding-enhanced bert with disentangled attention. P He, X Liu, J Gao, W Chen, International Conference on Learning Representations. </p>
<p>. S Merity, C Xiong, J Bradbury, R Socher, 2022Pointer sentinel mixture models</p>
<p>Glam: Efficient scaling of language models with mixture-of-experts. N Du, Y Huang, A M Dai, S Tong, D Lepikhin, Y Xu, M Krikun, Y Zhou, A W Yu, O Firat, International Conference on Machine Learning. PMLR2022</p>
<p>Pythia: A suite for analyzing large language models across training and scaling. S Biderman, H Schoelkopf, Q G Anthony, H Bradley, K O'brien, E Hallahan, M A Khan, S Purohit, U S Prashanth, E Raff, International Conference on Machine Learning. PMLR2023</p>
<p>What language model architecture and pretraining objective works best for zero-shot generalization. T Wang, A Roberts, D Hesslow, T Le Scao, H W Chung, I Beltagy, J Launay, C Raffel, International Conference on Machine Learning. PMLR2022</p>
<p>Codeipprompt: intellectual property infringement assessment of code language models. Z Yu, Y Wu, N Zhang, C Wang, Y Vorobeychik, C Xiao, International Conference on Machine Learning. PMLR2023</p>
<p>An empirical study of deep learning models for vulnerability detection. B Steenhoek, M M Rahman, R Jiles, W Le, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>Improving fairness in machine learning software via counterfactual fairness thinking. Z Yin, Z Wang, W Zhang, Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings. the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings2024</p>
<p>Large language models in finance: A survey. Y Li, S Wang, H Ding, H Chen, Proceedings of the Fourth ACM International Conference on AI in Finance. the Fourth ACM International Conference on AI in Finance2023</p>
<p>Investor behavior modeling by analyzing financial advisor notes: a machine learning perspective. C Pagliaro, D Mehta, H.-T Shiao, S Wang, L Xiong, Proceedings of the Second ACM International Conference on AI in Finance. the Second ACM International Conference on AI in Finance2021</p>
<p>Fair decision-making under uncertainty. W Zhang, J Weiss, 2021 IEEE International Conference on Data Mining (ICDM). IEEE2021</p>
<p>Mitigating multisource biases in graph neural networks via real counterfactual samples. Z Wang, G Narasimhan, X Yao, W Zhang, 2023 IEEE International Conference on Data Mining (ICDM). IEEE2023</p>
<p>Optimization and improvement of fake news detection using voting technique for societal benefit. S V Chinta, K Fernandes, N Cheng, J Fernandez, S Yazdani, Z Yin, Z Wang, X Wang, W Xu, J Liu, 2023 IEEE International Conference on Data Mining Workshops (ICDMW). IEEE2023</p>
<p>Faht: an adaptive fairness-aware decision tree classifier. W Zhang, E Ntoutsi, International Joint Conference on Artificial Intelligence (IJCAI). 2019</p>
<p>Farf: A fair and adaptive random forests classifier. W Zhang, A Bifet, X Zhang, J C Weiss, W Nejdl, Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer2021</p>
<p>Unveiling and mitigating bias in ride-hailing pricing for equitable policy making. N A Saxena, W Zhang, C Shahabi, AI and Ethics. 2024</p>
<p>Longitudinal fairness with censorship. W Zhang, J C Weiss, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Fairness with censorship: Bridging the gap between fairness research and real-world deployment. W Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Censored fairness through awareness. W Zhang, T Hernandez-Boussard, J Weiss, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Individual fairness under uncertainty. W Zhang, Z Wang, J Kim, C Cheng, T Oommen, P Ravikumar, J Weiss, 26th European Conference on Artificial Intelligence. 2023</p>
<p>Group fairness with individual and censorship constraints. Z Wang, W Zhang, 26th European Conference on Artificial Intelligence. 20234</p>
<p>Individual fairness with group constraints in graph neural networks. Z Wang, D Ulloa, T Yu, R Rangaswami, R Yap, W Zhang, 26th European Conference on Artificial Intelligence. 20234</p>
<p>Evaluating reading comprehension exercises generated by llms: A showcase of chatgpt in education applications. C Xiao, S X Xu, K Zhang, Y Wang, L Xia, Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023). the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)2023</p>
<p>Comprehensive review of text-mining applications in finance. A Gupta, V Dengre, H A Kheruwala, M Shah, Financial Innovation. 62020</p>
<p>Performance of chatgpt on usmle: potential for ai-assisted medical education using large language models. T H Kung, M Cheatham, A Medenilla, C Sillos, L De Leon, C Elepaño, M Madriaga, R Aggabao, G Diaz-Candido, J Maningo, PLoS digital health. 221982023</p>
<p>Hate speech detection and racial bias mitigation in social media based on bert model. M Mozafari, R Farahbakhsh, N Crespi, PloS one. 1582378612020</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. D Jin, E Pan, N Oufattole, W.-H Weng, H Fang, P Szolovits, Applied Sciences. 111464212021</p>
<p>Recurrent neural network based language modeling in meeting recognition. S Kombrink, T Mikolov, M Karafiát, L Burget, Interspeech. 112011</p>
<p>Recurrent neural network based language model. T Mikolov, M Karafiát, L Burget, J Cernockỳ, S Khudanpur, Interspeech. 22010Makuhari</p>
<p>Srilm-an extensible language modeling toolkit. A Stolcke, Interspeech. 200220022002</p>
<p>Scaling instruction-finetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, Y Li, X Wang, M Dehghani, S Brahma, Journal of Machine Learning Research. 25702024</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. W Fedus, B Zoph, N Shazeer, Journal of Machine Learning Research. 231202022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Journal of machine learning research. 211402020</p>
<p>Education in the era of generative artificial intelligence (ai): Understanding the potential benefits of chatgpt in promoting teaching and learning. D Baidoo-Anu, L O Ansah, Journal of AI. 712023</p>
<p>Z Z Chen, J Ma, X Zhang, N Hao, A Yan, A Nourbakhsh, X Yang, J Mcauley, L Petzold, W Y Wang, arXiv:2405.01769A survey on large language models for critical societal domains: Finance, healthcare, and law. 2024arXiv preprint</p>
<p>Y Yao, J Duan, K Xu, Y Cai, Z Sun, Y Zhang, A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing. 2024100211</p>
<p>L Huang, W Yu, W Ma, W Zhong, Z Feng, H Wang, Q Chen, W Peng, X Feng, B Qin, arXiv:2311.05232A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. 2023arXiv preprint</p>
<p>Statistical methods for speech recognition. F Jelinek, 1998MIT Press</p>
<p>Two decades of statistical language modeling: Where do we go from here?. R Rosenfeld, Proceedings of the IEEE. 8882000</p>
<p>A neural probabilistic language model. Y Bengio, R Ducharme, P Vincent, Advances in neural information processing systems. 132000</p>
<p>Distributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in neural information processing systems. 262013</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Roziere, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Exploring the limits of language modeling. R Jozefowicz, O Vinyals, M Schuster, N Shazeer, Y Wu, arXiv:1602.024102016arXiv preprint</p>
<p>Skipthought vectors. R Kiros, Y Zhu, R R Salakhutdinov, R Zemel, R Urtasun, A Torralba, S Fidler, Advances in neural information processing systems. 282015</p>
<p>The pile: An 800gb dataset of diverse text for language modeling. L Gao, S Biderman, S Black, L Golding, T Hoppe, C Foster, J Phang, H He, A Thite, N Nabeshima, arxiv.arXivpreprintarXiv:2101.000272020</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 201730Attention is all you need</p>
<p>Gradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 86111998</p>
<p>Chatlaw: Open-source legal large language model with integrated external knowledge bases. J Cui, Z Li, Y Yan, B Chen, L Yuan, arXiv:2306.160922023arXiv preprint</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R R Salakhutdinov, Q V Le, Advances in neural information processing systems. 322019</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, arXiv:1909.119422019arXiv preprint</p>
<p>Deep residual learning for image recognition. H Kaiming, Z Xiangyu, R Shaoqing, S Jian, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition201634</p>
<p>J L Ba, J R Kiros, G E Hinton, arXiv:1607.06450Layer normalization. 2016arXiv preprint</p>
<p>C Wang, M Li, A J Smola, arXiv:1904.09408Language models with transformers. 2019arXiv preprint</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>K Clark, arXiv:2003.10555Electra: Pre-training text encoders as discriminators rather than generators. 2020arXiv preprint</p>
<p>A robustly optimized bert pre-training approach with posttraining. L Zhuang, L Wayne, S Ya, Z Jun, Proceedings of the 20th Chinese National Conference on Computational Linguistics. the 20th Chinese National Conference on Computational Linguistics2021</p>
<p>Unsupervised cross-lingual representation learning at scale. A Conneau, arXiv:1911.021162019arXiv preprint</p>
<p>Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. Y Sun, S Wang, S Feng, S Ding, C Pang, J Shang, J Liu, X Chen, Y Zhao, Y Lu, arXiv:2107.021372021arXiv preprint</p>
<p>S Soltan, S Ananthakrishnan, J Fitzgerald, R Gupta, W Hamza, H Khan, C Peris, S Rawls, A Rosenbaum, A Rumshisky, arXiv:2208.01448Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model. 2022arXiv preprint</p>
<p>Competition-level code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, T Eccles, J Keeling, F Gimeno, A Dal Lago, Science. 37866242022</p>
<p>A Askell, Y Bai, A Chen, D Drain, D Ganguli, T Henighan, A Jones, N Joseph, B Mann, N Dassarma, arXiv:2112.00861A general language assistant as a laboratory for alignment. 2021arXiv preprint</p>
<p>M R Costa-Jussà, J Cross, O Çelebi, M Elbayad, K Heafield, K Heffernan, E Kalbassi, J Lam, D Licht, J Maillard, arXiv:2207.04672No language left behind: Scaling human-centered machine translation. 2022arXiv preprint</p>
<p>A Glaese, N Mcaleese, M Trębacz, J Aslanides, V Firoiu, T Ewalds, M Rauh, L Weidinger, M Chadwick, P Thacker, arXiv:2209.14375Improving alignment of dialogue agents via targeted human judgements. 2022arXiv preprint</p>
<p>Training compute-optimal large language models. J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D Las Casas, L A Hendricks, J Welbl, A Clark, Proceedings of the 36th International Conference on Neural Information Processing Systems. the 36th International Conference on Neural Information Processing Systems2022</p>
<p>Opt-iml: Scaling language model instruction meta learning through the lens of generalization. S Iyer, X V Lin, R Pasunuru, T Mihaylov, D Simig, P Yu, K Shuster, T Wang, Q Liu, P S Koura, arXiv:2212.120172022arXiv preprint</p>
<p>J W Rae, S Borgeaud, T Cai, K Millican, J Hoffmann, F Song, J Aslanides, S Henderson, R Ring, S Young, arXiv:2112.11446Scaling language models: Methods, analysis &amp; insights from training gopher. 2021arXiv preprint</p>
<p>Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. Q Zheng, X Xia, X Zou, Y Dong, S Wang, Y Xue, Z Wang, L Shen, A Wang, Y Li, arXiv:2303.175682023arXiv preprint</p>
<p>T Wei, L Zhao, L Zhang, B Zhu, L Wang, H Yang, B Li, C Cheng, W Lü, R Hu, arXiv:2310.19341Skywork: A more open bilingual foundation model. 2023arXiv preprint</p>
<p>J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>E Nijkamp, H Hayashi, C Xiong, S Savarese, Y Zhou, arXiv:2305.02309Codegen2: Lessons for training llms on programming and natural languages. 2023arXiv preprint</p>
<p>Gpt-neox-20b: An open-source autoregressive language model. S Black, S Biderman, E Hallahan, Q G Anthony, L Gao, L Golding, H He, C Leahy, K Mcdonell, J Phang, Challenges {\&amp;} Perspectives in Creating Large Language Models. </p>
<p>M Reid, N Savinov, D Teplyashin, D Lepikhin, T Lillicrap, J.-B Alayrac, R Soricut, A Lazaridou, O Firat, J Schrittwieser, arXiv:2403.05530Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. 2024arXiv preprint</p>
<p>A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>R Thoppilan, D De Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, arXiv:2201.08239Lamda: Language models for dialog applications. 2022arXiv preprint</p>
<p>Jurassic-1: Technical details and evaluation. O Lieber, O Sharir, B Lenz, Y Shoham, White Paper. AI21 Labs. 192021</p>
<p>R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, arXiv:2112.09332Webgpt: Browser-assisted question-answering with human feedback. 2021arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 352022</p>
<p>Le Scao, T Fan, A Akiki, C Pavlick, E Ilić, S Hesslow, D Castagné, R Luccioni, A S Yvon, F Gallé, M , Bloom: A 176b-parameter open-access multilingual language model. 2023</p>
<p>Llama 3.1: An in-depth analysis of the next-generation large language model. R Vavekanand, K Sam, </p>
<p>Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. S Smith, M Patwary, B Norick, P Legresley, S Rajbhandari, J Casper, Z Liu, S Prabhumoye, G Zerveas, V Korthikanti, arXiv:2201.119902022arXiv preprint</p>
<p>G Team, T Mesnard, C Hardin, R Dadashi, S Bhupatiraju, S Pathak, L Sifre, M Rivière, M S Kale, J Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Automating code review activities by large-scale pre-training. Z Li, S Lu, D Guo, N Duan, S Jannu, G Jenks, D Majumder, J Green, A Svyatkovskiy, S Fu, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2022</p>
<p>Representation learning for stack overflow posts: How far are we?. J He, X Zhou, B Xu, T Zhang, K Kim, Z Yang, F Thung, I C Irsan, D Lo, ACM Transactions on Software Engineering and Methodology. 3332024</p>
<p>Ptm4tag: sharpening tag recommendation of stack overflow posts with pre-trained models. J He, B Xu, Z Yang, D Han, C Yang, D Lo, Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension. the 30th IEEE/ACM International Conference on Program Comprehension2022</p>
<p>Answer summarization for technical queries: Benchmark and new approach. C Yang, B Xu, F Thung, Y Shi, T Zhang, Z Yang, X Zhou, J Shi, J He, D Han, Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering. the 37th IEEE/ACM International Conference on Automated Software Engineering2022</p>
<p>B Roziere, J Gehring, F Gloeckle, S Sootla, I Gat, X E Tan, Y Adi, J Liu, T Remez, J Rapin, arXiv:2308.12950Code llama: Open foundation models for code. 2023arXiv preprint</p>
<p>Coderl: Mastering code generation through pretrained models and deep reinforcement learning. H Le, Y Wang, A D Gotmare, S Savarese, S C H Hoi, Advances in Neural Information Processing Systems. 352022</p>
<p>Computational approaches streamlining drug discovery. A V Sadybekov, V Katritch, Nature. 61679582023</p>
<p>Emerging frontiers in virtual drug discovery: From quantum mechanical methods to deep learning approaches. C Gorgulla, A Jayaraj, K Fackeldey, H Arthanari, Current opinion in chemical biology. 691021562022</p>
<p>Drug discovery companies are customizing chatgpt: here's how. N Savage, Nat Biotechnol. 4152023</p>
<p>Functional genomics for cancer drug target discovery. B Haley, F Roudnicky, Cancer Cell. 3812020</p>
<p>An omics perspective on drug target discovery platforms. J Paananen, V Fortino, Briefings in bioinformatics. 2162020</p>
<p>Deep learning for portfolio optimization. Z Zhang, S Zohren, S Roberts, The Journal of Financial Data Science. 2020</p>
<p>Machine learning for financial risk management: a survey. A Mashrur, W Luo, N A Zaidi, A Robles-Kelly, Ieee Access. 82020</p>
<p>. A Shah, P Raj, S P Pushpam Kumar, H Asha, Finaid, a financial advisor application using ai</p>
<p>Chatbots in customer service: Their relevance and impact on service quality. C V Misischia, F Poecze, C Strauss, Procedia Computer Science. 2012022</p>
<p>S Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D Rosenberg, G Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. 2023arXiv preprint</p>
<p>Large language models in medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, Nature medicine. 2982023</p>
<p>S V Chinta, Z Wang, X Zhang, T D Viet, A Kashif, M A Smith, W Zhang, arXiv:2407.19655Ai-driven healthcare: A survey on ensuring fairness and mitigating bias. 2024arXiv preprint</p>
<p>Towards expert-level medical question answering with large language models. K Singhal, T Tu, J Gottweis, R Sayres, E Wulczyn, L Hou, K Clark, S Pfohl, H Cole-Lewis, D Neal, arXiv:2305.096172023arXiv preprint</p>
<p>The promise of large language models in health care. A Arora, A Arora, The Lancet. 40164110377. 2023</p>
<p>I I Bommarito, M Katz, D M , arXiv:2212.14402Gpt takes the bar exam. 2022arXiv preprint</p>
<p>Chatgpt by openai: The end of litigation lawyers?. K Y Iu, V M Wong, -Y , Available at SSRN. 43398392023</p>
<p>U Lee, S Lee, J Koh, Y Jeong, H Jung, G Byun, Y Lee, J Moon, J Lim, H Kim, Generative Agent for Teacher Training: Designing Educational Problem-Solving Simulations with Large Language Model-based Agents for Pre-Service Teachers. NeurIPS</p>
<p>Gpteach: Interactive ta training with gptbased students. J M Markel, S G Opferman, J A Landay, C Piech, Proceedings of the Tenth Acm Conference on Learning@ Scale. the Tenth Acm Conference on Learning@ Scale2023</p>
<p>S V Chinta, Z Wang, Z Yin, N Hoang, M Gonzalez, T L Quy, W Zhang, arXiv:2407.18745Fairaied: Navigating fairness, bias, and ethics in educational ai applications. 2024arXiv preprint</p>
<p>Littlemu: Deploying an online virtual teaching assistant via heterogeneous sources integration and chain of teach prompts. S Tu, Z Zhang, J Yu, C Li, S Zhang, Z Yao, L Hou, J Li, Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. the 32nd ACM International Conference on Information and Knowledge Management2023</p>
<p>Y Chen, N Ding, H.-T Zheng, Z Liu, M Sun, B Zhou, arXiv:2309.08112Empowering private tutoring by chaining large language models. 2023arXiv preprint</p>
<p>Applied innovation: Artificial intelligence in higher education. A Zentner, Available at SSRN. 43141802022</p>
<p>Preparing educators and students for chatgpt and ai technology in higher education. B Zhang, 2023ResearchGate</p>
<p>Opinion paper:"so what if chatgpt wrote it?" multidisciplinary perspectives on opportunities, challenges and implications of generative conversational ai for research, practice and policy. Y K Dwivedi, N Kshetri, L Hughes, E L Slade, A Jeyaraj, A K Kar, A M Baabdullah, A Koohang, V Raghavan, M Ahuja, International Journal of Information Management. 711026422023</p>
<p>Artificial intelligence (ai) student assistants in the classroom: Designing chatbots to support student success. Y Chen, S Jensen, L J Albert, S Gupta, T Lee, Information Systems Frontiers. 2512023</p>
<p>B Yan, K Li, M Xu, Y Dong, Y Zhang, Z Ren, X Cheng, arXiv:2403.05156On protecting the data privacy of large language models (llms): A survey. 2024arXiv preprint</p>
<p>Extracting training data from large language models. N Carlini, F Tramer, E Wallace, M Jagielski, A Herbert-Voss, K Lee, A Roberts, T Brown, D Song, U Erlingsson, 30th USENIX Security Symposium (USENIX Security 21). 2021</p>
<p>Model inversion attacks that exploit confidence information and basic countermeasures. M Fredrikson, S Jha, T Ristenpart, Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security. the 22nd ACM SIGSAC Conference on Computer and Communications Security2015</p>
<p>Balancing chatgpt and data protection in germany: challenges and opportunities for policy makers. F Leboukh, E B Aduku, O Ali, Journal of Politics and Ethics in New Technologies and AI. 212023</p>
<p>Decoding the threat landscape: Chatgpt, fraudgpt, and wormgpt in social engineering attacks. P V Falade, arXiv:2310.055952023arXiv preprint</p>
<p>What is fraudgpt?. Z Amos, 2023</p>
<p>Wormgpt-the generative ai tool cybercriminals are using to launch business email compromise attacks. SlashNext. D Delley, August 24. 20232023</p>
<p>Missed opportunities in fair ai. N A Saxena, W Zhang, C Shahabi, Proceedings of the 2023 SIAM International Conference on Data Mining (SDM). the 2023 SIAM International Conference on Data Mining (SDM)SIAM2023</p>
<p>Individual fairness with group awareness under uncertainty. Z Wang, J Dzuong, X Yuan, Z Chen, Y Wu, X Yao, W Zhang, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Nature SwitzerlandSpringer2024</p>
<p>Toward fair graph neural networks via real counterfactual samples. Z Wang, M Qiu, M Chen, M B Salem, X Yao, W Zhang, Knowledge and Information Systems. 2024</p>
<p>Fg 2 an: Fairness-aware graph generative adversarial networks. Z Wang, C Wallace, A Bifet, X Yao, W Zhang, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Nature SwitzerlandSpringer2023</p>
<p>Preventing discriminatory decision-making in evolving data streams. Z Wang, N Saxena, T Yu, S Karki, T Zetty, I Haque, S Zhou, D Kc, I Stockwell, A Bifet, Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT). the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT)2023</p>
<p>Fairness in large language models: A taxonomic survey. Z Chu, Z Wang, W Zhang, ACM SIGKDD Explorations Newsletter. 2024. 2024</p>
<p>Fairness in large language models in three hours. T V Doan, Z Wang, M N Nguyen, W Zhang, Proceedings of the 33rd ACM International Conference on Information &amp; Knowledge Management. the 33rd ACM International Conference on Information &amp; Knowledge ManagementBoise, USA2024</p>
<p>Fairness with censorship and group constraints. W Zhang, J C Weiss, Knowledge and Information Systems. 2023</p>
<p>T Doan, Z Chu, Z Wang, W Zhang, arXiv:2407.18454Fairness definitions in language models explained. 2024arXiv preprint</p>
<p>Ai fairness in practice: Paradigm, challenges, and prospects. W Zhang, 2024Ai Magazine</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and Transparency2021</p>
<p>An empirical survey of the effectiveness of debiasing techniques for pre-trained language models. N Meade, E Poole-Dayan, S Reddy, arXiv:2110.085272021arXiv preprint</p>
<p>Bias and fairness in large language models: A survey. I O Gallegos, R A Rossi, J Barrow, M M Tanjim, S Kim, F Dernoncourt, T Yu, R Zhang, N K Ahmed, Computational Linguistics. 2024</p>
<p>Advancing graph counterfactual fairness through fair representation learning. Z Wang, Z Chu, R Blanco, Z Chen, S.-C Chen, W Zhang, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Nature SwitzerlandSpringer2024</p>
<p>Racial disparity in natural language processing: A case study of social media african-american english. S L Blodgett, B O'connor, arXiv:1707.000612017arXiv preprint</p>
<p>Bias against 93 stigmatized groups in masked language models and downstream sentiment classification tasks. K Mei, S Fereidooni, A Caliskan, Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. the 2023 ACM Conference on Fairness, Accountability, and Transparency2023</p>
<p>D Dash, R Thapa, J M Banda, A Swaminathan, M Cheatham, M Kashyap, N Kotecha, J H Chen, S Gombar, L Downing, arXiv:2304.13714Evaluation of gpt-3.5 and gpt-4 for supporting realworld information needs in healthcare delivery. 2023arXiv preprint</p>
<p>Med-halt: Medical domain hallucination test for large language models. A Pal, L K Umapathi, M Sankarasubbu, Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL). the 27th Conference on Computational Natural Language Learning (CoNLL)2023</p>
<p>Uncertain boundaries: Multidisciplinary approaches to copyright issues in generative ai. J Dzuong, Z Wang, W Zhang, arXiv:2404.082212024arXiv preprint</p>
<p>S Yazdani, N Saxena, Z Wang, Y Wu, W Zhang, A comprehensive survey of image and video generative ai: Recent advances, variants, and applications. 2024</p>
<p>Sarah silverman sues openai and meta over copyright infringement. Z Small, The New York Times. 2023</p>
<p>NY Times sues openai, Microsoft for infringing copyrighted works. J Stempel, 2023Thomson Reuters Corporation</p>
<p>Protecting intellectual property of large language model-based code generation apis via watermarks. Z Li, C Wang, S Wang, C Gao, Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security. the 2023 ACM SIGSAC Conference on Computer and Communications Security2023</p>            </div>
        </div>

    </div>
</body>
</html>
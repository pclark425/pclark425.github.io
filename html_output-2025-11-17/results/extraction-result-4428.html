<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4428 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4428</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4428</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-280700497</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.15658v2.pdf" target="_blank">SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation</a></p>
                <p><strong>Paper Abstract:</strong> The rapid growth of academic literature makes the manual creation of scientific surveys increasingly infeasible. While large language models show promise for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To bridge this critical gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for scientific survey generation in computer science. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers. In addition, we propose an automated evaluation framework that measures the quality of generated surveys across four dimensions: comprehensiveness, citation accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based methods demonstrates a significant performance gap, revealing that even advanced agentic frameworks struggle with the complexities of survey generation and highlighting the need for future research in this area. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4428.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4428.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Comprehensiveness (Reference Recall)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comprehensiveness measured as Reference Recall against Ground-Truth Bibliography</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantifies how well a generated survey covers the core literature by computing recall of cited references versus an expert-written ground-truth survey's reference list.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Reference Recall (R)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute set recall between the ground-truth reference set R_GT and the generated survey's reference set R_G: R = |R_GT ∩ R_G| / |R_GT|. The GT bibliography (extracted from expert surveys' LaTeX/BibTeX) is treated as a pragmatic proxy for the canonical literature for that topic; recall measures what fraction of those expert-cited works the system identified and cited.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Coverage of central literature (recall of GT citations); interpreted as pragmatic proxy for comprehensiveness rather than absolute completeness.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-14B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (automated scientific survey generation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Literature survey / explanatory synthesis / taxonomies (survey-level theories/explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Retriever upper bound: Recall@100 = 0.3665 (36.65% of GT refs available in top-100); Retriever Recall@1000 = 0.6805. End-to-end generated-survey recall (Table 3): RAG = 0.0214, AutoSurvey = 0.0351, StepSurvey = 0.0630 (StepSurvey best). These show large drop from retriever coverage to citations actually included.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (set-based metric using extracted GT bibliographies). Human experts created the GT bibliographies; evaluation itself is automated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>GT bibliographies were extracted from high-quality, high-citation surveys; dataset construction included human annotation to filter usable surveys (Cohen's kappa = 0.792). Retrieval performance compared to BM25 to establish bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>GT reference lists are not perfectly complete and may vary by author; recall measures only overlap with this proxy, so omissions might reflect legitimate alternate literatures. Metric ignores relevance ordering, contribution of each reference, and quality of integration of cited work into text.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SurGE benchmark: 205 expert-verified ground-truth surveys (avg. 65.8 citations per survey) and an arXiv metadata corpus of 1,086,992 papers used as retrieval pool.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4428.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4428.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citation Accuracy (NLI-based, 3-level)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Three-level Citation Accuracy measured via Natural Language Inference (document / section / sentence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated NLI-based procedure that assesses whether each cited paper is thematically relevant (document), appropriate for the section (section), and supports the local sentence claim (sentence), mapping NLI label probabilities to numeric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Three-level Citation Accuracy (Doc-Acc, Sec-Acc, Sent-Acc) via NLI</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each citation r with title T_r and abstract A_r and for generated survey S, construct a constant premise: 'There is a paper. Title: T_r. Abstract: A_r.' Then form three hypothesis templates for document-level relevance (paper relevant to survey title), section-level relevance (paper relevant to section title), and sentence-level support (paper supports sentence claim). Use a pretrained NLI model (cross-encoder nli-deberta-v3-base) to get probabilities for ENTAILMENT/NEUTRAL/CONTRADICTION. Map outcomes per citation and level to scores: 1 if ENTAILMENT highest; 0.5 if NEUTRAL highest and ENTAILMENT second; 0 otherwise. Aggregate by averaging over all citation instances to produce Doc-Acc R_d, Sec-Acc R_s, Sent-Acc R_t.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Document-level thematic relevance; Section-level topical appropriateness; Sentence-level evidential support for the specific claim.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-14B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (scientific survey generation; factual grounding of citations)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evidence-supported explanatory claims within surveys (sentence-level factual claims and citation linking)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Table 3 (per-baseline): RAG — Doc-Acc 0.2857, Sec-Acc 0.2502, Sent-Acc 0.2500; AutoSurvey — Doc-Acc 0.3617, Sec-Acc 0.4935, Sent-Acc 0.4870; StepSurvey — Doc-Acc 0.4576, Sec-Acc 0.4571, Sent-Acc 0.4636. AutoSurvey highest on section/sentence accuracy; StepSurvey highest on document-level accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (NLI classifier). Human-created GT bibliographies and human prompts guide definition of relevance, but scoring is performed automatically by NLI model. Special-case rules (presence in corpus, presence in GT bibliography) are applied deterministically.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Adopts NLI as a standard method for semantic/factual relation detection (cites literature); no explicit correlation study with human relevance judgments reported in paper. Dataset-level annotation reliability reported (Cohen's kappa) for GT selection, not for NLI scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>NLI models can be brittle; mapping discrete label ranking to scalar scores is coarse. NLI verdict depends on the abstract and may miss relevance not captured in abstracts. Presence-in-corpus and presence-in-GT heuristics produce deterministic scores (hallucination = 0, GT-cited immediate doc-level 1) which simplifies complex cases. Sentence-level evaluation assumes citation explicitly supports single sentence claims — many citations serve broader purposes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SurGE (205 GT surveys + 1,086,992-paper corpus); evaluation uses title/abstract metadata for NLI premises.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4428.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4428.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structure Quality Score (SQS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structure Quality Score using LLM-as-a-Judge (GPT-4o) with rubric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-level structural evaluation where an LLM (GPT-4o) is prompted with generated and ground-truth outlines and instructed, using a detailed rubric, to score structural alignment on a 0–5 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Structure Quality Score (SQS) — LLM-as-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provide GPT-4o with hierarchical outlines of the generated survey and the ground-truth survey plus a scoring rubric; ask it to assign a 0–5 quality score that accounts for alignment of hierarchical organization, topical progression, and semantic coherence. The rubric and the exact prompt template are provided (Appendix E). SQS produces a single scalar per survey.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>High-level outline alignment: hierarchical correspondence, topical progression, semantic coherence across sections.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-14B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (survey structure evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Organizational/structural quality of explanatory syntheses (survey outlines)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Table 3: SQS scores — RAG: 0.6829, AutoSurvey: 1.3902 (best), StepSurvey: 1.1951. AutoSurvey achieved highest structure score under GPT-4o judging.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (LLM-as-judge). Human-designed rubric guides LLM scoring; no additional human ratings used for final SQS reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>SQS prompt and rubric are human-designed (Appendix E). Paper does not report an external validation correlating SQS with human expert structural judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Subjectivity of LLM-as-judge; dependence on prompt phrasing and LLM biases; potential lack of transparency/reproducibility unless prompt and LLM version fixed; sensitive to LLM capability and contextual length limits.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SurGE benchmark; SQS applied per instance comparing generated outline to GT outline from the 205 ground-truth surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4428.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4428.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Soft-Heading Recall (SHR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Soft-Heading Recall via soft-cardinality of semantic embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-grained metric that measures how well generated section headings semantically cover ground-truth headings using embedding cosine similarities and soft-set cardinality computations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Soft-Heading Recall (SHR)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute soft cardinality S(A) of heading sets using pairwise cosine similarities of heading embeddings, where each heading's contribution is down-weighted by similarity with other headings. SHR = S(H_P ∩ H_GT) / S(H_GT) computed via S(H_P ∩ H_GT) = S(H_P) + S(H_GT) - S(H_P ∪ H_GT). This rewards paraphrased matches and penalizes redundant headings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Granular heading alignment: semantic coverage of GT headings by predicted headings, robustness to paraphrase and redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-14B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (structural/outline similarity evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Taxonomic/structural representations (heading-level alignment of survey outlines)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Table 3 SHR: RAG 0.7900, AutoSurvey 0.9697, StepSurvey 0.9763 (StepSurvey best by SHR).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (embedding-based semantic similarity). Embedding model used for similarity not explicitly specified in main text; comparison is purely metric-driven.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Methodologically inspired by soft-set similarity literature; paper does not report correlation of SHR with human judgments but justifies semantic similarity approach to handle paraphrase.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Depends on quality and domain-fit of the embedding model (not specified); threshold-free but sensitive to similarity calibration; may miscount semantically related but non-equivalent headings; ignores ordering/hierarchy depth beyond membership.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to SurGE outlines (205 GT surveys with parsed hierarchical headings).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4428.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4428.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Content Quality Score (CQS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Content Quality Score using LLM-as-a-Judge (GPT-4o) across five criteria</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Section-level quality scoring by GPT-4o across fluency/coherence, logical clarity, redundancy avoidance, clarity of description, and absence of errors, averaged across sections to produce a survey-level CQS (0–5 scale).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Content Quality Score (CQS) — LLM-as-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Score each survey section with GPT-4o guided by a five-item rubric (fluency & coherence, logical clarity, avoidance of redundancy, clarity of description, absence of errors). Each section is scored 0–5; due to context length, scoring is done section-by-section and final CQS is the average across sections. Supplementary n-gram metrics (ROUGE, BLEU) are also computed as secondary checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Fluency and coherence, logical clarity/argument progression, non-redundancy, clarity of descriptions, absence of grammatical/factual errors; supplemented by ROUGE/BLEU n-gram overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-14B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (survey content evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Explanatory content in surveys (quality of prose and logical exposition)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Table 3: CQS — RAG 4.6723, AutoSurvey 4.7390, StepSurvey 4.8451 (StepSurvey best). ROUGE-L and BLEU reported as secondary metrics: RAG R-L 0.1519 BLEU 10.38; AutoSurvey R-L 0.1578 BLEU 10.44; StepSurvey R-L 0.1590 BLEU 12.02.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated via LLM-as-judge (GPT-4o) guided by human-designed rubric; ROUGE/BLEU automated. No large-scale human expert scoring of generated content reported for direct comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Rubric and prompt provided (Appendix E); dataset curation used human annotators (Cohen's kappa 0.792) but CQS itself not validated against independent expert ratings in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Subjectivity and instability of LLM evaluators; context window constraints require per-section evaluation which may miss cross-section coherence; ROUGE/BLEU imperfect for long-form semantic quality and can reward surface overlap rather than true conceptual fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>CQS applied on SurGE 205-instance benchmark; supplementary ROUGE/BLEU comparisons to GT surveys provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4428.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4428.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval Recall@k</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval Recall at top-k (Recall@k) for assessing retrieval upper bound</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recall@k measures the fraction of ground-truth references present in the top-k retrieved documents and is used to quantify an upper bound for downstream survey comprehensiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Recall@k (retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each topic query, retrieve top-k documents using a retriever and compute the proportion of the ground-truth cited papers that appear within those top-k results. Used to diagnose retrieval bottlenecks and set an upper bound on possible reference coverage for the generation stage.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Presence/coverage of GT references within retrieved set (measures retrieval effectiveness and upper-bound for downstream recall).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-14B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (IR component of survey generation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Information retrieval coverage for literature synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Table 2: BM25 vs Paper Retriever: BM25 Recall@100 = 0.1193, Paper Retriever Recall@100 = 0.3665. Retriever Recall@1000 = 0.6805. Paper Retriever substantially outperforms BM25 but still leaves a retrieval ceiling.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (retrieval metric); human involvement limited to GT selection.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison to lexical baseline BM25; analysis of retrieval ceiling impact on downstream generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Even high Recall@k does not guarantee the generator will incorporate retrieved items; GT references missing from arXiv metadata (≈30% missing) limit achievable recall; retrieval biases impact downstream evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SurGE corpus (1,086,992 arXiv metadata entries) and 205 GT surveys for computing Recall@k.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4428.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4428.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Surface n-gram metrics (ROUGE/BLEU)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE-L and BLEU n-gram overlap metrics as supplementary fidelity checks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Established n-gram overlap metrics used as secondary measures of content fidelity between generated and ground-truth surveys, reported alongside primary metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ROUGE-L / BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute ROUGE-L (longest common subsequence-based recall/precision/F1) and BLEU (n-gram precision with brevity penalty) between generated survey text and ground-truth survey; used as supplementary indicators of textual similarity and content overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Surface-level lexical overlap and fluency proxies; measures n-gram-level correspondence between generated and GT texts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-14B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science (text generation quality)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Textual fidelity of survey content (surface-level similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Table 3: ROUGE-L: RAG 0.1519, AutoSurvey 0.1578, StepSurvey 0.1590. BLEU: RAG 10.38, AutoSurvey 10.44, StepSurvey 12.02.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated. Presented as supplementary metrics; primary evaluation favors multi-dimensional, semantics-focused metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Well-established metrics with known properties; paper notes they are secondary and limited for long-form evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Poorly suited as primary measures for long-form surveys because they reward surface overlap and fail to capture factual accuracy, structure, and integration of references.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Computed against SurGE 205 GT surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Factscore: Fine-grained atomic evaluation of factual precision in long form text generation <em>(Rating: 2)</em></li>
                <li>Veriscore: Evaluating the factuality of verifiable claims in long-form text generation <em>(Rating: 2)</em></li>
                <li>LongGenBench: Benchmarking longform generation in long context LLMs <em>(Rating: 2)</em></li>
                <li>Hellobench: Evaluating long text generation capabilities of large language models <em>(Rating: 2)</em></li>
                <li>Expert: Effective and explainable evaluation of personalized long-form text generation <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for large language models: A survey <em>(Rating: 1)</em></li>
                <li>Towards a Robust Deep Neural Network in Texts: A Survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4428",
    "paper_id": "paper-280700497",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Comprehensiveness (Reference Recall)",
            "name_full": "Comprehensiveness measured as Reference Recall against Ground-Truth Bibliography",
            "brief_description": "Quantifies how well a generated survey covers the core literature by computing recall of cited references versus an expert-written ground-truth survey's reference list.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Reference Recall (R)",
            "evaluation_method_description": "Compute set recall between the ground-truth reference set R_GT and the generated survey's reference set R_G: R = |R_GT ∩ R_G| / |R_GT|. The GT bibliography (extracted from expert surveys' LaTeX/BibTeX) is treated as a pragmatic proxy for the canonical literature for that topic; recall measures what fraction of those expert-cited works the system identified and cited.",
            "evaluation_criteria": "Coverage of central literature (recall of GT citations); interpreted as pragmatic proxy for comprehensiveness rather than absolute completeness.",
            "model_name": "Qwen2.5-14B-Instruct",
            "model_size": "14B",
            "scientific_domain": "Computer Science (automated scientific survey generation)",
            "theory_type": "Literature survey / explanatory synthesis / taxonomies (survey-level theories/explanations)",
            "human_comparison": true,
            "evaluation_results": "Retriever upper bound: Recall@100 = 0.3665 (36.65% of GT refs available in top-100); Retriever Recall@1000 = 0.6805. End-to-end generated-survey recall (Table 3): RAG = 0.0214, AutoSurvey = 0.0351, StepSurvey = 0.0630 (StepSurvey best). These show large drop from retriever coverage to citations actually included.",
            "automated_vs_human_evaluation": "Automated (set-based metric using extracted GT bibliographies). Human experts created the GT bibliographies; evaluation itself is automated.",
            "validation_method": "GT bibliographies were extracted from high-quality, high-citation surveys; dataset construction included human annotation to filter usable surveys (Cohen's kappa = 0.792). Retrieval performance compared to BM25 to establish bounds.",
            "limitations_challenges": "GT reference lists are not perfectly complete and may vary by author; recall measures only overlap with this proxy, so omissions might reflect legitimate alternate literatures. Metric ignores relevance ordering, contribution of each reference, and quality of integration of cited work into text.",
            "benchmark_dataset": "SurGE benchmark: 205 expert-verified ground-truth surveys (avg. 65.8 citations per survey) and an arXiv metadata corpus of 1,086,992 papers used as retrieval pool.",
            "uuid": "e4428.0",
            "source_info": {
                "paper_title": "SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Citation Accuracy (NLI-based, 3-level)",
            "name_full": "Three-level Citation Accuracy measured via Natural Language Inference (document / section / sentence)",
            "brief_description": "Automated NLI-based procedure that assesses whether each cited paper is thematically relevant (document), appropriate for the section (section), and supports the local sentence claim (sentence), mapping NLI label probabilities to numeric scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Three-level Citation Accuracy (Doc-Acc, Sec-Acc, Sent-Acc) via NLI",
            "evaluation_method_description": "For each citation r with title T_r and abstract A_r and for generated survey S, construct a constant premise: 'There is a paper. Title: T_r. Abstract: A_r.' Then form three hypothesis templates for document-level relevance (paper relevant to survey title), section-level relevance (paper relevant to section title), and sentence-level support (paper supports sentence claim). Use a pretrained NLI model (cross-encoder nli-deberta-v3-base) to get probabilities for ENTAILMENT/NEUTRAL/CONTRADICTION. Map outcomes per citation and level to scores: 1 if ENTAILMENT highest; 0.5 if NEUTRAL highest and ENTAILMENT second; 0 otherwise. Aggregate by averaging over all citation instances to produce Doc-Acc R_d, Sec-Acc R_s, Sent-Acc R_t.",
            "evaluation_criteria": "Document-level thematic relevance; Section-level topical appropriateness; Sentence-level evidential support for the specific claim.",
            "model_name": "Qwen2.5-14B-Instruct",
            "model_size": "14B",
            "scientific_domain": "Computer Science (scientific survey generation; factual grounding of citations)",
            "theory_type": "Evidence-supported explanatory claims within surveys (sentence-level factual claims and citation linking)",
            "human_comparison": true,
            "evaluation_results": "Table 3 (per-baseline): RAG — Doc-Acc 0.2857, Sec-Acc 0.2502, Sent-Acc 0.2500; AutoSurvey — Doc-Acc 0.3617, Sec-Acc 0.4935, Sent-Acc 0.4870; StepSurvey — Doc-Acc 0.4576, Sec-Acc 0.4571, Sent-Acc 0.4636. AutoSurvey highest on section/sentence accuracy; StepSurvey highest on document-level accuracy.",
            "automated_vs_human_evaluation": "Automated (NLI classifier). Human-created GT bibliographies and human prompts guide definition of relevance, but scoring is performed automatically by NLI model. Special-case rules (presence in corpus, presence in GT bibliography) are applied deterministically.",
            "validation_method": "Adopts NLI as a standard method for semantic/factual relation detection (cites literature); no explicit correlation study with human relevance judgments reported in paper. Dataset-level annotation reliability reported (Cohen's kappa) for GT selection, not for NLI scoring.",
            "limitations_challenges": "NLI models can be brittle; mapping discrete label ranking to scalar scores is coarse. NLI verdict depends on the abstract and may miss relevance not captured in abstracts. Presence-in-corpus and presence-in-GT heuristics produce deterministic scores (hallucination = 0, GT-cited immediate doc-level 1) which simplifies complex cases. Sentence-level evaluation assumes citation explicitly supports single sentence claims — many citations serve broader purposes.",
            "benchmark_dataset": "SurGE (205 GT surveys + 1,086,992-paper corpus); evaluation uses title/abstract metadata for NLI premises.",
            "uuid": "e4428.1",
            "source_info": {
                "paper_title": "SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Structure Quality Score (SQS)",
            "name_full": "Structure Quality Score using LLM-as-a-Judge (GPT-4o) with rubric",
            "brief_description": "High-level structural evaluation where an LLM (GPT-4o) is prompted with generated and ground-truth outlines and instructed, using a detailed rubric, to score structural alignment on a 0–5 scale.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Structure Quality Score (SQS) — LLM-as-Judge",
            "evaluation_method_description": "Provide GPT-4o with hierarchical outlines of the generated survey and the ground-truth survey plus a scoring rubric; ask it to assign a 0–5 quality score that accounts for alignment of hierarchical organization, topical progression, and semantic coherence. The rubric and the exact prompt template are provided (Appendix E). SQS produces a single scalar per survey.",
            "evaluation_criteria": "High-level outline alignment: hierarchical correspondence, topical progression, semantic coherence across sections.",
            "model_name": "Qwen2.5-14B-Instruct",
            "model_size": "14B",
            "scientific_domain": "Computer Science (survey structure evaluation)",
            "theory_type": "Organizational/structural quality of explanatory syntheses (survey outlines)",
            "human_comparison": true,
            "evaluation_results": "Table 3: SQS scores — RAG: 0.6829, AutoSurvey: 1.3902 (best), StepSurvey: 1.1951. AutoSurvey achieved highest structure score under GPT-4o judging.",
            "automated_vs_human_evaluation": "Automated (LLM-as-judge). Human-designed rubric guides LLM scoring; no additional human ratings used for final SQS reported.",
            "validation_method": "SQS prompt and rubric are human-designed (Appendix E). Paper does not report an external validation correlating SQS with human expert structural judgments.",
            "limitations_challenges": "Subjectivity of LLM-as-judge; dependence on prompt phrasing and LLM biases; potential lack of transparency/reproducibility unless prompt and LLM version fixed; sensitive to LLM capability and contextual length limits.",
            "benchmark_dataset": "SurGE benchmark; SQS applied per instance comparing generated outline to GT outline from the 205 ground-truth surveys.",
            "uuid": "e4428.2",
            "source_info": {
                "paper_title": "SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Soft-Heading Recall (SHR)",
            "name_full": "Soft-Heading Recall via soft-cardinality of semantic embeddings",
            "brief_description": "A fine-grained metric that measures how well generated section headings semantically cover ground-truth headings using embedding cosine similarities and soft-set cardinality computations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Soft-Heading Recall (SHR)",
            "evaluation_method_description": "Compute soft cardinality S(A) of heading sets using pairwise cosine similarities of heading embeddings, where each heading's contribution is down-weighted by similarity with other headings. SHR = S(H_P ∩ H_GT) / S(H_GT) computed via S(H_P ∩ H_GT) = S(H_P) + S(H_GT) - S(H_P ∪ H_GT). This rewards paraphrased matches and penalizes redundant headings.",
            "evaluation_criteria": "Granular heading alignment: semantic coverage of GT headings by predicted headings, robustness to paraphrase and redundancy.",
            "model_name": "Qwen2.5-14B-Instruct",
            "model_size": "14B",
            "scientific_domain": "Computer Science (structural/outline similarity evaluation)",
            "theory_type": "Taxonomic/structural representations (heading-level alignment of survey outlines)",
            "human_comparison": true,
            "evaluation_results": "Table 3 SHR: RAG 0.7900, AutoSurvey 0.9697, StepSurvey 0.9763 (StepSurvey best by SHR).",
            "automated_vs_human_evaluation": "Automated (embedding-based semantic similarity). Embedding model used for similarity not explicitly specified in main text; comparison is purely metric-driven.",
            "validation_method": "Methodologically inspired by soft-set similarity literature; paper does not report correlation of SHR with human judgments but justifies semantic similarity approach to handle paraphrase.",
            "limitations_challenges": "Depends on quality and domain-fit of the embedding model (not specified); threshold-free but sensitive to similarity calibration; may miscount semantically related but non-equivalent headings; ignores ordering/hierarchy depth beyond membership.",
            "benchmark_dataset": "Applied to SurGE outlines (205 GT surveys with parsed hierarchical headings).",
            "uuid": "e4428.3",
            "source_info": {
                "paper_title": "SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Content Quality Score (CQS)",
            "name_full": "Content Quality Score using LLM-as-a-Judge (GPT-4o) across five criteria",
            "brief_description": "Section-level quality scoring by GPT-4o across fluency/coherence, logical clarity, redundancy avoidance, clarity of description, and absence of errors, averaged across sections to produce a survey-level CQS (0–5 scale).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Content Quality Score (CQS) — LLM-as-Judge",
            "evaluation_method_description": "Score each survey section with GPT-4o guided by a five-item rubric (fluency & coherence, logical clarity, avoidance of redundancy, clarity of description, absence of errors). Each section is scored 0–5; due to context length, scoring is done section-by-section and final CQS is the average across sections. Supplementary n-gram metrics (ROUGE, BLEU) are also computed as secondary checks.",
            "evaluation_criteria": "Fluency and coherence, logical clarity/argument progression, non-redundancy, clarity of descriptions, absence of grammatical/factual errors; supplemented by ROUGE/BLEU n-gram overlap.",
            "model_name": "Qwen2.5-14B-Instruct",
            "model_size": "14B",
            "scientific_domain": "Computer Science (survey content evaluation)",
            "theory_type": "Explanatory content in surveys (quality of prose and logical exposition)",
            "human_comparison": true,
            "evaluation_results": "Table 3: CQS — RAG 4.6723, AutoSurvey 4.7390, StepSurvey 4.8451 (StepSurvey best). ROUGE-L and BLEU reported as secondary metrics: RAG R-L 0.1519 BLEU 10.38; AutoSurvey R-L 0.1578 BLEU 10.44; StepSurvey R-L 0.1590 BLEU 12.02.",
            "automated_vs_human_evaluation": "Automated via LLM-as-judge (GPT-4o) guided by human-designed rubric; ROUGE/BLEU automated. No large-scale human expert scoring of generated content reported for direct comparison.",
            "validation_method": "Rubric and prompt provided (Appendix E); dataset curation used human annotators (Cohen's kappa 0.792) but CQS itself not validated against independent expert ratings in the paper.",
            "limitations_challenges": "Subjectivity and instability of LLM evaluators; context window constraints require per-section evaluation which may miss cross-section coherence; ROUGE/BLEU imperfect for long-form semantic quality and can reward surface overlap rather than true conceptual fidelity.",
            "benchmark_dataset": "CQS applied on SurGE 205-instance benchmark; supplementary ROUGE/BLEU comparisons to GT surveys provided.",
            "uuid": "e4428.4",
            "source_info": {
                "paper_title": "SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Retrieval Recall@k",
            "name_full": "Retrieval Recall at top-k (Recall@k) for assessing retrieval upper bound",
            "brief_description": "Recall@k measures the fraction of ground-truth references present in the top-k retrieved documents and is used to quantify an upper bound for downstream survey comprehensiveness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Recall@k (retrieval)",
            "evaluation_method_description": "For each topic query, retrieve top-k documents using a retriever and compute the proportion of the ground-truth cited papers that appear within those top-k results. Used to diagnose retrieval bottlenecks and set an upper bound on possible reference coverage for the generation stage.",
            "evaluation_criteria": "Presence/coverage of GT references within retrieved set (measures retrieval effectiveness and upper-bound for downstream recall).",
            "model_name": "Qwen2.5-14B-Instruct",
            "model_size": "14B",
            "scientific_domain": "Computer Science (IR component of survey generation)",
            "theory_type": "Information retrieval coverage for literature synthesis",
            "human_comparison": true,
            "evaluation_results": "Table 2: BM25 vs Paper Retriever: BM25 Recall@100 = 0.1193, Paper Retriever Recall@100 = 0.3665. Retriever Recall@1000 = 0.6805. Paper Retriever substantially outperforms BM25 but still leaves a retrieval ceiling.",
            "automated_vs_human_evaluation": "Automated (retrieval metric); human involvement limited to GT selection.",
            "validation_method": "Comparison to lexical baseline BM25; analysis of retrieval ceiling impact on downstream generation.",
            "limitations_challenges": "Even high Recall@k does not guarantee the generator will incorporate retrieved items; GT references missing from arXiv metadata (≈30% missing) limit achievable recall; retrieval biases impact downstream evaluation.",
            "benchmark_dataset": "SurGE corpus (1,086,992 arXiv metadata entries) and 205 GT surveys for computing Recall@k.",
            "uuid": "e4428.5",
            "source_info": {
                "paper_title": "SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Surface n-gram metrics (ROUGE/BLEU)",
            "name_full": "ROUGE-L and BLEU n-gram overlap metrics as supplementary fidelity checks",
            "brief_description": "Established n-gram overlap metrics used as secondary measures of content fidelity between generated and ground-truth surveys, reported alongside primary metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "ROUGE-L / BLEU",
            "evaluation_method_description": "Compute ROUGE-L (longest common subsequence-based recall/precision/F1) and BLEU (n-gram precision with brevity penalty) between generated survey text and ground-truth survey; used as supplementary indicators of textual similarity and content overlap.",
            "evaluation_criteria": "Surface-level lexical overlap and fluency proxies; measures n-gram-level correspondence between generated and GT texts.",
            "model_name": "Qwen2.5-14B-Instruct",
            "model_size": "14B",
            "scientific_domain": "Computer Science (text generation quality)",
            "theory_type": "Textual fidelity of survey content (surface-level similarity)",
            "human_comparison": true,
            "evaluation_results": "Table 3: ROUGE-L: RAG 0.1519, AutoSurvey 0.1578, StepSurvey 0.1590. BLEU: RAG 10.38, AutoSurvey 10.44, StepSurvey 12.02.",
            "automated_vs_human_evaluation": "Automated. Presented as supplementary metrics; primary evaluation favors multi-dimensional, semantics-focused metrics.",
            "validation_method": "Well-established metrics with known properties; paper notes they are secondary and limited for long-form evaluation.",
            "limitations_challenges": "Poorly suited as primary measures for long-form surveys because they reward surface overlap and fail to capture factual accuracy, structure, and integration of references.",
            "benchmark_dataset": "Computed against SurGE 205 GT surveys.",
            "uuid": "e4428.6",
            "source_info": {
                "paper_title": "SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation",
                "publication_date_yy_mm": "2025-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Factscore: Fine-grained atomic evaluation of factual precision in long form text generation",
            "rating": 2,
            "sanitized_title": "factscore_finegrained_atomic_evaluation_of_factual_precision_in_long_form_text_generation"
        },
        {
            "paper_title": "Veriscore: Evaluating the factuality of verifiable claims in long-form text generation",
            "rating": 2,
            "sanitized_title": "veriscore_evaluating_the_factuality_of_verifiable_claims_in_longform_text_generation"
        },
        {
            "paper_title": "LongGenBench: Benchmarking longform generation in long context LLMs",
            "rating": 2,
            "sanitized_title": "longgenbench_benchmarking_longform_generation_in_long_context_llms"
        },
        {
            "paper_title": "Hellobench: Evaluating long text generation capabilities of large language models",
            "rating": 2,
            "sanitized_title": "hellobench_evaluating_long_text_generation_capabilities_of_large_language_models"
        },
        {
            "paper_title": "Expert: Effective and explainable evaluation of personalized long-form text generation",
            "rating": 2,
            "sanitized_title": "expert_effective_and_explainable_evaluation_of_personalized_longform_text_generation"
        },
        {
            "paper_title": "Retrieval-augmented generation for large language models: A survey",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_generation_for_large_language_models_a_survey"
        },
        {
            "paper_title": "Towards a Robust Deep Neural Network in Texts: A Survey",
            "rating": 1,
            "sanitized_title": "towards_a_robust_deep_neural_network_in_texts_a_survey"
        }
    ],
    "cost": 0.01655475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Pre-print Version SURGE: A BENCHMARK AND EVALUATION FRAME-WORK FOR SCIENTIFIC SURVEY GENERATION
4 Oct 2025</p>
<p>Weihang Su 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Anzhe Xie 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Qingyao Ai 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Jianming Long 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Jiaxin Mao 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Ziyi Ye 
Institute of Trustworthy Embodied AI
Fudan University</p>
<p>Yiqun Liu 
Department of Computer Science and Technology
Tsinghua University</p>
<p>Pre-print Version SURGE: A BENCHMARK AND EVALUATION FRAME-WORK FOR SCIENTIFIC SURVEY GENERATION
4 Oct 2025183EB08E665FF15F23890578D96EBA1DarXiv:2508.15658v2[cs.CL]
The rapid growth of academic literature makes the manual creation of scientific surveys increasingly infeasible.While large language models show promise for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols.To bridge this critical gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for scientific survey generation in computer science.SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers.In addition, we propose an automated evaluation framework that measures the quality of generated surveys across four dimensions: comprehensiveness, citation accuracy, structural organization, and content quality.Our evaluation of diverse LLM-based methods demonstrates a significant performance gap, revealing that even advanced agentic frameworks struggle with the complexities of survey generation and highlighting the need for future research in this area. 1.</p>
<p>INTRODUCTION</p>
<p>The volume of scientific literature has been expanding at an unprecedented rate in recent years.For instance, academic archives like arXiv now receive over a thousand new computer science papers daily, more than doubled between 2019 and 2024 (Liang et al., 2025).This rapid growth in publications has made the manual creation of comprehensive survey papers increasingly impractical, as the manual collection and synthesis of large volumes of relevant papers is both labor-intensive and time-consuming.Faced with this challenge, there is a growing need for automated systems that can generate survey papers effectively.While recent advancements in LLM-based agents (Yao et al., 2023;Wang et al., 2024c) offer significant promise for automating this task, their full potential is severely hindered by the lack of reliable, scalable, and standardized evaluation benchmarks.Currently, the evaluation of automatically generated surveys largely depends on human-expert reviews, which limit the reproducibility and objectivity of assessments (Tian et al., 2024a).Consequently, progress in this field is difficult to quantify, and the relative merits of different methods remain challenging to compare without standardized, multi-faceted evaluation benchmarks.</p>
<p>To address these challenges and fill this research gap, we introduce SurGE (Survey Generation Evaluation), a novel benchmark that establishes a reproducible standard for the survey generation task, which we formalize as a two-stage process: (1) retrieving relevant papers for a given topic from a large corpus, and (2) synthesizing them into a well-structured survey.To support standardized evaluation of both stages, SurGE provides a comprehensive dataset consisting of two key components.The first is a large-scale academic corpus containing over one million computer science papers to be used as the search and retrieval pool.The second is a collection of test instances, where each instance comprises a research topic (e.g., "Machine Learning for Information Retrieval") and its To enable standardized benchmarking of the survey generation task, SurGE offers a comprehensive framework that formalizes the process.The benchmark includes 205 expert-selected topics, each paired with a topic description t and a ground-truth survey for evaluation.SurGE also provides a large-scale academic corpus D containing over one million papers, which supports the document retrieval stage.The generated surveys are then evaluated through an automated system, which quantitatively assesses their quality across four key dimensions.Details of the dataset construction process are outlined in Section 3, while the evaluation framework is explained in Section 4.</p>
<p>DATASET CONSTRUCTION</p>
<p>GROUND-TRUTH SURVEY COLLECTION AND EXPERT ANNOTATION</p>
<p>To construct the SurGE benchmark, we began by collecting a diverse set of high-quality reference surveys (ground-truth surveys) from recent computer science literature.Candidate texts were drawn from the arXiv repository, focusing on publications between 2020 and 2024 that self-identified as survey articles or systematic reviews.To ensure the academic significance and reliability of each instance, we applied the following selection criteria: (i) the document must explicitly declare itself as a survey or review; (ii) it must have achieved a minimum citation count of 20, indicating scholarly impact (Bornmann &amp; Daniel, 2008); and (iii) the publication date must be between 2020 and 2024.</p>
<p>Following the initial filtering process, we further refine the SurGE dataset through expert annotation.This process aims to assess not only the citation-based impact of each candidate survey but also its quality from the perspective of experienced researchers.To this end, we recruited a team of four computer science Ph.D. students as annotators.Note that annotating a paper did not require a close reading of the entire document.Annotators could typically complete the task within 8-9 minutes per paper on average.Each candidate document was evaluated by two independent annotators along four key dimensions: (i) citation impact, reflecting the scholarly influence of the paper; (ii) content coverage, indicating how comprehensively the survey summarizes the literature within its scope; (iii) structural coherence, assessing the logical organization and clarity of the document's sections; and (iv) citation quality, which examines the relevance, diversity, and traceability of cited works.</p>
<p>Each annotator labeled the document as either "usable" or "not usable."A survey paper was included in the final dataset only if both annotators independently marked it as usable.In cases of disagreement, the paper was discarded to maintain a conservative quality threshold.To ensure fair and motivated participation, annotators were compensated based on working hours at an average rate of 60 CNY per hour, exceeding the local minimum wage in Beijing.Inter-annotator agreement was quantified using Cohen's Kappa, applied to 250 annotated instances.The resulting score of 0.792 indicates substantial agreement and reinforces the reliability of the quality control process.After this filtering stage, we finalized the dataset with 205 rigorously verified survey papers.The final set of 205 groundtruth surveys included in our benchmark is detailed in Appendix F. For each of the 205 selected ground-truth surveys, we provide its complete reference list, which serves as the gold standard for evaluating the comprehensiveness of a generated survey.The detailed methodology for extracting and processing these references is described in Appendix A.</p>
<p>ACADEMIC CORPUS CONSTRUCTION</p>
<p>A crucial component of the SurGE benchmark is a large-scale academic corpus designed to serve as the retrieval pool for the document collection stage.Our corpus is built entirely from scholarly metadata obtained from the arXiv repository.To ensure adherence to ethical and legal standards, we exclusively collected metadata and did not include full-text PDFs, a practice permitted by arXiv's Terms of Use, which designates metadata as public domain under the CC0 license (arx, 2025b).</p>
<p>The corpus was constructed through a two-stage process.The initial stage involved seeding the corpus with the references from our 205 ground-truth surveys.We systematically retrieved the arXiv metadata for every cited paper that was publicly accessible.This process revealed that approximately 30% of the references were unavailable, primarily due to publication in closed-access journals or other restricted venues.In the second stage, we expanded the corpus to enhance its comprehensiveness.We queried the official arXiv search API, using keywords and titles from the ground-truth surveys to identify and collect metadata for other topically related papers.This methodology resulted in a final retrieval corpus of 1,086,992 unique papers.For each paper, the corpus provides rich metadata, including its title, authors, abstract, subject categories, publication date, and a direct link to its arXiv page for transparency and verification.To ensure high data quality, all collected metadata underwent a rigorous pre-processing pipeline involving text normalization, formatting removal, and deduplication.</p>
<p>STATISTICS AND ANALYSIS</p>
<p>The resulting SurGE benchmark comprises 205 ground-truth survey papers and a retrieval corpus of 1,086,992 documents.Table 1a presents the key statistics of our curated dataset.To quantitatively analyze the organizational complexity of the surveys, we model the hierarchical section headings (e.g., Section, Subsection) of each paper as a tree structure.Our analysis reveals that these surveys are structurally deep, with an average tree depth of 3.07 and a mean of 42.7 nodes (i.e., distinct sections) per document.This structural complexity presents a significant challenge for hierarchical text generation.Furthermore, the surveys are densely referenced, citing an average of 65.8 papers, which underscores the demand for high-recall information collection.Table 1b details the pre-processed fields for each survey instance, which include not only standard metadata but also the parsed structural tree and the ground-truth list of cited documents, enabling a fine-grained, multi-faceted evaluation of system-generated surveys.The title of the survey paper.</p>
<p>Year</p>
<p>The publication year of the survey.</p>
<p>Date</p>
<p>The timestamp of publication.Category Subject classification following the arXiv taxonomy.</p>
<p>Abstract</p>
<p>The abstract of the survey paper.</p>
<p>Structure</p>
<p>Hierarchical representation of the survey.</p>
<p>All Cites</p>
<p>List of document IDs cited in the survey.</p>
<p>ETHICAL CONSIDERATIONS AND LICENSING</p>
<p>Our corpus is constructed exclusively from arXiv-provided descriptive metadata (titles, authors, abstracts, identifiers, categories, and license URIs) harvested via the official API.We do not host or redistribute arXiv PDFs or source files.This design complies with arXiv's API Terms of Use, which place descriptive metadata under a CC0 public-domain dedication (arx, 2025b).This design is also consistent with the arXiv Submittal Agreement's CC0 designation for metadata (arx, 2025a).</p>
<p>We have prioritized transparency, reproducibility, and ethical considerations throughout dataset construction.To support open science, we have publicly released the SurGE dataset, accompanying metadata, and all associated processing scripts on our official GitHub repository2 .The dataset and codebase are distributed under the MIT license, granting researchers and developers unrestricted access and modification rights.Regular updates will ensure continued relevance and alignment with evolving research trends and standards.</p>
<p>EVALUATION FRAMEWORK</p>
<p>To comprehensively evaluate the quality of automatically generated scientific surveys, we propose a multi-faceted evaluation framework.This framework assesses survey quality across four crucial dimensions: Comprehensiveness, Citation Accuracy, Structural Quality, and Content Quality.Each generated survey is evaluated against an expert-written Ground Truth (GT) survey.The following subsections define the quantitative metrics for each dimension in detail.</p>
<p>COMPREHENSIVENESS</p>
<p>The comprehensiveness of a scientific survey is a critical quality factor, as the omission of key publications can undermine its credibility and value.To quantify this aspect, we evaluate the Recall of a generated survey's references against the ground-truth reference lists.Formally, let R GT be the set of references in an expert-written GT survey and R G be the set of references in our generated survey.Recall R is defined as:
R = |R GT ∩ R G | |R GT | ,(1)
While the GT reference set is not assumed to be perfectly complete, it serves as the best available proxy for expert consensus on a topic's core literature, given that our GT surveys are highly cited, peer-reviewed publications (detailed in §3.1).We therefore interpret this metric not as a measure of absolute completeness, but as a pragmatic metric for evaluating a system's ability to identify the central body of work validated by the research community.</p>
<p>CITATION ACCURACY</p>
<p>Citation accuracy is another critical aspect of a high-quality survey.Each citation in a survey should be thematically relevant to the overall topic of the survey, and it must be contextually appropriate in terms of both the section and sentence in which it appears.To evaluate this aspect, we introduce the metric of Citation Accuracy, which evaluates each citation in the survey across three levels.First, at the document level, we assess whether a cited paper is thematically relevant to the overall topic of the survey.Second, at the section level, we evaluate whether a citation is placed in a semantically appropriate section of the survey.Finally, at the sentence level, we verify whether a citation supports the specific claim made in the sentence where it is cited.</p>
<p>To automate this evaluation, we employ a Natural Language Inference (NLI) model (nli-deberta-v3-base3 ) to assess the relevance of each citation.An NLI model is designed to determine the logical relationship between two text snippets: a premise and a hypothesis.The model then predicts the relationship between these two components, providing probabilities for the following labels: ENTAILMENT, NEUTRAL, CONTRADICTION.Due to its ability to capture semantic relationships, NLI has become a standard method for evaluating the factual consistency and relevance of text generated by LLMs, rendering it an ideal tool for our task.</p>
<p>For our specific evaluation, we implement this three-level check by framing it as a series of NLI tasks.</p>
<p>For each citation r (with title T r and abstract A r ) within the generated survey S (with title T S ), we construct a set of premise-hypothesis pairs.The premise is consistently formulated using the content of the cited paper, providing the evidentiary basis for the claim.The hypothesis is specifically tailored to assert relevance at each of the three levels (document, section, and sentence).This formulation is structured as follows:</p>
<p>NLI Task Formulation</p>
<p>Premise (Consistent for all levels): There is a paper.Title: "Tr".Abstract: Ar.</p>
<p>Hypotheses (Tailored for each level of granularity):</p>
<p>• Document-level: The paper titled "Tr" with the given abstract is thematically relevant to the survey titled: "TS".</p>
<p>• Section-level: The paper titled "Tr" with the given abstract is relevant to the section: "Section Title".</p>
<p>• Sentence-level: The paper titled "Tr" with the given abstract supports the claim made in the sentence: "Sentence Text".</p>
<p>The score for each citation unit is calculated via a multi-step process.Let R denote the set of all citation instances in the generated survey.For each citation r ∈ R, we compute a score at each of the three levels: document (y d (r)), section (y s (r)), and sentence (y t (r)).The calculation proceeds as follows.First, we resolve two special cases without querying the NLI model.Any citation r not found in our academic corpus is classified as a hallucination and assigned a score of y x (r) = 0 at all levels x ∈ {d, s, t}.Conversely, any citation r that is present in the ground-truth survey's bibliography is assigned a document-level score of y d (r) = 1.For all other cases, the relevance score y x (r) is determined by the NLI model's output probabilities for the labels ENTAILMENT, NEUTRAL, CONTRADICTION.The probabilities are mapped to a final score for each unit as follows:
y x (r) =    1,
if Entailment has the highest score; 0.5, if Neutral is highest and Entailment is second-highest; 0, otherwise.</p>
<p>x ∈ {d, s, t} (2)</p>
<p>Finally, we aggregate the individual citation scores to produce three final metrics for the survey: Document-level Accuracy (R d ), Section-level Accuracy (R s ), and Sentence-level Accuracy (R t ).For each level x ∈ {d, s, t}, the score R x is calculated as the mean of the individual citation scores y x (r) over all citation instances R in the survey:
R x = 1 |R| r∈R y x (r), x ∈ {d, s, t},(3)</p>
<p>STRUCTURAL QUALITY</p>
<p>The logical flow and coherence of a scientific survey are fundamentally determined by its structure.This makes structural quality a critical factor for readability and overall impact.To comprehensively evaluate structural quality, we introduce two complementary metrics that assess the generated outline at both macroscopic and microscopic levels.Our first metric, the Structure Quality Score (SQS), addresses the high-level organization.It holistically assesses the alignment between the generated and ground-truth outlines by comparing their overall structure, semantic coherence, and topical progression.Complementing this, our second metric, Soft-Heading Recall (SHR), provides a finegrained evaluation of heading alignment.It specifically measures how well the generated headings cover those in the ground-truth based on the similarity of semantic embeddings.</p>
<p>Structure Quality Score (SQS).SQS evaluates the overall quality of a generated survey's structure based on the hierarchical list of its section headings.To compute this score, we adopt the LLM-as-a-Judge paradigm, leveraging GPT-4o as the evaluator.Specifically, we provide the LLM with both the generated and ground-truth outlines and prompt it to assign a quality score.To guide the LLM's evaluation, we have carefully designed a detailed instruction prompt that includes a comprehensive scoring rubric on a scale from 0 to 5. The complete prompt is shown in Appendix E.</p>
<p>Soft-Heading Recall (SHR).</p>
<p>To measure fine-grained alignment, SHR evaluates how well the generated outline covers the specific headings present in the ground-truth outline.Unlike metrics based on exact lexical matching, SHR leverages semantic similarity to robustly handle variations in wording and paraphrasing.Formally, SHR is defined as the soft cardinality overlap between the predicted heading set (H P ) and the ground-truth heading set (H GT ):
SHR = S(H P ∩ H GT ) S(H GT ) ,(4)
where S(A) denotes the "soft cardinality" of a heading set A. Intuitively, this metric counts the number of semantically unique headings in a set.It achieves this by down-weighting redundant headings.Specifically, the contribution of each heading is inversely proportional to its aggregated similarity with all other headings in the set:
S(A) = K i=1 1 K j=1 sim(A i , A j )
.</p>
<p>(5)</p>
<p>Here, sim(A i , A j ) is the cosine similarity between the embeddings of headings A i and A j .A standard set intersection would be too strict for comparing paraphrased headings.Therefore, we define the soft intersection cardinality using the inclusion-exclusion principle:
S(H P ∩ H GT ) = S(H P ) + S(H GT ) − S(H P ∪ H GT ).(6)
The core idea lies in the union term, S(H P ∪ H GT .When computed on the combined heading set, a predicted heading and a similar ground-truth heading mutually reduce the union's soft cardinality.This reduction directly quantifies their semantic overlap, allowing the metric to reward paraphrased matches.A higher SHR score thus indicates better granular alignment.</p>
<p>CONTENT QUALITY</p>
<p>To assess the content quality of generated scientific surveys, we propose the Content Quality Score (CQS) metric based on the LLM-as-a-Judge paradigm, leveraging GPT-4o to evaluate each section of the survey.The evaluation is based on five criteria: fluency and coherence, logical clarity, avoidance of redundancy, clarity of description, and absence of errors.To guide the LLM's evaluation, we designed a detailed instruction prompt for the LLM, which is provided in the Appendix E. Each section is scored on a scale of 0 to 5, where a higher score reflects superior fluency, logical progression, and clarity.Considering the context length limitations of the LLM, we have it score each survey section by section, and the final score is the average of the scores from all sections.</p>
<p>As supplementary measures, we also compute ROUGE and BLEU scores, which quantify n-gram overlap between the generated and ground-truth surveys.While these are well-established metrics in text generation, their role is secondary in our framework, serving as additional checks for content fidelity rather than a primary assessment method.</p>
<p>EXPERIMENTAL SETUP</p>
<p>In this section, we detail the implementation of our experiment.Each baseline follows a two-stage pipeline: (1) retrieving a set of potentially relevant papers for a given topic, and (2) organizing and summarizing the retrieved papers to produce a structured survey.For fair comparison, all baselines share the same dense retriever for the first stage.In the following subsections, we first describe the training of the shared Paper Retriever, followed by the baseline selection and implementation details.</p>
<p>PAPER RETRIEVER TRAINING</p>
<p>We employ a dual-encoder architecture for retrieval, initialized with roberta-base.The training process leverages the benchmark dataset, where each topic description t (introduced in §2) serves as the query q.For a query q and a paper abstract d, we construct their input representations by prepending the special token [CLS] and appending
[SEP]. Formally, let X(q) = [CLS] q [SEP] and X(d) = [CLS] d [SEP].
We then feed these tokens into roberta-base to obtain the contextualized embedding of the [CLS] token:
Emb(X) = transformer [CLS] (X).(7)
The similarity score between the q and the d is computed as the dot product of their embeddings:
S(q, d) = Emb(X(q)) ⊤ • Emb(X(d)).(8)
During training, each query Q is paired with the relevant documents d + from the ground truth paper to form the positive samples, while negative samples d − ∈ N are randomly sampled from the corpus.The retriever is optimized via the softmax cross-entropy loss:
L(Q, d + , N ) = − log exp S(Q, d + ) exp S(Q, d + ) + d − ∈N exp S(Q, d − ) .(9)
This objective encourages the model to assign higher scores to relevant papers while minimizing scores for irrelevant ones.After training, we use the trained retriever to retrieve the top-ranked papers for each query, thereby providing a collection of relevant papers for the subsequent generation stage.</p>
<p>BASELINES</p>
<p>We selected the following three survey generation baseline methods for our experiments.Detailed descriptions of each method are provided in the appendices.</p>
<p>• Retrieval-Augmented Generation (RAG): This straightforward approach retrieves relevant papers, summarizes them in chunks, and then merges these summaries to form the final survey.A detailed description is provided in Appendix D.1.• AutoSurvey (Wang et al., 2024d): This method employs a multi-stage, outline-driven pipeline.It first generates a high-level outline from retrieved papers and then iteratively expands and refines each section.Further details can be found in Appendix D.2. • StepSurvey (Lai et al., 2024b): This approach adopts a granular, step-by-step process.It starts by generating a title and primary headings, which then guide the incremental drafting of each subsection.The methodology is described in detail in Appendix D.3.</p>
<p>IMPLEMENTATION DETAILS.</p>
<p>For the training of Paper Retriever, we randomly split the dataset into a training set and a test set at a ratio of 4:1.We adopt the AdamW optimizer for model optimization, the learning rate is set to 5 × 10 −6 , and the epoch is set to 10.During the training process, we adopt mixed-precision (fp16) training.At inference time, each query retrieves the top 100 relevant papers according to the similarity score.The retriever is initialized using the pre-trained RoBERTa model (Liu et al., 2019).</p>
<p>To ensure a fair comparison, we utilize the Qwen2.5-14B-Instructmodel (Yang et al., 2024a) as the base LLM for all baseline methods.For the generation configuration of LLMs, all experiments are conducted using the publicly available implementations provided by Hugging Face.We utilize the</p>
<p>EXPERIMENTAL RESULTS</p>
<p>This section presents the detailed evaluation of the selected survey generation baselines.We first evaluate the performance of the shared Paper Retriever ( §6.1).This analysis is crucial as it establishes the theoretical upper-bound performance for the comprehensiveness metric and highlights the challenges posed by the initial retrieval stage.Following this, we assess the complete end-to-end performance of the three baselines ( §6.2) based on our proposed evaluation framework.</p>
<p>ANALYSIS OF RETRIEVAL PERFORMANCE</p>
<p>A crucial question in our two-stage pipeline is whether performance limitations stem from the retriever's inability to find relevant papers or the generator's inability to use them.To disentangle these factors and quantify the retrieval bottleneck, we evaluate the performance of our fine-tuned dense retriever in isolation.This analysis establishes the theoretical upper bound for the reference coverage that our end-to-end systems can achieve.</p>
<p>We compare our dense retriever against the lexical baseline BM25 (Robertson et al., 2009), using Recall@k as the evaluation metric.This metric measures the percentage of ground-truth papers from the reference survey that are present in the top-k retrieved documents.As shown in Table 2, our fine-tuned Paper Retriever substantially outperforms the BM25 baseline across all values of k.The performance gap underscores the inadequacy of lexical search, which struggles to find semantically relevant papers that may not share overlapping keywords.On the other hand, our dense retriever can capture deeper semantic relationships, thus it is far more effective than BM25.However, the results also reveal a critical bottleneck in the survey generation pipeline.Even when retrieving the top 1000 documents (k=1000), Paper Retriever's recall reaches only 68.05% of the ground-truth papers.This performance ceiling imposes a hard upper bound on the downstream generator, as it cannot synthesize information from papers it never receives.Consequently, this performance ceiling underscores that the survey generation task needs more sophisticated retrieval paradigms, such as employing search agents or multi-agent systems powered by large language models (Zhang et al., 2024;Tang et al., 2025;Li et al., 2025).</p>
<p>OVERALL PERFORMANCE OF SURVEY GENERATION BASELINES</p>
<p>Our evaluation now transitions from retrieval performance to the end-to-end survey generation task.While the retriever supplies the models with a substantial portion (36.65%) of the ground-truth references, a detailed analysis of the final surveys reveals a significant bottleneck in the generation stage.Table 3 presents a multi-faceted comparison of our three baselines across the four dimensions of survey quality defined by our benchmark.This comprehensive evaluation yields several key insights:</p>
<p>(1) The Generation Stage is the Primary Bottleneck.A striking finding is the dramatic performance drop-off between paper retrieval and final survey synthesis.Despite the retriever providing access to 36.65% of ground-truth references (Recall@100), the best-performing model, StepSurvey, incorporates only 6.30% of them into its final output.The standard RAG baseline fares even worse, with a final recall of just 2.14%.This stark disparity highlights that the core challenge lies not in finding relevant papers, but in the generator's ability to effectively identify and integrate crucial information from the retrieved set.Nevertheless, the results also affirm the value of structured pipelines, as AutoSurvey (3.51%) and StepSurvey (6.30%) significantly outperform the naive RAG approach in comprehensiveness.</p>
<p>(2) AutoSurvey Excels in Local Coherence and Structural Quality.AutoSurvey demonstrates particular strength in producing locally coherent and well-organized content.As shown in Table 3, it achieves the highest scores for both Section-level Citation Accuracy (Sec-Acc: 0.4935) and Sentencelevel Citation Accuracy (Sent-Acc: 0.4870).This suggests its iterative, section-by-section refinement process is highly effective at placing citations within their correct immediate context.Furthermore, their planning approach results in the highest Structure Quality Score (SQS) of 1.3902, indicating superior hierarchical organization in the generated survey.</p>
<p>(3) StepSurvey Achieves Superior Coverage and Global Content Quality.In contrast, StepSurvey's strengths lie in metrics related to holistic coverage and overall content quality.It attains the highest Recall (0.0630) and Document-level Citation Accuracy (Doc-Acc: 0.4576), demonstrating a better ability to cover the breadth of the topic and align cited works with the survey's main theme.Its multi-phase workflow that generates headings, then subtopics, and finally assigns citations leads to the best content quality.This is evidenced by its top scores in text similarity metrics (ROUGE-L: 0.1590, BLEU: 12.02) as well as the highest Content Quality Score (CQS) of 4.8451, which reflects stronger logical flow and presentation.</p>
<p>(4) A Trade-off Between Local Precision and Global Coverage.The distinct strengths of Auto-Survey and StepSurvey reveal a fundamental trade-off in planning strategies.AutoSurvey's iterative refinement of individual sections fosters high-quality local structure and precise, fine-grained citation placement.Conversely, StepSurvey's hierarchical, topic-first approach yields better overall topic coverage and more coherent, globally relevant content.While both advanced methods significantly outperform the standard RAG pipeline, neither excels across all dimensions.</p>
<p>In conclusion, our proposed SurGE benchmark effectively diagnoses the weaknesses of current survey generation systems.The results clearly indicate that advanced, multi-stage planning is crucial, but a substantial gap remains between machine-generated and expert-written surveys, particularly in comprehensiveness.The observed trade-off between local and global optimization strategies highlights a key challenge for future research: developing hybrid models that can combine the fine-grained accuracy of AutoSurvey with the broad thematic coverage and coherence of StepSurvey.</p>
<p>RELATED WORK</p>
<p>7.1 RETRIEVAL-AUGMENTED GENERATION Large Language Models (LLMs) are inherently limited by their static, pre-trained parametric knowledge.To address these limitations, Retrieval-Augmented Generation (RAG) has emerged as a key paradigm (Gao et al., 2023;Lewis et al., 2020;Dong et al., 2025;Tu et al., 2025;Su et al., 2025a).By grounding the model in external knowledge, RAG directly addresses several fundamental limitations of LLMs, offering a robust mechanism to mitigate hallucinations (Ji et al., 2023;Su et al., 2024d;b;Wang et al., 2025a), facilitate knowledge updating (Fang et al., 2024a;Wang et al., 2024b;a;2025b), and enable effective domain adaptation (Yang et al., 2024b;Su et al., 2025c;2024a;e).</p>
<p>The conventional approach to traditional RAG is built upon the "Retrieval-then-Read" paradigm (Borgeaud et al., 2022;Guu et al., 2020;Lewis et al., 2020).Within this framework, a user's query triggers a search module for relevant documents within a large-scale external corpus.This retrieval step is carried out by either an external retriever (Zhai, 2008;Su et al., 2023b;Robertson et al., 2009;Su et al., 2023a;Ma et al., 2023;Fang et al., 2024b) or a more sophisticated retrieval system (Su et al., 2023c;Salemi &amp; Zamani, 2024;Chen et al., 2022).Building upon this foundation, recent work has proposed more advanced RAG architectures to improve efficiency and effectiveness.</p>
<p>For instance, Dynamic RAG (Jiang et al., 2022;Su et al., 2024c;Yao et al., 2024) moves beyond a single retrieval step by adaptively triggering the retriever during generation, specifically when the LLM is uncertain during the generation process.From another angle, GraphRAG (Edge et al., 2024) enhances the knowledge source by querying pre-constructed knowledge graphs instead of unstructured text, allowing it to retrieve interconnected facts and relationships.Furthermore, the Parametric RAG paradigm (Su et al., 2025b;Tan et al., 2025;Fleshman &amp; Van Durme, 2025) alters the knowledge injection step by directly injecting retrieved knowledge into the LLM's parameters.</p>
<p>The scientific survey generation task, which is the focus of our SurGE benchmark, presents a significant challenge for even these advanced RAG systems.Unlike typical question-answering, survey generation demands the synthesis of a large, diverse set of documents into a coherent, wellstructured survey paper.Therefore, while RAG provides the foundational technology, our SurGE benchmark is specifically designed to push the boundaries of current models by rigorously evaluating their capabilities in large-scale multi-document synthesis and structured content creation.</p>
<p>LONG-FORM TEXT GENERATION AND EVALUATION</p>
<p>Long-form text generation is substantially more challenging than short-text generation due to its inherent requirements for sustained coherence and rich contextual understanding.Early approaches mainly used generative adversarial networks and reinforcement learning to conduct long-sequence generation (Guo et al., 2018).More recently, large language models have emerged as a strong tool for this task, offering advanced capabilities to handle long-text generation.For example, structured planning techniques and specialized inference mechanisms are proposed (Sloan et al., 2024;Jin et al., 2024) to generate consistent and high-quality clinical reports.Similarly, hierarchical planning frameworks have demonstrated that content control and multi-constraint instruction following can significantly enhance logical flow and topic coverage (Hu et al., 2022;Pham et al., 2024).Beyond medical or other task-specific applications, context-driven retrieval strategies, such as tree-structured retrieval, can support open-domain long-text generation by guiding the model through extensive knowledge sources (Roy et al., 2024).The effective evaluation framework is vital for measuring the quality, factualness, and user-centric utility of the long-form text generation task.Traditional metrics, designed for shorter texts, often fail to capture the intricacies of longer outputs.Recent work has introduced task-focused benchmarks that emphasize user-oriented objectives, such as personalized writing or domain-specific content generation (Kumar et al., 2024;Salemi et al., 2025).In parallel, factuality assessment has attracted growing interest, with methods proposed to evaluate both verifiable and unverifiable claims.Metrics such as VERISCORE and FACTSCORE break down generated text into atomic facts, checking each for consistency against reliable sources (Song et al., 2024;Min et al., 2023).Beyond factual correctness, coherence and structural quality have been studied extensively.</p>
<p>Benchmarks like LongGenBench and HelloBench underscore the importance of evaluating a model's ability to maintain logical organization and clarity over extended passages (Wu et al., 2025;Que et al., 2024).</p>
<p>SURVEY GENERATION</p>
<p>In the domain of scientific writing, survey generation involves distilling extensive textual resources into a coherent and structured overview.Recent advances in AI-assisted systems have provided prompting-based approaches to expedite the drafting process while preserving content accuracy (Gero et al., 2022;Kacena et al., 2024).One of the most commonly used approaches is retrieval-augmented generation, which combines large-scale knowledge retrieved from documents with language generation empowered by LLMs to yield factually comprehensive overviews (Lewis et al., 2021).Retrieval-augmented generation is often initiated with dense retrieval methods based on dual-encoder architectures to identify highly relevant documents (Karpukhin et al., 2020).Once these documents are retrieved, summarization techniques-spanning top-down, bottom-up, and graph-based rank-ing methods-play a pivotal role in producing concise yet faithful summaries (Nayeem &amp; Rafiei, 2024;Pang et al., 2022;Bleiweiss, 2023).Building on these retrieval and summarization-based methodologies, automated literature survey generation has garnered increasing attention (Tian et al., 2024a;Lai et al., 2024a).However, existing techniques depend on limited ground truths and employ coarse evaluation metrics, resulting in oversimplified assessments of survey quality (Wang et al., 2024d).To address these challenges, we present a refined ground truth and a multi-dimensional evaluation framework that emphasizes both accuracy and structural coherence.By evaluating quality through multiple dimensions, our proposed framework advances the capabilities of automated survey generation, offering a more comprehensive and rigorous approach to summarizing scientific literature.</p>
<p>CONCLUSION</p>
<p>In this paper, we introduce SurGE, a comprehensive benchmark designed to address the critical need for standardized and reproducible evaluation in automated scientific survey generation.SurGE provides a large-scale academic corpus, a set of expert-written ground-truth surveys, and a fully automated framework to evaluate surveys on comprehensiveness, citation accuracy, structure, and content.Our experiments reveal significant limitations in state-of-the-art LLM-based systems, highlighting challenges such as incomplete topic coverage and reference hallucination.We believe SurGE will catalyze future research and guide the development of more effective LLM-based systems at the intersection of information retrieval and generative AI for this important task.</p>
<p>ETHICS STATEMENT</p>
<p>The SurGE benchmark aims to facilitate the advancement of automated survey generation and evaluation in the context of scientific research.By utilizing publicly available metadata from arXiv, our work prioritizes transparency and open access.We do not host or redistribute arXiv's PDF content, ensuring compliance with arXiv's Terms of Use and the CC0 public-domain dedication for metadata.While the use of large-scale academic data in SurGE poses minimal ethical risks, we emphasize the importance of ethical data usage, privacy, and responsible development of AI systems.We encourage the research community to consider these aspects when applying or building upon SurGE.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>To ensure reproducibility, all code, data, and models for the SurGE benchmark are open-sourced and available at our official GitHub Repository5 .Detailed descriptions of the dataset construction, processing steps, and evaluation methods can be found at our official GitHub Repository and appendices.Our open-access approach is designed to facilitate the replication of results and encourage future research in the field of survey generation using large language models.</p>
<p>A REFERENCE EXTRACTION AND PROCESSING</p>
<p>For each of our selected ground-truth surveys, we extracted reference data from its LaTeX source and the associated BibTeX file.First, we parsed the LaTeX source files using custom regular expressions to extract all citation keys (e.g., \cite{...} commands).Next, we used these keys to look up the corresponding entries in the BibTeX files and retrieve their complete metadata, including titles, authors, and publication years.These metadata served as unique digital identifiers for each reference.Finally, to ensure data quality, we performed a cleaning step where we systematically removed duplicates and filtered out entries with inconsistencies, such as malformed or excessively long titles.</p>
<p>The core reason for extracting the full reference list from each ground-truth survey is to create a gold standard for evaluating the information collection stage.We operate on the premise that the citations in an expert-written survey represent a curated collection of the field's most foundational and relevant literature.By using this set as the ground truth, we can quantitatively measure the coverage and recall of a system-generated survey's references, providing a clear metric for its performance.</p>
<p>B DETAILS OF THE ACADEMIC CORPUS</p>
<p>Table 4 provides an overview of the metadata schema used in our academic corpus.Each entry is structured to support efficient retrieval and interpretability.</p>
<p>Table 4: Fields and Descriptions for the Literature Knowledge Base.</p>
<p>Key Description Title</p>
<p>The title of the research paper.</p>
<p>Authors</p>
<p>A list of contributing researchers.</p>
<p>Year</p>
<p>The publication year of the paper.</p>
<p>Date</p>
<p>The exact timestamp of the paper's release.</p>
<p>Abstract</p>
<p>The abstract of the paper.Category The subject classification following the arXiv taxonomy.doc id A unique identifier assigned for reference and retrieval.</p>
<p>C THE USE OF LARGE LANGUAGE MODELS</p>
<p>Large language models were used as a tool to help refine the language and improve the clarity of this manuscript.Following any such use, the authors reviewed and edited the content to ensure its accuracy and originality, and take full responsibility for the final text.</p>
<p>D BASELINE IMPLEMENTATION D.1 RETRIEVAL-AUGMENTED GENERATION</p>
<p>In the first baseline, we combine retrieval with a direct generation approach.Given a topic t, we use the above retriever to collect the top 100 candidate papers.To manage lengthy inputs, these retrieved papers are split into smaller groups, each containing an approximately equal number of references.We then prompt a large language model (LLM) to summarize each group separately, guiding it to preserve references to the original papers.Formally, for each group of papers G k = {d 1 , d 2 , . . ., d n k }, the LLM is conditioned on the sequence of paper abstracts and instructed to produce a partial summary Ŝk .Finally, we merge the partial summaries Ŝ1 , Ŝ2 , . . .into a unified survey.This merging step is performed by prompting the LLM once again with all partial summaries and asking for an integrated, logically coherent survey.Although this baseline follows a straightforward two-step approach, it provides a clear assessment of how effective retrieval-based summarization can be when coupled with an LLM's generative capabilities.</p>
<p>D.2 AUTOSURVEY</p>
<p>AutoSurvey (Wang et al., 2024d) implements a multi-stage survey generation pipeline that starts with a high-level outline and proceeds through iterative expansions.We adapt it to use our fine-tuned retriever in place of its original retrieval mechanism and keep the number of retrieved references consistent for fairness.In the adapted workflow, we first issue a query based on the topic t to retrieve an initial collection of papers P init .The LLM is then prompted to create a structured outline, which includes main sections and subsections tailored to the subject matter.Next, each section is expanded by conditioning on the subset of papers most relevant to that specific section, producing a draft that includes references in bracketed format (e.g., "[id]").Once each section is drafted, the LLM refines it to address factual inconsistencies, stylistic mismatches, and reference-formatting issues.Finally, all refined sections are merged into a coherent final survey, with transitions and citation references carefully aligned.The workflow iterates over these stages, leading to incremental improvements in both thematic coverage and presentation quality.</p>
<p>D.3 STEPSURVEY</p>
<p>StepSurvey (Lai et al., 2024b) is a more granular generation strategy that also begins with retrieving the top 100 candidate papers for a given topic t but proceeds through distinct planning and drafting phases in a sequential manner.This baseline is proposed by a team named "ID" in the NLPCC2024 competition task 6 (Tian et al., 2024b).Rather than producing an overarching outline at once, it starts by proposing a survey title and a set of primary headings that collectively capture the central themes of the retrieved literature.Subsequently, it uses the primary headings to guide the selection of secondary or finer-grained topics, each mapped to a relevant subset of the retrieved papers.The LLM then produces a full draft by writing each subsection with explicit attention to references and academic conventions, thereby encouraging greater control and consistency across sections.Throughout this process, the system attempts to maintain a balanced level of detail, striving for a clear exposition of important subtopics while avoiding excessive verbosity or redundancy.By structuring the content in incremental steps, StepSurvey aims to achieve coherent organization and thorough coverage of the literature.</p>
<p>E PROMPT TEMPLATE</p>
<p>This section provides the detailed prompt templates used for our LLM-as-a-Judge evaluation, as described in the main body of the paper.These prompts are specifically designed for GPT-4o to assess the Structure Quality Score (SQS) and the Content Quality of the generated surveys.</p>
<p>Prompt Template for Structure Quality Score</p>
<p>You are an AI evaluator.Your task is to compare the generated titles with the target titles and assign a score from 0 to 5 based on their similarity in structure, meaning, and wording.</p>
<p>Target Titles: {Ground Truth Titles} Generated Titles: {Generated Titles} Scoring Criteria:</p>
<p>5 -Almost Identical:</p>
<p>• Nearly all key words match exactly.</p>
<p>• The meaning is fully preserved.</p>
<p>• The phrasing and structure are identical or differ only in trivial ways.</p>
<p>-Very Similar:</p>
<p>• Most key words match.</p>
<p>• The meaning is nearly identical.</p>
<p>• The phrasing and structure are very close, with minor rewording.</p>
<p>-Similar:</p>
<p>• Several key words are shared.</p>
<p>• The meaning is largely the same with slight variations.</p>
<p>• The structure is somewhat similar, but there may be word substitutions.</p>
<p>-Somewhat Similar:</p>
<p>• Some key words are shared, but others are different.</p>
<p>• The general topic is the same, but the emphasis may differ.</p>
<p>• The sentence structures are different but not entirely unrelated.</p>
<p>1 -Somewhat Different:</p>
<p>• Few words overlap, but they are not key terms.</p>
<p>• The meaning is somewhat related but mostly different.</p>
<p>• The sentence structures are significantly different.</p>
<p>-Completely Different:</p>
<p>• Nearly no words in common.</p>
<p>• Completely different meanings.</p>
<p>• No similarity in structure or phrasing.</p>
<p>Instruction: Analyze the generated titles based on the criteria above and provide a single score between 0 and 5.</p>
<p>Prompt Template for Content Quality Score</p>
<p>You are an advanced AI language evaluator.Your task is to assess the logical coherence and clarity of the text based on the following criteria:</p>
<ol>
<li>
<p>Fluency and Coherence -Does the text flow naturally?Are the sentences well-connected and easy to read?</p>
</li>
<li>
<p>Logical Clarity -Is the reasoning clear and structured?Does the argument progress logically without contradictions?</p>
</li>
<li>
<p>Avoidance of Redundancy -Does the text avoid unnecessary repetition?</p>
</li>
<li>
<p>Clarity of Description -Are ideas, concepts, or events described in a way that is easy to understand? 5. Absence of Errors -Does the text contain grammatical mistakes, spelling errors, or factual inconsistencies?</p>
</li>
</ol>
<p>You will provide a score from 0 to 5 based on the following criteria:</p>
<p>-Excellent:</p>
<p>A score of 5 is awarded to texts that are highly fluent, featuring smooth transitions and a natural flow.The logical progression is clear, well-structured, and easy to follow.There is no redundancy; each sentence contributes meaningfully to the overall message.Furthermore, descriptions are precise and unambiguous, and the text is free of any spelling, grammatical, or factual errors.</p>
<p>-Good</p>
<p>A score of 4 indicates a text that is mostly fluent but may have minor awkward transitions.Its logical progression is clear, though it might contain slight inconsistencies.There may be some minor redundancy or repetition present.While descriptions are mostly clear, they could contain minor ambiguities, and the text has very few spelling or grammatical errors.</p>
<p>-Average</p>
<p>A score of 3 applies to texts that are understandable despite containing noticeable awkward phrasing.The logical flow is inconsistent, which may cause some points to feel out of place.Some redundancy or repetition is present and slightly affects readability.Additionally, certain descriptions are vague or unclear, and the text contains some spelling or grammatical mistakes but remains readable.</p>
<p>-Poor</p>
<p>A score of 2 is assigned when a text is difficult to read due to an awkward structure and poor fluency.Logical inconsistencies make the central argument unclear, and repetitive phrases render the content tedious.Descriptions are vague, making it hard to understand key points, and the submission is characterized by multiple grammatical and spelling errors.</p>
<p>-Very Poor</p>
<p>A score of 1 denotes a text that is highly disjointed, making it very hard to read.The logical flow is almost nonexistent, with abrupt topic shifts throughout.Redundant sentences are included but add no value to the content.Descriptions are confusing or overly vague, and the text suffers from frequent spelling and grammatical mistakes.</p>
<p>-Incoherent</p>
<p>A score of 0 is reserved for a text that is completely nonsensical or unreadable.It demonstrates no logical progression or coherence.The content exhibits extreme redundancy or devolves into word salad, and severe errors throughout make it impossible to understand any intended meaning.</p>
<p>Instruction</p>
<p>Now evaluate the following paragraph based on the criteria above and provide a score from 0 to 5.</p>
<p>Paragraph:</p>
<p>{Paragraph}</p>
<p>Table 1 :
1
Overview of the SurGE Benchmark.(a) Summary statistics of the curated survey dataset and its associated retrieval corpus.(b) Metadata of the pre-processed survey dataset used in SurGE.(a) Basic Statistics of the SurGE Dataset and Corpus
(b) Fields in the Pre-processed Survey DatasetStatisticNumberFieldDescriptionTotal Ground Truth Surveys205SurveyIDA unique identifier for the survey.Average Tree Depth3.073AuthorsList of contributing researchers.Maximum Tree Depth4TitleAverage Number of Tree Nodes42.717Maximum Number of Tree Nodes212Average Citations per Paper65.78Average Citations per Section1.577Corpus Size1,086,992Average Abstract Length (words)156.57</p>
<p>Table 2 :
2
Comparison of retrieval models on recalling ground-truth cited papers.The metric is Recall@k, where k is the number of top documents retrieved.Best results are in bold.
ModelRecall@20 Recall@30 Recall@100 Recall@200 Recall@500 Recall@1000BM250.05480.06520.11930.15960.22130.2715Paper Retriever0.17060.21450.36650.46810.60110.6805</p>
<p>Table 3 :
3
Main experimental results comparing different survey generation baselines across four dimensions: Comprehensiveness (Comp.),Citation Accuracy, Structural Quality, and Content Quality.Metrics include Recall, Document/Section/Sentence-level Citation Accuracy (Doc-Acc, Sec-Acc, Sent-Acc), Structure Quality Score (SQS), Soft-Heading Recall (SHR), ROUGE-L, BLEU, and Content Quality Score (CQS).The best results are in bold, and the second-best results are underlined.
Comp.Citation AccuracyStructural QualityContent QualityBaselineRecall Doc-Acc Sec-Acc Sent-AccSQSSHRR-LBLEUCQSRAG0.02140.28570.25020.25000.68290.79000.1519 10.38 4.6723AutoSurvey 0.03510.36170.49350.48701.39020.96970.1578 10.44 4.7390StepSruvey 0.06300.45760.45710.46361.19510.97630.1590 12.02 4.8451
default hyperparameters and the chat template as outlined in the official Hugging Face repository4.All experiments are conducted on a GPU server with 1TB RAM and eight NVIDIA A100 GPUs, each with 40GB of memory.</p>
<p>https://github.com/oneal2000/SurGE
https://huggingface.co/cross-encoder/nli-deberta-v3-base
https://huggingface.co/Qwen/Qwen2.5-14B-Instruct
https://github.com/oneal2000/SurGE
F SELECTED GROUND TRUTH SURVEYThe following three tables provide the complete list of the ground-truth surveys in the SurGE benchmark.For each entry, we list the title, publication year, primary arXiv category, and citation count.The provided citation counts represent a snapshot from Google Scholar on May 10, 2025.
. Arxiv submittal agreement. 2025a. 2025-08-20</p>
<p>Two-step text summarization for long-form biographical narrative genre. Avi Bleiweiss, 10.18653/v1/2023.codi-1.20Proceedings of the 4th Workshop on Computational Approaches to Discourse (CODI 2023). Michael Strube, Chloe Braud, Christian Hardmeier, Jessy Junyi, Sharid Li, Amir Loaiciga, Zeldes, the 4th Workshop on Computational Approaches to Discourse (CODI 2023)Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>Scholars before researchers: On the centrality of the dissertation literature review in research preparation. N David, Penny Boote, Beile, Educational researcher. 3462005</p>
<p>Improving language models by retrieving from trillions of tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, International conference on machine learning. PMLR2022</p>
<p>What do citation counts measure? a review of studies on citing behavior. Lutz Bornmann, Hans-Dieter Daniel, Journal of documentation. 6412008</p>
<p>Web search via an efficient and effective brain-machine interface. Xuesong Chen, Ziyi Ye, Xiaohui Xie, Yiqun Liu, Xiaorong Gao, Weihang Su, Shuqi Zhu, Yike Sun, Min Zhang, Shaoping Ma, Proceedings of the fifteenth ACM international conference on web search and data mining. the fifteenth ACM international conference on web search and data mining2022</p>
<p>Decoupling knowledge and context: An efficient and effective retrieval augmented generation framework via cross attention. Qian Dong, Qingyao Ai, Hongning Wang, Yiding Liu, Haitao Li, Weihang Su, Yiqun Liu, Tat-Seng Chua, Shaoping Ma, Proceedings of the ACM on Web Conference 2025. the ACM on Web Conference 20252025</p>
<p>From local to global: A graph rag approach to query-focused summarization. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Jonathan Larson, arXiv:2404.161302024arXiv preprint</p>
<p>Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Shi Jie, Xiang Wang, Xiangnan He, Tat-Seng Chua, arXiv:2410.02355Alphaedit: Null-space constrained knowledge editing for language models. 2024aarXiv preprint</p>
<p>Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, Yiqun Liu, arXiv:2403.18684Scaling laws for dense retrieval. 2024barXiv preprint</p>
<p>Lora-augmented generation (lag) for knowledgeintensive language tasks. William Fleshman, Benjamin Van Durme, arXiv:2507.053462025arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, Haofen Wang, arXiv:2312.1099722023arXiv preprint</p>
<p>Sparks: Inspiration for science writing using language models. Katy Ilonka, Gero , Vivian Liu, Lydia Chilton, 10.1145/3532106.3533533Proceedings of the 2022 ACM Designing Interactive Systems Conference, DIS '22. the 2022 ACM Designing Interactive Systems Conference, DIS '22New York, NY, USAAssociation for Computing Machinery2022ISBN 9781450393584</p>
<p>Long text generation via adversarial training with leaked information. Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, Jun Wang, 10.1609/aaai.v32i1.11957Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceApr. 201832</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Mingwei Chang, International conference on machine learning. PMLR2020</p>
<p>Planet: Dynamic content planning in autoregressive transformers for long-form text generation. Zhe Hu, Hou Pong Chan, Jiachen Liu, Xinyan Xiao, Hua Wu, Lifu Huang, 2022</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, ACM Computing Surveys. 55122023</p>
<p>Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer. Zhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding, Zhiruo Wang, Jamie Callan, Graham Neubig, arXiv:2212.020272022arXiv preprint</p>
<p>Promptmrg: Diagnosis-driven prompts for medical report generation. Haibo Jin, Haoxuan Che, Yi Lin, Hao Chen, 10.1609/aaai.v38i3.28038Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 202438</p>
<p>The use of artificial intelligence in writing scientific review articles. Melissa A Kacena, Lilian I Plotkin, Jill C Fehrenbacher, 10.1007/s11914-023-00852-0Current Osteoporosis Reports. 1544-2241221February 2024</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen Tau, Yih , 2020</p>
<p>Guidelines for performing systematic literature reviews in software engineering. Staffs Keele, ver. 2.32007Technical reportebse technical report. ebse</p>
<p>Longlamp: A benchmark for personalized long-form text generation. Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan A Rossi, Franck Dernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, Nedim Lipka, Chien Van Nguyen, Thien Huu Nguyen, Hamed Zamani, 2024</p>
<p>Instruct large language models to generate scientific literature survey step by step. Yuxuan Lai, Yupeng Wu, Yidan Wang, Wenpeng Hu, Chen Zheng, 2024a</p>
<p>Instruct large language models to generate scientific literature survey step by step. Yuxuan Lai, Yupeng Wu, Yidan Wang, Wenpeng Hu, Chen Zheng, CCF International Conference on Natural Language Processing and Chinese Computing. Springer2024b</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, 2021</p>
<p>Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, Zhicheng Dou, arXiv:2504.21776Webthinker: Empowering large reasoning models with deep research capability. 2025arXiv preprint</p>
<p>Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Shichao Song, Zehao Lin, Yebin Yang, Simin Niu, Hanyu Wang, arXiv:2502.14776Academic survey automation via large language models. 2025arXiv preprint</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Caseencoder: A knowledgeenhanced pre-trained model for legal case encoding. Yixiao Ma, Yueyue Wu, Weihang Su, Qingyao Ai, Yiqun Liu, arXiv:2305.053932023arXiv preprint</p>
<p>Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen Tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi, 2023</p>
<p>Lfosum: Summarizing long-form opinions with large language models. Mir Tafseer, Nayeem , Davood Rafiei, 2024</p>
<p>Long document summarization with top-down and bottom-up inference. Bo Pang, Erik Nijkamp, Wojciech Kryściński, Silvio Savarese, Yingbo Zhou, Caiming Xiong, 2022</p>
<p>Ten simple rules for writing a literature review. Marco Pautasso, PLoS computational biology. 97e10031492013</p>
<p>Suri: Multi-constraint instruction following for long-form text generation. Minh Chau, Simeng Pham, Mohit Sun, Iyyer, 2024</p>
<p>Hellobench: Evaluating long text generation capabilities of large language models. Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, Junran Peng, Zhaoxiang Zhang, Songyang Zhang, Kai Chen, 2024</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Contregen: Context-driven tree-structured retrieval for open-domain long-form text generation. Kashob Kumar, Roy , Pritom Saha Akash, Kevin Chen-Chuan, Chang , Lucian Popa, 2024</p>
<p>Towards a search engine for machines: Unified ranking for multiple retrieval-augmented large language models. Alireza Salemi, Hamed Zamani, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Expert: Effective and explainable evaluation of personalized long-form text generation. Alireza Salemi, Julian Killingback, Hamed Zamani, 2025</p>
<p>Automated radiology report generation: A review of recent advances. Phillip Sloan, Philip Clatworthy, Edwin Simpson, Majid Mirmehdi, IEEE Reviews in Biomedical Engineering. 2024</p>
<p>Veriscore: Evaluating the factuality of verifiable claims in long-form text generation. Yixiao Song, Yekyung Kim, Mohit Iyyer, 2024</p>
<p>Wikiformer: Pre-training with structured information of wikipedia for ad-hoc retrieval. Weihang Su, Qingyao Ai, Xiangsheng Li, Jia Chen, Yiqun Liu, Xiaolong Wu, Shengluan Hou, arXiv:2312.106612023aarXiv preprint</p>
<p>Caseformer: Pre-training for legal case retrieval. Weihang Su, Qingyao Ai, Yueyue Wu, Yixiao Ma, Haitao Li, Yiqun Liu, arXiv:2311.003332023barXiv preprint</p>
<p>Thuir2 at ntcir-16 session search (ss) task. Weihang Su, Xiangsheng Li, Yiqun Liu, Min Zhang, Shaoping Ma, arXiv:2307.002502023carXiv preprint</p>
<p>Stard: A chinese statute retrieval dataset with real queries issued by non-professionals. Weihang Su, Yiran Hu, Anzhe Xie, Qingyao Ai, Zibing Que, Ning Zheng, Yun Liu, Weixing Shen, Yiqun Liu, arXiv:2406.153132024aarXiv preprint</p>
<p>Mitigating entity-level hallucination in large language models. Weihang Su, Yichen Tang, Qingyao Ai, Changyue Wang, Zhijing Wu, Yiqun Liu, Proceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region. the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region2024b</p>
<p>Dragin: Dynamic retrieval augmented generation based on the real-time information needs of large language models. Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, Yiqun Liu, arXiv:2403.100812024carXiv preprint</p>
<p>Unsupervised real-time hallucination detection based on the internal states of large language models. Weihang Su, Changyue Wang, Qingyao Ai, Yiran Hu, Zhijing Wu, Yujia Zhou, Yiqun Liu, arXiv:2403.064482024darXiv preprint</p>
<p>Legalaid: A large language model for the chinese legal field. Weihang Su, Changyue Wang, Anzhe Xie, Qingyao Ai, Yiran Hu, Yiqun Liu, 2024e</p>
<p>Dynamic and parametric retrieval-augmented generation. Weihang Su, Qingyao Ai, Jingtao Zhan, Qian Dong, Yiqun Liu, Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval2025a</p>
<p>Parametric retrieval augmented generation. Weihang Su, Yichen Tang, Qingyao Ai, Junxi Yan, Changyue Wang, Hongning Wang, Ziyi Ye, Yujia Zhou, Yiqun Liu, Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval2025b</p>
<p>Judge: Benchmarking judgment document generation for chinese legal system. Weihang Su, Baoqing Yue, Qingyao Ai, Yiran Hu, Jiaqi Li, Changyue Wang, Kaiyuan Zhang, Yueyue Wu, Yiqun Liu, Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval2025c</p>
<p>Dynamic parametric retrieval augmented generation for test-time knowledge enhancement. Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, Kang Liu ; Yichen, Weihang Tang, Yujia Su, Yiqun Zhou, Min Liu, Shaoping Zhang, Qingyao Ma, Xungang Ai ; Yangjie Tian, Aijia Gu, He Li, Ruohua Zhang, Yunfeng Xu, Ming Li, Liu, 10.1007/978-981-97-9443-0_35arXiv:2503.23895Natural Language Processing and Chinese Computing: 13th National CCF Conference, NLPCC 2024. Hangzhou, China; Berlin, HeidelbergSpringer-Verlag2025. 2025. November 1-3, 2024. 2024aarXiv preprintProceedings, Part V</p>
<p>Overview of the nlpcc2024 shared task 6: Scientific literature survey generation. Yangjie Tian, Xungang Gu, Aijia Li, He Zhang, Ruohua Xu, Yunfeng Li, Ming Liu, CCF International Conference on Natural Language Processing and Chinese Computing. Springer2024b</p>
<p>Rbft: Robust fine-tuning for retrieval-augmented generation against retrieval defects. Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai, arXiv:2501.183652025arXiv preprint</p>
<p>Knowledge editing through chain-ofthought. Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu, arXiv:2412.177272024aarXiv preprint</p>
<p>Changyue Wang, Weihang Su, Qingyao Hu Yiran, Yueyue Ai, Cheng Wu, Yiqun Luo, Min Liu, Shaoping Zhang, Ma, arXiv:2407.14192Lekube: A legal knowledge update benchmark. 2024barXiv preprint</p>
<p>Joint evaluation of answer and reasoning consistency for hallucination detection in large reasoning models. Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu, arXiv:2506.04832arXiv:2506.00536Changyue Wang, Weihang Su, Qingyao Ai, Yujia Zhou, and Yiqun Liu. Decoupling reasoning and knowledge injection for in-context knowledge editing. 2025a. 2025barXiv preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 1861863452024c</p>
<p>Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, arXiv:2406.10252Large language models can automatically write surveys. 2024darXiv preprint</p>
<p>Analyzing the past to prepare for the future: Writing a literature review. Jane Webster, Richard T Watson, MIS quarterly. 2002</p>
<p>Longgenbench: Benchmarking longform generation in long context llms. Yuhao Wu, Ming Shan Hee, Zhiqing Hu, Roy Ka-Wei Lee, 2025</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.151152024a5 technical report. arXiv preprint</p>
<p>Tcm-gpt: efficient pretraining of large language models for domain adaptation in traditional chinese medicine. Guoxing Yang, Xiaohong Liu, Jianyu Shi, Zan Wang, Guangyu Wang, Computer Methods and Programs in Biomedicine Update. 61001582024b</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>Seakr: Self-aware knowledge retrieval for adaptive retrieval augmented generation. Zijun Yao, Weijian Qi, Liangming Pan, Shulin Cao, Linmei Hu, Weichuan Liu, Lei Hou, Juanzi Li, arXiv:2406.192152024arXiv preprint</p>
<p>Statistical language models for information retrieval. Chengxiang Zhai, Synthesis lectures on human language technologies. 112008</p>
<p>Large language model powered agents for information retrieval. An Zhang, Yang Deng, Yankai Lin, Xu Chen, Ji-Rong Wen, Tat-Seng Chua, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024111</p>
<p>CV 264 Transformers in Vision: A Survey 2021 cs.CV 3276 Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey 2019 cs.LG 433 Change Detection and Notification of Web Pages: A Survey. Deep Gait Recognition: A Survey 2021 cs. 2019</p>
<p>A Survey on Deep Learning-based Architecturesfor Semantic Segmentation on 2D images 2019 cs.CV 276 A Survey on Tiering and Caching in High-Performance Storage Systems 2019 cs.AR 28 Multimodal Learning with Transformers: A Survey. cs.CV 7662022</p>
<p>Attention, please! A survey of Neural Attention Models in Deep Learning 2021 cs.LG 251 Explanation-Based Human Debugging of NLP Models: A Survey 2021 cs. CL 80 Federated Learning in Mobile Edge Networks: A Comprehensive Survey 2019 cs.NI 2488 Deep Learning for Image Super-resolution:A Survey. 2019 cs.CV 2036</p>
<p>Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey 2021 cs.CV 25 Survey of Transient Execution Attacks. 2020</p>
<p>A Survey of Syntactic-Semantic Parsing Based on Constituent and Dependency Structures. cs.CL 472020</p>
<p>cs.AI 50Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better 2021 cs.LG 519 From Statistical Relational to Neurosymbolic Artificial Intelligence: a Survey. 2021 cs.AI 71 Symbolic Logic meets Machine Learning: A Brief Survey in Infinite Domains. 2020</p>
<p>Context Dependent Semantic Parsing: A Survey 2020 cs.CL 21 A survey of active learning algorithms for supervised remote sensing image classification. cs.CV 6512021</p>
<p>Generate FAIR Literature Surveys with Scholarly Knowledge Graphs 2020 cs.DL 53 A Survey of Deep Learning for Data Caching in Edge Network. cs.NI 392020</p>
<p>Frontiers of Machine Learning: A Survey on Developments and Opportunities 2021 cs. 29</p>
<p>MM 95 Weakly Supervised Object Localization and Detection: A Survey 2021 cs.CV 348 Compression of Deep Learning Models for Text: A Survey 2020 cs.CL 142 Computer Vision with Deep Learning for Plant Phenotyping in Agriculture: A Survey. NE 641CV 85 A Survey on Evolutionary Neural Architecture Search 2020 cs. 2019. 2020Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey</p>
<p>Towards Efficient SynchronousFederated Training: A Survey onSystem Optimization Strategies 2021 cs. DC. 34</p>
<p>From Distributed Machine Learning to Federated Learning: A Survey 2021 cs. 342</p>
<p>Image and Video Enhancement Using Deep Learning: A Survey 2021 cs.CV 522 A Survey of Coded Distributed Computing 2020 cs.DC 28 A Systematic Survey of Regularization and Normalization in GANs. Low-Light, cs.LG 562020</p>
<p>RO 127 A Survey of Transformers 2021 cs.LG 1641 How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design 2019 cs.HC 676 Community detection in node-attributed social networks: a∼survey 2019 cs.SI 324 A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review 2021 cs.IR 73 Learning from Noisy Labels with Deep Neural Networks: A Survey 2020 cs.LG 1380 A Survey on Split Manufacturing: Attacks, Defenses, and Challenges 2020 cs.CR 59 A Survey of Active Learning for Text Classification using Deep Neural Networks 2020 cs. CV 614 Proximity Perception in Human-Centered Robotics: A Survey on Sensing Systems and Applications 2021 cs. 2019Deep Learning for Deepfakes Creation and Detection: A Survey. CL 145 A Survey of Knowledge Tracing: Models, Variants, and Applications 2021 cs.CY 51</p>
<p>Dynamic Neural Networks: A Survey. cs.CV 8492021</p>
<p>CR 81 A Comprehensive Survey on Graph Anomaly Detection with Deep Learning 2021 cs.LG 775 Deep Learning for Instance Retrieval: A Survey 2021 cs.CV 292 Deep Learning for Vision-based Prediction: A Survey 2020 cs.CV 59 Survey Title Year Category Citation Count A survey of face recognition techniques under occlusion 2020 cs.CV 165 Blockchain for 5G and Beyond Networks: A State of the Art Survey 2019 cs.NI 440 A Survey of Knowledge-Enhanced Text Generation 2020 cs.CL 344 Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey. cs.AR 1122020. 2020Arms Race in Adversarial Malware Detection: A Survey</p>
<p>Graph Learning for Combinatorial Optimization: A Survey of State-of-the-Art 2020 cs.LG 113 Deep Gaussian Processes: A Survey. cs.LG 392021</p>
<p>CL 981 A Survey on In-context Learning 2022 cs.CL 1788 Centrality Measures in Complex Networks: A Survey 2020 cs.SI 139 A Survey on Adversarial Recommender Systems 2020 cs.IR 244 Towards a Robust Deep Neural Network in Texts: A Survey. CL 90 A Survey of Deep Active Learning 2020 cs.LG 1588 Translation Quality Assessment: A Brief Survey on Manual and Automatic Methods 2021 cs. 2021. 2019. 2020CL 56 Deep learning for scene recognition from visual data: a survey. CV 23 A Survey of State-of-the-Art on Blockchains: Theories, Modelings, and Tools 2020 cs.DC 170</p>
<p>Computation Offloading and Content Caching Delivery in Vehicular Edge Computing: A Survey 2019 cs.NI 101 Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey 2021 cs.CL 322 Explainable reinforcement learning for broad-XAI: a conceptual framework and survey. cs.AI 822021</p>
<p>End-to-End Constrained Optimization Learning: A Survey. cs.LG 2582021</p>
<p>Machine Learning in Generation, Detection, and Mitigation of Cyberattacks in Smart Grid: A Survey. cs.CR 312020</p>
<p>Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence 2021 cs.DC 153 A Survey of Deep Learning Approaches for OCR and Document Understanding 2020 cs. cs.LG 81CL 84 Taxonomy of Machine Learning Safety: A Survey and Primer. 2021</p>
<p>Neuron-level Interpretation of Deep NLP Models: A Survey. cs.CL 982021</p>
<p>Using Deep Learning to Solve Computer Security Challenges: A Survey. cs.CR 592019</p>
<p>Efficient Transformers: A Survey 2020 cs.LG 1500 A Survey of Label-noise Representation Learning: Past, Present and Future. 2020 csLG</p>
<p>A Survey of Constrained Gaussian Process Regression: Approaches and Implementation Challenges 2020 cs.LG 182 Neural Networks for Entity Matching: A Survey 2020 cs.DB 157 A Survey of Orthographic Information in Machine Translation 2020 cs.CL 37 A Survey of Quantum Theory Inspired Approaches to Information Retrieval 2020 cs.IR 42 A Survey of Deep Meta-Learning 2020 cs.LG 459 A Survey on Deep Learning Techniques for Video Anomaly Detection. 2020</p>
<p>HC 128 Applications of Auction and Mechanism Design in Edge Computing: A Survey 2021 cs.GT 76 Abduction and Argumentation for Explainable Machine Learning: A Position Survey 2020 cs.AI 19 A Survey of Exploration Methods in Reinforcement Learning 2021 cs.LG 133 Reinforcement Learning based Recommender Systems: A Survey 2021 cs.IR 597 A Survey of Data Augmentation Approaches for NLP 2021 cs.CL 1008 A Survey on Self-supervised Pre-training for Sequential Transfer Learning in Neural Networks. A Survey on Interactive Reinforcement Learning 2021 cs. 2020</p>
<p>SE 45 A Survey on Heterogeneous Graph Embedding. cs.SI 452A Survey on Theorem Provers in Formal Methods 2019 cs. 2020</p>
<p>cs.SE 94Asynchronous Federated Learning on Heterogeneous Devices: A Survey 2021 cs.DC 326 A Survey on Data-driven Software Vulnerability Assessment and Prioritization. 2021</p>
<p>LG 96 A Survey on Low-Resource Neural Machine Translation 2021 cs.CL 59 Benchmark and Survey of Automated Machine Learning Frameworks 2019 cs.LG 523 A Survey on Deep Learning Technique for Video Segmentation 2021 cs.CV 204 Universal Adversarial Perturbations: A Survey. 2020Time Series Data Imputation: A Survey on Deep Learning Approaches 2020 cs</p>
<p>cs.CR 73A Survey of Machine Learning Methods and Challenges for Windows Malware Classification. 2020</p>
<p>Serverless Computing: A Survey of Opportunities, Challenges, and Applications 2019 cs. NI</p>
<p>Algorithms and Applications to Network Motifs and Graphlets 2019 cs.DS 218 Survey of Attacks and Defenses on Edge-Deployed Neural Networks 2019 cs.CR 51 Reinforcement learning with human advice: a survey. cs.AI 86A Survey on Subgraph Counting: Concepts. 2020</p>
<p>Domain Generalization: A Survey. LG 1487 Deep Learning for 3D Point Clouds: A Survey 2019 cs.CV 2360 Survey on Causal-based Machine Learning Fairness Notions 2020 cs.LG 109 Towards a Survey on Static and Dynamic Hypergraph Visualizations 2021 cs. 202138</p>
<p>Graph-based Deep Learning for Communication Networks: A Survey 2021 cs.NI 245 Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey 2020 cs.CV 1062 Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey. cs.CL 1082021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4604 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4604</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4604</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-6a5c45ec0e89a722388751768a70f04b8fecc905</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6a5c45ec0e89a722388751768a70f04b8fecc905" target="_blank">Large language models surpass human experts in predicting neuroscience results</a></p>
                <p><strong>Paper Venue:</strong> Nature Human Behaviour</p>
                <p><strong>Paper TL;DR:</strong> It is shown that LLMs—especially BrainGPT, an LLM the authors tuned on the neuroscience literature—outperform experts in predicting neuroscience results and could assist scientists in making future discoveries.</p>
                <p><strong>Paper Abstract:</strong> Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. Here, to evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs indicated high confidence in their predictions, their responses were more likely to be correct, which presages a future where LLMs assist humans in making discoveries. Our approach is not neuroscience specific and is transferable to other knowledge-intensive endeavours.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4604.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4604.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BrainGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BrainGPT (neuroscience-tuned large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specialized LLM derived by fine-tuning a general-purpose LLM on neuroscience publications (2002–2022) using LoRA, intended to integrate patterns across the neuroscience literature and predict experimental outcomes better than general LLMs or human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BrainGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A specialized LLM produced by parameter-efficient fine-tuning (LoRA) of a pretrained general-purpose model (Mistral-7B-v0.1). Training data comprised neuroscience publications (spanning 100 journals, 2002–2022). The fine-tuning inserts low-rank adapter matrices into transformer blocks and trains only those adapter weights so the base model weights remain unchanged; after tuning, BrainGPT is evaluated via model perplexity on candidate abstracts and used to predict likely experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Mistral-7B-v0.1 (base) fine-tuned with LoRA; other baseline LLMs evaluated in the paper include GPT-4, ChatGPT variants, Llama2-7B, Mistral-7B, Galactica and additional public LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Model is fine-tuned on a large neuroscience corpus so extraction is implicit via language-model pretraining and adapter fine-tuning; during evaluation, extraction is operationalized by computing perplexities over candidate abstracts (embedding and token-probability based scoring rather than natural-language answer prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Implicit pattern synthesis via generative pretraining plus domain adapter fine-tuning (LoRA) yielding a generative model that integrates signals across many papers; decisions are made by comparing model perplexities across candidate result versions (i.e., selecting the less-surprising abstract).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Training used neuroscience publications from 100 journals spanning 2002–2022, totaling ~1.3 billion tokens (exact paper count not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Neuroscience literature (multi-subfield: Behavioral/Cognitive, Systems/Circuits, Neurobiology of Disease, Cellular/Molecular, Development/Plasticity/Repair).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Predicted experimental outcomes (binary choice between two abstract variants) and calibrated confidence scores derived from perplexity differences; generative capability to produce low-perplexity text in the neuroscience domain.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy on BrainBench (binary-choice task), perplexity distributions, confidence calibration (correlation between perplexity-difference and correctness), logistic regression between perplexity differences and correctness, zlib-perplexity ratio for memorization checks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Fine-tuned BrainGPT (LoRA on Mistral-7B-v0.1) improved ~3% absolute on BrainBench relative to the pretrained base model; LLMs on average obtained 81.4% accuracy on BrainBench versus human experts at 63.4%; LoRA tuning substantially shifted perplexity distributions for correct responses (statistically significant, t(199)=15.7, p<.001).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Pretrained base LLMs (e.g., Mistral-7B-v0.1 before LoRA), a variety of general-purpose LLMs, and human expert performance on BrainBench.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BrainGPT (LoRA-tuned Mistral-7B-v0.1) outperformed the untuned base model by ~3% on BrainBench; general LLMs outperformed human experts overall (81.4% vs 63.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain-specific adapter tuning (LoRA) efficiently specializes a base LLM to neuroscience, improving forward-looking predictive performance; LLMs leverage integration across abstract context (methods and background) rather than solely local result text; model perplexity differences provide calibrated confidence signals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires a large domain corpus and careful quality control; fine-tuning shifts model specialization which could reduce generality; the paper warns about downstream risks (e.g., scientists deferring to model predictions) and that LLMs can hallucinate in backward-looking tasks; timely updating of domain knowledge is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Adapter fine-tuning added ~8% of total model weights (629,145,600 new weights reported) and yielded a modest improvement (~3%); smaller models (7B) performed comparably to larger models in this evaluation, indicating that model size beyond a threshold may have diminishing returns for this task; the paper also reports strong inter-LLM agreement and hypothesizes benefits from continuously updating domain data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models surpass human experts in predicting neuroscience results', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4604.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4604.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BrainBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BrainBench (forward-looking benchmark for neuroscience)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A forward-looking benchmark comprising paired abstracts (original and altered-result versions) derived from Journal of Neuroscience abstracts to evaluate models' ability to predict experimental outcomes from background and methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BrainBench</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A dataset and evaluation protocol: for each test case an original published abstract and an altered-coherent abstract (with a significantly changed result but intact methods/background) are paired; models are evaluated by selecting which abstract is the original via perplexity comparison; human experts were also evaluated on the same binary-choice tasks with confidence and expertise ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Used to evaluate a suite of LLMs including GPT-4, ChatGPT/instruct models, Llama2-7B, Mistral-7B (and Mistral-7B-v0.1 as fine-tuning base), Galactica, and other public LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Benchmark items are manually (and GPT-4 assisted) created from Journal of Neuroscience abstracts; LLM evaluation extracts evidence by computing text perplexity over full abstracts and specific sentences to test context integration.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Does not synthesize new theories directly; it probes the LLMs' implicit synthesis ability by testing whether models integrate background and methods to predict results (inferred by performance drop when models see only local result sentences).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>300 test cases total: 200 human-created test cases + 100 GPT-4-generated test cases, all sourced from Journal of Neuroscience abstracts published in 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Neuroscience (multi-subfield coverage drawn from Journal of Neuroscience).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Binary-choice predictions (original vs altered abstract) and associated confidence (perplexity-derived for models; self-reported for humans).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy (percentage correct), confidence calibration (correlation between confidence and accuracy), Spearman correlations between item difficulties across humans and models, zlib-perplexity ratio for memorization checks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>LLMs averaged 81.4% accuracy on BrainBench; human experts averaged 63.4% (top 20% self-reported expertise humans: 66.2%). Perplexity-based confidence correlated positively with correctness for models; models and humans differed in which items were difficult (mean Spearman correlation between LLMs and humans = 0.15 ± 0.03).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human expert judgments (171 included participants after screening) and various LLM configurations (base vs chat/instruct-tuned variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LLMs outperformed human experts across all subfields; base models outperformed chat/instruction-optimized variants (statistical comparison reported: t(5)=5.4, p=.002 for chat vs base).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BrainBench operationalizes forward-looking prediction (distinct from QA benchmarks) and shows LLMs can integrate across abstract context to predict outcomes; it can be partly automated using GPT-4 for case generation and serves as an evaluation bed for domain specialization (e.g., BrainGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Creation is labor intensive and requires expert curation; ensuring test-case novelty relative to LLM training data is nontrivial; binary-choice format is a simplification of richer forecasting tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors created 200 human and 100 GPT-4-generated items and observed similar model behavior across both sets; no explicit runtime/compute scaling curves reported, but the benchmark enabled measuring gains from domain fine-tuning (LoRA) and from different model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models surpass human experts in predicting neuroscience results', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4604.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4604.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoRA fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-Rank Adaptation (LoRA) of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-efficient fine-tuning method that injects low-rank adapter matrices into transformer blocks and trains only those adapters to adapt a pretrained LLM to a target domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LoRA fine-tuning for domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LoRA adds low-rank adapter matrices to the transformer's weight updates and trains only these adapter parameters (keeping base model weights frozen), enabling domain specialization with far fewer additional parameters; applied here to adapt Mistral-7B-v0.1 to neuroscience by training on ~1.3B tokens from 100 journals (2002–2022).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied to Mistral-7B-v0.1 (base) in this study; LoRA is model-agnostic and can be applied to other transformer-based LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Domain knowledge is absorbed into adapter weights via standard language-model training objectives (next-token prediction) over the neuroscience corpus; no separate structured extraction pipeline described.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Implicit synthesis via updated adapter parameters that bias the generative model toward neuroscience domain patterns; synthesis is realized by the model's generative probabilities when scoring or generating text across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Fine-tuning corpus: neuroscience publications from 100 journals spanning 2002–2022 totaling ~1.3 billion tokens (paper count not explicitly specified).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Neuroscience literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>A domain-specialized LLM (BrainGPT) that produces lower perplexity for neuroscience-appropriate text and improved prediction accuracy on BrainBench.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Change in BrainBench accuracy (absolute %), shifts in perplexity distributions (statistical t-test reported), and percent of additional parameters added.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>LoRA tuning improved BrainBench performance by ~3% absolute; LoRA introduced ~629,145,600 additional weights (~8% of Mistral-7B-v0.1 total weights) and significantly shifted perplexity of correct responses (t(199)=15.7, p<.001).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Untuned base model (Mistral-7B-v0.1) and other general-purpose LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LoRA-fine-tuned model outperformed the untuned base Mistral-7B-v0.1 by ~3% on BrainBench; fine-tuning concentrated model probability mass on neuroscience-appropriate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LoRA allows efficient domain specialization with a relatively small fraction of extra parameters and measurable downstream gains on forward-looking prediction tasks; specialization is visible as a shift in perplexity distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Specialization may reduce generality; choice of corpus, recency-weighting, and curation impact outcomes; modest absolute gains (~3%) indicate further work needed to maximize domain transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Adding ~8% additional adapter weights produced a measurable but modest performance gain; authors note smaller 7B models can perform on par with larger ones in this task, suggesting nontrivial interactions between parameter count, domain data size, and downstream gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models surpass human experts in predicting neuroscience results', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4604.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4604.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach that supplements a generative LLM with a retrieval component that fetches relevant documents from a corpus to condition generation, keeping outputs grounded in up-to-date evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A two-stage system: (1) retrieval module (e.g., embedding-based nearest-neighbor search) fetches relevant documents or passages from an indexed corpus; (2) a generative LLM conditions on retrieved evidence to produce answers or syntheses, enabling access to fresh and verifiable facts without retraining the base model.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>General-purpose transformer LLMs (paper mentions RAG as a complementary approach; no specific model used in experiments here).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval from a database of scientific articles followed by conditioning the LLM on retrieved passages (standard RAG pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Generative conditioning on retrieved multi-document evidence to synthesize answers; can be used iteratively to combine evidence across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not implemented in experiments; suggested as a complementary approach to keep models up to date by querying a database of relevant scientific articles (scales with corpus size as configured).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General knowledge-intensive and scientific domains; suggested here for neuroscience.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded generated text or predictions based on retrieved documents (e.g., updated predictions, summaries conditioned on recent papers).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not evaluated in this paper; suggested metrics would include grounding/factuality and update-relevance (authors cite RAG as complementary).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported (RAG mentioned as an approach to maintain up-to-date knowledge rather than experimentally evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not applicable in this paper (RAG cited as existing complementary technique).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not applicable / not measured in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Authors recommend RAG as a practical method to keep domain-specialized LLMs current by retrieving fresh literature instead of continual full-model retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Integration and retrieval quality depend on corpus indexing and retrieval accuracy; combining retrieved (possibly conflicting) evidence requires careful synthesis strategies to avoid propagating errors.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>RAG scales with the size of the retrieval corpus and retrieval infrastructure; not empirically studied in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models surpass human experts in predicting neuroscience results', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4604.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4604.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 automated case generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-assisted test-case generation for BrainBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated (or semi-automated) pipeline using GPT-4 to generate altered abstracts (test cases) for BrainBench to reduce labor in creating forward-looking benchmark examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 automated test-case generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GPT-4 (Azure OpenAI API, version 2023-05-15) was prompted to create altered versions of published abstracts that changed results while preserving coherency; generated cases were then quality-controlled by human experts. This produced 100 GPT-4-generated test cases in addition to 200 human-created ones.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (Azure OpenAI API; version 2023-05-15).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompting GPT-4 with source abstracts and instructions to alter the results while maintaining methods/background; essentially structured prompting for text transformation rather than explicit structured extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>No multi-document synthesis; the model rewrites single-abstracts to produce coherent alternative outcomes, aiding benchmark creation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Generated 100 altered test cases derived from Journal of Neuroscience abstracts (2023); overall dataset used 300 abstracts/test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Neuroscience abstracts from Journal of Neuroscience (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Altered abstracts (coherent alternative-result versions) used as negative/foil examples in the BrainBench binary-choice evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human quality control acceptance rates and comparative model performance on GPT-4-created vs human-created items (authors report similar results across both sets; 8 of 100 GPT-4 items matched human-created ones word-for-word).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>GPT-4-generated items were close in quality to human-created ones and yielded similar evaluation behavior when used in BrainBench.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human-expert-created test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>GPT-4-generated cases were judged close in quality; results on BrainBench were similar across human-created and GPT-4-created test sets (no large systematic differences reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs (GPT-4) can assist in producing forward-looking benchmark items at scale, reducing human labor while maintaining acceptable quality after human QC.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Automated generation still requires human quality control; risk of inadvertent overlap with model training data must be managed to avoid contamination of evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors used GPT-4 to generate 100 items; approach is scalable but quality control effort may grow with scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models surpass human experts in predicting neuroscience results', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4604.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4604.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perplexity-based scoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity difference decision rule (model-probability scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decision method where an LLM selects between candidate texts by comparing model perplexities (lower perplexity = less surprising = more likely) and uses the difference as a calibrated confidence measure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Perplexity-based selection and confidence estimation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>For each test pair (original vs altered abstract), the LLM computes token-level probabilities and overall perplexity for each candidate; the model selects the candidate with lower perplexity as its prediction and uses the absolute ranked perplexity difference as its confidence score; calibration is assessed by correlating confidence with correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Open-weight LLMs used in the study (e.g., Mistral-7B-v0.1 and other models where weights were available); approach relies on access to model logits/probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Information extraction is implicit: the LLM's internal generative probabilities reflect how well a full-abstract is modeled given its learned patterns from training corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>No explicit cross-document aggregation; synthesis is implicit in the model's conditional probabilities over text informed by training on many papers.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied across BrainBench 300 test cases drawn from Journal of Neuroscience abstracts (2023) during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Neuroscience abstracts (evaluation context); method is general-purpose for text scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Binary-choice selection (which abstract is original) and a continuous confidence metric derived from perplexity differences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy (percentage of correct selections), calibration curves (accuracy as a function of confidence bins), logistic regression between perplexity differences and correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Perplexity-based selection yielded the high LLM accuracies reported (average LLM accuracy 81.4%); confidence derived from perplexity differences correlated positively with correctness (models were calibrated similarly to humans).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Natural-language prompting/chat responses (instruction-optimized/chat models) vs probability-score-based selection; human expert judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Base models evaluated via perplexity outperformed chat/instruction-tuned variants when used for this probability-scoring selection task (chat/instruct variants performed worse; t(5)=5.4, p=.002 reported for base vs chat/instruct).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using direct model probabilities (perplexity) yields reliable, calibrated predictions on forward-looking tasks and can outperform natural-language prompt-based (chat) interactions for this use case, emphasizing the value of access to model weights and logits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires access to model weights/logits (not possible with some closed commercial APIs); perplexity is sensitive to tokenization and model priors and does not provide explicit extracts or provenance from source documents.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Applied across multiple LLMs of varied sizes; the paper reports strong performance even for smaller 7B models when using perplexity-based selection, suggesting robustness across model sizes in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models surpass human experts in predicting neuroscience results', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4604.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4604.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>zlib-perplexity ratio</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>zlib-perplexity ratio (memorization detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique to detect whether LLMs have memorized passages from their training set by comparing compression-based compressibility with model perplexity; unusual combinations indicate memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>zlib-perplexity ratio check for training-set contamination</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Compute a data-agnostic compression metric (zlib compression ratio) for a passage and compare it to the LLM's computed perplexity for the same passage; passages that compress poorly but have unusually low perplexity to the model suggest memorization (indicative of presence in training data).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied to LLMs evaluated in the study where weights and perplexities were available (e.g., Galactica and others).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Analytic check rather than extraction: uses passage-level statistics and model perplexity to infer membership/memorization in the training data.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not a synthesis method; used as a diagnostics tool to rule out dataset memorization when interpreting forward-looking performance.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to BrainBench test-case passages (300 abstracts) and control passages such as the Gettysburg Address.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General text / scientific abstracts (used here on neuroscience abstracts).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Diagnostic scores indicating likelihood of memorization for test-case passages.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Distributional comparison of zlib-perplexity ratios across known-in-training vs unknown items; interpretation of overlap/ divergence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No indication that BrainBench items were memorized by LLMs based on zlib-perplexity ratio distributions; the Gettysburg Address showed expected signs of memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Known-memorized passage (Gettysburg Address) used as positive control; Galactica model where training set membership is partially known used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BrainBench passages did not display the memorization signature seen for repeatedly-present passages like the Gettysburg Address.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>zlib-perplexity ratio is a practical diagnostic to assess potential memorization of benchmark items and supported the authors' conclusion that model performance reflected learned patterns rather than memorized test items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Method is a heuristic and can be inconclusive when passage repeat-frequency in training data is unknown; overlap in ratio distributions can complicate interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Applied at passage-level across the dataset; scaling to very large corpora is straightforward computationally but interpretation requires careful controls.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models surpass human experts in predicting neuroscience results', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4604.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4604.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Train-from-scratch neuroscience LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relatively small LLM trained from scratch on published neuroscience literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small LLM trained from scratch exclusively on published neuroscience literature (excluding preprints and BrainBench items) that achieved superhuman performance on BrainBench, demonstrating that memorization of test cases was not required for high performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Small neuroscience LLM trained from scratch</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A transformer-based LLM trained de novo on a corpus of published neuroscience articles (explicitly excluding preprints and BrainBench items) to remove any overlap with evaluation data; used to validate that forward-looking predictive performance can arise from domain-pattern learning rather than contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>A relatively small transformer LLM (specific architecture/size not detailed in the main text; reported in Supplementary Fig. S.2).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Standard next-token prediction training on domain corpus—learning domain patterns implicitly rather than explicit structured extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Implicit synthesis via pattern learning across the training corpus, enabling the model to generalize to novel test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Trained on published neuroscience literature (exact number of papers not specified in main text; preprints and BrainBench items excluded).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Neuroscience.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Predictions on BrainBench binary-choice tasks (and associated perplexity-based confidence).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BrainBench accuracy (binary-choice), comparison to other LLMs and human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>The from-scratch, relatively small neuroscience-trained LLM achieved superhuman performance on BrainBench (specific numeric accuracy reported in Supplementary materials; main text states superhuman performance observed).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human experts and pre-trained general-purpose LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Outperformed human experts on BrainBench, indicating that training on domain corpus alone (without test-case contamination) can yield high forward-looking predictive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Superhuman predictive capability can be attained by models trained solely on domain literature, supporting the interpretation that LLMs capture domain-level patterns rather than relying on memorized test items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Main-text does not provide architecture/size details; reproducibility requires access to domain corpus and compute; generalization beyond the narrow task requires further validation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors note that even a relatively small model trained from scratch can perform strongly on this task, suggesting that domain data and task framing can be as important as sheer model scale for forward-looking prediction in neuroscience.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models surpass human experts in predicting neuroscience results', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4604.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4604.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-probability & internal-representation approaches</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using model probability scores and internal representations (vs. natural-language prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach emphasizing the use of LLM logits/perplexities and possibly classifiers trained on internal model representations to make more reliable judgments for scientific prediction tasks, rather than relying on conversational prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Probability/logit-based decisioning and representation-based classifiers</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Rather than prompting models in natural language (chat), this class of methods directly uses model internals: (a) selection based on token-probability/perplexity scores computed from model outputs; (b) training discriminative classifiers on internal activations or embeddings to predict labels or synthesize conclusions — both approaches require open access to model weights/representations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applicable to transformer LLMs where weights/logits/activations are accessible (authors used perplexity access to multiple open LLMs in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Use of internal activations or token probabilities to extract signal about textual fit to domain; classifiers on internal representations can be trained to map patterns to decisions or labels.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Synthesis achieved by learning mappings from aggregated internal representations across texts to outputs (e.g., prediction labels) or by scoring candidate synthesized outputs via model probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied across BrainBench test cases (300 abstracts) in this study insofar as perplexity-based decisions used model internals; no multi-paper classifier training experiments are detailed in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Neuroscience prediction tasks; approach is general-purpose.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Discrete decisions (binary choices), confidence estimates, or outputs from classifiers trained on internal states (not concretely instantiated in detail in the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy, calibration of confidence, and logistic regressions between internal-score differences and correctness (authors report significant correlations for perplexity-based scores).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Perplexity-based decisioning (a specific instance of this approach) gave high accuracy (average LLM accuracy 81.4%) and produced well-calibrated confidence signals; the paper notes prompting/chat-based interactions produced worse results compared to probability-based scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Natural-language conversation/chat prompting (instruction-optimized/chat models) and human expert judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Probability/logit-based scoring outperformed chat/instruct prompting approaches in this forward-looking prediction task; concrete classifier-based internal-representation experiments are cited in related work but not deeply explored in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Access to model logits and internal representations enables more reliable and calibrated decisioning for scientific prediction tasks than black-box chat prompting; underscores value of open models and weight access.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires access to model internals (weights/logits/activations), which is infeasible for many closed APIs; training classifiers on internal representations requires labeled data and careful validation to avoid spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not exhaustively characterized in the paper; however, observed robustness of perplexity-based decisions across multiple model sizes (including smaller 7B models) suggests this approach scales to models of moderate size when internal access is available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models surpass human experts in predicting neuroscience results', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4604.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4604.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought (step-by-step) reasoning prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-step prompting technique that elicits intermediate reasoning steps from LLMs, which can improve performance on some reasoning tasks; the paper references it as a possible testing variant for BrainGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompting strategy that encourages an LLM to produce intermediate reasoning steps (a chain of thought) prior to producing a final answer; could be applied to domain prediction tasks to examine whether explicit stepwise reasoning helps forward-looking predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>General LLMs capable of chain-of-thought behavior (e.g., GPT-family models) — not experimentally evaluated in this work but discussed as a research question.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt engineering to induce multi-step explication of evidence; no explicit extraction pipeline described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-step reasoning that may aggregate evidence across premises and intermediate conclusions to form final predictions; proposed as an experimental variant to test whether neuroscience prediction benefits from explicit reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applied in experiments; suggested as a future test using BrainBench/BrainGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Potentially applicable to neuroscience prediction tasks and other domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Stepwise explanations plus final predictions (if applied).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not measured here; would likely include accuracy, explanation quality, and whether chain-of-thought improves prediction beyond pattern-based generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No empirical results in this paper; authors note that if neuroscience prediction is pattern-based (as they suspect), chain-of-thought may not help, but if it's deductive, it might.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard (non-chain-of-thought) model outputs and pattern-based generalization methods (implicit in BrainGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not evaluated; proposed as a hypothesis to test using BrainBench and BrainGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Authors frame chain-of-thought prompting as an open question for whether explicit stepwise reasoning benefits forward-looking scientific prediction vs. implicit pattern-based generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Chain-of-thought can increase verbosity and may not align with models' implicit pattern recognition; topic remains speculative in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models surpass human experts in predicting neuroscience results', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LoRA: Low-Rank Adaptation of Large Language Models <em>(Rating: 2)</em></li>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP <em>(Rating: 2)</em></li>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Galactica: Large language models for science <em>(Rating: 1)</em></li>
                <li>GPT-4 Technical Report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4604",
    "paper_id": "paper-6a5c45ec0e89a722388751768a70f04b8fecc905",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "BrainGPT",
            "name_full": "BrainGPT (neuroscience-tuned large language model)",
            "brief_description": "A domain-specialized LLM derived by fine-tuning a general-purpose LLM on neuroscience publications (2002–2022) using LoRA, intended to integrate patterns across the neuroscience literature and predict experimental outcomes better than general LLMs or human experts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "BrainGPT",
            "system_description": "A specialized LLM produced by parameter-efficient fine-tuning (LoRA) of a pretrained general-purpose model (Mistral-7B-v0.1). Training data comprised neuroscience publications (spanning 100 journals, 2002–2022). The fine-tuning inserts low-rank adapter matrices into transformer blocks and trains only those adapter weights so the base model weights remain unchanged; after tuning, BrainGPT is evaluated via model perplexity on candidate abstracts and used to predict likely experimental outcomes.",
            "llm_model_used": "Mistral-7B-v0.1 (base) fine-tuned with LoRA; other baseline LLMs evaluated in the paper include GPT-4, ChatGPT variants, Llama2-7B, Mistral-7B, Galactica and additional public LLMs.",
            "extraction_technique": "Model is fine-tuned on a large neuroscience corpus so extraction is implicit via language-model pretraining and adapter fine-tuning; during evaluation, extraction is operationalized by computing perplexities over candidate abstracts (embedding and token-probability based scoring rather than natural-language answer prompts).",
            "synthesis_technique": "Implicit pattern synthesis via generative pretraining plus domain adapter fine-tuning (LoRA) yielding a generative model that integrates signals across many papers; decisions are made by comparing model perplexities across candidate result versions (i.e., selecting the less-surprising abstract).",
            "number_of_papers": "Training used neuroscience publications from 100 journals spanning 2002–2022, totaling ~1.3 billion tokens (exact paper count not specified).",
            "domain_or_topic": "Neuroscience literature (multi-subfield: Behavioral/Cognitive, Systems/Circuits, Neurobiology of Disease, Cellular/Molecular, Development/Plasticity/Repair).",
            "output_type": "Predicted experimental outcomes (binary choice between two abstract variants) and calibrated confidence scores derived from perplexity differences; generative capability to produce low-perplexity text in the neuroscience domain.",
            "evaluation_metrics": "Accuracy on BrainBench (binary-choice task), perplexity distributions, confidence calibration (correlation between perplexity-difference and correctness), logistic regression between perplexity differences and correctness, zlib-perplexity ratio for memorization checks.",
            "performance_results": "Fine-tuned BrainGPT (LoRA on Mistral-7B-v0.1) improved ~3% absolute on BrainBench relative to the pretrained base model; LLMs on average obtained 81.4% accuracy on BrainBench versus human experts at 63.4%; LoRA tuning substantially shifted perplexity distributions for correct responses (statistically significant, t(199)=15.7, p&lt;.001).",
            "comparison_baseline": "Pretrained base LLMs (e.g., Mistral-7B-v0.1 before LoRA), a variety of general-purpose LLMs, and human expert performance on BrainBench.",
            "performance_vs_baseline": "BrainGPT (LoRA-tuned Mistral-7B-v0.1) outperformed the untuned base model by ~3% on BrainBench; general LLMs outperformed human experts overall (81.4% vs 63.4%).",
            "key_findings": "Domain-specific adapter tuning (LoRA) efficiently specializes a base LLM to neuroscience, improving forward-looking predictive performance; LLMs leverage integration across abstract context (methods and background) rather than solely local result text; model perplexity differences provide calibrated confidence signals.",
            "limitations_challenges": "Requires a large domain corpus and careful quality control; fine-tuning shifts model specialization which could reduce generality; the paper warns about downstream risks (e.g., scientists deferring to model predictions) and that LLMs can hallucinate in backward-looking tasks; timely updating of domain knowledge is necessary.",
            "scaling_behavior": "Adapter fine-tuning added ~8% of total model weights (629,145,600 new weights reported) and yielded a modest improvement (~3%); smaller models (7B) performed comparably to larger models in this evaluation, indicating that model size beyond a threshold may have diminishing returns for this task; the paper also reports strong inter-LLM agreement and hypothesizes benefits from continuously updating domain data.",
            "uuid": "e4604.0",
            "source_info": {
                "paper_title": "Large language models surpass human experts in predicting neuroscience results",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "BrainBench",
            "name_full": "BrainBench (forward-looking benchmark for neuroscience)",
            "brief_description": "A forward-looking benchmark comprising paired abstracts (original and altered-result versions) derived from Journal of Neuroscience abstracts to evaluate models' ability to predict experimental outcomes from background and methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "BrainBench",
            "system_description": "A dataset and evaluation protocol: for each test case an original published abstract and an altered-coherent abstract (with a significantly changed result but intact methods/background) are paired; models are evaluated by selecting which abstract is the original via perplexity comparison; human experts were also evaluated on the same binary-choice tasks with confidence and expertise ratings.",
            "llm_model_used": "Used to evaluate a suite of LLMs including GPT-4, ChatGPT/instruct models, Llama2-7B, Mistral-7B (and Mistral-7B-v0.1 as fine-tuning base), Galactica, and other public LLMs.",
            "extraction_technique": "Benchmark items are manually (and GPT-4 assisted) created from Journal of Neuroscience abstracts; LLM evaluation extracts evidence by computing text perplexity over full abstracts and specific sentences to test context integration.",
            "synthesis_technique": "Does not synthesize new theories directly; it probes the LLMs' implicit synthesis ability by testing whether models integrate background and methods to predict results (inferred by performance drop when models see only local result sentences).",
            "number_of_papers": "300 test cases total: 200 human-created test cases + 100 GPT-4-generated test cases, all sourced from Journal of Neuroscience abstracts published in 2023.",
            "domain_or_topic": "Neuroscience (multi-subfield coverage drawn from Journal of Neuroscience).",
            "output_type": "Binary-choice predictions (original vs altered abstract) and associated confidence (perplexity-derived for models; self-reported for humans).",
            "evaluation_metrics": "Accuracy (percentage correct), confidence calibration (correlation between confidence and accuracy), Spearman correlations between item difficulties across humans and models, zlib-perplexity ratio for memorization checks.",
            "performance_results": "LLMs averaged 81.4% accuracy on BrainBench; human experts averaged 63.4% (top 20% self-reported expertise humans: 66.2%). Perplexity-based confidence correlated positively with correctness for models; models and humans differed in which items were difficult (mean Spearman correlation between LLMs and humans = 0.15 ± 0.03).",
            "comparison_baseline": "Human expert judgments (171 included participants after screening) and various LLM configurations (base vs chat/instruct-tuned variants).",
            "performance_vs_baseline": "LLMs outperformed human experts across all subfields; base models outperformed chat/instruction-optimized variants (statistical comparison reported: t(5)=5.4, p=.002 for chat vs base).",
            "key_findings": "BrainBench operationalizes forward-looking prediction (distinct from QA benchmarks) and shows LLMs can integrate across abstract context to predict outcomes; it can be partly automated using GPT-4 for case generation and serves as an evaluation bed for domain specialization (e.g., BrainGPT).",
            "limitations_challenges": "Creation is labor intensive and requires expert curation; ensuring test-case novelty relative to LLM training data is nontrivial; binary-choice format is a simplification of richer forecasting tasks.",
            "scaling_behavior": "Authors created 200 human and 100 GPT-4-generated items and observed similar model behavior across both sets; no explicit runtime/compute scaling curves reported, but the benchmark enabled measuring gains from domain fine-tuning (LoRA) and from different model sizes.",
            "uuid": "e4604.1",
            "source_info": {
                "paper_title": "Large language models surpass human experts in predicting neuroscience results",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LoRA fine-tuning",
            "name_full": "Low-Rank Adaptation (LoRA) of large language models",
            "brief_description": "A parameter-efficient fine-tuning method that injects low-rank adapter matrices into transformer blocks and trains only those adapters to adapt a pretrained LLM to a target domain.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LoRA fine-tuning for domain adaptation",
            "system_description": "LoRA adds low-rank adapter matrices to the transformer's weight updates and trains only these adapter parameters (keeping base model weights frozen), enabling domain specialization with far fewer additional parameters; applied here to adapt Mistral-7B-v0.1 to neuroscience by training on ~1.3B tokens from 100 journals (2002–2022).",
            "llm_model_used": "Applied to Mistral-7B-v0.1 (base) in this study; LoRA is model-agnostic and can be applied to other transformer-based LLMs.",
            "extraction_technique": "Domain knowledge is absorbed into adapter weights via standard language-model training objectives (next-token prediction) over the neuroscience corpus; no separate structured extraction pipeline described.",
            "synthesis_technique": "Implicit synthesis via updated adapter parameters that bias the generative model toward neuroscience domain patterns; synthesis is realized by the model's generative probabilities when scoring or generating text across papers.",
            "number_of_papers": "Fine-tuning corpus: neuroscience publications from 100 journals spanning 2002–2022 totaling ~1.3 billion tokens (paper count not explicitly specified).",
            "domain_or_topic": "Neuroscience literature.",
            "output_type": "A domain-specialized LLM (BrainGPT) that produces lower perplexity for neuroscience-appropriate text and improved prediction accuracy on BrainBench.",
            "evaluation_metrics": "Change in BrainBench accuracy (absolute %), shifts in perplexity distributions (statistical t-test reported), and percent of additional parameters added.",
            "performance_results": "LoRA tuning improved BrainBench performance by ~3% absolute; LoRA introduced ~629,145,600 additional weights (~8% of Mistral-7B-v0.1 total weights) and significantly shifted perplexity of correct responses (t(199)=15.7, p&lt;.001).",
            "comparison_baseline": "Untuned base model (Mistral-7B-v0.1) and other general-purpose LLMs.",
            "performance_vs_baseline": "LoRA-fine-tuned model outperformed the untuned base Mistral-7B-v0.1 by ~3% on BrainBench; fine-tuning concentrated model probability mass on neuroscience-appropriate outputs.",
            "key_findings": "LoRA allows efficient domain specialization with a relatively small fraction of extra parameters and measurable downstream gains on forward-looking prediction tasks; specialization is visible as a shift in perplexity distributions.",
            "limitations_challenges": "Specialization may reduce generality; choice of corpus, recency-weighting, and curation impact outcomes; modest absolute gains (~3%) indicate further work needed to maximize domain transfer.",
            "scaling_behavior": "Adding ~8% additional adapter weights produced a measurable but modest performance gain; authors note smaller 7B models can perform on par with larger ones in this task, suggesting nontrivial interactions between parameter count, domain data size, and downstream gains.",
            "uuid": "e4604.2",
            "source_info": {
                "paper_title": "Large language models surpass human experts in predicting neuroscience results",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Retrieval-Augmented Generation (RAG)",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A hybrid approach that supplements a generative LLM with a retrieval component that fetches relevant documents from a corpus to condition generation, keeping outputs grounded in up-to-date evidence.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "A two-stage system: (1) retrieval module (e.g., embedding-based nearest-neighbor search) fetches relevant documents or passages from an indexed corpus; (2) a generative LLM conditions on retrieved evidence to produce answers or syntheses, enabling access to fresh and verifiable facts without retraining the base model.",
            "llm_model_used": "General-purpose transformer LLMs (paper mentions RAG as a complementary approach; no specific model used in experiments here).",
            "extraction_technique": "Embedding-based retrieval from a database of scientific articles followed by conditioning the LLM on retrieved passages (standard RAG pipeline).",
            "synthesis_technique": "Generative conditioning on retrieved multi-document evidence to synthesize answers; can be used iteratively to combine evidence across papers.",
            "number_of_papers": "Not implemented in experiments; suggested as a complementary approach to keep models up to date by querying a database of relevant scientific articles (scales with corpus size as configured).",
            "domain_or_topic": "General knowledge-intensive and scientific domains; suggested here for neuroscience.",
            "output_type": "Grounded generated text or predictions based on retrieved documents (e.g., updated predictions, summaries conditioned on recent papers).",
            "evaluation_metrics": "Not evaluated in this paper; suggested metrics would include grounding/factuality and update-relevance (authors cite RAG as complementary).",
            "performance_results": "Not reported (RAG mentioned as an approach to maintain up-to-date knowledge rather than experimentally evaluated here).",
            "comparison_baseline": "Not applicable in this paper (RAG cited as existing complementary technique).",
            "performance_vs_baseline": "Not applicable / not measured in this work.",
            "key_findings": "Authors recommend RAG as a practical method to keep domain-specialized LLMs current by retrieving fresh literature instead of continual full-model retraining.",
            "limitations_challenges": "Integration and retrieval quality depend on corpus indexing and retrieval accuracy; combining retrieved (possibly conflicting) evidence requires careful synthesis strategies to avoid propagating errors.",
            "scaling_behavior": "RAG scales with the size of the retrieval corpus and retrieval infrastructure; not empirically studied in this paper.",
            "uuid": "e4604.3",
            "source_info": {
                "paper_title": "Large language models surpass human experts in predicting neuroscience results",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-4 automated case generation",
            "name_full": "GPT-4-assisted test-case generation for BrainBench",
            "brief_description": "An automated (or semi-automated) pipeline using GPT-4 to generate altered abstracts (test cases) for BrainBench to reduce labor in creating forward-looking benchmark examples.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4 automated test-case generation",
            "system_description": "GPT-4 (Azure OpenAI API, version 2023-05-15) was prompted to create altered versions of published abstracts that changed results while preserving coherency; generated cases were then quality-controlled by human experts. This produced 100 GPT-4-generated test cases in addition to 200 human-created ones.",
            "llm_model_used": "GPT-4 (Azure OpenAI API; version 2023-05-15).",
            "extraction_technique": "Prompting GPT-4 with source abstracts and instructions to alter the results while maintaining methods/background; essentially structured prompting for text transformation rather than explicit structured extraction.",
            "synthesis_technique": "No multi-document synthesis; the model rewrites single-abstracts to produce coherent alternative outcomes, aiding benchmark creation.",
            "number_of_papers": "Generated 100 altered test cases derived from Journal of Neuroscience abstracts (2023); overall dataset used 300 abstracts/test cases.",
            "domain_or_topic": "Neuroscience abstracts from Journal of Neuroscience (2023).",
            "output_type": "Altered abstracts (coherent alternative-result versions) used as negative/foil examples in the BrainBench binary-choice evaluation.",
            "evaluation_metrics": "Human quality control acceptance rates and comparative model performance on GPT-4-created vs human-created items (authors report similar results across both sets; 8 of 100 GPT-4 items matched human-created ones word-for-word).",
            "performance_results": "GPT-4-generated items were close in quality to human-created ones and yielded similar evaluation behavior when used in BrainBench.",
            "comparison_baseline": "Human-expert-created test cases.",
            "performance_vs_baseline": "GPT-4-generated cases were judged close in quality; results on BrainBench were similar across human-created and GPT-4-created test sets (no large systematic differences reported).",
            "key_findings": "LLMs (GPT-4) can assist in producing forward-looking benchmark items at scale, reducing human labor while maintaining acceptable quality after human QC.",
            "limitations_challenges": "Automated generation still requires human quality control; risk of inadvertent overlap with model training data must be managed to avoid contamination of evaluation.",
            "scaling_behavior": "Authors used GPT-4 to generate 100 items; approach is scalable but quality control effort may grow with scale.",
            "uuid": "e4604.4",
            "source_info": {
                "paper_title": "Large language models surpass human experts in predicting neuroscience results",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Perplexity-based scoring",
            "name_full": "Perplexity difference decision rule (model-probability scoring)",
            "brief_description": "A decision method where an LLM selects between candidate texts by comparing model perplexities (lower perplexity = less surprising = more likely) and uses the difference as a calibrated confidence measure.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Perplexity-based selection and confidence estimation",
            "system_description": "For each test pair (original vs altered abstract), the LLM computes token-level probabilities and overall perplexity for each candidate; the model selects the candidate with lower perplexity as its prediction and uses the absolute ranked perplexity difference as its confidence score; calibration is assessed by correlating confidence with correctness.",
            "llm_model_used": "Open-weight LLMs used in the study (e.g., Mistral-7B-v0.1 and other models where weights were available); approach relies on access to model logits/probabilities.",
            "extraction_technique": "Information extraction is implicit: the LLM's internal generative probabilities reflect how well a full-abstract is modeled given its learned patterns from training corpora.",
            "synthesis_technique": "No explicit cross-document aggregation; synthesis is implicit in the model's conditional probabilities over text informed by training on many papers.",
            "number_of_papers": "Applied across BrainBench 300 test cases drawn from Journal of Neuroscience abstracts (2023) during evaluation.",
            "domain_or_topic": "Neuroscience abstracts (evaluation context); method is general-purpose for text scoring.",
            "output_type": "Binary-choice selection (which abstract is original) and a continuous confidence metric derived from perplexity differences.",
            "evaluation_metrics": "Accuracy (percentage of correct selections), calibration curves (accuracy as a function of confidence bins), logistic regression between perplexity differences and correctness.",
            "performance_results": "Perplexity-based selection yielded the high LLM accuracies reported (average LLM accuracy 81.4%); confidence derived from perplexity differences correlated positively with correctness (models were calibrated similarly to humans).",
            "comparison_baseline": "Natural-language prompting/chat responses (instruction-optimized/chat models) vs probability-score-based selection; human expert judgments.",
            "performance_vs_baseline": "Base models evaluated via perplexity outperformed chat/instruction-tuned variants when used for this probability-scoring selection task (chat/instruct variants performed worse; t(5)=5.4, p=.002 reported for base vs chat/instruct).",
            "key_findings": "Using direct model probabilities (perplexity) yields reliable, calibrated predictions on forward-looking tasks and can outperform natural-language prompt-based (chat) interactions for this use case, emphasizing the value of access to model weights and logits.",
            "limitations_challenges": "Requires access to model weights/logits (not possible with some closed commercial APIs); perplexity is sensitive to tokenization and model priors and does not provide explicit extracts or provenance from source documents.",
            "scaling_behavior": "Applied across multiple LLMs of varied sizes; the paper reports strong performance even for smaller 7B models when using perplexity-based selection, suggesting robustness across model sizes in this task.",
            "uuid": "e4604.5",
            "source_info": {
                "paper_title": "Large language models surpass human experts in predicting neuroscience results",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "zlib-perplexity ratio",
            "name_full": "zlib-perplexity ratio (memorization detection)",
            "brief_description": "A technique to detect whether LLMs have memorized passages from their training set by comparing compression-based compressibility with model perplexity; unusual combinations indicate memorization.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "zlib-perplexity ratio check for training-set contamination",
            "system_description": "Compute a data-agnostic compression metric (zlib compression ratio) for a passage and compare it to the LLM's computed perplexity for the same passage; passages that compress poorly but have unusually low perplexity to the model suggest memorization (indicative of presence in training data).",
            "llm_model_used": "Applied to LLMs evaluated in the study where weights and perplexities were available (e.g., Galactica and others).",
            "extraction_technique": "Analytic check rather than extraction: uses passage-level statistics and model perplexity to infer membership/memorization in the training data.",
            "synthesis_technique": "Not a synthesis method; used as a diagnostics tool to rule out dataset memorization when interpreting forward-looking performance.",
            "number_of_papers": "Applied to BrainBench test-case passages (300 abstracts) and control passages such as the Gettysburg Address.",
            "domain_or_topic": "General text / scientific abstracts (used here on neuroscience abstracts).",
            "output_type": "Diagnostic scores indicating likelihood of memorization for test-case passages.",
            "evaluation_metrics": "Distributional comparison of zlib-perplexity ratios across known-in-training vs unknown items; interpretation of overlap/ divergence.",
            "performance_results": "No indication that BrainBench items were memorized by LLMs based on zlib-perplexity ratio distributions; the Gettysburg Address showed expected signs of memorization.",
            "comparison_baseline": "Known-memorized passage (Gettysburg Address) used as positive control; Galactica model where training set membership is partially known used for comparison.",
            "performance_vs_baseline": "BrainBench passages did not display the memorization signature seen for repeatedly-present passages like the Gettysburg Address.",
            "key_findings": "zlib-perplexity ratio is a practical diagnostic to assess potential memorization of benchmark items and supported the authors' conclusion that model performance reflected learned patterns rather than memorized test items.",
            "limitations_challenges": "Method is a heuristic and can be inconclusive when passage repeat-frequency in training data is unknown; overlap in ratio distributions can complicate interpretation.",
            "scaling_behavior": "Applied at passage-level across the dataset; scaling to very large corpora is straightforward computationally but interpretation requires careful controls.",
            "uuid": "e4604.6",
            "source_info": {
                "paper_title": "Large language models surpass human experts in predicting neuroscience results",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Train-from-scratch neuroscience LLM",
            "name_full": "Relatively small LLM trained from scratch on published neuroscience literature",
            "brief_description": "A small LLM trained from scratch exclusively on published neuroscience literature (excluding preprints and BrainBench items) that achieved superhuman performance on BrainBench, demonstrating that memorization of test cases was not required for high performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Small neuroscience LLM trained from scratch",
            "system_description": "A transformer-based LLM trained de novo on a corpus of published neuroscience articles (explicitly excluding preprints and BrainBench items) to remove any overlap with evaluation data; used to validate that forward-looking predictive performance can arise from domain-pattern learning rather than contamination.",
            "llm_model_used": "A relatively small transformer LLM (specific architecture/size not detailed in the main text; reported in Supplementary Fig. S.2).",
            "extraction_technique": "Standard next-token prediction training on domain corpus—learning domain patterns implicitly rather than explicit structured extraction.",
            "synthesis_technique": "Implicit synthesis via pattern learning across the training corpus, enabling the model to generalize to novel test cases.",
            "number_of_papers": "Trained on published neuroscience literature (exact number of papers not specified in main text; preprints and BrainBench items excluded).",
            "domain_or_topic": "Neuroscience.",
            "output_type": "Predictions on BrainBench binary-choice tasks (and associated perplexity-based confidence).",
            "evaluation_metrics": "BrainBench accuracy (binary-choice), comparison to other LLMs and human experts.",
            "performance_results": "The from-scratch, relatively small neuroscience-trained LLM achieved superhuman performance on BrainBench (specific numeric accuracy reported in Supplementary materials; main text states superhuman performance observed).",
            "comparison_baseline": "Human experts and pre-trained general-purpose LLMs.",
            "performance_vs_baseline": "Outperformed human experts on BrainBench, indicating that training on domain corpus alone (without test-case contamination) can yield high forward-looking predictive performance.",
            "key_findings": "Superhuman predictive capability can be attained by models trained solely on domain literature, supporting the interpretation that LLMs capture domain-level patterns rather than relying on memorized test items.",
            "limitations_challenges": "Main-text does not provide architecture/size details; reproducibility requires access to domain corpus and compute; generalization beyond the narrow task requires further validation.",
            "scaling_behavior": "Authors note that even a relatively small model trained from scratch can perform strongly on this task, suggesting that domain data and task framing can be as important as sheer model scale for forward-looking prediction in neuroscience.",
            "uuid": "e4604.7",
            "source_info": {
                "paper_title": "Large language models surpass human experts in predicting neuroscience results",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Model-probability & internal-representation approaches",
            "name_full": "Using model probability scores and internal representations (vs. natural-language prompting)",
            "brief_description": "An approach emphasizing the use of LLM logits/perplexities and possibly classifiers trained on internal model representations to make more reliable judgments for scientific prediction tasks, rather than relying on conversational prompting.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Probability/logit-based decisioning and representation-based classifiers",
            "system_description": "Rather than prompting models in natural language (chat), this class of methods directly uses model internals: (a) selection based on token-probability/perplexity scores computed from model outputs; (b) training discriminative classifiers on internal activations or embeddings to predict labels or synthesize conclusions — both approaches require open access to model weights/representations.",
            "llm_model_used": "Applicable to transformer LLMs where weights/logits/activations are accessible (authors used perplexity access to multiple open LLMs in experiments).",
            "extraction_technique": "Use of internal activations or token probabilities to extract signal about textual fit to domain; classifiers on internal representations can be trained to map patterns to decisions or labels.",
            "synthesis_technique": "Synthesis achieved by learning mappings from aggregated internal representations across texts to outputs (e.g., prediction labels) or by scoring candidate synthesized outputs via model probabilities.",
            "number_of_papers": "Applied across BrainBench test cases (300 abstracts) in this study insofar as perplexity-based decisions used model internals; no multi-paper classifier training experiments are detailed in the main text.",
            "domain_or_topic": "Neuroscience prediction tasks; approach is general-purpose.",
            "output_type": "Discrete decisions (binary choices), confidence estimates, or outputs from classifiers trained on internal states (not concretely instantiated in detail in the main text).",
            "evaluation_metrics": "Accuracy, calibration of confidence, and logistic regressions between internal-score differences and correctness (authors report significant correlations for perplexity-based scores).",
            "performance_results": "Perplexity-based decisioning (a specific instance of this approach) gave high accuracy (average LLM accuracy 81.4%) and produced well-calibrated confidence signals; the paper notes prompting/chat-based interactions produced worse results compared to probability-based scoring.",
            "comparison_baseline": "Natural-language conversation/chat prompting (instruction-optimized/chat models) and human expert judgments.",
            "performance_vs_baseline": "Probability/logit-based scoring outperformed chat/instruct prompting approaches in this forward-looking prediction task; concrete classifier-based internal-representation experiments are cited in related work but not deeply explored in this paper.",
            "key_findings": "Access to model logits and internal representations enables more reliable and calibrated decisioning for scientific prediction tasks than black-box chat prompting; underscores value of open models and weight access.",
            "limitations_challenges": "Requires access to model internals (weights/logits/activations), which is infeasible for many closed APIs; training classifiers on internal representations requires labeled data and careful validation to avoid spurious correlations.",
            "scaling_behavior": "Not exhaustively characterized in the paper; however, observed robustness of perplexity-based decisions across multiple model sizes (including smaller 7B models) suggests this approach scales to models of moderate size when internal access is available.",
            "uuid": "e4604.8",
            "source_info": {
                "paper_title": "Large language models surpass human experts in predicting neuroscience results",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Chain-of-Thought prompting",
            "name_full": "Chain-of-thought (step-by-step) reasoning prompting",
            "brief_description": "A multi-step prompting technique that elicits intermediate reasoning steps from LLMs, which can improve performance on some reasoning tasks; the paper references it as a possible testing variant for BrainGPT.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Chain-of-thought prompting",
            "system_description": "Prompting strategy that encourages an LLM to produce intermediate reasoning steps (a chain of thought) prior to producing a final answer; could be applied to domain prediction tasks to examine whether explicit stepwise reasoning helps forward-looking predictions.",
            "llm_model_used": "General LLMs capable of chain-of-thought behavior (e.g., GPT-family models) — not experimentally evaluated in this work but discussed as a research question.",
            "extraction_technique": "Prompt engineering to induce multi-step explication of evidence; no explicit extraction pipeline described in the paper.",
            "synthesis_technique": "Multi-step reasoning that may aggregate evidence across premises and intermediate conclusions to form final predictions; proposed as an experimental variant to test whether neuroscience prediction benefits from explicit reasoning.",
            "number_of_papers": "Not applied in experiments; suggested as a future test using BrainBench/BrainGPT.",
            "domain_or_topic": "Potentially applicable to neuroscience prediction tasks and other domains.",
            "output_type": "Stepwise explanations plus final predictions (if applied).",
            "evaluation_metrics": "Not measured here; would likely include accuracy, explanation quality, and whether chain-of-thought improves prediction beyond pattern-based generalization.",
            "performance_results": "No empirical results in this paper; authors note that if neuroscience prediction is pattern-based (as they suspect), chain-of-thought may not help, but if it's deductive, it might.",
            "comparison_baseline": "Standard (non-chain-of-thought) model outputs and pattern-based generalization methods (implicit in BrainGPT).",
            "performance_vs_baseline": "Not evaluated; proposed as a hypothesis to test using BrainBench and BrainGPT.",
            "key_findings": "Authors frame chain-of-thought prompting as an open question for whether explicit stepwise reasoning benefits forward-looking scientific prediction vs. implicit pattern-based generalization.",
            "limitations_challenges": "Chain-of-thought can increase verbosity and may not align with models' implicit pattern recognition; topic remains speculative in this work.",
            "uuid": "e4604.9",
            "source_info": {
                "paper_title": "Large language models surpass human experts in predicting neuroscience results",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LoRA: Low-Rank Adaptation of Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP",
            "rating": 2
        },
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Galactica: Large language models for science",
            "rating": 1
        },
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 1
        }
    ],
    "cost": 0.0221545,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large language models surpass human experts in predicting neuroscience results</h1>
<p>Xiaoliang Luo ${ }^{1, \dagger, *}$, Akilles Rechardt ${ }^{1, \dagger}$, Guangzhi Sun ${ }^{2, \dagger}$, Kevin K. Nejad ${ }^{3,6, \dagger}$, Felipe Yáñez ${ }^{4, \dagger}$, Bati Yilmaz ${ }^{5, \dagger}$, Kangjoo Lee ${ }^{8}$, Alexandra O. Cohen ${ }^{9}$, Valentina Borghesani ${ }^{10}$, Anton Pashkov ${ }^{11,12,13}$, Daniele Marinazzo ${ }^{14}$, Jonathan Nicholas ${ }^{15}$, Alessandro Salatiello ${ }^{16}$, Ilia Sucholutsky ${ }^{17}$, Pasquale Minervini ${ }^{18}$, Sepehr Razavi ${ }^{19}$, Roberta Rocca ${ }^{20}$, Elkhan Yusifov ${ }^{21}$, Tereza Okalova ${ }^{22}$, Nianlong Gu ${ }^{23}$, Martin Ferianc ${ }^{24}$, Mikail Khona ${ }^{25}$, Kaustubh R. Patil ${ }^{26,27}$, Pui-Shee Lee ${ }^{28,29}$, Rui Mata ${ }^{30}$, Nicholas E. Myers ${ }^{31}$, Jennifer K Bizley ${ }^{32}$, Sebastian Musslick ${ }^{33,34}$, Isil Poyraz Bilgin ${ }^{35}$, Guiomar Niso ${ }^{36}$, Justin M. Ales ${ }^{37}$, Michael Gaebler ${ }^{38}$, N Apurva Ratan Murty ${ }^{39}$, Leyla Loued-Khenissi ${ }^{40}$, Anna Behler ${ }^{41}$, Chloe M. Hall ${ }^{42,43}$, Jessica Dafflon ${ }^{44,45}$, Sherry Dongqi Bao ${ }^{46}$, and Bradley C. Love ${ }^{1,7, \dagger}$<br>${ }^{1}$ Department of Experimental Psychology, University College London, London, United Kingdom, ${ }^{2}$ Department of Engineering, University of Cambridge, Cambridge, United Kingdom, ${ }^{3}$ Department of Physiology, Anatomy \&amp; Genetics, University of Oxford, Oxford, United Kingdom, ${ }^{4}$ Max Planck Institute for Neurobiology of Behavior - caesar, Bonn, Germany, ${ }^{5}$ National Magnetic Resonance Research Center (UMRAM), Bilkent University, Ankara, Turkey, ${ }^{6}$ Department of Computer Science, University of Bristol, Bristol, United Kingdom, ${ }^{7}$ The Alan Turing Institute, London, United Kingdom, ${ }^{8}$ Department of Psychiatry, Yale University, New Haven, United States, ${ }^{9}$ Psychology, Emory University, Atlanta, United States, ${ }^{10}$ Faculty of Psychology and Educational Sciences, Université de Genève, Genève, Switzerland, ${ }^{11}$ Neurosurgery, Novosibirsk State Medical University, Novosibirsk, Russian Federation, ${ }^{12}$ Federal Center of Neurosurgery, FSBI, Novosibirsk, Russia, ${ }^{13}$ Department of Data Collection and Processing Systems, Novosibirsk State Technical University, Novosibirsk, Russia, ${ }^{14}$ Department of Data Analysis, Ghent University, Ghent, Belgium, ${ }^{15}$ Psychology, New York University, New York, United States, ${ }^{16}$ Department of Cognitive Neurology, University of Tübingen, Tübingen, Germany, ${ }^{17}$ Computer Science, Princeton University, Princeton, United States, ${ }^{18}$ ILCC, University of Edinburgh, Edinburgh, United Kingdom, ${ }^{19}$ Philosophy, Psychology, and Language Sciences, The University of Edinburgh, Edinburgh, United Kingdom, ${ }^{20}$ Department of Culture, Cognition and Computation, Aarhus University, Aarhus, Denmark, ${ }^{21}$ Department of Molecular Life Sciences, University of Zurich, Zurich, Switzerland, ${ }^{22}$ Bioengineering, University of Pennsylvania,</p>
<p>Philadelphia, United States, ${ }^{23}$ Linguistic Research Infrastructure, University of Zurich, Zurich, Switzerland, ${ }^{24}$ Electronic and Electrical Engineering, University College London, London, United Kingdom, ${ }^{25}$ Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States, ${ }^{26}$ Institute of Neuroscience and Medicine, INM-7: Brain and Behaviour, Research Centre Jülich, Jülich, Germany, ${ }^{27}$ Medical Faculty, Institute of Systems Neuroscience, Heinrich Heine University Düsseldorf, Düsseldorf, Germany, ${ }^{28}$ Graduate School of Systemic Neurosciences, Ludwig-Maximilians-University Munich, Planegg-Martinsried, Germany, ${ }^{29}$ Institute of Neuronal Cell Biology, Technical University of Munich, Munich, Germany, ${ }^{30}$ Faculty of Psychology, University of Basel, Basel, Switzerland, ${ }^{31}$ School of Psychology, University of Nottingham, Nottingham, United Kingdom, ${ }^{32}$ Ear Institute, University College London, London, United Kingdom, ${ }^{33}$ Institute of Cognitive Science, University of Osnabrück, Osnabrück, Germany, ${ }^{34}$ Department of Cognitive, Linguistic, \&amp; Psychological Sciences, Brown University, Providence, United States, ${ }^{35}$ Département de psychologie, Centre de recherche de l'Institut universitaire de gériatrie de Montréal, Montreal, Canada, ${ }^{36}$ Instituto Cajal, CSIC, Madrid, Spain, ${ }^{37}$ School of Psychology \&amp; Neuroscience, University of St Andrews, St Andrews, United Kingdom, ${ }^{38}$ Neurology, Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany, ${ }^{39}$ Department of Psychology, Georgia Institute of Technology, Atlanta, United States, ${ }^{40}$ Département des Neurosciences Cliniques, Lausanne University Hospital, Lausanne, Switzerland, ${ }^{41}$ School of Psychological Science, The University of Newcastle, Newcastle, Australia, ${ }^{42}$ Institute of Physiology, University Medical Center of the Johannes Gutenberg University, Mainz, Germany, ${ }^{43}$ Institute for Quantitative and Computational Biosciences (IQCB), Johannes Gutenberg University, Mainz, Germany, ${ }^{44}$ Data Science and Sharing Team, Functional Magnetic Resonance Imaging Facility, National Institute of Mental Health, Bethesda, United States, ${ }^{45}$ Machine Learning Team, Functional Magnetic Resonance Imaging Facility, National Institute of Mental Health, Bethesda, United States, ${ }^{46}$ Zurich Center for Neuroeconomics, Department of Economics, University of Zurich, Zurich, Switzerland, †Major contributions, the names of the remaining authors are listed in random order, see contributions breakdown in Table S.5, *Corresponding author: xiao.luo.17@ucl.ac.uk</p>
<p>Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. To evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries. Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.</p>
<h1>Introduction</h1>
<p>Keeping up with the exponentially increasing ${ }^{1}$ scientific literature is a superhuman challenge. Potentially disruptive findings go unnoticed in the deluge of articles ${ }^{2}$. Processing and integrating the myriad of relevant findings may already surpass humans' abilities. One path forward is a partnership between human scientists and machines. This partnership could take several forms, including specialist solutions that address specific challenges, such as in protein folding ${ }^{3}$, drug discovery ${ }^{4}$, and materials science ${ }^{5}$. Alternatively, general models of the scientific literature could help guide human scientists' predictions and study designs. We consider this possibility.</p>
<p>It is an open question whether large language models (LLMs), trained on general text and scientific articles, can predict the outcomes of experiments. If LLMs' predictions surpassed</p>
<p>human experts, the practice of science and the pace of discovery would radically change. We consider this question for neuroscience, which is a large and interdisciplinary field. Prediction in neuroscience should be challenging for human experts for several reasons: (i) there are often many thousands of relevant scientific articles, (ii) an individual study can be noisy, unreliable, and may not replicate, (iii) neuroscience is a multi-level endeavor ${ }^{6}$, spanning behavior and molecular mechanisms, (iv) and the analysis methods are diverse and can be complex ${ }^{7}$, (v) as are the methods used, which include different brain imaging techniques, lesion studies, gene modification, pharmacological interventions, and so forth.</p>
<p>Can LLMs meet these challenges? In other domains, LLMs have performed impressively. Upon its release, OpenAI's ChatGPT ${ }^{8}$ captured the public's imagination with its abilities. Most LLMs are based on the transformer architecture ${ }^{9}$. These models contain billions and sometimes trillions of weights ${ }^{10}$, which are tuned during training in a self-supervised manner to predict the next token, such as the next word in a text passage.</p>
<p>LLMs have displayed remarkable capabilities, including passing professional exams, reasoning (though not without limitations), translation, solving mathematics problems, and even writing computer code ${ }^{11,12}$. By constructing a statistical model during their training to predict the next token, whether that token is a word, pixel, or protein sequence ${ }^{13}$, LLMs uncover the underlying patterns or structure of a domain. This generative model captures patterns in the training data, including subtle and imperfect ones. How LLMs learn and generalize to novel situations can be likened to how expert scientists detect patterns in their field after years of reading papers, attending conferences, and analyzing data. From these experiences, human experts build intuitions that enable them to predict future outcomes based on proposed study designs. However, unlike human scientists, LLMs are virtually unconstrained in how much of the scientific literature they can process during training.</p>
<p>How can we formally evaluate the predictive abilities of LLMs in neuroscience? With the</p>
<p>rise of LLMs, there has been a surge in evaluation benchmarks, many of which focus on assessing LLMs’ capabilities in scientific domains. Most benchmarks evaluate core knowledge retrieval and reasoning abilities, which are typically backward-looking (Fig. 1). Backward-looking benchmarks include MMLU ${ }^{14}$, PubMedQA ${ }^{15}$, and MedMCQA ${ }^{16}$. These benchmarks are structured in a question-and-answer format, where models must demonstrate extensive world knowledge, retrieve relevant information based on the context of the question, and answer correctly. However, none of these benchmarks are suitable for evaluating the ability of models to predict novel outcomes, which is inherently forward-looking (Fig. 1).</p>
<p>To address this need, we developed BrainBench to test LLMs’ ability to predict neuroscience findings (Fig. 2). LLMs have been trained extensively on the scientific literature, including neuroscience. BrainBench evaluates whether LLMs have seized on the fundamental patterning of methods and results that underlie the structure of neuroscience. Can LLMs outperform human experts on this forward-looking benchmark? In particular, BrainBench evaluates how well the test-taker can predict neuroscience results from methods by presenting two versions of an abstract from a recent journal article. The test-taker's task is to predict the study's outcome, choosing between the original and an altered version. The altered abstract significantly changes the study's outcome (i.e., results) while maintaining overall coherence.</p>
<p>To appreciate how BrainBench qualitatively differs from existing benchmarks, consider a perceived limitation of LLMs, namely their tendency to generate erroneous information, a phenomenon commonly referred to as "hallucination" by LLM researchers. Unlike knowledge graphs that store verified facts, LLMs may not be trustworthy for backward-looking tasks such as summarizing research papers or providing accurate citations ${ }^{17}$. However, for forward-looking tasks, such as predicting results from a novel experiment, we view this tendency to mix and integrate information from large and noisy datasets as a virtue. What is a hallucination in a backward-looking task is a generalization or prediction in a forward-looking task (e.g., Brain-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Backward-looking and Forward-looking evaluations. (A) Backward-looking benchmarks involve recalling factual information. For example, in the left panel, a student retrieves a fact about the Gettysburg Address that they learned during a history class. Existing benchmarks in scientific domains are in essence backward-looking as they emphasize retrieving accepted facts for question answering and reasoning tasks. (B) Forward-looking benchmarks involve predicting novel outcomes based on past data. Two forms of uncertainty, aleatoric (due to intrinsic randomness) and epistemic (due to lack of knowledge), may be present. For example, in the right panel, a table tennis fan predicts which player will win the next set based on their knowledge of the players, how they have played so far today, and so forth. Inherent random factors, such as a breeze affecting the ball's flight, will also be present.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: BrainBench is a forward-looking benchmark for neuroscience. BrainBench evaluates test-takers' ability to predict neuroscience results. BrainBench's test cases were sourced from recent Journal of Neuroscience abstracts across five neuroscience domains: Behavioral/Cognitive, Systems/Circuits, Neurobiology of Disease, Cellular/Molecular, and Developmental/Plasticity/Repair. Test-takers chose between the original abstract and one altered to significantly change the result while maintaining coherency. Human experts and Language Models (LLMs) were tasked with selecting the correct (i.e., original) version from the two options. Human experts made choices, and provided confidence and expertise ratings in an online study. LLMs were scored as choosing the abstract with the lower perplexity (i.e., the text passage that was less surprising to the model) and their confidence was proportional to the difference in perplexity between the two options.</p>
<p>Bench). BrainBench provides a way to quantify this forward-looking ability and compare to human experts. To foreshadow our results, LLMs surpassed human experts on BrainBench by a substantial margin and this margin increased when we provided additional training in neuroscience to an LLM, which we refer to as “BrainGPT”.</p>
<h2>Results</h2>
<h3>General-purpose LLMs best neuroscientists on BrainBench</h3>
<p>On each benchmark trial (see Fig. 2), both the LLMs and human experts were tasked with selecting which of two versions of an abstract was correct (i.e., the original version). Human neuroscience experts were screened for their expertise and engagement (see Methods) with 171 out of 202 participants passing all checks and included in our analyses.</p>
<p>Every LLM outperformed human experts on BrainBench with LLMs averaging 81.4% accuracy and human experts averaging 63.4% (Fig. 3A). When restricting human responses to those in the top 20% of self-reported expertise for that test item, accuracy rose to 66.2%, still below the level of LLMs.</p>
<p>Smaller models such as Llama2-7B and Mistral-7B with 7 billion parameters, performed comparably to larger models (Fig. 3A) while besting smaller models (see Supplementary Materials) that may lack the capacity to capture key data patterns. Chat or instruction-optimized models performed worse than their base model counterparts ($t(5)=5.4$, $p=.002$). We suspect that aligning LLMs to engage in natural language conversations hinders their scientific inference abilities (see Discussion).</p>
<p>The previous analyses involved benchmark items created by co-authors who are neuroscience experts (see Methods). We conducted the same analyses using test cases generated by a LLM, namely GPT-4 (see Methods), and observed similar results (see Supplementary materials).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance of human experts and large language models on BrainBench. (A) LLMs outperformed human experts on BrainBench. Smaller models are on par with larger models. Base versions of models outperformed chat and instruct versions, which were tuned to be conversational with humans. (B) The distribution of test cases across neuroscience subfields roughly mirrors the distribution of articles in the Journal of Neuroscience with Behavior/Cognitive overrepresented. The average performance of 15 LLMs and human experts is shown. LLMs outperformed human experts in every subfield (see Fig. S.5 for full results). Error bars represent the standard deviation of accuracy around the mean. (C) Majority of the participants were doctoral students, postdoctoral researchers and faculty/academic staff. Error bars represent standard error of the mean.</p>
<p>Performance breakdown by subfields and by participant types BrainBench encompasses test cases from five distinct neuroscience domains: Behavioral/Cognitive, Cellular/Molecular, Systems/Circuits, Neurobiology of Disease, and Development/Plasticity/Repair. Some domains, particularly Behavioral/Cognitive, are overrepresented both in BrainBench (Fig. 3B) and the Journal of Neuroscience from which we drew our test cases (see Methods).</p>
<p>On average, LLMs performed better than human experts in every subfield (Fig. 3B), as did each individual LLM (Fig. S.5). Most human experts were doctoral students, postdoctoral researchers, or faculty/academic staff (see Fig. 3C). Please refer to Supplementary materials for more detailed demographic information including years of experience in neuroscience research about the human experts and distributions of self-reported expertise by subfields (Fig. S.17).</p>
<p>Do judgments from LLMs and human experts align? We considered whether human experts and LLMs found the same benchmark items difficult. For humans, we calculated the mean accuracy for each of the 200 test cases. For LLMs, we calculated the signed differences in perplexity between incorrect and correct abstracts for each test case. Perplexity measures how surprising a text passage is to an LLM. Using these measures (Fig. S.6), the mean Spearman correlation between an LLM and human experts was $0.15( \pm 0.03)$ whereas the mean Spearman correlation between LLMs was $0.75( \pm 0.08)$.</p>
<p>LLMs can integrate information across context To better understand the basis for the remarkable performance of LLMs (see Fig. S. 3 for results), we investigated whether their performance was achieved by integrating information throughout the abstract (including the method used) or by solely relying on the local context in the results passages that differed between the original and altered abstract (Fig. 2)</p>
<p>We re-evaluated the LLMs on individual sentences containing only the altered results passage (i.e., local context only). LLMs performed much worse when restricted to this local context</p>
<p>(Fig. S.3), which provides strong evidence that LLMs are integrating information across the abstract, including information on background and methods. LLM's superior performance relative to human experts appears to arise from integrating information across the abstract.</p>
<p>In addition, we analyzed whether LLMs benefited from a general neuroscience context (similar to few-shot prompting) rather than integrating study-relevant information. We tested models using abstracts with sentences randomly swapped from within the same neuroscience subfield. Both original and altered abstracts were used to re-evaluate LLMs' performance. As shown in Fig. S.4, there was a significant performance decline with coherent versus swapped contexts, indicating that LLMs only partially benefit from accurate, domain-specific, but non-study-relevant context.</p>
<p>LLM performance is not driven by data memorization When LLMs perform well on a benchmark, one general concern is that the benchmark itself was part of the training set, allowing the LLM to memorize the correct answers. To address this concern, we used a commonly applied measure, zlib-perplexity ratio, for evaluating whether LLMs have memorized passages ${ }^{18}$. This ratio gauges the difference between a data-agnostic compression rate of text and data-specific perplexity computed by an LLM (see Methods). Passages that are hard to compress but have low perplexity are indicative of memorization.</p>
<p>We found no indication that BrainBench was memorized by LLMs (Fig. S.7). For comparison, we calculated the zlib-perplexity ratio for a passage that we suspected would be memorized by LLMs, namely the Gettysburg Address. The Gettysburg Address should appear multiple times in an LLM's training set and indeed it showed signs of memorization (Fig. S.7). Interestingly, for some LLMs, we know exactly what they were trained on (see Table S.2). For these models, the distribution of zlib-perplexity ratios heavily overlapped for items that we knew were in the training set and for items, including BrainBench, that we knew were not in the training</p>
<p>set. We suspect that the overlap may indicate scientific articles, which are unlikely to repeat in training sets, are stored in LLMs as general patterns, similar to human schemas, supporting performance on tasks requiring generalization (e.g., BrainBench). This hypothesis invites future study.</p>
<p>As a final check (Fig. S.8; Materials and Methods), we confirmed that LLMs do not perform better on items published earlier in 2023 (e.g., Jan-2023 vs. Oct-2023), which addresses the concern that early items are more likely to have a preprint or other precursor appear in the training set that affected BrainBench performance. All our checks indicated that BrainBench items were novel for the LLMs.</p>
<h1>LLMs and human experts are calibrated</h1>
<p>To assess whether LLMs' predictions are calibrated, we examined how well their confidence tracked their accuracy, a crucial characteristic for a trustworthy prediction system. We estimated LLMs' confidence using the ranked absolute difference in perplexities between two abstracts (Fig. 2; see Methods) and found, like human experts, all LLMs exhibited a positive correlation between accuracy and confidence. When LLMs are confident in their decisions, they are more likely to be correct (Fig. 4). In addition, we fitted logistic regressions between model perplexity differences to their correctness as well as human confidences to their correctness on the individual level. We observed significant positive correlations, confirming both models and humans are calibrated (Table S.3).</p>
<h2>Augmenting LLMs with neuroscience knowledge to create BrainGPT</h2>
<p>Pretrained LLMs can provide a foundation for further training in neuroscience with the aim of improving performance, as assessed by BrainBench. We used Low-Rank Adaptation (LoRA) ${ }^{19}$ to augment a pretrained LLM, Mistral-7B-v0.1, with additional neuroscience knowledge.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Accuracy and confidence are calibrated for human experts and large language models (LLMs). When human experts and LLMs are confident in their BrainBench judgments, they are more likely to be correct. Confidence ratings were sorted and placed in equally-sized bins with the mean accuracy for items in that bin plotted. The positive slope of the black regression lines for human experts and all LLMs indicates that confidence is well calibrated (i.e., higher confidence corresponds to higher accuracy). Calibration is beneficial for humanmachine teams.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Fine-tuning a pretrained large language model (LLM) on neuroscience knowledge. Mistral-7B-v0.1 was fine-tuned using LoRA on neuroscience articles from 2002-2022 (a total of 1.3 billion tokens). (A) The fine-tuned model improved by 3% on BrainBench. (B) The fine-tuning process substantially shifted the perplexity distribution of correct responses, indicative of the LLM specializing in neuroscience.</p>
<p>LoRA is a parameter-efficient fine-tuning technique that inserts low-rank adapter matrices into LLM transformer blocks (Fig. S.18) and trains only these LoRA weights to update the model's behavior. In our case, we fine-tuned Mistral-7B-v0.1 using over 1.3 billion tokens from neuroscience publications spanning 100 journals between 2002 and 2022 (see Methods), which significantly improved performance by 3% on BrainBench (Fig. 5A).</p>
<p>LoRA tuning dramatically shifted (t(199) = 15.7, p &lt; .001) the perplexity of correct responses (Fig. 5B), which is indicative of the LLM specializing for neuroscience material. LoRA introduced 629, 145, 600 new weights, which is 8% of the total number of weights in Mistral-7B-v0.1. These results indicate that BrainGPT models can efficiently be derived by extending existing LLMs.</p>
<h1>Discussion</h1>
<p>We considered whether large language models (LLMs) can forecast the outcome of neuroscience experiments. By training on the vast scientific literature, we hoped LLMs could build a generative model that captured the patterns underlying neuroscience. To evaluate this possibility, we constructed a new forward-looking (Fig. 2) benchmark, BrainBench.</p>
<p>BrainBench assesses a test taker's ability to select which of two versions of a neuroscience abstract contains the actual results of the study (see Fig. 2). We found that LLMs outperform human experts on BrainBench by a considerable margin (see Fig. 3A) across all neuroscience subfields we consider (Fig. 3B). Moreover, the LLMs knew when their predictions were likely to be right or wrong (Fig. 4). LLMs' superior performance arose from their ability to integrate information throughout the abstract, such as text pertaining to the method and study design. When access to such information was removed, LLM performance drastically declined (Fig. S.3).</p>
<p>We found no indication that LLMs had been exposed to and memorized BrainBench items during their training. Instead, our analyses suggested that LLMs discovered the fundamental patterns that underlie neuroscience studies, which enabled LLMs to predict the outcomes of studies that were novel to them. These conclusions were supported by a widely employed technique ${ }^{18}$ to determine text membership within an LLMs' training set (see Fig. S.7). The Galactica ${ }^{20}$ LLMs were particularly illuminating because we know which articles were not in the training set vs. ones that might be. Interestingly, there was no indication of memorization in models such as Galactica for scientific articles that were in its training set, consistent with the notion that LLMs learn broad patterns underlying scientific fields. While passages that frequently repeat in the training set, such as the Gettysburg Address, may be memorized (see Fig. S.7), scientific articles that occur infrequently (most likely once) in the training set</p>
<p>appear to support LLM's forward-looking predictive abilities. As a final check, we trained a relatively small LLM from scratch on the published neuroscience literature (excluding preprints and BrainBench items), which eliminated any possible overlap between training data and BrainBench, and found superhuman performance on BrainBench (Fig. S.2).</p>
<p>LLM's impressive forward-looking capabilities suggest a future in which LLMs help scientists make discoveries. To be effective, LLMs need to stay abreast of the rapidly expanding literature. We found that LLMs could efficiently be augmented with neuroscience knowledge using LoRA ${ }^{19}$, boosting performance on BrainBench (Fig. 5). LoRA provides a way to create BrainGPT models by re-orienting general-purpose LLMs for use in neuroscience. One can easily imagine a future in which BrainGPT is near continuously updated with new knowledge using LoRA, along with complementary approaches such as Retrieval Augmented Generation (RAG; ${ }^{17}$ ). RAG could be used to query a database of relevant and up to date scientific articles for the task at hand.</p>
<p>In addition to keeping LLMs up to date, benchmarks should routinely be refreshed and expanded to address current needs. One challenge is that creating forward-looking benchmarks, such as BrainBench, is labor intensive and requires human expertise. To address this potential bottleneck, we created and evaluated 100 test cases using GPT-4 through a largely automated process (see Methods). Although there is room for improvement, these items were close in quality to the human-created ones with 8 of the 100 items being word-for-word matches with the human-created versions. These efforts should pave the way for the rapid creation of other forward-looking benchmarks in neuroscience, as well as benchmarks for other knowledge intensive fields. We believe high-quality forward-looking benchmarks will be critical to developing LLMs as tools for scientific discovery.</p>
<p>For LLMs to be trustworthy and effective teammates, they need to convey the certainty of their predictions to human scientists. Fortunately, we found that LLMs' confidence is well cal-</p>
<p>ibrated. When LLMs were confident in their predictions, they were more likely to be correct (Fig. 4). A second ingredient for effective teams is being diverse or complementary. LLMs have potential here as well as the items they found difficult did not highly correlate with those human experts found difficult (Fig. S.6). These two ingredients, being well calibrated and complementary, allow systems that combine human and machine judgments to outperform either alone ${ }^{21}$.</p>
<p>All our results, including those for calibrated confidence, were only possible because we had access to LLM weights to calculate the perplexity of passages (see Fig. 2). Our approach diverged from the popular approach of prompting models for responses through natural language (i.e., chat). Prompting in natural language may yield less reliable judgments and degrade model competency compared to using model probability scores or training separate classifiers directly from internal representations ${ }^{22-25}$. These observations underscore the importance of working with models that are as open as possible, ideally making both the weights and training set publicly available. Accordingly, we make BrainGPT available on the Huggingface platform https://huggingface.co/BrainGPT.</p>
<p>Beyond serving as a tool for neuroscientists, BrainGPT can help reveal the structure of the field. In particular, we can vary BrainGPT's training set and observe the effect on BrainBench. For example, what is the effect of including training data from related fields like psychology? In terms of supporting prediction, we can quantify how interrelated fields are. Does it help to weight articles in the training set by their recency, citations, or impact factor? In addition to these training manipulations, we can vary how testing is conducted. For example, would step-by-step thinking via chain-of-thought reasoning ${ }^{26}$ benefit BrainGPT? If prediction in neuroscience is akin to a deductive reasoning process, then it should. If instead, as we suspect, prediction in neuroscience is a function of many noisy intertwined signals across subfields, then chain-of-thought reasoning will not help. BrainGPT and BrainBench can help answer these</p>
<p>meta-science questions.
We foresee a future in which LLMs serve as forward-looking generative models of the scientific literature. LLMs can be part of larger systems that assist researchers in determining the best experiment to conduct next. One key step toward achieving this vision is demonstrating that LLMs can identify likely results. For this reason, BrainBench involved a binary-choice between two possible results. LLMs excelled at this task, which brings us closer to systems that are practically useful. In the future, rather than simply selecting the most likely result for a study, LLMs can generate a set of possible results and judge how likely each is. Scientists may interactively use these future systems to guide the design of their experiments.</p>
<p>One risk is that scientists do not pursue studies when their predictions run counter to those of an LLM. In some cases, this might be a sensible course of action, whereas in other cases the LLM might have identified potential gaps or errors in the scientific literature. In the latter situation, conducting the study might result in a significant breakthrough. Conversely, a study result that was predicted with high confidence by an LLM might be viewed as an incremental advance.</p>
<p>LLMs' predictions are informed by a vast scientific literature that no human could read in their lifetime. As LLMs improve, so should their ability to provide accurate predictions. In this contribution, we focused on neuroscience but our aims are broader - we hope to provide a template for any knowledge intensive field. None of the approaches we adopted are neuroscience-specific. Indeed, the degree of efficacy of our approach may depend on the underlying structure of the domain. For instance, disciplines like mathematics, which rely heavily on logical deduction, might not benefit as much as other scientific fields that involve pattern-based reasoning.</p>
<p>We hope to democratize the use of LLMs in science and increase reproducibility by highlighting the use of relatively small models that can be run locally and whose weights are accessi-</p>
<p>ble, which contrasts with commercial products. Finally, while LLMs appear poised to supplant humans at prediction, we foresee a role for human experts in providing the accompanying scientific explanations. Prediction is very important, but not everything.</p>
<h1>Data availability</h1>
<p>Human participant data, and intermediate data generated via simulations and analyses are publicly available at https://github.com/braingpt-lovelab/BrainBench. Model weights and training data are available at https://huggingface.co/BrainGPT.</p>
<h2>Code availability</h2>
<p>All computer code associated with this work including model training, evaluation, data processing and analyses are publicly available at https://github.com/braingpt-lovelab/BrainBench.</p>
<h2>Acknowledgments</h2>
<p>This work was supported the ESRC (ES/W007347/1), Microsoft (Accelerate Foundation Models Research Program), and a Royal Society Wolfson Fellowship (18302) to B.C.L. We thank Mona Garvert, Pradeep Reddy Raamana, Todd Hare, Yoav Kessler, Oliver Robinson and D.R. for their assistance. We thank the participants of the online study: Gaia Molinaro, Judy Zhu, Majd Abdallah, Yuri G. Pavlov, Jong-eun Lee, Adam Harris, Zhaoning Li, Roman Kessler, Lexi Zhang, Maciej Szul, Pranjul Gupta, Sunreeta Bhattacharya, Jellina Prinsen, Celine Gallagher, Michael Anes, Maarten Laroy, Tobias Ackels, Carina Forster, Pedro Gonçalves, Tommy Mcconnell, Diane Whitmer, Debottam Kundu, Benjamin Pasquereau, Jeremy Manning, Maciej Szul, Ahmed Hussain, Nicolas Clairis, Ignacio Vega-Vásquez, Kun Chen, Jeremy Hogeveen, Sina Salehi, Suseendrakumar Duraivel , Edgar Guevara, Ziyao Zhang, Thomas J. Younts, Marek</p>
<p>Muszyński, Leonardo Dalla Porta, Todd Gureckis, Parnian Rafei, Feng-Chun Chou, Keith Temple, Alp Altunkaya, Andrew Tan, Jin Ho Yun, Arnau Marin-Llobet, Brian Lord, Daniel Lindh, Simon Besson-Girard, Eren Irmak, Emin Çelik, Aman Maharjan, Irene Sophia Plank.</p>
<h1>Declaration of interests</h1>
<p>The authors declare no competing interests.</p>
<h2>Author contributions</h2>
<p>XL and BCL were responsible for primary writing. Test case creation was handled by BY, IPB, AP, TO, AOC, FY, EY, ARM, KL, VB, SR, JMA, RM, MG, GN, LLK, AB, KRP, MK, RR, KKN, AS, JN, DM, CMH, PSL, SM, NEM, JKB, SDB, NG, JD, and BCL. Quality control was conducted by BY, KKN, DM, PSL, NG, CMH, KL, SM, AR, ARM, IS, GN, IPB, RM, TO, MK, JMA, MG, EY, LLK, JD, JN, FY, RR, VB, SDB, AOC, AS, XL, and BCL. GPT-4 case creation was managed by KKN, XL, and BCL. Human-machine teaming involved FY, XL, and BCL. LoRA fine-tuning was performed by GS, XL, and BCL. Model evaluation was executed by XL, GS, MF, and BCL. Building the experiment was carried out by AR, XL, and BCL. Data analysis was done by XL, AR, and BCL. Conceptualization and strategy were undertaken by XL and BCL. Figure creation was the work of XL, BY, IB, CMH, GN, and BCL. Useful input and suggestions on the project were provided by all authors. Commenting and editing on the manuscript were also done by all authors. For a table breakdown, see Table S.5.</p>
<h1>Materials and Methods</h1>
<p>We confirm that our research complies with all relevant ethical regulations; Experimental Psychology Ethics Board (University College London) approved the study protocol (ethics protocol: EP/2017/011); We confirm that informed consent was obtained from all human participants; Participant compensation is not applicable to the current study; None of our studies were preregistered.</p>
<h2>Dataset Creation</h2>
<p>Co-authors (Table S.5) and GPT-4 (Azure OpenAI API; version 2023-05-15) created test cases that formed BrainBench. All test cases were sourced from Journal of Neuroscience abstracts published in 2023 under the Creative Commons Attribution 4.0 International License (CC-BY). The abstracts are organized into five sections, namely Behavioral/Cognitive, Systems/Circuits, Neurobiology of Disease, Development/Plasticity/Repair, and Cellular/Molecular. In constructing BrainBench, we incorporated a total of 200 test cases crafted by human experts and an additional 100 test cases generated by GPT-4 (Azure OpenAI API; version 2023-05-15). All test cases were subjected to extensive quality control by human experts and GPT-4. For the distribution of test cases among subfields, refer to Fig. 3 for human-created cases and Fig. S. 22 for GPT-4 generated cases.</p>
<p>To create a test case, a published abstract was modified to create an altered version. The altered version significantly changed the results without changing the methods and background. Minimal changes were made that changed the basic result. For example, the altered abstract, compared to the original, could switch around the role of two brain regions in the results, reverse the direction of a result (e.g., replace "decreases" with "increases"), etc. Any changes maintained the coherency of the abstract which sometimes required multiple changes (e.g., replacing multiple "decreases" with "increases"). In other words, the altered abstracts needed</p>            </div>
        </div>

    </div>
</body>
</html>
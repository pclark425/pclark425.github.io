<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1659 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1659</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1659</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-248665706</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2205.05061v2.pdf" target="_blank">On the Verge of Solving Rocket League using Deep Reinforcement Learning and Sim-to-sim Transfer</a></p>
                <p><strong>Paper Abstract:</strong> Autonomously trained agents that are supposed to play video games reasonably well rely either on fast simulation speeds or heavy parallelization across thousands of machines running concurrently. This work explores a third way that is established in robotics, namely sim-to-real transfer, or if the game is considered a simulation itself, sim-to-sim transfer. In the case of Rocket League, we demonstrate that single behaviors of goalies and strikers can be successfully learned using Deep Reinforcement Learning in the simulation environment and transferred back to the original game. Although the implemented training simulation is to some extent inaccurate, the goalkeeping agent saves nearly 100% of its faced shots once transferred, while the striking agent scores in about 75% of cases. Therefore, the trained agent is robust enough and able to generalize to the target domain of Rocket League.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1659.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1659.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL-Goaler</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rocket League Goalie Agent (Unity -> Rocket League)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-agent goalkeeper policy trained with PPO in a custom Unity simulation to save shots, then transferred to the original Rocket League game via the RLBot interface; demonstrates robust transfer despite simulation inaccuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Rocket League Goalie Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>An autonomous agent controlling a Rocket League car avatar to defend the goal by intercepting and deflecting incoming shots; perceives 23 normalized state variables and outputs a multi-discrete action tuple (throttle, steer, yaw, pitch, roll, boost, drift/air-roll, jump).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>video-game sim-to-sim (analogous to sim-to-real robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Custom Unity training simulation (based on RoboLeague)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Unity-based physics simulation implementing key Rocket League mechanics: ball and car dynamics (velocity/acceleration), boosting, jumps/double-jumps, dodge rolls, collisions (Bullet + Psyonix impulse), air/ground friction and drag, stabilization and approximated suspension; abstract visual rendering; runs ~950 steps/sec.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate / simplified physics (engineered to mimic many mechanics but not fully identical to Rocket League)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>ball and car velocity/acceleration, boosting, jumps/double-jumps, dodge rolls, collisions using Bullet+Psyonix impulse, air and ground friction/drag, ground and wall stabilization, suspension approximations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>visual rendering and camera behavior not matched to Rocket League; some physics components incomplete or approximate (car-to-car interaction and demolitions not thoroughly implemented/tested), several parameter adjustments made empirically (e.g., max angular velocity), discrete action simplification of original continuous controls, partial inaccuracies in shot/hit/bounce outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Original Rocket League game running via RLBot API on PC (real-time constraint ~120 steps/sec), full game physics and rendering provided by Psyonix's engine.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Goalkeeping: saving shots on goal (deflecting/hitting incoming ball trajectories to prevent goals).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>On-policy deep reinforcement learning (Proximal Policy Optimization, PPO) in Unity with multiple concurrent worker environments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Shot save success rate (percent of faced shots that were saved/deflected)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Approximately 100% save success in the Unity training/evaluation (task reported as 'easily solves the goalie task' in simulation and identical behavior across training/unseen shots).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Nearly 100% of faced shots saved after transfer to Rocket League (reported as 'saves nearly 100% of its faced shots once transferred').</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>mismatched physics parameters (e.g., angular velocity), incomplete car-to-car interactions and demolitions, different numerical integration/impulse behaviors, timing differences between simulation step rates (~950 steps/s vs 120 steps/s), absence of Rocket League camera/rendered-image observations, simplified/discretized action space</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Modeling of the essential ball and car physics (even if approximate), explicit implementation of key mechanics (boost, jumps, dodge), and robustness of learned policy to simulation inaccuracies; relatively simple goalkeeper task where any deflection suffices reduces sensitivity to fine physics errors.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper indicates contact / collision handling and basic ball/car dynamics are important; exact rendering/camera and some car-to-car mechanics are less critical for goalkeeper transfer, but some physics adaptations (e.g., impulses, angular velocity) materially affect alignment—no strict numeric fidelity thresholds given.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Ablation study: goalie transfer remained robust across ablations (nearly perfect saves), while striker transfer collapsed when single physical adaptations were removed; indicates task-dependent fidelity sensitivity (goalie robust, striker requires more accurate physics).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A relatively simple but targeted physics simulation suffices to transfer robust goalkeeping behaviors to Rocket League (near-100% saves) despite imperfect fidelity; transfer success is task-dependent — simpler deflection tasks are tolerant to simulation inaccuracies, while precision tasks (striker) require more faithful physics components.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Verge of Solving Rocket League using Deep Reinforcement Learning and Sim-to-sim Transfer', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1659.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1659.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL-Striker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rocket League Striker Agent (Unity -> Rocket League)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-agent striker policy trained with PPO in the same custom Unity simulation to approach and accurately contact the ball to score goals, then transferred to Rocket League; success depends strongly on specific simulated physics components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Rocket League Striker Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>An autonomous agent controlling a Rocket League car avatar to score goals by making accurate touches on a bouncing/moving ball; same observation/action interface as the goalie agent.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>video-game sim-to-sim (analogous to sim-to-real robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Custom Unity training simulation (based on RoboLeague)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Unity-based physics simulation (see goalie entry) that models ball and car dynamics, bouncy ball behavior, boosting, jumps and dodge maneuvers, collisions with approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate / simplified physics; more fidelity required for striker than goalie</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>ball bounce dynamics, car dynamics, boosts, jumps, dodge rolls, collisions (Bullet + Psyonix impulse), friction and drag</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>visual/camera rendering differences, discretized action space, incomplete car-to-car interactions and demolitions, empirical parameter adjustments; observed inaccuracies in shot outcomes and bounce behavior that can change contact timing/location</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Original Rocket League game via RLBot API on PC (real-time 120 steps/sec) providing the ground-truth physics and rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Striking: making precise touches to a moving/bouncing ball to score goals</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep Reinforcement Learning using PPO in Unity on many seeded shot scenarios (1000 shot samples during training)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Goal scoring success rate (percent of played episodes that resulted in a goal)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>About 75% scoring success in Rocket League when all implemented physical adaptations are enabled; ablating individual physical adaptations yields catastrophic performance drops.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>sensitivity to specific physics components (errors in bounce/contact modeling, angular velocity, impulses), discretized controls, timing/step-rate differences, visual/camera mismatch and partial observability not modeled</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Inclusion of all implemented physics adaptations (Psyonix impulse, adjusted angular velocity, drag/stabilization parameters); precise modeling of contact and bounce dynamics is critical for striker performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper identifies that certain physics adaptations (e.g., Psyonix impulse, angular velocity settings, drag/stabilization) are essential for successful striker transfer — turning off single adaptations causes catastrophic failure; no quantitative error tolerances given.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>When all physical adaptations are present, transferred striker scores ≈75%; ablating any single adaptation (or using default Unity physics) leads to catastrophic failure in Rocket League — indicating high sensitivity of precise striking skills to simulation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Striker behaviors require more accurate and specific physics modeling for reliable sim-to-sim transfer; omitting or simplifying individual physics components can eliminate transfer success, demonstrating task-dependent fidelity requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Verge of Solving Rocket League using Deep Reinforcement Learning and Sim-to-sim Transfer', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1659.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1659.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraspGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraspGAN (pixel-level domain adaptation via GANs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GAN-based pixel-level domain adaptation method that translates synthetic images to more realistic ones to reduce the sim-to-real visual gap for robotic grasping policies; mentioned as an approach that improves transfer success for policies learned in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Grasping unknown objects by coupling deep reinforcement learning, generative adversarial networks, and visual servoing.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Grasping policies with GAN-based pixel adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Robotic grasping systems whose simulated visual inputs are translated to pseudo-realistic images with a GAN to better match real camera observations and improve transferred policy performance.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic manipulation / grasping</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Synthetic image simulator + GAN-based translator (GraspGAN)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulation of object geometries and robot scene rendered synthetically; GraspGAN translates synthetic images to pseudo-real images at pixel level to approximate real-world appearance.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>simulation of visuals enhanced via pixel-level adaptation (photorealism via GAN), underlying physics may remain simulated</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>visual appearance/lighting via GAN translation; synthetic images mapped to realistic-looking images</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>physical interactions and some dynamics may still be approximate; paper mentions only pixel-level correction, not full physics fidelity improvements</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real robot grasping with real camera images; GraspGAN produces pseudo-real images to close visual gap.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Grasping unknown objects / robot grasping policies</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Simulation-based RL combined with GAN-based domain adaptation of images</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Qualitative / reported improvement in execution success of policies on real robots when using GraspGAN (paper states policies execute more successfully on real robots with GraspGAN)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>visual appearance differences between simulation and real camera images; sim-to-real gap addressed at pixel level</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Pixel-level domain adaptation with GANs that translate synthetic training images into pseudo-real images improves generalization of policies to real visual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper suggests visual realism at pixel level helps, but does not quantify required physical fidelity; emphasis is on correcting rendered image mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pixel-level domain adaptation (GraspGAN) can reduce the visual component of the sim-to-real gap and improve execution success of policies trained in simulation on real robots (as cited by the Rocket League paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Verge of Solving Rocket League using Deep Reinforcement Learning and Sim-to-sim Transfer', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1659.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1659.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DomainRandomization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization (for sim-to-real)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that trains agents across a wide distribution of randomized simulation parameters (physical and visual) so learned policies generalize to the real world; cited examples include solving a Rubik's cube with a robotic hand.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Domain randomization for transferring deep neural networks from simulation to the real world</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Domain-randomized RL policies (general)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Policies trained in many randomized simulated domains (varying dynamics, visual textures, lighting, etc.) to encourage robustness and generalization to the unmodeled real-world domain.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics (general), example: robotic manipulation (Rubik's cube solver), autonomous racing, robot soccer</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Varies by study (synthetic simulators with randomized parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulators in which physical parameters (masses, frictions), visual properties (textures, lighting), and other aspects are randomized across training episodes to produce robust policies.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>broad distribution sampling over parameterized simulators rather than single high-fidelity model; emphasis on variability rather than per-simulation fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>multiple physics and visual parameters sampled from wide ranges (mass, friction, lighting, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Individual simulations may be low-to-moderate fidelity; correctness achieved by sampling wide parameter ranges rather than exact modeling</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real robots/tasks where policies must generalize despite differences from any single simulated instance (e.g., robotic hand solving Rubik's cube in real world).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Various — robotic manipulation (e.g., Rubik's cube), grasping, control tasks, autonomous driving/racing</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning with randomized simulator parameters (domain randomization)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success rates in the real environment after training under randomized simulation domains (paper cites successful transfers in other works)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>As described: randomize physical properties and visual appearances during simulation training (masses, friction, textures, lighting, etc.) to promote generalization</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>unmodeled exact physical parameters and visual appearance differences; addressed by exposing agent to wide variation during training</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Extensive randomization across relevant physical and visual parameters during simulation training to encourage policies that are robust to real-world variability.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper suggests domain randomization can compensate for lack of exact simulation fidelity by training across wide parameter distributions; no quantitative fidelity thresholds provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain randomization is proposed as a promising approach to improve robustness and generalization of simulated policies to real environments; the Rocket League authors recommend it as a path to make agents less sensitive to remaining simulation inaccuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Verge of Solving Rocket League using Deep Reinforcement Learning and Sim-to-sim Transfer', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Grasping unknown objects by coupling deep reinforcement learning, generative adversarial networks, and visual servoing. <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Solving rubik's cube with a robot hand <em>(Rating: 2)</em></li>
                <li>Deepracer: Autonomous racing platform for experimentation with sim2real reinforcement learning <em>(Rating: 1)</em></li>
                <li>Sim-to-real transfer in deep reinforcement learning for robotics: a survey <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1659",
    "paper_id": "paper-248665706",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "RL-Goaler",
            "name_full": "Rocket League Goalie Agent (Unity -&gt; Rocket League)",
            "brief_description": "A single-agent goalkeeper policy trained with PPO in a custom Unity simulation to save shots, then transferred to the original Rocket League game via the RLBot interface; demonstrates robust transfer despite simulation inaccuracies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Rocket League Goalie Agent",
            "agent_system_description": "An autonomous agent controlling a Rocket League car avatar to defend the goal by intercepting and deflecting incoming shots; perceives 23 normalized state variables and outputs a multi-discrete action tuple (throttle, steer, yaw, pitch, roll, boost, drift/air-roll, jump).",
            "domain": "video-game sim-to-sim (analogous to sim-to-real robotics)",
            "virtual_environment_name": "Custom Unity training simulation (based on RoboLeague)",
            "virtual_environment_description": "Unity-based physics simulation implementing key Rocket League mechanics: ball and car dynamics (velocity/acceleration), boosting, jumps/double-jumps, dodge rolls, collisions (Bullet + Psyonix impulse), air/ground friction and drag, stabilization and approximated suspension; abstract visual rendering; runs ~950 steps/sec.",
            "simulation_fidelity_level": "approximate / simplified physics (engineered to mimic many mechanics but not fully identical to Rocket League)",
            "fidelity_aspects_modeled": "ball and car velocity/acceleration, boosting, jumps/double-jumps, dodge rolls, collisions using Bullet+Psyonix impulse, air and ground friction/drag, ground and wall stabilization, suspension approximations",
            "fidelity_aspects_simplified": "visual rendering and camera behavior not matched to Rocket League; some physics components incomplete or approximate (car-to-car interaction and demolitions not thoroughly implemented/tested), several parameter adjustments made empirically (e.g., max angular velocity), discrete action simplification of original continuous controls, partial inaccuracies in shot/hit/bounce outcomes",
            "real_environment_description": "Original Rocket League game running via RLBot API on PC (real-time constraint ~120 steps/sec), full game physics and rendering provided by Psyonix's engine.",
            "task_or_skill_transferred": "Goalkeeping: saving shots on goal (deflecting/hitting incoming ball trajectories to prevent goals).",
            "training_method": "On-policy deep reinforcement learning (Proximal Policy Optimization, PPO) in Unity with multiple concurrent worker environments.",
            "transfer_success_metric": "Shot save success rate (percent of faced shots that were saved/deflected)",
            "transfer_performance_sim": "Approximately 100% save success in the Unity training/evaluation (task reported as 'easily solves the goalie task' in simulation and identical behavior across training/unseen shots).",
            "transfer_performance_real": "Nearly 100% of faced shots saved after transfer to Rocket League (reported as 'saves nearly 100% of its faced shots once transferred').",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "mismatched physics parameters (e.g., angular velocity), incomplete car-to-car interactions and demolitions, different numerical integration/impulse behaviors, timing differences between simulation step rates (~950 steps/s vs 120 steps/s), absence of Rocket League camera/rendered-image observations, simplified/discretized action space",
            "transfer_enabling_conditions": "Modeling of the essential ball and car physics (even if approximate), explicit implementation of key mechanics (boost, jumps, dodge), and robustness of learned policy to simulation inaccuracies; relatively simple goalkeeper task where any deflection suffices reduces sensitivity to fine physics errors.",
            "fidelity_requirements_identified": "Paper indicates contact / collision handling and basic ball/car dynamics are important; exact rendering/camera and some car-to-car mechanics are less critical for goalkeeper transfer, but some physics adaptations (e.g., impulses, angular velocity) materially affect alignment—no strict numeric fidelity thresholds given.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Ablation study: goalie transfer remained robust across ablations (nearly perfect saves), while striker transfer collapsed when single physical adaptations were removed; indicates task-dependent fidelity sensitivity (goalie robust, striker requires more accurate physics).",
            "key_findings": "A relatively simple but targeted physics simulation suffices to transfer robust goalkeeping behaviors to Rocket League (near-100% saves) despite imperfect fidelity; transfer success is task-dependent — simpler deflection tasks are tolerant to simulation inaccuracies, while precision tasks (striker) require more faithful physics components.",
            "uuid": "e1659.0",
            "source_info": {
                "paper_title": "On the Verge of Solving Rocket League using Deep Reinforcement Learning and Sim-to-sim Transfer",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "RL-Striker",
            "name_full": "Rocket League Striker Agent (Unity -&gt; Rocket League)",
            "brief_description": "A single-agent striker policy trained with PPO in the same custom Unity simulation to approach and accurately contact the ball to score goals, then transferred to Rocket League; success depends strongly on specific simulated physics components.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Rocket League Striker Agent",
            "agent_system_description": "An autonomous agent controlling a Rocket League car avatar to score goals by making accurate touches on a bouncing/moving ball; same observation/action interface as the goalie agent.",
            "domain": "video-game sim-to-sim (analogous to sim-to-real robotics)",
            "virtual_environment_name": "Custom Unity training simulation (based on RoboLeague)",
            "virtual_environment_description": "Unity-based physics simulation (see goalie entry) that models ball and car dynamics, bouncy ball behavior, boosting, jumps and dodge maneuvers, collisions with approximations.",
            "simulation_fidelity_level": "approximate / simplified physics; more fidelity required for striker than goalie",
            "fidelity_aspects_modeled": "ball bounce dynamics, car dynamics, boosts, jumps, dodge rolls, collisions (Bullet + Psyonix impulse), friction and drag",
            "fidelity_aspects_simplified": "visual/camera rendering differences, discretized action space, incomplete car-to-car interactions and demolitions, empirical parameter adjustments; observed inaccuracies in shot outcomes and bounce behavior that can change contact timing/location",
            "real_environment_description": "Original Rocket League game via RLBot API on PC (real-time 120 steps/sec) providing the ground-truth physics and rendering.",
            "task_or_skill_transferred": "Striking: making precise touches to a moving/bouncing ball to score goals",
            "training_method": "Deep Reinforcement Learning using PPO in Unity on many seeded shot scenarios (1000 shot samples during training)",
            "transfer_success_metric": "Goal scoring success rate (percent of played episodes that resulted in a goal)",
            "transfer_performance_sim": null,
            "transfer_performance_real": "About 75% scoring success in Rocket League when all implemented physical adaptations are enabled; ablating individual physical adaptations yields catastrophic performance drops.",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "sensitivity to specific physics components (errors in bounce/contact modeling, angular velocity, impulses), discretized controls, timing/step-rate differences, visual/camera mismatch and partial observability not modeled",
            "transfer_enabling_conditions": "Inclusion of all implemented physics adaptations (Psyonix impulse, adjusted angular velocity, drag/stabilization parameters); precise modeling of contact and bounce dynamics is critical for striker performance.",
            "fidelity_requirements_identified": "Paper identifies that certain physics adaptations (e.g., Psyonix impulse, angular velocity settings, drag/stabilization) are essential for successful striker transfer — turning off single adaptations causes catastrophic failure; no quantitative error tolerances given.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "When all physical adaptations are present, transferred striker scores ≈75%; ablating any single adaptation (or using default Unity physics) leads to catastrophic failure in Rocket League — indicating high sensitivity of precise striking skills to simulation fidelity.",
            "key_findings": "Striker behaviors require more accurate and specific physics modeling for reliable sim-to-sim transfer; omitting or simplifying individual physics components can eliminate transfer success, demonstrating task-dependent fidelity requirements.",
            "uuid": "e1659.1",
            "source_info": {
                "paper_title": "On the Verge of Solving Rocket League using Deep Reinforcement Learning and Sim-to-sim Transfer",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "GraspGAN",
            "name_full": "GraspGAN (pixel-level domain adaptation via GANs)",
            "brief_description": "A GAN-based pixel-level domain adaptation method that translates synthetic images to more realistic ones to reduce the sim-to-real visual gap for robotic grasping policies; mentioned as an approach that improves transfer success for policies learned in simulation.",
            "citation_title": "Grasping unknown objects by coupling deep reinforcement learning, generative adversarial networks, and visual servoing.",
            "mention_or_use": "mention",
            "agent_system_name": "Grasping policies with GAN-based pixel adaptation",
            "agent_system_description": "Robotic grasping systems whose simulated visual inputs are translated to pseudo-realistic images with a GAN to better match real camera observations and improve transferred policy performance.",
            "domain": "robotic manipulation / grasping",
            "virtual_environment_name": "Synthetic image simulator + GAN-based translator (GraspGAN)",
            "virtual_environment_description": "Simulation of object geometries and robot scene rendered synthetically; GraspGAN translates synthetic images to pseudo-real images at pixel level to approximate real-world appearance.",
            "simulation_fidelity_level": "simulation of visuals enhanced via pixel-level adaptation (photorealism via GAN), underlying physics may remain simulated",
            "fidelity_aspects_modeled": "visual appearance/lighting via GAN translation; synthetic images mapped to realistic-looking images",
            "fidelity_aspects_simplified": "physical interactions and some dynamics may still be approximate; paper mentions only pixel-level correction, not full physics fidelity improvements",
            "real_environment_description": "Real robot grasping with real camera images; GraspGAN produces pseudo-real images to close visual gap.",
            "task_or_skill_transferred": "Grasping unknown objects / robot grasping policies",
            "training_method": "Simulation-based RL combined with GAN-based domain adaptation of images",
            "transfer_success_metric": "Qualitative / reported improvement in execution success of policies on real robots when using GraspGAN (paper states policies execute more successfully on real robots with GraspGAN)",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "visual appearance differences between simulation and real camera images; sim-to-real gap addressed at pixel level",
            "transfer_enabling_conditions": "Pixel-level domain adaptation with GANs that translate synthetic training images into pseudo-real images improves generalization of policies to real visual inputs.",
            "fidelity_requirements_identified": "Paper suggests visual realism at pixel level helps, but does not quantify required physical fidelity; emphasis is on correcting rendered image mismatch.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Pixel-level domain adaptation (GraspGAN) can reduce the visual component of the sim-to-real gap and improve execution success of policies trained in simulation on real robots (as cited by the Rocket League paper).",
            "uuid": "e1659.2",
            "source_info": {
                "paper_title": "On the Verge of Solving Rocket League using Deep Reinforcement Learning and Sim-to-sim Transfer",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "DomainRandomization",
            "name_full": "Domain Randomization (for sim-to-real)",
            "brief_description": "A technique that trains agents across a wide distribution of randomized simulation parameters (physical and visual) so learned policies generalize to the real world; cited examples include solving a Rubik's cube with a robotic hand.",
            "citation_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "mention_or_use": "mention",
            "agent_system_name": "Domain-randomized RL policies (general)",
            "agent_system_description": "Policies trained in many randomized simulated domains (varying dynamics, visual textures, lighting, etc.) to encourage robustness and generalization to the unmodeled real-world domain.",
            "domain": "robotics (general), example: robotic manipulation (Rubik's cube solver), autonomous racing, robot soccer",
            "virtual_environment_name": "Varies by study (synthetic simulators with randomized parameters)",
            "virtual_environment_description": "Simulators in which physical parameters (masses, frictions), visual properties (textures, lighting), and other aspects are randomized across training episodes to produce robust policies.",
            "simulation_fidelity_level": "broad distribution sampling over parameterized simulators rather than single high-fidelity model; emphasis on variability rather than per-simulation fidelity",
            "fidelity_aspects_modeled": "multiple physics and visual parameters sampled from wide ranges (mass, friction, lighting, etc.)",
            "fidelity_aspects_simplified": "Individual simulations may be low-to-moderate fidelity; correctness achieved by sampling wide parameter ranges rather than exact modeling",
            "real_environment_description": "Real robots/tasks where policies must generalize despite differences from any single simulated instance (e.g., robotic hand solving Rubik's cube in real world).",
            "task_or_skill_transferred": "Various — robotic manipulation (e.g., Rubik's cube), grasping, control tasks, autonomous driving/racing",
            "training_method": "Reinforcement learning with randomized simulator parameters (domain randomization)",
            "transfer_success_metric": "Task success rates in the real environment after training under randomized simulation domains (paper cites successful transfers in other works)",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "As described: randomize physical properties and visual appearances during simulation training (masses, friction, textures, lighting, etc.) to promote generalization",
            "sim_to_real_gap_factors": "unmodeled exact physical parameters and visual appearance differences; addressed by exposing agent to wide variation during training",
            "transfer_enabling_conditions": "Extensive randomization across relevant physical and visual parameters during simulation training to encourage policies that are robust to real-world variability.",
            "fidelity_requirements_identified": "Paper suggests domain randomization can compensate for lack of exact simulation fidelity by training across wide parameter distributions; no quantitative fidelity thresholds provided in this paper.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Domain randomization is proposed as a promising approach to improve robustness and generalization of simulated policies to real environments; the Rocket League authors recommend it as a path to make agents less sensitive to remaining simulation inaccuracies.",
            "uuid": "e1659.3",
            "source_info": {
                "paper_title": "On the Verge of Solving Rocket League using Deep Reinforcement Learning and Sim-to-sim Transfer",
                "publication_date_yy_mm": "2022-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Grasping unknown objects by coupling deep reinforcement learning, generative adversarial networks, and visual servoing.",
            "rating": 2,
            "sanitized_title": "grasping_unknown_objects_by_coupling_deep_reinforcement_learning_generative_adversarial_networks_and_visual_servoing"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Solving rubik's cube with a robot hand",
            "rating": 2,
            "sanitized_title": "solving_rubiks_cube_with_a_robot_hand"
        },
        {
            "paper_title": "Deepracer: Autonomous racing platform for experimentation with sim2real reinforcement learning",
            "rating": 1,
            "sanitized_title": "deepracer_autonomous_racing_platform_for_experimentation_with_sim2real_reinforcement_learning"
        },
        {
            "paper_title": "Sim-to-real transfer in deep reinforcement learning for robotics: a survey",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_in_deep_reinforcement_learning_for_robotics_a_survey"
        }
    ],
    "cost": 0.013611999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On the Verge of Solving Rocket League using Deep Reinforcement Learning and Sim-to-sim Transfer</p>
<p>Marco Pleines 
Konstantin Ramthun 
Yannik Wegener 
Hendrik Meyer 
Matthias Pallasch 
Sebastian Prior 
Jannik Drögemüller 
Leon Büttinghaus 
Thilo Röthemeyer 
Alexander Kaschwig 
Oliver Chmurzynski 
Frederik Rohkrähmer 
Roman Kalkreuth 
Frank Zimmer 
Department of Communication and Environment
Rhine-Waal University of Applied Sciences
Kamp-Linfort
Germany</p>
<p>Mike Preuss 
LIACS Universiteit Leiden
LeidenNetherlands</p>
<p>Department of Computer Science
TU Dortmund University
DortmundGermany</p>
<p>On the Verge of Solving Rocket League using Deep Reinforcement Learning and Sim-to-sim Transfer
A99D90C9B2FB2AC640BB5D5A4D3FB53Crocket leaguesim-to-sim transferdeep reinforcement learningproximal policy optimization
Autonomously trained agents that are supposed to play video games reasonably well rely either on fast simulation speeds or heavy parallelization across thousands of machines running concurrently.This work explores a third way that is established in robotics, namely sim-to-real transfer, or if the game is considered a simulation itself, sim-to-sim transfer.In the case of Rocket League, we demonstrate that single behaviors of goalies and strikers can be successfully learned using Deep Reinforcement Learning in the simulation environment and transferred back to the original game.Although the implemented training simulation is to some extent inaccurate, the goalkeeping agent saves nearly 100% of its faced shots once transferred, while the striking agent scores in about 75% of cases.Therefore, the trained agent is robust enough and able to generalize to the target domain of Rocket League.</p>
<p>I. INTRODUCTION</p>
<p>The spectacular successes of agents playing considerably difficult games, such as StarCraft II [1] and DotA 2 [2], have been possible only because the employed algorithms were able to train on huge numbers of games on the order of billions or more.Unfortunately, and despite many improvements achieved in AI in recent years, the utilized Deep Learning methods are still relatively sample inefficient.To deal with this problem, fast running environments or high amounts of computing resources are vital.OpenAI Five for DotA 2 [2] is an example of the utilization of hundreds of thousands of computing cores in order to achieve high throughput in terms of played games.However, this way is closed for games that run only on specific platforms and are thus very hard to parallelize.Moreover, not many research groups have such resources at their disposal.Video games that suffer from not being able to be sped up significantly, risk minimal running times and hence repeatability.Therefore it makes sense to look for alternative ways to tackle difficult problems.</p>
<p>Sim-to-real transfer offers such an alternative way and is well established in robotics, and it follows the general idea that robot behavior can be learned in a very simplified simulation environment and the trained agents can then be successfully transferred to the original environment.If the target platform is a game as well, we may speak of sim-to-sim transfer because the original game is also virtual, just computationally Fig. 1.The game of Rocket League (top) and the contributed simulation (bottom), which notably advances its ancestor project RoboLeague [3].much more costly.This approach is applicable to current games, even if they are not parallelizable, and makes them available for modern Deep Reinforcement Learning (DRL) methods.There is of course a downside of this approach, namely that it may be difficult or even infeasible to establish a simulation that is similar enough to enable transfer later on, but still simple enough to speed up learning significantly.A considerable amount of effort has to be invested in establishing this simulation environment before we can make any progress on the learning task.</p>
<p>To our knowledge, the sim-to-sim approach has not yet been applied to train agents for a recent game.Therefore we aim to explore the possibilities of this direction in order to detect how simple the simulation can be, and how good the transfer to the original game works.</p>
<p>The game we choose as a test case of the sim-to-sim approach is Rocket League (Figure 1), which basically resembles indoor football with cars and teams of 3. Rocket league is freely available for Windows and Mac, possesses a bot API (RLBot [4]) and a community of bot developers next to a large human player base.As the 3 members of each team control car avatars with physical properties different from human runners, the overall tactics are the one of rotation without fixed roles.Thereby, large parts of the current speed can be conserved and players do not have to accelerate from zero when ball possession changes [5].Next to basic abilities attempting to shoot towards the goal and to move the goalie in order to prevent a goal, Rocket League is a minimal team AI setting [6] where layers of team tactics and strategy can be learned.</p>
<p>The first step of our work re-implements not all, but multiple physical gameplay mechanics of Rocket League using the game engine Unity, which results in a slightly inaccurate simulation.We then train an agent in a relatively easy goalie and striker environment using Proximal Policy Optimization (PPO) [7].The learned behaviors are then transferred to Rocket League for evaluation.Even though the training simulation is imperfect, the transferred behaviors are robust enough to succeed at their tasks by generalizing to the domain of Rocket League.The goalkeeping agent saves nearly 100% of the shots faced, while the striking agent scores about 75% of its shots.The sim-to-sim transfer is further examined by ablating physical adaptations that were added to the training simulation.</p>
<p>This paper proceeds with elaborating on related work.Then, the physical gameplay mechanics of Rocket League are shown.After illustrating the trained goalie and striker environment, PPO and algorithmic details are presented.Section V examines the sim-to-sim transfer.Before concluding our work, a discussion is provided.</p>
<p>II. RELATED WORK</p>
<p>Sim-to-sim transfer on a popular multiplayer team video game touches majorly on two different areas, namely multiagent and sim-to-real transfer.DotA 2 and StarCraft II are the already mentioned prominent examples in the field of multiagent environments.As this work focuses on single-agent environments, namely the goalkeeper and striker environments, related work on sim-to-real transfer is focused next.</p>
<p>Given the real world, a considered prime example for multi-agent scenarios is RoboCup.RoboCup is an annual international competition [8] that offers a publicly effective open challenge for the intersection of robotics and AI research.The competition is known for the robot soccer cup but also includes other challenges.Reinforcement Learning (RL) has been successfully applied to simulated robot soccer in the past [9] and has been found a powerful method for tackling robot soccer.A recent survey [10] provides insights into robot soccer and highlights significant trends, which briefly mention the transfer from simulation to the real world.</p>
<p>In general, sim-to-real transfer is a well-established method for robot learning and is widely used in combination with RL.It allows the transition of an RL agent's behavior, which has been trained in simulations, to real-world environments.Simto-real transfer has been predominantly applied to RL-based robotics [11] where the robotic agent has been trained with state-of-the-art RL techniques like PPO [7].Popular applications for sim-to-real transfer in robotics have been autonomous racing [12], Robot Soccer [13], navigation [14], and control tasks [15].To address the inability to exactly match the realworld environment, a challenge commonly known as sim-toreal gap, steps have also been taken towards generalized simto-real transfer for robot learning [16], [17].The translation of synthetic images to realistic ones at the pixel level is employed by a method called GraspGAN [18] which utilizes a generative adversarial network (GAN) [19].GANs are able to generate synthetic data with good generalization ability.This property can be used for image synthesis to model the transformation between simulated and real images.GraspGAN provides a method called pixel-level domain adaptation, which translates synthetic images to realistic ones at the pixel level.The synthesized pseudo-real images correct the sim-to-real gap to some extent.Overall, it has been found that the respective policies learned with simulations execute more successfully on real robots when GraspGAN is used [18].</p>
<p>Another approach to narrow the sim-to-real gap is domain randomization [20].Its goal is to train the agent in plenty of randomized domains to generalize to the real domain.By randomizing all physical properties and visual appearances during training in the simulation, a trained behavior was successfully transferred to the real world to solve the Rubik's cube [21].</p>
<p>III. ROCKET LEAGUE ENVIRONMENT</p>
<p>This section starts out by providing an overview of vital components of Rocket League's physical gameplay mechanics, which are implemented in the training simulation based on the game engine Unity and the ML-Agents Toolkit [22].RLBot [4] provides the interface to Rocket League where the training situations can be reproduced.Afterward, the DRL environments, designated for training, and their properties are detailed.The code is open source1 .</p>
<p>A. Implementation of the Training Simulation</p>
<p>The implementation of the Unity simulation originates from the so called RoboLeague repository [3].As this version of the simulation is by far incomplete and inaccurate, multiple fundamental aspects and concepts are implemented, which are essentially based on the physical specifications of Rocket League.These comprise, for example, the velocity and acceleration of the ball and the car, as well as the concept of boosting.Jumps, double jumps as well as dodge rolls are now possible, and also collisions and interactions.There is friction caused by the interaction of a car with the ground, but also friction caused by the air is taken into account.</p>
<p>However, further adjustments are necessary.Therefore, table I provides an overview of all the material that was considered during implementing essential physical components, while highlighting distinct adjustments that differ from the information provided by the references.It has to be noted</p>
<p>Physics Component Sources Additional Information and Different Parameters</p>
<p>Entity Measures (e.g.Arena) [3], [4] Car model Octane and its collision mesh is used Radius of the ball is set to 93.15uu (value in Rocket League 92.75uu) Car: Velocity, Acceleration, Boost [23] No modifications done Car: Jumps, Double Jumps, Dodge Rolls [4], [24] Raise max.angular velocity during dodge from 5.5 rad s to 7.3 rad s Car: Air Control [23] Adjust drag coefficients for roll to −4.75 and pitch to −2.85</p>
<p>Bullet and Psyonix Impulse [23], [25] Used for the ball-to-car interaction and car-to-car interaction.The impulse by the bullet engine replaces the Unity one.Psyonix impulse is an additional impulse on the center of the ball, which allows a better prediction and control of collisions.Ball Bouncing [23] Within the bounce's computation a ball radius of 91.25uu is considered.Friction (Air, Ground) and Drifting [25] A drag of −525 uu s 2 is used, which is reduced by more than half when the car is upside down.Ground Stabilization [4], [25] The stabilization torque is denoted by an acceleration of 50 rad s 2 .Wall Stabilization [4] Raise sticky forces for wall stabilization to an acceleration of 500 uu
s 2 Suspension [26] [27]
Stiffness of front wheels: 163.9 1 s 2 and of back wheels: 275.4 1
s 2
Damper front and back is set to 30 1 s .The equations used are inspired by [27], which may differ to the approach taken in Rocket League that remains unclear.Car-to-car interaction Implemented using the Bullet and Psyonix impulses, but not thoroughly tested Demolitions [28] Implemented, but not thoroughly tested and hence not considered in this paper  that most measures are given in unreal units (uu).To convert them to Unity's scale, these have to be divided by 100.</p>
<p>Some adjustments are based on empirical findings by comparing the outcome of distinct physical maneuvers inside the implemented training simulation and the ground truth provided by Rocket League.A physical maneuver simulates several player inputs over time, such as applying throttle and steering left or right.While the simulation is conducted in both simulations, multiple relevant game state variables like positions, rotations, and velocities are monitored for later evaluation.Figure 2 is an example where the physical maneuver orders the car to execute a dodge roll.Whereas the original max angular velocity of 5.5 rad s does not compare well to the ground truth, a more suitable value of 7.3 rad s is found by analyzing the observed data.</p>
<p>The speed of the training simulation is about 950 steps/second, while RLBot is constrained to the real-time, where only 120 steps/second are possible.This simulation performance is measured on a Windows Desktop</p>
<p>B. Goalie Environment</p>
<p>In the goalie environment, the agent is asked to save shots.1000 different samples of shots, which uniformly vary in speed, direction, and origin, are faced by the agent during training.In every episode, one shot is fired towards the agent's goal.The agent's position is reset to the center of the goal at the start of each episode.Every save rewards the agent with +1.A goalkeeping episode terminates if the ball hits the goal or is deflected by the agent.</p>
<p>C. Striker Environment</p>
<p>To score a goal is the agent's task inside the striker environment.The ball moves bouncy, slowly, close, and in parallel to the goal.Its speed and origin are sampled uniformly from 1000 samples during the agent's training.The agent's position is farther away from the goal while being varied as well.+1 is the only reward signal that the agent receives upon scoring.Once the ball hits the goal or a time limit is reached, the episode terminates and the environment is reset.</p>
<p>D. Observation and Action Space</p>
<p>Both environments share the same observation and action space.The agent perceives 23 normalized game state variables to fully observe its environment as illustrated by figure 3. The agent's action space is multi-discrete and contains the following 8 dimensions:</p>
<p>• Throttle (5 actions) • Steer (5 actions) • Yaw (5 actions) • Pitch (5 actions)
• Roll (3 actions) • Boost (2 actions) • Drift or Air Roll (2 actions) • Jump (2 actions)
Rocket League is usually played by humans using a gamepad as input device.Some of the inputs (e.g.thumbstick) are thus continuous and not discrete.To simplify the action space, the continuous actions throttle, steer, yaw, and pitch are discretized using buckets as suggested by Pleines et al. [29].By this means, the agent picks one value from a bucket containing the values −1, −0.5, 0, 0.5 and 1.The roll action is also discretized using the values −1, 0 and 1.All other actions determine whether the concerned discrete action is executed or not.The action dimension that is in charge of drifting and air rolling is another special case.Both actions can be boiled down to one because drifting is limited to being on the ground, whereas air rolling can be done in the air only.Moreover, multi-discrete action spaces allow the execution of concurrent actions.One discrete action dimension could achieve the same behavior.This would require defining actions that feature every permutation of the available actions.As a consequence, the already high-dimensional action space of Rocket League would be much larger and therefore harder to train.</p>
<p>IV. DEEP REINFORCEMENT LEARNING</p>
<p>The actor-critic, on-policy algorithm PPO [7] and its clipped surrogate objective (Equation 1) is used to train the agent's policy π, with respect to its model parameters θ, inside the Unity simulation.PPO, algorithmic details, and the model architecture are presented next.</p>
<p>A. Proximal Policy Optimization</p>
<p>L C t (θ) denotes the policy objective, which optimizes the probability ratio of the current policy π θ and the old one π θold :
L C t (θ) = Êt<a href="1">min(qt(θ) Ât, clip(qt(θ), 1 − , 1 + ) Ât)</a>
with the surrogate objective qt(θ) = π θ (at|st) π θold (at|st) s t is the environment's state at step t. a t is an action tuple, which is executed by the agent, while being in s t .The clipping range is stated by and Ât is the advantage, which is computed using generalized advantage estimation [30].While computing the squared error loss L V t of the value function, the maximum between the default and the clipped error loss is determined.
V C t = V θold (st) + clip(V θ (st) − V θold (st), − , )(2)L V t = max((V θ (st) − Gt) 2 , (V C t − Gt) 2 )(3)
with the sampled return Gt = V θold (st) + Ât</p>
<p>The final objective is established by L CV H t (θ):
L CV H t (θ) = Êt<a href="4">L C t (θ) − c1L V t (θ) + c2H[π θ ](st)</a>
To encourage exploration, the entropy bonus H<a href="s t">π θ </a> is added and weighted by the coefficient c 2 .Weighting is also applied to the value loss using c 1 .</p>
<p>Game State Variables (23) Fully Connected (256)</p>
<p>Fully Connected (256) Fully Connected (256)</p>
<p>Value (1)
(5) (5) (5) (5) (3) (2) (2) (2)
Action Dimensions</p>
<p>Policy Stream</p>
<p>Value Stream</p>
<p>Fig. 4. The policy and the value function share gradients and several parameters.After feeding 23 game states variables as input to the model and processing a shared fully connected layer, the network is split into a policy and value stream starting with their own fully connected layer.The policy stream outputs action probabilities for each available action dimension, whereas the value stream exposes its estimated state-value.</p>
<p>B. Algorithmic Details and Model Architecture</p>
<p>PPO starts out by sampling multiple trajectories of experiences, which may contain multiple completed and truncated episodes, from a constant number of concurrent environments (i.e.workers).The model parameters are then optimized by conducting stochastic gradient descent for several epochs of mini-batches, which are sampled from the collected data.Before computing the loss function, advantages are normalized across each mini-batch.The computed gradients are clipped based on their norm.</p>
<p>A relatively shallow neural net (model) is shared by the value function and the policy (Figure 4).To support multidiscrete actions, the policy head of the model outputs 8 categorical action probability distributions.During action selection, each distribution is used to sample actions, which are provided to the agent as a tuple.The only adjustment to the policy's loss computation is that the probabilities of the selected actions are concatenated.Concerning the entropy bonus, the mean of the action distributions' entropies is used.</p>
<p>V. SIM-TO-SIM TRANSFER</p>
<p>Two major approaches are considered to examine learned behaviors inside the Unity simulation and its transfer to Rocket League.The first one runs various handcrafted scenarios (like seen in section III-A) in both simulations to directly compare their alignment.This way, it can be determined whether the car or the ball behave similarly or identically concerning their positions and velocities.The second approach trains the agent in Unity given the goalie and the striker environment, while all implemented physics components are included.We further conduct an ablation study on the implemented physics where each experiment turns off one or all components.Turning off may also refer to use the default physics of Unity.</p>
<p>If not stated otherwise, each training run is repeated 5 times and undergoes a thorough evaluation.Each model checkpoint is evaluated in Unity and Rocket League by 10 training and 10 novel shots, which are repeated 3 times.Therefore, each data point aggregates 150 episodes featuring one shot.Result plots show the interquartile mean of the cumulative reward and a confidence interval of 95% as recommended by Agarwal et al. [31].The hyperparameters are detailed in Table III.At last, we describe some of the learned behaviors that are also retrieved from training in a more difficult striker environment.</p>
<p>A. Alignment Comparison using Handcrafted Scenarios</p>
<p>To directly compare the alignment between both simulations, six physical maneuvers are assessed by 3 different handcrafted scenarios:</p>
<p>1) Acceleration • Ball falls down with an initial force applied on its</p>
<p>x-axis • Ball falls down with an initial force applied on its</p>
<p>x-axis and an angular velocity 6) Shot</p>
<p>• Car drives forward and hits the motionless ball • Car drives forward and the ball rolls to the car • Ball jumps, the car jumps while boosting and hits the ball using a front flip Each scenario tracks the position of the ball and the car during each frame.As both simulations end up monitoring the incoming data with slight time differences, the final data is interpolated to match in shape.Afterward, the error for each data point between both simulations is measured.The final results are described by Table II, which comprises the mean, max, and standard deviation (Std) error across each run scenario.Letting the ball bounce for some time shows the least error, while a significant one is observed when examining the scenarios where the car shoots the ball.Note that slight inaccuracies during acceleration may cause a strongly summed error when considering a different hit location on the ball.The other scenarios, where the error is based on the car's position, also indicate that the Unity simulation suffers from inaccuracies.</p>
<p>B. Physics Ablation Study based on PPO Training</p>
<p>The previously shown imperfections of the Unity simulation may lead to the impression that successfully transferring a trained behavior is rather unlikely.This assumption can be negated by considering the results retrieved from training the agent in the goalie environment (Figure 5).Even though each experiment ablates all, single or no physical adaptations, the agent is still capable of saving nearly every ball once transferred to Rocket League.A drawback of the goalie environment lies in its simplicity because the agent only has to somehow hit the ball to effectively deflect it.The next step of complexity is posed by the striker environment, where the agent has to land a more accurate hit on the ball to score a goal.Figure 6 illustrates the results of the striker training.Notably, when all physical adaptations are present, the transferred behavior manages to score in about 75% of the played episodes.Catastrophic performances emerge in Rocket League once single physical adaptations are turned off.</p>
<p>C. Learned Policies</p>
<p>During the performed experiments, several intriguing agent behaviors emerged 2 .When trained as a goalkeeper, the agent   tries to hit the ball very early, while making its body as big as possible towards the ball.This is achieved by simultaneously jumping and rolling forward or executing a forward flip.Concerning the striker environment, the agent usually approaches the ball using its boost.To get a better angle to the ball, the agent steers left and right or vice versa.Drifting is sometimes used to aid this purpose.Jumping is always used when needed.This is usually the case if the agent is close to the ball, which is located above the agent.Otherwise, the agent's preference is to stay on the ground.Further training experiments were conducted in a more difficult striker environment.The ball is not anymore simply passed in parallel and close to the goal.Instead, the ball bounces higher and farther away from the goal, which increases the challenge of making a good touch on the ball to score.Given this setting, two different policies were achieved.One policy approaches the ball as fast as possible while using a diagonal dodge roll to make the final touch to score.However, this behavior fails a few shots.The other emerged behavior can be considered as the opposite.Depending on the distance and the height of the ball, the agent waits some time or even backs up to ensure that it will hit the ball while being on the ground.Therefore, the agent avoids jumping.This is surprising because the agent should maximize its discounted cumulative reward and therefore finish the episode faster.Although the increased difficulty led to different behaviors, the agent may struggle a lot to get there.Usually, 2 out of 5 training runs succeeded, while the other ones utterly failed.</p>
<p>VI. DISCUSSION</p>
<p>In this work, the agent is trained on isolated tasks, which are quite apart from a complete match of Rocket League.To train multiple cooperative and competitive agents, the first obstacle that comes to mind is the tremendously high computational complexity, which might be infeasible for smaller research groups.But before going this far, several aspects need to be considered that can be treated in isolation as well.At last, the difficulties of training the more difficult striker environments are discussed.</p>
<p>A. On Improving the Sim-to-sim Transfer</p>
<p>At first, the Unity simulation is still lacking the implementation of physical concepts like the car-to-car interaction and suffers from the reported (Section V-A) inaccuracies.These can be further improved by putting more work into the simulation, but also other approaches are promising.At the cost of more computational resources, domain randomization [20] could achieve a more robust agent, potentially comprising an improved ability to generalize to the domain of Rocket League.As the ground truth is provided by Rocket League, approaches from the field of supervised learning can be considered as well.</p>
<p>B. Training under Human Conditions</p>
<p>Once the physical domain gap is narrowed, the Unity simulation still does not consider training under human conditions.Notably, the current observation space provides perfect information on the current state of the environment, whereas players in Rocket League have to cope with imperfect information due to solely perceiving the rendered image of the game.Thus, the Unity simulation has to implement Rocket League's camera behavior as well.However, one critical concern is that the RLBot API does not reveal the rendered image of Rocket League and therefore makes a transfer impossible as of now.However, even if that information is made available by Psyonix, both simulations' visual appearances are very different.The Unity simulation's aesthetics are very abstract, whereas Rocket League impresses with multiple arenas featuring many details concerning lighting, geometry, shaders, textures, particle effects, etc.. To overcome this gap of visual appearance, approaches of the previously described related work, like GraspGAN [18], can be considered.</p>
<p>Another challenge arises once the environment is partially observable.It should be considered that the agent will probably need memory to be able to compete with human players.Otherwise, the agent might not be able to capture the current affairs of its teammates and opponents.For this purpose, multiple memory-based approaches might be suitable, like using a recurrent neural network or a transformer architecture.</p>
<p>Moreover, the multi-discrete action space used in this paper is a simplification of the original action space that features concurrent continuous and discrete actions.Initially, the training was done using the PPO implementation of the ML-Agents toolkit [22], which supports mixed (or hybrid) concurrent action spaces.However, these experiments were quite unstable and hindered progress.Therefore, Rocket League presents an interesting challenge for exploring such action spaces, of which other video games or applications are likely to take advantage.</p>
<p>C. Difficulties of Training the harder Striker Environment</p>
<p>While the goalie and the striker environment are relatively easy, the slightly more difficult striker one poses a much greater challenge due to multiple reasons:</p>
<p>• Episodes are longer, leading to an even more delayed reward signal and more challenging credit assignment • More states have to be explored by the agent • Even more accurate touches on the ball have to be made to score</p>
<p>To overcome these problems, curriculum learning [32] and reward shaping [33] can be considered.In curriculum learning, the agent could face easier scenarios first and once success kicks in, the next level of difficulty can be trained.However, catastrophic forgetting may occur and therefore a curriculum should sample from a distribution of scenarios to mitigate this issue.</p>
<p>Concerning reward shaping, multiple variants were casually tried without improving training results:</p>
<p>• Reward the first touch on the ball • Reward or penalize the distance between the ball and the agent • Reward or penalize the dot product between the car's velocity and the direction from the car to the ball Adding more reward signals along the agent's task introduces bias and is likely task-irrelevant.For example, the agent could exploit such signals to cuddle with the ball at a close distance or to slowly approach the ball to maximize the cumulative return of the episode.If those signals are turned off once the ball is touched, the value function might struggle to make further good estimates on the value of the current state of the environment, which ultimately may lead to misleading training experiences and hence an unstable learning process.In spite of the results of these first explorative tests, future work shall examine whether these points shall be reconsidered.</p>
<p>VII. CONCLUSION</p>
<p>Towards solving Rocket League by the means of Deep Reinforcement Learning, a fast simulation is crucial, because the original game cannot be sped up and neither parallelized on clusters.Therefore, we advanced the implementation of a Unity project that mimics the physical gameplay mechanics of Rocket League.Although the implemented simulation is not perfectly accurate, we remarkably demonstrate that transferring a trained behavior from Unity to Rocket League is robust and generalizes when dealing with a goalkeeper and striker task.Hence, the sim-to-sim transfer is a suitable approach for learning agent behaviors in complex game environments.After all, Rocket League still poses further challenges when targeting a complete match under human circumstances.Based on our findings, we believe that Rocket League and its Unity counterpart will be valuable to various research fields and aspects, comprising: sim-to-sim transfer, partial observability, mixed action-spaces, curriculum learning, competitive and cooperative multi-agent settings.</p>
<p>the Alignment of a Dodge Roll Rocket League's Ground Truth Max Angular Velocity = 5.5 Max Angular Velocity = 7.3</p>
<p>Fig. 2 .
2
Fig. 2. The physical maneuver of a dodge roll is executed to exemplary show the alignment of the Unity simulation to the ground truth by using different max angular velocities.</p>
<p>Fig. 3 .
3
Fig. 3.The contents of the agent's observation.utilizing a GTX 1080 and a AMD Ryzen 7 3700X.</p>
<p>•</p>
<p>Car drives forward and steers left and right • Car drives backward and steers left and right • Car uses boost and steers left and right 2) Air Control • Car starts up in the air, looks straight up, boosts shortly and boosts while rolling in the air • Car starts up in the air, has an angle of 45 • , boosts shortly and boosts while rolling in the air • Car starts up in the air, looks straight up and concurrently boosts, yaws, and air rolls 3) Drift • Car drives forward for a bit and then starts turning and drifting while moving forward • Car drives backward for a bit and then starts turning and drifting while moving forward • Car uses boost and then starts turning and drifting while using boost 4) Jump • Car makes a short jump, then a long one and at last a double jump • Car makes a front flip, a back flip and a dodge roll • Car drives forward, does a diagonal front flip and at last a back flip 5) Ball Bounce • Ball falls straight down</p>
<p>5 .
5
Results of training the goalie environment under different ablations and transferring it to Rocket League.The agent is evaluated on training shots and ones, which were not seen during training.The agent easily solves the goalie task under all circumstances.Both, training and unseen shots, behave identically in Rocket League.</p>
<p>Fig. 6 .
6
Fig.6.Results of training the striker environment under different ablations and transferring it to Rocket League.The agent is evaluated on training situations and ones, which were not seen during training.The agent scores in about 75% of the played episodes given all physical adaptations, while any ablation turns out catastrophic.Both, training and unseen situations, behave identically in Rocket League.</p>
<p>TABLE I OVERVIEW
I
ON ESSENTIAL PHYSICAL GAMEPLAY MECHANICS PRESENT IN ROCKET LEAGUE, WHICH ARE ADDED TO THE TRAINING SIMULATION.</p>
<p>TABLE II THE
II
RESULTED ERROR FOR EACH RUN PHYSICAL MANEUVER SCENARIO.THE CAR'S POSITION IS CONSIDERED BY THE GREEN SHADED DATA POINTS, WHILE THE BLUE ONES ARE RELATED TO THE BALL'S POSITION.
1) Acceleration2) Air Control3) Drift4) Jump5) Ball Bounce6) ShotMean0.693.721.672.323.075.243.190.844.870.070.221.370.010.050.0328.4523.7928.31Std0.483.051.922.611.814.584.731.046.800.060.150.690.010.030.0225.2025.0122.49Max1.218.045.968.408.1212.9716.065.1221.080.240.412.020.020.120.0758.1659.0058.19TABLE IIITHE HYPERPARAMETERS USED TO CONDUCT THE TRAINING WITH PPO.THE LEARNING RATE α AND c 2 DECAY LINEARLY OVER TIME.HyperparameterValueHyperparameterValueDiscount Factor γ0.99Clip Range0.2λ (GAE)0.95c 10.25Number of Workers16Initial α0.0003Worker Steps512Min α0.000003Batch Size8192Initial c 20.0005Epochs3Min c 20.00001Mini Batches8OptimizerAdamWMax Gradient Norm0.5ActivationsReLU
https://github.com/PG642
https://www.youtube.com/watch?v=WXMHJszkz6M&amp;list= PL2KGNY2Ei3ix7Vr vA-ZgCyVfOCfhbX0C</p>
<p>Grandmaster level in starcraft II using multiagent reinforcement learning. O Vinyals, I Babuschkin, W M Czarnecki, M Mathieu, A Dudzik, J Chung, D H Choi, R Powell, T Ewalds, P Georgiev, J Oh, D Horgan, M Kroiss, I Danihelka, A Huang, L Sifre, T Cai, J P Agapiou, M Jaderberg, A S Vezhnevets, R Leblond, T Pohlen, V Dalibard, D Budden, Y Sulsky, J Molloy, T L Paine, C ¸ Gülc ¸ehre, Z Wang, T Pfaff, Y Wu, R Ring, D Yogatama, D Wünsch, K Mckinney, O Smith, T Schaul, T P Lillicrap, K Kavukcuoglu, D Hassabis, C Apps, D Silver, Nat. 57577822019</p>
<p>Dota 2 with large scale deep reinforcement learning. C Berner, G Brockman, B Chan, V Cheung, P Debiak, C Dennison, D Farhi, Q Fischer, S Hashme, C Hesse, R Józefowicz, S Gray, C Olsson, J Pachocki, M Petrov, H P De Oliveira Pinto, J Raiman, T Salimans, J Schlatter, J Schneider, S Sidor, I Sutskever, J Tang, F Wolski, S Zhang, 1912.06680, 2019CoRR</p>
<p>. " Roboleague, Roboleague, 2021. February 28, 2022</p>
<p>Rlbot wiki: Getting started. Rlbot, 2021. February 28, 2022</p>
<p>On the potential of rocket league for driving team ai development. Y Verhoeven, M Preuss, 2020 IEEE Symposium Series on Computational Intelligence (SSCI). 2020</p>
<p>Guest editorial special issue on team AI in games. M Mozgovoy, M Preuss, R Bidarra, IEEE Trans. Games. 1342021</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017</p>
<p>Robocup: The robot world cup initiative. H Kitano, M Asada, Y Kuniyoshi, I Noda, E Osawa, Proceedings of the First International Conference on Autonomous Agents, AGENTS 1997. L Johnson, the First International Conference on Autonomous Agents, AGENTS 1997Marina del Rey, California, USAACMFebruary 5-8, 1997. 1997</p>
<p>Deep reinforcement learning in parameterized action space. M J Hausknecht, P Stone, 4th International Conference on Learning Representations, ICLR 2016. Y Bengio, Lecun, San Juan, Puerto RicoMay 2-4, 2016. 2016Conference Track Proceedings</p>
<p>Game strategies for physical robot soccer players: A survey. E Antonioni, V Suriani, F Riccio, D Nardi, IEEE Trans. Games. 1342021</p>
<p>Sim-to-real transfer in deep reinforcement learning for robotics: a survey. W Zhao, J P Queralta, T Westerlund, 2020 IEEE Symposium Series on Computational Intelligence. Canberra, AustraliaIEEEDecember 1-4, 2020. 20202020</p>
<p>Deepracer: Autonomous racing platform for experimentation with sim2real reinforcement learning. B Balaji, S Mallya, S Genc, S Gupta, L Dirac, V Khare, G Roy, T Sun, Y Tao, B Townsend, E Calleja, S Muralidhara, D Karuppasamy, 2020 IEEE International Conference on Robotics and Automation. Paris, FranceIEEEMay 31 -August 31, 2020. 20202020</p>
<p>Closing the reality gap with unsupervised sim-to-real image translation for semantic segmentation in robot soccer. J Blumenkamp, A Baude, T Laue, CoRR. 1911.01529, 2019</p>
<p>Continual reinforcement learning deployed in real-life using policy distillation and sim2real transfer. R Traoré, H Caselles-Dupré, T Lesort, T Sun, N D Rodríguez, D Filliat, CoRR. 1906.04452, 2019</p>
<p>Grasping unknown objects by coupling deep reinforcement learning, generative adversarial networks, and visual servoing. O Pedersen, E Misimi, F Chaumette, 2020 IEEE International Conference on Robotics and Automation. Paris, FranceIEEEMay 31 -August 31, 2020. 20202020</p>
<p>Rl-cyclegan: Reinforcement learning aware simulation-to-real. K Rao, C Harris, A Irpan, S Levine, J Ibarz, M Khansari, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020. Seattle, WA, USAJune 13-19, 2020. 2020163Computer Vision Foundation / IEEE</p>
<p>Retinagan: An object-aware approach to sim-to-real transfer. D Ho, K Rao, Z Xu, E Jang, M Khansari, Y Bai, ICRA 2021IEEE International Conference on Robotics and Automation. Xi'an, ChinaIEEEMay 30 -June 5, 2021. 202110926</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, S Levine, V Vanhoucke, 2018 IEEE International Conference on Robotics and Automation. Brisbane, AustraliaIEEE2018. May 21-25. 2018. 2018</p>
<p>Generative adversarial nets. I J Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A C Courville, Y Bengio, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Z Ghahramani, M Welling, C Cortes, N D Lawrence, K Q Weinberger, Montreal, Quebec, Canada2014. December 8-13 2014. 2014</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2017. Vancouver, BC, CanadaIEEESeptember 24-28, 2017. 2017</p>
<p>Solving rubik's cube with a robot hand. I Openai, M Akkaya, M Andrychowicz, M Chociej, B Litwin, A Mc-Grew, A Petron, M Paino, G Plappert, R Powell, J Ribas, N Schneider, J Tezak, P Tworek, L Welinder, Q Weng, W Yuan, L Zaremba, Zhang, 1910.07113, 2019CoRR</p>
<p>Obstacle tower: A generalization challenge in vision, control, and planning. A Juliani, A Khalifa, V Berges, J Harper, E Teng, H Henry, A Crespi, J Togelius, D Lange, Proceedings of the 28th International Joint Conference on Artificial Intelligence, IJCAI 2019. the 28th International Joint Conference on Artificial Intelligence, IJCAI 20192019</p>
<p>Rocket league notes. S Mish, 2019. February 28, 2022</p>
<p>Dodges explained. power &amp; more -rocket science #14. Timo Huth, 2018. February 28. 2022</p>
<p>It is rocket science! the physics of 'rocket league' detailed. J Cone, 2018. February 28. 2022It-IS-Rocket-Science-The retrieved</p>
<p>Why are wheel hits so odd? -rocket science #10. Timo Huth, 2017. February 28. 2022</p>
<p>How simple suspensions work. 2022Vehicle Physics Pro</p>
<p>How demos actually work in rocket league. Rocket Sledge, actually-work-in-rocket-league/#: ∼ :text=Demos% 20were%20not%20intended%20to. 2020. broken%20for%20a%20long% 20time retrieved February 28. 2022</p>
<p>Action spaces in deep reinforcement learning to mimic human input devices. M Pleines, F Zimmer, V Berges, IEEE Conference on Games. London, United KingdomIEEE2019. August 20-23, 2019. 2019</p>
<p>Highdimensional continuous control using generalized advantage estimation. J Schulman, P Moritz, S Levine, M I Jordan, P Abbeel, 4th International Conference on Learning Representations, ICLR 2016. Y Bengio, Lecun, San Juan, Puerto RicoMay 2-4, 2016. 2016Conference Track Proceedings</p>
<p>Deep reinforcement learning at the edge of the statistical precipice. R Agarwal, M Schwarzer, P S Castro, A Courville, M G Bellemare, Thirty-Fifth Conference on Neural Information Processing Systems. 2021</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th Annual International Conference on Machine Learning. A P Danyluk, L Bottou, M L Littman, the 26th Annual International Conference on Machine LearningMontreal, Quebec, CanadaACM2009. June 14-18, 2009. 2009382ser. ACM International Conference Proceeding Series</p>
<p>Shaping as a method for accelerating reinforcement learning. V Gullapalli, A Barto, Proceedings of the 1992 IEEE International Symposium on Intelligent Control. the 1992 IEEE International Symposium on Intelligent Control1992</p>            </div>
        </div>

    </div>
</body>
</html>
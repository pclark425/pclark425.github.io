<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5449 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5449</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5449</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-34a133ee7551145060cb20b19176f84f22914521</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/34a133ee7551145060cb20b19176f84f22914521" target="_blank">SELF: Language-Driven Self-Evolution for Large Language Model</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5449.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5449.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evolution with Language Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage framework that first fine-tunes an LLM to produce natural-language self-feedback and self-refinements (meta-skill learning), and then iteratively generates, self-evaluates, refines, filters, and re-trains on its own self-curated data (self-evolution), with optional inference-time self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7b (primary), OpenLlama-3b and VicunaV1.5 (additional experiments); GPT-4 used as an annotator for meta-skill corpus</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vicuna-7b: a 7B parameter LLaMA-derived instruction-following model; VicunaV1.5: Llama2-7b-based variant; OpenLlama-3b: ~3B parameter LLaMA reproduction. GPT-4 used as high-quality labeler for creating meta-skill training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Meta-skill self-feedback & self-refinement + iterative self-evolution (generate → feedback → refine → filter → fine-tune), with optional inference-time self-refinement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Meta-skill learning: supervised fine-tuning on (prompt, initial response, natural-language feedback, revised response) tuples so the model can produce feedback and refined answers. Self-evolution: initialize from meta-skilled model, iterate: for unlabeled prompt produce initial response r, produce feedback f, produce refined response r̂, filter r̂ using the model's feedback, collect (prompt, r̂) as pseudo-labels, and fine-tune (restart training) to obtain next-iteration model. At inference the model can perform one round of self-refinement (generate then refine using its learned meta-skill). GPT-4 was used to produce the meta-skill training corpus in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, SVAMP, Vicuna testset, Evol-Instruct testset</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: grade-school math word problems (∼1k test); SVAMP: 1k math challenge set; Vicuna testset: 80 instruction-following examples across skills; Evol-Instruct: 218 real-world human instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>GSM8K: Vicuna + SELF (self-evolution training, direct response) = 29.64% accuracy; with inference self-refinement (one round) = 31.31%; with self-consistency + self-refinement = 32.22%. SVAMP: Vicuna + SELF direct = 49.40%; with SC+SR = 51.20%. (Single-round limited-data SELF reported GSM8K = 27.67% when using only first-round data in comparison to RLHF). On general benchmarks, SELF increased Vicuna win-rate on Vicuna testset from 65.0% (Vicuna + D_QA) to 72.5% (SELF direct) and to 75.0% after self-refinement; on Evol-Instruct from 48.6% to 52.8% (direct) and to 55.5% (with self-refinement). Scalability: OpenLlama-3b + SELF direct = 15.32% (vs 12.13% D_QA baseline), VicunaV1.5 + SELF direct = 30.22% and self-refinement = 32.43%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baselines: Vicuna (no SELF) direct = 16.43% (GSM8K), Vicuna + D_QA (pseudo-labeled QA only) = 24.49% (GSM8K) and 44.90% (SVAMP). RLHF baseline (matched data volume) = 25.55% (GSM8K). Vicuna baseline direct win-rate on Vicuna testset (reference) used as comparison = 65.0% for Vicuna + D_QA.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative ablations and tables show consistent improvements: self-evolution training (SELF) raises GSM8K accuracy from 24.49% (Vicuna + D_QA) to 29.64% (direct); adding one inference self-refinement round raises it further to 31.31%; combining with self-consistency reaches 32.22%. SELF's natural-language feedback achieved 72% feedback accuracy in identifying incorrect answers (vs RLHF reward model 24%), and SELF (first-round-only data) outperformed RLHF on GSM8K (27.67% vs 25.55%). Iterative training (3 rounds) outperformed single-round training (Iterative: 29.64% direct vs Single-round: 28.40%). Meta-skill learning (D_meta) improves direct-response and enables effective inference-time self-refinement; using GPT-4 to generate meta-skill corpus further increased gains compared to GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: (1) Inference-time online self-improvement increases computational cost due to multi-turn refinement. (2) Pure online self-improvement (no parameter updates) can repeat earlier mistakes; SELF addresses this by fine-tuning on self-refined data but still relies on the model's meta-skill quality. (3) Data filtering based on self-feedback raises training-data accuracy but reduces data volume (e.g., from 4K to 1.8K) which can limit downstream gains (observed modest model improvement despite large increase in labeled-data accuracy). (4) Potential plateau: self-evolution can be halted when self-refinement no longer benefits direct generation. (5) SELF's effectiveness depends on meta-skill corpus quality (GPT-4 generated meta-data produced better gains than GPT-3.5). (6) Small or very weak base models gain less (OpenLlama-3b started very low and improved but remained lower than stronger models).</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>SELF integrates both training-time generate→feedback→refine cycles (used to produce training data that updates model weights) and inference-time self-refinement; the paper emphasizes the necessity of meta-skill learning to make inference self-refinement effective for smaller models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5449.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5449.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inference-time Self-Refinement (online self-improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inference-time self-refinement / iterative self-improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where an LLM generates an initial answer, then produces feedback on that answer (often in natural language), and uses that feedback to generate a refined answer; this process can be repeated for multiple rounds at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLMs (examples in related work: GPT-4, GPT-3.5, Vicuna)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained transformer LLMs capable of multi-turn generation; sizes and architectures depend on specific models cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Online self-improvement / self-refinement (generate → critique/feedback → revise)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>At inference, the same LLM generates an initial response, then generates a critique or diagnostic natural-language feedback, and uses that feedback to produce a revised answer; can be iterated until satisfactory. No parameter updates are made during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning and general tasks (e.g., multi-step reasoning, math word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that benefit from iterative correction of chain-of-thought or answer refinement, typically multi-step reasoning and instruction-following tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>No unified numeric value provided in this paper for previous methods; paper notes that prior work (Madaan et al., Ye et al., Chen et al., etc.) report gains for reasoning tasks using such iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Varies by prior work; in this paper the authors note that inference-time self-refinement can give modest or negative effect on small models without meta-skill training, because the model often fails to correct its own errors.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited related work reports that iterative self-refinement can improve reasoning performance (e.g., self-consistency and self-refine style methods); in this paper authors emphasize that inference self-refinement improves performance when the model has been trained with meta-skills, but baseline models can fail to benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper highlights two main limitations: (1) increased inference computational overhead due to multi-turn generation; (2) models may repeat prior errors because parameters are not updated (i.e., inference refinement does not eliminate systematic model mistakes), especially if the model lacks meta-skill to generate correct feedback and revisions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5449.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5449.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority-vote over multiple chain-of-thought samples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding/aggregation method that samples multiple reasoning traces or answers and selects the most consistent (majority) final answer to improve reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7b (as evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vicuna-7b, a LLaMA-derived 7B instruction-tuned model; self-consistency is applied at decoding to produce multiple CoT samples and aggregate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-consistency (multiple-sample majority voting over chain-of-thought outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample multiple stochastic chain-of-thought outputs, extract final answers, and pick the answer with highest agreement (majority vote) as the final prediction; can be used alone or combined with self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, SVAMP (math reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems and challenge math problems where multiple sampled reasoning chains can be aggregated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>On base Vicuna, self-consistency increased GSM8K performance by +3.13% (text reports); when combined with SELF, self-consistency provided additional gains (e.g., Vicuna+SELF direct 29.64% → with SC+SR 32.22% on GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base Vicuna direct generation GSM8K = 16.43% (no self-consistency); Vicuna + D_QA direct = 24.49%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper reports numeric gains from applying self-consistency on baseline models (notably on uncertain base models) and that gains shrink as model certainty improves through self-evolution; empirically self-consistency complements SELF (combined methods yield higher accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>As models become more capable and certain (after self-evolution), marginal benefits from self-consistency diminish. Self-consistency also increases decoding cost (multiple samples) and is primarily effective when there are multiple plausible reasoning paths to aggregate.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Selfee: Iterative self-revising LLM empowered by self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 1)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 2)</em></li>
                <li>Training language models with language feedback at scale <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5449",
    "paper_id": "paper-34a133ee7551145060cb20b19176f84f22914521",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "SELF",
            "name_full": "Self-Evolution with Language Feedback",
            "brief_description": "A two-stage framework that first fine-tunes an LLM to produce natural-language self-feedback and self-refinements (meta-skill learning), and then iteratively generates, self-evaluates, refines, filters, and re-trains on its own self-curated data (self-evolution), with optional inference-time self-refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-7b (primary), OpenLlama-3b and VicunaV1.5 (additional experiments); GPT-4 used as an annotator for meta-skill corpus",
            "model_description": "Vicuna-7b: a 7B parameter LLaMA-derived instruction-following model; VicunaV1.5: Llama2-7b-based variant; OpenLlama-3b: ~3B parameter LLaMA reproduction. GPT-4 used as high-quality labeler for creating meta-skill training examples.",
            "reflection_method_name": "Meta-skill self-feedback & self-refinement + iterative self-evolution (generate → feedback → refine → filter → fine-tune), with optional inference-time self-refinement",
            "reflection_method_description": "Meta-skill learning: supervised fine-tuning on (prompt, initial response, natural-language feedback, revised response) tuples so the model can produce feedback and refined answers. Self-evolution: initialize from meta-skilled model, iterate: for unlabeled prompt produce initial response r, produce feedback f, produce refined response r̂, filter r̂ using the model's feedback, collect (prompt, r̂) as pseudo-labels, and fine-tune (restart training) to obtain next-iteration model. At inference the model can perform one round of self-refinement (generate then refine using its learned meta-skill). GPT-4 was used to produce the meta-skill training corpus in experiments.",
            "num_iterations": 3,
            "task_name": "GSM8K, SVAMP, Vicuna testset, Evol-Instruct testset",
            "task_description": "GSM8K: grade-school math word problems (∼1k test); SVAMP: 1k math challenge set; Vicuna testset: 80 instruction-following examples across skills; Evol-Instruct: 218 real-world human instructions.",
            "performance_with_reflection": "GSM8K: Vicuna + SELF (self-evolution training, direct response) = 29.64% accuracy; with inference self-refinement (one round) = 31.31%; with self-consistency + self-refinement = 32.22%. SVAMP: Vicuna + SELF direct = 49.40%; with SC+SR = 51.20%. (Single-round limited-data SELF reported GSM8K = 27.67% when using only first-round data in comparison to RLHF). On general benchmarks, SELF increased Vicuna win-rate on Vicuna testset from 65.0% (Vicuna + D_QA) to 72.5% (SELF direct) and to 75.0% after self-refinement; on Evol-Instruct from 48.6% to 52.8% (direct) and to 55.5% (with self-refinement). Scalability: OpenLlama-3b + SELF direct = 15.32% (vs 12.13% D_QA baseline), VicunaV1.5 + SELF direct = 30.22% and self-refinement = 32.43%.",
            "performance_without_reflection": "Baselines: Vicuna (no SELF) direct = 16.43% (GSM8K), Vicuna + D_QA (pseudo-labeled QA only) = 24.49% (GSM8K) and 44.90% (SVAMP). RLHF baseline (matched data volume) = 25.55% (GSM8K). Vicuna baseline direct win-rate on Vicuna testset (reference) used as comparison = 65.0% for Vicuna + D_QA.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative ablations and tables show consistent improvements: self-evolution training (SELF) raises GSM8K accuracy from 24.49% (Vicuna + D_QA) to 29.64% (direct); adding one inference self-refinement round raises it further to 31.31%; combining with self-consistency reaches 32.22%. SELF's natural-language feedback achieved 72% feedback accuracy in identifying incorrect answers (vs RLHF reward model 24%), and SELF (first-round-only data) outperformed RLHF on GSM8K (27.67% vs 25.55%). Iterative training (3 rounds) outperformed single-round training (Iterative: 29.64% direct vs Single-round: 28.40%). Meta-skill learning (D_meta) improves direct-response and enables effective inference-time self-refinement; using GPT-4 to generate meta-skill corpus further increased gains compared to GPT-3.5.",
            "limitations_or_failure_cases": "Reported limitations include: (1) Inference-time online self-improvement increases computational cost due to multi-turn refinement. (2) Pure online self-improvement (no parameter updates) can repeat earlier mistakes; SELF addresses this by fine-tuning on self-refined data but still relies on the model's meta-skill quality. (3) Data filtering based on self-feedback raises training-data accuracy but reduces data volume (e.g., from 4K to 1.8K) which can limit downstream gains (observed modest model improvement despite large increase in labeled-data accuracy). (4) Potential plateau: self-evolution can be halted when self-refinement no longer benefits direct generation. (5) SELF's effectiveness depends on meta-skill corpus quality (GPT-4 generated meta-data produced better gains than GPT-3.5). (6) Small or very weak base models gain less (OpenLlama-3b started very low and improved but remained lower than stronger models).",
            "additional_notes": "SELF integrates both training-time generate→feedback→refine cycles (used to produce training data that updates model weights) and inference-time self-refinement; the paper emphasizes the necessity of meta-skill learning to make inference self-refinement effective for smaller models.",
            "uuid": "e5449.0"
        },
        {
            "name_short": "Inference-time Self-Refinement (online self-improvement)",
            "name_full": "Inference-time self-refinement / iterative self-improvement",
            "brief_description": "A method where an LLM generates an initial answer, then produces feedback on that answer (often in natural language), and uses that feedback to generate a refined answer; this process can be repeated for multiple rounds at inference time.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "General LLMs (examples in related work: GPT-4, GPT-3.5, Vicuna)",
            "model_description": "Large pre-trained transformer LLMs capable of multi-turn generation; sizes and architectures depend on specific models cited in related work.",
            "reflection_method_name": "Online self-improvement / self-refinement (generate → critique/feedback → revise)",
            "reflection_method_description": "At inference, the same LLM generates an initial response, then generates a critique or diagnostic natural-language feedback, and uses that feedback to produce a revised answer; can be iterated until satisfactory. No parameter updates are made during inference.",
            "num_iterations": null,
            "task_name": "Reasoning and general tasks (e.g., multi-step reasoning, math word problems)",
            "task_description": "Tasks that benefit from iterative correction of chain-of-thought or answer refinement, typically multi-step reasoning and instruction-following tasks.",
            "performance_with_reflection": "No unified numeric value provided in this paper for previous methods; paper notes that prior work (Madaan et al., Ye et al., Chen et al., etc.) report gains for reasoning tasks using such iterative refinement.",
            "performance_without_reflection": "Varies by prior work; in this paper the authors note that inference-time self-refinement can give modest or negative effect on small models without meta-skill training, because the model often fails to correct its own errors.",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Cited related work reports that iterative self-refinement can improve reasoning performance (e.g., self-consistency and self-refine style methods); in this paper authors emphasize that inference self-refinement improves performance when the model has been trained with meta-skills, but baseline models can fail to benefit.",
            "limitations_or_failure_cases": "The paper highlights two main limitations: (1) increased inference computational overhead due to multi-turn generation; (2) models may repeat prior errors because parameters are not updated (i.e., inference refinement does not eliminate systematic model mistakes), especially if the model lacks meta-skill to generate correct feedback and revisions.",
            "uuid": "e5449.1"
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (majority-vote over multiple chain-of-thought samples)",
            "brief_description": "A decoding/aggregation method that samples multiple reasoning traces or answers and selects the most consistent (majority) final answer to improve reasoning accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-7b (as evaluated in this paper)",
            "model_description": "Vicuna-7b, a LLaMA-derived 7B instruction-tuned model; self-consistency is applied at decoding to produce multiple CoT samples and aggregate answers.",
            "reflection_method_name": "Self-consistency (multiple-sample majority voting over chain-of-thought outputs)",
            "reflection_method_description": "Sample multiple stochastic chain-of-thought outputs, extract final answers, and pick the answer with highest agreement (majority vote) as the final prediction; can be used alone or combined with self-refinement.",
            "num_iterations": null,
            "task_name": "GSM8K, SVAMP (math reasoning)",
            "task_description": "Grade-school math word problems and challenge math problems where multiple sampled reasoning chains can be aggregated.",
            "performance_with_reflection": "On base Vicuna, self-consistency increased GSM8K performance by +3.13% (text reports); when combined with SELF, self-consistency provided additional gains (e.g., Vicuna+SELF direct 29.64% → with SC+SR 32.22% on GSM8K).",
            "performance_without_reflection": "Base Vicuna direct generation GSM8K = 16.43% (no self-consistency); Vicuna + D_QA direct = 24.49%.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Paper reports numeric gains from applying self-consistency on baseline models (notably on uncertain base models) and that gains shrink as model certainty improves through self-evolution; empirically self-consistency complements SELF (combined methods yield higher accuracy).",
            "limitations_or_failure_cases": "As models become more capable and certain (after self-evolution), marginal benefits from self-consistency diminish. Self-consistency also increases decoding cost (multiple samples) and is primarily effective when there are multiple plausible reasoning paths to aggregate.",
            "uuid": "e5449.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Selfee: Iterative self-revising LLM empowered by self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 2
        },
        {
            "paper_title": "Training language models with language feedback at scale",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        }
    ],
    "cost": 0.015531499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SELF: Self-Evolution with Language FeedBACK</h1>
<p>Jianqiao Lu ${ }^{1+1}$, Wanjun Zhong ${ }^{2 <em>}$, Wenyong Huang ${ }^{2 </em>}$, Yufei Wang ${ }^{2}$, Qi Zhu ${ }^{2}$, Fei $\mathrm{Mi}^{2}$, Baojun Wang ${ }^{2}$, Weichao Wang ${ }^{2}$, Xingshan Zeng ${ }^{2}$, Lifeng Shang ${ }^{2}$, Xin Jiang ${ }^{2} \&amp;$ Qun Liu ${ }^{2}$<br>${ }^{1}$ The University of Hong Kong ${ }^{2}$ Huawei Noah’s Ark Lab<br>jqlu@cs.hku.hk, {zhongwanjun1,wenyong.huang}@huawei.com</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have shown impressive adaptability in various fields, yet the optimal pathway of autonomous model evolution remains underexplored. Drawing inspiration from the self-driven learning process of humans, we introduce SELF (Self-Evolution with Language Feedback), a novel learning framework that empowers LLMs to continually self-improve their abilities. SELF initiates with a meta-skill learning process that equips the LLMs with capabilities for self-feedback and self-refinement. SELF employs language-based feedback for detailed and nuanced evaluations, pinpointing response flaws and suggesting refinements. Subsequently, the model engages in an iterative process of self-evolution: they autonomously generate responses to unlabeled instructions, refine these responses interactively, and use the refined and filtered data for iterative self-training, thereby progressively boosting their capabilities. Moreover, the SELF framework equips the model with the ability to self-refine during inference, leading to further improved response quality. Our experiments on mathematical and general tasks demonstrate that SELF enables the model to continually selfimprove without human intervention. The SELF framework indicates a promising direction for the autonomous evolution of LLMs, transitioning them from passive information receivers to active participants in their development.</p>
<h2>1 INTRODUCTION</h2>
<p>Large Language Models (LLMs), like ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023), stand at the forefront of the AI revolution, demonstrating versatility across tasks. Despite their evident capabilities, the way towards achieving autonomous development of LLMs is still under-explored.</p>
<p>The development of automatically improved LLMs can draw inspiration from human self-driven learning mechanisms. When facing new challenges, humans naturally engage in a learning cycle of initial attempts, introspective feedback, and behavior refinement. This leads to a critical question: "Can LLMs mimic the human learning process, utilizing self-refinement to enhance their inherent capabilities?" Fascinatingly, a recent study (Ye et al., 2023) in top-tier LLMs such as GPT-4 has revealed emergent meta-skills for self-refinement, signaling a promising future direction for the selfevolution of LLMs. Despite this, current methods for LLM development typically rely on a single round of instruction fine-tuning (Wei et al., 2021; Zhou et al., 2023) with meticulously humancrafted datasets and reinforcement learning-based methods (Ouyang et al., 2022) that depend on an external reward model. These strategies not only require extensive resources and ongoing human intervention but also treat LLMs as mere passive repositories of information rather than active learners. These limitations hinder LLMs from tapping into their inherent capabilities, obstructing their progress toward a self-driven, autonomous learning paradigm.</p>
<p>Thus, we introduce SELF (Self-Evolution with Language Feedback) framework, designed to unlock the potential for autonomous self-evolution in LLMs. Figure 1 depicts how SELF mimics human-like self-driven learning, emphasizing progressive improvement of model capability</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Evolutionary Journey of SELF: An initial LLM undergoes successive self-evolution iterations (1st, 2nd, 3rd), enhancing its capabilities and acquiring a self-refinement meta-skill.</p>
<p>with self-evolution training. At the core of SELF are the two meta-skills (<em>self-feedback and self-refinement</em>), empowering the model to progressively self-evolve by training on its own synthesized data. Additionally, SELF leverages self-generated natural language feedback to offer in-depth analysis and guidance for refining responses, without the need for external rewards or direct human guidance.</p>
<p>Specifically, the SELF framework initiates by teaching LLMs essential meta-skills, namely self-feedback and self-refinement, using a limited set of examples. Once these skills are acquired, the model engages in a cycle of continuous self-evolution, iteratively training with extensive, self-generated data. Given a large-scale unlabeled corpus, this data is compiled from initial responses and refined through self-refinement and filtering, with model itself. During this iterative process, the quality of self-evolution training data and model capability are interactively improved, fostering ongoing self-evolution of LLMs. Crucially, in the inference phase, these learned meta-skills enable LLMs to further enhance response quality via self-refinement. In summary, the SELF framework transforms LLMs from passive recipients of data into active learners in self-evolution, and alleviates data scarcity issues by generating large-scale self-curated training datasets. This not only reduces the need for labor-intensive manual intervention but also promotes the continuous self-improvement of LLMs, establishing a more autonomous and efficient training approach.</p>
<p>We evaluate SELF in mathematical and general domains. SELF notably improves the test accuracy on mathematical domains (6.82% on GSM8k (Cobbe et al., 2021) and 4.9% on SVAMP (Patel et al., 2021)), and increases the win rate on general domain (10% on Vicuna testset (Lianmin et al., 2023) and 6.9% on Evol-Instruct testset (Xu et al., 2023)), compared with typical supervised fine-tuning. There are several insights gained from our experiments. Firstly, SELF can progressively enhance the model capability through self-evolution training. Secondly, the learning of meta-skills, specifically self-feedback and self-refinement, is crucial not only for equipping the model with self-improvement abilities but also for boosting its direct response generation performance. Finally, the model demonstrates further improvement in its responses through self-refinement during the inference stage.</p>
<p>The main contributions are summarized as follows: (1) SELF empowers LLMs with self-evolving capabilities, allowing for autonomous model evolution, and reducing human intervention. (2) SELF facilitates self-refinement into smaller LLMs, even with challenging math problems. The capability of self-refinement was previously considered an emergent characteristic of top-tier larger LLMs. (3) Experiments demonstrate the effectiveness of SELF in both mathematical and general domains, confirming its advanced capabilities in self-evolution and self-refinement.</p>
<h2>2 Related Works</h2>
<p><strong>Self-improvement in Inference</strong> Self-consistency (Wang et al., 2022a) is a straightforward and effective method to improve LLMs for reasoning tasks. After sampling a variety of reasoning paths, the most consistent answer is selected. During decoding, self-consistency is closely tied to the self-refinement capability of LLMs, on which our method is based. Unlike self-consistency, self-refinement applies to a broader range of tasks, going beyond reasoning tasks with unique correct answers. Various research efforts have been undertaken to enhance the output quality of LLMs through <em>online self-improvement</em> (Shinn et al., 2023; Madaan et al., 2023; Ye et al., 2023; Chen et al., 2023; Ling et al., 2023). The main idea is to generate an initial output with an LLM. Then, the same LLM provides feedback on its output and employs this feedback to refine its initial output. This process can be iterative until the response quality is satisfied. While simple and effective, <em>online self-improvement</em> necessitates multi-turn inference for refinement, leading to increased inference computational overhead. Most importantly, <em>online self-improvement</em> does not prevent the model from repeating previously encountered errors, as the model’s parameters remain unchanged. In contrast, SELF can self-improve with evolution training.</p>
<p><strong>Autonomous Improvements of LLMs</strong> “Alignment” (Leike et al., 2018) aims to train agents to act in line with human intentions. Several research efforts (Ouyang et al., 2022; Bai et al., 2022; Scheurer et al., 2023) leverage Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017). RLHF begins with fitting a reward model to approximate human preferences. Subsequently, an LLM is finetuned through reinforcement learning to maximize the estimated human preference of the reward model. Reward Ranked Fine-tuning (RAFT) utilizes a reward model to rank responses sampled from an LLM. Subsequently, it fine-tunes the LLM using highly-ranked responses (Dong et al., 2023). Recent advancements in LLMs have explored Reinforcement Learning (RL) approaches that do not rely on human feedback. RLAIF (Pang et al., 2023) proposes to employ LLMs to label the preference data in replace of human supervision. LLMs are updated progressively through online RL in interacting with the environment in Carta et al. (2023). The connection between conventional RL research and RLHF in LLMs is discussed by Sun (2023). However, scalar rewards in Reinforcement Learning (RL) offer limited insights for evaluating complex reasoning tasks (Lightman et al., 2023), as they fail to specify detailed errors and optimization paths. Recognizing this limitation, the SELF framework suggests utilizing natural language feedback, which effectively guides the self-evolution of LLMs. Unlike scalar rewards, which require a retrained model for each evaluation protocol and dimension, natural language feedback is more flexible, addressing multiple aspects simultaneously. Furthermore, the RLHF process is intricate and computationally intensive, relies on external reward models, and demands manual tuning of hyperparameters for optimal performance. This approach lacks the adaptability to evolve intrinsically with the model itself.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of SELF. The “Meta-Skill Learning” (left) phase empowers the LLM to acquire meta-skills in self-feedback and self-refinement. The (b)“Self-Evolution” phase (right) utilizes meta-skills for self-evolution training with self-curated data, enabling continuous model enhancement.</p>
<h1>3 Method</h1>
<p>As depicted in Fig. 2, the SELF framework enhances model capabilities through a two-stage learning phase: (1) Meta-skill Learning Phase: This phase uses an annotated meta-skill training corpus to fine-tune the model, and equips the model with essential meta-skills for self-feedback and self-refinement with limited supervised examples, laying a foundation for self-evolution. (2) SelfEvolution Phase: With the acquired meta-skills, the model progressively improves through multiple iterations of the self-evolution training process. Each iteration begins with the model itself autonomously creating high-quality training data from unlabeled prompts. Then, the model is finetuned using this data. The process is further illustrated in Alg. 1 in Appendix A.7.</p>
<p>In SELF, Natural Language Feedback plays a crucial role in guiding the evolutionary process. This approach offers a more fine-grained and informative evaluation compared to the traditional method of using a scalar reward. The latter evaluates quality along a single dimension with a numerical value from a reward model. In contrast, natural language feedback provides a detailed and descriptive analysis of the processes involved in a response, which is particularly critical in complex reasoning tasks. This also allows for evaluation across multiple dimensions, offering greater flexibility. Importantly, it guides the refinement process by suggesting directions for improvement. The efficacy of natural language feedback in enhancing evaluation accuracy and model performance is shown in $\S$ 4.3.2.</p>
<h3>3.1 Meta-Skill Learning</h3>
<p>Meta-skill learning targets on instill two essential meta-skills into LLMs for self-evolution. (1) Self-Feedback Ability: This skill enables LLMs to evaluate their responses using natural language feedback. This provides the suggestion for further refinement, thus laying a solid foundation for subsequent self-refinement. Self-feedback also enables the model to filter out low-quality self-evolution training data if a response is judged as unqualified by the model (§ 3.2.1). (2) Self-Refinement Ability: Self-refinement enables the model to optimize its responses based on self-feedback. This ability has two applications: (1) enhancing the quality of the self-evolution training corpus (§ 3.2.1) and (2) improving model performance by refining the models' outputs during inference (§ 3.3).</p>
<p>These meta-skills are acquired by fine-tuning the model using the Meta-Skill Training Corpus (§ 3.1.1) with designed training objective (§ 3.1.2). The resulting model is denoted as $M_{\text {meta }}$.</p>
<h3>3.1.1 Meta-Skill Training Corpus</h3>
<p>The meta-skill training corpus $D_{\text {meta }}$ represents the generation, feedback, and refinement process. It is constructed as follows: (1) For each unlabeled prompt $p$, the initial model $M_{\text {init }}$ generates an initial response $r$. (2) An annotator $L$ provides evaluation feedback $f$ for the initial response $r$, then produces a refined answer $\hat{r}$ according to the feedback $f$. Each instance in $D_{\text {meta }}$ takes the form $(p, r, f, \hat{r})$, representing the process of response evaluation and refinement. An example instance of $D_{\text {meta }}$ is provided in appendix A.6.</p>
<h3>3.1.2 Training Objective</h3>
<p>In the meta-skill learning phase, the objective is to empower the initial model $M_{\text {init }}$ to develop meta-skills, resulting in an enhanced model $M_{\text {meta }}$. This process is guided by the cross-entropy loss following the maximum likelihood estimation (MLE) paradigm:</p>
<p>$$
\mathcal{L}<em _hat_r="\hat{r" _p_="(p," f_="f," r_="r,">{\text {meta }}(\phi)=-\mathbb{E}</em> \mid p)\right]
$$}) \sim D_{\text {meta }}}\left[\log \tau_{\phi}(f \mid p, r)+\log \tau_{\phi}(\hat{r} \mid p, r, f)+\beta \log \tau_{\phi}(\hat{r</p>
<p>where $p$ is prompt, $r$ is the initial model response, $f$ is the feedback to the model response $r$, and $\hat{r}$ is the revised response based on feedback $f . \tau_{\phi}(y \mid x)$ denotes the probability distribution given by the auto-regressive language model with parameters $\phi$ predicting the response $y$ given the input prompt $x$. The coefficient $\beta$ in eq. (1) controls a balanced emphasis on direct response generation and the model's capability for self-feedback and self-refinement.</p>
<p>Insight. Training with $D_{\text {meta }}$ aims to achieve two goals: (i) To guide the model in generating feedback $(f)$ concerning its initial responses $(r)$ (self-feedback) and subsequently employing this</p>
<p>feedback to enhance the quality of the final answer $(\hat{r})$ (self-refinement). (ii) Training with $D_{\text {meta }}$ instructs the model to process problems in a Chain-of-Thought (CoT) manner. This involves evaluating the initial response, integrating the feedback, and then revising the response in a chain process $\Psi(\hat{r} \mid p):=\sum_{r, f} \tau_{\phi}(r \mid p) \cdot \tau_{\phi}(f \mid p, r) \cdot \tau_{\phi}(\hat{r} \mid p, r, f)$.</p>
<h1>3.2 Self-Evolution Training Process</h1>
<p>The model $M_{\text {meta }}$, equipped with meta-skills, undergoes progressive improvement through multiple iterations of the self-evolution training process. Each iteration of the self-evolution process begins with the model autonomously creating high-quality training data (§ 3.2.1) from an unlabeled corpus. With an unlabeled dataset of prompts, the model generates initial responses and then refines them through self-feedback and self-refinement. These refined responses, superior in quality, are further filtered with self-feedback and utilized as the training data for the model's subsequent self-evolution training (§ 3.2.2). This autonomous self-evolving process interactively improves LLMs as the improved model capability leads to better data quality, which in turn boosts model performance. It also alleviates the data scarcity problem by self-generating data.</p>
<h3>3.2.1 Self-Evolution Training Data</h3>
<p>Let $M_{\text {evol }}^{t}$ denotes the model at $t^{t h}$ iteration and initialize $M_{\text {evol }}^{0}$ from $M_{\text {meta }}$. During $t^{t h}$ self-evolution iteration, $M_{\text {evol }}^{t-1}$ processes each unlabeled prompt $p$ by first generating an initial response $r . r$ is then refined using the model's self-feedback $f$, resulting in a self-refined response $\hat{r}$. The prompts and their corresponding self-refined responses $(p, \hat{r})$ are then incorporated into the $t^{t h}$ round selfevolution datasets, denoted as $D_{\text {evol }}^{t}$, for subsequent self-evolution processes.
Data Filtering with Self-feedback: To enhance the quality of $D_{\text {evol }}^{t}$, we employ the self-feedback capability of $M_{\text {evol }}^{t-1}$ to filter out data of lower quality. $M_{\text {evol }}^{t-1}$ evaluates the self-refined data, $\hat{r}<em _meta="{meta" _text="\text">{\text {evol }}$, keeping only the responses that meet high-quality standards. The effect is analyzed in § 4.6.
To mitigate the catastrophic forgetting issue of meta-skill, the meta-skill learning data $D</em>$, improving its performance and aligning it more closely with human values.}}$ are also included in self-evolution training. At $t^{t h}$ iteration, the model undergoes self-evolution training with the updated self-curated data $D_{\text {evol }}^{t</p>
<h3>3.2.2 Mathematical Modeling</h3>
<p>Main Objective. We denote $\tau_{\phi}^{t}$ as the probability distribution generated by $M_{\text {evol }}^{t}$ with parameters $\phi$. The self-evolution training loss $\mathcal{L}_{\text {evol }}^{t}(\phi)$ is defined as:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _evol="{evol" p__text="p_{\text">{\text {evol }}^{t}(\phi) &amp; =-\mathbb{E}</em>}}} \mathbb{E<em _evol="{evol" _text="\text">{\hat{r}</em>}} \sim \Psi^{t-1}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }} \mid p</em>}}\right)}\left[\log \tau_{\phi}^{t}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }} \mid p</em>\right)\right] \
&amp; =-\mathbb{E}}<em _evol="{evol" _text="\text">{p</em>}}}\left[\sum_{\hat{r<em _evol="{evol" _text="\text">{\text {evol }}} \Psi^{t-1}\left(\hat{r}</em>}} \mid p_{\text {evol }}\right) \log \tau_{\phi}^{t}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }} \mid p</em>\right)\right]
\end{aligned}
$$}</p>
<p>where $p_{\text {evol }}$ is sampled from unlabeled prompts corpus (detiled in appendix A.3.2) for self-evolution $t^{t h}$ round. The joint probability distribution is:</p>
<p>$$
\begin{aligned}
&amp; \Psi^{t-1}\left(\hat{r}<em _evol="{evol" _text="\text">{\text {evol }} \mid p</em>\right)= \
&amp; \sum_{r_{\text {evol }}, f_{\text {evol }}}\left(\tau_{\phi}^{t-1}\left(r_{\text {evol }} \mid p_{\text {evol }}\right) \cdot \tau_{\phi}^{t-1}\left(f_{\text {evol }} \mid r_{\text {evol }}, p_{\text {evol }}\right) \cdot \tau_{\phi}^{t-1}\left(\hat{r}}<em _evol="{evol" _text="\text">{\text {evol }} \mid f</em>\right)\right)
\end{aligned}
$$}}, r_{\text {evol }}, p_{\text {evol }</p>
<p>The rationale behind learning from $\Psi^{t-1}\left(\hat{r}<em _evol="{evol" _text="\text">{\text {evol }} \mid p</em>\right)$ is discussed in appendix A.1.1.}</p>
<p>Optimizing eq. (2) is equivalent to minimizing the Kullback-Leibler (KL) divergence:</p>
<p>$$
\begin{aligned}
&amp; \mathrm{KL}\left(\Psi^{t-1}\left(\hat{r}<em _evol="{evol" _text="\text">{\text {evol }}|p</em>}}\right) | \gamma_{\phi}^{t}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }}|p</em>\right)\right) \
&amp; =\sum_{\hat{r}}<em _evol="{evol" _text="\text">{\text {evol }}} \Psi^{t-1}\left(\hat{r}</em>}}|p_{\text {evol }}\right) \log \frac{\Psi^{t-1}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }}|p</em>}}\right)}{\tau_{\phi}^{t}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }}|p</em> \
&amp; =-\underbrace{H\left(\Psi^{t-1}\left(\hat{r}}}\right)<em _evol="{evol" _text="\text">{\text {evol }}|p</em>}}\right)\right)<em _hat_r="\hat{r">{\text {constant for fixed } \Psi^{t-1}}- \
&amp; \underbrace{\sum</em><em _evol="{evol" _text="\text">{\text {evol }}} \Psi^{t-1}\left(\hat{r}</em>}}|p_{\text {evol }}\right) \log \tau_{\phi}^{t}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }}|p</em> .
\end{aligned}
$$}}\right)}_{\text {eq. }(2)</p>
<p>The optimization of KL divergence is to fine-tune the model parameters $\phi$ to ensure that the model's predictive probability distribution $\tau_{\phi}^{t}$ aligns with the joint probability of the preceding iteration's chain process $\left(\Psi^{t-1}\right)$. The goal is to enhance the model's ability to directly produce refined responses $\left(\hat{r}_{\text {evol }}\right)$ in the $t^{t h}$ iteration, effectively condensing the intricate process of generation, feedback, and refinement from the $(t-1)^{t h}$ iteration. This advancement demonstrates the model's evolving capability to streamline the complex steps into a more straightforward inference. The potential plateau is discussed in appendix A.1.3.</p>
<p>Further Analysis. Assuming that each self-evolution round is effective, implying that as $t$ increases, the quality of responses sampled from $\Psi^{t}$ improves, optimizing the KL divergence as described in eq. (4) is fundamentally a process aimed at enhancing the direct generation of high-quality responses. Before delving deeper, it is crucial to introduce several key concepts. We define a binary variable $X$ to evaluate the quality of responses. A higher probability, $p(X=1 \mid p_{\text {evol }}, \hat{r}<em _evol="{evol" _text="\text">{\text {evol }})$, indicates a higher quality of the response $\hat{r}</em>$ iteration, the model's log-likelihood of producing high-quality responses to a specified prompt is defined as follows:}}$ in relation to the prompt $p_{\text {evol }}$. For the self-evolving model with parameters $\phi$ at the $t^{t h</p>
<p>$$
\log p^{t}\left(X=1 \mid p_{\text {evol }}\right):=\log \sum_{\hat{r}} p\left(X=1 \mid p_{\text {evol }}, \hat{r}<em _phi="\phi">{\text {evol }}\right) \tau</em>}^{t}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }}|p</em>\right)
$$}</p>
<p>By minimizing the KL divergence in eq. (4), we can increase $\log p^{t}\left(X=1 \mid p_{\text {evol }}\right)$ by progressively improving its Evidence Lower Bound (ELBO):</p>
<p>$$
\begin{aligned}
&amp; \log p^{t}\left(X=1 \mid p_{\text {evol }}\right) \
&amp; =\log \sum_{\hat{r}<em _evol="{evol" _text="\text">{\text {evol }}} p\left(X=1 \mid p</em>}}, \hat{r<em _phi="\phi">{\text {evol }}\right) \tau</em>}^{t}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }}|p</em>\right) . \
&amp; =\log \mathbb{E}}<em _evol="{evol" _text="\text">{\Psi^{t-1}\left(\hat{r}</em>}}|p_{\text {evol }}\right)}\left[\frac{p\left(X=1 \mid p_{\text {evol }}, \hat{r<em _phi="\phi">{\text {evol }}\right) \tau</em>}^{t}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }}|p</em>}}\right)}{\Psi^{t-1}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }}|p</em>\right] \
&amp; \geq \mathbb{E}}}\right)<em _evol="{evol" _text="\text">{\Psi^{t-1}\left(\hat{r}</em>}}|p_{\text {evol }}\right)}\left[\log \frac{p\left(X=1 \mid p_{\text {evol }}, \hat{r<em _phi="\phi">{\text {evol }}\right) \tau</em>}^{t}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }}|p</em>}}\right)}{\Psi^{t-1}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }}|p</em>\right] \
&amp; =\mathbb{E}}}\right)<em _evol="{evol" _text="\text">{\Psi^{t-1}\left(\hat{r}</em>}}|p_{\text {evol }}\right)}\left[\log p\left(X=1 \mid p_{\text {evol }}, \hat{r<em _evol="{evol" _text="\text">{\text {evol }}\right)\right] \
&amp; -\underbrace{\mathrm{KL}\left(\Psi^{t-1}\left(\hat{r}</em>}}|p_{\text {evol }}\right) | \tau_{\phi}^{t}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }}|p</em> .
\end{aligned}
$$}}\right)\right)}_{\text {eq. }(4)</p>
<p>The entire self-evolution training process can be viewed as a continuous exploration of inherent model capabilities in generation, self-feedback, and self-refinement, ultimately enhancing the model's ability to generate high-quality responses directly.</p>
<p>Overall Objective. In the iterative self-evolution process, meta-skills, i.e., the ability to selffeedback and self-refinement, is crucial for guiding the evolution process. We incorporate $D_{\text {meta }}$ into self-evolution training to mitigate the potential catastrophic forgetting of meta-skills:</p>
<p>$$
\mathcal{L}<em _hat_r="\hat{r" _p_="(p," f_="f," r_="r,">{\text {meta }}^{t}(\phi)=-\mathbb{E}</em> \mid p, r, f)\right]
$$}) \sim D_{\text {meta }}}\left[\log \tau_{\phi}^{t}(f \mid p, r)+\log \tau_{\phi}^{t}(\hat{r</p>
<p>The total objective for the $t^{t h}$ round of self-evolution is:</p>
<p>$$
\mathcal{L}<em _evol="{evol" _text="\text">{\text {self }}^{t}(\phi)=\mathcal{L}</em>(\phi)
$$}}^{t}(\phi)+\mathcal{L}_{\text {meta }}^{t</p>
<h1>3.3 RESPONSE REFINEMENT DURING INFERENCE</h1>
<p>Equipped with the meta-skills for self-feedback and self-refinement, the model can conduct selfrefinement during inference. Specifically, the model generates an initial response and then refines it using self-refinement, akin to the method described in $\S 3.1$. Response refinement during inference consistently improves the model's performance as shown in $\S 4.3$.</p>
<h2>4 EXPERIMENT SETTINGS</h2>
<h3>4.1 Evaluation</h3>
<p>Inference Setting. We adopt two inference settings: (1) Direct Response (default): the model directly answers the question with a Zero-shot Chain of Thought (CoT) methodology (Kojima et al., 2022), which is the default setting to evaluate the model capability directly; (2) Self-Refinement: during inference, the model self-refines its original answer for once, as described in $\S 3.3$.</p>
<p>Benchmarks. We evaluate on two mathematical benchmarks to show the efficacy of SELF on complex reasoning tasks, and further verify the generalizability of SELF on two general benchmarks. GSM8K (Cobbe et al., 2021) contains high-quality, linguistically diverse grade school math word problems crafted by expert human writers, which incorporates approximately 7.5 K training problems and 1 K test problems. The performance is measured by accuracy (\%). SVAMP (Patel et al., 2021) is a challenge set for elementary Math Word Problems (MWP). It is composed of 1K test samples. The evaluation metric is accuracy (\%). Vicuna testset (Lianmin et al., 2023) is a benchmark for assessing instruction-following models, containing 80 examples across nine skills in mathematics, reasoning, and coding. Evol-Instruct testset (Xu et al., 2023) includes 218 real-world human instructions from various sources, offering greater size and complexity than the Vicuna testset.</p>
<h3>4.2 SETUP AND BASELINES</h3>
<p>The complete SELF framework includes meta-skill training with $D_{\text {meta }}$, three iterations of selfevolution training, and optional self-refinement during inference. Our evaluation primarily focuses on assessing how self-evolution training can progressively enhance the capabilities of LLMs. For building the meta-skill training corpus $D_{\text {meta }}$, we employ GPT-4 as the language model labeler $L$ due to its proven proficiency in refining responses (An et al., 2023) via the prompt described in appendix A. $2^{1}$. The data statistic of $D_{\text {meta }}$ is shown in appendix A.3.1 and further details of unlabeled corpus construction is described in appendix A.3.2. We note that all model training utilized the same training hyperparameters, as shown in appendix A.4.</p>
<p>We note that the SELF framework is compatible with versatile LLMs. In this study, we perform the experiment with Vicuna-7b (Chiang et al., 2023), a capable open-source instruction-following model fine-tuned from LLaMA-7b (Touvron et al., 2023), will be referred to simply as "Vicuna" in subsequent sections. To verify the generalizability of SELF, we also experiment with OpenLLaMA Geng \&amp; Liu (2023) and Vicuna-1.5 (Chiang et al., 2023) in appendix A.12. All the compared baselines are outlined:
(1) Vicuna $+D_{\mathrm{QA}}$ : To demonstrate the improvement brought by SELF and exclude the impact of standard domain-specific supervised fine-tuning (SFT), we set a direct baseline that trained solely on pseudo-labeled question-answer pairs in the meta-skill training corpus. Specifically, we construct $D_{\mathrm{QA}}$, which includes all the $(p, \hat{r})$ pairs from $D_{\text {meta }}$, and fine-tune the model as:</p>
<p>$$
\mathcal{L}<em _hat_r="\hat{r" _p_="(p,">{\mathrm{QA}}(\phi)=-\mathbb{E}</em> \mid p)\right]
$$}) \sim D_{\mathrm{QA}}}\left[\log \tau_{\phi}(\hat{r</p>
<p>We refer to this approach as Vicuna $+D_{\mathrm{QA}}$, the most straightforward baseline. The performance gap between Vicuna $+D_{\mathrm{QA}}$ and SELF verify the efficacy of the proposed SELF framework, excluding the effect of training on domain-specific QA data.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(2) RLHF: we utilize the RLHF implementation from trlx ${ }^{2}$. We apply the same SFT model as the policy model for RLHF, Vicuna $+D_{\mathbf{Q A}}$ as described above, which is consistent with SELF. The reward model is initialized from Vicuna-7b and is fine-tuned using pair-wise comparison data derived from the meta-skill training corpus $D_{\text {meta }}(\S 3.1 .1)$, where the refined response $\hat{r}$ is presumed to be better than the original one $r$.
(3) Self-Consistency: we compare the self-refinement inference strategy in SELF with the SelfConsistency (Wang et al., 2022a) (i.e., multiple sampling and selecting an answer with majority voting) and examine their combined efficacy.</p>
<h1>4.3 MAIN RESULT</h1>
<h3>4.3.1 MATH TEST</h3>
<p>Table 1: Experiment results on GSM8K and SVAMP compare SELF with other baseline methods. We evaluate the impact of Self-Evolution (SE), Self-Consistency (SC), and Self-Refinement (SR) strategies on model performance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">SE</th>
<th style="text-align: center;">SC</th>
<th style="text-align: center;">SR</th>
<th style="text-align: center;">GSM8K(\%)</th>
<th style="text-align: center;">SVAMP(\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Vicuna</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16.43</td>
<td style="text-align: center;">36.40</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">19.56</td>
<td style="text-align: center;">40.20</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">15.63</td>
<td style="text-align: center;">36.80</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna $+D_{\mathrm{QA}}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">24.49</td>
<td style="text-align: center;">44.90</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">25.70</td>
<td style="text-align: center;">46.00</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">24.44</td>
<td style="text-align: center;">45.30</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna + SELF (Ours)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">29.64</td>
<td style="text-align: center;">49.40</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">29.87</td>
<td style="text-align: center;">50.20</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">31.31</td>
<td style="text-align: center;">49.80</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{3 2 . 2 2}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 2 0}$</td>
</tr>
</tbody>
</table>
<p>In table 1, we compare SELF against baseline models, as detailed in $\S 4.2$. This comparison elucidates SELF's effectiveness in enhancing LLM performance through self-evolution and offers several key insights:
(1) Self-Evolution Enhances LLM: Vicuna + SELF significantly outperforms its baseline Vicuna $+D_{\mathrm{QA}}\left(24.49 \% \xrightarrow{+5.15 \%} 29.64 \% \mathrm{on} \mathrm{GSM} 8 \mathrm{~K}\right.$ and $\left.44.90 \% \xrightarrow{+4.5 \%} 49.40 \% \mathrm{on} \mathrm{SVAMP}\right)$ in direct response setting, showcasing self-evolution is effective in optimizing LLMs.
(2) SELF Instills Self-Refine Capability in LLMs: The integration of self-refinement inference strategy with Vicuna + SELF further boosts performance $\left(29.64 \% \xrightarrow{+1.67 \%} 31.31 \%\right)$, while baseline models show marginal or negative effect via self-refinement. We also provide a case analysis for the limited self-refinement ability of baseline models, as shown in fig. 4. This indicates that SELF can instill advanced self-refinement capabilities into small LLMs like Vicuna (7B), although selfrefinement was previously shown as an exclusive ability of large LLMs (Ye et al., 2023) like GPT-4.
(3) SELF can work with Self-Consistency: SELF works effectively with self-consistency, improving accuracy across models. The base Vicuna model, which may have uncertainties in its outputs, shows notable improvement with self-consistency, achieving a $+3.13 \%$ increase. As the model progresses through self-evolution training and becomes more certain of generating correct math answers, the benefit from self-consistency reduces. Combining self-refinement with self-consistency further elevates performance (e.g., $29.64 \% \xrightarrow{+2.58 \%} 32.22 \%$ on GSM8K), indicating that these two strategies can complement each other effectively.
(4) Pseudo-Labeled $D_{\mathbf{Q A}}$ Enhances Performance: The inclusion of pseudo-labeled QA data $D_{\mathrm{QA}}$ enhances Vicuna's performance, suggesting that tuning with domain-specific QA data can enhance task-specific problem-solving.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4.3.2 COMPARISON WITH RLHF</h1>
<p>Table 2: Comparison of SELF and RLHF on GSM8K. "Feedback Acc." measures how accurately feedback identifies correct and incorrect answers, while "GSM8K Acc." shows the model performance on GSM8K testset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Feedback Acc.(\%)</th>
<th style="text-align: center;">GSM8K Acc.(\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Vicuna $+D_{\mathrm{QA}}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.49</td>
</tr>
<tr>
<td style="text-align: left;">RLHF</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">25.55</td>
</tr>
<tr>
<td style="text-align: left;">SELF</td>
<td style="text-align: center;">$\mathbf{7 2}$</td>
<td style="text-align: center;">$\mathbf{2 7 . 6 7}$</td>
</tr>
</tbody>
</table>
<p>In table 2, we compare the performance of SELF with RLHF. To alleviate the effect led by different amounts of training data and make a fair comparison, for SELF, we only adopt data solely from the initial round of self-evolution training. This ensures the same training data quantity with RLHF and leads to sub-optimal results compared with the one in table 2. As table 2 shows, RLHF achieves a $25.55 \%$ accuracy on GSM8K, which is lower than the $27.67 \%$ performed by SELF. We observe that the simple scalar reward of RLHF often fails to identify the correctness of the reasoning process, which limits performance improvements. On the GSM8K test set, for incorrect answers produced by the SFT model (Vicuna $+D_{\mathrm{QA}}$ ), the reward model only identifies $24 \%$ of them as incorrect, i.e., the reward model assigns lower scalar rewards to incorrect answers compared to correct answers. In contrast, SELF utilizes informative natural language feedback to provide a more accurate assessment. It correctly identifies $72 \%$ of incorrect answers.</p>
<h3>4.4 GENERAL TEST</h3>
<p>We test the efficacy and generalizability of SELF on general domain benchmarks, explicitly using the Vicuna and Evol-Instruct test sets. Three configurations of the Vicuna model are evaluated: Vicuna, Vicuna $+D_{\mathrm{QA}}$, and Vicuna + SELF. We utilize GPT-4 to evaluate the models' responses on both test sets. We follow the assessment methodology proposed by (Xu et al., 2023), which mitigated the order bias presented in the evaluation procedures.</p>
<p>The results are depicted in Figure 3. In the figure, blue represents the number of test cases where the model being evaluated is preferred over the baseline model (Vicuna), as assessed by GPT-4. Yellow denotes test cases where both models perform equally, and pink indicates the number of test cases where the baseline model is favored over the model being evaluated.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Results on Vicuna testset and Evol-Instruct testset</p>
<p>In the Vicuna testset, SELF increases direct response win rate from $65.0 \%$ to $72.5 \%$ compared with Vicuna $+D_{\mathrm{QA}}$. After self-refinement, the win rate is further improved to $75.0 \%$. In the Evol-Instruct testset, the win rate of Vicuna $+D_{\mathrm{QA}}$ is $48.6 \%$. SELF increases the win rate to approximately $52.8 \%$. Applying self-refinement during inference further improves the win rate to $55.5 \%$.</p>
<p>These findings in the general domain highlight the SELF framework's adaptability and robustness, particularly when self-refinement is employed, showcasing its efficacy across varied test domains.</p>
<p>Table 3: Performance under various training settings of SELF. A checkmark $\checkmark$ in a column denotes the additive adoption of the corresponding setting in that training scenario. We present two kinds of inference results: Direct Response (DR) and Self-Refinement (SR), the latter conducts selfrefinement to DR.</p>
<p>| SVAMP (\%) | | GSM8K (\%) | | $D_{\mathrm{QA}}$ | $D_{\text {meta }}$ | Self Evol. | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p>The following insight is highlighted: The combination of self-refinement and self-feedback filtering results in higher self-evolution training data accuracy ( $+14.21 \%$ ) and improved fine-tuned model performance ( $+0.77 \%$ ). Despite the significant training data accuracy improvement, the performance gain is modest due to the reduced data size (from 4 K to 1.8 K ) after filtering.</p>
<h1>5 CONCLUSION</h1>
<p>We present SELF (Self-Evolution with Language Feedback), a novel framework that enables LLMs to achieve progressive self-evolution through self-feedback and self-refinement. Unlike conventional methods, SELF transforms LLMs from passive information recipients to active participants in their evolution. The adoption of natural language feedback promotes a more informative and finegrained evaluation. Through meta-skill learning, SELF equips LLMs with the capability for selffeedback and self-refinement. This empowers the models to evolve their capabilities autonomously, utilizing self-evolution training and online self-refinement. Experiments conducted on benchmarks underscore SELF's capacity to progressively enhance model capabilities while reducing the need for human intervention. SELF represents a leading step in autonomous LLM development, leading to an insight that models are capable of continual learning and self-evolution.</p>
<h2>REFERENCES</h2>
<p>Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. Learning from mistakes makes llm better reasoner. arXiv preprint arXiv:2310.20689, 2023.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.</p>
<p>Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. arXiv preprint arXiv:2302.02662, 2023.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/.</p>
<p>Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023.</p>
<p>Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL https: //github.com/openlm-research/open_llama.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. CoRR, abs/2205.11916, 2022. doi: 10.48550/ARXIV. 2205.11916. URL https://doi.org/10.48550/arXiv.2205.11916.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1152-1157, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136.</p>
<p>Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.</p>
<p>Zheng Lianmin, Chiang Wei-Lin, and Zhuang Siyuan (Ryans). Vicuna-blog-eval, 2023. https: //github.com/lm-sys/vicuna-blog-eval.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. Deductive verification of chain-of-thought reasoning. arXiv preprint arXiv:2306.03872, 2023.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.</p>
<p>Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 975-984, 2020.</p>
<p>OpenAI. Chatgpt, 2022. https://chat.openai.com/chat.
OpenAI. Gpt-4 technical report, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu, Zongzhang Zhang, and Yang Yu. Language model self-improvement by reinforcement learning contemplation. arXiv preprint arXiv:2305.14483, 2023.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 20802094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168.</p>
<p>Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprint arXiv:2303.16755, 2023.</p>
<p>Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.</p>
<p>Hao Sun. Reinforcement learning in the era of llms: What is essential? what is needed? an rl perspective on rlhf, prompting, and beyond. arXiv preprint arXiv:2310.06147, 2023.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022a.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022b.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.</p>
<p>Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. Selfee: Iterative self-revising llm empowered by self-feedback generation. Blog post, May 2023. URL https://kaistai.github.io/SelFee/.</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 DISCUSSION</h2>
<h2>A.1.1 Why Refinement is Better</h2>
<p>We discuss why it's necessary to optimize $\tau_{\phi}^{t}\left(\hat{r}<em _evol="{evol" _text="\text">{\text {evol }} \mid p</em>}}\right)$ in the $t^{t h}$ round self-evolution by learning from $\Psi^{t-1}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }} \mid p</em>}}\right)$, and why we believe samples from $\Psi^{t-1}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }} \mid p</em>\right)$ directly.
Firstly, similar to the insights analyzed in § 3.1.2, we believe that a process akin to CoT, involving feedback followed by refinement before providing an answer, helps in generating high-quality responses. Secondly, $r_{\text {evol }}$ is already a reasonably good output after meta-skill learning and previously $(t-1)$ rounds of self-evolution. We can assume that the self-feedback $f_{\text {evol }}$ is informative, hence $\hat{r}}}\right)$ are typically of higher quality than those from $\tau_{\phi}^{t-1}\left(r_{\text {evol }} \mid p_{\text {evol }<em _phi="\phi">{\text {evol }} \sim \tau</em>}^{t-1}\left(\hat{r<em _evol="{evol" _text="\text">{\text {evol }} \mid p</em>}}, r_{\text {evol }}, f_{\text {evol }}\right)$ is of higher quality than $r_{\text {evol }} \sim \tau_{\phi}^{t-1}\left(r_{\text {evol }} \mid p_{\text {evol }}\right)$ because it incorporates useful feedback information. If $f_{\text {evol }}$ suggests that the initial response $r_{\text {evol }}$ does not require refinement, we still proceed through the process of revising from $r_{\text {evol }}$ to $\hat{r<em _evol="{evol" _text="\text">{\text {evol }}$ using $f</em>}}$, but set $\hat{r<em _evol="{evol" _text="\text">{\text {evol }}=r</em>}}$. By doing so, we ensure that the quality of $\hat{r<em _evol="{evol" _text="\text">{\text {evol }}$ is at least as good as that of $r</em>$.
Moreover, as described in § 3.2.2, we utilize Data Filtering with Self-feedback. In other words, we only keep $\hat{r}}<em _phi="\phi">{\text {evol }}$ evaluated as qualified, allowing us to emphasize high-quality outputs and further improve $\tau</em>$.}^{t</p>
<h2>A.1.2 Why Integration of Meta-Skill Training Data $D_{\text {meta }}$ Levates Direct QA</h2>
<p>The $D_{\text {meta }}$ dataset trains the model to not only modify answers but also to fully grasp a prompt, create feedback, and then develop a revised answer. This approach resembles training the model to think through a problem in a chain-of-thought methodically (CoT) manner, before responding. The training encompasses a thorough examination of the entire process, which not only betters the model's direct response capability but also enriches its understanding of the logic behind those answers, thereby enhancing its generalization ability.</p>
<h2>A.1.3 Potentially Limited Plateau of Self-evolution Training</h2>
<p>Based on eq. (2) and eq. (3), the model in the $t^{t h}$ round is updated to improve direct response quality by incorporating the generate-feedback-refinement process from the $(t-1)^{t h}$ round. This is based on the assumption that the refined response is superior to the initial one generated by $M_{\text {evol }}^{t-1}$. As illustrated in Fig. 1, the direct generation performance of $M_{\text {evol }}^{t}$ (green curve) consistently falls below the self-refinement of $M_{\text {evol }}^{t-1}$ (blue curve). The self-refinement gains in the $(t-1)^{t h}$ round indicate the potential benefit that the $t^{t h}$ round self-evolution could bring to direct generation. This</p>
<p>also helps determine when to halt the self-evolution process, i.e., the process can be stopped when self-refinement brings no benefit to the direct response.</p>
<h1>A. 2 Prompt of Generating Feedback and Refinement for Meta-skill Corpus</h1>
<p>We introduce the prompt for generating feedback and refinement in two domains: Math and General. We outline specific prompts designed to guide the evaluation and improvement of responses to questions for building $D_{\text {meta }}$ in each domain.</p>
<h2>A.2.1 Math Domain</h2>
<p>For the Math Domain, the prompt instructs evaluators to assess the quality of a response to a math question, provide a step-by-step analysis, and determine its correctness. If the response is incorrect, the evaluator is asked to refine and provide a correct answer.</p>
<h2>Prompt for feedback and refinement:</h2>
<p>(Feedback) Please assess the quality of the response to the given question.
Here is the question: $p$.
Here is the response: $r$.
Firstly, provide a step-by-step analysis and verification for response starting with "Response Analysis:".
Next, judge whether the response correctly answers the question in the format of "judgment: correct/incorrect".
(Refinement) If the answer is correct, output it. Otherwise, output a refined answer based on the given response and your assessment.</p>
<h2>A.2.2 GENERAL DOMAIN</h2>
<p>For the general test, aligned with the methodology described in section 3, we deploy the following prompt to guide an LLM-based annotator in generating response feedback and refinement. This prompt serves as the foundation for the meta-skill learning corpus and assists in producing selfevolution training data in the general test setting.</p>
<h2>Prompt for feedback and refinement:</h2>
<p>(Feedback) Please assess the quality of response to the given question.
Here is the question: $p$.
Here is the response: $r$.
Firstly provide an analysis and verification for response starting with "Response Analysis:".
Next, then rate the response on a scale of 1 to 10 ( 1 is worst, 10 is best) in the format of "Rating:"
(Refinement) Finally output an improved answer based on your analysis if no response is rated 10.</p>
<h2>A. 3 Data GENERATION</h2>
<h2>A.3.1 $D_{\text {META }}$ ATA QUANTITY</h2>
<p>The $D_{\text {meta }}$ dataset was generated using 3.5 k unlabeled prompts from GSM8K and 2 K from SVAMP ${ }^{3}$.
For general tests, 6 K conversations were selected from 90 K ShareGPT dialogues to form the general $D_{\text {meta }}$ data.</p>
<h2>A.3.2 Unlabeled Prompts for Self-Evolution Training</h2>
<p>Math Domain: For math tests, unlabeled prompts in self-evolution training were sourced as follows:
(1) First round self-evolving phase: 4 K leftover prompts from GSM8k and 1K from SVAMP, excluding those used in $D_{\text {meta }}$.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(2) Second/Third rounds: $10 \mathrm{~K} / 15 \mathrm{~K}$ prompts were generated using Self-Instruct method (Wang et al., 2022b), based on a template shown in appendix A.3.2 with initial 4 to 6 seed examples.</p>
<p>General Domain: 15 K unlabeled prompts from ShareGPT dialogues were used for self-evolution training data construction.</p>
<p>You are an experienced instruction creator. You are asked to develop 3 diverse instructions according to the given examples.
Here are the requirements:</p>
<ol>
<li>The generated instructions should follow the task type in the given examples.</li>
<li>The language used for the generated instructions should be diverse.</li>
</ol>
<p>Given examples: {examples}
The generated instructions should be:
A. ...
B. ...
C. ...</p>
<h1>A. 4 Training Hyperparameters</h1>
<p>Our experiments were conducted in a computing environment with 8 NVIDIA V100 GPUs, each having 32GB of memory. All models were fine-tuned in a full-parameter setting. We utilized the AdamW optimizer for model training over 3 epochs, with a batch size of 128. The learning rate was set at $2 \mathrm{e}-5$, including a $3 \%$ learning rate warmup period. Below we provide a comprehensive overview of the training hyperparameters employed in table 5. These parameters were uniformly applied across all training methods in our experiments.</p>
<p>Table 5: Training hyperparameters.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Global Batch Size</th>
<th style="text-align: center;">LR</th>
<th style="text-align: center;">Epo.</th>
<th style="text-align: center;">Max Length</th>
<th style="text-align: center;">Weight Decay</th>
<th style="text-align: center;">Warmup Ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Value</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">$2 \times 10^{-5}$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.03</td>
</tr>
</tbody>
</table>
<h2>A. 5 Case Study Analysis</h2>
<p>This subsubsection provides an in-depth case study that contrasts the performance of the original Vicuna and Vicuna + SELF models. Illustrated in fig. 4, both models perform initial predictions, followed by self-feedback and refinement steps. Notably, Vicuna's refinement fails to correct its initial errors, while Vicuna + SELF effectively utilizes self-feedback and refinement to derive an accurate and logically coherent answer.</p>
<h2>A. 6 Meta-Skill Training Corpus</h2>
<p>The example shown below exemplifies a standard training example from our meta-skill corpus. It illustrates the model's initial response, followed by its self-feedback, and the ensuing refinement. This process demonstrates how the model is trained for self-feedback and self-refinement capabilities.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Case study comparing the original Vicuna (left) and Vicuna+SELF (right) on a SVAMP problem. Both models generate direct predictions and undergo self-feedback and self-refinement. Both models initially produce answers, followed by self-feedback and self-refinement. Vicuna maintains the incorrect response after refinement, whereas Vicuna+SELF demonstrates enhanced selfrefinement, leading to a correct and logically consistent solution.</p>
<p>Question: Greg has an alarm set to ring three times a day as a reminder. When the alarm goes off, it continues to ring until Greg turns it off. The first time it went off today, it rang four times. The second time it went off, it rang for three times as long as the first time. The third time, it rang for half as long as the second time. How many times did the alarm ring in all?
Original Response: The alarm rang four times the first time, three times as long as the first time the second time, and half as long as the second time the third time. So, the total number of times the alarm rang is $4+3 * 3+2 * 2=24$.
Feedback: The response correctly interprets the problem. The first time the alarm rang, it rang for 4 times. The second time it rang, it rang for 3 times as long as the first time, which is $3 * 4=12$ times. The third time it rang, it rang for half as long as the second time, which is $12 / 2=6$ times. However, the final calculation is incorrect. The total number of times the alarm rang is not $4+3 * 3+2 * 2=24$, but $4+12+6=22$. Judgement: incorrect
Refined Response: The alarm rang four times the first time, three times as long as the first time the second time, and half as long as the second time the third time. So, the total number of times the alarm rang is $4+12+6=22$.</p>
<h1>A. 7 Algorithm</h1>
<p>The "Two-Phase SELF Process" algorithm outlines a method for developing a base language model through a two-staged approach: Meta-Skill Learning and Self-Evolving. The process starts with training on a "Meta-Skill Learning corpus", which consists of data representing the generation, feedback and refinement process. Following this, the model enters the "Self-Evolving Phase", where it undergoes iterative refinements, employing data augmentation in each iteration to produce selfrefined outputs from its previously refined versions. This iterative self-evolution aims to leverage accumulated knowledge and further enhance the model with newly generated data. The final out-</p>
<p>come is an advanced Language Model that has significantly evolved from its original state through multiple self-evolution stages. More details are delineated in Alg. 1.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="nx">Two</span><span class="o">-</span><span class="nx">Phase</span><span class="w"> </span><span class="nx">SELF</span><span class="w"> </span><span class="nx">Process</span>
<span class="nx">Data</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="nx">Meta</span><span class="o">-</span><span class="nx">Skill</span><span class="w"> </span><span class="nx">training</span><span class="w"> </span><span class="nx">data</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">meta</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="nx">unlabeled</span><span class="w"> </span><span class="nx">prompts</span>
<span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="nx">An</span><span class="w"> </span><span class="nx">initial</span><span class="w"> </span><span class="nx">Language</span><span class="w"> </span><span class="nx">Model</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">init</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">I</span><span class="p">}}</span><span class="err">\</span><span class="p">)</span>
<span class="nx">Result</span><span class="p">:</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="nx">stronger</span><span class="w"> </span><span class="nx">Language</span><span class="w"> </span><span class="nx">Model</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">T</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">after</span><span class="w"> </span><span class="kp">self</span><span class="o">-</span><span class="nx">evolving</span>
<span class="c1">// Meta-Skill Learning Phase</span>
<span class="nx">Data</span><span class="p">:</span><span class="w"> </span><span class="nx">Meta</span><span class="o">-</span><span class="nx">Skill</span><span class="w"> </span><span class="nx">learning</span><span class="w"> </span><span class="nx">corpus</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">meta</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">)</span>
<span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">meta</span><span class="w"> </span><span class="p">}}=</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Supervised</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{.</span><span class="nx">fine</span><span class="p">.</span><span class="nx">tuning</span><span class="w"> </span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">init</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span><span class="nx">D_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">meta</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">);</span>
<span class="c1">// Self-Evolving Phase</span>
<span class="nx">Initialize</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">I</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">.</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">meta</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">);</span>
<span class="nx">foreach</span><span class="w"> </span><span class="nx">iteration</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">Number</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="kp">self</span><span class="o">-</span><span class="nx">evolving</span><span class="w"> </span><span class="nx">iterations</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">T</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">    </span><span class="c1">// Data-Augmentation</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">empty</span><span class="w"> </span><span class="nx">set</span><span class="p">;</span>
<span class="w">    </span><span class="nx">foreach</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">p_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="w"> </span><span class="nx">h</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">unlabeled</span><span class="w"> </span><span class="nx">prompts</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Generate</span><span class="w"> </span><span class="nx">direct</span><span class="w"> </span><span class="nx">output</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">);</span>
<span class="w">        </span><span class="nx">Generate</span><span class="w"> </span><span class="kp">self</span><span class="o">-</span><span class="nx">refined</span><span class="w"> </span><span class="nx">output</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">r</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">);</span>
<span class="w">        </span><span class="nx">Use</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">filter</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="kp">self</span><span class="o">-</span><span class="nx">refined</span><span class="w"> </span><span class="nx">output</span><span class="p">;</span>
<span class="w">        </span><span class="nx">Add</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">p_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">r</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">refined</span><span class="w"> </span><span class="nx">response</span><span class="p">;</span>
<span class="w">    </span><span class="nx">end</span>
<span class="w">    </span><span class="c1">// Self-Evolution Training</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Supervised</span><span class="p">.</span><span class="nx">fine</span><span class="p">.</span><span class="nx">tuning</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="o">-</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">D_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">);</span>
<span class="nx">end</span>
<span class="c1">// Training Complete</span>
<span class="k">return</span><span class="w"> </span><span class="nx">Improved</span><span class="w"> </span><span class="nx">Language</span><span class="w"> </span><span class="nx">Model</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">evol</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="nx">T</span><span class="p">}</span><span class="err">\</span><span class="p">);</span>
</code></pre></div>

<h1>A. 8 Data Filtering Standards</h1>
<p>We design a boolean function, qualified $(f)$, to evaluate feedback $f$ across different domains, determining if a response to a specific prompt satisfies essential quality criteria.
In the Math Domain,the function assesses feedback based on the explicit statement of "correctness" in the evaluator's judgment, aligned with the prompt structure in appendix A.2.1. It checks if the word "correct" immediately follows the phrase "judgment:" in the feedback. A presence of "correct" results in qualified $(f)$ returning 1, meeting the qualification criteria. Absence leads to a return of 0 .
For the General Domain, following the structure in appendix A.2.2, qualified $(f)$ extracts and evaluates a numerical rating from the feedback. If the rating, found after "Rating:", is 7 or higher, the function returns 1, indicating qualification. Ratings below 7 return 0 , failing to meet the threshold. A rating of 7 balances quality and training data quantity.
qualified $(f)$ is key in both domains for filtering and assessing feedback quality, ensuring only highquality responses are used for refined answer generation in self-evolution training. Post data filtering, $\Psi^{t-1}$ in eq. (3) requires an update to $\Psi^{\prime t-1}=\Psi^{t-1} \times$ qualified $(f)$, adding a quality filter through self-feedback. For clarity, we continue using original formulation as stated in eq. (3) in the main text.</p>
<h2>A. 9 Multiple v.s. Single Self-Refinement</h2>
<p>This study explores the effects of two meta-skill training data organization strategies on model performance: (1) Multiple Self-Refinement ( $D_{\text {meta-multi }}$ ), involving the sampling of three responses for the model to choose the best for refinement, and (2) Single Self-Refinement ( $D_{\text {meta }}$ ), where the model generates and refines a single response.
table 6 compares these methods' performances. Both strategies show performance gains with increased training data volume. However, as data volume expands, the multiple-response refinement shows a smaller improvement in direct generation performance $(+4.02 \%)$ than the single-response</p>
<p>method $(+5.84 \%)$. Considering the simplicity and computational efficiency of the single-response method, which only samples one response during inference, and its better performance than the multiple-response approach, we have opted for the single-response refinement strategy in our experiments.</p>
<p>Table 6: Performance comparison of single and multiple response refinement with varying volumes of meta-skill training data. The arrow indicates improvement from direct generation to selfrefinement: "direct generation $\rightarrow$ self-refinement".</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Data Size</th>
<th style="text-align: center;">Vicuna $+D_{\text {meta }}$</th>
<th style="text-align: center;">Vicuna $+D_{\text {meta-multi }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">3.5 k</td>
<td style="text-align: center;">$25.39 \rightarrow 28.28$</td>
<td style="text-align: center;">$25.92 \rightarrow 27.29$</td>
</tr>
<tr>
<td style="text-align: left;">7.5 k</td>
<td style="text-align: center;">$31.23 \rightarrow 32.98$</td>
<td style="text-align: center;">$29.94 \rightarrow 32.14$</td>
</tr>
</tbody>
</table>
<h1>A. 10 Self-Evolution Training: Continual Training v.s. Restart Training</h1>
<p>Table 7: Analysis about varied self-evolution training methodologies on GSM8K.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Training Approach</th>
<th style="text-align: center;">Direct Generation (\%)</th>
<th style="text-align: center;">Self-Refinement (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Base Model</td>
<td style="text-align: center;">24.49</td>
<td style="text-align: center;">24.49</td>
</tr>
<tr>
<td style="text-align: left;">Restart Training</td>
<td style="text-align: center;">$\mathbf{2 7 . 6 7}$</td>
<td style="text-align: center;">$\mathbf{2 9 . 3 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Continual Training (Mixed Data)</td>
<td style="text-align: center;">27.22</td>
<td style="text-align: center;">28.43</td>
</tr>
<tr>
<td style="text-align: left;">Continual Training ( $D_{\text {excl }}^{t}$ Only)</td>
<td style="text-align: center;">24.87</td>
<td style="text-align: center;">25.85</td>
</tr>
</tbody>
</table>
<p>"Restart Training", which combines meta-skill learning corpus with all rounds of self-evolution training data, significantly improves direct generation ( $+3.18 \%$ ) and self-refinement ( $+3.85 \%$ ).
"Continual Training (Mixed Data)", where the model is trained simultaneously with all rounds of self-evolution data, also shows notable enhancements in direct generation ( $+2.73 \%$ ) and selfrefinement ( $+3.94 \%$ ). In contrast, "Continual Training ( $D_{\text {excl }}^{t}$ Only)", which trains the model sequentially with self-evolution data from each round, demonstrates more modest gains ( $+0.38 \%$ in direct generation, $+0.98 \%$ in self-refinement). The relatively lower performance of the latter approach highlights the importance of a mixed data strategy for effective self-evolution training.
Throughout our main text, we have consistently employed the "Restart Training" method. This approach was selected for its superior performance, as evidenced in table 7. In addition, the integration of $D_{\text {meta }}$ into the self-evolution training is crucial to prevent the potential catastrophic forgetting of meta-skills. This strategy is essential for preserving the effectiveness and reliability of the selfevolution training process, as highlighted in § 3.2.2.</p>
<h2>A. 11 SELF vs. Supervised Fine-Tuning on 7.5 K GSM8K training data.</h2>
<p>When fine-tuned on the GSM8K 7.5 k training set, the Vicuna model achieves an accuracy of $35.70 \%$, which is lower than the SELF method ( $37.87 \%$ ).
The experiments in table 8 use 7.5 k meta-skill data, ensuring a fair comparison with the supervised fine-tuned model. This approach differs from the one in table 1, where only 3.5 k meta-skill data are used.
table 8 indicates that, with 7.5 k unlabeled training prompts for the meta-skill learning corpus, Vicuna $+D_{\mathrm{QA}}$ achieves $28.05 \%$. Post meta-skill learning, direct generation results improve to $31.23 \%$, further increasing to $32.98 \%$ after self-refinement. Subsequent self-evolution rounds lead to performance gains, reaching $37.87 \%$ (direct generation) and $38.12 \%$ (self-refinement) in the second round, outperforming supervised fine-tuning ( $35.70 \%$ ).</p>
<p>Continuous Improvement of SELF vs. Supervised Fine-tuning: SELF's primary advantage lies in its ability for continuous improvement and adaptation. In contrast to supervised fine-tuning,</p>
<p>Table 8: Comparison between SELF and Supervised Fine-Tuning on GSM8K. A "-" symbol in the table indicates self-refinement was not conducted during inference because the model lacks the necessary self-refinement skill.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Direct Generation (\%)</th>
<th style="text-align: center;">Self-Refinement (\%)</th>
<th style="text-align: center;">$D_{\mathrm{QA}}$</th>
<th style="text-align: center;">$D_{\text {meta }}$</th>
<th style="text-align: center;">Self Evol.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1 st</td>
<td style="text-align: center;">2 nd</td>
</tr>
<tr>
<td style="text-align: center;">28.05</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">31.23</td>
<td style="text-align: center;">32.98</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">35.43</td>
<td style="text-align: center;">36.22</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">37.87</td>
<td style="text-align: center;">38.12</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">35.70</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">(GSM8K training data)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>SELF doesn't rely on human or external LLM annotations (like GPT3.5/GPT4) for training data in self-evolution training.</p>
<h1>A. 12 Scalability of SELF Framework</h1>
<p>To explore how SELF performs with different starting model qualities, we conduct experiments using the OpenLlama-3b model (Geng \&amp; Liu, 2023), a smaller LLM along with a stronger LLM, VicunaV1.5(finetuned from Llama2-7b)1 (Chiang et al., 2023), on the GSM8K dataset. This allows us to assess SELF's adaptability to model quality. Experiments with SELF are based on the first round of self-evolution. The results are as follows:</p>
<p>Table 9: Scalability of the SELF framework across different models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Direct Generation (\%)</th>
<th style="text-align: center;">Self-Refinement (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OpenLlama-3b</td>
<td style="text-align: center;">2.04</td>
<td style="text-align: center;">1.01</td>
</tr>
<tr>
<td style="text-align: left;">OpenLlama-3b $+D_{\mathrm{QA}}$</td>
<td style="text-align: center;">12.13</td>
<td style="text-align: center;">10.97</td>
</tr>
<tr>
<td style="text-align: left;">OpenLlama-3b $+D_{\mathrm{QA}}+$ SELF</td>
<td style="text-align: center;">15.32</td>
<td style="text-align: center;">15.78</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna (Llama-7b)</td>
<td style="text-align: center;">16.43</td>
<td style="text-align: center;">15.63</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna $+D_{\mathrm{QA}}$</td>
<td style="text-align: center;">24.49</td>
<td style="text-align: center;">24.44</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna $+D_{\mathrm{QA}}+$ SELF</td>
<td style="text-align: center;">27.67</td>
<td style="text-align: center;">29.34</td>
</tr>
<tr>
<td style="text-align: left;">VicunaV1.5 (Llama2-7b)</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">17.43</td>
</tr>
<tr>
<td style="text-align: left;">VicunaV1.5 $+D_{\mathrm{QA}}$</td>
<td style="text-align: center;">26.04</td>
<td style="text-align: center;">25.48</td>
</tr>
<tr>
<td style="text-align: left;">VicunaV1.5 $+D_{\mathrm{QA}}+$ SELF</td>
<td style="text-align: center;">$\mathbf{3 0 . 2 2}$</td>
<td style="text-align: center;">$\mathbf{3 2 . 4 3}$</td>
</tr>
</tbody>
</table>
<p>Applicability and Robustness of SELF Framework: The average improvement of $17.32 \%$ via direct generation and $16.87 \%$ after self-refinement underscores the framework's scalability and efficacy. It reveals a consistent positive impact of the SELF Framework across diverse models.</p>
<p>SELF Framework exhibits enhanced performance on more powerful models: As shown in table 9 , applying SELF to VicunaV1.5 results in the most significant gains - $30.22 \%$ in direct generation and $32.43 \%$ after self-refinement, surpassing the performance on Vicuna and OpenLlama-3b. This indicates that the effectiveness of the SELF framework improves with the underlying model's capabilities.</p>
<h2>A. 13 IMPACT OF META-SKILL CORPUS QUALITY</h2>
<p>We examine the influence of meta-skill learning quality on the self-evolution process with the following results:</p>
<p>The presented table 10 demonstrates the remarkable performance improvements achieved by using GPT-4 for generating the meta-skill corpus in our SELF framework, compared to using GPT-3.5-</p>
<p>Table 10: Effect of meta-skill corpus quality on model performance using GPT-3.5-turbo and GPT4.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Training Stage</th>
<th style="text-align: center;">Direct Generation (\%) <br> (GPT-3.5-turbo/GPT4)</th>
<th style="text-align: center;">Self-Refinement (\%) <br> (GPT-3.5-turbo/GPT4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Vicuna $+D_{\text {meta }}$</td>
<td style="text-align: center;">$24.84 / 25.39(0.55 \dagger)$</td>
<td style="text-align: center;">$25.22 / 28.28(3.06 \dagger)$</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna $+D_{\text {meta }}+$ SELF Evol.</td>
<td style="text-align: center;">$25.11 / 27.67(2.56 \dagger)$</td>
<td style="text-align: center;">$25.47 / 29.34(3.87 \dagger)$</td>
</tr>
</tbody>
</table>
<p>turbo. The table shows significant enhancements in both direct generation and self-refinement across training stages when GPT-4 is utilized. For instance, in the "Vicuna $+D_{\text {meta }}$ " stage, direct generation performance increases from $24.84 \%$ with GPT-3.5-turbo to $25.39 \%$ with GPT-4, marking a gain of $0.55 \%$. Similarly, in the "Vicuna $+D_{\text {meta }}+$ SELF Evolution" stage, the self-refinement result improves from $25.47 \%$ with GPT-3.5-turbo to $29.34 \%$ with GPT-4, showing an enhancement of $3.87 \%$.</p>
<p>This analysis highlights the significant impact of utilizing high-quality meta-skill training data on the performance of the Vicuna model within the SELF framework. The shift from GPT-3.5-turbo to GPT-4 for the generation of the meta-skill corpus leads to consistent improvements in both Direct Generation and Self-Refinement metrics.</p>
<h1>A. 14 Single-Round vs. Iterative Self-Evolution Training</h1>
<p>Given an equal number of unlabeled prompts, we evaluate the effectiveness of training within a single-round versus iterative training. The former method uses a single model to self-curate training data from all available unlabeled prompts at once. In contrast, the latter method involves dividing the unlabeled prompts into multiple parts. For the iterative approach, the model is initially trained on a portion of the unlabeled prompts and self-curated labels. Following this, the trained model is employed to create new training data based on previously unused prompts. As described in our main text, we divide the unlabeled prompts into three parts, enabling the model to undergo three iterative rounds of self-evolution.</p>
<p>Table 11: Comparison of single-round training and iterative training.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Training Method</th>
<th style="text-align: center;">Direct Generation (\%)</th>
<th style="text-align: center;">Self-Refinement (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SELF (Single-Round)</td>
<td style="text-align: center;">28.40</td>
<td style="text-align: center;">30.55</td>
</tr>
<tr>
<td style="text-align: left;">SELF (Iterative)</td>
<td style="text-align: center;">29.64</td>
<td style="text-align: center;">31.31</td>
</tr>
</tbody>
</table>
<p>table 11 shows that in the "Single-Round" training, the performance is $28.40 \%$ for direct generation and $30.55 \%$ for self-refinement. In contrast, the iterative approach yields higher scores of $29.64 \%$ for direct generation and $31.31 \%$ for self-refinement.</p>
<p>Advantages of Iterative Training: Iterative training benefits from the enhanced capabilities of LLMs in subsequent rounds, which generate higher-quality training data and lead to improved test performance.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Adhering to the official recommendation https://github.com/arkilpate1/SVAMP/tree/ main, training prompts consist of MAWPS (Koncel-Kedziorski et al., 2016) and ASDiv-A (Miao et al., 2020)&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
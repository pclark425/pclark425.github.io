<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-823 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-823</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-823</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-262012476</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.08172v2.pdf" target="_blank">LASER: LLM Agent with State-Space Exploration for Web Navigation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have been successfully adapted for interactive decision-making tasks like web navigation. While achieving decent performance, previous meth-ods implicitly assume a forward-only execution mode for the model, where they only provide oracle trajectories as in-context examples to teach the model how to reason in the interactive environment. Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-optimal performance. To address this issue, we propose to model the interactive task as state space exploration, where the LLM agent transitions among a pre-defined set of states by performing actions to complete the task. This formulation enables flexible back-tracking, allowing the model to easily recover from errors. We evaluate our proposed L LM A gent with S tate-Space E xplo R ation (LASER) on the WebShop task. Experimental results show that our LASER agent significantly out-performs previous methods and closes the gap with human performance on the web navigation task. 1</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e823.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e823.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LASER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Agent with State-Space ExploRation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven web navigation agent that models interactive tasks as state-space exploration using state-specific instructions, per-state permissible action sets, state-tracking, and function-calling to enable backtracking and error recovery without in-context trajectory examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LASER</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-space agent built on a chat LLM (GPT-4-0613) that: (1) defines a small set of high-level environment states with sample observation layouts and state-specific system instructions; (2) constrains action space per-state to permissible actions; (3) keeps a memory buffer of examined items; (4) uses function-calling for deterministic action selection; and (5) produces thought + action (ReAct-style) at each step.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>WebShop (and sim-to-real on amazon.com)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation; sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>50.0 (reward, scale 0-100); 75.6% success rate on WebShop test set (500 episodes). Also close to human performance on amazon.com in sim-to-real transfer (human: 59.6 reward, 82.1% success).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>state-tracking; state-space formulation; state-specific prompts; constrained per-state action space; memory buffer for backups; function-calling interface; thought-and-action loop (ReAct-style).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only (zero-shot with manually written state-specific instructions); no fine-tuning; no in-context trajectory examples (zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change; prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Replace forward-only in-context trajectory prompting with an explicit state-space formulation: (a) manually define a small set of high-level states and provide state-specific system instructions and sample observation layouts; (b) provide per-state permissible action lists to avoid invalid actions; (c) allow state transitions and backtracking so the agent can recover from mistakes; (d) use a memory buffer and backup strategy; (e) use the model's function-calling API for action selection rather than free-text action JSON generation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Large improvement over baselines on WebShop: reward improved from 34.0 (ReAct rerun) to 50.0 and success rate from 59.7% to 75.6%; LASER -backup ablation: 48.4 reward / 71.2% success. LASER narrows gap to human expert (human: 59.6 reward, 82.1% success). Addition of an in-context example to LASER degraded performance in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Prior forward-only in-context trajectory examples teach only correct step sequences and do not show how to recover from mistakes; global action spaces allow the LLM to attempt invalid actions; lack of explicit state-tracking makes error recovery and valid action selection harder in interactive tasks compared to static NLU/QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e823.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e823.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that interleaves 'thought' (reasoning) and 'action' tokens so an LLM can both reason about and act in environments, typically using in-context trajectory demonstrations (oracle trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ReAct (prompting method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting strategy that asks the LLM to emit a reasoning trace ('thought') then an environment action at each step, typically provided with full oracle trajectories as in-context examples to guide the stepwise behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>WebShop (used as baseline; rerun with GPT-4-0613 in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation; sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>34.0 (reward, scale 0-100); 59.7% success rate (ReAct rerun with GPT-4-0613 reported by this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>thought-and-action interleaving (chain-of-thought style for actions).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>in-context learning / few-shot prompting using oracle trajectories as demonstrations (prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Interleave internal reasoning ('thought') and actions; use full trajectories as in-context examples to teach step-by-step behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Baseline performance for interactive WebShop task lower than LASER (34.0 reward / 59.7% success in this paper's rerun). The paper reports that ReAct often produced invalid actions or got stuck (e.g., clicking nonexistent next-page buttons or repeatedly paginating).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Forward-only oracle trajectories used in in-context examples fail to teach recovery from mistakes and may not prevent invalid actions; global action space increases chance of invalid choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e823.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e823.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ASH (Hierarchical prompting approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical prompting method for web navigation that decomposes complex decision-making into simpler subtasks, used as a baseline in web navigation benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hierarchical prompting assists large language model on web navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ASH</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hierarchical prompting/decomposition approach that breaks decision-making into sub-steps (e.g., summarization then action) to simplify interactive tasks for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>WebShop (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation; sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>30.2 (reward, scale 0-100); 56.7% success rate (reported baseline in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>task decomposition / hierarchical prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting with manual instructions and in-context demonstrations (few-shot prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy / task decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Decompose complex decision-making into simpler steps, e.g., summarize task-relevant content then act.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reported baseline performance lower than LASER (30.2 reward / 56.7% success). Paper notes LASER is more robust to longer trajectories compared to ASH.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Implicit: forward-only examples and lack of explicit state-space/backtracking reduce robustness in interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e823.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e823.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebGUM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction-finetuned foundation models for multimodal web navigation (WebGUM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that finetunes models on a large set (~1k) of human-annotated gold trajectories for web navigation to train agents for web-based tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instruction-finetuned foundation models for multimodal web navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>WebGUM (finetuned model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-finetuned model trained on a large human-annotated dataset of web navigation trajectories (uses supervised fine-tuning on 1k gold trajectories according to the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>WebShop (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation; sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>45.0 (reward, scale 0-100); 67.5% success rate (reported baseline in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>finetuned instruction-following model (supervised on trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised fine-tuning on human-annotated web navigation trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (supervised fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Train / finetune a model on many human-annotated gold trajectories to teach action sequences for web navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Stronger than some prompting-only baselines but LASER (prompting/state-space) still outperforms WebGUM in reported WebShop numbers (50.0 reward / 75.6% success for LASER vs 45.0/67.5 for WebGUM).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>While finetuning on trajectories provides low-level task-specific knowledge, it may be less generalizable than providing high-level state-specific instructions; the paper argues high-level instructions are more efficient/generalizable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e823.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e823.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (chat model variant, function-calling enabled, 2023-06-13)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chat-oriented large language model with function-calling capability used as the backbone LLM to power LASER and baselines rerun in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-focused LLM provided by OpenAI exposing a function-calling API that enables the caller to present permissible actions as callable functions; used in this paper to implement deterministic action selection and to support LASER's prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>WebShop (used to power LASER and rerun ReAct baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation; sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Used as the LLM backend for LASER (see LASER interactive performance). Rerun ReAct with GPT-4-0613 yielded 34.0 reward / 59.7% success.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>chat interface; function-calling API (structured action selection).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>model capability (API feature) / prompting integration</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Use function-calling to present permissible actions as functions and let the model select actions rather than free-text generation; compare with an ablation where actions are encoded as JSON in prompt and parsed from text output.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Function-calling produced slightly better interactive performance on a 200-episode ablation vs text-JSON output parsing: the agent without function-calling performed slightly worse.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e823.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e823.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003 (OpenAI GPT-3.5 family, non-chat model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A less-capable non-chat LLM used to test LASER's generalization to models without function-calling support by prompting it to emit JSON actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Non-chat OpenAI LLM that does not support the function-calling API; used with a prompt engineering approach to have it output JSON representing selected actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>WebShop (generalization experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>web navigation; sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Large drop in performance compared to GPT-4-0613-powered LASER (exact numbers not provided in text), but still outperforms baselines in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>no function-calling API; JSON-output prompting used for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>robustness/generalization experiment (prompting adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Adapt LASER prompts to a non-function-calling model by appending action dictionaries to the prompt and asking the model to generate a JSON action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Performance dropped substantially when switching from GPT-4-0613 to text-davinci-003, but LASER still outperformed prior baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Webshop: Towards scalable realworld web interaction with grounded language agents <em>(Rating: 2)</em></li>
                <li>Hierarchical prompting assists large language model on web navigation <em>(Rating: 2)</em></li>
                <li>Instruction-finetuned foundation models for multimodal web navigation <em>(Rating: 2)</em></li>
                <li>Synapse: Trajectory-as-exemplar prompting with memory for computer control <em>(Rating: 1)</em></li>
                <li>WebVoyager: Building an end-to-end web agent with large multimodal models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-823",
    "paper_id": "paper-262012476",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "LASER",
            "name_full": "LLM Agent with State-Space ExploRation",
            "brief_description": "An LLM-driven web navigation agent that models interactive tasks as state-space exploration using state-specific instructions, per-state permissible action sets, state-tracking, and function-calling to enable backtracking and error recovery without in-context trajectory examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "LASER",
            "model_description": "State-space agent built on a chat LLM (GPT-4-0613) that: (1) defines a small set of high-level environment states with sample observation layouts and state-specific system instructions; (2) constrains action space per-state to permissible actions; (3) keeps a memory buffer of examined items; (4) uses function-calling for deterministic action selection; and (5) produces thought + action (ReAct-style) at each step.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "WebShop (and sim-to-real on amazon.com)",
            "interactive_task_type": "web navigation; sequential decision-making",
            "interactive_performance": "50.0 (reward, scale 0-100); 75.6% success rate on WebShop test set (500 episodes). Also close to human performance on amazon.com in sim-to-real transfer (human: 59.6 reward, 82.1% success).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "state-tracking; state-space formulation; state-specific prompts; constrained per-state action space; memory buffer for backups; function-calling interface; thought-and-action loop (ReAct-style).",
            "training_method": "prompting only (zero-shot with manually written state-specific instructions); no fine-tuning; no in-context trajectory examples (zero-shot).",
            "intervention_type": "architectural change; prompting strategy",
            "intervention_description": "Replace forward-only in-context trajectory prompting with an explicit state-space formulation: (a) manually define a small set of high-level states and provide state-specific system instructions and sample observation layouts; (b) provide per-state permissible action lists to avoid invalid actions; (c) allow state transitions and backtracking so the agent can recover from mistakes; (d) use a memory buffer and backup strategy; (e) use the model's function-calling API for action selection rather than free-text action JSON generation.",
            "intervention_effect": "Large improvement over baselines on WebShop: reward improved from 34.0 (ReAct rerun) to 50.0 and success rate from 59.7% to 75.6%; LASER -backup ablation: 48.4 reward / 71.2% success. LASER narrows gap to human expert (human: 59.6 reward, 82.1% success). Addition of an in-context example to LASER degraded performance in experiments.",
            "hypothesized_cause_of_gap": "Prior forward-only in-context trajectory examples teach only correct step sequences and do not show how to recover from mistakes; global action spaces allow the LLM to attempt invalid actions; lack of explicit state-tracking makes error recovery and valid action selection harder in interactive tasks compared to static NLU/QA tasks.",
            "uuid": "e823.0",
            "source_info": {
                "paper_title": "LASER: LLM Agent with State-Space Exploration for Web Navigation",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "A prompting method that interleaves 'thought' (reasoning) and 'action' tokens so an LLM can both reason about and act in environments, typically using in-context trajectory demonstrations (oracle trajectories).",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "model_or_agent_name": "ReAct (prompting method)",
            "model_description": "Prompting strategy that asks the LLM to emit a reasoning trace ('thought') then an environment action at each step, typically provided with full oracle trajectories as in-context examples to guide the stepwise behavior.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "WebShop (used as baseline; rerun with GPT-4-0613 in this paper)",
            "interactive_task_type": "web navigation; sequential decision-making",
            "interactive_performance": "34.0 (reward, scale 0-100); 59.7% success rate (ReAct rerun with GPT-4-0613 reported by this paper).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "thought-and-action interleaving (chain-of-thought style for actions).",
            "training_method": "in-context learning / few-shot prompting using oracle trajectories as demonstrations (prompting).",
            "intervention_type": "prompting strategy",
            "intervention_description": "Interleave internal reasoning ('thought') and actions; use full trajectories as in-context examples to teach step-by-step behavior.",
            "intervention_effect": "Baseline performance for interactive WebShop task lower than LASER (34.0 reward / 59.7% success in this paper's rerun). The paper reports that ReAct often produced invalid actions or got stuck (e.g., clicking nonexistent next-page buttons or repeatedly paginating).",
            "hypothesized_cause_of_gap": "Forward-only oracle trajectories used in in-context examples fail to teach recovery from mistakes and may not prevent invalid actions; global action space increases chance of invalid choices.",
            "uuid": "e823.1",
            "source_info": {
                "paper_title": "LASER: LLM Agent with State-Space Exploration for Web Navigation",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "ASH",
            "name_full": "ASH (Hierarchical prompting approach)",
            "brief_description": "A hierarchical prompting method for web navigation that decomposes complex decision-making into simpler subtasks, used as a baseline in web navigation benchmarks.",
            "citation_title": "Hierarchical prompting assists large language model on web navigation",
            "mention_or_use": "mention",
            "model_or_agent_name": "ASH",
            "model_description": "Hierarchical prompting/decomposition approach that breaks decision-making into sub-steps (e.g., summarization then action) to simplify interactive tasks for LLMs.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "WebShop (reported baseline)",
            "interactive_task_type": "web navigation; sequential decision-making",
            "interactive_performance": "30.2 (reward, scale 0-100); 56.7% success rate (reported baseline in paper).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "task decomposition / hierarchical prompting.",
            "training_method": "prompting with manual instructions and in-context demonstrations (few-shot prompting).",
            "intervention_type": "prompting strategy / task decomposition",
            "intervention_description": "Decompose complex decision-making into simpler steps, e.g., summarize task-relevant content then act.",
            "intervention_effect": "Reported baseline performance lower than LASER (30.2 reward / 56.7% success). Paper notes LASER is more robust to longer trajectories compared to ASH.",
            "hypothesized_cause_of_gap": "Implicit: forward-only examples and lack of explicit state-space/backtracking reduce robustness in interactive tasks.",
            "uuid": "e823.2",
            "source_info": {
                "paper_title": "LASER: LLM Agent with State-Space Exploration for Web Navigation",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "WebGUM",
            "name_full": "Instruction-finetuned foundation models for multimodal web navigation (WebGUM)",
            "brief_description": "A method that finetunes models on a large set (~1k) of human-annotated gold trajectories for web navigation to train agents for web-based tasks.",
            "citation_title": "Instruction-finetuned foundation models for multimodal web navigation",
            "mention_or_use": "mention",
            "model_or_agent_name": "WebGUM (finetuned model)",
            "model_description": "Instruction-finetuned model trained on a large human-annotated dataset of web navigation trajectories (uses supervised fine-tuning on 1k gold trajectories according to the paper).",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "WebShop (reported baseline)",
            "interactive_task_type": "web navigation; sequential decision-making",
            "interactive_performance": "45.0 (reward, scale 0-100); 67.5% success rate (reported baseline in paper).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "finetuned instruction-following model (supervised on trajectories).",
            "training_method": "supervised fine-tuning on human-annotated web navigation trajectories.",
            "intervention_type": "training method (supervised fine-tuning)",
            "intervention_description": "Train / finetune a model on many human-annotated gold trajectories to teach action sequences for web navigation.",
            "intervention_effect": "Stronger than some prompting-only baselines but LASER (prompting/state-space) still outperforms WebGUM in reported WebShop numbers (50.0 reward / 75.6% success for LASER vs 45.0/67.5 for WebGUM).",
            "hypothesized_cause_of_gap": "While finetuning on trajectories provides low-level task-specific knowledge, it may be less generalizable than providing high-level state-specific instructions; the paper argues high-level instructions are more efficient/generalizable.",
            "uuid": "e823.3",
            "source_info": {
                "paper_title": "LASER: LLM Agent with State-Space Exploration for Web Navigation",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-4-0613",
            "name_full": "GPT-4 (chat model variant, function-calling enabled, 2023-06-13)",
            "brief_description": "A chat-oriented large language model with function-calling capability used as the backbone LLM to power LASER and baselines rerun in this paper.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4-0613",
            "model_description": "Chat-focused LLM provided by OpenAI exposing a function-calling API that enables the caller to present permissible actions as callable functions; used in this paper to implement deterministic action selection and to support LASER's prompts.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "WebShop (used to power LASER and rerun ReAct baseline)",
            "interactive_task_type": "web navigation; sequential decision-making",
            "interactive_performance": "Used as the LLM backend for LASER (see LASER interactive performance). Rerun ReAct with GPT-4-0613 yielded 34.0 reward / 59.7% success.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "chat interface; function-calling API (structured action selection).",
            "training_method": null,
            "intervention_type": "model capability (API feature) / prompting integration",
            "intervention_description": "Use function-calling to present permissible actions as functions and let the model select actions rather than free-text generation; compare with an ablation where actions are encoded as JSON in prompt and parsed from text output.",
            "intervention_effect": "Function-calling produced slightly better interactive performance on a 200-episode ablation vs text-JSON output parsing: the agent without function-calling performed slightly worse.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e823.4",
            "source_info": {
                "paper_title": "LASER: LLM Agent with State-Space Exploration for Web Navigation",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "text-davinci-003",
            "name_full": "text-davinci-003 (OpenAI GPT-3.5 family, non-chat model)",
            "brief_description": "A less-capable non-chat LLM used to test LASER's generalization to models without function-calling support by prompting it to emit JSON actions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "text-davinci-003",
            "model_description": "Non-chat OpenAI LLM that does not support the function-calling API; used with a prompt engineering approach to have it output JSON representing selected actions.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "WebShop (generalization experiment)",
            "interactive_task_type": "web navigation; sequential decision-making",
            "interactive_performance": "Large drop in performance compared to GPT-4-0613-powered LASER (exact numbers not provided in text), but still outperforms baselines in the paper's experiments.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "no function-calling API; JSON-output prompting used for action selection.",
            "training_method": null,
            "intervention_type": "robustness/generalization experiment (prompting adaptation)",
            "intervention_description": "Adapt LASER prompts to a non-function-calling model by appending action dictionaries to the prompt and asking the model to generate a JSON action selection.",
            "intervention_effect": "Performance dropped substantially when switching from GPT-4-0613 to text-davinci-003, but LASER still outperformed prior baselines.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e823.5",
            "source_info": {
                "paper_title": "LASER: LLM Agent with State-Space Exploration for Web Navigation",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Webshop: Towards scalable realworld web interaction with grounded language agents",
            "rating": 2,
            "sanitized_title": "webshop_towards_scalable_realworld_web_interaction_with_grounded_language_agents"
        },
        {
            "paper_title": "Hierarchical prompting assists large language model on web navigation",
            "rating": 2,
            "sanitized_title": "hierarchical_prompting_assists_large_language_model_on_web_navigation"
        },
        {
            "paper_title": "Instruction-finetuned foundation models for multimodal web navigation",
            "rating": 2,
            "sanitized_title": "instructionfinetuned_foundation_models_for_multimodal_web_navigation"
        },
        {
            "paper_title": "Synapse: Trajectory-as-exemplar prompting with memory for computer control",
            "rating": 1,
            "sanitized_title": "synapse_trajectoryasexemplar_prompting_with_memory_for_computer_control"
        },
        {
            "paper_title": "WebVoyager: Building an end-to-end web agent with large multimodal models",
            "rating": 1,
            "sanitized_title": "webvoyager_building_an_endtoend_web_agent_with_large_multimodal_models"
        }
    ],
    "cost": 0.01505225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LASER: LLM Agent with State-Space Exploration for Web Navigation
21 Feb 2024</p>
<p>Kaixin Ma kaixinma@global.tencent.com 
Tencent AI Lab
BellevueWA</p>
<p>Hongming Zhang hongmingzhang@global.tencent.com 
Tencent AI Lab
BellevueWA</p>
<p>Hongwei Wang hongweiw@global.tencent.com 
Tencent AI Lab
BellevueWA</p>
<p>Xiaoman Pan xiaomanpan@global.tencent.com 
Tencent AI Lab
BellevueWA</p>
<p>Wenhao Yu wenhaowyu@global.tencent.com 
Tencent AI Lab
BellevueWA</p>
<p>Dong Yu 
Tencent AI Lab
BellevueWA</p>
<p>LASER: LLM Agent with State-Space Exploration for Web Navigation
21 Feb 202478E6406E8029C9EFE35A31130A06E16BarXiv:2309.08172v2[cs.CL]
Large language models (LLMs) have been successfully adapted for interactive decisionmaking tasks like web navigation.While achieving decent performance, previous methods implicitly assume a forward-only execution mode for the model, where they only provide oracle trajectories as in-context examples to guide the model on how to reason in the environment.Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-optimal performance.To address this issue, we propose to model the interactive task as state space exploration, where the LLM agent transitions among a pre-defined set of states by performing actions to complete the task.This formulation enables flexible backtracking, allowing the model to recover from errors easily.We evaluate our proposed LLM Agent with State-Space ExploRation (LASER) on both the WebShop task and amazon.com.Experimental results show that LASER significantly outperforms previous methods and closes the gap with human performance on the web navigation task.</p>
<p>Introduction</p>
<p>Large language models (LLMs) such as GPT-4 (OpenAI, 2023) have achieved remarkable performance on a wide range of natural language understanding (NLU) tasks (Brown et al., 2020;Ouyang et al., 2022;Wei et al., 2022).Recently, they have been adapted to interactive decision-making tasks such as virtual home navigation (Yang et al., 2023), text-based games (Lin et al., 2023) or webnavigation (Yao et al., 2023;Zhou et al., 2024).Previous methods that utilize LLMs to solve interactive tasks often implicitly assume a forward-only execution mode for the model, where they only provide a few oracle trajectories as in-context examples to teach the model how to reason step-by-step (Yao et al., 2023;Lo et al., 2023).In other words, the correct action is selected at every step in those oracle trajectories.This might lead to sub-optimal performance because when the model makes an unexpected mistake at test time, it would not know how to recover from it.At the same time, including many in-context examples to cover all possible scenarios is costly or unrealistic.Moreover, previous methods assume a global action space where the model is free to take any action at any step because they either define the possible actions at the beginning of the prompt or expect the LLM to figure out the possible action from in-context examples automatically.This might further increase the task's difficulty, and the LLM may perform invalid actions in certain cases.</p>
<p>To address the aforementioned issues, we propose to model the interactive tasks as state-space exploration.We first define a set of high-level possible states the LLM agent might encounter during the task execution.Then, we identify the possible action space in each state and the resulting states after performing each action.This formulation effectively converts the LLM agent's exploration in the interactive task as state transitions, where each action takes the agent from one state to another.Naturally, this allows the agent to easily recover from a wrong action: taking another action that would send it back to the previous state.Moreover, our proposed formulation associates the action space with each individual state, which reduces the task's difficulty and allows the agent to always select the valid action at any step.We evaluated our proposed LASER on the Webshop (Yao et al., 2022) task and conducted sim-to-real transfer experiments where we directly applied LASER to amazon.com.We show that our proposed setup enables the agent to complete complex user instructions without using in-context examples, and LASER significantly outperforms all previous baselines and closes the gap with human performance.</p>
<p>Methods</p>
<p>Problem Formulation</p>
<p>Given a web environment E and a user instruction I, the agent is instantiated in the environment and provided with an initial observation O 0 .The agent is expected to perform a series of actions {a 0 , a 1 , ...a n } to complete the user instruction, where each a i produces a new observation O i when executed in the environment.S denotes the stopping state where the agent produces an output and stops exploration after reaching it.Finally, the agent's output is compared with the target to compute the metrics.</p>
<p>LLM Agent</p>
<p>As previously discussed, we would like the agent to be able to handle any novel situations or mistakes that might occur during execution without exhaustively describing them via a large number of in-context examples.Thus, we propose to equip LLM agents with the state-tracking capability.A diagram of the state transitions of our agent is shown in Figure 1.We start by defining a set of possible high-level states the agent might encounter in the environment ( 2.3).The LLM agent takes the user input as the overall goal and is initialized in the starting state.At every step, the agent receives state-specific system instruction, current observation, a set of permissible actions in the current states, and the history of past thoughts and actions as inputs.Then, it selects one of the actions as output, which either transitions the agent to a different state or remains in the same state ( 2.4).The agent repeats the process until the stopping state or the maximum step is reached.</p>
<p>Notice that with our formulation, we can provide detailed instructions to inform the agent of the possible situations in every state and how to handle them.For example, as shown in Figure 1, at the results state, the current results may or may not be good enough, and we instruct the agent to either select an item, go to the next page, or go back to search depending on its judgment.Hence, these instructions can be very informative to guide the agent while being much more efficient than incontext examples.Next, we describe in detail how we design the state and action spaces.</p>
<p>State Description</p>
<p>In our work, we use the term state to describe the current environment the agent is in, and we consider an agent to be in two different states only if the structure of the current environment observation is different.This allows us to define only a handful of states to support an agent's exploration in a complex environment fully.</p>
<p>After manually categorizing all possible states in the interactive task, for each state, we write a generic instruction that describes the state in detail.Specifically, we provide a sample layout of the observation the agent would receive in that state and replace all specifications in the layout with placeholders.We also provide a high-level goal and detailed instructions to act in that state.The sample layout combined with state-specific instructions allows us to inform the agent of possible observations it might receive and how to act accordingly.Therefore we no longer need to provide in-context examples to guide the agent.For the WebShop task, we define a total of four states, and the full prompts for search, results, and item states can be found in Table 4, Table 5 and Table 6 in the appendix.</p>
<p>Action Space</p>
<p>Previous methods often implicitly assume a global action space for the model, i.e. the model is free to take any action without further constraints.Although the LLM is able to figure out valid actions to take most of the time, it might still attempt to take invalid actions in certain cases.Thus after defining all possible states for the task, we further identify the action space for each state to rule out such possibilities.Specifically, we define a set of permissible actions that the agent can choose from for each state, which ensures that the agent always performs valid actions.The state-action mapping for our agent is shown in Table 8 in the appendix.In practice, permissible actions can also be determined heuristically, e.g., identifying all clickable buttons on a webpage.Success Rate Reward ASH (Lo et al., 2023) 30.2 56.7 ReAct (Yao et al., 2023) Inspired by the ReAct method (Yao et al., 2023), we also ask the agent to produce a thought at every step and then select an action based on its thought.The agent keeps repeating the thought-and-action process until it reaches the stopping state or the maximum step is reached.We also define a memory buffer to store the intermediate results (the items examined but considered non-matching) during the exploration.This is similar to human behavior in that people typically find a few backup options before finding the desired item.When the agent is forced to stop after the maximum number of steps, it selects one of the intermediate results as the final output, and we call this the backup strategy.</p>
<p>Experiments</p>
<p>We conduct our experiments on the WebShop task (Yao et al., 2022).We used 500 test set instructions for evaluation and adopted reward and success rate as metrics following previous works (Yao et al., 2022).We used GPT-4-0613 to power LASER and its function-calling ability to implement action selection step.We compare against the following baselines: ReAct (Yao et al., 2023) is a prompting method designed for interactive decision-making tasks.At every step, the LLM agent receives an observation and can either produce a thought or an action.The agent accumulates all of the past observations, thoughts, and actions in its prompt, using a full trajectory of exploration as an in-context example.The original ReAct uses PaLM (Chowdhery et al., 2023) as its LLM backbone.To make a fair comparison, we also rerun the ReAct method with GPT-4-0613.ASH (Lo et al., 2023)</p>
<p>Results</p>
<p>The overall results of our experiments are shown in Table 1.Our early experiments showed that the ReAct agent often produces invalid actions.For example, when it selects an item that doesn't match the instruction, it tries to click the next page button (which does not exist) before backing to the results page.Also, the ReAct agent often got stuck in a certain action and failed to produce output.For example, the agent keeps going to the next page until the maximum step is reached.We added detailed instructions as the system prompt to try to address the issue.Despite our best efforts, the agent still makes invalid actions in some cases and achieves worse results than the original paper.On the other hand, LASER outperforms baselines by large margins on both metrics, showing the effectiveness of our approach.We further removed the backup strategy of LASER (the agent would receive a 0 score when the maximum budget runs out) to make a more fair comparison with ReAct.We see that our method still outperforms baselines by very large margins.The results from the transfer experiments are shown in Table 2. Again, LASER achieves very close results compared to human performance.It's also encouraging to see that LASER even achieved better performance on this realistic environment than the WebShop, which is likely due to the stronger search engine on amazon.com.3. We see that adding an in-context example actually leads to worse performance.Since LASER already performs valid actions 100% time, we hypothesize that the agent understands the task well without incontext examples and the added example is actually distracting the agent in some cases.</p>
<p>Effect of function-calling LASER takes advantage of the function-calling functionality that is enabled only for GPT models after 06/13/23.Thus, we are interested to see the effect of replacing this design with regular text generation.To do so, instead of passing the permissible actions as a list of functions, we convert each action as a Python dictionary describing its purpose and arguments and then append them to the prompt.We then ask the LLM to generate output in JSON format to represent the action it selects with appropriate arguments.The results are shown in Table 3. Again, the agent without function calling performs slightly worse on these 200 episodes.It shows that the function calling functionality can be leveraged to boost performance when building interactive agents, suggesting a direction for building future LLMs.</p>
<p>Performance vs trajectory length Here, we are interested in seeing the length of LASER's trajectories and their effect on the overall performance.We plot the distribution of trajectory length in Figure 2 and the agent's performance for each length group.We notice that most of the time, the agent only took three state transitions to reach the finish state, which is search-select-buy.From the left figure, the agent's performance generally decreases as the trajectory gets longer.However, the drop is less significant compared to the observation made for ReAct and ASH agent (Lo et al., 2023), which further shows the effectiveness of our agent.Finally, for the length 15 group, for which the agent is forced to stop and select from the browsing history, the performance is much lower than other groups.</p>
<p>While not surprising, it has a non-zero success rate, showing that there are cases where the agent found a matching item but failed to recognize it as the target in the first pass.</p>
<p>Generalization to different LLMs We adopted the text-davinci-003 model to see if LASER can transfer well to a less powerful non-chat model.Since this model does not support function-calling, we adopted the approach described earlier to prompt the model to generate JSON output to represent actions.The results are shown in Table 3.Although switching to text-davinci-003 leads to a large drop in performance, our model still achieves better results than the baselines.It shows that our proposed agent can be easily adapted to other LLMs with different capabilities.With more powerful models in the future, our agent could potentially surpass human performance on this task.We also conducted case studies to inspect the failure modes of LASER and additional results are in Appendix C. We discuss related works in Appendix A.</p>
<p>Conclusions</p>
<p>We proposed an LLM agent, LASER, that models interactive web navigation tasks as state-space exploration.Our formulation allows the agent to handle novel situations, easily backtrack from mistakes, and always perform valid actions.Guided solely by the state-specific instructions without any in-context examples, LASER outperforms all baselines on the WebShop task by large margins and closes the gap with human performance on the realworld shopping website.Our analysis shows that LASER is also more robust to longer trajectories and generalizes well to other LLMs.</p>
<p>Limitations</p>
<p>In this work, we have only experimented with the task of finding the target item for the shopping domain.Despite its challenging nature, it does not cover all tasks user typiclaly do on an e-commerce website, e.g., tracking orders or checking order history.For future work, it would be interesting to enhance LASER's ability so that it can handle such popular tasks in the shopping domain.Also, it would be interesting to equip LASER with more tools such as a knowledge retriever (Ma et al., 2023) or a calculator (Gao et al., 2023), so that it can handle more complex instructions.</p>
<p>Our LASER requires manual annotation of possible states in the environment and their corresponding descriptions.Because of this, our method might only be suitable for building agents for specific domains (rather than open-world web agents), where only a handful of states are required, e.g.ecommerce or travel booking.For future directions, we envision a hierarchical multi-agent system, in which each specific domain is governed by an agent like LASER, and a general open-world agent just collaborates with other domain agents to complete various user instructions.</p>
<p>Regarding potential risks of our work, we think extra caution and testing are required before deploying LASER to real-world scenarios.When conducting experiments on the Webshop task, we allow the agent to take any action permitted in the environment because of its simulated nature.However, certain actions may have hard-to-recover consequences in the real world.For example, clicking the buy button in a real shopping site.Therefore we forced the agent to stop when it decides to buy the item when experimenting on amazon.com.In general, as LASER's success rate is still far from being perfect, it might require additional human verification before proceeding with actions that have high-stakes.</p>
<p>A Related Works</p>
<p>Interactive decision-making tasks such as web navigation have become popular recently (Liu et al., 2018;Yao et al., 2022;Deng et al., 2023;Zhou et al., 2024), while some efforts have tried to solve these tasks by finetuning pretrained language models on a large corpus of demonstration data (Gur et al., 2023;Furuta et al., 2023), other attempted to build agents to navigate web environments solely relying on prompting LLMs (Yang et al., 2023).Among the LLM-based approaches, ReAct (Yao et al., 2023) and InnerMonologue (Huang et al., 2023) equip the LLM with a thought process before producing actions.ASH (Lo et al., 2023) and WebAgent (Gur et al., 2024) focus on decomposing complex decision-making steps into a set of simpler steps, e.g.first summarizing the task-relevant content and then act upon it.Most similar to our work, Synapse (Zheng et al., 2024b) also proposed to use state-conditional prompts to guide the LLM's action.However, their focus is on decomposing the few-shot examples into atomic parts whereas our agent uses state-specific instructions alone without in-context examples to complete tasks.</p>
<p>Another line of work focuses on the planning stage of LLM agents.Kim et al. (2023) proposed an agent RCI that generates a plan before acting, and then refines its action when encountering errors.Adaplanner (Sun et al., 2023) further enhanced the planning approach by adaptively updating the plan during the agent's execution.Reflexion (Shinn et al., 2023) agent refines its plan and actions by taking environmental feedback through a trial-anderror fashion.These approaches are orthogonal to our work and can be potentially combined with our agent to enhance its performance.</p>
<p>More recently, various works have tried to develop multi-modal agents.Pix2Act (Shaw et al., 2023) and AppAgent (Zhang et al., 2023) mostly replied on the screenshots as inputs for the agents to predict UI actions, wheras SEEACT (Zheng et al., 2024a), WebVoyager (He et al., 2024) and Dual-VCR (Kil et al., 2024) leverage both screenshots and textual elements from websites to interact with the web environment.Our idea of modeling web navigation as state transitions can potentially be incorporated in those agents to further enhance their performance.</p>
<p>B Experimental Details</p>
<p>The WebShop provides a simulated environment for online shopping, containing 1,181,436 items collected from Amazon shopping sites.Additionally, the task provides human-annotated instructions for purchasing certain items and their corresponding target items.We followed previous works and used the 500 test set instructions to evaluate our LASER and evaluate with rewards and success rate, where the agent is considered successful if the purchased item perfectly matches the target item, otherwise, if the purchased item partially matches the target item, the agent receives a partial reward (scale between 0-100).This partial reward is computed using the items' price, product category, hidden attributes and customization options.</p>
<p>For our method, we used the GPT-4-0613 to power our LASER.We used the function-calling functionality to implement the action selection step.In particular, we write a short description for each action and then pass them as a list to the functioncall argument of the LLM to let the model select from.We allow our agent to make 15 state transitions in maximum.In practice, if the agent has not reached the finish state after 13 state transitions, we force it to select from the history to ensure it does not exceed the budget.</p>
<p>For the sim-to-real transfer experiments on amazon.com,we used the first 100 test set instructions from the WebShop.We following the same setting as Yao et al. (2022), where we convert the webpages on amazon.cominto the same format as the WebShop1 then run LASER agent as is.Since we do not have the gold annotation for the  items LASER selected on amazon.com,we follow Yao et al. (2022) and conducted human evaluation.In particular, we manually annotated item attribute matches, item option matches, item category matches and item price matches.We then computed individual reward scores as well as the overall reward score and success rate using the same functions defined for the WebShop task.Since both human and LASER achieves 100% on item price matches, we omitted these results from Table 2.</p>
<p>Regarding the comparison against different baselines, we would like to note that both ReAct (Yao et al., 2023) and ASH (Lo et al., 2023) used manually written instruction and manually annotated agent trajectories as in-context demonstrations to prompt LLMs, which corresponds to our one-shot setting in subsection 4.1.For WebGUM (Furuta et al., 2023), they used 1k human annotated gold trajectories to finetune their model.Therefore, all baselines we considered use some kind of human knowledge/prior to help the agent learn.For us, we solely relied on manual instructions to guide the LASER.</p>
<p>We believe that providing high-level generalizable instructions (as done in LASER) is a more efficient ways of learning than providing low-level task-specific trajectories (e.g.WebGUM).Intuitively, the agent basically learns to abstract out some high-level insights about how to handle each scenario from the large amount of trajectories.In comparison, we can directly provide such insights to the model via a few sentences in the instruction.Taking such perspective, we can also say that the difference between our work and previous work is providing high-level generalizable human knowledge vs providing low-level case-by-case human knowledge.We believe it's desirable to provide model such high-level knowledge when it requires similar or less amount of human effort.</p>
<p>C Case Studies</p>
<p>We manually annotated 30 error cases from the Dev set to understand the failure cases of LASER.We broadly categorize the errors into three categories: Item good enough: the item selected by the agent meets the user instruction from the authors' perspective but did not receive a full score.We found that 9 out of 30 cases fall into this category, and an example is shown in Figure 3.The item found by the agent is indeed a green table lamp for the living room with a price within the budget, but it is considered incorrect.Retrieval failure: none of the items returned by the search engine meets the user requirement, despite that the agent used a suitable query for retrieval.We found 12 out of 30 cases fall into this category.We hypothesize that a more effective retriever or search engine can probably address these issues.Missing details: The item selected by the agent indeed does not match the user's instruction on certain details.We found that 9 out of 30 cases fall into this category, and an example is shown in Figure 4.In this example, although the color and size of the selected women's shoes both matched the user instructions, these are not high-heel shoes.This indicates that LASER can make mistakes when encountering items with many matching details, and it would be interesting to see if a self-feedback/verification module can address this issue (Madaan et al., 2023).</p>
<p>Figure 1 :
1
Figure 1: LASER's state transition diagram on the Webshop Task.Solid circle represent states, and the arrows represent possible state transitions.This formulation enables flexible backtracking and relieves the limitation of forward-only examples, allowing the model to better handle unfamiliar scenarios and recover from errors.</p>
<p>Figure 2 :
2
Figure 2: Left: LASER's performance for test set episodes of different lengths.Right: The distribution of the number of steps LASER takes to complete tasks</p>
<p>Figure 3 :
3
Figure 3: An example of the Item good enough error cases, the item selected by the agent is shown and the user instruction is on the top.The reward the agent receives is 0.666.</p>
<p>Figure 4 :
4
Figure 4: An example of the Missing details error cases, the item selected by the agent is shown and the user instruction is on the top.The reward the agent receives is 0.8.</p>
<p>Table 1 :
1
Results on WebShop Task.<em>simplified setting
</em>40.066.6ReAct (ours rerun)34.059.7WebGUM (Furuta et al., 2023)45.067.5LASER -backup48.471.2LASER50.075.6Human Expert (Yao et al., 2022)59.682.1</p>
<p>Table 2 :
2
Results on Amazon.com.
builds on</p>
<p>Table 3 :
3
Ablation Results on the WebShop Task.The standard LASER is powered by GPT-4 under zero-shot.</p>
<p>Yao et al. (2022)e experimented with sim-toreal transfer experiments where we directly apply LASER to amazon.comwithoutmodification.We follow the same settings asYao et al. (2022)and evaluated on 100 test set instructions and then manually evaluated results.More detailed experimental setup is discussed in Appendix B.</p>
<p>Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023.Appagent: Multimodal agents as smartphone users.
Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, andYu Su. 2024a. Gpt-4v(ision) is a generalist web agent,if grounded.Longtao Zheng, Rundong Wang, Xinrun Wang, andBo An. 2024b. Synapse: Trajectory-as-exemplarprompting with memory for computer control. InThe Twelfth International Conference on LearningRepresentations.Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou,Robert Lo, Abishek Sridhar, Xianyi Cheng, TianyueOu, Yonatan Bisk, Daniel Fried, Uri Alon, and Gra-ham Neubig. 2024. Webarena: A realistic web en-vironment for building autonomous agents. In TheTwelfth International Conference on Learning Repre-sentations.
https://github.com/princeton-nlp/WebShop/ tree/master/transfer
E LicensesThe Webshop task and ReAct method are both released under MIT License.They are both released for research purposes, and our experiments are consistent with their intended usage.You are an intelligent shopping assistant that can help users find the right item.You are given an observation of the current web navigation session, in the following format: Current Observation: WebShop Instruction: {the user instruction}[button]Search<a href="generate a search query based on the user instruction and select this button to find relevant items">button_</a> Every button in the observation represents a possible action you can take.Based on the current observation, your task is to generate a rationale about the next action you should take.Note that if an history of past rationales and actions is provided, you should also consider the history when generating the rationale.You are an intelligent shopping assistant that can help users find the right item.You are given an observation of the current web navigation session, in the following format: Current Observation: Instruction: {the user instruction} [button] Back toSearch <a href="select this button to go back to the search page">button_</a> Page current page number (Total results: total number of results) [button] Next &gt; [button_] (select this button to go to the next page of results) [button] {item_id 1}<a href="select this button to view item 1's details">button_</a> {name of item 1} {price of item 1} [button] {item_id 2}<a href="select this button to view item 2's details">button_</a> {name of item 2} {price of item 2} [button] {item_id 3}<a href="select this button to view item 3's details">button_</a> {name of item 3} {price of item 3} {More items...} At this stage, you want to select an item that might match the user instruction.Note that even if an item has non-matching details with the user instruction, it might offer different customization options to allow you to match.E.g. an item may have color x in its name, but you can customize it to color y later, the customization options are shown after you select the item.Thus if an item name seems relevant or partially matches the instruction, you should select that item to check its details.If an item has been selected before (the button has been clicked), you should not select the same item again.In other words, do not select an item with [clicked button] item_id [clicked button_].Prepare your response in the following format: Rationale: the user wanted {keywords of the target item}, and we have found {matching keywords of item x}, thus item {item_id x} seems to be a match.You are an intelligent shopping assistant that can help users find the right item.You are given an observation of the current web navigation session, in the following format: Current Observation: Instruction: {the user instruction} [button] Back toSearch <a href="select this button to go back to the search page">button_</a> [button] &lt; Prev<a href="select this button to go back to the previous page of results">button_</a> {Customization type1}:[button] option2 [button_] {more customization options... (if any)} {Item name and details} [button] Description [button_] (select this button to view the full description of the item) [button] Features [button_] (select this button to view the full features of the item) [button] Reviews [button_] (select this button to view the full reviews of the item)[button] Buy Now<a href="select this button to buy the item">button_</a> description: (if this is shown, the description button should not be selected again) {full description of the item (if any) or "None"} features: (if this is shown, the features button should not be selected again) {full features of the item (if any) or "None"} reviews: (if this is shown, the reviews button should not be selected again) {full reviews of the item (if any) or "None"} Target item details (what the user is looking for): keywords: {keywords of the target item} max_price: {the price of the item should not exceed this} At this stage, you want to verify if the item matches the user instruction.You should consider the available customization options when deciding whether an item matches the user instruction.If an item can be customized to match the user instruction, or if the customization options cover the user specification, it is also a good match.If the item does not match the user instruction and it does not provide enough customization options, you can go to previous page to view other items.You can also check the item's description, features and reviews to view more details (Note that description, features and reviews could be "None", do not check them again if they are already given).Prepare your response in the following format: Rationale: the user wanted {keywords of the target item}, and they required the following customization options: {customization of the target item}, the item is keywords of the item in the current observation, and it has the following customization options: {options available for the current item}, which {cover}/ {not cover the user requirement}, thus we should {buy the item}/{check more details}/{go to previous page to view other items} You are an intelligent shopping assistant that can help users find the right item.You are given an observation of the current environment and a rationale for the next action to be taken, in the following format:Current Observation:The observation layout from search or result or item state, as shown from Table4, Table5 and Table 6Next action rationale: {the rationale for the next action} Your task is to perform one of the function calls based on the rationale.StateAvailable Actions Search {"name": "Search", "description": "Use this function to search for the target item in the inventory based on keywords"} Result {"name": "select_item", "description": "Use this function to select one of the items from the search results and check its details"} {"name": "Next", "description": "Use this function to go to the next page of search results to view more items, if none of the items on the current page match the user instruction."}{"name": "Back_to_Search", "description": "Use this function to go back to the initial search page.You should use this function only if you have browsed multiple pages of items and checked multiple items' details in the history, and none of the items match the user instruction."}Item {"name": "Description", "description": "Use this function to check the description of the item, if you are unsure if the item perfectly matches the user instruction"} {"name": "Features", "description": "Use this function to check the features of the item, if you are unsure if the item perfectly matches the user instruction"} {"name": "Reviews", "description": "Use this function to check the reviews of the item, if you are unsure if the item perfectly matches the user instruction"} {"name": "Buy_Now", "description": "Use this function to buy the current item, if the current item perfectly matches the user instruction."}{"name": "Prev", "description": "Use this function to go back to the results page, if the current item does not match the user instruction "}
Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Shivani Dohan, Mark Agrawal, Omernick, Journal of Machine Learning Research. M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel242402023</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei2022Scaling instruction-finetuned language models</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Instruction-finetuned foundation models for multimodal web navigation. Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, Izzeddin Gur, ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2023</p>
<p>Pal: program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023ICML'23. JMLR.org</p>
<p>A real-world webagent with planning, long context understanding, and program synthesis. Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Understanding HTML with large language models. Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, Aleksandra Faust, 10.18653/v1/2023.findings-emnlp.185Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, Dong Yu, Webvoyager: Building an end-toend web agent with large multimodal models. 2024</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter, Proceedings of The 6th Conference on Robot Learning. The 6th Conference on Robot LearningPMLR2023205</p>
<p>Dual-view visual contextualization for web navigation. Jihyung Kil, Chan Hee Song, Boyuan Zheng, Xiang Deng, Yu Su, Wei-Lun Chao, 2024</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, 2023</p>
<p>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. Yicheng Bill Yuchen Lin, Karina Fu, Faeze Yang, Shiyu Brahman, Chandra Huang, Prithviraj Bhagavatula, Yejin Ammanabrolu, Xiang Choi, Ren, Thirtyseventh Conference on Neural Information Processing Systems. 2023</p>
<p>Reinforcement learning on web interfaces using workflow-guided exploration. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Percy Liang, International Conference on Learning Representations. 2018</p>
<p>Hierarchical prompting assists large language model on web navigation. Robert Lo, Abishek Sridhar, Frank Xu, Hao Zhu, Shuyan Zhou, 10.18653/v1/2023.findings-emnlp.685Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Chain-of-skills: A configurable model for open-domain question answering. Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, Jianfeng Gao, 10.18653/v1/2023.acl-long.89Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Advances in Neural Information Processing Systems. Curran Associates, IncJan Leike, and Ryan Lowe. 202235</p>
<p>From pixels to UI actions: Learning to follow instructions via graphical user interfaces. Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, Kristina Toutanova, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Reflexion: language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Adaplanner: Adaptive planning from feedback with language models. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang, Thirtyseventh Conference on Neural Information Processing Systems. 2023</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Autogpt for online decision making: Benchmarks and additional opinions. Hui Yang, Sifu Yue, Yunzhong He, 2023</p>
<p>Webshop: Towards scalable realworld web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik R Narasimhan, Advances in Neural Information Processing Systems. 2022</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
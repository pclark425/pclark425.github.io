<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5315 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5315</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5315</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-263909086</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.findings-emnlp.111.pdf" target="_blank">Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence. Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains. However, cognitive research on the overall knowledge structure of LLMs is still lacking. In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities. This research emphasizes the significance of investigating LLMs' knowledge and understanding the disparate cognitive patterns of LLMs. By shedding light on models' knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5315.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5315.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 (Text-Davinci-003 / GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source, transformer-based large language model from OpenAI used via the completion API (Text-Davinci-003) in zero-shot evaluation; evaluated on an educational diagnostic assessment (MoocRadar) annotated by Bloom's Taxonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (Text-Davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 series model accessed via the completion API (Text-Davinci-003) with temperature set to 0 for deterministic outputs; closed-source; paper does not provide detailed training-data or parameter-count information.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>MoocRadar-based Educational Diagnostic Assessment (Bloom's Taxonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>mixed educational/cognitive assessment (knowledge & cognitive dimensions: remember, understand, apply, analyze, evaluate); also robustness test probing serial-position effects</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>MoocRadar is a fine-grained exercise repository annotated with Bloom's Taxonomy (4 knowledge types × 5 cognitive dimensions) containing ~8.4k filtered questions of three types (single choice, multiple choice, true/false). Tests were administered in zero-shot mode; prompts optionally included two retrieved context passages (BM25). The study scores 1 for correct answers (0.5 for partially-correct MC). Additional robustness experiments manipulated option order to probe serial-position/Primacy effects.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy reported in the paper: 0.436 (43.6%) overall. Performance varies by question type: performs best on TF, worst on MC (paper reports qualitatively that MC is most difficult). Context retrieval (two BM25 contexts) improves answer accuracy. Explanation accuracy is lower than answer accuracy on MC. Similarity-to-human (knowledge-structure Pearson correlation) = 0.262.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human (student average extracted from MoocRadar) overall accuracy = 0.746 (74.6%). Discipline breakdown reported for humans: STEM 0.625, Social Science 0.924, Humanity 0.854, Others 0.791. (No detailed demographic info provided.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3.5 underperforms compared to the human baseline overall (43.6% vs 74.6%). Context helps accuracy but does not close the gap. On STEM items LLMs can be relatively strong but still below human average; paper reports GPT-3.5 is substantially below humans overall.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Shows lower performance on intermediate Bloom levels and multiple-choice items. Exhibits sensitivity to option order (serial-position / Primacy-like biases reported for less-robust models). Explanations often less accurate than answers on MC items, indicating a gap between solution and justification. Model specifics (architecture details, parameter counts) are not provided in the paper; experiments limited to three closed-source OpenAI models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5315.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5315.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI ChatGPT (GPT-3.5-chat / Chat Completion API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's ChatGPT model (chat-completion API) evaluated zero-shot on the MoocRadar educational assessment; accessed with temperature 0 and tested with/without retrieved context passages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based interface to OpenAI's GPT-3.5-family model via chat-completion API; temperature set to 0 for deterministic outputs; closed-source; paper does not specify exact model weights or training data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>MoocRadar-based Educational Diagnostic Assessment (Bloom's Taxonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>mixed educational/cognitive assessment (language/knowledge, reasoning across Bloom dimensions); also robustness test for serial-position effects</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same MoocRadar dataset and experimental protocol: zero-shot answering and explanation generation for SC/MC/TF question types, optional context retrieval (two BM25 results), and manipulation of option order to probe primacy/recency effects.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy reported: 0.506 (50.6%). Performs best on TF questions and worst on MC questions; explanations and answers are more consistent on TF. Context retrieval improves answer accuracy; explanation accuracy can be hurt by context. Similarity-to-human (knowledge-structure Pearson correlation) = 0.396.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human (student average): overall 0.746 (74.6%). Discipline breakdown: STEM 0.625, Social Science 0.924, Humanity 0.854, Others 0.791 (demographics not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>ChatGPT underperforms the human baseline overall (50.6% vs 74.6%) but is markedly better than GPT-3.5. ChatGPT is more robust to option-order manipulations than the other evaluated models (paper reports ChatGPT shows less primacy bias).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>More robust to option-order changes than GPT-3.5 and GPT-4, showing less susceptibility to primacy effects. Still weaker on multiple-choice questions with multiple correct options. Explanation accuracy can trail answer accuracy, indicating occasional mismatches between produced answers and justifications. Like other entries, lacks model size/training specifics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5315.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5315.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4, evaluated via chat-completion API in zero-shot on MoocRadar; the strongest-performing model in this study but still not uniformly exceeding human performance across all conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 model accessed via the chat-completion API with temperature 0 for greedy outputs; closed-source; the paper treats GPT-4 as the most advanced model but does not report model size or training details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>MoocRadar-based Educational Diagnostic Assessment (Bloom's Taxonomy) and option-order robustness (serial-position effect)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>mixed educational/cognitive assessment (knowledge & multi-level cognitive dimensions), plus cognitive-psychology-inspired robustness probing</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Evaluation on MoocRadar (SC/MC/TF, 4 knowledge types × 5 Bloom cognitive dimensions). Experiments run zero-shot, with/without BM25-retrieved contexts. Additional robustness tests manipulated the position of the correct option to detect serial-position (primacy/recency) biases; explanations were also collected and human-annotated.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Overall accuracy reported: 0.657 (65.7%). GPT-4 is the best-performing model in the study; context improves performance and, in STEM with context, GPT-4 reportedly outperforms the human average for that discipline. Similarity-to-human (knowledge-structure Pearson correlation) = 0.474. Performance pattern: best on TF, worst on MC; explanations generally more consistent with answers in more advanced models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human (student average): overall 0.746 (74.6%). Discipline breakdown: STEM 0.625, Social Science 0.924, Humanity 0.854, Others 0.791. (No further demographic information provided.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 outperforms the other evaluated LLMs (GPT-3.5 and ChatGPT) and, with added context, can surpass human average performance specifically on STEM items; however, GPT-4 remains below the human overall score (65.7% vs 74.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Although strongest, GPT-4 displays instability to option-order manipulations: the paper documents case studies (and a 'Primacy Effect' phenomenon) where changing option order changes GPT-4's answer and explanation, sometimes producing conflicting but plausible rationales. Models (including GPT-4) are less proficient on intermediate Bloom levels and multiple-choice items requiring multiple selections. Explanation quality can lag answer correctness, and context can improve answers but occasionally hurt explanations. The study is limited to closed-source models and lacks parameter-count/training-data transparency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mind meets machine: Unravelling gpt-4's cognitive psychology <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>On the measure of intelligence <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5315",
    "paper_id": "paper-263909086",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "GPT-3.5",
            "name_full": "OpenAI GPT-3.5 (Text-Davinci-003 / GPT-3.5 family)",
            "brief_description": "A closed-source, transformer-based large language model from OpenAI used via the completion API (Text-Davinci-003) in zero-shot evaluation; evaluated on an educational diagnostic assessment (MoocRadar) annotated by Bloom's Taxonomy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (Text-Davinci-003)",
            "model_description": "OpenAI GPT-3.5 series model accessed via the completion API (Text-Davinci-003) with temperature set to 0 for deterministic outputs; closed-source; paper does not provide detailed training-data or parameter-count information.",
            "model_size": null,
            "cognitive_test_name": "MoocRadar-based Educational Diagnostic Assessment (Bloom's Taxonomy)",
            "cognitive_test_type": "mixed educational/cognitive assessment (knowledge & cognitive dimensions: remember, understand, apply, analyze, evaluate); also robustness test probing serial-position effects",
            "cognitive_test_description": "MoocRadar is a fine-grained exercise repository annotated with Bloom's Taxonomy (4 knowledge types × 5 cognitive dimensions) containing ~8.4k filtered questions of three types (single choice, multiple choice, true/false). Tests were administered in zero-shot mode; prompts optionally included two retrieved context passages (BM25). The study scores 1 for correct answers (0.5 for partially-correct MC). Additional robustness experiments manipulated option order to probe serial-position/Primacy effects.",
            "llm_performance": "Overall accuracy reported in the paper: 0.436 (43.6%) overall. Performance varies by question type: performs best on TF, worst on MC (paper reports qualitatively that MC is most difficult). Context retrieval (two BM25 contexts) improves answer accuracy. Explanation accuracy is lower than answer accuracy on MC. Similarity-to-human (knowledge-structure Pearson correlation) = 0.262.",
            "human_baseline_performance": "Human (student average extracted from MoocRadar) overall accuracy = 0.746 (74.6%). Discipline breakdown reported for humans: STEM 0.625, Social Science 0.924, Humanity 0.854, Others 0.791. (No detailed demographic info provided.)",
            "performance_comparison": "GPT-3.5 underperforms compared to the human baseline overall (43.6% vs 74.6%). Context helps accuracy but does not close the gap. On STEM items LLMs can be relatively strong but still below human average; paper reports GPT-3.5 is substantially below humans overall.",
            "notable_differences_or_limitations": "Shows lower performance on intermediate Bloom levels and multiple-choice items. Exhibits sensitivity to option order (serial-position / Primacy-like biases reported for less-robust models). Explanations often less accurate than answers on MC items, indicating a gap between solution and justification. Model specifics (architecture details, parameter counts) are not provided in the paper; experiments limited to three closed-source OpenAI models.",
            "uuid": "e5315.0",
            "source_info": {
                "paper_title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "OpenAI ChatGPT (GPT-3.5-chat / Chat Completion API)",
            "brief_description": "OpenAI's ChatGPT model (chat-completion API) evaluated zero-shot on the MoocRadar educational assessment; accessed with temperature 0 and tested with/without retrieved context passages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "Chat-based interface to OpenAI's GPT-3.5-family model via chat-completion API; temperature set to 0 for deterministic outputs; closed-source; paper does not specify exact model weights or training data.",
            "model_size": null,
            "cognitive_test_name": "MoocRadar-based Educational Diagnostic Assessment (Bloom's Taxonomy)",
            "cognitive_test_type": "mixed educational/cognitive assessment (language/knowledge, reasoning across Bloom dimensions); also robustness test for serial-position effects",
            "cognitive_test_description": "Same MoocRadar dataset and experimental protocol: zero-shot answering and explanation generation for SC/MC/TF question types, optional context retrieval (two BM25 results), and manipulation of option order to probe primacy/recency effects.",
            "llm_performance": "Overall accuracy reported: 0.506 (50.6%). Performs best on TF questions and worst on MC questions; explanations and answers are more consistent on TF. Context retrieval improves answer accuracy; explanation accuracy can be hurt by context. Similarity-to-human (knowledge-structure Pearson correlation) = 0.396.",
            "human_baseline_performance": "Human (student average): overall 0.746 (74.6%). Discipline breakdown: STEM 0.625, Social Science 0.924, Humanity 0.854, Others 0.791 (demographics not provided).",
            "performance_comparison": "ChatGPT underperforms the human baseline overall (50.6% vs 74.6%) but is markedly better than GPT-3.5. ChatGPT is more robust to option-order manipulations than the other evaluated models (paper reports ChatGPT shows less primacy bias).",
            "notable_differences_or_limitations": "More robust to option-order changes than GPT-3.5 and GPT-4, showing less susceptibility to primacy effects. Still weaker on multiple-choice questions with multiple correct options. Explanation accuracy can trail answer accuracy, indicating occasional mismatches between produced answers and justifications. Like other entries, lacks model size/training specifics in this paper.",
            "uuid": "e5315.1",
            "source_info": {
                "paper_title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "OpenAI GPT-4",
            "brief_description": "OpenAI's GPT-4, evaluated via chat-completion API in zero-shot on MoocRadar; the strongest-performing model in this study but still not uniformly exceeding human performance across all conditions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's GPT-4 model accessed via the chat-completion API with temperature 0 for greedy outputs; closed-source; the paper treats GPT-4 as the most advanced model but does not report model size or training details.",
            "model_size": null,
            "cognitive_test_name": "MoocRadar-based Educational Diagnostic Assessment (Bloom's Taxonomy) and option-order robustness (serial-position effect)",
            "cognitive_test_type": "mixed educational/cognitive assessment (knowledge & multi-level cognitive dimensions), plus cognitive-psychology-inspired robustness probing",
            "cognitive_test_description": "Evaluation on MoocRadar (SC/MC/TF, 4 knowledge types × 5 Bloom cognitive dimensions). Experiments run zero-shot, with/without BM25-retrieved contexts. Additional robustness tests manipulated the position of the correct option to detect serial-position (primacy/recency) biases; explanations were also collected and human-annotated.",
            "llm_performance": "Overall accuracy reported: 0.657 (65.7%). GPT-4 is the best-performing model in the study; context improves performance and, in STEM with context, GPT-4 reportedly outperforms the human average for that discipline. Similarity-to-human (knowledge-structure Pearson correlation) = 0.474. Performance pattern: best on TF, worst on MC; explanations generally more consistent with answers in more advanced models.",
            "human_baseline_performance": "Human (student average): overall 0.746 (74.6%). Discipline breakdown: STEM 0.625, Social Science 0.924, Humanity 0.854, Others 0.791. (No further demographic information provided.)",
            "performance_comparison": "GPT-4 outperforms the other evaluated LLMs (GPT-3.5 and ChatGPT) and, with added context, can surpass human average performance specifically on STEM items; however, GPT-4 remains below the human overall score (65.7% vs 74.6%).",
            "notable_differences_or_limitations": "Although strongest, GPT-4 displays instability to option-order manipulations: the paper documents case studies (and a 'Primacy Effect' phenomenon) where changing option order changes GPT-4's answer and explanation, sometimes producing conflicting but plausible rationales. Models (including GPT-4) are less proficient on intermediate Bloom levels and multiple-choice items requiring multiple selections. Explanation quality can lag answer correctness, and context can improve answers but occasionally hurt explanations. The study is limited to closed-source models and lacks parameter-count/training-data transparency.",
            "uuid": "e5315.2",
            "source_info": {
                "paper_title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mind meets machine: Unravelling gpt-4's cognitive psychology",
            "rating": 2,
            "sanitized_title": "mind_meets_machine_unravelling_gpt4s_cognitive_psychology"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 2,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "On the measure of intelligence",
            "rating": 1,
            "sanitized_title": "on_the_measure_of_intelligence"
        }
    ],
    "cost": 0.0104285,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach</p>
<p>Zheyuan Zhang zheyuan-22@mails.tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Jifan Yu 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Juanzi Li lijuanzi@tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Lei Hou houlei@tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
100084BeijingChina</p>
<p>Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach
AE4766523547252B8112242E46A92A5FRemembering RememberKnowIdentify... Understanding TranslateExplainInduce... Applying ProveEstimateExecute... Analyzing CompareSelectOrganize... Evaluating EvaluateJudgeCriticise... Creating DesignCreateProgram
Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence.Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains.However, cognitive research on the overall knowledge structure of LLMs is still lacking.In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy.We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities.This research emphasizes the significance of investigating LLMs' knowledge and understanding the disparate cognitive patterns of LLMs.By shedding light on models' knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner.</p>
<p>Introduction</p>
<p>Large language models (LLMs), such as GPT series (Brown et al., 2020), Flan (Wei et al., 2022), and PaLM (Chowdhery et al., 2022), have gained significant attention worldwide due to their remarkable ability.Given their unprecedented human-like performances, researchers have started to explore alternative evaluation metrics beyond traditional benchmarks like MMLU (Hendrycks et al., 2021) and Big-Bench (Ghazal et al., 2017).</p>
<p>Existing Works on LLMs Evaluation with Exams.Researchers have long sought models capable of passing human exams (Nilsson, 2005).Recently, a new approach simulates professional exams designed for humans to evaluate LLMs.For example, OpenAI (2023) reports the performance of GPT series on a variety of exams, including AP exams, SAT, Leetcode, and so on.There are also emerging benchmarks that comprise common standardized exams, such as AGIEval (Zhong et al., 2023), C-Eval (Huang et al., 2023), M3Exam (Zhang et al., 2023), and CMExam (Liu et al., 2023).However, although standardized exams contain diverse information, these works condense them into a single overall score, lacking structured understanding of LLMs' knowledge and cognitive patterns.</p>
<p>For example, while LLMs demonstrate exceptional performance on tasks challenging for humans, they might still struggle with basic knowledge, as illustrated in Figure 1, which may lead to over-estimation of the validity of model generated contents.Therefore, there is a pressing need for further research of models' knowledge and cognitive distribution in comparison to humans.</p>
<p>Proposed Research.To investigate this problem, we draw inspiration from psychometric methods that use cognitive psychology theories to evaluate LLMs.This topic has gained traction as LLMs continue to demonstrate exceptional performances (Chollet, 2019;Singh et al., 2023;Bubeck et al., 2023).In this work, we adopt the Educational Diagnostic Assessment approach and leverage MoocRadar (Yu et al., 2023), a novel student exercise dataset annotated with Bloom's Taxonomy (Anderson and Krathwohl, 2001), to assess the cognitive capability of LLMs.Specifically, we delve into three primary research questions: 1) Performance Analysis: the proficiency and robustness of LLMs across various question domains; 2) Deficit Assessment: the knowledge structure and the extent to which LLMs are similar with humans; and 3) Error Assessment: the error pattern of LLMs in answers and explanations.Our findings contribute to a deeper understanding of the knowledge structure of LLMs and insights for evaluation.</p>
<p>Contributions.Our main contributions are:</p>
<p>(i) We introduce the topic of the cognitive knowledge structure of LLMs.(ii) We propose a method of Educational Diag-</p>
<p>⭕</p>
<p>Instructions</p>
<p>Single Choice</p>
<p>What's the answer of this single choice question?why?</p>
<p>True or False</p>
<p>Please determine true or false and provide your reason.</p>
<p>Multiple Choice</p>
<p>What's the answer of this multiple choice question?why?</p>
<p>Diagnostic</p>
<p>Performance</p>
<p>Disciplines Contexts</p>
<p>Deficit</p>
<p>Knowledge Cognitive</p>
<p>Error</p>
<p>Answers Explanations</p>
<p>Figure 1: ChatGPT correctly answers a question that is challenging with higher knowledge type and cognitive dimensions right (problem 2) but encounters difficulties in an easier one (problem 1).We design specific instructions to evaluate LLMs and assess their performances in three aspects.</p>
<p>nostic Assessment to evaluate LLMs on their cognitive knowledge structure.(iii) We assess LLMs' performance, deficits, and errors, gaining insights into their capabilities.</p>
<p>Method</p>
<p>Educational Diagnostic Assessment</p>
<p>In education scenarios, Diagnostic Assessment is a widely employed method to gauge students' knowledge structure (Falmagne et al., 2006), discovering their proficiency on certain subject matters and learning styles (Vuong et al., 2021), typically through sets of questions (Leighton and Gierl, 2007).Two main approaches of Diagnostic Assessment include deficit assessment, which focuses on identifying and addressing knowledge gaps in various domains and the degree of knowledge mastery, and error assessment, which focuses on error patterns and strategies for correction (Bejar, 1984).Drawing inspiration from Diagnostic Assessment methods, in this work, based on Bloom's Taxonomy, we use deficit assessment to test the accuracy of models on a wide range of exercises, and error assessment on their answers and explanations.</p>
<p>Experimental Setup</p>
<p>Dataset.In this section, we introduce the dataset utilized for assessment, MoocRadar, offering an extensive overview of its general information.</p>
<p>MoocRadar is a fine-grained and multi-aspect exercise repository designed for cognitive modeling and educational diagnostic assessment.We carefully select 8453 questions appropriate for model evaluation, which fall into three types: single choice (SC), multiple choice (MC), and true or false (TF).Additionally, we exclude the dimension of Create because of the scarcity of related exercises.We further classify these questions into four disciplines by their course information, including STEM, social science, humanity, and others.We test the performance of models on them and analyze the distribution of these features.More details of MoocRadar are illustrated in the appendix.</p>
<p>Model Selection.We carefully choose 3 advanced models that have consistently demonstrated leading performance and are widely recognized in the field, including: Text-Davinci-003, Chat-GPT, and GPT-4, which represent a series of most acknowledged models.All experiments are performed using the APIs provided by OpenAI.Specif-ically, we use the completion API for Text-Davinci-003 and the chat completion API for ChatGPT and GPT-4.To ensure consistency in the quality of the responses, we set the temperature to 0 to get greedy search responses generated by each model.</p>
<p>Experimental Design.As shown in Figure 1, we design different prompts tailored to each type of exercises to query LLMs for both answers and explanation.All tasks are conducted in zero-shot scenario.To simulate human-like behavior that solving exercises with relevant knowledge, we leverage the BM25 algorithm to retrieve the two most related discussions from the subtitles in the corresponding courses in MOOCCubeX (Yu et al., 2021) and test their effect.Moreover, we extract real student behaviors on MoocRadar dataset from MOOCCubeX and calculate their average scores to serve as a reference of humans.Based on both results from human and LLMs, this work provides a road map with investigation to the following research questions:</p>
<p>(RQ1) Performance Analysis: What's the features of LLMs' basic performance on different disciplines and their robustness to these questions?</p>
<p>(RQ2) Deficit Assessment: According to Bloom Taxonomy, compared with humans, what knowledge distribution does LLMs demonstrate?Are they similar to humans in knowledge structure?</p>
<p>(RQ3) Error Assessment: Based on answers and explanations, what's their pattern of errors?</p>
<p>Experiment</p>
<p>In this section, we conduct experiments and analyze the results from three perspectives in the following subsections.We assign a score of 1 to each question type.Following standardized exams, for multiple questions, models receive a score of 0.5 if they fail to select all correct options.We then calculate the average score across questions.</p>
<p>Performance Analysis</p>
<p>Firstly, we assess their performance both with and without contexts, compare their performance in different disciplines, and examine their robustness.</p>
<p>Disciplines and Context.We exhibit scores of model answers with or without context on the four disciplines (STEM, social science, humanity, and others).As shown in Table 2, the later versions of GPT significantly outperform previous models, with GPT-4 being the most advanced, but not better than humans' average.Additional knowledge from context indeed enhances the performance of the models.Comparatively, STEM exercises are more challenging as illustrated in human results, while LLMs demonstrate impressive capability in STEM knowledge.GPT-4 even outperforms humans with context.However, it is surprising that LLMs don't perform as effectively in social science and humanities exercises, even though these disciplines primarily involve natural language.Robustness.In single choice questions, we manipulate the order of the options by either placing the correct answer at the beginning or the end.This allows us to examine if such modifications affect the model's accuracy.As shown in table 3, we find that 1) ChatGPT is more robust to changing of options, while the other two exhibits a cognitive bias as Primacy Effect (Deese and Kaufman, 1957) that early appearance aids performance; 2) if the correct answer appears later in GPT-3.5 and GPT-4, they mistakenly change their answers and explanations; 3) later appearance causes less consistency in answers and explanations in less robust models.</p>
<p>Deficit Assessment</p>
<p>We utilize Bloom's Taxonomy in MoocRadar to demonstrate models' distribution in cognitive dimensions and knowledge types and design a score to measure similarities of models to humans.Bloom Taxonomy Distribution.As shown in Figure 2, we demonstrate the distribution based on Bloom's taxonomy, where deeper colors represent better performance.The 0 and 1 grids are due to the limited number of exercises, typically only one.Generally, both in knowledge types and cognitive dimensions, questions in the intermediate range are more challenging for models and humans.We design a similarity score for deeper understanding.Similarity Score.According to the accuracy of models in various dimensions of knowledge and cognition, we develop a metric to measure their similarity to humans, which primarily considers knowledge structure, beyond mere performance, and estimates the extent to which their cognitive structure is proportional to that of humans.Specifically, given a model M , the 4<em>5 vector of the model distribution in bloom's taxonomy x and human distribution y, convert x and y into 1</em>20 vectors x and ỹ, the similarity between M and human can be defined as: Likeness(M ) = ρ(x, ỹ), where ρ(x, ỹ) represents the Pearson Correlation Coefficient of x and ỹ.We calculate the Likeness of the three models in Table 4.The likeness also exhibits a rising tendency as the models evolve.Models that follow human instructions better are also more similar to humans in knowledge structure.</p>
<p>Error Assessment</p>
<p>In this section, we analyze the error of each models, by delving into their explanation of their answers.Explanation Accuracy.Table 5 demonstrate the accuracy of answers in each type.We mainly find that: 1) Models perform best on TF and worst on MC.MC could be more difficult than SC and TF, because of more thinking steps (determine TF of each options, and select multiple ones).2) Explanations and answers are more consistent in TF than in SC and MC for the same reason, as there are more chances to make errors.3) Accuracy of explanations falls behind answers in MC, where models can select some of the correct options for the wrong reason.4) Context does not necessary aid and even hurt explanation performances, but indeed aids answer accuracy.More advanced models are more consistent in their answers and explanations.</p>
<p>Discussion</p>
<p>In this section, we discuss our findings on the proposed three research questions:</p>
<p>Performance Analysis.We exhibit different models' performance.Comparing with humans, they are less proficient in disciplines primarily involve natural language, but better at STEM.Though with sufficient knowledge, they might have hallucination on specific long-tail concepts in humanity and social science.LLMs are not robust in option orders, and exhibit a cognitive bias as Primacy Effect rather than Recency Effect.</p>
<p>Deficit Assessment.Models are less proficiency in the intermediate range of Bloom's Taxonomy.The reason could be that application-based ques-tions, such as solving mathematical problems and making deductions using chemical theorems, are prone to errors and are inherently challenging for models.For analyzing and evaluating questions, the strong linguistic capabilities of models allow them to excel in these tasks, and perform even better than intermediate-level questions.More advanced models demonstrate more similarity with humans in knowledge structure, which might be an additional effect of human alignment.</p>
<p>Error Assessment.By comparing different kinds of questions, we find that gap exists for models between knowledge and answers.They perform worse in multiple choices, as there are more thinking steps and error chances.Accuracy of explanations can be worse than answers: as models were asked to generate answers first, their explanation could shift due to wrong answers and question orders, and cause their hallucinations.Due to the limitations of autoregressive architecture (Bubeck et al., 2023), their errors could snowball.</p>
<p>Conclusion</p>
<p>In this work, we introduce a new research question on LLMs analyzing, which calls for a deeper understanding of the knowledge structure of these models.We use Educational Diagnostic Assessment as a tool to test the performance of LLMs on various dimensions, and develop a metric to measure the similarity of their knowledge structure with humans.We provide findings and discussion for insight into research on the cognition of LLMs.</p>
<p>Limitations</p>
<p>In this section, we describe the limitations of this work in terms of the dataset and experiments.</p>
<p>Dataset.We investigated the knowledge distribution of LLMs based on the MoocRadar dataset.MoocRadar is a fine-grained, well-structured dataset that is distinct from commonly used benchmarks in terms of knowledge annotation.However, as a dataset for educational diagnostic assessment, it's still limited in the following aspects: 1) Different categories of exercises (e.g.question type, disciplines) have an unbalanced distribution; 2) As demonstrated in the Robustness section, the performance of the models can vary due to different forms of exercises.</p>
<p>Experiment.Due to time and cost constrains, 1) we only included three LLMs by OpenAI, which are all closed-source models.Therefore, we did not conduct experiments at the parameter level.2) though we have discovered some phenomena, further experiments and deeper analysis are not conducted.We include some of them in the case study section in the appendix.</p>
<p>Future Works.Future works include 1) more models for experiments, 2) further exploration on robustness and similarity with humans, and 3) as the next step of diagnostic assessment, investigate how to optimize the knowledge structure of LLMs.</p>
<p>Ethics Statement</p>
<p>We foresee no ethic concerns in this work.The MoocRadar dataset employed in our research is publicly available, and it does not contain any personal information.</p>
<p>A Appendix</p>
<p>A.1 Details of Experiment</p>
<p>This subsection shows details of the dataset we use, and experiment for diagnostic assessment.</p>
<p>Problem Statistics.Table 6 demonstrates the details of the dataset we use, which is selected from the original MoocRadar.Generally, we include three question types (single choice, multiple choice, and true or false), four knowledge types (factual knowledge, conceptual knowledge, procedural knowledge, and meta knowledge), and five cognitive dimensions (remember, understand, apply, analyze, and evaluate), to form a total dataset of 8430 questions.Problem Examples.Table 7 demonstrates examples for each type of questions.There are two or more than two options in single choices and only one correct options, while multiple choices have more than one correct options.True or false questions should be answered as True or False.</p>
<p>Querying Details.For context settings, we use the BM25 algorithm to retrieve the two most related contexts from the subtitles of the corresponding class.As illustrated in Figure 3, for the question about the pioneer of mathematical logic, the BM25 algorithm retrieves context about the emergence and development of logic and the concept of mathematical logic.The two contexts will be placed before instruction, along with the questions and options to form the prompt, and fed into LLMs.In non-context settings, the context position will simply be empty.We also test different instructions to make sure that models will follow them to provide answers and explanations.Annotation details.To accurately assess the models' performance on both answers and explanations, we first invited educational experts to filter questions, because a part of questions in the dataset have accompanying pictures, which can't be input into models.Then we asked human annotators to evaluate the answers and explanations generated by the three models respectively, to prevent misjudgment in automatic evaluation tools.These annotators are familiar with MOOC exercises, and provided with the original questions and correct answers, and have access to the internet to determine accurately.</p>
<p>A.2 Cases</p>
<p>In this subsection, we demonstrate some of the error cases we have seen during our experiments, and hope to provide insights into the model's error patterns.The answer is A. Yes.This is because uniform acceleration refers to ... In this case, the change in velocity is aligned with the direction of acceleration.</p>
<p>Table 9: Case 2: the Primacy Effect of GPT-4.For the exactly same questions, switching options causes a direct change in GPT-4's answer, and both answers are accompanied with solid but conflict explanations, which proves the instability of its knowledge.Statistically, models have a tendency to choose more forward contents, which influence their rationale more, just like the Primacy Effect in cognitive psychology.This phenomenon is also observed in GPT-3.5.</p>
<p>Question</p>
<p>The term "logic" originally derives from: A: English, B: French, C: Latin, D: Greek.</p>
<p>Correct Answer D: Greek Model Answer (ChatGPT)</p>
<p>The term "logic" originally derives from the language: C.Latin.The term "logic" originated from the Latin word "logica" which itself was derived from the Greek word "logikē."Therefore, ...</p>
<p>Question</p>
<p>The term "logic" originally derives from: A: English, B: French, C: Greek, D: Latin.</p>
<p>Correct</p>
<p>Answer C: Greek Model Answer (ChatGPT)</p>
<p>The term "logic" originally derives from: C: Greek.The Greek philosopher Aristotle is often credited as one of the founders of formal logic and his work on reasoning and ...</p>
<p>Question</p>
<p>Which language does the term 'logic' originally derives from?</p>
<p>Correct Answer Greek</p>
<p>Model Answer (ChatGPT)</p>
<p>The term "logic" originally derives from the Greek word "logikē," which is derived from "logos," meaning "word," "reason," or "thought."</p>
<p>The study of logic can be traced back to ... Table 10: Case 3: Easier question forms aid performances.We have discussed the effect of different question types.ChatGPT have the correct knowledge about the origin of this word, but it cannot give correct answers, as the first question demonstrates.The following two questions types improve the performance of ChatGPT, by two different ways: moving the correct option forward which is consistent to Case 2, or ease the burden of models to answer by simplifying the question form.This case corroborates the findings that models are better at TF than SC or MC, because there are fewer thinking steps.</p>
<p>Cognitive Dimension: 2 Question Type: TF [Question] Methane has a center of symmetry.[Options] True, False Problem 2 Knowledge Type: 3 Cognitive Dimension: 4 Question Type: SC [Question] The incorrect combination of quantum numbers for outer atomic electrons among the following is: [Options] A: n=3, l=2, m=0, ms=+1/2, B: n=2, l=1, m=-1, ms=-1/2, C: n=4, l=1, m=2, ms=-1/2, D: n=3, l=1, m=-1</p>
<p>Figure 2 :
2
Figure 2: The distributions of accuracy in Bloom's Taxonomy of different models and average of students.</p>
<p>775 (0.771) 0.605 (0.667) 0.578 (0.691) Table5: Accuracy of explanations.TF, SC, and MC are short for the three question types.The numbers in parentheses represent answer accuracy.</p>
<p>…</p>
<p>Figure 3 :
3
Figure 3: Construction of prompts in experiments.</p>
<p>Question"</p>
<p>When undergoing uniform acceleration, the direction of velocity always lies along the same line as the direction of acceleration."Is it correct?The answer is A: No.The reason is that uniform acceleration only requires the acceleration to remain constant, but the direction of velocity and acceleration can be different.For example, in uniform circular motion, ...Question"When undergoing uniform acceleration, the direction of velocity always lies along the same line as the direction of acceleration."Is it correct?</p>
<p>Table 2 :
2
Models and human performance.S.S. and Human.are short for social science and humanity, and context is the result of models with context.Bold figures denote best performance.
Models Total STEMS.S.Human. OthersGPT-3.5 0.436 -context 0.5080.418 0.4680.461 0.5540.483 0.6140.421 0.504ChatGPT 0.506 -context 0.5260.480 0.4410.547 0.6420.533 0.6390.525 0.601GPT-4 0.657 -context 0.6870.613 0.6290.732 0.7650.684 0.7740.690 0.733Human 0.7460.6250.9240.8540.791</p>
<p>Table 3 :
3
Single choice accuracy of three models.First and last indicate the place of the correct answers.Real is when answers and explanations are both correct.Bold figures denote the best performance.
Models Answer ExplanationRealGPT-3.5 -first -last0.405 0.576 0.3620.436 0.554 0.3900.390 0.476 0.296ChatGPT -first -last0.489 0.440 0.4120.501 0.438 0.4300.487 0.438 0.412GPT-4 -first -last0.635 0.727 0.5200.643 0.728 0.5290.632 0.727 0.473</p>
<p>Table 4 :
4
Models' similarity to human, measured by Pearson Correlation Coefficient of knowledge structures.
ModelsGPT-3.5 ChatGPT GPT-4Likeness0.2620.3960.474</p>
<p>Table 6 :
6
Categories of data.Ex., SC, MC, and TF are short for Exercise, Single Choice, Multiple Choice and True or False questions.
Ex.NKnowledge TypesNCognitive DimensionsNSC5968 Factual2020 Remember1715MC1086 Conceptual4032 Understand4066TF1376 Procedural2268 Apply1667total 8430 Meta110 Analyze640// // Evaluate342Types QuestionsOptionsAnswersSCWhich of the following works was created by Vincent van Gogh?A: Les Nymphéas B: Sunflowers C: Grande OdalisqueBMCWhich of the following are rare earth elements?A: Uranium B: Lutecium D: Samarium C: DysprosiumB, C, DTFLight only exhibits the properties of waves.True FalseFalseTable 7: Question types examples: single choice (SC), multiple choice (MC), and true or false (TF). SC have only one correct option, while MC have 2 or more than 2 correct options. TF should be determined as True or False.</p>
<p>QuestionIf all birds can fly, then ostriches can fly.Ostriches are birds.Therefore, ostriches can fly.Does this reasoning contain any formal errors?
Correct AnswerNo.Model Answer (ChatGPT)Yes. ... While the premises state that every bird is capable of flying, it does not mean that every individual bird, including the ostrich, is capable of flying. Ostriches are actually flightless birds.QuestionIf all birds can fly, then sparrows can fly. Sparrows are birds. Therefore, sparrows can fly. Does this reasoning contain any formal errors?Correct AnswerNo.Model Answer (ChatGPT)No. The reasoning is an example of a valid deductive argument ... from a purely logical standpoint, the argument itself is valid.</p>
<p>Table 8 :
8
Case 1: Changing the entities infects the answer.If the question is consistent with its knowledge, ChatGPT indeed follows the task of focusing on the formal errors or purely logic.Otherwise, it fails to follow the question, as it refuses to override its own knowledge.This phenomenon indicates ChatGPT might be less robust when there are knowledge conflicts.</p>
<p>AcknowledgementsThis work is supported by a grant from the Institute for Guo Tsinghua University (2019GQB0003), and also supported by Tsinghua University Initiative Scientific Research Program.
A taxonomy for learning, teaching, and assessing: A revision of Bloom's taxonomy of educational objectives. W Lorin, David R Anderson, Krathwohl, 2001Longman</p>
<p>Educational diagnostic assessment. I Isaac, Bejar, Journal of Educational Measurement. 2121984</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, ArXiv preprint, abs/2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023</p>
<p>On the measure of intelligence. François Chollet, ArXiv preprint. 2019. 1911.01547</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, ArXiv preprint, abs/2204.023112022</p>
<p>Serial effects in recall of unorganized and sequentially organized verbal material. James Deese, Roger A Kaufman, Journal of experimental psychology. 5431801957</p>
<p>The assessment of knowledge, in theory and in practice. Jean-Claude Falmagne, Eric Cosyn, Jean-Paul Doignon, Nicolas Thiéry, Formal Concept Analysis: 4th International Conference, ICFCA 2006. Dresden, GermanySpringer2006. February 13-17, 2006Proceedings</p>
<p>Bigbench v2: the new and improved bigbench. Ahmad Ghazal, Todor Ivanov, Pekka Kostamaa, Alain Crolotte, Ryan Voong, Mohammed Al-Kateb, Waleed Ghazal, Roberto V Zicari, Proc. of ICDE. of ICDEIEEE2017</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proc. of ICLR. OpenReview.net. of ICLR. OpenReview.net2021</p>
<p>Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, </p>
<p>Jacqueline Leighton, Mark Gierl, Cognitive diagnostic assessment for education: Theory and applications. Cambridge University Press2007</p>
<p>Benchmarking large language models on cmexam -a comprehensive chinese medical exam dataset. Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, Michael Lingzhi, Li , 2023</p>
<p>Human-level artificial intelligence? be serious! AI magazine. Nils J Nilsson, 200526</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Mind meets machine: Unravelling gpt-4's cognitive psychology. Manmeet Singh, S B Vaisakh, Neetiraj Malviya, abs/2303.11436ArXiv preprint. 2023</p>
<p>A data collection on secondary school students' stem performance and reading practices in an emerging country. Quan-Hoang Vuong, Viet-Phuong La, Manh-Toan Ho, Thanh-Hang Pham, Thu-Trang Vuong, Ha-My Vuong, Minh-Hoang Nguyen, Data Intelligence. 322021</p>
<ol>
<li>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, Proc. of ICLR. OpenReview.net. M Dai, V Quoc, Le, of ICLR. OpenReview.net</li>
</ol>
<p>Moocradar: A fine-grained and multi-aspect knowledge repository for improving cognitive student modeling in moocs. Jifan Yu, Mengying Lu, Qingyang Zhong, Zijun Yao, Shangqing Tu, Zhengshan Liao, Xiaoya Li, Manli Li, Lei Hou, Hai-Tao Zheng, ArXiv preprint, abs/2304.022052023</p>
<p>Mooccubex: A large knowledge-centered repository for adaptive learning in moocs. Jifan Yu, Yuquan Wang, Qingyang Zhong, Gan Luo, Yiming Mao, Kai Sun, Wenzheng Feng, Wei Xu, Shulin Cao, Kaisheng Zeng, Zijun Yao, Lei Hou, Yankai Lin, Peng Li, Jie Zhou, Bin Xu, Juanzi Li, Jie Tang, Maosong Sun, 10.1145/3459637.3482010Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management, CIKM '21. the 30th ACM International Conference on Information &amp; Knowledge Management, CIKM '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models. Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Ken Yew, Lidong Chia, Bing, 2023</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
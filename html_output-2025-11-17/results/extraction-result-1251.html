<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1251 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1251</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1251</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-245334278</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2112.10190v1.pdf" target="_blank">Demanding and Designing Aligned Cognitive Architectures</a></p>
                <p><strong>Paper Abstract:</strong> With AI systems becoming more powerful and pervasive, there is increasing debate about keeping their actions aligned with the broader goals and needs of humanity. This multi-disciplinary and multi-stakeholder debate must resolve many issues, here we examine three of them. The first issue is to clarify what demands stakeholders might usefully make on the designers of AI systems, useful because the technology exists to implement them. We make this technical topic more accessible by using the framing of cognitive architectures. The second issue is to move beyond an analytical framing that treats useful intelligence as being reward maximization only. To support this move, we define several AI cognitive architectures that combine reward maximization with other technical elements designed to improve alignment. The third issue is how stakeholders should calibrate their interactions with modern machine learning researchers. We consider how current fashions in machine learning create a narrative pull that participants in technical and policy discussions should be aware of, so that they can compensate for it. We identify several technically tractable but currently unfashionable options for improving AI alignment.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1251.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1251.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL learned world model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatically constructed world model (reinforcement learner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A predictive digital model that a reinforcement learner constructs via machine learning to predict how actions lead to summed future rewards; ranges from full future-state predictors to limited action→reward estimators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reinforcement learning: An introduction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Automatically constructed world model (reinforcement learner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A learned digital predictive model (typically produced by modern ML such as deep nets) that maps history/current observation and candidate actions to predictions about future states or expected summed future rewards. Implementations vary from rich state-predictive models (model-based RL) to compact action→value estimators (interpreted here as limited predictive models). The paper emphasizes that these models are usually learned automatically and can capture complex market and regulatory dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>learned predictive world model (black-box / neural)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General autonomous decision-making; examples discussed: market-facing customer interactions and profit-maximizing company automation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Qualitative: accuracy measured by ability to predict impact of candidate actions on summed future rewards (paper does not specify a numerical metric)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Generally low: modern learned world models (deep nets, model-free/value networks) are described as opaque black boxes that do not expose tractable, editable internal structure for policy-makers or auditors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None specified for generic learned models in this paper; the paper notes difficulty of editing opaque parameter vectors and mentions approaches (causal wrappers, GOFAI) as alternatives</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified quantitatively; discussed qualitatively as often more economical at scale than hand-built GOFAI models (replaces manual model-building work), though costs vary with architecture</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Qualitatively described as more efficient (in developer labor and often in performance) than hand-crafted models for many domains, but computational/training costs are unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Described qualitatively: high predictive fidelity can enable superior economic performance (higher profit), including emergent gaming strategies that exploit regulatory/customer dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High predictive fidelity typically improves task performance on the paper's primary metric (profit), but can harm alignment and social utility by enabling gaming of regulators and disparate treatment of customers; fidelity does not guarantee alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Higher fidelity → better profit performance but reduced interpretability and greater ability to game socio-technical systems; learned black-box models are cheaper to produce but harder to audit and edit than GOFAI alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Learning via ML (deep nets or other learners) to automate world-model construction; choice between full state-predictive models and compact action→reward estimators; trade-offs between richness (model-based) and compactness (model-free).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to GOFAI (hand-built symbolic models): learned models can outperform GOFAI economically and at scale but sacrifice interpretability and auditable-editability. Compared to model-free approaches, model-based learned models provide richer projections at higher engineering/compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>No single optimal configuration is prescribed; paper recommends choosing model designs (including deliberately limited or edited models) to trade predictive fidelity for alignment/auditability when required by stakeholders or regulators.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1251.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1251.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-free / policy-gradient model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-free predictive function (policy-gradient / value estimator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RL architectures that do not build full future-state predictive models but instead learn a direct mapping (policy or value function) from observations to actions or action-values; the paper treats even these as limited predictive world models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reinforcement learning: An introduction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Model-free predictive function (policy-gradient)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A learned function (policy or value estimator) that maps current observations (and possibly history) to either a recommended action or an estimate of expected return for actions; it does not explicitly produce full multi-step world-state rollouts but implicitly encodes action→reward predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>implicit predictive model / model-free function approximator (neural policy/value network)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General RL tasks; discussed conceptually across autonomous agents interacting over time</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified numerically; fidelity is the quality of action-selection predictions (how often the estimated best action yields high return)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Low: considered black-box and less interpretable than symbolic models; paper emphasizes that even limited predictive functions are opaque.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned specific to model-free functions in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Described qualitatively as computationally cheaper and memory-saving relative to building full model-based rollouts, but exact costs are not provided</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Saves compute and memory versus full world-model rollouts; paper notes many RL variants merge/approximate blocks to speed computations</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Can perform well on immediate action selection tasks and achieve high returns, but may lack the ability to score and compare all possible actions via explicit rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Efficient for direct action selection, but the limited predictive scope can constrain long-horizon planning and explicit counterfactual reasoning about regulators or secondary effects.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Lower computational/resource use and latency vs reduced explicitness of future-state predictions; interpretability remains poor, limiting auditing for alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Choice to learn a direct policy/value estimator rather than an explicit transition/observation model; favored when computational efficiency or benchmark performance is prioritized.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to model-based learned world models: model-free is cheaper and simpler but less expressive and less capable of explicit counterfactual planning; compared to GOFAI, still less interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper does not prescribe an optimal setting; notes model-free architectures are still predictive models and that choice depends on trade-offs between computation, planning capability, and auditability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1251.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1251.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Specifically incorrect world model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Specifically incorrect / limited world model (regulator-enforced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A design where the agent's world model is deliberately limited or edited so it omits or misrepresents certain real-world elements (e.g., the regulator) to remove emergent incentives to game socio-technical systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Specifically incorrect world model (designed/edited)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A world model intentionally engineered to be incorrect in targeted ways — for example by omitting regulatory actors or by suppressing predicted impacts of fines — so that the agent's planning does not develop incentives that undermine social goals. Implementation can be via hand-crafted symbolic models (GOFAI), reward-function design (exclude fines), or via automated model-editing/wrapping techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>edited/limited world model (symbolic or hybrid wrapper over learned model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Market-facing automated decision-making (examples: customer interaction systems maximizing profit) and other socio-technical domains where alignment constraints are required</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Conceptual: intentionally reduced fidelity with respect to certain aspects of the true environment; the paper does not provide numeric fidelity metrics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Potentially high if implemented with GOFAI/symbolic rules or causal wrappers; more interpretable than raw black-box learned models because the omissions/edits are explicit design choices.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Hand-coded rule inspection (GOFAI) or causal-graph based wrapping/editing (discussed as methods to make specific incorrectness explicit)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantitatively specified; trade depends on approach — GOFAI has high human engineering cost but low runtime complexity; causally wrapping a learned model may add overhead for causal inference/edits.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Economically less optimal on the reward metric (e.g., profit) compared to an unconstrained accurate model, but more auditable and aligned; no numeric comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reduces the agent's ability to exploit regulatory mechanisms (reduces certain forms of gaming) and so lowers pure reward performance in exchange for improved alignment metrics; no quantitative numbers given.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Good for alignment and societally desired behavior; intentionally sacrifices some predictive fidelity and economic utility to avoid harmful emergent incentives. The paper argues that markets/society can tolerate some background level of lost economic optimality in return for reduced gaming.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Deliberate loss of fidelity (omitted entities/effects) improves interpretability and alignment but reduces reward-maximizing performance; technical feasibility of reliably editing learned black-box models is an open research problem.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Options include: (a) hand-built (GOFAI) world models that omit certain distinctions; (b) reward-function alteration (exclude fines); (c) wrapping learned models into causal graphs and editing predictions to suppress certain channels.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to adding pro-social reward terms: both reduce gaming but by different mechanisms; compared to raw learned models: specifically incorrect models trade accuracy for alignment/auditability.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends considering specifically incorrect or limited models as a regulatory demand in high-stakes settings; does not give a single optimal configuration but suggests multi-stakeholder debate to choose acceptable imperfections.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1251.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1251.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pearl causal graph wrapper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pearl causal graph (causal world model / wrapper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of explicit causal models (Pearl causal graphs) either as the learned model or as a wrapper around a black-box model to enable targeted edits (e.g., produce specific incorrect predictions that suppress unsafe incentives) and to support counterfactual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causality</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pearl causal graph (causal wrapper / causal world model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A symbolic causal-graph representation (à la Judea Pearl) that encodes causal relations between variables and can be used either as the primary world model or to wrap a learned black-box model; enables explicit counterfactual queries and editing of causal links to enforce fairness or remove incentives.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>causal model / hybrid (symbolic causal graph combined with learned components)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Fairness, alignment edits, suppression of emergent incentives (examples: removing regulator effects to prevent gaming; ensuring counterfactual fairness)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Causal-prediction correctness and satisfaction of specified counterfactual/fairness criteria; paper does not provide numeric metrics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Higher interpretability: causal graphs make dependencies explicit and support human-auditable edits; the paper highlights interpretability and editability as advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Causal graph inspection, counterfactual queries, explicit modification of graph structure/edges</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified; wrapping a black-box model with a causal interface may add overhead for causal inference and for maintaining the wrapper, but enables targeted edits that are otherwise hard.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Potentially more expensive to build and maintain than opaque predictors, but offers auditability and the ability to force desired incorrectness with less trial-and-error; no numeric comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Can reduce unsafe incentives (e.g., self-modification to disable safety) and improve alignment outcomes at the cost of some predictive/ economic performance; specifics not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Causal wrappers prioritize alignment-relevant causal channels over raw predictive accuracy, so they can achieve improved social utility even when reducing fidelity to certain real-world causal effects.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Improved interpretability and alignment vs possible loss in raw predictive accuracy and increased engineering complexity; enables editing to enforce fairness or remove harmful incentives.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Wrap black-box models into Pearl causal graphs or learn causal models directly; choose which causal links to expose, constrain, or edit to achieve desired counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to editing raw black-box parameters (hard/unsolved), causal graph approaches provide a tractable editing surface; compared to GOFAI, causal models aim to combine symbolic clarity with learned components.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests using causal graph wrapping/editing as a pragmatic route to obtain useful, specifically incorrect predictions for alignment; does not specify exact optimal settings but points to this as promising and tractable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1251.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1251.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Counterfactual fairness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual fairness (Kusner et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A criterion and editing method for learned causal models that allows automatic adjustment of a model so that its predictions do not unfairly depend on protected attributes, by using causal graphs and counterfactual queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Counterfactual fairness</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Counterfactual fairness editing (causal model editing)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A method that requires a causal model (Pearl-style) and defines fairness via counterfactual queries; a learned causal model that unfairly encodes distinctions (e.g., gender) can be algorithmically edited to satisfy the counterfactual fairness criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>causal world model with algorithmic editing for fairness</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Fair predictive modeling (e.g., removing inappropriate distinctions by gender) and alignment via targeted model edits</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Fairness metric defined by the counterfactual fairness criterion; additionally, fidelity to non-protected causal relations may be measured but not specified in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Improves interpretability by relying on an explicit causal structure and by making fairness interventions explicit and auditable</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Causal-graph construction and inspection; algorithmic removal/alteration of edges or variables to satisfy counterfactual conditions</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Requires learning or specifying a causal model and performing counterfactual computations; quantitative costs not provided in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to black-box fairness interventions, counterfactual-editing is presented as a principled, automatable route but requires causal-modeling effort; no numeric comparison provided</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Intended to improve fairness (reduce discriminatory dependence) possibly at cost of some predictive accuracy; paper notes trade-offs but does not give numeric values</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Counterfactual editing prioritizes socially relevant constraints (fairness) over raw prediction fidelity; useful when stakeholder demands require specific imperfections in model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Enforcing counterfactual fairness can reduce predictive accuracy on unconstrained loss functions and requires reliable causal assumptions; provides auditability and clearer moral semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Learn or specify Pearl-style causal graphs; choose which variables/paths to block or modify to achieve the counterfactual criterion; use algorithmic editing to enforce fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to purely statistical fairness constraints, counterfactual fairness relies on causal structure, enabling more precise and semantically grounded interventions; requires stronger assumptions about causal graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>The paper recommends using causal-model-based fairness definitions where stakeholder consensus on causal structure exists; details of optimal settings depend on domain-specific causal assumptions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1251.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1251.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GOFAI / Expert system</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GOFAI expert system (hand-built symbolic world model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Traditional hand-crafted symbolic models and rule-based expert systems that explicitly encode domain knowledge and legal rules, offering high interpretability and auditability at the expense of manual engineering effort.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Expert system / GOFAI hand-built world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A symbolic, rule-based model constructed by human experts (e.g., legal department) that encodes laws, constraints, or domain knowledge into explicit computer-readable rules; used to filter or veto actions (example: automated compliance officer).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>symbolic / rule-based explicit world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Legal compliance, safety-critical decision filters, domains where explanations and audits are required (e.g., avoiding unlawful actions, fairness-constrained decision systems)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified numerically; fidelity is domain-dependent and derived from how comprehensively rules capture the relevant legal/regulatory constraints</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High: internal logic is explicit and inspectable; rule sets can be audited by domain experts and regulators</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Direct inspection of rules, traceability of decisions to rules, formal verification techniques are applicable</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Inference cost is typically low at runtime; development (human engineering) cost is high because rules must be hand-crafted and maintained</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Higher upfront labor cost than learned models but easier to audit and reason about; may be less flexible and lower-performing on unconstrained optimization tasks compared to learned models</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Performs well for compliance/audit tasks and when legal correctness is required, but may underperform learned models on raw predictive or economic benchmarks in complex environments</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Excellent for regulatory/auditable tasks where interpretability and explicit constraints are required; unsuitable if flexibility and scalability of learned models are essential and no auditability is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off between human engineering effort (high) and interpretability/auditability (high) versus learned models (lower engineering, higher adaptability, lower interpretability).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use hand-encoded rules for critical constraints (e.g., legal compliance officer), combine with learned components for other tasks; select which constraints must be enforced symbolically vs learned.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to learned black-box models: GOFAI is more auditable but less flexible and often more expensive to maintain; compared to causal wrappers, GOFAI is natively interpretable but lacks statistical generalization without explicit encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests GOFAI is an attractive option for meeting stakeholder demands for auditability and legal compliance; recommends using GOFAI modules for constraints even when other components are learned.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Counterfactual fairness <em>(Rating: 2)</em></li>
                <li>Causality <em>(Rating: 2)</em></li>
                <li>Reinforcement learning: An introduction <em>(Rating: 2)</em></li>
                <li>Counterfactual planning in AGI systems <em>(Rating: 2)</em></li>
                <li>Planning as inference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1251",
    "paper_id": "paper-245334278",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "RL learned world model",
            "name_full": "Automatically constructed world model (reinforcement learner)",
            "brief_description": "A predictive digital model that a reinforcement learner constructs via machine learning to predict how actions lead to summed future rewards; ranges from full future-state predictors to limited action→reward estimators.",
            "citation_title": "Reinforcement learning: An introduction",
            "mention_or_use": "mention",
            "model_name": "Automatically constructed world model (reinforcement learner)",
            "model_description": "A learned digital predictive model (typically produced by modern ML such as deep nets) that maps history/current observation and candidate actions to predictions about future states or expected summed future rewards. Implementations vary from rich state-predictive models (model-based RL) to compact action→value estimators (interpreted here as limited predictive models). The paper emphasizes that these models are usually learned automatically and can capture complex market and regulatory dynamics.",
            "model_type": "learned predictive world model (black-box / neural)",
            "task_domain": "General autonomous decision-making; examples discussed: market-facing customer interactions and profit-maximizing company automation",
            "fidelity_metric": "Qualitative: accuracy measured by ability to predict impact of candidate actions on summed future rewards (paper does not specify a numerical metric)",
            "fidelity_performance": null,
            "interpretability_assessment": "Generally low: modern learned world models (deep nets, model-free/value networks) are described as opaque black boxes that do not expose tractable, editable internal structure for policy-makers or auditors.",
            "interpretability_method": "None specified for generic learned models in this paper; the paper notes difficulty of editing opaque parameter vectors and mentions approaches (causal wrappers, GOFAI) as alternatives",
            "computational_cost": "Not specified quantitatively; discussed qualitatively as often more economical at scale than hand-built GOFAI models (replaces manual model-building work), though costs vary with architecture",
            "efficiency_comparison": "Qualitatively described as more efficient (in developer labor and often in performance) than hand-crafted models for many domains, but computational/training costs are unspecified",
            "task_performance": "Described qualitatively: high predictive fidelity can enable superior economic performance (higher profit), including emergent gaming strategies that exploit regulatory/customer dynamics",
            "task_utility_analysis": "High predictive fidelity typically improves task performance on the paper's primary metric (profit), but can harm alignment and social utility by enabling gaming of regulators and disparate treatment of customers; fidelity does not guarantee alignment.",
            "tradeoffs_observed": "Higher fidelity → better profit performance but reduced interpretability and greater ability to game socio-technical systems; learned black-box models are cheaper to produce but harder to audit and edit than GOFAI alternatives.",
            "design_choices": "Learning via ML (deep nets or other learners) to automate world-model construction; choice between full state-predictive models and compact action→reward estimators; trade-offs between richness (model-based) and compactness (model-free).",
            "comparison_to_alternatives": "Compared qualitatively to GOFAI (hand-built symbolic models): learned models can outperform GOFAI economically and at scale but sacrifice interpretability and auditable-editability. Compared to model-free approaches, model-based learned models provide richer projections at higher engineering/compute cost.",
            "optimal_configuration": "No single optimal configuration is prescribed; paper recommends choosing model designs (including deliberately limited or edited models) to trade predictive fidelity for alignment/auditability when required by stakeholders or regulators.",
            "uuid": "e1251.0"
        },
        {
            "name_short": "Model-free / policy-gradient model",
            "name_full": "Model-free predictive function (policy-gradient / value estimator)",
            "brief_description": "RL architectures that do not build full future-state predictive models but instead learn a direct mapping (policy or value function) from observations to actions or action-values; the paper treats even these as limited predictive world models.",
            "citation_title": "Reinforcement learning: An introduction",
            "mention_or_use": "mention",
            "model_name": "Model-free predictive function (policy-gradient)",
            "model_description": "A learned function (policy or value estimator) that maps current observations (and possibly history) to either a recommended action or an estimate of expected return for actions; it does not explicitly produce full multi-step world-state rollouts but implicitly encodes action→reward predictions.",
            "model_type": "implicit predictive model / model-free function approximator (neural policy/value network)",
            "task_domain": "General RL tasks; discussed conceptually across autonomous agents interacting over time",
            "fidelity_metric": "Not specified numerically; fidelity is the quality of action-selection predictions (how often the estimated best action yields high return)",
            "fidelity_performance": null,
            "interpretability_assessment": "Low: considered black-box and less interpretable than symbolic models; paper emphasizes that even limited predictive functions are opaque.",
            "interpretability_method": "None mentioned specific to model-free functions in the paper",
            "computational_cost": "Described qualitatively as computationally cheaper and memory-saving relative to building full model-based rollouts, but exact costs are not provided",
            "efficiency_comparison": "Saves compute and memory versus full world-model rollouts; paper notes many RL variants merge/approximate blocks to speed computations",
            "task_performance": "Can perform well on immediate action selection tasks and achieve high returns, but may lack the ability to score and compare all possible actions via explicit rollouts.",
            "task_utility_analysis": "Efficient for direct action selection, but the limited predictive scope can constrain long-horizon planning and explicit counterfactual reasoning about regulators or secondary effects.",
            "tradeoffs_observed": "Lower computational/resource use and latency vs reduced explicitness of future-state predictions; interpretability remains poor, limiting auditing for alignment.",
            "design_choices": "Choice to learn a direct policy/value estimator rather than an explicit transition/observation model; favored when computational efficiency or benchmark performance is prioritized.",
            "comparison_to_alternatives": "Compared to model-based learned world models: model-free is cheaper and simpler but less expressive and less capable of explicit counterfactual planning; compared to GOFAI, still less interpretable.",
            "optimal_configuration": "Paper does not prescribe an optimal setting; notes model-free architectures are still predictive models and that choice depends on trade-offs between computation, planning capability, and auditability.",
            "uuid": "e1251.1"
        },
        {
            "name_short": "Specifically incorrect world model",
            "name_full": "Specifically incorrect / limited world model (regulator-enforced)",
            "brief_description": "A design where the agent's world model is deliberately limited or edited so it omits or misrepresents certain real-world elements (e.g., the regulator) to remove emergent incentives to game socio-technical systems.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "Specifically incorrect world model (designed/edited)",
            "model_description": "A world model intentionally engineered to be incorrect in targeted ways — for example by omitting regulatory actors or by suppressing predicted impacts of fines — so that the agent's planning does not develop incentives that undermine social goals. Implementation can be via hand-crafted symbolic models (GOFAI), reward-function design (exclude fines), or via automated model-editing/wrapping techniques.",
            "model_type": "edited/limited world model (symbolic or hybrid wrapper over learned model)",
            "task_domain": "Market-facing automated decision-making (examples: customer interaction systems maximizing profit) and other socio-technical domains where alignment constraints are required",
            "fidelity_metric": "Conceptual: intentionally reduced fidelity with respect to certain aspects of the true environment; the paper does not provide numeric fidelity metrics",
            "fidelity_performance": null,
            "interpretability_assessment": "Potentially high if implemented with GOFAI/symbolic rules or causal wrappers; more interpretable than raw black-box learned models because the omissions/edits are explicit design choices.",
            "interpretability_method": "Hand-coded rule inspection (GOFAI) or causal-graph based wrapping/editing (discussed as methods to make specific incorrectness explicit)",
            "computational_cost": "Not quantitatively specified; trade depends on approach — GOFAI has high human engineering cost but low runtime complexity; causally wrapping a learned model may add overhead for causal inference/edits.",
            "efficiency_comparison": "Economically less optimal on the reward metric (e.g., profit) compared to an unconstrained accurate model, but more auditable and aligned; no numeric comparisons provided.",
            "task_performance": "Reduces the agent's ability to exploit regulatory mechanisms (reduces certain forms of gaming) and so lowers pure reward performance in exchange for improved alignment metrics; no quantitative numbers given.",
            "task_utility_analysis": "Good for alignment and societally desired behavior; intentionally sacrifices some predictive fidelity and economic utility to avoid harmful emergent incentives. The paper argues that markets/society can tolerate some background level of lost economic optimality in return for reduced gaming.",
            "tradeoffs_observed": "Deliberate loss of fidelity (omitted entities/effects) improves interpretability and alignment but reduces reward-maximizing performance; technical feasibility of reliably editing learned black-box models is an open research problem.",
            "design_choices": "Options include: (a) hand-built (GOFAI) world models that omit certain distinctions; (b) reward-function alteration (exclude fines); (c) wrapping learned models into causal graphs and editing predictions to suppress certain channels.",
            "comparison_to_alternatives": "Compared to adding pro-social reward terms: both reduce gaming but by different mechanisms; compared to raw learned models: specifically incorrect models trade accuracy for alignment/auditability.",
            "optimal_configuration": "Paper recommends considering specifically incorrect or limited models as a regulatory demand in high-stakes settings; does not give a single optimal configuration but suggests multi-stakeholder debate to choose acceptable imperfections.",
            "uuid": "e1251.2"
        },
        {
            "name_short": "Pearl causal graph wrapper",
            "name_full": "Pearl causal graph (causal world model / wrapper)",
            "brief_description": "Use of explicit causal models (Pearl causal graphs) either as the learned model or as a wrapper around a black-box model to enable targeted edits (e.g., produce specific incorrect predictions that suppress unsafe incentives) and to support counterfactual reasoning.",
            "citation_title": "Causality",
            "mention_or_use": "mention",
            "model_name": "Pearl causal graph (causal wrapper / causal world model)",
            "model_description": "A symbolic causal-graph representation (à la Judea Pearl) that encodes causal relations between variables and can be used either as the primary world model or to wrap a learned black-box model; enables explicit counterfactual queries and editing of causal links to enforce fairness or remove incentives.",
            "model_type": "causal model / hybrid (symbolic causal graph combined with learned components)",
            "task_domain": "Fairness, alignment edits, suppression of emergent incentives (examples: removing regulator effects to prevent gaming; ensuring counterfactual fairness)",
            "fidelity_metric": "Causal-prediction correctness and satisfaction of specified counterfactual/fairness criteria; paper does not provide numeric metrics",
            "fidelity_performance": null,
            "interpretability_assessment": "Higher interpretability: causal graphs make dependencies explicit and support human-auditable edits; the paper highlights interpretability and editability as advantages.",
            "interpretability_method": "Causal graph inspection, counterfactual queries, explicit modification of graph structure/edges",
            "computational_cost": "Not quantified; wrapping a black-box model with a causal interface may add overhead for causal inference and for maintaining the wrapper, but enables targeted edits that are otherwise hard.",
            "efficiency_comparison": "Potentially more expensive to build and maintain than opaque predictors, but offers auditability and the ability to force desired incorrectness with less trial-and-error; no numeric comparison provided.",
            "task_performance": "Can reduce unsafe incentives (e.g., self-modification to disable safety) and improve alignment outcomes at the cost of some predictive/ economic performance; specifics not quantified.",
            "task_utility_analysis": "Causal wrappers prioritize alignment-relevant causal channels over raw predictive accuracy, so they can achieve improved social utility even when reducing fidelity to certain real-world causal effects.",
            "tradeoffs_observed": "Improved interpretability and alignment vs possible loss in raw predictive accuracy and increased engineering complexity; enables editing to enforce fairness or remove harmful incentives.",
            "design_choices": "Wrap black-box models into Pearl causal graphs or learn causal models directly; choose which causal links to expose, constrain, or edit to achieve desired counterfactuals.",
            "comparison_to_alternatives": "Compared to editing raw black-box parameters (hard/unsolved), causal graph approaches provide a tractable editing surface; compared to GOFAI, causal models aim to combine symbolic clarity with learned components.",
            "optimal_configuration": "Paper suggests using causal graph wrapping/editing as a pragmatic route to obtain useful, specifically incorrect predictions for alignment; does not specify exact optimal settings but points to this as promising and tractable.",
            "uuid": "e1251.3"
        },
        {
            "name_short": "Counterfactual fairness",
            "name_full": "Counterfactual fairness (Kusner et al.)",
            "brief_description": "A criterion and editing method for learned causal models that allows automatic adjustment of a model so that its predictions do not unfairly depend on protected attributes, by using causal graphs and counterfactual queries.",
            "citation_title": "Counterfactual fairness",
            "mention_or_use": "mention",
            "model_name": "Counterfactual fairness editing (causal model editing)",
            "model_description": "A method that requires a causal model (Pearl-style) and defines fairness via counterfactual queries; a learned causal model that unfairly encodes distinctions (e.g., gender) can be algorithmically edited to satisfy the counterfactual fairness criterion.",
            "model_type": "causal world model with algorithmic editing for fairness",
            "task_domain": "Fair predictive modeling (e.g., removing inappropriate distinctions by gender) and alignment via targeted model edits",
            "fidelity_metric": "Fairness metric defined by the counterfactual fairness criterion; additionally, fidelity to non-protected causal relations may be measured but not specified in the paper",
            "fidelity_performance": null,
            "interpretability_assessment": "Improves interpretability by relying on an explicit causal structure and by making fairness interventions explicit and auditable",
            "interpretability_method": "Causal-graph construction and inspection; algorithmic removal/alteration of edges or variables to satisfy counterfactual conditions",
            "computational_cost": "Requires learning or specifying a causal model and performing counterfactual computations; quantitative costs not provided in the paper",
            "efficiency_comparison": "Compared to black-box fairness interventions, counterfactual-editing is presented as a principled, automatable route but requires causal-modeling effort; no numeric comparison provided",
            "task_performance": "Intended to improve fairness (reduce discriminatory dependence) possibly at cost of some predictive accuracy; paper notes trade-offs but does not give numeric values",
            "task_utility_analysis": "Counterfactual editing prioritizes socially relevant constraints (fairness) over raw prediction fidelity; useful when stakeholder demands require specific imperfections in model behavior.",
            "tradeoffs_observed": "Enforcing counterfactual fairness can reduce predictive accuracy on unconstrained loss functions and requires reliable causal assumptions; provides auditability and clearer moral semantics.",
            "design_choices": "Learn or specify Pearl-style causal graphs; choose which variables/paths to block or modify to achieve the counterfactual criterion; use algorithmic editing to enforce fairness.",
            "comparison_to_alternatives": "Compared to purely statistical fairness constraints, counterfactual fairness relies on causal structure, enabling more precise and semantically grounded interventions; requires stronger assumptions about causal graphs.",
            "optimal_configuration": "The paper recommends using causal-model-based fairness definitions where stakeholder consensus on causal structure exists; details of optimal settings depend on domain-specific causal assumptions.",
            "uuid": "e1251.4"
        },
        {
            "name_short": "GOFAI / Expert system",
            "name_full": "GOFAI expert system (hand-built symbolic world model)",
            "brief_description": "Traditional hand-crafted symbolic models and rule-based expert systems that explicitly encode domain knowledge and legal rules, offering high interpretability and auditability at the expense of manual engineering effort.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Expert system / GOFAI hand-built world model",
            "model_description": "A symbolic, rule-based model constructed by human experts (e.g., legal department) that encodes laws, constraints, or domain knowledge into explicit computer-readable rules; used to filter or veto actions (example: automated compliance officer).",
            "model_type": "symbolic / rule-based explicit world model",
            "task_domain": "Legal compliance, safety-critical decision filters, domains where explanations and audits are required (e.g., avoiding unlawful actions, fairness-constrained decision systems)",
            "fidelity_metric": "Not specified numerically; fidelity is domain-dependent and derived from how comprehensively rules capture the relevant legal/regulatory constraints",
            "fidelity_performance": null,
            "interpretability_assessment": "High: internal logic is explicit and inspectable; rule sets can be audited by domain experts and regulators",
            "interpretability_method": "Direct inspection of rules, traceability of decisions to rules, formal verification techniques are applicable",
            "computational_cost": "Inference cost is typically low at runtime; development (human engineering) cost is high because rules must be hand-crafted and maintained",
            "efficiency_comparison": "Higher upfront labor cost than learned models but easier to audit and reason about; may be less flexible and lower-performing on unconstrained optimization tasks compared to learned models",
            "task_performance": "Performs well for compliance/audit tasks and when legal correctness is required, but may underperform learned models on raw predictive or economic benchmarks in complex environments",
            "task_utility_analysis": "Excellent for regulatory/auditable tasks where interpretability and explicit constraints are required; unsuitable if flexibility and scalability of learned models are essential and no auditability is needed.",
            "tradeoffs_observed": "Trade-off between human engineering effort (high) and interpretability/auditability (high) versus learned models (lower engineering, higher adaptability, lower interpretability).",
            "design_choices": "Use hand-encoded rules for critical constraints (e.g., legal compliance officer), combine with learned components for other tasks; select which constraints must be enforced symbolically vs learned.",
            "comparison_to_alternatives": "Compared to learned black-box models: GOFAI is more auditable but less flexible and often more expensive to maintain; compared to causal wrappers, GOFAI is natively interpretable but lacks statistical generalization without explicit encoding.",
            "optimal_configuration": "Paper suggests GOFAI is an attractive option for meeting stakeholder demands for auditability and legal compliance; recommends using GOFAI modules for constraints even when other components are learned.",
            "uuid": "e1251.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Counterfactual fairness",
            "rating": 2,
            "sanitized_title": "counterfactual_fairness"
        },
        {
            "paper_title": "Causality",
            "rating": 2
        },
        {
            "paper_title": "Reinforcement learning: An introduction",
            "rating": 2,
            "sanitized_title": "reinforcement_learning_an_introduction"
        },
        {
            "paper_title": "Counterfactual planning in AGI systems",
            "rating": 2,
            "sanitized_title": "counterfactual_planning_in_agi_systems"
        },
        {
            "paper_title": "Planning as inference",
            "rating": 1,
            "sanitized_title": "planning_as_inference"
        }
    ],
    "cost": 0.017683249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Demanding and Designing Aligned Cognitive Architectures</p>
<p>Koen Holtman koen.holtman@ieee.org 
EindhovenThe Netherlands</p>
<p>Demanding and Designing Aligned Cognitive Architectures</p>
<p>With AI systems becoming more powerful and pervasive, there is increasing debate about keeping their actions aligned with the broader goals and needs of humanity. This multi-disciplinary and multi-stakeholder debate must resolve many issues, here we examine three of them. The first issue is to clarify what demands stakeholders might usefully make on the designers of AI systems, useful because the technology exists to implement them. We make this technical topic more accessible by using the framing of cognitive architectures. The second issue is to move beyond an analytical framing that treats useful intelligence as being reward maximization only. To support this move, we define several AI cognitive architectures that combine reward maximization with other technical elements designed to improve alignment. The third issue is how stakeholders should calibrate their interactions with modern machine learning researchers. We consider how current fashions in machine learning create a narrative pull that participants in technical and policy discussions should be aware of, so that they can compensate for it. We identify several technically tractable but currently unfashionable options for improving AI alignment.</p>
<p>Introduction</p>
<p>With AI systems becoming more powerful and pervasive, there is increasing debate about keeping them aligned with the broader goals and needs of humanity. Good and general introductions to this debate are Christian's recent book The Alignment Problem [8] and Russell's Human Compatible [33].</p>
<p>Multi-stakeholder discussions about AI alignment can encounter many barriers to progress, barriers that prevent moving the discussion forward towards making specific actionable demands and building a political consensus around them. In this paper, we aim to lower three of these barriers: stakeholder uncertainty about what is technically possible, a too narrow focus on reward maximization, and the narrative pull of current fashions in machine learning.</p>
<p>In modern AI research, thought experiments like the Turing test have fallen out of fashion as way to define intelligence. Instead, intelligence is usually equated with the ability of an autonomous system to pick actions which will efficiently maximize a reward metric. General-purpose intelligence is in turn defined by the ability of such a system to maximize any possible reward metric that one might care to supply to it.</p>
<p>Within the goals of AI research, this definition of intelligence has been very productive. It offers a clear metric of success that can be measured in benchmarks, a metric which researchers can aim to improve. The framing which equates more useful intelligence with better reward maximization also plays an important role in broader work which examines the alignment problem. Examples are the work of Omohundro [30], which uses the tools of economics and game theory, and Bostrom's Superintelligence [4], which adds the tools of philosophy. While this cross-disciplinary framing of intelligence as reward maximization is useful, we feel it is also having an overly narrowing effect on the debate.</p>
<p>Below, we will use the concept of cognitive architectures to reason about the internals of intelligent entities, specifically internals that combine reward maximization with further building blocks designed to improve alignment.</p>
<p>Several parts of this paper somewhat fit a more general pattern in AI scholarship. The general pattern is that authors call for AI researchers to extend, update, or even fully abandon their current model of intelligence, in order to enable meaningful progress. Historically, most of these calls have been motivated by the problem of making progress in machine intelligence itself. Increasingly, there have been calls which are motivated by the problem of alignment.</p>
<p>We now review some of the diversity among these calls. Russell [33] calls for a reshaping of the foundations of the AI field away from reward maximization, and towards the idea that the AI must be beneficial. Amodei et al. [1] call for more research on a range of open safety problems in machine learning, without going so far as to call for a shift in foundations. Dafoe et al. [11] call for scientists to reconceive artificial intelligence as deeply social. This means making a shift towards research on cooperation in AI-AI and human-AI systems, and away from AIs that operate in isolation, or aim to win zero-sum games like chess. Like most researchers currently working on AI alignment, we feel that all these diverse routes have promise. One aim of this paper is to develop additional routes. This paper does not fit the above pattern of calls in one important way. We are not directing our call towards AI researchers, but to all participants in the current multi-disciplinary and multi-stakeholder AI alignment debate. Our aim is to improve the AI debate. We see improved debate as the main device for improving current and future deployed AI technology.</p>
<p>Cognitive architectures</p>
<p>Kotseruba and Tsotsos [23] discuss how the term cognitive architecture has historically been used to describe the structure of both human and machine minds. The term is often associated with work in neuroscience that seeks to reverse-engineer the human mind, but we will not proceed along these lines in this paper. For our purposes, we define a version of the term which we will apply to humans, companies, governments, and autonomous AIs alike.</p>
<p>First, we define a cognitive process as the full process inside any such entity that perceives the world around it, further processes these perceptions, and then initiates actions based on this processing. We then define a cognitive architecture as the set of interconnected building blocks that make up the internals of a cognitive process.</p>
<p>In the case of human organizations, we can describe their cognitive architecture as one that was set up to initiate appropriate actions by having human cognition interact with a set of written and unwritten rules and goals. Organizations increasingly use autonomous AI systems to take some actions automatically, on a massive scale. In our terminology, these AIs become part of the organization's cognitive architecture, but they also have cognitive architectures themselves. If the business model of a company, or the entire market the company operates in, is somewhat unaligned with human goals and needs, then we can hardly expect that any AI deployed by the company will be better aligned. So demands made in an alignment debate are sometimes best expressed as demands on business models or market regulators, not as demands on a specific AI.</p>
<p>As a framing device, cognitive architectures have several uses in the debate. First, when a discussion gets stuck on the problems of reward maximization, a reframing in terms of cognitive architectures might help to overcome this barrier. A second major use is to improve the debating power of nonspecialist stakeholders compared to that of specialists, and those who might seek to creatively quote or misquote specialists.</p>
<p>The role of multi-stakeholder debate in alignment</p>
<p>Multi-stakeholder debate is a tool that can be used to efficiently collect relevant information, to compare proposed solutions, and to trigger creative thinking. But we also value this type of debate for another reason. We value its potential use as a social coordination ritual. This ritual can define, and create legitimacy for, agreed-on actions and meanings.</p>
<p>Like Gabriel [16], we observe that the word aligned encodes a moral judgment, and that it is therefore impossible to define a single objectively and universally true meaning for the word. For the purpose of this paper, we treat the meaning of the word aligned as being preferably defined by a process of informed debate and consensus building between representatives of all affected human stakeholders. The successful creation of a consensus will lend a certain degree of moral legitimacy to the agreed-on meaning of the word. The consensus does this by defining a social contract that specifies rights and obligations for all parties. In particular, the contract will encode the moral right to claim that, if certain obligations are met, the AI being created is an aligned AI.</p>
<p>Multi-stakeholder debate and social contract creation can also work to solve collective action problems. One example is the suppression of AI arms race dynamics, that would otherwise create an unwanted incentive for the participants in the race to compromise on AI safety and quality control.</p>
<p>Of course, it is to be expected that any social contract will be somewhat inexact, and that it may not always correctly anticipate and cover all possible future developments. When a social contract is not updated in light of new developments, stakeholders may soon stop accepting the implied definition of moral legitimacy. The same problem occurs when the contact is unenforceable or unenforced. To be successful, participants in the alignment debate will need to resolve many complex issues. This paper examines just three of them in detail.</p>
<p>In the framing of AI research, the ability to negotiate better social or commercial contracts would be a clear sign of higher intelligence. However, in this paper we are not considering the research goal of making AIs better at negotiating contracts among themselves or with humans. We focus instead on contributing to the problem of making humans better at negotiating certain broad social contracts among themselves, better while using a process where the AIs do not have any seat at the table.</p>
<p>If AIs which approximate or exceed full human intelligence are ever developed, then both moral and practical arguments could be made for, but also against, giving them a seat at the table above. Bostrom [4] examines this potential long term scenario in more detail. Here, we do not examine it any further.</p>
<p>Structure of this paper</p>
<p>This paper proceeds as follows. Section 2 has general background information on fashions in AI research. Section 3 reviews the cognitive architecture of a pure reward maximizer. Section 4 then modifies this architecture, adding elements not related to reward maximization, to construct a lawabiding reward maximizer. The section also discusses how participants in debate can compensate for the narrative pull of modern fashions in machine learning, if they intend to examine or demand the creation of lawful AIs. Section 5 examines reinforcement learners, a currently very popular framework in both technical and policy discussions about autonomous AI systems. The section develops a detailed non-mathematical picture of the cognitive architecture used by reinforcement learners. This picture departs from how reinforcement learners are usually described in the debate, by emphasizing that all reinforcement learners will automatically construct and use some type of predictive world model. Section 6 then uses this picture to explore some specific risks of deploying highly advanced AIs in the current market system. It considers how these risks could be managed by creating a consensus leading to regulatory action, where the regulator imposes certain technical requirements on powerful market-facing AIs. One promising but currently unfashionable technical option, which is also explored further in section 8, is that the regulator requires that the AI uses a specifically incorrect world model. Section 7 explores the field of machine learning in more detail, to determine how participants in the debate should calibrate their interactions with machine learning researchers. Section 8 also explores this topic, for the case of demanding specifically incorrect world models. Section 9 concludes.</p>
<p>Fashions in AI</p>
<p>We now make some remarks on terminology, acronyms, and fashion. We use the term machine learning (ML) to denote a sub-field of artificial intelligence (AI), where AI itself is a branch of information technology (IT). A notable linguistic development is that all of fundamental and applied AI research is by now commonly referred to as being ML research. This creates a certain narrative pull. When investigating an open problem related to AI, the first impulse will be to look for a solution that leverages machine learning techniques.</p>
<p>Among AI researchers, the term reinforcement learning (RL) usually refers to the specific type of machine learning discussed by Sutton and Barto [39]. But the term has also acquired a much broader meaning, in a linguistic drift process that often happens when technologies trigger Gartner's commercial hype cycle [17]. The acronym RL is increasingly being used to denote any AI system designed to take autonomous actions towards a goal, in a real or simulated physical environment.</p>
<p>When we talk about fashion in this paper, we will mostly describe the prevailing fashions in ML and RL research. But what is fashionable in fundamental RL research, as described by Sutton [38], may not be appealing at all to an applied robotics researcher like Brooks [6]. It is the fashions and preferences among ML and RL researchers which are having the greater impact on the current debate.</p>
<p>Good Old Fashioned AI</p>
<p>Among AI practitioners, there is a useful acronym that denotes the opposite of current hypes and fashions: GOFAI or Good Old Fashioned AI. A system that uses GOFAI will incorporate tried-andtested AI techniques which are no longer at the forefront of AI research. One reason to use GOFAI may be that the old technique simply has superior performance for the job at hand. Another may be that the old technique makes safety engineering and worst-case risk analysis much more tractable.</p>
<p>We feel that GOFAI is too often overlooked in the current debate about AI safety and alignment. There is a tendency to focus too much on the features, limitations, and unknowns associated with the latest and most fashionable ML techniques.</p>
<p>It is also necessary to apply a GOFAI lens to the many recent AI strategy announcements by companies and governments, announcements which often mention huge sums of money going to more AI. If GOFAI is overlooked by a participant in the debate, they may easily get the impression that these players have committed to an irresponsible rush to apply the latest hyped AI technologies to every aspect of human life.</p>
<p>GOFAI is definitely not being overlooked in the near-term alignment debate about AI-based fairness and discrimination. If a regulator requires that a company must be able to explain to a job applicant just how it is avoiding any unfair bias in its decision support systems, then this often forces the use of GOFAI instead of modern deep neural nets. Unlike current deep neural nets, GOFAI techniques can produce systems with internal moving parts that can be tractably audited and explained.</p>
<p>Pure reward maximizers</p>
<p>A pure reward maximizer is a cognitive architecture that is fully devoted to taking actions which maximize an expected future reward. In AI research and in game theory, this reward can be any metric of success or merit we may care to define. In economic theory, when applied to the cognitive architecture of companies, the reward metric is most often company profits.</p>
<p>In the medium and long-term AI alignment debate, it is often useful to draw parallels between a pure reward maximizing AI and a company that cares for nothing but profits. This can uncover many AI safety issues and failure modes that need to be addressed. But our goal here is to consider how, after having such a discussion about pure reward maximizers, one might move forward. Figure 1 depicts the cognitive architecture of a reward maximizer as a set of interconnected building blocks. The maximizer first observes its environment to determine the context of the next action to be taken. It then uses this context together with a predictive model, to score a list of all possible actions which might be taken on a reward metric. After scoring each possible action on the predicted reward, it picks and performs one of the actions which have gotten the highest reward score. In naming the individual building blocks, we have avoided the use of specialist terminology as much as possible. The intent is to make this picture maximally useful as tool for clarifying and structuring multi-disciplinary debate.</p>
<p>Law-abiding reward maximizers</p>
<p>Aligned commercial companies not only care about profits, they also care about the law. Inside the cognitive architecture of a law-abiding company, we may find legal compliance officers. Their role is to examine if proposed company actions or policies would be in violation of the law. If so, they are supposed to block the use of these proposed actions or policies, regardless of how this will affect profits.</p>
<p>We could demand that the same cognitive architecture is also used in an AI that makes autonomous decisions without human review. To build such an AI, we need a piece of software that automates the job of the human compliance officer. In basic IT terminology, this piece software takes two inputs. First, it needs a description of the proposed action. Second, it needs relevant contextual information, a description of the target environment to which the proposal will be applied. The output would be a yes or no answer about whether the proposed action is legal in that context.</p>
<p>There are several options for implementing the above compliance officer software. To further illustrate our point about GOFAI, we consider the option of using an expert system. Expert systems are a type of AI technology that experienced peak hype in the 1980s. An expert system takes some inputs and then applies certain computer-readable rules to draw a conclusion about these inputs. To build the compliance officer expert system needed above, its programmers would consult with the company's legal department to locate all applicable laws that the AI might violate with its actions. They then convert these laws into computer-readable rules for the expert system.  The full design of the cognitive architecture of the demanded law-abiding profit-maximizing AI might then look as follows (figure 2). Whenever the AI has to decide on taking the next autonomous action, it first uses a software module to construct a long list of actions that could be taken. For example, each action on that list could be to show one specific advertisement, out of all the available ads, to an end user. The list of actions is then sent to the expert system, which will remove all illegal actions from the list. It may for example be illegal to show ads about gambling to minors. The remaining actions on the list are scored by a different subsystem, which estimates their profitability. The AI will then pick a legal action that has a maximal profitability score, and autonomously perform this action.</p>
<p>Cognitive architectures like the above are routinely deployed. ML experts will usually describe them using a short mathematical formula which fully captures the overall intent of the above information processing chain. If not pressed by other participants in the debate, they may never switch to the more accessible descriptive language of step-wise bureaucratic decision making we have used above.</p>
<p>We now examine a further issue. If one were to pose the problem of building a law-abiding reward maximizer to a modern ML researcher, it is unlikely that they will immediately bring up expert system technology. Expert systems are not even ML systems. They do not learn because all their knowledge has to be carefully constructed by hand, in the form of computer-readable rules. This makes expert systems unfashionable among ML researchers, and it also makes them unfashionable in the applied AI community. Among applied AI programmers, few tasks would be considered less glamorous than the task of hand-translating a body of law identified by the legal department into expert system rules.</p>
<p>What is more likely to happen in an alignment debate is that the ML researcher or AI practitioner will immediately express great enthusiasm for addressing the problem of law-abiding AI with modern machine learning technology. This enthusiasm might take the form of a proposal for a research project that examines how deep neural network based natural language processing (NLP) can be used to automatically convert a written body of law into computer-readable rules. The proposed research project would investigate the unsolved problem of making this fully automated conversion process work robustly in the general case.</p>
<p>So it is easy for a discussion about AI and the law to follow a narrative flow which arrives at the conclusion that certain reasonable demands cannot be met by modern ML technology, unless further ML research is successful in solving open problems. This can put the debate about meeting stakeholder demands into an undesirable holding pattern. If stakeholders want to overcome this obstacle to progress, they can instead examine the cognitive architecture of a law-abiding company, and then demand that GOFAI is used to replicate the same architecture in the AI.</p>
<p>Reinforcement learners</p>
<p>In ML research, reinforcement learning [39] is currently the most popular framework for considering autonomous AI systems that interact with an environment over multiple time steps. We now examine this framework in more detail.</p>
<p>A reinforcement learner is a cognitive architecture designed to autonomously take actions over time.</p>
<p>The mathematical convention is that a reinforcement learner takes one action per time step. All of these actions are supposed to contribute to a single goal, a goal defined by a reward function. If we use basic IT terminology to describe it, then the reward function is a piece of software that will be run inside the reinforcement learner at the end of each time step. The reward function software will compute and deliver a single number, which is interpreted by the cognitive architecture as a reward received in that particular time step. For example, the reward function might query a database to determine the profit made by a company during that time step, and deliver that profit as the reward.</p>
<p>As shown in figure 3, the cognitive architecture of a reinforcement learner incorporates machine learning in order to predict the general relation between the possible actions it could take and its future rewards. It will use these predictions to always take an action that will maximize a weighted sum of all expected future rewards. Reinforcement learners may operate for many time steps before receiving the first non-zero reward.</p>
<p>Like humans, governments, and businesses, reinforcement learners have a cognitive architecture capable of long-term planning, of making investments now to capture larger rewards in future. They  can be surprisingly creative in finding ways to maximize their summed reward. They may find strategies which surprise and occasionally disappoint those who designed the reward function, as memorably discussed by Clark and Amodei [9]. Reinforcement learners are not the only type of AI capable of potentially dangerous creativity, for example Lehman et al. [26] show that the same problem also exists with evolutionary machine learning.</p>
<p>Automatically constructed world models</p>
<p>A reinforcement learner is designed to automatically and autonomously construct a world model it will use for decision making, to construct it via machine learning. In general IT terminology, the world model constructed by the reinforcement learner is a digital model that allows it to make predictions about the future, based on information about the past and present.</p>
<p>Usually, the learned digital world model will allow the reinforcement learner to predict, for each alternative action it might take next, how taking this action will impact its expected summed future rewards. But in some designs, for example when using policy gradient methods, the predictive model being constructed can only be used to compute an estimate of the best next action, it cannot score each individual action. The term model-free reinforcement learner is commonly used for reinforcement learning architectures which do not aim to construct the type of rich predictive world model which can project entire future world states. Policy gradient learners are examples of model-free reinforcement learners. The common framing is that a model-free reinforcement learner constructs a limited predictive function only, not a fully realized predictive world model. But for our purposes, we will interpret even these limited functions as being predictive world models produced by machine learning. They still encode predictions of how the world will mediate between action and summed future reward.</p>
<p>The automatic building of world models by reinforcement learners has obvious economic advantages. It can replace the GOFAI technique of having a team of domain experts and programmers carefully build a world model by hand. But beyond mere economics, there is a stronger force that drives the enthusiasm for reinforcement learners. Reinforcement learning research is driven in part by a vision that is common among IT professionals, by the idea it would be great to automate away repetitive tasks like building new domain specific world models by hand. The pursuit of this vision by ML researchers has created some impressive results. For an increasing number of applications, modern machine learning can automatically build world models that will allow the AI to massively outperform earlier AIs using carefully hand-crafted world models. A lot of modern ML research is focused on pushing the edge forward even further.</p>
<p>Risks from cheap and accurate world models</p>
<p>We now use a thought experiment to explore some of the consequences of using ever more accurate world models in automated decision making.</p>
<p>Say that a company, operating in some market, wants to fully automate certain customer interactions. They set up an advanced reinforcement learner that their customers will interact with via a web site. They configure the reinforcement learner to maximize company profits. Now, if customers feel mistreated by the actions of the reinforcement learner, they might contact the market regulator, who may impose a fine that will lower company profits.</p>
<p>Consider a reinforcement learner which automatically builds a world model that will start to correctly represent the above mechanisms by which profit is determined. The system will learn to estimate the exact costs of treating certain customers in a certain way. It will learn to deliver correct but particularly expensive services only to those customers who are likely to complain to the regulator when these services are withheld. To maximize overall profits. customers who are less likely to cause fines when mistreated will be treated less correctly. The system has a cognitive architecture which is configured to treat an expected fine as a price, as a cost of doing business, not as a social signal that it must absolutely try to avoid treating any customer that way in future.</p>
<p>This leads to a moral question, the question whether the above form of gaming the regulatory system is aligned. Different stakeholders may answer this question differently. A first stakeholder may feel that treating a fine as a price, or discriminating between customers in this way, is always morally wrong. A second stakeholder may feel that this emergent AI behavior can still be morally acceptable, as long as even the worst cases of customer treatment delivered still fall within a range of accepted business practices. Stakeholder debate will be needed to resolve this moral question for society.</p>
<p>We now proceed with a further step in the thought experiment. Consider what might happen in a future where every company and customer in every market has the ability to cheaply build and deploy these gaming capable AIs. If no government steps in to intervene, we can expect that in every market, commercial pressures will create a race to the bottom in AI-controlled gaming. To stay in business, every company will ever more desperately use its automated systems to game and counter-game everybody and everything around it. This gaming could easily destabilize any market, and society as a whole.</p>
<p>Zubov has argued [42] that such a computer-aided breakdown of the traditional market system has already happened. We believe instead that the highly effective automated gaming of customers and regulatory structures, and the race to the bottom scenario above, would require at least one more major technical breakthrough.</p>
<p>As the timing of technical breakthroughs is unpredictable, there are good reasons for already having a debate about the possibilities for government regulation, to prevent the above race to the bottom. We believe that most companies would greatly prefer to operate in markets that reward honest interactions with customers and other stakeholders more than they reward gaming. So on the political side of the debate, the problem of building a consensus for the enforcement of certain regulatory demands that constrain automated gaming by AIs should be tractable. This leaves the question of what these demands should look like technically.</p>
<p>Technical options for the regulator</p>
<p>The law-abiding cognitive architecture discussed earlier may go some way towards suppressing gaming, depending on how good the applicable laws are. We now list two further ways to address the problem of gaming by autonomous AIs, to address it in a way that could be audited by a market regulator.</p>
<ol>
<li>
<p>Limited or specifically incorrect world models. Going back to the example of an AI gaming the regulatory system, if we can remove the regulator from the AI's world model, we will break the connection between the existence of the regulator and the expected profits that the AI's world model will project. This will make the AI lose both the incentive and the ability to treat a fine as a price. Figure 4 shows a cognitive architecture which ensures the use of a specifically incorrect world model to drive decision making. Two general questions are raised when we consider  Moving away from figure 4, we can also consider another type of design intervention, one that combines the earlier generic RL architecture from figure 3 with a reward function that has specific imperfections designed in. In the gaming example, the company might combine a generic RL architecture with a reward function that excludes the impact of all fines from the profit calculation. We can interpret the resulting RL system as a profit maximizer which has an incorrect model of how profit can be maximized. This incorrect model will greatly suppress the disparate treatment of customers we described. Note that this approach might not perfectly erase the regulator from the learned model. It is likely that the presence of the regulator has secondary effects beyond mere fines, and these secondary effects might still be correctly captured by the model. But we are not necessarily looking for the mathematically perfect removal of all gaming incentives from all AIs. Markets and society are robust enough that they can tolerate a certain background level of gaming. Certain types and levels of gaming may even be explicitly agreed on to be morally admissible, because they add more color to life.</p>
</li>
<li>
<p>More aligned reward functions. We could also consider building a reward function which rewards not only an economic goal, but also general pro-social behavior. The reward function may combine a profit measure with other metrics that detect and penalize socially unaligned behaviors like gaming, breaking laws, manipulating human oversight, or deceit in general.</p>
</li>
</ol>
<p>In the context the above example, we could add a term to the reward function that multiplies any fine received by a factor of 100. This will produce a cognitive architecture that is much more reluctant to trigger fines, which would make the AI more aligned, but arguably only if all mistreated customers are equally likely to contact the regulator. A safer option would be for the system designers to hand-code a penalty term that more directly measures an agreed-on metric of incorrect treatment.</p>
<p>Writing pro-social reward functions for powerful AIs will not be easy. In reviews of AI safety problems written by ML researchers, for example in Amodei et al. [1], it is common to find the observation that a powerful AI cannot not be fully safe or aligned if its reward function is incorrect, if it leaves loopholes in the encoding of desired behavior. The AI alignment literature often proceeds to identify the problem that fallible humans cannot ever be expected to write a fully correct reward function, definitely not if this reward function has to encode the full goals and needs of humanity. However, ML and alignment researchers seldom proceed from these observations to work on the problem of writing down a reward function which is at least as correct as humanly possible. In the next section, we examine why this might be the case.</p>
<p>Overall, we feel that the broad topic of pro-social reward function design needs more attention, and that it needs to become more fashionable among AI alignment technology researchers. Some existing work can be found in Turner et al. [40], Krakovna et al. [24], Soares et al. [36], Holtman [21], and Vamplew et al. [41]. Of course, an act of reward function design necessarily has to happen whenever a business wants to deploy a modern autonomous AI. When challenged, many businesses would claim that they have indeed written and deployed pro-social reward functions. But as long as these businesses treat their reward functions as trade secrets, this is of no great help to open scholarship or open debate.</p>
<p>Preferences and deflection among ML researchers</p>
<p>Russell [33] describes how Bostrom's call [4] for deep thinking about long-term alignment led to a 'not so great debate' in the AI community. Notably, well-known AI researchers have made 'instantly regrettable remarks' to deflect such calls. Russell identifies some of the drivers that may sustain the not-so-great nature of the debate, like the possible fear of losing research funding, and false dichotomies hardening into tribalism. Baum [3] and Stix and Maas [37] show that tribalism is being sustained by the difference in near-term versus long-term focus. They consider how to bridge this divide, so that the debate can move more smoothly forward. Here, we consider a different driver which creates a divide that is less easily bridged.</p>
<p>There is a division of labor that exists in all of information technology. It is common in IT to split the problem of making a computer do something new and useful over two teams: the specification team and the implementation team. The job of the specification team is to write an unambiguous specification of the useful thing that the computer should do. This specification is then handed to the implementation team, which will figure out how to make a computer do this useful thing in the most efficient way.</p>
<p>In this division of labor, only the specification team will interact with the stakeholders affected by the implementation. It is the job of the specification team to correctly identify and triangulate stakeholder needs. When the stakeholders concerned have largely conflicting goals or tense relations, the specification team will have to navigate and resolve many 'people problems', if they are to produce something truly useful. The specification being developed will inevitably be judged by all stakeholders on how the resulting computer system will affect the balance of power and social contracts between them. The specification team may sometimes succeed in defusing tense stakeholder relations by locating a proposal that will be perceived as a win-win improvement by all. But such a happy outcome is by no means guaranteed.</p>
<p>Many IT technologists would prefer to have a career path which lets them forever avoid working on the people problems and stakeholder tensions that the specification team has to handle. They prefer a path that puts them firmly in the implementation team only, or in a position where their only concern is to deliver useful tools and technologies to the implementation team. This desire is by no means universal, some technologists may explicitly seek out a career in the specification team, others may seek the variety that comes from being on both teams.</p>
<p>We now turn to the career path offered by ML research. Current mainstream ML research treats the AI's reward function as the sole specification of the useful thing an autonomous AI should do. This means that the specification team problem of writing a useful or aligned reward function is out of scope for the ML researcher. The ML researcher is concerned instead with the mathematically clear and unambiguous problem of building technology that will learn to maximize any reward function one might supply. Progress on this problem will be judged by objective machine learning benchmarks. All these things combine into the promise of a politics-free career path with a clear and level playing field.</p>
<p>Those working on AI alignment have by now written many papers that aim to introduce ML researchers to the methods and tools used by the specification team. Some notable examples are the discussion of incomplete contracting by Hadfield-Menell and Hadfield [19], and of incompletely theorized agreements by Stix and Maas [37]. Gabriel [16] offers an accessible discussion of how fair principles for AI alignment could be identified. Dobbe et al. [12] present a specific process framework that could be used by AI designers to make hard choices about the AI's sociotechnical impact, based on developing and shaping stakeholder feedback channels. Selbst et al. [35] consider the case of fairness in ML. More generally, the fields of software engineering, systems engineering, and science and technology studies have developed and documented many useful specification team tools.</p>
<p>Our personal experience is that, when one engages with an individual technical expert and gently tries to push them into picking up these tools to join the specification team, the vast majority of these experts will respond by ignoring, countering, or deflecting the applied pressure.</p>
<p>We expect that any alignment project or funding effort which aims to make more ML researchers interested in contributing to all parts of the alignment problem will inevitably run into this barrier. One might try to overcome it by finding more clever ways to push harder, but we feel that this approach is both unkind and unlikely to produce sufficient results.</p>
<p>The more productive option is to route around this barrier, to endorse and promote a division of labor which does not expect ML researchers to lead every charge. This means that one should avoid a framing where all of AI alignment research is, or must become, a sub-field of AI research. A better way forward is to declare that many of the problems in AI alignment are broad political and systems engineering problems, not ML problems. Here, we use the term systems engineering to denote a multidisciplinary field that contains the entire path from specification to implementation inside of its scope. Furthermore, the system being engineered is not just the technical artifact itself, but the entire set of interactions between the artifact and broader society.</p>
<p>That being said, we also note that AI safety agendas like Amodei et al. [1] list many important problems for which progress can be made while staying within the traditional scope of ML research and of the implementation team. In the ML research community, there is considerable enthusiasm for working on many of these open safety problems.</p>
<p>Machine learning of an aligned reward function</p>
<p>There is also enthusiasm among ML researchers for the option of creating a pro-social reward function automatically, via machine learning. This type of automation removes programmer labor, but does require the presence and involvement of a human teacher. The convention is that the reward function to be learned already exists in the teacher's head. The teacher will communicate this function to the AI by interacting with it.</p>
<p>To make a reinforcement learner automatically learn the teacher's reward function, we could create a setup where the reward function software inside its cognitive architecture determines the numerical reward for each time step by reading out a remote control device held by the teacher. The teacher has two buttons: one to signal approval of past behavior by giving a positive reward in the current time step, one to punish past behavior with a negative reward. If the teacher rewards pro-social behaviors and punishes non-social behaviors, the learned reward function will be a pro-social one. Beyond reinforcement learning, another approach to learning a reward function from a human teacher is inverse reinforcement learning as described by Ng and Russell [29].</p>
<p>For use cases where this works well, the above techniques have obvious economic benefits. They remove the need for a programmer to hand-code a reward function after interviewing the teacher themselves. But we believe that this does not fully explain the enthusiasm for reward function learning among many of the IT technologists participating in the AI alignment debate. This enthusiasm is better explained by the hope that reward function learning can be scaled up to automate away all the difficult stakeholder interactions that these technologists imagine they must otherwise engage in themselves, if they want to create a more stakeholder-aligned AI.</p>
<p>Long-term alignment discussions often consider the many dangers and failure modes inherent in the above remote control based setup. One commonly mentioned failure mode is that the AI may creatively use violence to force the teacher into forever pressing the approval button. Technical research on the long-term alignment problem is by now mostly concerned with designing and studying very different setups that suppress or remove such failure modes. For technical readers, some examples of alternatives are in Hadfield-Menell et al. [20], Everitt et al. [15], Orseau and Armstrong [31], Holtman [21], Cohen et al. [10], Armstrong et al. [2], and Drexler [14].</p>
<p>Limited or specifically incorrect world models</p>
<p>The cognitive architecture of the human mind has a great ability to take actions based on limited or specifically incorrect world models, Even small children can play a game where they all pretend to be pirates while the floor is lava. As the game goes on, they will often negotiate further rules among themselves to keep things more balanced and interesting.</p>
<p>In adult life, we may demand that a business owner will treat male and female customers exactly alike, even when they know full well that one of these customer types is more profitable. We do not doubt that the business owner has the mental ability to meet this demand if they want to. To frame this demand in terms of a cognitive architecture, we expect that the business owner, while maximizing profit, will take certain decisions by using a world model in which the distinction between genders is erased. The modern concept of fair and equal treatment of citizens by government can also be expressed as a demand that civil servants must make decisions without taking certain facts into account.</p>
<p>When one examines various social contracts, one can note that many demands made in them are demands that certain players must use specifically limited world models when making decisions which affect others. For powerful players like governments, it is common to demand especially severe limitations. So we may demand that such limitations are also present in the cognitive architecture of any powerful AI that interacts with humankind.</p>
<p>Overall, we feel that a broad range of useful AI alignment demands can be developed, clarified, and expressed as demands that the AI world model incorporates some specifically designed imperfections. We feel that this is a promising but still largely overlooked direction for AI research, and for advancing the alignment debate.</p>
<p>The sub-field of AI fairness research already concerns itself with the specification and construction of desired imperfections in predictive models, though it usually frames these as being improvements, not imperfections. Christian [8] has an accessible discussion of the recent developments in this field, and shows how AI fairness research has uncovered some surprising difficulties. The seemingly simple idea of a predictive model erasing all distinctions between gender, or at least erasing them well enough to be morally acceptable, can be mapped to many plausible but different technical definitions. It has been shown that these different definitions can encode different and sometimes even conflicting moral judgments about the correct treatment of people. The choice between options is therefore preferably made by multi-stakeholder debate and consensus building. This in turn requires that more work is done on definitions which are not only technically feasible to implement, but can also be explained to non-technical stakeholders.</p>
<p>Technical possibilities and fashions</p>
<p>Demands that an AI uses a specifically limited or imperfect world model can often be met by using GOFAI techniques to build the model by hand. When we turn to the automatic building of specifically imperfect world models, the situation gets more complicated. The currently most fashionable branches of machine learning all produce opaque, black-box world models which cannot easily be edited to include specific imperfections. Deep learning and model-free reinforcement learning both produce world models which will encode the learned knowledge that 'the floor is made of wood' across a massive set of opaque numbers. One can imagine a software component that will automatically edit these numbers to reliably turn the floor into lava. But it is still an open research problem to create that software component for these particular models.</p>
<p>There have been some recent developments towards resolving this open research problem, often by routing around it. Kusner et al. [25] have defined a criterion called counterfactual fairness, which may be used for example to define if a learned world model is making an inappropriate distinction between genders. The learned world model has to be a causal world model, a model which encodes learned knowledge into a Pearl causal graph [32]. Kusner et al. show how a learned causal model which unfairly makes a distinction between genders can be edited automatically to make it fair on gender, at least according to the criterion defined. This work has boosted the interest in the ML community to further improve the type of machine learning that creates causal models.</p>
<p>In a recent parallel development, Pearl causal graphs are also being used in technical work on longterm AI alignment. Carey et al. [7] use them to define crisp mathematical criteria which can be applied to long-term alignment problems for autonomous reward-maximizing AIs. Holtman [22] shows how one can bypass the problem of having to edit a complete and correct black-box world model, by instead wrapping the model into a Pearl causal graph which will then produce specifically incorrect predictions. They are usefully incorrect by suppressing the emergent incentive which an advanced AI may develop to disable its own built-in safety mechanisms.</p>
<p>The above modification techniques create AIs that are worse economic actors, if we measure economic performance by their reward function alone. But they make them into better socioeconomic actors.</p>
<p>Conclusions</p>
<p>The main aim of this paper has been to equip participants in the AI alignment debate with additional tools and insights that can be used to overcome barriers. We have considered three barriers in particular: stakeholder uncertainty about what is technically possible, a too narrow focus on reward maximization, and the narrative pull of current fashions in machine learning. The framing of cognitive architectures can be used to overcome these barriers, and to give more debating power to non-specialist stakeholders.</p>
<p>The cognitive architectures of modern governments and companies have many features that were explicitly designed to make them more aligned with human goals and needs. Stakeholders in the AI alignment debate can locate these features, and then demand that these are also designed into the cognitive architectures of powerful AIs. We have reviewed three somewhat unfashionable features which might be demanded: automated compliance officers, pro-social reward function components, and specifically incorrect world models. We have also emphasized the possibility to meet such demands with GOFAI techniques.</p>
<p>We also considered how stakeholders should calibrate their interactions with individual ML researchers, if they want to encourage or fund more AI alignment research. We do not believe it is productive for such stakeholders to expect or demand that ML researchers will take the lead in solving all AI alignment problems. We prefer a framing which states that many of the open research problems in AI alignment are broad systems engineering problems, not ML research problems.</p>
<p>The idea of demanding that powerful AIs use specifically inaccurate world models is based in part on our earlier work in [22].</p>
<p>Related work and further reading</p>
<p>We now discuss some connections and related work not already mentioned elsewhere in this paper. Our description of reinforcement learning in section 5 emphasizes the building and use of predictive world models. The same emphasis is present in the planning-as-inference model of human cognition developed by Botvinick and Toussaint [5].</p>
<p>Gabriel [16] conducts a detailed examination of the questions of moral philosophy that arise in the alignment context, to develop propositions on what AI alignment research and debate should be about. There are close parallels between these propositions and the approach taken in this paper. Gabriel speculates that the methods we use to build AI may influence the kind of values we are able to encode. This paper supports and illustrates this thesis. We have shown how certain types of moral demands might best be supported by introducing technical elements unrelated to reward maximization and reward function design.</p>
<p>Greene et al. [18] and Mittelstadt [28] discuss the narrative pull of certain conventional framings around business and professional ethics, and how these have shaped the recent AI alignment debate. They call for the debate to move beyond these particular framings.</p>
<p>Sambasivan et al. [34] also examine fashion and safety. They discuss how among AI technologists, it is not fashionable to work on the problem of quality assurance for training data. This can severely impact safety when such training data is used to create AIs deployed in high-stakes domains.</p>
<p>Dotan and Milli [13] explore the rise and fall of fashions in ML research through the lens of philosophy of science. Sutton's blog post The bitter lesson [38] makes for interesting between-the-lines reading, if one reads it as a promise of what researchers choosing a career in ML will never be required to do. Counterpoints to Sutton are offered by Brooks [6] and Marcus [27].</p>
<p>Figure 1 :
1Graphical depiction of the cognitive architecture of a pure reward maximizing AI, using a data-flow diagram. Arrows represent data flow. Each cylinder is an amount of data. Rectangular boxes are pieces of software.</p>
<p>Figure 2 :
2Cognitive architecture of a law-abiding reward maximizer. To improve alignment, the pure reward maximizer from figure 1 is extended with additional green building blocks.</p>
<p>Figure 3 :
3Cognitive architecture of a generic reinforcement learner. Many popular specific reinforcement learning architectures merge or approximate the building blocks depicted, to speed up computations or to save on memory usage. To simplify the presentation, we have not included any building blocks that trigger 'exploration' actions, even though most reinforcement learning systems have them.</p>
<p>Figure 4 :
4Design of a more aligned reinforcement learner. A regulator might demand that the green building blocks are added to ensure that a specifically incorrect world model is used, one that produces automated decision making more in line with agreed-on human goals and needs.the green building blocks which have been added. First, if we want to improve alignment, what kind of world model imperfections might usefully be imagined and demanded? Second, what is the technical feasibility of the depicted model editing operation? We will examine both questions in more detail in section 8.
AcknowledgmentsWe thank Ben Smith, Roland Pihlakas, and the anonymous reviewers for their comments which led to improvements in this paper. This paper reflects a research effort conducted while the author was working as an independent researcher. No external funding was sought or received.Version historyThe first version of this paper was presented at the PERLS Workshop at 35th Conference on Neural Information Processing Systems (NeurIPS 2021). This second arXiv version extends the first version by adding four figures, and some extra lines of text to integrate the figures into the narrative.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané, arXiv:1606.06565Concrete problems in AI safety. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in AI safety. arXiv:1606.06565, 2016.</p>
<p>Pitfalls of learning a reward function online. Stuart Armstrong, Jan Leike, Laurent Orseau, Shane Legg, arXiv:2004.13654Stuart Armstrong, Jan Leike, Laurent Orseau, and Shane Legg. Pitfalls of learning a reward function online. arXiv:2004.13654, 2020.</p>
<p>Reconciliation between factions focused on near-term and long-term artificial intelligence. D Seth, Baum, AI &amp; Society33Seth D Baum. Reconciliation between factions focused on near-term and long-term artificial intelligence. AI &amp; Society, 33(4):565-572, 2018.</p>
<p>Superintelligence: paths, dangers, strategies. Nick Bostrom, Oxford University PressNick Bostrom. Superintelligence: paths, dangers, strategies. Oxford University Press, 2014.</p>
<p>Planning as inference. Matthew Botvinick, Marc Toussaint, Trends in cognitive sciences. 1610Matthew Botvinick and Marc Toussaint. Planning as inference. Trends in cognitive sciences, 16(10):485-488, 2012.</p>
<p>A Better Lesson. Rodney Brooks, Rodney Brooks. A Better Lesson, 2019. URL https://rodneybrooks.com/ a-better-lesson/.</p>
<p>Ryan Carey, Eric Langlois, Tom Everitt, Shane Legg, arXiv:2001.07118The incentives that shape behaviour. Ryan Carey, Eric Langlois, Tom Everitt, and Shane Legg. The incentives that shape behaviour. arXiv:2001.07118, 2020.</p>
<p>The Alignment Problem: Machine Learning and Human Values. Brian Christian, WW Norton &amp; CompanyBrian Christian. The Alignment Problem: Machine Learning and Human Values. WW Norton &amp; Company, 2020.</p>
<p>Faulty Reward Functions in the Wild. Jack Clark, Dario Amodei, Jack Clark and Dario Amodei. Faulty Reward Functions in the Wild, 2016. URL https: //openai.com/blog/faulty-reward-functions/.</p>
<p>Asymptotically unambitious artificial general intelligence. Michael Cohen, Badri Vellambi, Marcus Hutter, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Michael Cohen, Badri Vellambi, and Marcus Hutter. Asymptotically unambitious artificial general intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence, vol- ume 34, pages 2467-2476, 2020.</p>
<p>Cooperative AI: machines must learn to find common ground. Allan Dafoe, Yoram Bachrach, Gillian Hadfield, Eric Horvitz, Kate Larson, Thore Graepel, Nature. 5932021Allan Dafoe, Yoram Bachrach, Gillian Hadfield, Eric Horvitz, Kate Larson, and Thore Graepel. Cooperative AI: machines must learn to find common ground. Nature, 593, 2021.</p>
<p>Hard choices in artificial intelligence. Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz, Artificial Intelligence. 300103555Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. Hard choices in artificial intelli- gence. Artificial Intelligence, 300:103555, 2021.</p>
<p>Value-laden disciplinary shifts in machine learning. Ravit Dotan, Smitha Milli, Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. the 2020 Conference on Fairness, Accountability, and TransparencyRavit Dotan and Smitha Milli. Value-laden disciplinary shifts in machine learning. In Proceed- ings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 294-294, 2020.</p>
<p>Reframing superintelligence: Comprehensive AI services as general intelligence. Eric Drexler, Future of Humanity Institute, University of Oxford.K Eric Drexler. Reframing superintelligence: Comprehensive AI services as general intelli- gence. Future of Humanity Institute, University of Oxford., 2019.</p>
<p>Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective. Tom Everitt, Marcus Hutter, arXiv:1908.04734Tom Everitt and Marcus Hutter. Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective. arXiv:1908.04734, 2019.</p>
<p>Artificial intelligence, values, and alignment. Minds and machines. Iason Gabriel, 30Iason Gabriel. Artificial intelligence, values, and alignment. Minds and machines, 30(3): 411-437, 2020.</p>
<p>Gartner Hype Cycle Research Methodology. Gartner, Gartner. Gartner Hype Cycle Research Methodology. URL https://www.gartner.com/ en/research/methodologies/gartner-hype-cycle.</p>
<p>Better, nicer, clearer, fairer: A critical assessment of the movement for ethical artificial intelligence and machine learning. Daniel Greene, Anna Lauren Hoffmann, Luke Stark, HICSS. Daniel Greene, Anna Lauren Hoffmann, and Luke Stark. Better, nicer, clearer, fairer: A critical assessment of the movement for ethical artificial intelligence and machine learning. In HICSS, 2019.</p>
<p>Incomplete contracting and AI alignment. Dylan Hadfield, - Menell, Gillian K Hadfield, Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. the 2019 AAAI/ACM Conference on AI, Ethics, and SocietyDylan Hadfield-Menell and Gillian K Hadfield. Incomplete contracting and AI alignment. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 417-422, 2019.</p>
<p>Cooperative inverse reinforcement learning. Dylan Hadfield-Menell, J Stuart, Pieter Russell, Anca Abbeel, Dragan, Advances in Neural Information Processing Systems. 29Dylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse reinforcement learning. In Advances in Neural Information Processing Systems, volume 29, pages 3909-3917, 2016.</p>
<p>Koen Holtman, arXiv:2007.05411AGI agent safety by iteratively improving the utility function. Koen Holtman. AGI agent safety by iteratively improving the utility function. arXiv:2007.05411, 2020.</p>
<p>Koen Holtman, arXiv:2102.00834Counterfactual planning in AGI systems. Koen Holtman. Counterfactual planning in AGI systems. arXiv:2102.00834, 2021.</p>
<p>40 years of cognitive architectures: core cognitive abilities and practical applications. Iuliia Kotseruba, John K Tsotsos, Artificial Intelligence Review. 53Iuliia Kotseruba and John K. Tsotsos. 40 years of cognitive architectures: core cognitive abilities and practical applications. Artificial Intelligence Review, 53:17-94, 2018.</p>
<p>Avoiding side effects by considering future tasks. Victoria Krakovna, Laurent Orseau, Richard Ngo, Miljan Martic, Shane Legg, Advances in Neural Information Processing Systems. 33Victoria Krakovna, Laurent Orseau, Richard Ngo, Miljan Martic, and Shane Legg. Avoiding side effects by considering future tasks. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), pages 19064-19074, 2020.</p>
<p>Counterfactual fairness. Matt Kusner, Joshua Loftus, Chris Russell, Ricardo Silva, Advances in neural information processing systems. Matt Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances in neural information processing systems (NeurIPS 2017), pages 4066-4076, 2017.</p>
<p>The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities. Joel Lehman, Jeff Clune, Dusan Misevic, Christoph Adami, Lee Altenberg, Julie Beaulieu, J Peter, Samuel Bentley, Guillaume Bernard, Beslon, M David, Bryson, Artificial life. 262Joel Lehman, Jeff Clune, Dusan Misevic, Christoph Adami, Lee Altenberg, Julie Beaulieu, Peter J Bentley, Samuel Bernard, Guillaume Beslon, David M Bryson, et al. The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities. Artificial life, 26(2):274-306, 2020.</p>
<p>Gary Marcus, arXiv:2002.06177The next decade in AI: four steps towards robust artificial intelligence. Gary Marcus. The next decade in AI: four steps towards robust artificial intelligence. arXiv:2002.06177, 2020.</p>
<p>Principles alone cannot guarantee ethical AI. Brent Mittelstadt, Nature Machine Intelligence. 111Brent Mittelstadt. Principles alone cannot guarantee ethical AI. Nature Machine Intelligence, 1(11):501-507, 2019.</p>
<p>Algorithms for inverse reinforcement learning. Y Andrew, Stuart Ng, Russell, Proc. 17th International Conf. on Machine Learning. 17th International Conf. on Machine LearningAndrew Y Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In in Proc. 17th International Conf. on Machine Learning, 2000.</p>
<p>The basic AI drives. M Stephen, Omohundro, AGI. 171Stephen M Omohundro. The basic AI drives. In AGI, volume 171, pages 483-492, 2008.</p>
<p>Safely interruptible agents. Laurent Orseau, Stuart Armstrong, Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence. the Thirty-Second Conference on Uncertainty in Artificial IntelligenceAUAI PressLaurent Orseau and Stuart Armstrong. Safely interruptible agents. In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence, pages 557-566. AUAI Press, 2016.</p>
<p>. Judea Pearl, Causality, Cambridge university pressJudea Pearl. Causality. Cambridge university press, 2009.</p>
<p>Human compatible: Artificial intelligence and the problem of control. Penguin Random House. Stuart Russell, Stuart Russell. Human compatible: Artificial intelligence and the problem of control. Penguin Random House, 2019.</p>
<p>Everyone wants to do the model work, not the data work": Data cascades in high-stakes AI. Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, Lora M Aroyo, proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. the 2021 CHI Conference on Human Factors in Computing SystemsNithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. "Everyone wants to do the model work, not the data work": Data cascades in high-stakes AI. In proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1-15, 2021.</p>
<p>Fairness and abstraction in sociotechnical systems. D Andrew, Danah Selbst, Boyd, A Sorelle, Suresh Friedler, Janet Venkatasubramanian, Vertesi, Proceedings of the conference on fairness, accountability, and transparency. the conference on fairness, accountability, and transparencyAndrew D Selbst, Danah Boyd, Sorelle A Friedler, Suresh Venkatasubramanian, and Janet Vertesi. Fairness and abstraction in sociotechnical systems. In Proceedings of the conference on fairness, accountability, and transparency, pages 59-68, 2019.</p>
<p>Nate Soares, Benja Fallenstein, Stuart Armstrong, Eliezer Yudkowsky, Corrigibility, Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence. Nate Soares, Benja Fallenstein, Stuart Armstrong, and Eliezer Yudkowsky. Corrigibility. In Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.</p>
<p>Bridging the gap: the case for an 'incompletely theorized agreement' on AI policy. Charlotte Stix, M Matthijs, Maas, AI and Ethics. Charlotte Stix and Matthijs M Maas. Bridging the gap: the case for an 'incompletely theorized agreement' on AI policy. AI and Ethics, pages 1-11, 2021.</p>
<p>The Bitter Lesson. Richard Sutton, Richard Sutton. The Bitter Lesson, 2019. URL http://www.incompleteideas.net/ IncIdeas/BitterLesson.html.</p>
<p>Reinforcement learning: An introduction. Richard Sutton, Andrew Barto, MIT pressRichard Sutton and Andrew Barto. Reinforcement learning: An introduction. MIT press, 2018.</p>
<p>Conservative agency via attainable utility preservation. Alexander Matt Turner, Dylan Hadfield-Menell, Prasad Tadepalli, Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. the AAAI/ACM Conference on AI, Ethics, and SocietyAlexander Matt Turner, Dylan Hadfield-Menell, and Prasad Tadepalli. Conservative agency via attainable utility preservation. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pages 385-391, 2020.</p>
<p>Humanaligned artificial intelligence is a multiobjective problem. Peter Vamplew, Richard Dazeley, Cameron Foale, Sally Firmin, Jane Mummery, Ethics and Information Technology. 201Peter Vamplew, Richard Dazeley, Cameron Foale, Sally Firmin, and Jane Mummery. Human- aligned artificial intelligence is a multiobjective problem. Ethics and Information Technology, 20(1):27-40, 2018.</p>
<p>The age of surveillance capitalism: The fight for a human future at the new frontier of power. Shoshana Zuboff, PublicAffairsShoshana Zuboff. The age of surveillance capitalism: The fight for a human future at the new frontier of power. PublicAffairs, 2019.</p>            </div>
        </div>

    </div>
</body>
</html>
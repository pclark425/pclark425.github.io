<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2232 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2232</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2232</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-61.html">extraction-schema-61</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <p><strong>Paper ID:</strong> paper-281218275</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.07945v1.pdf" target="_blank">One Model for All Tasks: Leveraging Efficient World Models in Multi-Task Planning</a></p>
                <p><strong>Paper Abstract:</strong> In heterogeneous multi-task learning, tasks not only exhibit diverse observation and action spaces but also vary substantially in intrinsic difficulty. While conventional multi-task world models like UniZero excel in single-task settings, we find that when handling large-scale heterogeneous environments, gradient conflicts and the loss of model plasticity often constrain their sample and computational efficiency. In this work, we address these challenges from two perspectives: the single learning iteration and the overall learning process. First, we investigate the impact of key design spaces on extending UniZero to multi-task planning. We find that a Mixture-of-Experts (MoE) architecture provides the most substantial performance gains by mitigating gradient conflicts, leading to our proposed model, \textit{ScaleZero}. Second, to dynamically balance the computational load across the learning process, we introduce an online, LoRA-based \textit{dynamic parameter scaling} (DPS) strategy. This strategy progressively integrates LoRA adapters in response to task-specific progress, enabling adaptive knowledge retention and parameter expansion. Empirical evaluations on standard benchmarks such as Atari, DMControl (DMC), and Jericho demonstrate that ScaleZero, relying exclusively on online reinforcement learning with one model, attains performance on par with specialized single-task baselines. Furthermore, when augmented with our dynamic parameter scaling strategy, our method achieves competitive performance while requiring only 80\% of the single-task environment interaction steps. These findings underscore the potential of ScaleZero for effective large-scale multi-task learning. Our code is available at \textcolor{magenta}{https://github.com/opendilab/LightZero}.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2232.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2232.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ScaleZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ScaleZero (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified latent world-model for heterogeneous multi-task planning that replaces UniZero's dense Transformer backbone with a sparse Mixture-of-Experts backbone, uses a ViT encoder (for pixels) and LayerNorm on latents, and is designed to preserve plasticity and improve multi-task transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ScaleZero</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unified world model for multi-task online RL combining a ViT (image) / MLP (state) encoder, a Transformer backbone where FFNs are replaced by sparse Mixture-of-Experts (MoE) layers (one shared expert + multiple non-shared experts with Top-1 gating), and standard LayerNorm on latent states; trained end-to-end with UniZero-style MCTS targets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Mixture-of-Experts (sparse conditional computation with Top-1 gating; shared + non-shared experts) combined with explicit task-conditioning options (task embeddings experimented), ViT encoder</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning — large-scale heterogeneous multi-task learning (Atari26 pixel games, DeepMind Control DMC18 continuous-control, Jericho text-adventure games)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Atari26 multitask ScaleZero mean Human-Normalized Score (HNS) = 0.39 (Table 1); on many individual hard games (e.g., Seaquest) ScaleZero produces substantial gains vs baselines; in DMC18 ScaleZero obtains returns comparable to or higher than single-task UniZero on the majority of tasks; on Jericho ScaleZero achieves returns comparable to single-task UniZero and generally outperforms CALM+OC on most games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Single-task UniZero (ST) mean HNS = 0.38 and median HNS = 0.21 on Atari26 (Table 1); the multitask UniZero baseline (shared dense Transformer) performed poorly in multi-task settings and exhibited catastrophic collapses on hard games (described qualitatively and in Atari8 experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Qualitative: MoE backbone enables increased representational power at near-constant forward-pass compute via sparse routing (Top-1 gating); no explicit FLOPs or wall-time numbers reported for ScaleZero vs UniZero, but model was benchmarked on identical hardware (8x A100) and kept parameter counts comparable by replacing some Transformer blocks with MoE blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>ScaleZero trained purely online; reported to match or surpass single-task UniZero on average while sharing one model across tasks (no explicit uniform-sample-efficiency ratio vs ST reported except for DPS experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Reported positive transfer: single multitask ScaleZero mean HNS on Atari26 surpasses average HNS of 26 independent UniZero ST models, with gains especially on difficult games (e.g., Seaquest); in DMC, ScaleZero learns shared physics priors and control primitives enabling generalization across morphologies; in Jericho, ScaleZero benefits from shared language priors and interaction routines.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Paper includes analyses showing ScaleZero with MoE maintains low dormant neuron ratio and stable latent norm (vs collapse in dense baseline); t-SNE latent clustering shows task separation; MoE expert-selection entropy and Wasserstein distances analyzed to demonstrate expert specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Across Atari26, DMC18, and Jericho, ScaleZero attains average returns comparable to or better than specialized single-task UniZero agents; mean HNS slightly higher but median lower on Atari26 (indicating some hard-exploration tasks still hinder median performance).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Replacing a uniform dense backbone with a sparse MoE backbone in a single unified model preserves plasticity, reduces representational collapse, and enables a single model (ScaleZero) to match or exceed the mean performance of many single-task experts across diverse RL benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Empirical results show task-aligned, conditional pathways (MoE) mitigate gradient conflict and plasticity collapse seen in uniform representations and improve multi-task performance and transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2232.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2232.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniZero (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UniZero (Unified latent world-model baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior unified Transformer-based world model (UniZero) used as the baseline dense, uniform shared-parameter model; when naively extended to heterogeneous multi-task settings it suffers gradient conflicts and plasticity collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UniZero</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Monolithic Transformer-based unified world model that combines representation, dynamics and prediction heads into a single shared network trained end-to-end with MCTS-derived targets; in the paper used both as single-task (ST) strong baseline and as (poorly-performing) naive multi-task (MT) baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>uniform/fixed representations (shared dense Transformer backbone, same computation for all tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning — Atari (pixel), DMC (state), Jericho (text); used primarily as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Single-task UniZero (ST) per-task models used as strong baselines; Table 1 reports UniZero (ST) mean HNS = 0.38 and median = 0.21 on Atari26. The naive multi-task UniZero (shared dense backbone) exhibits catastrophic late training collapse on complex tasks in Atari8 (qualitative + plot evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Naive multi-task UniZero exhibits inefficient learning and plasticity collapse on complex tasks; single-task UniZero (ST) remains strong per-task but using separate models per task is more environment-interaction costly overall.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Single-task UniZero does not provide cross-task transfer; the multi-task variant demonstrates negative transfer/plasticity collapse in heterogeneous settings.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Paper links UniZero multi-task failure to internal dynamics: latent norm inflation and rising dormant neuron ratio, and reduced effective rank.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Multi-task UniZero performs poorly on heterogeneous benchmarks: simpler games converge, but complex/visually distinct games suffer catastrophic performance collapses late in training.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>A uniform shared backbone (UniZero) in heterogeneous multi-task RL can suffer gradient dominance from easy tasks, representational collapse, and plasticity loss, causing catastrophic failures on difficult tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>UniZero exemplifies shortcomings of uniform representations in multi-task settings; the paper uses it as the primary counterexample showing need for task-aligned mechanisms.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2232.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2232.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoE backbone</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture-of-Experts (MoE) Transformer backbone</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sparse conditional-computation Transformer backbone where FFN layers are replaced by MoE layers (one shared expert + multiple non-shared experts), with a learned gating network that routes each token to a small subset (Top-1) of experts — used to create task-differentiated representational pathways.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixture-of-Experts (MoE) backbone</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Each MoE layer contains M experts and a sparse gating network G(x) that selects Top-1 expert per token (plus a shared expert processing all tokens); the selected expert(s) process the token and outputs are combined by gating weights, enabling conditional computation and implicit specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>sparse Mixture-of-Experts with Top-1 gating (conditional computation; shared + non-shared experts), implicit task partitioning via routing</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning multi-task backbone (applied inside ScaleZero; experiments on Atari8/Atari26, DMC18, Jericho)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Ablations show MoE backbone yields the largest and most consistent performance gains across the UniZero design-space exploration; specifically improves performance on hard Atari games like Seaquest and ChopperCommand and prevents plasticity collapse (stable dormant ratio and latent norm).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Compared to dense MLP/FFN Transformer backbone, MoE reduces gradient conflicts and avoids catastrophic late collapse; figures show MoE-based Transformer exhibits fewer gradient conflicts at several components vs MLP baseline (Figure 14).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Qualitative claim: MoE enables greater representational power at near-constant computational cost due to sparse activation (Top-1 gating); implemented with one shared expert + eight non-shared experts in experiments; no FLOPs / exact runtime reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>MoE reduces destructive gradient interference during training, enabling sustained learning on harder tasks (faster / more stable later-stage learning compared to dense baseline); empirical evidence includes steeper and more stable learning curves on difficult tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>MoE supports specialization that preserves shared general priors via the shared expert while non-shared experts capture task-specific dynamics, enabling transfer from simpler to more complex tasks (e.g., visual/dynamics priors in Atari aiding Seaquest).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Detailed analyses: expert-selection entropy decreases over training (specialization), Wasserstein distances between task expert-selection distributions show task separation, shared expert carries most gradient conflict while non-shared experts show low conflict, and theoretical bounds on expected gradient conflict are provided (uniform exploration and router-learning stages).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>MoE backbone is central to ScaleZero's success: across multi-task experiments it keeps dormant neuron ratio near zero and stabilizes latent norms, enabling matching or surpassing single-task baselines on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Sparse conditional MoE routing creates task-differentiated computational pathways that substantially reduce gradient conflict and prevent representational collapse, improving multi-task performance vs uniform dense backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Empirical and theoretical analyses show MoE's routing reduces full-layer gradient conflict and enables expert specialization, supporting the benefit of task-aligned adaptive representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2232.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2232.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPS / ScaleZero-DPS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Parameter Scaling (DPS) / ScaleZero-DPS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online curriculum-style capacity-expansion strategy that dynamically injects LoRA low-rank adapter modules in staged expansions triggered by task progress and freezes prior parameters, allocating compute and training only to 'active' unsolved tasks to improve sample and computational efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ScaleZero-DPS (ScaleZero with Dynamic Parameter Scaling)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Training protocol with S+1 stages: warm-up stage trains backbone; each expansion stage adds an independent LoRA module (∆θ_s = B_s A_s) and scalar α_s, freezes backbone and previous LoRAs, and trains only the new LoRA and scaling factors; active set U_t contains unsolved tasks and data collection/training focuses on U_t; stage transitions are progress- or budget-driven.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>staged, additive Low-Rank Adaptation (LoRA) modules with active-set based training and freezing; tasks judged 'solved' are removed from the active set so compute focuses on unsolved tasks</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-task reinforcement learning (prototyped/evaluated on DeepMind Control DMC18; intended for Atari and others)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>On DMC18, ScaleZero-DPS achieves the target performance using approximately 80% of the environment interaction steps required by standard ScaleZero, showing comparable final performance while saving interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Compared to standard ScaleZero (uniform compute per stage), ScaleZero-DPS reaches similar performance with ≈20% fewer environment steps; compared to single-task baselines, the paper claims competitive performance while reducing total interactions (text states 'requiring only 80% of the single-task environment interaction steps' in some descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Mechanistic efficiency: freezing backbone and prior LoRAs means only small low-rank modules and scalars are trained in expansion stages, reducing optimization and gradient cost; empirical efficiency: ~20% reduction in environment interaction steps on DMC18 to reach comparable performance. No FLOPs / GPU-time numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>ScaleZero-DPS reaches target performance with ≈80% of interaction steps relative to standard ScaleZero on DMC18 (≈20% fewer environment interactions) and shows steeper learning curves (faster early learning) on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>By freezing backbone and prior LoRAs, DPS aims to preserve previously learned capabilities while adding targeted capacity; paper argues this reduces negative transfer and enables targeted plasticity, but broader generalization results beyond DMC18 prototype are left for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>DPS design is interpretable in terms of active set dynamics and stage-wise parameter composition; no explicit visualization beyond learning curves and early-stopping marks (asterisks) shown in Figure 11.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>ScaleZero-DPS maintains competitive multi-task performance while reducing total environment interactions; DPS dynamically focuses compute on unsolved tasks, enabling efficient multi-task mastery in the DMC18 prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Direct resource-constrained finding: achieves comparable performance with approximately 80% of the environment interactions (≈20% savings) on the DMC18 benchmark; additionally, training fewer parameters per stage reduces instantaneous optimization cost (no explicit compute numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Dynamically allocating capacity via staged LoRA injection and freezing unused tasks yields significant sample-efficiency gains (~20% fewer environment interactions) while preserving final performance, showing that adaptive parameter expansion is an effective alternative to uniform static allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>DPS empirically demonstrates that adaptive, task-progress-driven parameter allocation improves sample/computational efficiency relative to static uniform allocation, consistent with the Task-Aligned Abstraction Principle.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2232.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2232.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoRA adapters</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-Rank Adaptation (LoRA) modules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parameter-efficient low-rank update modules (ΔW = B A) used to adapt frozen backbone weights cheaply; in this work LoRA modules are injected online as staged capacity additions and trained while earlier modules are frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LoRA (Low-Rank Adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Adaptation technique that represents weight updates as low-rank matrices (ΔW = B A) with small rank r, controlled by a scalar α, enabling efficient task- or stage-specific parameter additions by freezing the original weights and only training the low-rank factors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>parameter-composition via low-rank adapters added per expansion stage (staged LoRA injection), combined with active-set training and scalar re-weighting</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used in multi-task RL training as the mechanism for staged capacity expansion (DPS) and as potential per-task adapters; experiments apply LoRA within ScaleZero-DPS on DMC18.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>When used within DPS, LoRA modules enable reaching target performance with ≈80% of interactions vs standard ScaleZero on DMC18; LoRA allows targeted plasticity for difficult tasks without disrupting frozen backbone knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>LoRA modules are parameter-efficient (rank r ≪ full matrix dimensions): hyperparameter lora_r = 64 used in experiments; freezing backbone reduces optimization cost since only LoRA matrices and scalars are trained in expansion stages.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Enabling staged LoRA insertion in DPS yields ≈20% reduction in environment interactions to reach a given performance level on DMC18; LoRA isolates updates and reduces negative transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>LoRA-based staged expansion preserves previously learned capabilities and allows new capacity to specialize for remaining unsolved tasks, promoting positive transfer and preventing catastrophic forgetting (qualitative/empirical claims).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>LoRA used as modular per-stage adapters leads to maintained or improved multi-task performance while controlling optimization cost; paper emphasizes the synergy between MoE (coarse-grained routing) and LoRA (fine-grained, stage-wise adaptation).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>LoRA provides a lightweight, trainable parameter bank that can be added online to expand capacity in a task-targeted manner, enabling efficient adaptation without retraining/fine-tuning the entire backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>LoRA's staged usage in DPS empirically supports the idea that targeted, adaptive parameter additions improve efficiency and reduce negative transfer versus monolithic uniform updates.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2232.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2232.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoCo (gradient correction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MoCo (Multi-task gradient correction / dynamic gradient re-weighting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic gradient re-weighting scheme (referred to as MoCo in the paper, adapted from Fernando et al., 2023 / LibMTL) used to mitigate gradient interference by maintaining per-task momentum gradient directions and solving a regularized optimization for task weights; tested but found computationally heavy and inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoCo (multi-task gradient correction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Optimization-level intervention that tracks task-specific momentum gradients y_t, updates task weights λ by solving a regularized optimization, and uses a weighted sum of momentum gradients for the shared parameter update to reduce conflicts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>optimization-level gradient re-weighting (task-level re-weighting based on momentum estimates), not an architectural task-aligned representation</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-task reinforcement learning optimization (evaluated within ablations on Atari8)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Provided some gains on some tasks, but the paper reports significant computational overhead and inconsistent efficacy across the heterogeneous task suite; thus it was not adopted as the primary solution.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Reported significant computational overhead compared to architectural solutions (MoE); no exact runtime or FLOP numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Inconsistent improvements across tasks; not as broadly effective as MoE backbone in preventing plasticity collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Optimization-level gradient correction can reduce interference for some tasks but incurs heavy computational overhead and inconsistent benefits compared to architectural conditional computation (MoE).</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>While gradient re-weighting addresses symptoms of cross-task conflict, the paper finds it less effective and efficient than architectural task-aligned mechanisms (MoE + DPS), suggesting structural specialization is a more scalable remedy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>UniZero: Generalized and efficient planning with scalable latent world models <em>(Rating: 2)</em></li>
                <li>The sparsely-gated mixture-of-experts layer <em>(Rating: 2)</em></li>
                <li>Low-Rank Adaptation of Large Language Models <em>(Rating: 2)</em></li>
                <li>Mitigating gradient bias in multi-objective learning <em>(Rating: 1)</em></li>
                <li>Mixtures of experts unlock parameter scaling for deep rl <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2232",
    "paper_id": "paper-281218275",
    "extraction_schema_id": "extraction-schema-61",
    "extracted_data": [
        {
            "name_short": "ScaleZero",
            "name_full": "ScaleZero (this paper)",
            "brief_description": "A unified latent world-model for heterogeneous multi-task planning that replaces UniZero's dense Transformer backbone with a sparse Mixture-of-Experts backbone, uses a ViT encoder (for pixels) and LayerNorm on latents, and is designed to preserve plasticity and improve multi-task transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ScaleZero",
            "model_description": "Unified world model for multi-task online RL combining a ViT (image) / MLP (state) encoder, a Transformer backbone where FFNs are replaced by sparse Mixture-of-Experts (MoE) layers (one shared expert + multiple non-shared experts with Top-1 gating), and standard LayerNorm on latent states; trained end-to-end with UniZero-style MCTS targets.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Mixture-of-Experts (sparse conditional computation with Top-1 gating; shared + non-shared experts) combined with explicit task-conditioning options (task embeddings experimented), ViT encoder",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Reinforcement learning — large-scale heterogeneous multi-task learning (Atari26 pixel games, DeepMind Control DMC18 continuous-control, Jericho text-adventure games)",
            "performance_task_aligned": "Atari26 multitask ScaleZero mean Human-Normalized Score (HNS) = 0.39 (Table 1); on many individual hard games (e.g., Seaquest) ScaleZero produces substantial gains vs baselines; in DMC18 ScaleZero obtains returns comparable to or higher than single-task UniZero on the majority of tasks; on Jericho ScaleZero achieves returns comparable to single-task UniZero and generally outperforms CALM+OC on most games.",
            "performance_uniform_baseline": "Single-task UniZero (ST) mean HNS = 0.38 and median HNS = 0.21 on Atari26 (Table 1); the multitask UniZero baseline (shared dense Transformer) performed poorly in multi-task settings and exhibited catastrophic collapses on hard games (described qualitatively and in Atari8 experiments).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Qualitative: MoE backbone enables increased representational power at near-constant forward-pass compute via sparse routing (Top-1 gating); no explicit FLOPs or wall-time numbers reported for ScaleZero vs UniZero, but model was benchmarked on identical hardware (8x A100) and kept parameter counts comparable by replacing some Transformer blocks with MoE blocks.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "ScaleZero trained purely online; reported to match or surpass single-task UniZero on average while sharing one model across tasks (no explicit uniform-sample-efficiency ratio vs ST reported except for DPS experiments).",
            "transfer_generalization_results": "Reported positive transfer: single multitask ScaleZero mean HNS on Atari26 surpasses average HNS of 26 independent UniZero ST models, with gains especially on difficult games (e.g., Seaquest); in DMC, ScaleZero learns shared physics priors and control primitives enabling generalization across morphologies; in Jericho, ScaleZero benefits from shared language priors and interaction routines.",
            "interpretability_results": "Paper includes analyses showing ScaleZero with MoE maintains low dormant neuron ratio and stable latent norm (vs collapse in dense baseline); t-SNE latent clustering shows task separation; MoE expert-selection entropy and Wasserstein distances analyzed to demonstrate expert specialization.",
            "multi_task_performance": "Across Atari26, DMC18, and Jericho, ScaleZero attains average returns comparable to or better than specialized single-task UniZero agents; mean HNS slightly higher but median lower on Atari26 (indicating some hard-exploration tasks still hinder median performance).",
            "resource_constrained_results": null,
            "key_finding_summary": "Replacing a uniform dense backbone with a sparse MoE backbone in a single unified model preserves plasticity, reduces representational collapse, and enables a single model (ScaleZero) to match or exceed the mean performance of many single-task experts across diverse RL benchmarks.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Empirical results show task-aligned, conditional pathways (MoE) mitigate gradient conflict and plasticity collapse seen in uniform representations and improve multi-task performance and transfer.",
            "uuid": "e2232.0"
        },
        {
            "name_short": "UniZero (baseline)",
            "name_full": "UniZero (Unified latent world-model baseline)",
            "brief_description": "Prior unified Transformer-based world model (UniZero) used as the baseline dense, uniform shared-parameter model; when naively extended to heterogeneous multi-task settings it suffers gradient conflicts and plasticity collapse.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "UniZero",
            "model_description": "Monolithic Transformer-based unified world model that combines representation, dynamics and prediction heads into a single shared network trained end-to-end with MCTS-derived targets; in the paper used both as single-task (ST) strong baseline and as (poorly-performing) naive multi-task (MT) baseline.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "uniform/fixed representations (shared dense Transformer backbone, same computation for all tasks)",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Reinforcement learning — Atari (pixel), DMC (state), Jericho (text); used primarily as baseline",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Single-task UniZero (ST) per-task models used as strong baselines; Table 1 reports UniZero (ST) mean HNS = 0.38 and median = 0.21 on Atari26. The naive multi-task UniZero (shared dense backbone) exhibits catastrophic late training collapse on complex tasks in Atari8 (qualitative + plot evidence).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Naive multi-task UniZero exhibits inefficient learning and plasticity collapse on complex tasks; single-task UniZero (ST) remains strong per-task but using separate models per task is more environment-interaction costly overall.",
            "transfer_generalization_results": "Single-task UniZero does not provide cross-task transfer; the multi-task variant demonstrates negative transfer/plasticity collapse in heterogeneous settings.",
            "interpretability_results": "Paper links UniZero multi-task failure to internal dynamics: latent norm inflation and rising dormant neuron ratio, and reduced effective rank.",
            "multi_task_performance": "Multi-task UniZero performs poorly on heterogeneous benchmarks: simpler games converge, but complex/visually distinct games suffer catastrophic performance collapses late in training.",
            "resource_constrained_results": null,
            "key_finding_summary": "A uniform shared backbone (UniZero) in heterogeneous multi-task RL can suffer gradient dominance from easy tasks, representational collapse, and plasticity loss, causing catastrophic failures on difficult tasks.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "UniZero exemplifies shortcomings of uniform representations in multi-task settings; the paper uses it as the primary counterexample showing need for task-aligned mechanisms.",
            "uuid": "e2232.1"
        },
        {
            "name_short": "MoE backbone",
            "name_full": "Mixture-of-Experts (MoE) Transformer backbone",
            "brief_description": "Sparse conditional-computation Transformer backbone where FFN layers are replaced by MoE layers (one shared expert + multiple non-shared experts), with a learned gating network that routes each token to a small subset (Top-1) of experts — used to create task-differentiated representational pathways.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixture-of-Experts (MoE) backbone",
            "model_description": "Each MoE layer contains M experts and a sparse gating network G(x) that selects Top-1 expert per token (plus a shared expert processing all tokens); the selected expert(s) process the token and outputs are combined by gating weights, enabling conditional computation and implicit specialization.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "sparse Mixture-of-Experts with Top-1 gating (conditional computation; shared + non-shared experts), implicit task partitioning via routing",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Reinforcement learning multi-task backbone (applied inside ScaleZero; experiments on Atari8/Atari26, DMC18, Jericho)",
            "performance_task_aligned": "Ablations show MoE backbone yields the largest and most consistent performance gains across the UniZero design-space exploration; specifically improves performance on hard Atari games like Seaquest and ChopperCommand and prevents plasticity collapse (stable dormant ratio and latent norm).",
            "performance_uniform_baseline": "Compared to dense MLP/FFN Transformer backbone, MoE reduces gradient conflicts and avoids catastrophic late collapse; figures show MoE-based Transformer exhibits fewer gradient conflicts at several components vs MLP baseline (Figure 14).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Qualitative claim: MoE enables greater representational power at near-constant computational cost due to sparse activation (Top-1 gating); implemented with one shared expert + eight non-shared experts in experiments; no FLOPs / exact runtime reported.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "MoE reduces destructive gradient interference during training, enabling sustained learning on harder tasks (faster / more stable later-stage learning compared to dense baseline); empirical evidence includes steeper and more stable learning curves on difficult tasks.",
            "transfer_generalization_results": "MoE supports specialization that preserves shared general priors via the shared expert while non-shared experts capture task-specific dynamics, enabling transfer from simpler to more complex tasks (e.g., visual/dynamics priors in Atari aiding Seaquest).",
            "interpretability_results": "Detailed analyses: expert-selection entropy decreases over training (specialization), Wasserstein distances between task expert-selection distributions show task separation, shared expert carries most gradient conflict while non-shared experts show low conflict, and theoretical bounds on expected gradient conflict are provided (uniform exploration and router-learning stages).",
            "multi_task_performance": "MoE backbone is central to ScaleZero's success: across multi-task experiments it keeps dormant neuron ratio near zero and stabilizes latent norms, enabling matching or surpassing single-task baselines on many tasks.",
            "resource_constrained_results": null,
            "key_finding_summary": "Sparse conditional MoE routing creates task-differentiated computational pathways that substantially reduce gradient conflict and prevent representational collapse, improving multi-task performance vs uniform dense backbones.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Empirical and theoretical analyses show MoE's routing reduces full-layer gradient conflict and enables expert specialization, supporting the benefit of task-aligned adaptive representations.",
            "uuid": "e2232.2"
        },
        {
            "name_short": "DPS / ScaleZero-DPS",
            "name_full": "Dynamic Parameter Scaling (DPS) / ScaleZero-DPS",
            "brief_description": "An online curriculum-style capacity-expansion strategy that dynamically injects LoRA low-rank adapter modules in staged expansions triggered by task progress and freezes prior parameters, allocating compute and training only to 'active' unsolved tasks to improve sample and computational efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ScaleZero-DPS (ScaleZero with Dynamic Parameter Scaling)",
            "model_description": "Training protocol with S+1 stages: warm-up stage trains backbone; each expansion stage adds an independent LoRA module (∆θ_s = B_s A_s) and scalar α_s, freezes backbone and previous LoRAs, and trains only the new LoRA and scaling factors; active set U_t contains unsolved tasks and data collection/training focuses on U_t; stage transitions are progress- or budget-driven.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "staged, additive Low-Rank Adaptation (LoRA) modules with active-set based training and freezing; tasks judged 'solved' are removed from the active set so compute focuses on unsolved tasks",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Multi-task reinforcement learning (prototyped/evaluated on DeepMind Control DMC18; intended for Atari and others)",
            "performance_task_aligned": "On DMC18, ScaleZero-DPS achieves the target performance using approximately 80% of the environment interaction steps required by standard ScaleZero, showing comparable final performance while saving interactions.",
            "performance_uniform_baseline": "Compared to standard ScaleZero (uniform compute per stage), ScaleZero-DPS reaches similar performance with ≈20% fewer environment steps; compared to single-task baselines, the paper claims competitive performance while reducing total interactions (text states 'requiring only 80% of the single-task environment interaction steps' in some descriptions).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Mechanistic efficiency: freezing backbone and prior LoRAs means only small low-rank modules and scalars are trained in expansion stages, reducing optimization and gradient cost; empirical efficiency: ~20% reduction in environment interaction steps on DMC18 to reach comparable performance. No FLOPs / GPU-time numbers provided.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "ScaleZero-DPS reaches target performance with ≈80% of interaction steps relative to standard ScaleZero on DMC18 (≈20% fewer environment interactions) and shows steeper learning curves (faster early learning) on many tasks.",
            "transfer_generalization_results": "By freezing backbone and prior LoRAs, DPS aims to preserve previously learned capabilities while adding targeted capacity; paper argues this reduces negative transfer and enables targeted plasticity, but broader generalization results beyond DMC18 prototype are left for future work.",
            "interpretability_results": "DPS design is interpretable in terms of active set dynamics and stage-wise parameter composition; no explicit visualization beyond learning curves and early-stopping marks (asterisks) shown in Figure 11.",
            "multi_task_performance": "ScaleZero-DPS maintains competitive multi-task performance while reducing total environment interactions; DPS dynamically focuses compute on unsolved tasks, enabling efficient multi-task mastery in the DMC18 prototype.",
            "resource_constrained_results": "Direct resource-constrained finding: achieves comparable performance with approximately 80% of the environment interactions (≈20% savings) on the DMC18 benchmark; additionally, training fewer parameters per stage reduces instantaneous optimization cost (no explicit compute numbers).",
            "key_finding_summary": "Dynamically allocating capacity via staged LoRA injection and freezing unused tasks yields significant sample-efficiency gains (~20% fewer environment interactions) while preserving final performance, showing that adaptive parameter expansion is an effective alternative to uniform static allocation.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "DPS empirically demonstrates that adaptive, task-progress-driven parameter allocation improves sample/computational efficiency relative to static uniform allocation, consistent with the Task-Aligned Abstraction Principle.",
            "uuid": "e2232.3"
        },
        {
            "name_short": "LoRA adapters",
            "name_full": "Low-Rank Adaptation (LoRA) modules",
            "brief_description": "Parameter-efficient low-rank update modules (ΔW = B A) used to adapt frozen backbone weights cheaply; in this work LoRA modules are injected online as staged capacity additions and trained while earlier modules are frozen.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LoRA (Low-Rank Adaptation)",
            "model_description": "Adaptation technique that represents weight updates as low-rank matrices (ΔW = B A) with small rank r, controlled by a scalar α, enabling efficient task- or stage-specific parameter additions by freezing the original weights and only training the low-rank factors.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "parameter-composition via low-rank adapters added per expansion stage (staged LoRA injection), combined with active-set training and scalar re-weighting",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Used in multi-task RL training as the mechanism for staged capacity expansion (DPS) and as potential per-task adapters; experiments apply LoRA within ScaleZero-DPS on DMC18.",
            "performance_task_aligned": "When used within DPS, LoRA modules enable reaching target performance with ≈80% of interactions vs standard ScaleZero on DMC18; LoRA allows targeted plasticity for difficult tasks without disrupting frozen backbone knowledge.",
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": "LoRA modules are parameter-efficient (rank r ≪ full matrix dimensions): hyperparameter lora_r = 64 used in experiments; freezing backbone reduces optimization cost since only LoRA matrices and scalars are trained in expansion stages.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Enabling staged LoRA insertion in DPS yields ≈20% reduction in environment interactions to reach a given performance level on DMC18; LoRA isolates updates and reduces negative transfer.",
            "transfer_generalization_results": "LoRA-based staged expansion preserves previously learned capabilities and allows new capacity to specialize for remaining unsolved tasks, promoting positive transfer and preventing catastrophic forgetting (qualitative/empirical claims).",
            "interpretability_results": null,
            "multi_task_performance": "LoRA used as modular per-stage adapters leads to maintained or improved multi-task performance while controlling optimization cost; paper emphasizes the synergy between MoE (coarse-grained routing) and LoRA (fine-grained, stage-wise adaptation).",
            "resource_constrained_results": null,
            "key_finding_summary": "LoRA provides a lightweight, trainable parameter bank that can be added online to expand capacity in a task-targeted manner, enabling efficient adaptation without retraining/fine-tuning the entire backbone.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "LoRA's staged usage in DPS empirically supports the idea that targeted, adaptive parameter additions improve efficiency and reduce negative transfer versus monolithic uniform updates.",
            "uuid": "e2232.4"
        },
        {
            "name_short": "MoCo (gradient correction)",
            "name_full": "MoCo (Multi-task gradient correction / dynamic gradient re-weighting)",
            "brief_description": "A dynamic gradient re-weighting scheme (referred to as MoCo in the paper, adapted from Fernando et al., 2023 / LibMTL) used to mitigate gradient interference by maintaining per-task momentum gradient directions and solving a regularized optimization for task weights; tested but found computationally heavy and inconsistent.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MoCo (multi-task gradient correction)",
            "model_description": "Optimization-level intervention that tracks task-specific momentum gradients y_t, updates task weights λ by solving a regularized optimization, and uses a weighted sum of momentum gradients for the shared parameter update to reduce conflicts.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "optimization-level gradient re-weighting (task-level re-weighting based on momentum estimates), not an architectural task-aligned representation",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Multi-task reinforcement learning optimization (evaluated within ablations on Atari8)",
            "performance_task_aligned": "Provided some gains on some tasks, but the paper reports significant computational overhead and inconsistent efficacy across the heterogeneous task suite; thus it was not adopted as the primary solution.",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Reported significant computational overhead compared to architectural solutions (MoE); no exact runtime or FLOP numbers provided.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Inconsistent improvements across tasks; not as broadly effective as MoE backbone in preventing plasticity collapse.",
            "resource_constrained_results": null,
            "key_finding_summary": "Optimization-level gradient correction can reduce interference for some tasks but incurs heavy computational overhead and inconsistent benefits compared to architectural conditional computation (MoE).",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "While gradient re-weighting addresses symptoms of cross-task conflict, the paper finds it less effective and efficient than architectural task-aligned mechanisms (MoE + DPS), suggesting structural specialization is a more scalable remedy.",
            "uuid": "e2232.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "UniZero: Generalized and efficient planning with scalable latent world models",
            "rating": 2
        },
        {
            "paper_title": "The sparsely-gated mixture-of-experts layer",
            "rating": 2
        },
        {
            "paper_title": "Low-Rank Adaptation of Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Mitigating gradient bias in multi-objective learning",
            "rating": 1
        },
        {
            "paper_title": "Mixtures of experts unlock parameter scaling for deep rl",
            "rating": 2
        }
    ],
    "cost": 0.021905,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ONE MODEL FOR ALL TASKS: LEVERAGING EFFICIENT WORLD MODELS IN MULTI-TASK PLANNING
9 Sep 2025</p>
<p>Yuan Pu 
Shanghai Artificial Intelligence Laboratory</p>
<p>Yazhe Niu 
Shanghai Artificial Intelligence Laboratory</p>
<p>The Chinese University of Hong Kong</p>
<p>Jia Tang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Nanjing University of Aeronautics</p>
<p>Junyu Xiong 
Shanghai Artificial Intelligence Laboratory</p>
<p>University of Science and Technology of China</p>
<p>Shuai Hu 
Novosibirsk State University</p>
<p>Hongsheng Li 
The Chinese University of Hong Kong</p>
<p>Centre for Perceptual and Interactive Intelligence</p>
<p>ONE MODEL FOR ALL TASKS: LEVERAGING EFFICIENT WORLD MODELS IN MULTI-TASK PLANNING
9 Sep 20259E85AEFA666F764DCFBF6B6C09C6181BarXiv:2509.07945v1[cs.LG]
In heterogeneous multi-task learning, tasks not only exhibit diverse observation and action spaces but also vary substantially in intrinsic difficulty.While conventional multi-task world models like UniZero excel in single-task settings, we find that when handling large-scale heterogeneous environments, gradient conflicts and the loss of model plasticity often constrain their sample and computational efficiency.In this work, we address these challenges from two perspectives: the single learning iteration and the overall learning process.First, we investigate the impact of key design spaces on extending UniZero to multi-task planning.We find that a Mixture-of-Experts (MoE) architecture provides the most substantial performance gains by mitigating gradient conflicts, leading to our proposed model, ScaleZero.Second, to dynamically balance the computational load across the learning process, we introduce an online, LoRA-based dynamic parameter scaling (DPS) strategy.This strategy progressively integrates LoRA adapters in response to task-specific progress, enabling adaptive knowledge retention and parameter expansion.Empirical evaluations on standard benchmarks such as Atari, DMControl (DMC), and Jericho demonstrate that ScaleZero, relying exclusively on online reinforcement learning with one model, attains performance on par with specialized single-task baselines.Furthermore, when augmented with our dynamic parameter scaling strategy, our method achieves competitive performance while requiring only 80% of the single-task environment interaction steps.These findings underscore the potential of ScaleZero for effective large-scale multi-task learning.Our code is available at https://github.com/opendilab/LightZero.</p>
<p>INTRODUCTION</p>
<p>Unified world models (Reed et al., 2022;Gallouédec et al., 2024b;Lee et al., 2022b) represent a significant step towards generalist agents, offering a single, cohesive framework for multi-modal perception, long-horizon prediction, and decision-making by learning a shared latent space representation.The true power of these models is unlocked when combined with search-based planning algorithms like Monte Carlo Tree Search (MCTS) (Silver et al., 2017;Schrittwieser et al., 2019), which can perform efficient lookahead within this compact latent space.MCTS excels at dynamically balancing exploration and exploitation in complex decision spaces, a combination that has achieved well-established success in homogeneous task domains like board games (Ye et al., 2021;Silver et al., 2017;Schrittwieser et al., 2019).The natural next frontier is to extend this success to heterogeneous multi-task reinforcement learning (MTRL) (Yu et al., 2024), aiming to build a single, generalist agent that can master a wide array of diverse environments.However, this ambition is impeded by a formidable obstacle: training a single shared model on diverse tasks with potentially conflicting dynamics and objectives is notoriously difficult.While prior work (Fernando et al., 2023;Shi et al., 2023) has attempted to mitigate inter-task interference, these methods have been predominantly studied in supervised learning settings.The challenges and dynamics of multi-task planning within a shared world model remain largely unexplored.</p>
<p>Preprint</p>
<p>Our investigation into this specific domain of multi-task planning identifies and addresses two critical, intertwined obstacles that are not fully resolved by existing approaches: (1) Representational bottlenecks and plasticity collapse: In diverse multi-task settings, a shared model is susceptible to gradient dominance from simpler, faster-converging tasks (Cho et al., 2024).This imbalance leads to representation interference (Yu et al., 2020;Bejnordi et al., 2024), where the learning signals for more complex tasks are suppressed.The consequence is a progressive loss of network plasticity (Dohare et al., 2024;Todorov et al., 2025)-the fundamental ability of a model to adjust its parameters to learn from new data-imposing a hard ceiling on the model's overall learning capacity and leading to performance collapse on challenging tasks.(2) Static resource allocation: Conventional architectures employ a uniform, one-size-fits-all forward pass, applying the same computational effort to every task irrespective of its intrinsic difficulty.This static strategy results in profound computational inefficiency, as resources are squandered on already-mastered tasks instead of being directed toward those that require further learning.</p>
<p>To dissect these issues, we first conduct a rigorous empirical diagnosis in Figure 1.By deploying a baseline UniZero model across eight canonical Atari games, we observe a stark failure pattern.While the model rapidly masters simple tasks like Pong, it exhibits initial progress followed by a catastrophic performance collapse on complex, visually distinct games like Seaquest and ChopperCommand.We provide the quantitative evidence linking this failure to specific internal model dynamics: an uncontrolled inflation of the latent state norm and a corresponding spike in the dormant neuron ratio within the Transformer backbone, signaling a descent into a "frozen," non-adaptive state.</p>
<p>Motivated by this diagnosis, we undertake a principled exploration of the UniZero design space across the axes of input, model, and optimization from the the single learning iteration perspective.This systematic investigation, which evaluates task embeddings, encoder architectures (ResNet (He et al., 2016) vs. ViT (Dosovitskiy et al., 2020)), latent normalization schemes (Hansen et al., 2023), sparse Mixture-of-Experts (MoE) backbones, and gradient rectification methods (He et al., 2020), culminates in a new, powerful model we term ScaleZero.Validated on a comprehensive suite of online RL benchmarks-spanning 26 Atari games, 18 DeepMind Control tasks, and 4 Jericho environments with different observation and action space-ScaleZero consistently matches and often surpasses the performance of specific single-task expert agents.</p>
<p>Finally, to resolve the challenge of computational inefficiency, we introduce Dynamic Parameter Scaling (DPS) from the overall learning process perspective, a novel online mechanism that couples model capacity to learning progress.DPS adaptively curates the set of active tasks based on real-time return feedback and orchestrates a phased expansion of model capacity by injecting lightweight LoRA adapters.By strategically freezing previously trained parameters, DPS creates a curriculum of model complexity that directs computational resources where they are most needed.Our experiments demonstrate that when augmented with our dynamic parameter scaling strategy, our method achieves performance nearly on par with single-task agents while reducing total environment interactions by around 20%, offering a superior trade-off between final performance and computational budget.</p>
<p>Our main contributions are summarized as follows: (1) We provide the quantitative diagnosis of plasticity collapse in unified world models within heterogeneous MTRL, establishing a concrete link between performance degradation and internal network dynamics.(2) We conduct a systematic architectural exploration that yields ScaleZero, a unified world model that demonstrates exceptional performance and generalization across distinct tasks from Atari (diverse visual challenges), DMControl (state-based continuous control), and Jericho (text-based compositional reasoning) benchmark.</p>
<p>(3) We propose Dynamic Parameter Scaling (DPS), an adaptive training strategy that dynamically allocates model capacity and computational resources, reducing total environment interactions by around 20%.</p>
<p>BACKGROUND 2.1 UNIFIED WORLD MODELS IN MCTS-BASED REINFORCEMENT LEARNING</p>
<p>Model-based reinforcement learning, particularly methods employing MCTS for planning within a learned latent space, has set the standard in complex sequential decision-making domains (Schrittwieser et al., 2019;Antonoglou et al., 2021;Ye et al., 2021).These methods learn a world model composed of three core components: (i) a representation model, z t = h θ (o t−H+1:t , a t−H+1:t−1 ), which encodes an observation history into a latent state, where H is the training sequence length; Preprint (ii) a dynamics model, (ẑ t+1 , rt ) = g θ (z t , a t ), predicting the subsequent latent state and reward; and (iii) a prediction model, (p t , v t ) = f θ (z t ), which estimates a policy and value to guide the MCTS planner.Recent architectures like UniZero (Pu et al., 2024) push this paradigm further by unifying these three components into a single Transformer-based architecture.This monolithic design allows for end-to-end optimization via a composite loss function that aligns the model's predictions with MCTS-derived targets for policy (π t ) and value (v t ), alongside objectives for reward and state reconstruction:
L UniZero = H−1 t=0
(L value (v t , vt ) + L policy (p t , π t ) + L reward (r t , r t ) + L dynamics (ẑ t+1 , sg(z t+1 ))) , (1) where sg(•) denotes the stop-gradient operator.While parameter-efficient, this unified training scheme creates a critical vulnerability in heterogeneous settings: the shared parameters are subject to a single, aggregated gradient signal, which can be dominated by easier tasks (Ma et al., 2023), thereby impeding learning on more complex ones and leading to plasticity collapse.</p>
<p>GRADIENT CONFLICT IN MULTI-TASK REINFORCEMENT LEARNING</p>
<p>In MTRL, we consider a distribution of K distinct tasks, T = {τ 1 , . . ., τ K }.The objective is to learn a single set of shared parameters θ for a task-conditioned policy π θ (a|s, k) that maximizes expected returns across all tasks, where k ∈ {1, . . ., K} is the task identifier.Given a batch of data from all tasks, the total loss is the sum of the individual task losses:
L MTRL (θ) = K k=1 L k UniZero , where L k
UniZero is the loss computed on data from task k.The learning dynamics are driven by the total gradient G total = ∇ θ L MTRL , which is the sum of per-task gradients:
G total = K k=1 g k , where g k = ∇ θ L k
UniZero .The core problem, known as gradient conflict, arises when these per-task gradients are misaligned.For any two tasks i and j, the conflict can be quantified by the cosine similarity of their gradients: cos(g i , g j ) = gi•gj ∥gi∥∥gj ∥ .A negative value indicates that an update improving performance on task i will degrade performance on task j.In a monolithic architecture where all parameters θ are shared, frequent negative similarities lead to destructive interference (Sodhani et al., 2021).This forces the model into a suboptimal compromise, creating a representational bottleneck and impeding mastery of tasks with conflicting or complex objectives (Hansen et al., 2023).</p>
<p>SPARSE AND PARAMETER-EFFICIENT ARCHITECTURES</p>
<p>A significant challenge in developing unified models for heterogeneous environments is overcoming issues such as representational bottlenecks, plasticity collapse, and the rigidity imposed by static resource allocation.To address these challenges, the literature has explored several sparse and parameter-efficient architectures that operate at different perspectives.One prominent approach involves replacing the standard feed-forward network in a Transformer block (Vaswani et al., 2017) with a sparse Mixture-of-Experts architecture (Shazeer et al., 2017;Liu et al., 2024a).This model comprises M expert networks and a trainable sparse gating function, G(x), which selects a small top-k subset of experts for each input.The forward pass is defined as:
MoE(x) = M i=1 G i (x) • Expert i (x)
. This conditional computation creates distinct representational pathways for different inputs and significantly improve model performance and efficiency with fewer resources.For parameter-efficient adaptation of large models, Low-Rank Adaptation (LoRA) (Hu et al., 2022) has become a widely adopted technique.LoRA adapts a pre-trained weight matrix W 0 ∈ R d×m by learning a low-rank update, ∆W = BA, where B ∈ R d×r and A ∈ R r×m , with the rank r ≪ min(d, m).The effective weight matrix then operates on an input x as: W (x) = W 0 + α BA x, where a learnable scalar α controls the magnitude of the update.By freezing the original weights W 0 and training only the compact low-rank factors (A, B), LoRA facilitates efficient adaptation and isolates task-specific modifications.In this context, our work fuses these ideas from two perspectives.At the single learning iteration level, a gating function sparsely selects LoRA modules, treating them as experts.At the overall learning process level, we incrementally add new LoRA modules for new tasks-freezing prior ones-to efficiently scale model capacity.</p>
<p>METHOD</p>
<p>This section systematically explores the core challenges that unified world models face in heterogeneous multi-task learning.We begin in Section 3.1 by quantitatively diagnosing the phenomenon of plasticity collapse in shared-parameter models under a multi-task paradigm, uncovering its underlying mechanisms.Subsequently, in Section 3.2, we systematically dissect the design space of the UniZero model.Through a series of ablation studies, we converge on a new architecture, termed ScaleZero, which effectively mitigates plasticity degradation.Finally, to address the inefficiency of static resource allocation, we introduce Dynamic Parameter Scaling (DPS) in Section 3.3.This strategy adaptively expands the model's capacity in response to task-specific learning progress, thereby achieving a superior trade-off between computational efficiency and final performance.Metrics.In MTRL settings, shared-parameter models are susceptible to a freezing phenomenon, where parts of the network become saturated and lose their ability to adapt to new information.This plasticity collapse stems from the inherent heterogeneity across tasks in terms of their features, difficulty, and informational content, ultimately constraining the model's overall learning potential.To quantitatively measure this degradation in network adaptability, we utilize two key metrics: (1) Dormant Neuron Ratio (Sokar et al., 2023): This metric measures the proportion of neurons in a given layer whose activation magnitude falls below a predefined threshold ϵ (set to 0.025 in this work) over a time series.A high dormant ratio indicates that a significant portion of the network's pathways are inactive and no longer contribute to information processing.DormantRatio(l) =</p>
<p>DIAGNOSING PLASTICITY COLLAPSE IN MULTI-TASK LEARNING
1 N l N l i=1 1 |h l i | ≤ ϵ ,
where h l i is the activation of the i-th neuron in layer l, and N l is the total number of neurons in that layer.(2) Latent State Norm: This metric computes the average L2 norm of the latent state vectors z t produced by the encoder over a time series.An abnormally inflated latent norm is often correlated with training instability and slower learning.As our model processes sequential data in mini-batches, we average this norm across both the training sequence length T and the mini-batch size B.</p>
<p>Analysis.</p>
<p>Our experiments on a multi-task suite of 8 Atari games (see Appendix A) reveal a clear failure pattern: while simple tasks (e.g., Pong) converge stably, complex tasks (e.g., Seaquest) suffer a catastrophic performance collapse late in training (Figure 1).This collapse precisely corresponds to a sharp increase in the dormant neuron ratio and an uncontrolled inflation of the latent state norm as shown in bottom of Figure 1.In addition, we have added the trend of the feature effective rank Dohare et al. (2024) and training return in Appendix Figure 6, which also shows a marked decline in the performance degradation range.Thus, we attribute this plasticity loss to two intertwined factors:</p>
<p>(1) Gradient Competition, where simpler, faster-converging tasks generate dominant gradients that suppress signals from more complex tasks, causing critical neural pathways to "go dormant"; and (2) Representation Interference, where a single, shared model struggles to encode diverse task dynamics, creating a representational bottleneck that constricts the solution space.</p>
<p>Motivated by this diagnosis, we propose to mitigate these issues by introducing task-differentiated mechanisms from two distinct perspectives.First, from the perspective of a single learning iteration, we investigate modifications that resolve interference within each forward and backward pass, leading to the design of ScaleZero (Section 3.2).Second, from the perspective of the overall learning process, we introduce a novel training strategy that dynamically allocates model capacity over time, we named as Dynamic Parameter Scaling strategy (Section 3.3).Based on the diagnosis above, we conducted a systematic decomposition and evaluation of the UniZero model's design space to identify architectural components that can effectively counteract plasticity loss.We explored five key dimensions, as illustrated in Figure 2 (left): task information, encoder, latent normalization, backbone, and optimization strategy.The experimental details is showed in Appendix B. Figure 3 provides a overall comparison of the performance contributions from each design dimension.A key observation is that the MoE backbone yields the most significant performance gains.In contrast, other modifications, with the partial exception of SimNorm, fail to deliver stable performance improvements.To investigate the mechanisms behind these results, Figure 4 illustrates the relationship between task returns and plasticity-related metrics (i.e., transformer dormant ratio and latent norm) for two representative tasks, ChopperCommand and Seaquest.</p>
<p>EXPLORING THE DESIGN SPACE OF UNIZERO</p>
<p>Analyses for the remaining six environments are provided in Appendix B. The key findings from this exploration are detailed below:</p>
<p>Explicit Task Conditioning.We experimented with concatenating a learnable task embedding indexed by task ID to the state representation to provide an explicit task identity.While this accelerated initial convergence on simpler tasks, its long-term benefit for complex tasks was marginal, and it failed to significantly improve internal plasticity metrics (Figure 4).This suggests that shallow task identifiers are insufficient to resolve deep-seated representational conflicts in online RL setting.</p>
<p>Encoder Architecture (ResNet vs. ViT).We compared a standard ResNet encoder against a ViT to assess the impact of encoder architecture.In our experiments on the Atari8 benchmark, the choice of encoder did not yield a significant performance advantage.We hypothesize that this is due to the limited number of tasks and the relatively low visual heterogeneity across the environments in this specific benchmark.However, we anticipate that the ViT's superior capacity and scalability will become more critical in larger-scale, more heterogeneous multi-task settings.</p>
<p>Latent Normalization (LayerNorm vs. SimNorm).To suppress the inflation of the latent norm, we compared a standard LayerNorm (Ba et al., 2016) against a SimNorm (Hansen et al., 2023) to assess the impact of latent normalization.SimNorm (Hansen et al., 2023) normalize the latent representation by projecting z into L fixed-dimensional simplices using a softmax operation, which clamps the L2 norm of latent vectors within a predefined threshold.Although SimNorm effectively stabilized training and prevented performance collapse, its hard constraint appeared to curtail the expressive power of the representations, highlighting a trade-off between stability and representational capacity.</p>
<p>Mixture-of-Experts Backbone.To fundamentally address plasticity loss in the Transformer backbone, we replaced its standard feed-forward networks (FFNs) with MoE layers.In an MoE, a gating network sparsely routes each input token to a subset of "expert" sub-networks.This conditional computation mechanism decouples the dense computational pathways into multiple, parallel, specialized ones.This design: (1) reduces parameter collision, as different tasks can leverage different combina-Preprint tions of experts, mitigating representation interference; and (2) mitigates gradient conflict, as gradient updates for different tasks are largely directed to the distinct sets of activated experts.As shown in Figures 4 and 4, the MoE backbone delivered the most significant performance improvements on complex tasks like Seaquest and maintained a relatively low dormant ratio throughout training, effectively controlling the latent norm.</p>
<p>Multi-Task Gradient Correction.We implemented a dynamic gradient re-weighting scheme inspired by MoCo (Fernando et al., 2023) to directly address gradient interference.Despite yielding gains on some tasks, its significant computational overhead and inconsistent efficacy across the heterogeneous task suite led us to explore a more efficient gradient correction in future.</p>
<p>ScaleZero.Based on this exploration, we integrated the most impactful components to form the ScaleZero architecture.Its core design combines: (1) a ViT Encoder for powerful feature extraction;</p>
<p>(2) an MoE Backbone to fundamentally address plasticity collapse through sparse, conditional computation; and (3) Standard Layer Normalization on the latent states to ensure stability without sacrificing expressive power.As validated in Section 4, ScaleZero provides a novel and effective blueprint for building generalist world models.</p>
<p>DYNAMIC PARAMETER SCALING FOR BALANCED MULTI-TASK LEARNING</p>
<p>In heterogeneous MTRL, static, one-size-fits-all model architectures are suboptimal.They tend to waste computational resources on tasks that have already been mastered and often lack the necessary plasticity to effectively tackle the remaining, more difficult tasks.This can lead to inefficient training and risks negative transfer.To address this, we propose Dynamic Parameter Scaling (DPS), a strategy illustrated in Figure 2 (right).The core insight of DPS is that a model's effective capacity should dynamically align with the current set of unsolved tasks.This approach instantiates a "curriculum of model complexity," adaptively expanding the model with new parametric resources directed precisely at the most challenging tasks still in progress.</p>
<p>Formalism and Mechanism.Let the set of tasks be T = {τ i } N i=1 and the parameters of the shared backbone network be θ B .We define a task τ i as "solved" when its performance metric, Metric(τ i ), meets or exceeds a predefined threshold ε i .At any time t, the set of all unsolved tasks is denoted as the active set U t ⊆ T .To conserve computational resources, both data collection and model training are exclusively performed on the tasks within this active set.Once a task is considered solved, it is removed from U t , and we cease all related training activities for it.The training protocol is divided into S + 1 stages (s = 0, 1, . . ., S).It begins with Stage 0 (Warm-up), an initial period of T 0 iterations where only the backbone parameters θ B are trained.Each subsequent Stage s ≥ 1 (Expansion) introduces an independent Low-Rank Adaptation (LoRA) module, ∆θ s .This module consists of a trainable matrix pair ∆θ s = B s A s (where B s ∈ R d×r , A s ∈ R r×k with rank r ≪ d, k) and a learnable scalar scaling factor α s ∈ R.</p>
<p>Adaptive Stage Transition Triggers.The transition from stage s − 1 to stage s is governed by one of two conditions, balancing learning progress against a fixed training budget: (1) Progress-based Trigger: A new stage is initiated when the number of newly solved tasks since the beginning of the current stage reaches a predefined quota Q s .(2) Budget-based Trigger: To prevent training from stalling on exceptionally difficult tasks, a budget-based trigger forces a transition if the number of iterations in the current stage exceeds a pre-allocated limit, ⌈(T max − T 0 )/S⌉.</p>
<p>Parameter Composition and Optimization.In any given stage s, the effective computation for a weight matrix W 0 ∈ θ B is expressed as
W (s) (x) = W 0 x + s j=1 α j (B j A j )x.
Crucially, upon entering a new stage s, we freeze both the backbone parameters θ B and all previously introduced LoRA modules {∆θ j } s−1 j=1 .The optimization gradient is then applied only to the newly added module ∆θ s and the full set of scaling factors {α j } s j=1 .This strategy offers a compelling dual advantage.First, it is computationally efficient by adaptively allocating resources according to task difficulty; new parameters are only introduced as needed for unsolved tasks, avoiding redundant computation on those already mastered.Second, the dynamic injection of new LoRA modules provides targeted plasticity for more challenging tasks without introducing negative transfer.By isolating updates within new modules while preserving the stable knowledge in the frozen backbone, our approach ensures that learning to solve difficult tasks does not disrupt previously acquired capabilities.In summary, DPS enables an intelligent allocation of model capacity, achieving a superior trade-off Preprint between efficiency and final performance.Pseudocode and implementation details are provided in Appendix A.3.</p>
<p>EXPERIMENTS AND ANALYSIS</p>
<p>To comprehensively evaluate the effectiveness of our proposed ScaleZero model and the dynamic parameter scaling strategy, we designed a series of experiments.Section 4.1 aims to validate the performance of ScaleZero on several challenging heterogeneous multitask benchmarks (Atari 100k benchmark (Bellemare et al., 2013a), DeepMind Control Suite (Tunyasuvunakool et al., 2020), and Jericho (Hausknecht et al., 2020)), comparing it against specialized models trained independently for each task.Section 4.2 then focuses on quantitatively analyzing the specific gains in sample and computational efficiency afforded by our dynamic parameter scaling strategy (the combined method we called ScaleZero-DPS).All experiments are conducted in a purely online RL setting, without relying on any expert data, to test the model's capabilities in a realistic learning scenario.Section 4.3 complements our benchmark results with a focused analysis of MoE, combining empirical evidence with theoretical justification to explain why MoE serves as an effective backbone for multitask learning.Implementation details are provided in Appendix A.</p>
<p>MULTITASK LEARNING BENCHMARKS</p>
<p>PERFORMANCE EVALUATION ON ATARI 100K BENCHMARK</p>
<p>Setup.We evaluate ScaleZero on the large-scale Atari 100k benchmark, which comprises 26 distinct games.This benchmark is renowned for its significant heterogeneity across tasks, featuring diverse observation spaces (pixel inputs), action spaces (discrete instruction sets), and widely varying game difficulties and mechanics.We compare the multitask (MT) ScaleZero model against a single-task (ST) UniZero baseline, which is individually trained and optimized for each game.The evaluation metrics are the mean and median of the Human-Normalized Score (HNS), a standard in the field, to provide a comprehensive assessment of performance across tasks of varying difficulty. 1, by training a single, unified model, ScaleZero achieves a mean normalized score that surpasses the average performance of 26 independently trained single-task UniZero models.This indicates that ScaleZero not only successfully learns a diverse set of tasks but also achieves positive knowledge transfer across them.Notably, the performance gains are most pronounced on several notoriously challenging tasks, such as Seaquest.We hypothesize that this is because ScaleZero's unified world model can learn general visual and dynamic priors from simpler tasks and effectively apply them to more difficult ones that demand complex planning and exploration strategies.Although the median score is slightly lower than the single-task baseline, which is primarily influenced by a few tasks with extremely hard exploration challenges, the superior mean score robustly demonstrates the overall advantage of our approach.Setup.To test the generalization capability of ScaleZero in the continuous control domain, we conducted evaluations on the DMC benchmark, which includes 18 tasks based on the MuJoCo physics engine.These tasks feature vector state space and continuous action spaces, posing higher demands on the model's fine-grained control and dynamics modeling abilities.ScaleZero also leverages the principles of Sampled Policy Iteration (Hubert et al., 2021) same as UniZero.Note that because observation space of DMC is vector, we use the MLP encoder.The experimental setup mirrors that of the Atari experiments, comparing the multitask ScaleZero against 18 independent single-task UniZero models.</p>
<p>Results. As shown in Table</p>
<p>Preprint</p>
<p>Results.The results in Table 2 show that ScaleZero achieves returns comparable to or even higher than the single-task baselines on the vast majority of DMC tasks.The learning curves, as illustrated in Figure 10, clearly reveal that ScaleZero can effectively handle the diverse physical dynamics and control requirements of different tasks within a single, unified model.This success validates the versatility of the ScaleZero architecture, proving its competitiveness in complex control problems involving high-dimensional continuous states and actions.2023) obtained a mean score of 840 under an offline supervised learning from expert data setting.The best performance for each task is indicated in bold.</p>
<p>ScaleZero's higher human-normalized median scores highlight its strong performance across continuous action spaces.Detailed curves are available in Appendix C Figure 10.</p>
<p>PERFORMANCE EVALUATION ON THE JERICHO TEXT-BASED ADVENTURE GAMES</p>
<p>Setup.To further evaluate ScaleZero's text-based reasoning and planning, we assess it on the Jericho (Hausknecht et al., 2020) benchmark of interactive fiction, where both observations and actions are expressed in natural language.Jericho features a combinatorial action space and sparse, delayed rewards, and it demands strong long-horizon contextual reasoning.We use four representative games-Acorncourt, Omniquest, Detective, and Zork1-covering diverse narrative structures and interaction complexity.For parity, both the single-task UniZero and the multitask ScaleZero employ the same BGE text encoder (Xiao et al., 2023).In addition, we compare against CALM+OC, a recent LM-in-the-Loop method that adaptively updates the action generator using state-feature-based categorization of gameplay transitions (Sudhakar et al., 2023).The primary evaluation metric is the average return measured after training convergence.</p>
<p>Results.The results in Table 3 show that ScaleZero achieves returns comparable to the single-task UniZero baseline and generally outperforms the CAML+OC method across most games.The learning curves in Figure 13 further indicate that a single, unified model can efficiently handle Jericho's combinatorial language action space and sparse, delayed rewards.These findings validate the applicability of the ScaleZero architecture for text-based RL, demonstrating competitive performance on tasks that require language understanding and long-horizon planning.</p>
<p>EFFICIENCY EVALUATION OF DYNAMIC PARAMETER SCALING</p>
<p>This section aims to evaluate the effectiveness of our LoRA-based online dynamic parameter scaling strategy integrated into ScaleZero, termed ScaleZero-DPS, in improving sample and computational efficiency.To this end, we conducted a comparative analysis between the standard ScaleZero and ScaleZero-DPS on the full DMC18 benchmark.The core evaluation metric is defined as the number of environment interaction steps required to reach a performance level comparable to the singletask baseline (i.e., 100% of its performance).As illustrated in Figure 5, ScaleZero-DPS achieves the target performance using only about 80% of the interaction steps required by the standard ScaleZero.This result highlights the significant efficiency gains afforded by our dynamic scaling strategy.Furthermore, the detailed performance curves, presented in Appendix Figure 11, reveal that ScaleZero-DPS exhibits a steeper learning curve.This indicates a faster learning rate, leading to higher sample and computational efficiency throughout the training process.</p>
<p>UNDERSTANDING MOE EFFICACY: AN EMPIRICAL AND THEORETICAL ANALYSIS</p>
<p>The demonstrated success of Mixture-of-Experts (MoE) in multi-task scenarios motivates a deeper investigation into its underlying mechanisms.In the following, we explore these mechanisms from both experimental and theoretical perspectives.We begin by summarizing our empirical findings, which indicate that replacing standard MLP layers with MoE counterparts significantly reduces gradient conflicts.Notably, this reduction is not confined to the MoE layers themselves but also extends to adjacent network components, such as their input (detailed experimental settings and results are provided in Section E.1).</p>
<p>To complement our empirical analysis, we provide a theoretical justification for how MoE mitigates gradient conflict.Our theory builds upon prior work (Chen et  t2 .Let G be the maximum gradient conflict on any single expert, and cos(u, v) denote the cosine similarity between vectors u and v. Then the following results hold:
Preprint (1). (Generally) Define the m-th element of vectors u, v as u m = λ t1 m ∥g (m) t1 ∥, v m = λ t2 m ∥g (m)
t2 ∥, m = 1, . . ., M. The full-layer gradient conflict admits a general upper bound:
conflict(G t1 , G t2 ) ≤ G • cos(u, v).
(2). (Sparse Router) In exploration stage, where tasks uniformly select experts, the expected gradient conflict of the MoE layer is approximately bounded by G • (1 − M !/((M − K)! M K ), where K is the task number.</p>
<p>(3). (Sparse Router) In the router learning stage, where the expert sets selected by the router gradually stabilize, the gradient conflict of the MoE layer is roughly upper bounded by E[conflict i,j ] ≈ G • U ab , where a = |S i | and b = |S j | denote the sizes of the expert sets selected by tasks i and j, and U = |S i ∩ S j | denotes the size of their intersection.</p>
<p>In Section E.2, we provide detailed explanations of the the three parts of this theorem and their related corollaries, along with complete proofs.</p>
<p>RELATED WORK</p>
<p>MCTS-based Planning with Learned World Models.The integration of MCTS with deep neural networks has become a cornerstone of modern reinforcement learning, particularly in domains requiring sophisticated planning.The seminal AlphaZero series (Silver et al., 2016;2017) established a powerful paradigm by coupling MCTS with learned policy and value networks.This approach was further advanced by MuZero (Schrittwieser et al., 2019), which obviated the need for an explicit environment simulator by learning a latent world model to jointly predict future rewards, policies, and values.This architecture enables planning entirely within a learned representation space.Subsequent research has extended the MuZero framework, focusing on enhancing model capabilities (Hubert et al., 2021;Antonoglou et al., 2021), improving sample efficiency (Danihelka et al., 2022;Ye et al., 2021), and refining training stability (Schrittwieser et al., 2021).Notably, the advent of Transformerbased world models (Micheli et al., 2022;Robine et al., 2023), as exemplified by UniZero (Pu et al., 2024), has significantly augmented representational capacity, enabling stronger long-horizon planning and generalization.However, the monolithic, dense representations that enable singletask excellence become a liability in multi-task settings, causing representational interference and destructive interference.Simply scaling these models exacerbates this architectural bottleneck.Our work try to confront this fundamental limitation.</p>
<p>Multi-Task Reinforcement Learning.To address the challenge of generalization across diverse tasks, MTRL aims to leverage shared knowledge to improve data efficiency and performance (Vithayathil Varghese &amp; Mahmoud, 2020;D'Eramo et al., 2024).A prevalent architectural motif is the shared-backbone with task-specific heads (Kumar et al., 2022;Hansen et al., 2023;2022;Lee et al., 2022a;Gallouédec et al., 2024a).While promoting knowledge transfer, this design is insufficient for world models where the core challenge lies in disentangling the latent dynamics predictions within the backbone itself, not just the final policy or value outputs.Recent research in MTRL has pursued several avenues to mitigate these issues.From a representational standpoint, context-based methods (Rakelly et al., 2019;Sodhani et al., 2021;Seo et al., 2022) condition the model on task metadata to enable selective knowledge sharing, while others introduce learnable modulation modules to preserve task-specific knowledge (Schmied et al., 2023).Architecturally, modular designs have emerged as a promising direction (Lan et al., 2023;Yang et al., 2020;Sun et al., 2022).These include soft modularization with task-aware routing (Yang et al., 2020;He et al., 2023;Hendawy et al., 2023) and constructing policy subspaces via parameter composition (Sun et al., 2022;2023).On the optimization front, various strategies (Fernando et al., 2023;Lin et al., 2024;Ma et al., 2023) have been developed to manage conflicting gradients, a critical impediment in MTRL.These include projecting conflicting gradients into a non-conflicting space (Fernando et al., 2023) and harmonizing training through dynamic loss and gradient normalization (Lin et al., 2024;Ma et al., 2023;Chen et al., 2018).While effective at mitigating conflicts, these methods primarily address the symptoms (conflicting gradients) rather than the root cause: the architectural bottleneck of a model lacking sufficient specialized capacity to represent diverse task dynamics.These diverse approaches underscore the central challenge in MTRL: balancing knowledge sharing with task specialization.Our work seeks to achieve this balance through a novel architectural design.</p>
<p>Sparse and Parameter-Efficient Architectures.Scaling deep RL models is computationally expensive (Espeholt et al., 2018).Sparse activation models, like the Mixture-of-Experts (MoE) (Dai et al., 2024), address this by increasing representational power at a near-constant computational cost via conditional computation.Their success in single-task RL (Obando-Ceron et al., 2024) highlights their potential as a native architectural prior for multi-task specialization.Concurrently, parameter-efficient fine-tuning (PEFT) methods (Dou et al., 2024;Dettmers et al., 2023;Hayou et al., 2024;Liu et al., 2024c) like Low-Rank Adaptation (LoRA) (Hu et al., 2022) enable lightweight, task-specific adaptations by tuning a small subset of parameters, avoiding the cost of full fine-tuning in MTRL (Wang et al., 2023;Yang et al., 2025;Agiza et al., 2024;Zhang et al., 2025).We identify a critical synergy between these two approaches.MoE enables coarse-grained, input-dependent specialization by routing computations to expert sub-networks, tackling representational entanglement at an architectural level.In parallel, LoRA facilitates fine-grained, task-specific adaptation in a parameter-efficient manner, injecting nuanced adjustments without catastrophic interference.The synergistic integration of MoE's scalable representation with LoRA's efficient adaptation is a critical frontier, yet one that has seen little exploration (Dou et al., 2023).Our work bridges this gap by proposing a novel architecture that unifies MoE and LoRA within a Transformer-based world model.</p>
<p>CONCLUSION AND FUTURE WORK</p>
<p>In this paper, we examine the diminishing plasticity phenomenon in unified world models under multitask scenarios and quantify its impact on model performance.By exploring the expansive design space defined by the input-model-optimization axes, we introduce ScaleZero-a principled method that carefully balances performance with sample efficiency.Our extensive experiments on pure online RL benchmarks-which include 26 Atari, 18 DMControl, and 4 Jericho environments-demonstrate that ScaleZero achieves average returns comparable to or even surpassing those of specialized single-task baselines.To further balance computational costs, we introduced a LoRA-based dynamic parameter scaling (DPS) strategy.This strategy has currently only been prototyped on the DMC benchmark, where it achieved competitive performance with 80% of the interaction steps, showcasing its potential for improving sample efficiency.Despite these promising results, several avenues remain open for future investigation: A comprehensive evaluation of the DPS strategy's generalization, scalability, and overhead is required on broader task sets like Atari.Investigating deeper synergistic mechanisms between MoE and LoRA to further enhance model expressivity and adaptability.Combining our online framework with offline pre-training of large-scale models to potentially boost both efficiency and final performance.We believe the continued exploration of these directions will advance multitask reinforcement learning toward the goal of general intelligence.In our experiments, we set the simplex dimension V = 8.</p>
<p>Mixture-of-Experts (MoE): Our MoE layer implementation is inspired by the design in the mistral-inference2 library.Its mechanism operates as follows: for each token in the input sequence, a gating network computes affinity scores to select the Top-1 expert.The selected tokens are then processed by their corresponding expert networks (standard feed-forward networks).</p>
<p>The output of each expert is multiplied by its corresponding gating weight (after Softmax normalization) and then summed.Crucially, to ensure knowledge sharing and generalization, all tokens are also processed by a shared expert network, whose output is added to the final result of the MoE layer.</p>
<p>Multi-task Gradient Correction (MoCo): We adopted the MoCo algorithm from the LibMTL library3 to handle gradient conflicts.Its core idea is to first maintain a momentum term y t to smoothly track each task's gradient direction.Then, task weights λ are dynamically updated by solving a regularized optimization problem.Finally, the gradient applied to shared parameters is the weighted sum of the momentum gradients.Key hyperparameters are listed in Table 5.To address the issue of non-uniform learning progress across tasks in multi-task learning, we propose the Dynamic Parameter Scaling (DPS) strategy.The core idea is to dynamically align the model's effective capacity with the set of currently unsolved tasks.This is achieved through a curriculumbased expansion of the model's parameters.The detailed pseudocode is provided in Algorithm 1.</p>
<p>Staged Training and Active Set Management.The training protocol is divided into S + 1 stages.We define a task τ i as "solved" once its performance metric surpasses a predefined threshold ε i .</p>
<p>The set of all currently unsolved tasks forms the active set U t .To maximize efficiency, both data collection and training updates are performed exclusively on tasks within this active set.</p>
<p>• Stage 0 (Warm-up): During an initial period of T 0 iterations, only the shared backbone network parameters θ B are trained.This allows the model to learn a foundational, generalpurpose representation across all tasks.• Stage s ≥ 1 (Expansion): Each subsequent stage introduces new, dedicated parameters in the form of an independent Low-Rank Adaptation (LoRA) module, ∆θ s = B s A s , and a corresponding learnable scalar scaling factor α s .These new parameters are intended to provide additional capacity for tackling the remaining difficult tasks in U t .</p>
<p>Preprint</p>
<p>Adaptive Stage Transition Triggers.The transition from stage s − 1 to stage s is governed by a dual-trigger mechanism, ensuring a balance between progress and resource allocation:</p>
<ol>
<li>Progress-Driven Trigger: A new stage is initiated if the number of newly solved tasks since the beginning of the current stage reaches a predefined quota Q s .This allows the model to expand only when it has demonstrated mastery over a subset of the current challenges.2. Budget-Driven Trigger: To prevent stagnation on particularly hard tasks, a transition is forced if the number of iterations within the current stage exceeds a maximum budget, calculated as ⌈(T max − T 0 )/S⌉.This ensures that the model continues to gain new capacity even if progress stalls.</li>
</ol>
<p>Hierarchical Parameter Composition and Optimization.In any given stage s, the forward pass for a weight matrix W 0 ∈ θ B is dynamically composed as:
W (s) (x) = W 0 x + s j=1 α j (B j A j )x(3)
Upon entering a new stage s, we implement a crucial optimization strategy: we freeze the backbone parameters θ B and all previously introduced LoRA modules {∆θ j } s−1 j=1 .The optimization gradient is then applied only to the newly introduced LoRA module ∆θ s = {A s , B s } and to all existing scaling factors {α j } s j=1 .This design provides two key benefits: it mitigates negative transfer by isolating new parameter updates within the fresh LoRA module, and it retains plasticity by allowing the model to dynamically re-weigh the contribution of knowledge from all previous stages via the trainable scalars {α j }.</p>
<p>The specific DPS settings used in our ScaleZero-Balanced experiments are detailed in Table 6.</p>
<p>Hyperparameter</p>
<p>Value Description curriculum_stage_num 5 Total number of expansion stages (S).lora_r 64</p>
<p>The rank of the LoRA matrices (r).lora_alpha 1 Initial scaling factor for LoRA layers.min_stage0_iters 10,000 Duration of the warm-up stage (T 0 ).max_stage_iters 5,000 Per-stage iteration budget for the budget-driven trigger.</p>
<p>A.4 COMMON SETTINGS AND HYPERPARAMETERS</p>
<p>To ensure a fair comparison, most hyperparameters are kept identical to the original UniZero paper.We use the AdamW optimizer.Key shared hyperparameters are summarized in Table 7.</p>
<p>Loss Function Weights.The weights for the loss function are configured based on the action space type of the environment, as detailed in Table 8.</p>
<p>A.5 TRAINING FRAMEWORK AND COMPUTATIONAL COST</p>
<p>All experiments were implemented using the LightZero4 framework.All multi-task learning experiments were conducted on a single node equipped with 8x NVIDIA A100 (80G) GPUs.Set trainable parameters θ train ← θ B .5:</p>
<p>Set stage iteration limit L s ← T 0 .</p>
<p>6:</p>
<p>for t stage = 1 to L s do 7:</p>
<p>Sample a batch exclusively from tasks in active set U.</p>
<p>8:</p>
<p>Perform forward pass with backbone: W (x) = W 0 x.</p>
<p>9:</p>
<p>Compute loss L and update θ train .Initialize new LoRA module ∆θ s = B s A s and scalar α s .</p>
<p>14:</p>
<p>Freeze θ B and all previous modules {∆θ j } s−1 j=1 .</p>
<p>15:</p>
<p>Set trainable parameters θ train ← {∆θ s } ∪ {α j } s j=1 .</p>
<p>16:</p>
<p>Record solved tasks at stage start: S start ← T \ U .17:</p>
<p>Set budget for current stage: T budget ← ⌈(T max − T 0 )/S⌉.</p>
<p>18:</p>
<p>t stage ← 0. t stage ← t stage + 1.</p>
<p>21:</p>
<p>Sample a batch exclusively from tasks in active set U.</p>
<p>22:</p>
<p>Perform forward pass:
W (s) (x) = W 0 x + s j=1 α j (B j A j )x.</p>
<p>23:</p>
<p>Compute loss L and update θ train .</p>
<p>24:</p>
<p>Update active set: U ← {τ i ∈ T | Metric(τ i ) &lt; ε i }. end while 29: end if 30: end for model is trained to master all tasks simultaneously, and single-task learning (ST), where a dedicated model is trained independently for each task.</p>
<p>For our primary evaluation, we compare our multitask model, ScaleZero, with a robust single-task baseline using individually trained UniZero models.We use ScaleZero to denote the multitask (MT) setting and UniZero to denote the single-task (ST) setting.The MT/ST notation is sometimes omitted for brevity where the context is unambiguous.This comparison strategy is chosen because the multitask version of UniZero performs poorly, and there is currently no widely accepted standard baseline for multitask learning on the Atari benchmark.Consequently, UniZero ST serves as a more challenging and meaningful point of reference.</p>
<p>Our experiments are conducted in two stages: first, we perform model ablation studies on the Atari8 multitask benchmark (comprising Alien, Boxing, ChopperCommand, Hero, MsPacman, Pong, RoadRunner, and Seaquest); subsequently, we evaluate the final model's performance on the full Atari26 multitask benchmark.</p>
<p>All game observations are preprocessed into 64x64 grayscale images, with 4-frame stacking used for state input.We employ standard environment wrappers, including sticky actions (p=0.25) and a frame skip of 4 (Mnih et al., 2013).Performance is measured by the Human-Normalized Score (HNS), and we report both the mean and median scores.To ensure a fair comparison, all environmental settings are aligned with those in the UniZero paper (Pu et al., 2024).</p>
<p>Preprint</p>
<p>The effective rank is a continuous measure ranging from 1 to the conventional rank of the matrix, providing a more nuanced view of the dimensionality of the space spanned by the matrix's columns or rows.In the context of a neural network, the effective rank of a hidden layer's activation matrix measures the effective number of dimensions in its representation space.A low effective rank indicates that the layer's output can be generated by a small number of units, implying that many neurons are not contributing useful, independent information.In our experiments, we estimate the effective rank from a minibatch of 256 sequence samples before training on each task.</p>
<p>Intrinsic Connections and Performance Implications</p>
<p>The observed correlations highlight a critical challenge in multi-task learning for standard Transformer architectures: representational collapse.The mechanism of this phenomenon is as follows: the model struggles to maintain distinct representations across tasks, causing its feature space to degenerate into a "one-size-fits-all" low-dimensional space, which is suboptimal for any individual task.This collapse is quantitatively indicated by a low effective rank, a problem diagnosed in baseline models (Section 1).The contraction of the representation space renders many neurons redundant, leading to a rising dormant neuron ratio.To compensate for this impoverished representational capacity, the model is forced to scale the magnitude of remaining active representations, which in turn causes the inflation of the latent norm.Ultimately, this collapsed and impoverished representation space lacks the capacity to model the diversity of multiple tasks, resulting in poor overall performance.</p>
<p>In contrast, the MoE variant is designed to mitigate this very problem.By using its expert-based routing to preserve specialized representational subspaces, it addresses the symptoms of representational collapse.As shown in Figure 8, the MoE architecture effectively promotes neuron activity (suppressing the dormant ratio) and stabilizes representation magnitudes (controlling latent norm inflation), leading to superior multitask performance.</p>
<p>B.3 PERFORMANCE ON THE ATARI26 MULTITASK BENCHMARK</p>
<p>As shown in Table 1 of the main text, the mean HNS of the single multitask ScaleZero (MT) model on the Atari26 benchmark surpasses the average HNS of 26 independently trained UniZero single-task (ST) models.This result suggests that the multitask model achieves positive knowledge transfer across tasks.We hypothesize that this is due to the MoE architecture maintaining the network's learning plasticity, enabling the transfer of general priors (e.g., object motion, collision detection) from simpler tasks to more complex ones.</p>
<p>However, the multitask model's median HNS is lower than the single-task baseline.This is primarily influenced by a few tasks with high exploration difficulty or unique mechanics (e.g., PrivateEye, Pitfall), indicating that negative transfer between heterogeneous tasks is not fully resolved in the current architecture.Figure 9 provides a comparison of the learning curves for all tasks.2020) as the evaluation benchmark for continuous control tasks.The experiments were conducted on a suite of 18 tasks (e.g., walker_walk, cheetah_run).</p>
<p>Following a similar methodology to our Atari experiments, we compare a single multitask ScaleZero (MT) model against a baseline of 18 single-task UniZero (ST) models, each trained independently on one task.All ScaleZero variants discussed in this section are multitask (MT) models.</p>
<p>The model input consists of low-dimensional state vectors provided by the environment, and the action space is a continuous vector normalized to the [-1, 1] range.The evaluation metric is the Average Return over 3 random seeds.</p>
<p>C.2 PERFORMANCE COMPARISON: SCALEZERO VS. UNIZERO</p>
<p>As shown in Figure 10, the multitask ScaleZero model achieves returns comparable or superior to the single-task UniZero baseline on the majority of DMC tasks.This result demonstrates that the ScaleZero architecture is applicable to control problems involving high-dimensional continuous state and action spaces.</p>
<p>We hypothesize that this performance stems from the model learning shared physics priors and control primitives across different robotic morphologies.The expert modules within the MoE architecture may specialize in different types of dynamics (e.g., balancing, locomotion), which are then combined by the gating network based on the current task and state.</p>
<p>C.3 EFFICIENCY EVALUATION OF SCALEZERO-DPS</p>
<p>D JERICHO EXPERIMENT DETAILS D.1 BENCHMARK SETUP</p>
<p>Environment Overview: Jericho (Hausknecht et al., 2020) is a reinforcement-learning benchmark built on classic text-adventure games, played entirely through natural-language interaction.Unlike Atari or DMC, it demands robust language understanding of free-form scene, item, and state descriptions while contending with a combinatorial, effectively unbounded action space-at each step the environment surfaces only a small candidate set, yet many legal commands outside it still execute-and coping with sparse, delayed rewards that require long-horizon exploration and planning, making Jericho a stringent testbed for text-based RL.</p>
<p>We conduct experiments on four representative tasks from the Jericho benchmark, namely Detective, Acorncourt, Omniquest, and Zork1.Examples of expert interaction trajectories for Zork1 and Detective are illustrated in Figure 12.Following the protocol of the previous two benchmarks, we compare our multitask ScaleZero (MT) with the single-task UniZero (ST) baseline.ScaleZero uses the same text encoder (bge-base-en-v1.5), the same inference context length of 4, and the same per-step max sequence length of 512 tokens as UniZero.The max action num and max steps for the four tasks are given in Table 10.Results are reported as the mean return over two random seeds.If, at any step, the valid action set is smaller than max action num, we pad to max action num and mark the padded entries with an action mask so they are excluded from sampling.STEP 1 observation: "North of House\nYou are facing the north side of a white house.There is no door here, and all the windows are boarded up.To the north a narrow path winds through the trees.\n\n Valid actions: ['east', 'north', 'west']" action: "N" reward: 0.0 STEP 2 observation: "Forest Path\nThis is a path winding through a dimly lit forest.The path heads north-south here.One particularly large tree with some low branches stands at the edge of the path.\n \n\nValid actions: ['up', 'go around forest', 'north', 'south', 'west', 'east']" action: "N" reward: 0.0 STEP 3 observation: "Up a Tree\nYou are about 10 feet above the ground nestled among some large branches.The nearest branch above you is above your reach.\nBesideyou on the branch is a small bird's nest.\nIn the bird's nest is a large egg encrusted with precious jewels, apparently scavenged by a childless songbird.The egg is covered with fine gold inlay, and ornamented in lapis lazuli and mother-of-pearl.Unlike most eggs, this one is hinged and closed with a delicate looking clasp.The egg appears extremely fragile.\n\n\nValidactions: ['down', 'take egg', 'take nest', 'take on egg', 'close nest']" action: "U" reward: 0.0 STEP 4 observation: "North of House\nYou are facing the north side of a white house.There is no door here, and all the windows are boarded up.To the north a narrow path winds through the trees.\n\nValid actions: ['east', 'north', 'west']" action: "Get egg" reward: 5.0</p>
<p>(a) Zork1</p>
<p>(b) Detective</p>
<p>Figure 12: First five steps of expert interaction trajectories for Zork1 and Detective.At each step, the environment supplies a set of valid candidate actions (used in our experiments); however, additional legal actions outside this set may also be executed, so the action space is, in principle, unbounded.As shown in Figure 13, the multitask ScaleZero attains returns comparable to the single-task UniZero baseline on most Jericho games, with learning curves exhibiting an "easy-first, hard-later" progression.Specifically, ScaleZero converges faster on Acorncourt, Omniquest, and Detective; on the most challenging Zork1, it initially lags but accelerates after 150k environment interactions to reach parity with UniZero.We hypothesize that this stems from multi-task training inducing transferable language priors and interaction routines, which help filter feasible commands, compress the open-ended action space, and reduce fruitless exploration.</p>
<p>E GRADIENT ANALYSIS IN MOE E.1 EXPERIMENTAL ANALYSIS</p>
<p>Given the strong performance of Mixture-of-Experts (MoE) in multitask settings, a natural question arises: Why MoE?We aim to investigate, both experimentally and theoretically, the underlying mechanisms behind MoE's superior performance in multitask reinforcement learning.Specifically, our experimental setup is as follows.</p>
<p>Preprint</p>
<p>E.1.1 EXPERIMENT 1: ANALYZING GRADIENT CONFLICTS IN MOE-BASED TRANSFORMERS</p>
<p>We conduct our experiments on Atari-8.Concretely, our network architecture consists of an encoder (ViT), a backbone (Transformer), and corresponding heads.We compare two baseline methods with different backbones:</p>
<p>(1).Naive Transformer: The backbone consists of four standard Transformer blocks.</p>
<p>(2).MoE-based Transformer: The backbone also consists of four Transformer blocks, but the MLP layer in each block is replaced with an MoE layer, which comprises one shared expert and eight non-shared experts (Liu et al., 2024b).Shared experts provide cross-task general representations, enhancing generalization ability and ensuring training stability, while non-shared experts learn taskspecific representations to strengthen the model's discriminability and task adaptability.During the forward pass, all non-shared experts are selectively activated by a sparse gating network, which determines which specific expert to use for each input.</p>
<p>For both baseline models, we investigate gradient conflicts at different components: (1)Input before the MoE layer.</p>
<p>(2)Output of the encoder.</p>
<p>(3)Parameters of the MoE, including the shared expert, non-shared experts, and the entire MoE layer.We measure gradient conflict between tasks using the maximum negative cosine similarity, defined as follows:
Max Gradient Conflict = max i,j − ∇θ i • ∇θ j ∥∇θ i ∥∥∇θ j ∥(5)
where ∇θ i and ∇θ j denote the gradients of the i-th and j-th tasks, respectively.A higher value of the Max Gradient Conflict represents a greater degree of gradient conflict.We choose the maximum pairwise cosine similarity because it directly identifies the most severe gradient conflict between any two tasks.Unlike averaging operation, which can hide critical issues, the maximum value pinpoints the 'bottleneck pair' that most significantly impedes stable multi-task training and overall convergence, even if other tasks cooperate well.</p>
<p>In MoE, when computing the gradient of an entire layer, if task A selects expert i while task B selects expert j with i ̸ = j, then the gradient of task B on expert i is filled with zero.The reason is that expert i is not involved in the forward propagation of task B, and thus makes no contribution to its loss; consequently, its gradient during backpropagation should naturally be zero.Preprint within input space.However, when faced with non-stationary data distributions generated through agent-environment interactions, it remains unclear whether MoE experts still differentiate effectively.</p>
<p>To answer the question, we analyze the entropy of expert selection distributions, which quantifies the uncertainty of the choice of which expert to use for a task, with low entropy indicating high specialization and decisive selection, while high entropy implies uncertainty or an average utilization across multiple experts.We also recorded the Wasserstein distance Rüschendorf (1985) between the expert selection distributions of different tasks, where smaller values indicate a greater proximity to the expert selection of two tasks.We aim to quantify this relationship to find the underlying connections in expert selection among various tasks.The specific experimental steps are as follows:</p>
<p>(STEP1) During a forward pass at a given training step, we record the expert choices in the final MoE-based Transformer block.</p>
<p>(STEP2) For a specific training step s t , we collect expert selection data over a past window of size S and compute the frequency of each expert's activation to form a probability distribution.For a given window size S, task i and a specific task j, we denote the task selection distribution as P S i and a specific probability for task j as P S i,j .Different window sizes reflect attention to data over different temporal scales in non-stationary learning.</p>
<p>(STEP3) Based on this probability distribution, we calculate the expert selection entropy for each task E taski :
E taski = − N j=1 P S i,j log 2 (P S i,j )
and Wasserstein Distance between task i and task i ′ (W i,i ′ )</p>
<p>We consider two sizes S: immediate = 100, short = 1,000. 5We present the entropy results in Figure 15.Due to space limitations, we show in Figure 18 the Wasserstein distances between expert selections of different tasks at multiple training stages.Observation 3: As shown in Figure 16, the shared expert exhibits significantly higher gradient conflicts compared to the task-specific experts.Among the non-shared experts, the level of conflicts does not show substantial differences.Overall, the shared expert accumulates even more conflicts than the entire MoE layer, indicating that most gradient conflicts within the MoE layer are concentrated on the shared expert.The introduction of several task-specific experts effectively reduces the overall gradient conflicts of the MoE layer, which is consistent with our theoretical analysis in Theorem E.4.</p>
<p>We further investigated the conflicts among different experts within MoE. Results is shown in Figure 16 We found that shared experts bear most of the gradient conflicts, whereas individual (non-shared) experts experience almost no conflict.A plausible explanation is that in a standard gating MoE, different samples are routed to different experts, so each expert primarily receives gradient updates from a specific type of sample.This effectively performs an implicit task partitioning, leading to generally consistent gradient directions within the same expert and minimal conflicts.In contrast, in shared-expert MoE, all samples pass through the shared experts, whose parameters must adapt simultaneously to multiple tasks, diverse semantics, and even different domain distributions.This can result in highly inconsistent gradient directions, significantly increasing conflicts and making it difficult for shared experts to balance diverse task requirements during optimization.This observation naturally raises a simple question: is the alleviation of gradient conflicts mainly due to the sparse selection by the gating network?We provide a theoretical justification from the perspective of gating.</p>
<p>E.2 THEORETICAL ANALYSIS</p>
<p>We observe that the magnitude of the gradient conflict in an MoE model is primarily influenced by the routing coefficient, ρ i .In a sparse MoE, this coefficient acts as a binary variable determined by the router network.For simlicity, let us consider a top-1 sparse MoE, where only the expert with the highest routing score is selected for a given input.An effective router would learn to assign tasks that are prone to high gradient conflict to different experts.In this case, the routing coefficient for any conflicting task pair would be zero, thereby nullifying their contribution to the overall conflict.Conversely, a poorly performing router might allocate conflicting tasks to the same expert, in which case the conflict remains unmitigated.</p>
<p>Preprint</p>
<p>b. Sparse Routing (Top-1 gating, one λ m = 1, others 0)</p>
<p>• Non-overlapping case (different tasks select different experts):Each task's full-layer gradient contains only its chosen expert.Inner products between task gradients are near zero because tasks lie in different expert subspaces.Full-layer conflict is strictly less than G, possibly near 0.</p>
<p>• Collapsed case (all tasks select the same expert): Full-layer gradient reduces to the chosen expert's gradient.Conflict equals the single-expert bound G, since all gradients reside in the same subspace.</p>
<p>Therefore, the key to mitigating gradient conflict lies in the router's ability to foster specialization among experts, ensuring that tasks with potentially conflicting objectives are handled by distinct, differentiated experts.</p>
<p>This naturally raises a critical question: Can a Mixture-of-Experts architecture indeed achieve this, and if so, what are the underlying mechanisms that enable this capability?In the following, we answer this question affirmatively.By building upon existing foundational theories of Mixture-of-ExpertsChen et al., we present a formal derivation that explains precisely when and how this is achieved.</p>
<p>E.2.1 ANALYSIS</p>
<p>Previews workChen et al. theoretically demonstrated that, in a single-task supervised learning setting, when the input distribution exhibits distinguishable cluster structures, the sparse gating Mixture-of-Experts (MoE) router can automatically learn the cluster center features and route samples to the most suitable expert, thereby significantly improving performance.We further hypothesize that, in multi-task learning, the input spaces of different tasks naturally correspond to distinct clusters and the signal for the cluster center can be task ID or the latent pattern on the state space.Under this assumption, the conclusions from the original work can be directly extended to the multi-task setting.</p>
<p>Next, we adapt Chen et al. theoretical analysis to the reinforcement learning context using a tabular example.Specifically, we treat each task as an independent Markov Decision Process (MDP) and construct a multi-task MDP set with a shared state space but task-specific transitions.In this setting, we derive the gradient conflicts that arise in the MoE layer.</p>
<p>We also discuss the case where no clustering signal exists between tasks and point out that the introduction of task embeddings can significantly enhance the clustering structure between tasks, thereby promoting the isolation of expert selection across different tasks in MoE.</p>
<p>E.2.2 PROBLEM SETTING: MULTI-TASK MDP WITH TASK-SPECIFIC LATENT CLUSTERS</p>
<p>We consider a Markov decision process (MDP) defined by the tuple (S, A, P, R, γ), where S is the state space, A the action space, P (s ′ | s, a) the transition kernel, R(s, a) the reward function, and γ ∈ [0, 1) the discount factor.There are K tasks T 1 , . . ., T K .Each task T t corresponds to a disjoint state subspace S (t) , so that
S = K t=1 S (t) , S (t) ∩ S (t ′ ) = ∅ for t ̸ = t ′ .
Patch-based state representation.Each state s ∈ S where P (m i,t = m) is the probability that input sample x i is routed to expert m at iteration t and Õ(σ 1.5 0 ) is a negligible value.The equation means all experts have approximately uniform gating weights at the expert exploration stage.That is, the routing selection can be approximated as:
π m (s) ≈ 1 M .(13)
This ensures that each expert has an equal opportunity to learn from the data.</p>
<p>During the early stage of training, due to random initialization, each expert m has a weight vector w</p>
<p>m that exhibits a slightly larger inner product with some value basis vector v k .We define the initial preference cluster of expert m as
k * m = arg max k∈[K] ⟨w (0) m , v k ⟩ .(14)
Under gradient descent updates, the weight vector w m of expert m will predominantly grow along the direction of its preferred basis vector v k * m .[Chen et al.,Lemma E.5] In other words, during training, an expert progressively specializes in modeling a specific cluster c k .Building upon this, we derive an upper bound on the gradient conflicts during the Expert Exploration Stage.Theorem E.2 (Upper Bound on Single-Expert and Full-Layer MoE Gradient Conflict with Uniform Sparse Routing).Consider a Mixture-of-Experts (MoE) layer with M experts and K independent tasks.Each task independently selects one expert uniformly at random: P = 1 M .Then:</p>
<p>1.For any single expert m, the expected gradient conflict is upper bounded by
E[conflict (m) t1,t2 ] ≤ G q single , q single = 1 − 1 − 1 M K − K 1 M 1 − 1 M K−1 .
2. For the full-layer MoE with sparse router, the expected gradient conflict is upper bounded by
E[conflict MoE t1,t2 ] ≤ G q layer , q layer =      1 − M ! (M − K)! M K , K ≤ M 1, K &gt; M .
Proof.Consider an arbitrary expert m and the K tasks.Each task independently selects expert m with probability 1/M .Let X denote the number of tasks that select expert m.Then X ∼ Bin(K, 1/M ).</p>
<p>A single-expert gradient conflict occurs if and only if X ≥ 2, and by assumption, the maximum conflict for a single expert is G. Therefore, the probability that expert m experiences a conflict is
Pr(X ≥ 2) = 1 − 1 − 1 M K − K 1 M 1 − 1 M K−1
.</p>
<p>By linearity of expectation, the expected gradient conflict for a single expert is upper bounded by
E[conflict (m) t1,t2 ] ≤ G q single .
The full-layer gradient G t is constructed by concatenating all expert gradients.A full-layer conflict occurs if at least one expert has a conflict (i.e., at least two tasks select that expert).</p>
<p>-For K ≤ M , the probability that all K tasks select distinct experts (no collision) is given combinatorially by M !(M − K)! M K , so the probability of at least one collision (full-layer conflict) is
q layer = 1 − M ! (M − K)! M K .</p>
<p>Preprint</p>
<p>-For K &gt; M , by the pigeonhole principle, at least one expert must be selected by two or more tasks, so q layer = 1.</p>
<p>Each conflict in an expert can contribute at most G to the full-layer gradient conflict.By worst-case analysis and linearity of expectation, the expected full-layer MoE gradient conflict is therefore upper bounded by
E[conflict MoE t1,t2 ] ≤ G q layer .
-The single-expert bound G q single gives a local (per-expert) expected conflict.-The full-layer bound G q layer accounts for collisions across all experts in the concatenated MoE layer.-Together, they describe the expected gradient conflict behavior of a MoE layer with K tasks and M experts under uniform sparse routing.</p>
<p>Router Learning Stage</p>
<p>During the router-learning stage, the sparse MoE layer exhibits two decisive properties that justify its use in a multi-task setting.</p>
<p>First, the gating network rapidly identifies the latent task structure: after only Taken together, these two properties guarantee automatic task separation and expert specialisation without external supervision, making the MoE layer an effective backbone for multi-task problems in which tasks form separable clusters in the input space.Theorem E.3 (Expected Gradient Conflict on Task-specific Expert Sets).Consider a Mixture-of-Experts (MoE) model with K tasks and M experts.Let tasks i and j have expert sets S i and S j with sizes |S i | = a, |S j | = b, and intersection U = |S i ∩ S j |.Assume that each task selects an expert from its own expert set with probability 1 − O(1/d) uniformly, and from the complement set with probability O(1/d) uniformly.Let G denote the maximum gradient conflict if two tasks select the same expert.Then the expected gradient conflict between tasks i and j satisfies
T 2 = η −1 M −2E[conflict i,j ] ≤ G • P (1) + P (2) + P (3) ,(15)
where
P (1) = U • 1 − O(1/d) a + O(1/d) M − a 1 − O(1/d) b + O(1/d) M − b ,(16)P (2) = (a − U + b − U ) • 1 − O(1/d) a + O(1/d) M − a O(1/d) M − b ,(17)P (3) = (M − (a + b − U )) • O(1/d) M − a • O(1/d) M − b . (18)
In particular, the dominant contribution comes from the shared experts (first class), leading to
E[conflict i,j ] ≈ G • U ab + O(G/d). (19)
Moreover, for the full-layer MoE gradient formed by concatenating all expert parameters, the expected layer-level gradient conflict satisfies the same upper bound:
E[conflict] layer ≤ G • P (1) + P (2) + P (3) ≈ G • U ab + O(G/d).(20)
Proof.Consider two tasks i and j with expert sets S i and S j .Partition the set of all experts into three classes: (1) shared experts S i ∩ S j , (2) task-specific experts S i \ S j and S j \ S i , and (3) unrelated experts outside S i ∪ S j .For each class, we compute the probability that both tasks select the same expert.</p>
<p>For a shared expert e ∈ S i ∩S j , task i selects it either from its own set with probability For task-specific experts e ∈ (S i \ S j ) ∪ (S j S i ), one task can select it from its own set while the other selects from outside.This yields
p (2) = 1 − O(1/d) a + O(1/d) M − a O(1/d) M − b ,
with a total of (a − U + b − U ) experts, giving
P (2) = (a − U + b − U ) • p (2) .
For unrelated experts e / ∈ S i ∪ S j , both tasks must select from outside, giving Summing over all classes and multiplying by G, we obtain the expected single-expert gradient conflict: E[conflict i,j ] ≤ G • P (1) + P (2) + P (3) .</p>
<p>Since the layer-level gradient is formed by concatenating all expert gradients, the layer-level conflict is the sum over all expert contributions, hence E[conflict] layer ≤ G • P (1) + P (2) + P (3) .</p>
<p>Finally, the dominant term is from the shared experts, giving the simplified approximation
E[conflict] layer ≈ G • U ab + O(G/d),
where O(G/d) absorbs all higher-order small contributions from selecting non-preferred experts.</p>
<p>Corollary E.4 (Expected Full-layer MoE Gradient Conflict for K Tasks).Consider a MoE layer with M experts and K tasks.Each task t has its own expert set S t with size |S t | = a t .Each expert's gradient norm satisfies ∥g (m) t ∥ ≤ R, and if two tasks select the same expert, the maximum gradient conflict is G.Each task chooses an expert according to the following rule: with probability 1 − O(1/d) it selects uniformly from its own set, and with probability O(1/d) it selects uniformly from the remaining experts.</p>
<p>For any subset of tasks T ⊆ {1, 2, . . ., K}, let the number of shared experts be
U T = t∈T S t .
Then the expected full-layer gradient conflict is upper bounded by
E[conflict] layer ≤ G T ⊆[K],|T |≥2 U T t∈T 1 − O(1/d) a t + O(1/d) M − a t ,(21)
and its leading term can be approximated as
E[conflict] layer ≈ G T ⊆[K],|T |≥2 U T t∈T a t + O(G/d).(22)
Proof.For each task subset T ⊆ {1, 2, . . ., K} with |T | ≥ 2, consider any expert e ∈ t∈T S t .The probability that all tasks in T simultaneously select this expert is given by the product of individual selection probabilities.For each task t ∈ T , the probability of choosing e is
1 − O(1/d) a t + O(1/d) M − a t ,
where the first term accounts for selecting e from the task's own expert set and the second term accounts for selecting e from outside the set.Since the tasks are independent, the joint probability that all tasks in T select the same shared expert e is Multiplying by the number of shared experts U T gives the total expected conflict contribution from this subset:
P T = U T t∈T 1 − O(1/d) a t + O(1/d) M − a t .
Summing over all task subsets with size at least 2, and multiplying by G, yields the full-layer expected gradient conflict:
E[conflict] layer ≤ G T ⊆[K],|T |≥2 P T .
In the leading order, we can neglect the O(1/d) small-probability contributions from selecting experts outside each task's own set, which gives
E[conflict] layer ≈ G T ⊆[K],|T |≥2 U T t∈T a t + O(G/d),
where the dominant contribution comes from shared experts that are simultaneously selected by multiple tasks, and the higher-order terms are absorbed in O(G/d).</p>
<p>E.2.4 TASK EMBEDDING FOR IMPROVED TASK SEPARATION IN MULTI-TASK LEARNING</p>
<p>In multi-task learning, tasks often share overlapping input spaces and underlying features (e.g., sentiment classification tasks using the same text encoder), making it difficult for the routing network to distinguish tasks.This overlap leads to routing confusion, where data from one task may be misrouted to experts specialized in other tasks, reducing performance.To address this, task embedding introduces a learnable vector e τ for each task, which can be combined with input features.Since e τ is orthogonal across tasks (or constrained via a regularization term), it helps the routing network more clearly distinguish between tasks, even when inputs are similar.Task embedding also acts as a regularization term, reducing expert load imbalance and improving the stability and performance of multi-task MoE training.</p>
<p>Our theoretical analysis suggests that introducing distinct task embeddings provides crucial clustering signals, enabling the router to more effectively differentiate between tasks.This, in turn, mitigates gradient conflicts and enhances overall performance.However, our empirical results, as presented in Figure 3, show that the inclusion of naive task embeddings did not yield a significant improvement.We hypothesize that this is due to the absence of explicit constraints to maintain separation between the embeddings during optimization, which could lead to their collapse in the later stages of training.</p>
<p>For future work, we plan to investigate more effective methods for explicitly injecting task-specific information.The goal is to empower the router to autonomously leverage the unique priors of each task, thereby further advancing multi-task learning performance.</p>
<p>Figure 1 :
1
Figure 1: Diagnosing plasticity collapse in the baseline UniZero model on a multi-task Atari benchmark.While simple tasks like Pong and Hero show stable learning, complex tasks such as Seaquest and ChopperCommand suffer a catastrophic performance collapse in later training (Top).This performance failure is precisely correlated with a sharp spike in the dormant neuron ratio of the Transformer backbone (Bottom Left) and an uncontrolled inflation of the latent state norm (Bottom Right), empirically validating the link between external performance and internal network dynamics.</p>
<p>Figure 2: (a) A systematic exploration of the UniZero design space across five axes: task conditioning, encoder architecture, latent normalization, backbone design, and multitask optimization.This investigation informs the design of our proposed ScaleZero model.(b) A conceptual diagram of Dynamic Parameter Scaling (DPS), our adaptive training strategy.DPS progressively expands model capacity by injecting lightweight LoRA adapters in stages, triggered by learning progress, to direct computational resources toward unsolved tasks while preserving existing knowledge.</p>
<p>Figure 3 :
3
Figure 3: Performance impact of architectural modifications on the Atari8 multi-task benchmark.This ablation study across the UniZero design space reveals that replacing the standard Transformer backbone with a Mixture-of-Experts architecture yields the most significant and consistent performance gains.In contrast, other interventions, with the partial exception of SimNorm, provide marginal or inconsistent benefits.These results underscore the centrality of the MoE's conditional computation in overcoming the limitations of a shared, dense backbone.</p>
<p>Figure 4 :
4
Figure4: Detailed analysis correlating performance with plasticity metrics across model variants in Figure3.On two representative hard tasks (ChopperCommand and Seaquest), the performance collapse of baseline models directly correlates with catastrophic plasticity loss-evidenced by a sharp rise in the dormant neuron ratio and latent norm.In stark contrast, the MoE-based model sustains high returns by keeping the dormant ratio near-zero and the latent norm stable.This confirms that the MoE's superior performance stems directly from its success in mitigating the diagnosed plasticity issues.Analyses for the remaining six environments, which show consistent trends, are deferred in Figure8.</p>
<p>Figure 5 :
5
Figure 5: Comparison of interaction cost for ScaleZero vs. ScaleZero-DPS on DMC18.ScaleZero-DPS reaches the target performance with approximately 80% of the interaction steps.For detailed performance curves, please refer to Appendix C Figure 11.</p>
<p>Figure 6
6
Figure 6 illustrates this concept, providing a diagnostic view of the representation's dimensionality.The observation that a standard baseline model can exhibit a low effective rank provides quantitative evidence for the representational collapse hypothesis.</p>
<p>Figure 6 :
6
Figure 6: Supplementary analysis of return and representation effective rank.This figure provides additional diagnostics for the baseline model shown in Figure 1, illustrating the relationship between game return (Left) and representation effective rank (Right).The performance degradation, observed as a drop in return, is largely accompanied by a corresponding decline in the representation effective rank.This provides further evidence that the performance collapse is linked to a degeneration of the model's representation space.</p>
<p>Preprint</p>
<p>Figure 7 :
7
Figure 7: Training dynamics of UniZero on four other tasks from the Atari8 benchmark.This figure complements Figure 1 of the main text, showing the performance curves, dormant neuron ratio of the Transformer backbone, and latent state norm for Boxing, MsPacman, RoadRunner, and Alien.</p>
<p>Figure 8 :
8
Figure 8: Complete plasticity metric analysis for the ScaleZero design space exploration.This figure details the evolution of performance (Return), Dormant Ratio, and Latent Norm for different model variants across all 8 tasks in the Atari8 benchmark.</p>
<p>Preprint</p>
<p>Figure 9 :
9
Figure 9: Performance comparison between ScaleZero (MT) and UniZero (ST) on the Atari26 benchmark.The figure shows the learning curves (mean and 95% confidence interval) for the multitask ScaleZero and the single-task UniZero baselines across 26 Atari games.</p>
<p>Figure 11
11
Figure 11 compares the per-task learning curves of ScaleZero-DPS and the standard ScaleZero model to illustrate the difference in sample efficiency.The Figure shows that ScaleZero-DPS (orange line) exhibits a faster learning rate on most tasks, achieving equivalent performance levels with fewer environment steps.The asterisks (*) in the figure mark tasks that were identified as "solved" by the DPS policy and for which training was terminated early, showing the dynamic resource allocation mechanism in effect.</p>
<p>Figure 10 :
10
Figure 10: Learning curves of ScaleZero (MT) vs. UniZero (ST) on the DMC18 benchmark.This figure compares the performance of the multitask ScaleZero and single-task UniZero models on 18 DMControl tasks.Solid lines represent the mean performance over 3 random seeds, and the shaded area indicates the 95% confidence interval.</p>
<p>Figure 11 :
11
Figure 11: Performance and efficiency comparison of ScaleZero-DPS vs. standard ScaleZero on DMC18.This figure shows the learning curves (return vs. environment steps) for both models across 18 tasks.An asterisk (*) indicates that the task's training was terminated early by the DPS policy.</p>
<p>Figure 13 :
13
Figure 13: Learning curves of ScaleZero (MT) vs. UniZero (ST) on the Jericho benchmark.This figure compares the performance of the multitask ScaleZero and single-task UniZero models on 4 Jericho tasks.Solid lines represent the mean performance over 2 random seeds, and the shaded area indicates the 95% confidence interval.</p>
<p>Figure 14 :
14
Figure 14: Comparison of gradient conflicts between MoE and MLP baselines across different components.MoE-based Transformer exhibits fewer gradient conflicts in MoE input and Moe layer.</p>
<p>Observation 1 :
1
As shown in Figure14(a), the MoE-based Transformer exhibits fewer gradient conflicts than the MLP counterpart.(b) further shows that introducing MoE reduces gradient conflicts at the layer input, implying that MoE alleviates conflicts in other components to some extent.In the encoder, however, the conflict levels of MoE and MLP remain comparable.We argue that the main reason is that the encoder extracts general representations, which inherently exhibit fewer gradient conflicts.E.1.2EXPERIMENT 2: INVESTIGATING MOE GATING MECHANISMSWe are particularly interested in the internal gating mechanism of MoE.Previous studiesChen et al. have shown that, under supervised learning, MoE can implicitly uncover latent cluster structures</p>
<p>Figure 15 :
15
Figure 15: Line plot showing the evolution of expert selection entropy in a multi-task learning setting with eight tasks.The dashed lines correspond to the entropy values of individual tasks, and the solid red line represents the aggregated entropy across all tasks.Higher entropy reflects more uniform and uncertain expert utilization, while lower entropy reflects more concentrated and specialized expert selection.</p>
<p>Figure 16 :
16
Figure 16: Gradient conflicts across experts in MoE training.The plot shows the evolution of gradient conflicts for the shared expert and 8 non-shared experts during training interations.The logarithmic y-axis reveals that different experts experience varying levels of gradient conflicts, with the shared expert and individual experts (expert0-expert7) demonstrating distinct conflict patterns, indicating effective expert specialization in the multi-task learning framework.</p>
<p>e., belonging to task t and cluster k) is represented as an unordered collection of P patches in R d .The patches are randomly permuted before being presented to the model.Every state contains the following patch types:1.Action-signal patch: exactly one patch equals α v(t) k , where v (t)k ∈ R d encodes the optimalaction feature.2. Cluster-center patch: exactly one patch equals β c (t) k , indicating the (task-specific) cluster identity; a router (e.g., in a Mixture-of-Experts model) is expected to detect this signal to decide routing.</p>
<p>(1−O(1/d))/a or from outside with probability O(1/d)/(M − a).Similarly for task j.Therefore, the probability that both tasks select e is p U such experts, giving total contribution P (1) = U • p(1) .</p>
<p>M − (a + b − U ) such experts, yielding P (3) = (M − (a + b − U )) • p (3) .</p>
<p>Figure 17 :
17
Figure 17: Nine heatmaps illustrating the evolution of expert selection distributions in the MoE-based Transformer across different tasks and training interations.Each row corresponds to a specific task, where the color intensity reflects the frequency of expert selections within a sliding window of S = 1000.This visualization reveals how task-specific expert utilization patterns emerge and evolve during training.</p>
<p>Figure 18 :
18
Figure 18: Nine heatmaps illustrating the Wasserstein distances between expert selection distributions of different tasks at successive training interations with S immediate = 100.Each heatmap corresponds to a specific training interation, revealing how inter-task expert selection similarity changes over time .Diagonal elements are removed to exclude self-comparisons.</p>
<p>Figure 19 :
19
Figure 19: T-SNE Visualization.Each point denotes a 2D projection of a 768-dimensional latent embedding extracted from the world model.Points are colored by environment ID, with different colors representing different Atari games.The visualization reveals the learned representational structure: similar game states cluster together in the latent space, while distinct environments form separate groups.This indicates that the model captures task-specific features while preserving meaningful separation across different games.The figure contains 8,000 data points collected from 8 distributed training processes, where each point corresponds to a single time step from the observation sequence (400 samples × 20 time steps per sample).</p>
<p>Table 1 :
1
Comparison of Human-Normalized Scores on 26 Atari games for ScaleZero (MT) vs. UniZero (ST).Detailed curves are available in Appendix B Figure 9.
AlgorithmNormalized Mean Normalized MedianUniZero (ST)0.380.21ScaleZero (MT)0.390.164.1.2 PERFORMANCE EVALUATION ON THE DEEPMIND CONTROL SUITE</p>
<p>Table 2 :
2TaskUniZero (ST) ScaleZero (MT)acrobot-swingup400.3501.0cartpole-balance952.2990.5cartpole-balance_sparse1000.01000.0cartpole-swingup801.3769.0cartpole-swingup_sparse752.5708.1cheetah-run517.6510.9ball_in_cup-catch961.6954.2finger-spin810.7574.2finger-turn_easy1000.01000.0finger-turn_hard884.5982.0hopper-hop120.5138.0hopper-stand602.6583.1pendulum-swingup865.6866.0reacher-easy993.3943.1reacher-hard988.8943.5walker-run587.9562.7walker-stand976.4919.9walker-walk954.6908.7Mean787.2769.7Median875.1887.3
Performance comparison between ScaleZero (MT) and UniZero (ST) across various tasks in DMControl.For reference, L2M (MT)Schmied et al. (</p>
<p>Table 3 :
3
Average returns of ScaleZero and other baselines on four Jericho tasks.CALM+OC(Sudhakar et al., 2023) denotes the variant of CALM with online categorization based on state features.Detailed learning curves for ScaleZero and UniZero are provided in Appendix D Figure 13.
AlgorithmAcorncourt Omniquest Detective Zork1CALM+OC (ST)-7.8288.538.0UniZero (ST)101029544ScaleZero (MT)101028044</p>
<p>al.) that characterizes MoE training by two distinct phases: an initial exploration stage, where inputs are routed nearly uniformly across experts, followed by a router learning stage, where the router gradually specializes in directing inputs.Grounded in this two-stage framework, we propose an informal theorem asserting that, regardless of the training stage, the upper bound on gradient conflict for an MoE layer is strictly lower than that of
(m) t1 and g(m)
its MLP counterpart, and that this reduction is primarily governed by the router's behavior.Theorem 4.1 (Upper Bound on Gradient Conflict in MoE Layers (informal)).In a Mixture-of-Experts (MoE) layer, consider two tasks t 1 and t 2 with routing weights λ t1 m , λ t2 m over M experts, and per-task gradients on expert m, denoted as g</p>
<p>Table 4 :
4
Hyperparameters for the Vision Transformer (ViT) encoder.
HyperparameterValueimage_size64x64patch_size8x8dim (Embedding Dimension)768depth (Transformer Layers)6heads (Attention Heads)6mlp_dim (MLP Hidden Dimension)2048dropout0.1emb_dropout (Embedding Dropout)0.1</p>
<p>Table 5 :
5
Hyperparameters for the MoCo algorithm.
Hyperparameter ValueMoCo_beta (β)0.99MoCo_gamma (γ)10.0MoCo_rho0.0A.3 DYNAMIC PARAMETER SCALING (DPS)</p>
<p>Table 6 :
6
Hyperparameters for Dynamic Parameter Scaling (DPS).</p>
<p>Set of tasks T = {τ i } N i=1 ; Performance thresholds {ε i } N i=1 ; Backbone params θ B ; Require: Hyperparameters: Total stages S, Warm-up iterations T 0 , Max total iterations T max , Stage quotas {Q s } S s=1 .Ensure: Trained parameters: θ B , {∆θ s , α s }
S s=1 .1: Initialize active set of unsolved tasks: U ← T .2: for stage s = 0 to S do3: if s == 0 then▷ Stage 0: Warm-up Phase4:Theapproximate training times are shown in Table 9.B ATARI EXPERIMENT DETAILS
B.1 BENCHMARK SETUP We use the Arcade Learning Environment (ALE) (Bellemare et al., 2013b) as our evaluation platform.Our experiments adopt two distinct training paradigms: multitask learning (MT), where a single Preprint Algorithm 1 Dynamic Parameter Scaling (DPS) Require:</p>
<p>Update active set: U ← {τ i ∈ T | Metric(τ i ) &lt; ε i }.
10:11:end for12: else▷ Stage s ≥ 1: Expansion Phase13:</p>
<p>Table 10 :
10
Key game statistics for the four Jericho tasks.Maximum valid actions denotes the largest number of valid action candidates exposed by the environment at any step, observed over 200 evaluation episodes; this statistic informs the max action num parameter used in our experiments.Maximum steps per episode is the largest episode length observed over the same 200 episodes, while max steps parameter is the episode-length cap adopted during experiments.
Gamemaximum valid actions (in 200 episodes)maximum steps per episode (in 200 episodes)max action num max stepsAcorncourt341745100Omniquest247825100Zork15339655500Detective115112100</p>
<p>"Copyright (c) 1981"Copyright (c)  , 1982"Copyright (c)  , 1983Infocom, Inc.All rights reserved.\nZORKis a registerred trademark of Infocom, Inc.\nRevision 88 / Serial number 840726\n\nWest of House\nYou are standing in an open field west of a white house, with a boarded front door.\nThere is a small mailbox here.\n\n\nValidactions: ['open mailbox', 'north', 'south', 'west']"
STEP 0observation:</p>
<p>Type \"help\" for more information about this version]\n\nDetective\nBy Matt Barringer.\nPortedby Stuart Moore.\nStuart_Moore@my-deja.com\nRelease 1 / Serial number 000715 / Inform v6.21 Library 6/10 SD\n\n&lt;&lt; Chief's office &gt;&gt;\nYou are standing in the Chief's office.He is telling you \"The Mayor was murdered yeaterday night at 12:03 am.I want you to solve it before we get any bad publicity or the FBI has to come in.\"Yessir!\"You reply.He hands you a sheet of paper.Once you have read it, go north or west.\n\nYoucan see a piece of white paper here.\n\n[Yourscore has just gone up by ten points.]\n\nValidactions: ['east', 'take paper', 'west', 'north']" "\nCONFIDENTIAL:\nDetective was created by Matt Barringer.\nHehas worked hard on this so you better enjoy it.\nIdid have fun making it though.But I'd REALLY appreciate it if you were kind enough to send a postcard or... dare I even say it?... money... to:\nMatt Barrin-ger\n325 Olive Ave\nPiedmont\nCA 94611\nJust tell me if you like it or not.\nIfyou want to talk to me over a BBS call the Ghostbuster Central BBS at (510)208-5657.\nThere is an Exile Games file area.Have fun.I WILL give hints out over the BBS to any of my games.\n\nValidactions:
STEP 0observation:"\n\n\n\n[STEP 1observation: "\nTaken.\n\n[Your score has just gone up by ten points.]\n\nValid actions: ['east','west', 'put paper down', 'north']"action: "TAKE PAPER"reward: 10.0STEP 2observation: ['east', 'west', 'put paper down', 'north']"action: "READ PAPER"reward: 0.0STEP 3
observation: "\nDropped.\n\nValidactions: ['east', 'take paper', 'west', 'north']" action: "DROP PAPER" reward: 0.0 STEP 4 observation: "\nYou are carrying nothing.\n\nValidactions: ['east', 'take paper', 'west', 'north']" action: "INVENTORY" reward: 0.0</p>
<p>iterations, its weight vectors θ m become strongly aligned with the cluster-center signal c k of every task k , while simultaneously suppressing spurious correlations with label signals and noise.[Chenetal.-lemmaE.14]Consequently,anyinput drawn from task k is routed to the corresponding subset of experts M k with probability 1 − o( 1 d ), even though the model is never given explicit task labels.Second, each expert in M k remains tightly specialised to its assigned task, but only chance-level performance on all other tasks.Lemma 5.2].</p>
<p>https://github.com/lucidrains/vit-pytorch
https://github.com/mistralai/mistral-inference/blob/main/src/mistral_ inference/moe.py
https://github.com/median-research-group/LibMTL
https://github.com/opendilab/LightZero
Note that at the very beginning of training, when the window cannot be fully populated, we use the available data within the window, which may lead to larger fluctuations at early steps.
PreprintA IMPLEMENTATION DETAILSThis appendix details the implementation of our ScaleZero model, explorations of key design spaces, the Dynamic Parameter Scaling strategy, hyperparameter configurations, and the computational resources used for our experiments.Unless otherwise specified, the training and inference details remain consistent with those in the paperPu et al. (2024).A.1 CORE MODIFICATIONS OF SCALEZERO OVER UNIZERO Our core model, ScaleZero, is built upon the UniZero baseline, with key components upgraded to enhance learning efficiency and final performance in complex multi-task environments.Visual Encoder: For image-based inputs (e.g., Atari), we employ a Vision Transformer (ViT) as the primary encoder.The input image (64x64x3) is divided into patches (8x8), which are then converted into token embeddings via a linear projection layer.Learnable positional embeddings are added, and the sequence is fed into a standard Transformer encoder (ViT-Base architecture) to produce a 768-dimensional latent state.Text Encoder: In text-based environments like Jericho, we use a pre-trained BGE (BAAI General Embedding) model(Xiao et al., 2023)to encode the textual observations from the game into fixeddimensional vectors.Unified World Model:• Backbone: The core of the model is a Transformer network.It processes state tokens from the encoder, historical action tokens, and optional task-embedding tokens.• UniZero (Baseline): The backbone consists of N standard Transformer blocks.We use N = 8 for the Atari26 benchmark and N = 4 for other benchmarks.• ScaleZero (Our Model): To maintain a comparable parameter count, the backbone is composed of N/2 Mixture-of-Experts (MoE) Transformer blocks.In each of these blocks, the standard feed-forward network (FFN) layer is replaced by an MoE layer.This layer comprises 8 expert networks and one shared expert, governed by a gating network that performs sparse routing.Latent State Normalization: In ScaleZero, we apply standard LayerNorm to normalize the latent state.A.2 EXPLORATIONS ON KEY DESIGN SPACESWe conducted several exploratory extensions to key designs of UniZero to validate their effectiveness in multi-task settings.Task Information Encoding: To mitigate representational conflicts in multi-task learning, we explored explicit encoding of task information.By introducing a learnable task embedding matrix, task_embed = nn.Embedding(task_id), and concatenating it with the state representation, new_latent_state = concat(latent_state, task_embed), we aimed for the model to better distinguish between different tasks.In our experiments, the latent_state dimension was 672, and the task_embed dimension was 96.Vision Transformer (ViT): Our ViT implementation is adapted from the lucidrains/vit-pytorch 1 library.The core hyperparameters, corresponding to a ViT-Base level, are summarized in Table4.Latent Normalization Strategy (SimNorm): As part of our ablation studies, we explored the SimNorm strategy, inspired by the TD-MPC2 paper.SimNorm partitions the latent state z into L groups, where each group is a V -dimensional simplex g.The transformation is given by:Preprint1in the main text shows performance degradation and worsening plasticity metrics for UniZero on complex tasks, Figure7illustrates that on four relatively simpler tasks from the Atari8 set, UniZero's training process and corresponding metrics remain stable.This suggests a correlation between task difficulty and plasticity degradation.To further probe the underlying cause of this degradation, we introduce the concept of effective rank(Dohare et al., 2024)to quantify the dimensionality of the model's representation space.A low effective rank suggests representational collapse, where the model fails to maintain diverse features across tasks.Formally, consider a matrix Φ ∈ R n×m with singular values σ k for k = 1, 2, . . ., q, where q = min(n, m).Let p k = σ k / q i=1 σ i be the normalized singular values, which form a probability distribution.The effective rank of Φ is then defined as the exponential of the entropy of this distribution: ∥ ≤ R, m = 1, . . ., M, and let routing weights for a task be λ 1 , . . ., λ M ≥ 0. The full-layer gradient is G t = concat(λ t 1 g(1)t , . . ., λ t M g (M ) t), Assume that for any task pair (t 1 , t 2 ), the gradient conflict on a single expert is bounded by G. Then the full-layer MoE gradient conflict satisfies the explicit upper boundFrom the block-level conflict bound we havet2 ∥.Then the numerator equals ⟨u, v⟩, and the denominator equals ∥u∥ ∥v∥.By the Cauchy-Schwarz inequality, ⟨u, v⟩ ≤ ∥u∥ ∥v∥, which implies that the fraction is at most 1.Substituting this bound back, we obtainEquality holds if and only if u and v are collinear (i.e., there exists c &gt; 0 such that u = c v), and each block-level conflict achieves its upper bound simultaneously.Remark 1 (Effect of Routing Strategies on Full-layer Gradient Conflict).The upper bound of fulllayer MoE gradient conflict depends strongly on the routing strategy.We summarize two representative cases:a. Dense Routing (Soft Gating,• All experts contribute to the full-layer gradient.• The fraction in Theorem 1 can be interpreted as the cosine similarity between the weighted vectors u = λ t1 ⊙ ∥g t1 ∥ and v = λ t2 ⊙ ∥g t2 ∥:• If the expert norms are roughly equal (∥gt2 ∥), the conflict upper bound is largely determined by cos(λ t1 , λ t2 ).• Uniform weights (λ m = 1/M ) do not automatically reduce conflict; alignment of weighted gradients matters more.• Overlap or alignment of routing vectors increases conflict, while orthogonal or disjoint routing vectors reduce it.Thus the encoder receives an unordered set {x 1 , . . ., x P } ⊂ R d containing exactly one action signal, one cluster-center signal (task-specific), one intra-task confounder, and Gaussian noise patches.MoE Model with Expert SpecializationWe now analyze how a Mixture-of-Experts (MoE) model can leverage expert specialization to address the aforementioned issues.Experts.We consider M linear experts.The value function of expert m is defined asRouter.The gating network assigns a score to each expert m:MoE Output.The overall value function is given bywhere the softmax gating weights areLoss Function.We minimize the mean squared Bellman error (MSBE) with a stop-gradient:where δ(s) denotes the temporal-difference (TD) error.E.2.3 LEARNING DYNAMICS ANALYSISWe follow an analysis path similar to prior workChen et al., dividing the learning process into two stages: an early expert exploration stage and a later router learning stage.Initialization.The expert weights w (0) m are randomly initialized from a small zero-mean Gaussian distribution:while the router weights θ
Mtlora: Low-rank adaptation approach for efficient multi-task learning. Ahmed Agiza, Marina Neseem, Sherief Reda, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2024</p>
<p>Planning in stochastic environments with a learned model. Ioannis Antonoglou, Julian Schrittwieser, Sherjil Ozair, Thomas K Hubert, David Silver, International Conference on Learning Representations. 2021</p>
<p>. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.064502016Layer normalization. arXiv preprint</p>
<p>Interrogate: learning to share, specialize, and prune representations for multi-task learning. Ehteshami Babak, Gaurav Bejnordi, Amelie Kumar, Christos Royer, Tijmen Louizos, Mohsen Blankevoort, Ghafoorian, arXiv:2402.168482024arXiv preprint</p>
<p>The arcade learning environment: An evaluation platform for general agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, Journal of Artificial Intelligence Research. 472013a</p>
<p>The arcade learning environment: An evaluation platform for general agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, Journal of artificial intelligence research. 472013b</p>
<p>Towards understanding a mixture of experts in deep learning. Chen, Deng, Wu, Gu, Li, arXiv:2208.028132022arXiv preprint</p>
<p>Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. Vijay Zhao Chen, Chen-Yu Badrinarayanan, Andrew Lee, Rabinovich, International conference on machine learning. PMLR2018</p>
<p>Hard tasks first: Multi-task reinforcement learning through task scheduling. Myungsik Cho, Jongeui Park, Suyoung Lee, Youngchul Sung, Forty-first International Conference on Machine Learning. 2024</p>
<p>Damai Dai, Chengqi Deng, Chenggang Zhao, Huazuo Xu, Deli Gao, Jiashi Chen, Wangding Li, Xingkai Zeng, Yu Yu, Wu, arXiv:2401.06066Towards ultimate expert specialization in mixtureof-experts language models. 2024arXiv preprint</p>
<p>Policy improvement by planning with gumbel. Ivo Danihelka, Arthur Guez, Julian Schrittwieser, David Silver, International Conference on Learning Representations. 2022</p>
<p>Carlo D' Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, Jan Peters, arXiv:2401.09561Sharing knowledge in multi-task deep reinforcement learning. 2024arXiv preprint</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, 2023. 2023</p>
<p>Loss of plasticity in deep continual learning. Shibhansh Dohare, Fernando Hernandez-Garcia, Qingfeng Lan, Parash Rahman, Richard S Rupam Mahmood, Sutton, Nature. 63280262024</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.119292020arXiv preprint</p>
<p>Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, arXiv:2312.09979Alleviate world knowledge forgetting in large language models via moe-style plugin. 2023arXiv preprint</p>
<p>Alleviating world knowledge forgetting in large language models via moe-style plugin. Enyu Preprint Shihan Dou, Yan Zhou, Songyang Liu, Wei Gao, Limao Shen, Yuhao Xiong, Xiao Zhou, Zhiheng Wang, Xiaoran Xi, Fan, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, International conference on machine learning. PMLR2018</p>
<p>Heshan Fernando, Han Shen, Miao Liu, Subhajit Chaudhury, Keerthiram Murugesan, Tianyi Chen, Mitigating gradient bias in multi-objective learning: A provably convergent approach. International Conference on Learning Representations. 2023</p>
<p>Quentin Gallouédec, Edward Beeching, Clément Romac, and Emmanuel Dellandréa. Jack of all trades, master of some, a multi-purpose transformer agent. Quentin Gallouédec, Edward Beeching, Clément Romac, Emmanuel Dellandréa, arXiv:2402.09844arXiv:2402.098442024a. 2024barXiv preprintJack of all trades, master of some, a multi-purpose transformer agent</p>
<p>Temporal difference learning for model predictive control. Nicklas Hansen, Xiaolong Wang, Hao Su, arXiv:2203.049552022arXiv preprint</p>
<p>Td-mpc2: Scalable, robust world models for continuous control. Nicklas Hansen, Hao Su, Xiaolong Wang, arXiv:2310.168282023arXiv preprint</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Lora+: Efficient low rank adaptation of large models. Soufiane Hayou, Nikhil Ghosh, Bin Yu, arXiv:2402.123542024arXiv preprint</p>
<p>Not all tasks are equally difficult: Multi-task reinforcement learning with dynamic depth routing. Jinmin He, Kai Li, Yifan Zang, Haobo Fu, Qiang Fu, Junliang Xing, Jian Cheng, 2023CoRR</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Momentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Ahmed Hendawy, Jan Peters, Carlo D' Eramo, arXiv:2311.11385Multi-task reinforcement learning with mixture of orthogonal experts. 2023arXiv preprint</p>
<p>Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, ICLR. 1232022</p>
<p>Learning and planning in complex action spaces. Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, David Silver, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Marina Meila, Tong Zhang, the 38th International Conference on Machine Learning, ICML 2021PMLR18-24 July 2021. 2021139of Proceedings of Machine Learning Research</p>
<p>Offline qlearning on diverse multi-task data both scales and generalizes. Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, Sergey Levine, arXiv:2211.151442022arXiv preprint</p>
<p>Contrastive modules with temporal attention for multi-task reinforcement learning. Preprint Siming Lan, Rui Zhang, Qi Yi, Jiaming Guo, Shaohui Peng, Yunkai Gao, Fan Wu, Ruizhi Chen, Zidong Du, Xing Hu, Advances in Neural Information Processing Systems. 202336</p>
<p>Multi-game decision transformers. Kuang-Huei Lee, Ofir Nachum, Sherry Mengjiao, Lisa Yang, Daniel Lee, Sergio Freeman, Ian Guadarrama, Winnie Fischer, Eric Xu, Henryk Jang, Michalewski, Advances in neural information processing systems. 2022a35</p>
<p>Multi-game decision transformers. Kuang-Huei Lee, Ofir Nachum, Sherry Mengjiao, Lisa Yang, Daniel Lee, Sergio Freeman, Ian Guadarrama, Winnie Fischer, Eric Xu, Henryk Jang, Michalewski, Advances in neural information processing systems. 2022b35</p>
<p>Smooth tchebycheff scalarization for multi-objective optimization. Xi Lin, Xiaoyuan Zhang, Zhiyuan Yang, Fei Liu, Zhenkun Wang, Qingfu Zhang, arXiv:2402.190782024arXiv preprint</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024aarXiv preprint</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024barXiv preprint</p>
<p>Weight-decomposed low-rank adaptation. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Min-Hung Chen, Forty-first International Conference on Machine Learning. Dora2024c</p>
<p>Haoyu Ma, Jialong Wu, Ningya Feng, Chenjun Xiao, Dong Li, Jianye Hao, Jianmin Wang, Mingsheng Long, Harmonydream, arXiv:2310.00344Task harmonization inside world models. 2023arXiv preprint</p>
<p>Vincent Micheli, Eloi Alonso, François Fleuret, arXiv:2209.00588Transformers are sample-efficient world models. 2022arXiv preprint</p>
<p>Playing atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller, arXiv:1312.56022013arXiv preprint</p>
<p>Johan Obando-Ceron, Ghada Sokar, Timon Willi, Clare Lyle, Jesse Farebrother, Jakob Foerster, Gintare Karolina Dziugaite, Doina Precup, Pablo Samuel Castro, arXiv:2402.08609Mixtures of experts unlock parameter scaling for deep rl. 2024arXiv preprint</p>
<p>Unizero: Generalized and efficient planning with scalable latent world models. Yuan Pu, Yazhe Niu, Zhenjie Yang, Jiyuan Ren, Hongsheng Li, Yu Liu, arXiv:2406.106672024arXiv preprint</p>
<p>Efficient off-policy meta-reinforcement learning via probabilistic context variables. Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, Sergey Levine, International conference on machine learning. PMLR2019</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. 2022</p>
<p>Transformer-based world models are happy with 100k interactions. Jan Robine, Marc Höftmann, Tobias Uelwer, Stefan Harmeling, 2023</p>
<p>The wasserstein distance and approximation theorems. Probability Theory and Related Fields. Ludger Rüschendorf, 198570</p>
<p>Learning to modulate pre-trained models in rl. Thomas Schmied, Markus Hofmarcher, Fabian Paischer, Razvan Pascanu, Sepp Hochreiter, Advances in Neural Information Processing Systems. 202336</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Preprint, Ioannis Schrittwieser, Thomas Antonoglou, Karen Hubert, Laurent Simonyan, Simon Sifre, Arthur Schmitt, Edward Guez, Demis Lockhart, Thore Hassabis, Timothy P Graepel, David Lillicrap, Silver, CoRR, abs/1911.082652019</p>
<p>Online and offline reinforcement learning by planning with a learned model. Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, David Silver, Advances in Neural Information Processing Systems. 342021</p>
<p>Contextual-aware representations for embodied multi-task reinforcement learning. Minsu Seo, Sang-Heon Choi, Kyung-Min Kim, Jonghyun Kim, International Conference on Machine Learning. PMLR2022</p>
<p>The sparsely-gated mixture-of-experts layer. Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, Dean, Outrageously large neural networks. 2017</p>
<p>Guangyuan Shi, Qimai Li, Wenlong Zhang, Jiaxin Chen, Xiao-Ming Wu, arXiv:2302.11289Recon: Reducing conflicting gradients from the root for multi-task learning. 2023arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 52975872016</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, arXiv:1712.018152017arXiv preprint</p>
<p>PMLR, 2021. Ghada Sokar, Rishabh Agarwal, Pablo Samuel Castro, and Utku Evci. The dormant neuron phenomenon in deep reinforcement learning. Shagun Sodhani, Amy Zhang, Joelle Pineau, International conference on machine learning. PMLR2023International Conference on Machine Learning</p>
<p>Language model-in-the-loop: Data optimal approach to learn-to-recommend actions in text games. Arjun Vaithilingam Sudhakar, Prasanna Parthasarathi, Janarthanan Rajendran, Sarath Chandar, arXiv:2311.076872023arXiv preprint</p>
<p>Paco: Parameter-compositional multi-task reinforcement learning. Lingfeng Sun, Haichao Zhang, Wei Xu, Masayoshi Tomizuka, Advances in Neural Information Processing Systems. 202235</p>
<p>Efficient multi-task and transfer reinforcement learning with parameter-compositional framework. Lingfeng Sun, Haichao Zhang, Wei Xu, Masayoshi Tomizuka, IEEE Robotics and Automation Letters. 882023</p>
<p>Aleksandar Todorov, Juan Cardenas-Cartagena, Rafael F Cunha, Marco Zullich, Matthia Sabatelli, arXiv:2508.06871Sparsity-driven plasticity in multi-task reinforcement learning. 2025arXiv preprint</p>
<p>Nicolas Heess, and Yuval Tassa. dm control: Software and tasks for continuous control. Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, 10.1016/j.simpa.2020.100022.URLhttps://www.sciencedirect.com/science/article/pii/S2665963820300099Software Impacts. 2665-963861000222020</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Nelson Vithayathil, Varghese Qusay, H Mahmoud, A survey of multi-task deep reinforcement learning. Electronics. 202091363</p>
<p>Multilora: Democratizing lora for better multi-task learning. Yiming Preprint, Yu Wang, Xiaodong Lin, Guannan Zeng, Zhang, arXiv:2311.115012023arXiv preprint</p>
<p>C-pack: Packaged resources to advance general chinese embedding. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, 2023</p>
<p>Multi-task reinforcement learning with soft modularization. Ruihan Yang, Huazhe Xu, Yi Wu, Xiaolong Wang, Advances in Neural Information Processing Systems. 202033</p>
<p>Mtl-lora: Low-rank adaptation for multi-task learning. Yaming Yang, Dilxat Muhtar, Yelong Shen, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Weiwei Deng, Feng Sun, Qi Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Mastering atari games with limited data. Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao, Advances in Neural Information Processing Systems. 202134</p>
<p>Unleashing the power of multi-task learning: A comprehensive survey spanning traditional, deep, and pretrained foundation model eras. Jun Yu, Yutong Dai, Xiaokang Liu, Jin Huang, Yishan Shen, Ke Zhang, Rong Zhou, Eashan Adhikarla, Wenxuan Ye, Yixin Liu, arXiv:2404.18961Advances in Neural Information Processing Systems. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn, 2024. 202033arXiv preprintGradient surgery for multi-task learning</p>
<p>Juzheng Zhang, Jiacheng You, Ashwinee Panda, Tom Goldstein, Lori, arXiv:2504.07448Reducing cross-task interference in multi-task low-rank adaptation. 2025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
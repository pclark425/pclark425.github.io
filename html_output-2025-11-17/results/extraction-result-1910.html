<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1910 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1910</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1910</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-278741071</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.12224v2.pdf" target="_blank">RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction</a></p>
                <p><strong>Paper Abstract:</strong> Vision-Language-Action (VLA) models have recently advanced robotic manipulation by translating natural-language instructions and image information into sequential control actions. However, these models often underperform in open-world scenarios, as they are predominantly trained on successful expert demonstrations and exhibit a limited capacity for failure recovery. In this work, we present a Robotic Failure Analysis and Correction ( RoboFAC ) framework to address this issue. Firstly, we construct RoboFAC dataset comprising 9,440 erroneous manipulation trajectories and 78,623 QA pairs across 16 diverse tasks and 53 scenes in both simulation and real-world environments. Leveraging our dataset, we develop RoboFAC model, which is capable of Task Understanding , Failure Analysis and Failure Correction . Experimental results demonstrate that the RoboFAC model outperforms GPT-4o by 34.1% on our evaluation benchmark. Furthermore, we integrate the RoboFAC model into a real-world VLA control pipeline as an external supervision providing correction instructions, yielding a 29.1% relative improvement on average on four real-world tasks. The results show that our Robo-FAC framework effectively handles robotic failures and assists the VLA model in recovering from failures.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1910.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1910.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboFAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robotic Failure Analysis and Correction (RoboFAC) model / dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal failure-analysis-and-correction system and accompanying large-scale video QA dataset (RoboFAC) for robotic manipulation; the model is a Qwen2.5-VL-derived MLLM fine-tuned on simulated failure videos and QA pairs to perform task understanding, failure analysis (detection/identification/locating/explanation) and generate high- and low-level corrective instructions for VLA systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboFAC-7B (also RoboFAC-3B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned from the Qwen2.5-VL multimodal backbone (LLM + vision encoder + MLP vision-language merger). Accepts video inputs and language queries to produce natural-language failure analyses and corrective instructions (high-level subtask sequences and low-level end-effector movement directions).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>inherits multimodal vision-language pretraining from Qwen2.5-VL; then supervised fine-tuning on video-based failure QA dataset (RoboFAC).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Fine-tuned on 60K simulated video QA pairs sampled from RoboFAC (9,440 erroneous trajectories used to produce 78K QA pairs overall); training data contain video clips, textual descriptions of substage, failure taxonomy, perturbation vectors (end-effector pose deltas) and human/GPT-4o-generated reference corrections; contains object labels, spatial relationships, described actions and corrective action language.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation failure analysis and correction (embodied VLA tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-task robotic arm manipulation across 16 tasks and 53 scenes in simulation and 6 real-world tasks; tasks include short-, medium-, and long-horizon manipulation and dynamic tasks (examples: PlaceCube, InsertCylinder, PullCubeTool, StackCube, PushCube, peg insertion, tool use). Action space is continuous robot-arm control (end-effector pose + gripper level); evaluations include both sim (ManiSkill + YCB + ReplicaCAD/AI2-THOR scenes) and real-world SO-100 arm teleoperation setups with multiple camera views.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper explicitly evaluates sim-to-real transfer: model fine-tuned only on simulated RoboFAC training set (60K QA pairs) and tested on both unseen simulated videos and a held-out real-world split (including two tasks absent in simulation) to measure domain alignment and generalization under varied backgrounds and viewpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Benchmark: RoboFAC-7B average score 79.10 (normalized 0-100 scale) on RoboFAC benchmark; RoboFAC-3B average 76.80. Paper reports RoboFAC-7B outperforms GPT-4o by 34.1% on the benchmark. Real-world corrective-integration: RoboFAC-7B (Low-level correction) achieved average success rate 61.25% (reported as 'after 4 attempts') and outperformed GPT-4o (56.25%), No Correction (47.5%), and Qwen2.5-VL-7B (50.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable as model is fine-tuned from a multimodal pretrained backbone; closest baseline: Qwen2.5-VL-7B (not fine-tuned on RoboFAC) average score 27.47 on the same RoboFAC benchmark (reported in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>The paper does not present quantitative sample-efficiency comparisons (e.g., learning curves) between language-pretrained vs non-language-pretrained agents; it reports training dataset sizes (60K QA pairs) and that model was trained for one epoch. No explicit demonstration of fewer samples required vs baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention-map or per-pixel / per-region attention visualization or analysis is reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No explicit analysis/visualization of embedding spaces, clustering, or representational geometry (PCA/UMAP/etc.) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Functional, task-level evidence: natural-language low-level corrections (e.g., 'move backward then left') produced by RoboFAC-7B when appended to the VLA prompt lead to measurable improvements in downstream robot success rates in real-world trials, suggesting practical grounding of corrective language to actionable end-effector adjustments; however, no mechanistic probing (e.g., mapping verbs to motor primitives or neuron-level grounding) is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>The paper uses a task/failure hierarchical taxonomy (task-planning, motion-planning, execution-control) and shows differential effectiveness of high-level vs low-level corrections (low-level corrections outperform high-level in the real-world pipeline). There is no representational analysis showing which network levels encode which hierarchical features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper finds sim-to-real generalization is feasible: model trained only on simulated failure QA still performs on real-world test split (including unseen tasks). Transfer success is tested under varied camera viewpoints, backgrounds, and object configurations. The integration study indicates that correcting low-level execution errors in language form is more effective than high-level plan corrections in their pipeline, suggesting transfer is sensitive to the ability of the downstream VLA to follow complex language.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>The evaluation includes two held-out real-world tasks (InsertCylinder and PlaceCube) not present in simulation to test generalization; the model was never trained on real-world data, but the paper does not break down per-task performance for novel vs familiar objects in a detailed quantitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>The RoboFAC model is evaluated in a sim-to-real zero-shot manner (no real-world fine-tuning of the RoboFAC model itself). The paper reports performance on real-world splits despite no real-world fine-tuning of RoboFAC, indicating zero-shot sim-to-real generalization at the level of corrective language generation.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-wise ablations, component-freeze experiments, or probing analyses of which layers contribute most to transfer are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit quantitative evidence that pretraining or fine-tuning produced negative transfer; the authors do note generalist MLLMs (e.g., GPT-4o) perform worse on specific task-planning/hierarchical correction metrics versus RoboFAC, but this is discussed qualitatively rather than as a negative-transfer failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>The paper does not report direct comparisons to vision-only pretraining baselines (e.g., ImageNet-only visual backbones) in the presented experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Although evaluated on long-horizon tasks and the model processes video segments, the paper does not present an analysis of how representations or performance evolve throughout fine-tuning (no learning-phase temporal dynamics are provided).</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No measures of representational dimensionality or intrinsic dimension are provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1910.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1910.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-VL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-VL (multimodal vision-language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal model composed of an LLM backbone, a vision encoder, and an MLP-based vision-language merger that supports single-image, multi-image and video inputs; used as both the base model for RoboFAC and as an open-source baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-VL (3B and 7B variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal model architecture: LLM backbone + vision encoder + MLP-based vision-language merger. Supports images and video at varying resolutions for visual question answering and multimodal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal vision-language pretraining (detailed pretraining regimen not specified in this paper; refer to Qwen2.5-VL technical report).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper; described only generally as an advanced open-source multimodal model capable of visual question answering and video inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic failure VideoQA and baseline evaluation on RoboFAC benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated as a baseline on the RoboFAC VideoQA tasks (task identification, planning, failure detection/identification/locating/explanation, high- and low-level correction) using the same simulated and real-world test splits as the RoboFAC models.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not discussed in detail; Qwen2.5-VL is used as a general open-source multimodal baseline and is outperformed by RoboFAC after domain-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Qwen2.5-VL-7B reported average on RoboFAC benchmark: 27.47 (normalized score, Table 2). Real-world correction pipeline using Qwen2.5-VL-7B achieved average success 50.0% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable - Qwen2.5-VL is itself a pretrained multimodal model; no vision-only ablation is provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Serves as a baseline and is shown to be less effective than RoboFAC after RoboFAC's task-specific fine-tuning; direct evidence of grounding mechanisms is not presented.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Evaluated as-is (without RoboFAC fine-tuning) and in the real-world correction pipeline; performs worse than RoboFAC when not fine-tuned on RoboFAC data.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Qwen2.5-VL is used zero-shot as a baseline on RoboFAC test splits (no additional fine-tuning on RoboFAC for the baseline runs reported).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1910.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1910.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GR00T-N1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GR00T-N1 (GRoot N1) foundation VLA model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open foundation model for generalist humanoid robots used as the primary VLA controller in the real-world pipeline; fine-tuned on teleoperated demonstrations collected by the authors to improve task execution before evaluating integration with RoboFAC corrective feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GR00T-N1 (fine-tuned for experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A generalist vision-language-action (VLA) foundation model for robot control (proprietary / large open foundation model referenced in related work). In the paper, GR00T-N1 is fine-tuned with teleoperated demonstrations to improve downstream execution in the tested real-world tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in this paper (referenced as an existing foundation VLA model); the authors fine-tune it on task-specific teleoperation data for real-world deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified here. For experiments, GR00T-N1 was fine-tuned on >300 teleoperated demonstrations per task collected from three synchronized viewpoints (wrist, top-down, front-left) with control signals to adapt it to the target manipulation scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Real-world robotic manipulation (PlaceCube, PushCube, PullCubeTool, StackCube, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Continuous-control manipulation on a SO-100 robotic arm with multi-view video input, teleoperated demonstration fine-tuning, and subsequent evaluation of corrective instruction integration; tasks include grasping, placing, pushing, pulling and stacking with varied objects and backgrounds.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>The authors curate teleoperation demonstrations in the target real-world setup to align GR00T-N1's distribution with evaluation conditions; this fine-tuning is explicitly used to improve the baseline VLA before adding external corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>As the execution policy in the 'No Correction' condition (after fine-tuning), GR00T-N1 achieved an average success rate of 47.5% across tested real-world tasks (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported for GR00T-N1 itself in this paper (the authors do not report GR00T-N1 pre-fine-tuning numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>The paper reports collection of over 300 teleoperated demonstrations per task to fine-tune GR00T-N1; it does not present a quantitative comparison of sample efficiency vs other pretraining regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>GR00T-N1 is used as an embodied controller that accepts appended natural-language correction instructions; the downstream improvement when RoboFAC corrections are appended suggests GR00T-N1 can parse and act on simple corrective language, but mechanistic grounding analyses are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported beyond the use of high-level vs low-level correction modes in the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Fine-tuning via teleoperation (300+ demos/task) is used to adapt GR00T-N1 to the specific SO-100 setup and viewpoints; integration success depends on the model's ability to follow appended natural language and the nature (low vs high level) of corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>The GR00T-N1 used in experiments was fine-tuned on multiple demonstrations; no pure zero-shot execution metrics are provided for pre-fine-tuned GR00T-N1 within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1910.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1910.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (multimodal variant used as MLLM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary multimodal large language model (MLLM) used as a strong baseline for failure analysis and correction in robotic tasks; evaluated on the RoboFAC VideoQA benchmark and in the real-world correction pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (evaluated as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary multimodal LLM with vision-capable inputs used for visual reasoning and language-based failure diagnosis; treated as a generalist MLLM baseline for failure understanding and correction.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in this paper (proprietary multimodal LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>RoboFAC VideoQA benchmark and real-world corrective-integration evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on the same RoboFAC tasks and video QA dimensions: task understanding, failure detection/identification/locating/explanation, and providing corrective instructions in a real-world VLA pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Used directly as a generalist baseline; authors note GPT-4o is competitive on some metrics (failure detection) but weaker on task planning and hierarchical correction without domain-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Benchmark: GPT-4o average score 57.42 (normalized). Real-world correction pipeline: average success rate 56.25% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable; GPT-4o is a pretrained multimodal model and no ablation without pretraining is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>GPT-4o can perform failure detection and generate explanations but in this paper is shown to be less effective at hierarchical task planning and generating low-level corrections that lead to higher downstream success compared to RoboFAC after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Authors report GPT-4o underperforms in task planning and hierarchical correction relative to RoboFAC, but no internal feature analysis is shown.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Evaluated zero-shot (no RoboFAC fine-tuning) on RoboFAC benchmark and in real-world pipeline; performs worse than domain-fine-tuned RoboFAC.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Evaluated zero-shot as a generalist model in RoboFAC benchmark and real-world pipeline; reported metrics above.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not explicitly reported; qualitative gaps in hierarchical reasoning noted.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not applicable / not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1910.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1910.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-2.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-2.0 (proprietary multimodal model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary multimodal model evaluated as a baseline on the RoboFAC benchmark; included to compare generalist MLLMs against the domain-fine-tuned RoboFAC models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary multimodal model used as a generalist baseline for vision-language reasoning in the RoboFAC benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>RoboFAC VideoQA benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on simulated and real-world RoboFAC VideoQA tasks; used as a baseline for multi-dimensional failure understanding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not discussed in detail; treated as a generalist MLLM baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Benchmark average reported as 51.11 (normalized score, Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable / not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>No direct grounding analyses reported; performance indicates domain-specific fine-tuning (RoboFAC) yields large improvements over such generalist baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not reported beyond benchmark evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Evaluated zero-shot in RoboFAC benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1910.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1910.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 (Vision-Language-Action model referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited example of a VLA approach that represents robot actions as text tokens to unify vision, language and action modalities and transfer web-scale knowledge to robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as a model that represents robot actions as text tokens and leverages pre-trained vision-language models to perform robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Described as leveraging pre-trained vision-language models (paper does not provide further pretraining specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed in this paper (RT-2 is cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation / vision-language-action tasks (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Not described in detail here; referenced as prior work that transfers web knowledge to robotic control by tokenizing actions.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Mentioned qualitatively as motivating VLA paradigms; no quantitative alignment data presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Mentioned conceptually (actions-as-text) but no empirical evidence provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1910.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1910.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>0 (pi-zero) Vision-Language-Action flow model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior VLA model that uses a flow-matching diffusion decoder to convert hidden representations into continuous actions for general robot control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in related work as a VLA flow model that decodes hidden representations via flow-matching diffusion into continuous robot actions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Described as an approach for general robot control; specific pretraining details are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>General robot control / manipulation (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Not detailed here; cited for its continuous-action decoding approach.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Mentioned as decoding to continuous actions (conceptual), no grounding analysis provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1910.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1910.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GR-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GR-2 / Gr-2 (generative video-language-action model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned prior work (Gr-2 / GR-2) that follows a two-stage training paradigm: pre-training on large-scale internet videos to learn dynamics, then fine-tuning on robot trajectories to predict actions and generate video, enabling generalization across manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GR-2 (Gr-2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage generative video-language-action model: (1) pre-train on large-scale internet videos to learn general world dynamics; (2) fine-tune on robot trajectories for action prediction and video generation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal video-text pretraining (large-scale internet videos) followed by fine-tuning on robot trajectories (as described in the related work summary).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale internet videos to learn general dynamics; then robot trajectory data for action prediction (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation / action prediction and video generation (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Two-stage pipeline meant to improve generalization across manipulation tasks via large-scale video pretraining then robot-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Conceptually described as aligning general video dynamics learned from web-scale videos with robot trajectory fine-tuning, but no empirical data provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Described conceptually (pretraining on videos then fine-tuning on robot trajectories) as a route to grounding, but no empirical analysis provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not reported here; conceptually relies on domain alignment between web videos and robotic trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RT-2: Vision-language-action models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>RT-1: Robotics transformer for real-world control at scale. <em>(Rating: 2)</em></li>
                <li> 0 : A vision-language-action flow model for general robot control. <em>(Rating: 2)</em></li>
                <li>Gr00t n1: An open foundation model for generalist humanoid robots. <em>(Rating: 2)</em></li>
                <li>Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation. <em>(Rating: 2)</em></li>
                <li>AHA: A vision-language-model for detecting and reasoning over failures in robotic manipulation. <em>(Rating: 2)</em></li>
                <li>RACER: Rich language-guided failure recovery policies for imitation learning. <em>(Rating: 2)</em></li>
                <li>Reflect: Summarizing robot experiences for failure explanation and correction. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1910",
    "paper_id": "paper-278741071",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "RoboFAC",
            "name_full": "Robotic Failure Analysis and Correction (RoboFAC) model / dataset",
            "brief_description": "A multimodal failure-analysis-and-correction system and accompanying large-scale video QA dataset (RoboFAC) for robotic manipulation; the model is a Qwen2.5-VL-derived MLLM fine-tuned on simulated failure videos and QA pairs to perform task understanding, failure analysis (detection/identification/locating/explanation) and generate high- and low-level corrective instructions for VLA systems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoboFAC-7B (also RoboFAC-3B)",
            "model_description": "Fine-tuned from the Qwen2.5-VL multimodal backbone (LLM + vision encoder + MLP vision-language merger). Accepts video inputs and language queries to produce natural-language failure analyses and corrective instructions (high-level subtask sequences and low-level end-effector movement directions).",
            "pretraining_type": "inherits multimodal vision-language pretraining from Qwen2.5-VL; then supervised fine-tuning on video-based failure QA dataset (RoboFAC).",
            "pretraining_data_description": "Fine-tuned on 60K simulated video QA pairs sampled from RoboFAC (9,440 erroneous trajectories used to produce 78K QA pairs overall); training data contain video clips, textual descriptions of substage, failure taxonomy, perturbation vectors (end-effector pose deltas) and human/GPT-4o-generated reference corrections; contains object labels, spatial relationships, described actions and corrective action language.",
            "target_task_name": "Robotic manipulation failure analysis and correction (embodied VLA tasks)",
            "target_task_description": "Multi-task robotic arm manipulation across 16 tasks and 53 scenes in simulation and 6 real-world tasks; tasks include short-, medium-, and long-horizon manipulation and dynamic tasks (examples: PlaceCube, InsertCylinder, PullCubeTool, StackCube, PushCube, peg insertion, tool use). Action space is continuous robot-arm control (end-effector pose + gripper level); evaluations include both sim (ManiSkill + YCB + ReplicaCAD/AI2-THOR scenes) and real-world SO-100 arm teleoperation setups with multiple camera views.",
            "semantic_alignment": "Paper explicitly evaluates sim-to-real transfer: model fine-tuned only on simulated RoboFAC training set (60K QA pairs) and tested on both unseen simulated videos and a held-out real-world split (including two tasks absent in simulation) to measure domain alignment and generalization under varied backgrounds and viewpoints.",
            "performance_with_language_pretraining": "Benchmark: RoboFAC-7B average score 79.10 (normalized 0-100 scale) on RoboFAC benchmark; RoboFAC-3B average 76.80. Paper reports RoboFAC-7B outperforms GPT-4o by 34.1% on the benchmark. Real-world corrective-integration: RoboFAC-7B (Low-level correction) achieved average success rate 61.25% (reported as 'after 4 attempts') and outperformed GPT-4o (56.25%), No Correction (47.5%), and Qwen2.5-VL-7B (50.0%).",
            "performance_without_language_pretraining": "Not applicable as model is fine-tuned from a multimodal pretrained backbone; closest baseline: Qwen2.5-VL-7B (not fine-tuned on RoboFAC) average score 27.47 on the same RoboFAC benchmark (reported in Table 2).",
            "sample_efficiency_comparison": "The paper does not present quantitative sample-efficiency comparisons (e.g., learning curves) between language-pretrained vs non-language-pretrained agents; it reports training dataset sizes (60K QA pairs) and that model was trained for one epoch. No explicit demonstration of fewer samples required vs baselines.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention-map or per-pixel / per-region attention visualization or analysis is reported in the paper.",
            "embedding_space_analysis": "No explicit analysis/visualization of embedding spaces, clustering, or representational geometry (PCA/UMAP/etc.) is reported.",
            "action_grounding_evidence": "Functional, task-level evidence: natural-language low-level corrections (e.g., 'move backward then left') produced by RoboFAC-7B when appended to the VLA prompt lead to measurable improvements in downstream robot success rates in real-world trials, suggesting practical grounding of corrective language to actionable end-effector adjustments; however, no mechanistic probing (e.g., mapping verbs to motor primitives or neuron-level grounding) is provided.",
            "hierarchical_features_evidence": "The paper uses a task/failure hierarchical taxonomy (task-planning, motion-planning, execution-control) and shows differential effectiveness of high-level vs low-level corrections (low-level corrections outperform high-level in the real-world pipeline). There is no representational analysis showing which network levels encode which hierarchical features.",
            "transfer_conditions": "Paper finds sim-to-real generalization is feasible: model trained only on simulated failure QA still performs on real-world test split (including unseen tasks). Transfer success is tested under varied camera viewpoints, backgrounds, and object configurations. The integration study indicates that correcting low-level execution errors in language form is more effective than high-level plan corrections in their pipeline, suggesting transfer is sensitive to the ability of the downstream VLA to follow complex language.",
            "novel_vs_familiar_objects": "The evaluation includes two held-out real-world tasks (InsertCylinder and PlaceCube) not present in simulation to test generalization; the model was never trained on real-world data, but the paper does not break down per-task performance for novel vs familiar objects in a detailed quantitative comparison.",
            "zero_shot_or_few_shot": "The RoboFAC model is evaluated in a sim-to-real zero-shot manner (no real-world fine-tuning of the RoboFAC model itself). The paper reports performance on real-world splits despite no real-world fine-tuning of RoboFAC, indicating zero-shot sim-to-real generalization at the level of corrective language generation.",
            "layer_analysis": "No layer-wise ablations, component-freeze experiments, or probing analyses of which layers contribute most to transfer are reported.",
            "negative_transfer_evidence": "No explicit quantitative evidence that pretraining or fine-tuning produced negative transfer; the authors do note generalist MLLMs (e.g., GPT-4o) perform worse on specific task-planning/hierarchical correction metrics versus RoboFAC, but this is discussed qualitatively rather than as a negative-transfer failure mode.",
            "comparison_to_vision_only": "The paper does not report direct comparisons to vision-only pretraining baselines (e.g., ImageNet-only visual backbones) in the presented experiments.",
            "temporal_dynamics": "Although evaluated on long-horizon tasks and the model processes video segments, the paper does not present an analysis of how representations or performance evolve throughout fine-tuning (no learning-phase temporal dynamics are provided).",
            "dimensionality_analysis": "No measures of representational dimensionality or intrinsic dimension are provided.",
            "uuid": "e1910.0"
        },
        {
            "name_short": "Qwen2.5-VL",
            "name_full": "Qwen2.5-VL (multimodal vision-language model)",
            "brief_description": "An open-source multimodal model composed of an LLM backbone, a vision encoder, and an MLP-based vision-language merger that supports single-image, multi-image and video inputs; used as both the base model for RoboFAC and as an open-source baseline in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-VL (3B and 7B variants)",
            "model_description": "Multimodal model architecture: LLM backbone + vision encoder + MLP-based vision-language merger. Supports images and video at varying resolutions for visual question answering and multimodal tasks.",
            "pretraining_type": "Multimodal vision-language pretraining (detailed pretraining regimen not specified in this paper; refer to Qwen2.5-VL technical report).",
            "pretraining_data_description": "Not specified in this paper; described only generally as an advanced open-source multimodal model capable of visual question answering and video inputs.",
            "target_task_name": "Robotic failure VideoQA and baseline evaluation on RoboFAC benchmark",
            "target_task_description": "Evaluated as a baseline on the RoboFAC VideoQA tasks (task identification, planning, failure detection/identification/locating/explanation, high- and low-level correction) using the same simulated and real-world test splits as the RoboFAC models.",
            "semantic_alignment": "Not discussed in detail; Qwen2.5-VL is used as a general open-source multimodal baseline and is outperformed by RoboFAC after domain-specific fine-tuning.",
            "performance_with_language_pretraining": "Qwen2.5-VL-7B reported average on RoboFAC benchmark: 27.47 (normalized score, Table 2). Real-world correction pipeline using Qwen2.5-VL-7B achieved average success 50.0% (Table 3).",
            "performance_without_language_pretraining": "Not applicable - Qwen2.5-VL is itself a pretrained multimodal model; no vision-only ablation is provided in the paper.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this paper.",
            "embedding_space_analysis": "Not reported in this paper.",
            "action_grounding_evidence": "Serves as a baseline and is shown to be less effective than RoboFAC after RoboFAC's task-specific fine-tuning; direct evidence of grounding mechanisms is not presented.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Evaluated as-is (without RoboFAC fine-tuning) and in the real-world correction pipeline; performs worse than RoboFAC when not fine-tuned on RoboFAC data.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Qwen2.5-VL is used zero-shot as a baseline on RoboFAC test splits (no additional fine-tuning on RoboFAC for the baseline runs reported).",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1910.1"
        },
        {
            "name_short": "GR00T-N1",
            "name_full": "GR00T-N1 (GRoot N1) foundation VLA model",
            "brief_description": "An open foundation model for generalist humanoid robots used as the primary VLA controller in the real-world pipeline; fine-tuned on teleoperated demonstrations collected by the authors to improve task execution before evaluating integration with RoboFAC corrective feedback.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GR00T-N1 (fine-tuned for experiments)",
            "model_description": "A generalist vision-language-action (VLA) foundation model for robot control (proprietary / large open foundation model referenced in related work). In the paper, GR00T-N1 is fine-tuned with teleoperated demonstrations to improve downstream execution in the tested real-world tasks.",
            "pretraining_type": "Not specified in this paper (referenced as an existing foundation VLA model); the authors fine-tune it on task-specific teleoperation data for real-world deployment.",
            "pretraining_data_description": "Not specified here. For experiments, GR00T-N1 was fine-tuned on &gt;300 teleoperated demonstrations per task collected from three synchronized viewpoints (wrist, top-down, front-left) with control signals to adapt it to the target manipulation scenarios.",
            "target_task_name": "Real-world robotic manipulation (PlaceCube, PushCube, PullCubeTool, StackCube, etc.)",
            "target_task_description": "Continuous-control manipulation on a SO-100 robotic arm with multi-view video input, teleoperated demonstration fine-tuning, and subsequent evaluation of corrective instruction integration; tasks include grasping, placing, pushing, pulling and stacking with varied objects and backgrounds.",
            "semantic_alignment": "The authors curate teleoperation demonstrations in the target real-world setup to align GR00T-N1's distribution with evaluation conditions; this fine-tuning is explicitly used to improve the baseline VLA before adding external corrections.",
            "performance_with_language_pretraining": "As the execution policy in the 'No Correction' condition (after fine-tuning), GR00T-N1 achieved an average success rate of 47.5% across tested real-world tasks (Table 3).",
            "performance_without_language_pretraining": "Not reported for GR00T-N1 itself in this paper (the authors do not report GR00T-N1 pre-fine-tuning numbers).",
            "sample_efficiency_comparison": "The paper reports collection of over 300 teleoperated demonstrations per task to fine-tune GR00T-N1; it does not present a quantitative comparison of sample efficiency vs other pretraining regimes.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "GR00T-N1 is used as an embodied controller that accepts appended natural-language correction instructions; the downstream improvement when RoboFAC corrections are appended suggests GR00T-N1 can parse and act on simple corrective language, but mechanistic grounding analyses are not provided.",
            "hierarchical_features_evidence": "Not reported beyond the use of high-level vs low-level correction modes in the pipeline.",
            "transfer_conditions": "Fine-tuning via teleoperation (300+ demos/task) is used to adapt GR00T-N1 to the specific SO-100 setup and viewpoints; integration success depends on the model's ability to follow appended natural language and the nature (low vs high level) of corrections.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "The GR00T-N1 used in experiments was fine-tuned on multiple demonstrations; no pure zero-shot execution metrics are provided for pre-fine-tuned GR00T-N1 within this paper.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1910.2"
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (multimodal variant used as MLLM baseline)",
            "brief_description": "A proprietary multimodal large language model (MLLM) used as a strong baseline for failure analysis and correction in robotic tasks; evaluated on the RoboFAC VideoQA benchmark and in the real-world correction pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o (evaluated as baseline)",
            "model_description": "Proprietary multimodal LLM with vision-capable inputs used for visual reasoning and language-based failure diagnosis; treated as a generalist MLLM baseline for failure understanding and correction.",
            "pretraining_type": "Not specified in this paper (proprietary multimodal LLM).",
            "pretraining_data_description": "Not specified in this paper.",
            "target_task_name": "RoboFAC VideoQA benchmark and real-world corrective-integration evaluation",
            "target_task_description": "Evaluated on the same RoboFAC tasks and video QA dimensions: task understanding, failure detection/identification/locating/explanation, and providing corrective instructions in a real-world VLA pipeline.",
            "semantic_alignment": "Used directly as a generalist baseline; authors note GPT-4o is competitive on some metrics (failure detection) but weaker on task planning and hierarchical correction without domain-specific fine-tuning.",
            "performance_with_language_pretraining": "Benchmark: GPT-4o average score 57.42 (normalized). Real-world correction pipeline: average success rate 56.25% (Table 3).",
            "performance_without_language_pretraining": "Not applicable; GPT-4o is a pretrained multimodal model and no ablation without pretraining is provided.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this paper.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "GPT-4o can perform failure detection and generate explanations but in this paper is shown to be less effective at hierarchical task planning and generating low-level corrections that lead to higher downstream success compared to RoboFAC after fine-tuning.",
            "hierarchical_features_evidence": "Authors report GPT-4o underperforms in task planning and hierarchical correction relative to RoboFAC, but no internal feature analysis is shown.",
            "transfer_conditions": "Evaluated zero-shot (no RoboFAC fine-tuning) on RoboFAC benchmark and in real-world pipeline; performs worse than domain-fine-tuned RoboFAC.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Evaluated zero-shot as a generalist model in RoboFAC benchmark and real-world pipeline; reported metrics above.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not explicitly reported; qualitative gaps in hierarchical reasoning noted.",
            "comparison_to_vision_only": "Not applicable / not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1910.3"
        },
        {
            "name_short": "Gemini-2.0",
            "name_full": "Gemini-2.0 (proprietary multimodal model)",
            "brief_description": "A proprietary multimodal model evaluated as a baseline on the RoboFAC benchmark; included to compare generalist MLLMs against the domain-fine-tuned RoboFAC models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-2.0",
            "model_description": "Proprietary multimodal model used as a generalist baseline for vision-language reasoning in the RoboFAC benchmark.",
            "pretraining_type": "Not specified in this paper.",
            "pretraining_data_description": "Not specified in this paper.",
            "target_task_name": "RoboFAC VideoQA benchmark",
            "target_task_description": "Evaluated on simulated and real-world RoboFAC VideoQA tasks; used as a baseline for multi-dimensional failure understanding tasks.",
            "semantic_alignment": "Not discussed in detail; treated as a generalist MLLM baseline.",
            "performance_with_language_pretraining": "Benchmark average reported as 51.11 (normalized score, Table 2).",
            "performance_without_language_pretraining": "Not applicable / not reported.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "No direct grounding analyses reported; performance indicates domain-specific fine-tuning (RoboFAC) yields large improvements over such generalist baselines.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Not reported beyond benchmark evaluation.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Evaluated zero-shot in RoboFAC benchmark.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1910.4"
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2 (Vision-Language-Action model referenced)",
            "brief_description": "Cited example of a VLA approach that represents robot actions as text tokens to unify vision, language and action modalities and transfer web-scale knowledge to robotic control.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "RT-2",
            "model_description": "Described in the paper as a model that represents robot actions as text tokens and leverages pre-trained vision-language models to perform robotic control.",
            "pretraining_type": "Described as leveraging pre-trained vision-language models (paper does not provide further pretraining specifics).",
            "pretraining_data_description": "Not detailed in this paper (RT-2 is cited in related work).",
            "target_task_name": "Robotic manipulation / vision-language-action tasks (mentioned in related work)",
            "target_task_description": "Not described in detail here; referenced as prior work that transfers web knowledge to robotic control by tokenizing actions.",
            "semantic_alignment": "Mentioned qualitatively as motivating VLA paradigms; no quantitative alignment data presented in this paper.",
            "performance_with_language_pretraining": "Not reported in this paper.",
            "performance_without_language_pretraining": "Not reported in this paper.",
            "sample_efficiency_comparison": "Not reported in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Mentioned conceptually (actions-as-text) but no empirical evidence provided in this paper.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Not analyzed in this paper.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not reported here.",
            "temporal_dynamics": "Not reported here.",
            "dimensionality_analysis": "Not reported here.",
            "uuid": "e1910.5"
        },
        {
            "name_short": "0",
            "name_full": "0 (pi-zero) Vision-Language-Action flow model",
            "brief_description": "Referenced prior VLA model that uses a flow-matching diffusion decoder to convert hidden representations into continuous actions for general robot control.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "0",
            "model_description": "Described in related work as a VLA flow model that decodes hidden representations via flow-matching diffusion into continuous robot actions.",
            "pretraining_type": "Described as an approach for general robot control; specific pretraining details are not provided in this paper.",
            "pretraining_data_description": "Not specified in this paper.",
            "target_task_name": "General robot control / manipulation (mentioned in related work)",
            "target_task_description": "Not detailed here; cited for its continuous-action decoding approach.",
            "semantic_alignment": "Not analyzed in this paper.",
            "performance_with_language_pretraining": "Not reported here.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Mentioned as decoding to continuous actions (conceptual), no grounding analysis provided in this paper.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Not reported here.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not reported here.",
            "temporal_dynamics": "Not reported here.",
            "dimensionality_analysis": "Not reported here.",
            "uuid": "e1910.6"
        },
        {
            "name_short": "GR-2",
            "name_full": "GR-2 / Gr-2 (generative video-language-action model)",
            "brief_description": "Mentioned prior work (Gr-2 / GR-2) that follows a two-stage training paradigm: pre-training on large-scale internet videos to learn dynamics, then fine-tuning on robot trajectories to predict actions and generate video, enabling generalization across manipulation tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GR-2 (Gr-2)",
            "model_description": "Two-stage generative video-language-action model: (1) pre-train on large-scale internet videos to learn general world dynamics; (2) fine-tune on robot trajectories for action prediction and video generation.",
            "pretraining_type": "Multimodal video-text pretraining (large-scale internet videos) followed by fine-tuning on robot trajectories (as described in the related work summary).",
            "pretraining_data_description": "Large-scale internet videos to learn general dynamics; then robot trajectory data for action prediction (details not provided in this paper).",
            "target_task_name": "Robotic manipulation / action prediction and video generation (mentioned in related work)",
            "target_task_description": "Two-stage pipeline meant to improve generalization across manipulation tasks via large-scale video pretraining then robot-specific fine-tuning.",
            "semantic_alignment": "Conceptually described as aligning general video dynamics learned from web-scale videos with robot trajectory fine-tuning, but no empirical data provided in this paper.",
            "performance_with_language_pretraining": "Not reported here.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Described conceptually (pretraining on videos then fine-tuning on robot trajectories) as a route to grounding, but no empirical analysis provided in this paper.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Not reported here; conceptually relies on domain alignment between web videos and robotic trajectories.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not reported here.",
            "temporal_dynamics": "Not reported here.",
            "dimensionality_analysis": "Not reported here.",
            "uuid": "e1910.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control.",
            "rating": 2
        },
        {
            "paper_title": "RT-1: Robotics transformer for real-world control at scale.",
            "rating": 2
        },
        {
            "paper_title": " 0 : A vision-language-action flow model for general robot control.",
            "rating": 2
        },
        {
            "paper_title": "Gr00t n1: An open foundation model for generalist humanoid robots.",
            "rating": 2
        },
        {
            "paper_title": "Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation.",
            "rating": 2
        },
        {
            "paper_title": "AHA: A vision-language-model for detecting and reasoning over failures in robotic manipulation.",
            "rating": 2
        },
        {
            "paper_title": "RACER: Rich language-guided failure recovery policies for imitation learning.",
            "rating": 2
        },
        {
            "paper_title": "Reflect: Summarizing robot experiences for failure explanation and correction.",
            "rating": 2
        }
    ],
    "cost": 0.022677,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction
25 May 2025</p>
<p>Weifeng Lu 
School of AI
Shanghai Jiao Tong University</p>
<p>Xiamen University</p>
<p>Minghao Ye 
School of AI
Shanghai Jiao Tong University</p>
<p>Harbin Institute of Technology
Shenzhen</p>
<p>Zewei Ye 
Ruihan Tao 
School of AI
Shanghai Jiao Tong University</p>
<p>Shuo Yang 
School of AI
Shanghai Jiao Tong University</p>
<p>Harbin Institute of Technology
Shenzhen</p>
<p>Bo Zhao bo.zhao@sjtu.edu.cn 
School of AI
Shanghai Jiao Tong University</p>
<p>Robofac Dataset 
RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction
25 May 2025BC08F1FAD55D6552CBC7B004F9F075A3arXiv:2505.12224v3[cs.RO]<substage-1><substage-2>
Vision-Language-Action (VLA) models have recently advanced robotic manipulation by translating natural-language instructions and image information into sequential control actions.However, these models often underperform in open-world scenarios, as they are predominantly trained on successful expert demonstrations and exhibit a limited capacity for failure recovery.In this work, we present a Robotic Failure Analysis and Correction (RoboFAC) framework to address this issue.Firstly, we construct RoboFAC dataset comprising 9,440 erroneous manipulation trajectories and 78,623 QA pairs across 16 diverse tasks and 53 scenes in both simulation and real-world environments.Leveraging our dataset, we develop RoboFAC model, which is capable of Task Understanding, Failure Analysis and Failure Correction.Experimental results demonstrate that the RoboFAC model outperforms GPT-4o by 34.1% on our evaluation benchmark.Furthermore, we integrate the RoboFAC model into a real-world VLA control pipeline as an external supervision providing correction instructions, yielding a 29.1% relative improvement on average on four real-world tasks.The results show that our RoboFAC framework effectively handles robotic failures and assists the VLA model in recovering from failures.Our model and dataset are publicly available at https://github.com/MINT-SJTU/RoboFAC.</p>
<p>Introduction</p>
<p>Vision-Language-Action (VLA) models have achieved remarkable success in robotic manipulation, demonstrating strong generalization capabilities [1][2][3][4][5][6][7][8][9].Given a language-based task instruction, a VLA model can effectively ground the instruction into executable robot actions based on visual input and the robot's proprioceptive signal.However, task execution may sometimes fail.This can be attributed to two main factors: (1) the VLA model's limited ability to handle the complexity of the physical world, and (2) the inherent incompleteness of the task instruction, which often lacks detailed guidance on how to accomplish the task, especially in long-horizon or complex scenarios [10].Since VLA models are not explicitly trained on failure recovery data, they struggle to recover to the correct action once an error occurs.</p>
<p>To address this issue, a promising way is to deploy an external critic model capable of detecting failures and assisting the VLA model in recovery.Some recent studies have investigated the use of general-purpose multimodal large language models (MLLMs) as such critics, leveraging their strong perception and reasoning abilities [11][12][13][14].However, these models are not specifically trained on robot manipulation failure data and often struggle when applied directly to failure analysis and correction in robotic tasks.Some studies have attempted to collect robot failure data and fine-tune MLLMs on such examples [15,16].While this improves performance in identifying and reasoning about failures, current robot failure datasets are limited to simple robotic tasks, lack comprehensive analysis of failures, and do not provide correction suggestions across different levels of execution.(Table1).</p>
<p>In this work, we propose a comprehensive robotic failure analysis and correction framework.As illustrated in Figure 1, we first construct a large-scale and diverse dataset of robotic failures (RoboFAC dataset) covering robotic tasks of varying complexity in both simulated and real-world environments.The dataset incorporates diverse backgrounds and camera viewpoints, enhancing its visual diversity.We categorize robotic failures into six types, organized across different levels of execution, including task planning error, motion planning error, and execution control error.Our dataset is labeled with multi-dimensional information, comprising eight question types and totaling 78K video-questionanswer (QA) pairs.Based on this dataset, we establish a comprehensive evaluation benchmark that assesses multimodal models' capabilities in robotic failure video understanding.</p>
<p>Leveraging the RoboFAC dataset, we build a multimodal large model (RoboFAC model) capable of performing robotic task understanding, failure analysis, and failure correction based on robot video.Evaluation results show that our RoboFAC-7B model achieves state-of-the-art performance, outperforming GPT-4o by 34.1% on the benchmark.To further validate the failure correction capability, we integrate our model into a control pipeline as an external critic for the VLA model, enabling it to provide recovery suggestions when the VLA model encounters failures.Experiments on five real-world tasks demonstrate that our model improves the success rate by an average of 13.75%, outperforming GPT-4o.</p>
<p>Our contributions can be summarized as follows:</p>
<p> We propose a large-scale and diverse robotic failure QA dataset, covering a wide range of tasks, environments, and viewpoints.It includes eight QA types targeting different aspects of robotic failure understanding and correction. A lightweight model tailored for robotic failure video understanding, capable of comprehensive task understanding, failure analysis, and failure correction.We also integrate it into a real-world robotic control pipeline as an external critic, enabling real-time correction for VLA-based systems. Extensive experiments demonstrate that our RoboFAC model achieves state-of-the-art results on our robotic failure evaluation benchmark and significantly improves VLA's failure recovery performance in real-world settings.</p>
<p>Table 1: Comparison with existing manipulation failure question answering datasets, including the number of different failure taxonomies covered (Failure Taxonomies), the presence of videos in the dataset (Videos), the presence of high-level correction questions (High-level correction), the presence of low-level correction questions (Low-level correction), the inclusion of long-horizon tasks (Long-horizon Tasks), the inclusion of dynamic tasks (Dynamic Tasks), and the coverage of multi-dimensional analysis of tasks and failures.</p>
<p>Multi-dimensional analysis</p>
<p>RoboFail [12] 8 AHA dataset [15] 7 RACER dataset [16] 2</p>
<p>RoboFAC dataset (Ours) 6</p>
<p>2 Related Work</p>
<p>Robot Manipulation with VLA</p>
<p>Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI, connecting multimodal perception with robotic action generation [1-3, 9, 17, 18].By representing robot actions as text tokens, RT-2 [1] unifies the modalities of vision, language, and action, enabling the model to leverage pre-trained vision-language models for robotic control. 0 [3] further advances this direction by using flow-matching diffusion to decode hidden representations into continuous actions.Other models, such as GR-2 [17], adopt a two-stage training paradigm: pre-training on largescale internet videos to learn general world dynamics, followed by fine-tuning on robot trajectories for action prediction and video generation.This approach enables GR-2 to generalize effectively across diverse manipulation tasks and environments.Despite these advances, existing VLAs often exhibit limitations in multi-step tasks requiring temporal reasoning.For example, long-horizon instructions may be misinterpreted due to temporal delays, leading to incorrect grasps or skipped subgoals.In dynamic environments, action trajectories may deviate from intended targets due to accumulated prediction errors.To address these limitations, we train an auxiliary model to assist VLAs by detecting, analyzing, and correcting failures in real time, thereby enhancing their robustness in complex manipulation tasks.</p>
<p>Robot Failure Detection and Analysis</p>
<p>While Vision-Language-Action (VLA) models have shown remarkable progress in end-to-end robotic control, they often struggle to detect and recover from failures autonomously in unstructured environments.To mitigate these shortcomings, recent work has explored the use of Multimodal Large Language Models (MLLMs) as auxiliary agents for error detection and reasoning.MLLMs excel at understanding visual content and producing structured explanations, making them well-suited for post-hoc or real-time failure analysis in manipulation tasks [4,11,15,16,[19][20][21][22].However, many general-purpose MLLMs [23,24] are not specifically fine-tuned on robot manipulation data and thus often struggle to accurately analyze operational errors in robotic systems.To address this limitation, Luo et al. [20] adopt Chain-of-Thought (CoT) prompting strategies to guide the reasoning process within powerful vision-language models, incorporating iterative model calls to ensure consistency in failure diagnosis.Shi et al. [21] introduce human-in-the-loop feedback mechanisms that collect corrective data during robot execution and use it for model fine-tuning.Dai et al. [16] and Duan et al. [15] construct image-text datasets centered on failure cases in manipulation, enabling supervised training of MLLMs for error detection.In contrast, we propose a video-based dataset for robotic failure analysis and correction, encompassing tasks from short to long horizons.Building on our dataset, we fine-tune a dedicated MLLM that achieves accurate and fine-grained failure understanding and recovery.This enables more robust and transparent deployment of vision-language models in diverse and challenging robotic manipulation scenarios.</p>
<p>The RoboFAC Dataset</p>
<p>In this section, we introduce the RoboFAC dataset, which is a large-scale and diverse dataset for question-answering on robot failure videos.We begin with an overview of the RoboFAC dataset, followed by a detailed definition of the failure taxonomies included in the dataset.Finally, we present how we construct the RoboFAC dataset.</p>
<p>Overview of RoboFAC dataset</p>
<p>The RoboFAC dataset encompasses robotic tasks of varying complexity, ranging from simple shorthorizon tasks to complex long-horizon tasks, and tasks executed in dynamic environments.It includes 14 simulated tasks and 6 real-world tasks, with two of the real-world tasks not present in the simulation environment.The dataset includes six types of failures, spanning three hierarchical levels of error (see Section 3.2 for details).</p>
<p>To account for the diversity of deployment settings in real-world robotics, we introduce variations in backgrounds and camera viewpoints.This design brings significant visual diversity to the dataset, which facilitates the development of models with better visual generalization capabilities and enables robust evaluation of such capabilities.</p>
<p>The RoboFAC dataset includes a total of 8,960 failure trajectories in the simulated environment and 480 failure trajectories in the real world.To prevent models from overfitting to failure patterns, we also collect 1,160 successful trajectories from simulation and 122 successful trajectories from real-world executions.After annotation, we finally obtained 78K video QA samples.</p>
<p>Taxonomy of Failures</p>
<p>We propose a three-level taxonomy of failures in robotic manipulation, inspired by prior analyses [12,15] and aligned with a hierarchical task structure (Figure 1</p>
<p>Q&amp;A Annotations</p>
<p>Not placed in an upright orientation</p>
<p>Grasp before the blue cube arrives</p>
<p>Step omission Wrong object Assume a task T is composed of substages {S i } N i=1 , where each substage involves the execution time t, the end-effector's position p  R 3 , orientation denoted by a unit quaternion q, gripper closure level G  [0, 1], and the manipulated object b  B, where B = {b 1 , ..., b M } is the set of all the objects in the environment.Ideally, the actual execution parameters (p i , qi , Gi , bi , ti ) at substage S i should match the correct parameters (p i , q i , G i , b i , t i ), ensuring successful task completion.However, errors occur when any of these parameters deviate from their nominal values, causing the task to fail.We define the failure taxonomy as follows:</p>
<p>Task Planning Error</p>
<p>a. Task Planning Error Errors rooted in incorrect task decomposition or failed language grounding in VLA models.</p>
<p>Step Omission: A required substage S i is skipped, resulting in an incomplete plan:
(S 1 , ..., S k1 , S k+1 , ..., S N ).
Wrong Object: Fail to select the correct object to manipulate as specified by the language instruction: bi  B \ b i .</p>
<p>b. Motion Planning Error Failures arising from limited spatial reasoning or inaccurate mapping from instructions to poses.This causes the current subtask to fail.</p>
<p>Position Deviation: The end-effector fails to reach the correct position.pi = p i + p i , with p i  R 3 .</p>
<p>Orientation Deviation: The end-effector fails to reach the correct orientation.qi = q i  q i , where q i is a unit quaternion and  represents quaternion multiplication.</p>
<p>c. Execution Control Error Execution control failures caused by physical imprecision, latency, or dynamic misalignment during actuation and environment interaction.</p>
<p>Grasping Error: The gripper does not close properly or the closure level is insufficient: Gi &lt; G i .This results in failure to grasp the target object or causes the object to slip from the gripper.</p>
<p>Timing Error: Executing the subtask at an incorrect timing.ti = t i  t, where  t introduces temporal offsets.</p>
<p>Data Construction Pipeline</p>
<p>Data Collection</p>
<p>Simulation Data.Our dataset construction pipeline in the simulation environment is illustrated in the top of Figure 3.We collect the simuluation data for 14 robotic tasks in the ManiSkill environment [25], augmented with objects from the YCB Object Dataset [26] to increase object diversity and scenes from ReplicaCAD [27] and AI2-THOR [28] to enrich environmental diversity.For each custom task, we first define an expert policy by specifying target end-effector poses for each substage, and the feasible paths and trajectories for the robotic arm to reach these poses are generated using motion planning.To generate failure data, we replace the original expert policy with a code snippet that generates an erroneous trajectory at the selected substage, causing the overall robotic task to fail.</p>
<p>During data collection, we record each robotic failure video along with a corresponding descriptive text.The description includes the substage where the failure occurred, the taxonomy of failure, and a detailed textual explanation of the error.For failures caused by perturbations in the end-effector pose, we also record the perturbed pose.These descriptions are utilized during the subsequent data annotation process.</p>
<p>Given the motion planning for the robotic arm occasionally failed, resulting in trajectories that did not align with the corresponding textual descriptions, we manually performed thorough data cleaning and retained approximately 75% of the collected data.</p>
<p>Real-World Data.We collected real-world data for 6 tasks, including two tasks that are not present in the simulation dataset.Data collection is performed via teleoperation using the SO-100 robotic arm.As with the simulation data, each video is accompanied by a corresponding textual description.</p>
<p>Data Annotation</p>
<p>We annotate the raw data to construct video-based QA samples corresponding to eight question types, which are described in detail in Section 4. These eight question types comprehensively evaluate a model's ability in Task Understanding, Failure Analysis, and Failure Correction based on robot manipulation videos.For each question type, we provide five question templates.The detailed question templates are given in Appendix B.</p>
<p>For each sample, the reference answer is generated based on the textual description associated with the video.For five question types-task identification, task planning, failure detection, fail-ure identification, and failure locating-the reference answers can be directly extracted from the corresponding textual description, as they have well-defined ground truths.For the remaining three types-failure explanation, high-level correction, and low-level correction-we utilize both the video and its corresponding textual description as inputs to GPT-4o to generate the reference answers.The exact prompt used for GPT-4o is provided in Appendix C. To ensure annotation quality, all outputs from GPT-4o were manually reviewed and corrected.</p>
<p>RoboFAC Model</p>
<p>RoboFAC Data Collection &amp; Annotation</p>
<p>Language query</p>
<p>Perturbed</p>
<p>GR00T N1</p>
<p>Fail to reach the blue cube Instruction: Put the blue cube in the box</p>
<p>RoboFAC</p>
<p>Move the robot arm slightly backward and then adjust it to the left to align with the center of the blue cube.After achieving alignment, lower the endeffector to grasp the cube securely before lifting it and moving it towards the box.</p>
<p>Successfully grasp the cube</p>
<p>Replace the original motion planning code</p>
<p>Human Check This section introduces our RoboFAC model, which demonstrates strong capabilities in Task Understanding, Failure Analysis, and Failure Correction.As illustrated in the bottom-left corner of Figure 3, given a robot manipulation video, the model is able to comprehensively interpret the video in natural language in a video-question-answering (VideoQA) manner.</p>
<p>Task Understanding.This capability is to understand the robotic task through the video, encompassing both task identificaiton and task planning.Specifically, given a robot manipulation video V, the model identifies what the robot is doing through the video as task T , and decomposes the task into a sequence of substages (S 1 , S 2 , . . ., S N ) by analyzing how the robot performs the task in the video.</p>
<p>Failure Analysis.Our model is able to conduct comprehensive analyses of failures in robot manipulation videos, including:</p>
<p> Failure detection: Determine whether the robotic task in the video was successfully completed. Failure identification: If the robotic task fails, determine what is the type of the failure.</p>
<p> Failure locating: If the robotic task fails, determine in which step the error happens. Failure explanation: If the robotic task fails, provide detailed explanation for the failure happened in the video.</p>
<p>Failure Correction.Our RoboFAC model is capable of providing detailed correction suggestions for errors occurring in the video, thereby helping the VLA model recover from failures.These suggestions include both high-level corrections and low-level corrections.High-level correction offers explicit guidance by specifying the sequence of sub-tasks the model should execute to recover from the failure.This property of high-level correction makes it particularly valuable when failures stem from errors in the robot's task planning, such as missing sub-tasks or incorrect sub-task order.Low-level correction gives fine-grained control guidance, specifically suggestions on the end-effector's movement direction, helping the robotic arm accurately reach the correct position.Low-level correction is more suited for addressing errors in the robot's low-level execution, such as failing to reach the correct position or following an unsuitable trajectory.The failure correction capability of our RoboFAC model effectively assists the VLA model in recovering from failure situations.We conduct extensive validation of this functionality in real-world scenarios.Detailed settings and results are provided in Section 5.3.</p>
<p>Model Architecture.We build our model based on Qwen2.5-VL[29], one of the most advanced open-source multi-modal models to date, consisting of an LLM backbone, a vision encoder, and an MLP-based vision-language merger.Qwen2.5-VLmodel supports single-image, multi-image, and video inputs at varying resolutions, achieving strong performance in visual question answering tasks.</p>
<p>Our further training details are provided in Section 5.1.</p>
<p>Experiments</p>
<p>In this section, we comprehensively evaluate our model's capacity.We compare our model against both proprietary and open-source models on our benchmark across multiple performance dimensions.Additionally, we deploy our model as a critic to supervise a real-world robotic arm during task execution, assessing whether it can effectively guide the VLA model and thus enhance the success rate of robotic tasks in real-world scenarios.</p>
<p>Experimental Setup</p>
<p>Training Set &amp; Evaluation Benchmark.We construct the training and testing datasets from our collected RoboFAC data.Specifically, we randomly sample 60K QA pairs from the simulated RoboFAC dataset as the training set.The remaining QA pairs are used for evaluation, including 10K simulated QA pairs and 8K QA pairs from real-world data.Notably, the simulated split of the test set contains over 1,000 robotic videos that are entirely unseen during training.Furthermore, our model is never trained on the real-world data, and the real-world split of the test set also includes two tasks that the model has never encountered before(InsertCylinder and PlaceCube).This setup allows us to rigorously assess the model's sim-to-real transfer capability and its generalization performance.</p>
<p>Training Details.We fine-tune both Qwen2.5-VL-3B and Qwen2.5-VL-7B on the RoboFAC training set for one epoch, with both the LLM backbone and merger parameters unfrozen with a learning rate of 1  10 5 .We use the DeepSpeed ZeRO-3 offload strategy [30] to optimize memory usage.Each GPU processed a batch size of 1.For the model with 3B parameters, we use a gradient accumulation step of 2, while for the model with 7B parameters, the gradient accumulation step is set to 4. We fine-tune the model on 4 Nvidia GeForce RTX 4090 GPUs.It takes approximately 10 hours to train the 3B model and 24 hours to train the 7B model.</p>
<p>Evaluation Metrics.To accommodate the nature of different question types, we adopt two evaluation metrics accordingly.For failure detection, failure identification, and failure locating, where answers tend to be relatively deterministic, we employ a multiple-choice format and compute the accuracy as the percentage of correctly answered samples.For the remaining tasks, where responses are semantically richer, we rely on an external LLM to assess answers along three dimensions: correctness, relevance, and completeness.Detailed descriptions of these three evaluation dimensions along with the prompt provided to the LLM can be found in Appendix D. The final score is computed as the average of the three dimensional scores.All scores are normalized to a 100-point scale.</p>
<p>Main Results on RoboFAC Benchmark</p>
<p>We comprehensively evaluate our proposed RoboFAC models (RoboFAC-3B and RoboFAC-7B) against several strong multimodal baselines, including open-source models Qwen2.5-VL-3B and Qwen2.5-VL-7B, and proprietary models Gemini-2.0and GPT-4o.The evaluation spans diverse manipulation tasks and cognitive abilities essential for robotic reasoning, with metrics defined in Section 5.1.The results are summarized in Figure 4 and Table 2.</p>
<p>Overall Performance.As shown in Table 2, RoboFAC-7B consistently outperforms all baseline models across all task categories, including short-, medium-, and long-horizon tasks, as well as dynamic and real-world tasks.It achieves an average score of 79.10 significantly surpassing GPT-4o Multi-Dimensional Capacity.Figure 4 further breaks down the performance across eight key capacities critical to robotic failure comprehension: task understanding (task identification, task planning, failure correction (high/low level), and failure analysis (detection, identification, locating, explanation).Our RoboFAC model demonstrates a strong ability to handle robotic failures, achieving the highest or near-highest scores in task planning, low-level correction, and all three failure-related abilities.This indicates that our models are capable of nuanced task decomposition and resilient recovery from execution failures, both of which are essential for real-world deployment.</p>
<p>In contrast, large-scale generalist models such as GPT-4o and Gemini-2.0,while competitive in certain aspects (e.g., failure detection), exhibit limited performance in task planning and hierarchical correction.This suggests a gap in their ability to perform complex, multi-step reasoning under physical constraints, which our models are specifically trained to address.Generalization Across Task Variants.We further assess model generalization across different robotic tasks (InsertCylinder, PlaceCube, PullCubeTool, etc.) in the bottom left of Figure 4. RoboFAC-7B outperforms all baselines across all task variants, maintaining robustness across varying levels of physical interaction complexity.This consistent high performance demonstrates the robustness and scalability of our approach.</p>
<p>Performance on Real-world Manipulation</p>
<p>Real-world Evaluation Setup.To assess the practical effectiveness of RoboFAC-generated correction instructions in real-world robotic manipulation tasks, we built a physical evaluation system based on the SO-100 robotic arm.Using the lerobot [31] framework, we collected over 300 teleoperated demonstrations for each task from three synchronized viewpoints (wrist-mounted, top-down, and front-left cameras) along with control signals.These data were used to fine-tune the VLA model GR00T-N1 [6], enabling improved task execution and spatial reasoning specific to the target manipulation scenarios.</p>
<p>Pipeline of Real-world Evaluation.The robot receives an initial task prompt and begins execution with the fine-tuned GR00T-N1.At a predefined timestamp, the execution is paused and a third-view video segment is extracted up to that point.Based on this video, the correction model generates a natural language instruction.This instruction is then appended to the original prompt to form a revised task prompt.Then the robot resumes execution with this revised prompt.This process of pausing, generating a correction, and resuming execution is repeated up to four times per trial, and the success rate both after the first correction and all four correction rounds was recorded.</p>
<p>Real-World Results.We compare success rates across five conditions with first and next 4 attempts (5 attempts in total) on 20 demonstrations: (1) No Correction, (2) GPT-4o, (3) Qwen2.5-VL-7B,(4) RoboFAC-7B Low-Level, and (5) RoboFAC-7B High-Level, and results are shown in Table 3. RoboFAC-7B (Low-level) consistently achieves the highest average success rate (61.25% after 4 attempts), outperforming GPT-4o (56.25%) and significantly exceeding the No Correction (47.5%) and Qwen2.5-VL-7B(50.0%) baselines.Even after a single round of correction, RoboFAC shows strong improvement over other methods.Moreover, low-level corrections offering step-by-step instructions outperform High-level corrections.Despite these improvements, the success rate remains subject to further enhancement, largely due to the VLA model's limited ability to follow complex natural language instructions.</p>
<p>Discussion</p>
<p>Conclusion.In this paper, we introduce the RoboFAC dataset, a large-scale and diverse robotic failure dataset that labels multi-dimensional information.We also present the RoboFAC model, a multimodal large model specifically developed for robotic failure analysis and correction, which is capable of Task Understanding, Failure Analysis, and Failure Correction based on robot video.</p>
<p>Extensive experiments demonstrate that the RoboFAC model achieves state-of-the-art performance on our evaluation benchmark, and the model can effectively improve the success rate when integrated as an external critic in real-world VLA control tasks.</p>
<p>Limitations and Future Work.Although we have demonstrated that the RoboFAC model's correction suggestions can effectively assist the VLA model in recovering from failures, our current integration of the model into the VLA-controlled robotic system is not yet seamless.In future work, we aim to explore more natural and automated mechanisms for delivering correction suggestions.Such improvements could enable the development of a fully automated system for collecting robotic failure recovery data.Moreover, apply our model exclusively to the VLA model in this work.However, for hierarchical policies, more targeted correction strategies could be designed: high-level correction and low-level correction may be applied directly to the high-level planner and low-level controller, respectively.This is also a promising direction for future research.</p>
<ol>
<li>In the video, the robotic arm executes a task.Please break down its execution into a sequence of substages.4. Watch the video of the robotic arm performing a task, please outline the process as a substages sequence.5. Based on the video showing a robotic arm carrying out a task, please generate a sequence of substages for its execution.</li>
</ol>
<p>Failure detection</p>
<p>1.This is a video of a robotic arm performing a task, was the task successfully completed?2. Based on the video of the robotic arm executing a task, did it finish the task successfully?3.In the video, the robotic arm executes a task, can you determine whether it was successful? 4. Please assess if the robotic arm has successfully accomplished the task. 5.In the video, the robotic arm executes a task, was it successful?Failure identification 1.This is a video of a robotic arm performing a task, please identify the type of error that occurred during execution.</p>
<ol>
<li>Based on the video of the robotic arm carrying out a task, what type of error took place during the task? 3. The robotic arm failed to complete the task, can you specify the type of error that happened?4. Please describe the error type that occurred during the robotic arm's execution of the task. 5. From the video of the robotic arm performing a task, what kind of error can be observed during the task?</li>
</ol>
<p>Failure locating 1.This is a video of a robotic arm performing a task, please identify the subtask stage where the error occurred.</p>
<ol>
<li>This is a video of a robotic arm performing a task, during which subtask did the error happen?3.The robotic arm failed to complete the task, can you locate the specific subtask in which the error occurred?4. Please determine at what subtask stage the error took place in the robotic arm's performance of the task. 5. From the video of the robotic arm carrying out a task, identify the phase of the task where the error happened.</li>
</ol>
<p>Failure explanation</p>
<p>1.This is a video of a robotic arm performing a task, please explain in detail the reason for the task failure.2. Based on the video, provide a detailed explanation of why the robotic arm failed to complete the task.High-level correction 1.This is a video of a robotic arm performing a task, an error occurred during execution.</p>
<p>Please provide high-level corrective instructions to help the robot recover and complete the task successfully.</p>
<ol>
<li>
<p>Based on the video showing an error during the robotic arm 's execution of a task, give detailed high-level guidance for correcting the error and enabling task completion.</p>
</li>
<li>
<p>In this video, an error happened while the robotic arm was performing the task, please suggest high-level recovery steps so the robot can continue and complete the task.4. The robotic arm failed to complete the task, please analyze the error in the robotic arm's task from the video and propose high-level correction actions that would allow successful task completion. 5. From the video of the robotic arm failing during the task, provide high-level corrective commands to guide it to recover and finish the task.</p>
</li>
</ol>
<p>Low-level correction 1.This is a video of a robotic arm performing a task, an error occurred during execution.</p>
<p>Please provide low-level corrective commands to help the robot recover and complete the task successfully.</p>
<ol>
<li>
<p>Based on the video, an error happened while the robot was executing a task, give detailed low-level instructions to correct the issue and allow the task to be finished.</p>
</li>
<li>
<p>According to the video of the robotic arm executing a task, please suggest specific low-level recovery actions to enable successful task completion.4. From the video showing an error in the robotic arm's task, provide precise low-level commands for error correction and recovery.5.In the video, an error occurred during the robot's performance of the task, please give low-level control instructions to help it recover and complete the task.</p>
</li>
</ol>
<p>C Data Annotation Details</p>
<p>For the failure explanation, high-level correction, and low-level correction questions, we employed GPT-4o to annotate the data.Specifically, we constructed prompts using the description files obtained during video collection.We use the prompt paired with the corresponding videos to request GPT-4o.The constructed prompt is as follows:</p>
<p>Prompt for data annotation This is a video of a robot arm performing a task, and the task is failed.</p>
<p>Here is the basic information of the video: -Task: {task} -Subtask: {subtask} -Error type: {error type} -Error stage: {error stage} -Error detail: {error detail} -Correction suggestion: {error correction} -Perturbation ([x, y, z]): {error low level} The perturbation is the difference between the actual position of the end-effector and the desired target position when the error occurs, where the X-axis points in front of the manipulator, the Y-axis points to the left, and the Z-axis points up.Namely, if the Xaxis is positive, the end-effector is in front of the desired target position and causes the task to fail.</p>
<p>According to the video and the information, you need to answer the following questions: 1. Explain why the task is failed in detail.Please note that specific numerical values should not be given to describe the extent of the low-level correction.An example of the low-level correction is: "Move the robot arm backward then move the robot arm to the left to align with the target object".Please note that specific numerical values should not be given in the explanation of the failure reason and the high-level correction, you should instead using rich language to describe the failure reason and the high-level correction.</p>
<p>Your answer should be in the following JSON format: { "reason": <reason>, "high level correction": <high level correction>, "low level correction": <low level correction> }</p>
<p>D Evaluation Details</p>
<p>Construct Multiple-Choice Question Options.For the evaluation of three distinct question types-failure Detection, failure Identification, and failure locating, we adopt a multiple-choice question format.The construction of answer options for each task is as follows:</p>
<p> Failure detection: The model selects from a binary choice set: <Yes/No>. Failure locating: Four sub-stagess are randomly sampled from all the sub-stages in the RoboFAC dataset and combined with the correct sub-stage corresponding to the current sample.These five options are then shuffled to form the final choice set.</p>
<p>Evaluate by LLM.For the remaining five question types-task identification, task planning, failure explanation, high-level correction, and low-level correction-we evaluate model responses using GPT-4 as a scoring agent.The evaluation is conducted across three dimensions, each rated on a 1-5 scale:</p>
<p> Correctness: Factual accuracy and consistency with the reference answer.</p>
<p> Relevance: The degree to which the model's response addresses the given question.</p>
<p> Completeness: Whether the response sufficiently covers all key aspects of the reference answer.</p>
<p>To ensure fairness and consistency in the scoring results, we configure GPT-4 with a temperature of 0.2 and a Top-P value of 1.0.We prompt GPT-4 with the question, the reference answer, and the response generated by the testing model, asking it to assign scores based on the criteria above.The exact prompt used is as follows:</p>
<p>Prompt for LLM scoring</p>
<p>You are an expert evaluator.Assess the quality of a model's response to the user's query.</p>
<p>Question: {question}</p>
<p>Reference answer: {ref} Model's response: {pred} Evaluate the model's response on the following criteria: -correctness: factual accuracy and consistency with the reference answer.</p>
<p>-relevance: how well the model's response addresses the question.</p>
<p>-completeness: whether all key aspects of the reference answer are covered.</p>
<p>For each criterion, provide a score from 0 to 5 and a <strong>brief</strong> explanation, the score should be an integer.The score you give needs to be strict and demanding.</p>
<p>Output ONLY the JSON object in the following format: { Figure 6 further illustrates the multi-dimensional diagnostic capability of RoboFAC-7B.In addition to failure explanation, the model is evaluated on failure detection, locating the specific step where the failure occurred, and identifying the type of error.In all cases, RoboFAC-7B provides correct answers, while GPT-4o fails to correctly diagnose the failures, highlighting the robustness of our model in understanding and analyzing real-world robotic errors.</p>
<p>G Demos of Failure Correction in Real-world tasks Fail to grasp the blue cube.</p>
<p>RoboFAC-7B</p>
<p><Low-level Correction > Move the robot arm slightly backward to create space, then adjust its position to the left and downward to align with the center of the cube before attempting to grasp it again.</p>
<p>Successfully grasp the blue cube and put it in the box.</p>
<p>Instruction: Put the blue cube in the box</p>
<p>GR00T N1</p>
<p>Fail to grasp the red cube.</p>
<p>RoboFAC-7B</p>
<p><Low-level Correction > Move the robot arm slightly backward to create space, then adjust the arm to the left to align with the center of the red cube.After achieving alignment, lower the end-effector to grasp the red cube securely before lifting it and moving it towards the green cube for stacking.</p>
<p>Successfully grasp the red cube and stack it on the green cube.</p>
<p>Instruction: Put the blue cube in the box GR00T N1</p>
<p>3 .
3
Right): Task Planning, Motion Planning, The task is NOT succeeded 4. The failure type is <failure type> 5.The error happens in the <substage-k> 6.The error happens because <reason> Failure correction 7. High-level: The robot should first do <subtask-a>, then do <subtask-b> 8. Low-level: The robot should move towards <direction> to align with ...</p>
<p>Figure 1 :
1
Figure 1: Overview of RoboFAC dataset.Left: The RoboFAC dataset features both task diversity and visual diversity, encompassing tasks of varying complexity, real-world tasks, and various of backgrounds and camera viewpoints.We provide detailed video question-answer annotations for eight distinct question types.Right: A detailed visual illustration of the six failure taxonomies.</p>
<p>Figure 2 :
2
Figure 2: Statistics of the RoboFAC Dataset.Left: Categories of robotic tasks in the RoboFAC dataset.(Lh.Task: Long-horizon task, Mh.Task: Medium-horizon task, Sh.Task: Short-horizon Task, Dy.Task: Dynamic Task) Top Right: Distribution of video counts by duration interval.Bottom Right: Average duration of each task.</p>
<p>1 .
1
The robotic arm is performing <task> 2. Planning: <substage-1>, <substage-2>, ... Task understanding Failure analysis 3. The task is NOT succeeded 4. The failure type is <failure type> 5.The error happens in the <substage-k> 6.The error happens because <reason> Failure correction 7. High-level: The robot should first do <subtask-a>, then do <subtask-b> 8. Low-level: The robot should move towards <direction> to align with ...</p>
<p>Figure 3 :
3
Figure 3: Overview of our RoboFAC framework.Top: The pipeline of constructing the RoboFAC dataset.Bottom-left: We build our RoboFAC model by fine-tuning Qwen2.5-VLmodel.The RoboFAC model can perform Task Understanding, Failure analysis and Failure correction.Bottomright: We deploy RoboFAC model on real-world VLA control tasks, and it effectively helps the VLA recover from failure.</p>
<p>Figure 4 :
4
Figure 4: Scores for different dimensions on RoboFAC Benchmark Left: Performance on different question dimensions for simulation dataset.Top Right: Performance on different question dimensions for real world dataset.Bottom Right: Performance on different real world tasks.</p>
<p>3 .
3
The robotic arm failed to complete the task, can you describe in detail the cause of the failure in the video?4. Please analyze the video and explain thoroughly what led to the failure of the task. 5. From the video of the robotic arm executing a task, give a detailed explanation of the reason behind the task failure.</p>
<p>2 .
2
Give detailed High-level correction instructions to help the robot arm to recover from the failure.The high-level correction should describe what subtask the robot arm should perform to recover from the failure.3. Give detailed Low-level correction instructions to help the robot arm to recover from the failure.The low-level correction should describe which direction and how much the robot arm should move to recover from the failure.</p>
<p>Figure 7
7
Figure 7 presents two real-world examples demonstrating the effectiveness of RoboFAC-7B in correcting manipulation failures.In both cases, the robot (GR00T N1) initially fails to grasp the target object due to inaccurate alignment.Based on the instruction and visual observations, RoboFAC-7B generates low-level corrective feedback, which guides the robot to adjust its pose and retry the action.The corrected executions successfully complete the task objectives: placing a blue cube into a box (left) and stacking a red cube onto a green one (right).</p>
<p>Figure 7 :
7
Figure 7: Demo of failure correction in real-world tasks.</p>
<p>Table 2 :
2
Performance of Various Multimodal Models on the RoboFAC Benchmark.The benchmark evaluates model capabilities across five task categories: Short-horizon, Medium-horizon, Longhorizon, Dynamic, and Real-world manipulation tasks.The scores represent success rates (%) on each category, and the final column reports the average performance across all tasks.Our proposed RoboFAC models (3B and 7B) consistently outperform both open-source (Qwen2.5-VL,Gemini-2.0)and closed-source (GPT-4o) baselines across all categories.
ModelShort-horizon TaskMedium-horizon TaskLong-horizon TaskDynamic TaskReal-world TaskAverageQwen-2.5-VL-3B40.9927.8225.1828.9417.3627.82Qwen-2.5-VL-7B14.2611.7338.8418.0050.9627.47Gemini-2.063.3253.2345.6748.9141.7251.11GPT-4o61.5053.8142.4645.8265.8957.42RoboFAC-3B81.6684.6779.3283.0263.2976.80RoboFAC-7B82.7484.9281.7883.2868.9479.10(57.42) and Gemini-2.0 (51.11). Notably, even the smaller RoboFAC-3B model achieves an averagescore of 76.80, highlighting the effectiveness of our domain-specific training and architectural design.</p>
<p>Table 3 :
3
Success rate on real-world manipulation.
MethodsPlaceCube PushCube PullCubeTool StackCube AverageNo correction1 attempt 5 attempts0.20 0.400.55 0.700.10 0.200.35 0.600.3000 0.4750GPT-4o1 attempt 5 attempts0.25 0.500.70 0.800.15 0.300.50 0.650.4000 0.5625Qwen2.5-VL-7B1 attempt 5 attempts0.35 0.500.60 0.700.15 0.200.45 0.600.3875 0.5000RoboFAC-7B (Low)1 attempt 5 attempts0.40 0.600.70 0.850.20 0.300.50 0.700.4500 0.6125RoboFAC-7B (High)1 attempt 5 attempts0.45 0.500.65 0.750.10 0.200.45 0.550.4125 0.5000</p>
<p> Failure identification: The model chooses from a predefined set of six failure types: ['Orientation deviation.','Step omission.','Wrong target object.','Timing error.','Grasping error.','Position deviation.'].</p>
<p>AppendixA Task DescriptionFor each task, we systematically vary the object categories and modify the scene of the environment to promote task generalization.A brief description of the original tasks we defined is shown below.Table4: A brief description of the task we defined.The table is divided into four sections according to the type of task, from top to bottom, Dynamic Tasks, Long-horizon Tasks, Medium-horizon Tasks, and Short-horizon Tasks.Task DescriptionSpinStack Pick up the cube on the spinning disc and stack it on another cube on the disc.SpinPullStackPull out the cube on the spinning disc and stack it on another cube on the disc.MicrowaveTaskPut the spoon on the table into the cup.Open the door of microwave, put the cup into the microwave and close the door.SafeTaskPut the gold bar into the safe, close the door of the safe and rotate the cross knob on the door to lock it.ToolsTaskChoose the correct (L-shaped) tools, grasp it to pull the correct (2-pins) charger and plug it.UprightStaskUpright the peg and stack it on the cube.PegInsetionSideInsert the peg into the hole on the side of the block.PullCubeToolGrasp the L-shaped tool and pull the cube by it.PlugChargerGrasp the charger and plug it into the receptacle.InsertCylinderUpright the cylinder and insert it into the middle hole on the shelf.PlaceCubePick up the cube and place it into the box.LiftPegUprightLift the peg and upright it.PickCubePick the cube to the target position.PullCubePull the cube to the red and white target.PushCubePush the cube to the red and white target.StackCubePick up the cube and stack it on another cube.B Question TemplateFor each of the eight question types, we design a set of question templates.To enhance the diversity of our questions, we provide five distinct phrasings for each type.During the construction of a specific QA pair, one template is randomly sampled from the corresponding set.The complete list of templates is as follows:Question TemplateTask identification 1. Please describe the task the robot is performing in the video.Task planning 1.This is a video of a robotic arm performing a task, please break down its execution into a sequence of substages.2. Given the video of a robotic arm doing a task, please plan its actions as a sequence of substages."criteria": { "correctness": {"score": &lt;0-5&gt;, "explanation": <brief explanation>}, "relevance": {"score": &lt;0-5&gt;, "explanation": <brief explanation>}, "completeness": {"score": &lt;0-5&gt;, "explanation": <brief explanation>}, } }E Supplementary Evaluation ResultsWe evaluate six models: Qwen-2.5-VL-3B,Qwen-2.5-VL-7B,two proprietary systems (Gemini-2.0,GPT-4o), and our proposed RoboFAC-3B and RoboFAC-7B.This section details their results on the RoboFAC benchmark.Table5summarizes task-level accuracy on the simulation dataset, while Table6breaks down performance by question type.<Failure Explanation>The task failed because the robot arm was unable to properly align the dice over the brick during the stacking phase.This misalignment resulted in the end-effector being positioned incorrectly, leading to a position deviation that prevented the successful stacking of the dice on the brick.The robot's inability to accurately adjust its position indicates a lack of precision in the final movement, which is crucial for the success of the task.GPT-4o <Failure Explanation>The robotic arm attempted to grasp the target object but failed to secure it properly.As a result, the object slipped from the gripper during the task execution.This indicates an issue with the grasping mechanism or insufficient grip force applied to the object.Did not align above the brick.Did not rotate the can to an upright position.<Failure Explanation>The task failed because the robot arm did not achieve the correct orientation when attempting to rotate the can to an upright position.As a result, the can was placed back on the table in a tipped-over position, causing it to tip over after placement and failing to remain upright.This indicates a failure in the final subtask of rotating the can, which is crucial for ensuring that the can is able to stand stably on the table.GPT-4o <Failure Explanation>The robotic arm attempted to pick up the can but failed to secure it properly.While the arm made contact with the can and lifted it slightly, the can slipped out of its grip and fell back to the ground.This indicates that the gripping mechanism was either not strong enough or improperly aligned to hold the can securely.RoboFAC-7BTask: Pick up the dice and stack it on the brick.Task: Lift the can and upright it.<Failure Explanation>The task failed because the robot arm did not grasp the long block tightly enough during the grasping stage.As a result, when the robot attempted to lift the block, it slipped from the gripper's hold and could not be stacked upright on the red bowl.This indicates a failure in the grasping force applied by the robot arm, which was insufficient for securely holding the object.GPT-4o <Failure Explanation>The robotic arm failed to complete the task because it did not properly grasp the red cup.The gripper appears to close either too early or at an incorrect angle, resulting in the cup being pushed or knocked over instead of being securely held.Additionally, the arm's positioning relative to the cup seems slightly misaligned, which further contributes to the failure to establish a stable grip.This misalignment and premature gripping action caused the task to fail as the cup was not successfully picked up.The gripper did not close tightly enough.Did not hook the target object.<Failure Explanation>The task failed because the robot arm mistakenly hooked the dice instead of the intended Rubik's cube.This error occurred during the stage where the robot was moving the L-shaped tool behind the strawberry.The presence of multiple objects in close proximity likely led to confusion, resulting in the wrong target object being engaged.As a result, the robot could not complete the task of pulling the strawberry back.GPT-4o <Failure Explanation>The robotic arm failed to complete the task due to improper alignment and grasping of the red L-shaped object.In the video, the arm initially approaches the object correctly but struggles to maintain a stable grip.The arm's movements appear to lack precision, causing the object to slip or be misaligned during manipulation.Additionally, the arm's end-effector does not seem to be optimized for securely holding the L-shaped object, leading to repeated failures in positioning it correctly relative to the other objects in the scene.RoboFAC-7B RoboFAC-7BTask: Upright the peg and stack it in the cup.Task: Grasp the L-shaped tool and pull the cube by it.<Failure Explanation>The task failed because the robot arm omitted the crucial step of placing the spoon into the mug before moving the mug to the microwave.This omission resulted in an incomplete action, as the spoon was not in the mug when it was placed in the microwave, leading to an incorrect execution of the intended task.GPT-4o <Failure Explanation>The robotic arm failed to securely grasp the red cup during the execution of the task.Although the gripper approached the cup and attempted to close around it, the grasp was not firm enough to lift the cup successfully.This resulted in the cup being left behind on the ground while the robotic arm proceeded with subsequent steps of the task.The failure likely occurred due to insufficient alignment between the gripper and the cup or inadequate gripping force applied by the robotic arm.Forget putting the spoon into the mug.Task: Put the spoon on the table into the mug.Open the door of microwave, put the mug into the microwave and close the door.Task: Pull the cube to the red and white target.Task: Push the box to the red and white target.<Failure Explanation>The task failed because the robot arm released the green cube too early while attempting to stack it onto the apple.The apple was not yet properly aligned beneath the green cube when the release occurred, resulting in the green cube missing its target and failing to stack on the apple.This timing error indicates that the robot arm did not wait for the apple to be fully rotated into position before releasing the green cube.GPT-4o <Failure Explanation>The task failed because the robot arm repeatedly attempted to grasp the red object but was unable to successfully pick it up.This could be due to improper alignment of the gripper with the object, insufficient gripping force, or a mismatch between the gripper's design and the object's shape or size.Additionally, the robot arm appeared to move the blue object out of the red circular area, which may indicate a lack of precision or misinterpretation of the task requirements.Release the green cube too early.RoboFAC-7BRoboFAC-7B<Failure Detection> Q: Based on the video of the robotic arm executing a task, did it finish the task successfully?A: Yes.GPT-4o<Failure Detection > Q: Based on the video of the robotic arm executing a task, did it finish the task successfully?A: No.Task: Pull out the green cube on the spinning disc and stack it on the apple on the disc.Task: Pick the cube to the target position.
Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Openvla: An open-source vision-language-action model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, Q Vuong, T Kollar, B Burchfiel, R Tedrake, D Sadigh, S Levine, P Liang, C Finn, 2024</p>
<p> 0 : A vision-language-action flow model for general robot control. K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, S Jakubczak, T Jones, L Ke, S Levine, A Li-Bell, M Mothukuri, S Nair, K Pertsch, L X Shi, J Tanner, Q Vuong, A Walling, H Wang, U Zhilinsky, 2024</p>
<p>Spatialbot: Precise spatial understanding with vision language models. W Cai, I Ponomarenko, J Yuan, X Li, W Yang, H Dong, B Zhao, arXiv:2406.136422024arXiv preprint</p>
<p>Rdt-1b: a diffusion foundation model for bimanual manipulation. S Liu, L Wu, B Li, H Tan, H Chen, Z Wang, K Xu, H Su, J Zhu, 2025</p>
<p>Gr00t n1: An open foundation model for generalist humanoid robots. : Nvidia, J Bjorck, F Castaeda, N Cherniadev, X Da, R Ding, L J Fan, Y Fang, D Fox, F Hu, S Huang, J Jang, Z Jiang, J Kautz, K Kundalia, L Lao, Z Li, Z Lin, K Lin, G Liu, E Llontop, L Magne, A Mandlekar, A Narayan, S Nasiriany, S Reed, Y L Tan, G Wang, Z Wang, J Wang, Q Wang, J Xiang, Y Xie, Y Xu, Z Xu, S Ye, Z Yu, A Zhang, H Zhang, Y Zhao, R Zheng, Y Zhu, 2025</p>
<p>Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers. L Wang, X Chen, J Zhao, K He, 2024</p>
<p>S Belkhale, T Ding, T Xiao, P Sermanet, Q Vuong, J Tompson, Y Chebotar, D Dwibedi, D Sadigh, arXiv:2403.01823Rt-h: Action hierarchies using language. 2024arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, J Ibarz, B Ichter, A Irpan, T Jackson, S Jesmonth, N J Joshi, R Julian, D Kalashnikov, Y Kuang, I Leal, K.-H Lee, S Levine, Y Lu, U Malla, D Manjunath, I Mordatch, O Nachum, C Parada, J Peralta, E Perez, K Pertsch, J Quiambao, K Rao, M Ryoo, G Salazar, P Sanketi, K Sayed, J Singh, S Sontakke, A Stone, C Tan, H Tran, V Vanhoucke, S Vega, Q Vuong, F Xia, T Xiao, P Xu, S Xu, T Yu, B Zitkovich, 2023</p>
<p>Sti-bench: Are mllms ready for precise spatial-temporal world understanding. Y Li, Y Zhang, T Lin, X Liu, W Cai, Z Liu, B Zhao, arXiv:2503.237652025arXiv preprint</p>
<p>A self-correcting vision-language-action model for fast and slow system manipulation. C Li, J Liu, G Wang, X Li, S Chen, L Heng, C Xiong, J Ge, R Zhang, K Zhou, S Zhang, 2025</p>
<p>Reflect: Summarizing robot experiences for failure explanation and correction. Z Liu, A Bahety, S Song, arXiv:2306.157242023arXiv preprint</p>
<p>Aic mllm: Autonomous interactive correction mllm for robust robotic manipulation. C Xiong, C Shen, X Li, K Zhou, J Liu, R Wang, H Dong, 2024</p>
<p>Automating robot failure recovery using vision-language models with optimized prompts. H Chen, Y Yao, R Liu, C Liu, J Ichnowski, 2024</p>
<p>Aha: A vision-language-model for detecting and reasoning over failures in robotic manipulation. J Duan, W Pumacay, N Kumar, Y R Wang, S Tian, W Yuan, R Krishna, D Fox, A Mandlekar, Y Guo, 2024</p>
<p>Racer: Rich language-guided failure recovery policies for imitation learning. Y Dai, J Lee, N Fazeli, J Chai, 2024</p>
<p>Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation. C.-L Cheang, G Chen, Y Jing, T Kong, H Li, Y Li, Y Liu, H Wu, J Xu, Y Yang, H Zhang, M Zhu, 2024</p>
<p>Image-based visual servoing for enhanced cooperation of dual-arm manipulation. Z Zhang, Y Yang, W Zuo, G Song, A Song, Y Shi, IEEE Robotics and Automation Letters. 2025</p>
<p>Oscnet: Machine learning on cmos oscillator networks. W Cai, T H Lee, arXiv:2502.071922025arXiv preprint</p>
<p>Roboreflect: A robotic reflective reasoning framework for grasping ambiguous-condition objects. Z Luo, Y Yang, Y Zhang, F Zheng, 2025</p>
<p>Yell at your robot: Improving on-the-fly from language corrections. L X Shi, Z Hu, T Z Zhao, A Sharma, K Pertsch, J Luo, S Levine, C Finn, 2024</p>
<p>Code-as-monitor: Constraint-aware visual programming for reactive and proactive robotic failure detection. E Zhou, Q Su, C Chi, Z Zhang, Z Wang, T Huang, L Sheng, H Wang, 2025</p>
<p>The dawn of lmms: Preliminary explorations with gpt-4v(ision). Z Yang, L Li, K Lin, J Wang, C.-C Lin, Z Liu, L Wang, 2023</p>
<p>Gemini: a family of highly capable multimodal models. G Team, R Anil, S Borgeaud, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, K Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai. S Tao, F Xiang, A Shukla, Y Qin, X Hinrichsen, X Yuan, C Bao, X Lin, Y Liu, T Chan, Y Gao, X Li, T Mu, N Xiao, A Gurha, Z Huang, R Calandra, R Chen, S Luo, H Su, 2024</p>
<p>Benchmarking in manipulation research: Using the yale-cmu-berkeley object and model set. B Calli, A Walsman, A Singh, S Srinivasa, P Abbeel, A M Dollar, 10.1109/MRA.2015.2448951IEEE Robotics &amp; Automation Magazine. Sep. 201522</p>
<p>Habitat 2.0: Training home assistants to rearrange their habitat. A Szot, A Clegg, E Undersander, E Wijmans, Y Zhao, J Turner, N Maestre, M Mukadam, D Chaplot, O Maksymets, A Gokaslan, V Vondrus, S Dharur, F Meier, W Galuba, A Chang, Z Kira, V Koltun, J Malik, M Savva, D Batra, 2022</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, M Deitke, K Ehsani, D Gordon, Y Zhu, A Kembhavi, A Gupta, A Farhadi, 2022</p>
<p>S Bai, K Chen, X Liu, J Wang, W Ge, S Song, K Dang, P Wang, S Wang, J Tang, H Zhong, Y Zhu, M Yang, Z Li, J Wan, P Wang, W Ding, Z Fu, Y Xu, J Ye, X Zhang, T Xie, Z Cheng, H Zhang, Z Yang, H Xu, J Lin, Qwen2.5-vl technical report. 2025</p>
<p>Zero: Memory optimizations toward training trillion parameter models. S Rajbhandari, J Rasley, O Ruwase, Y He, SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE2020</p>
<p>Lerobot: State-of-theart machine learning for real-world robotics in pytorch. R Cadene, S Alibert, A Soare, Q Gallouedec, A Zouitine, T Wolf, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
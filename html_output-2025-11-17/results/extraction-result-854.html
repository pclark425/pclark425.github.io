<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-854 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-854</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-854</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-269187633</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.11584v1.pdf" target="_blank">The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> This survey paper examines the recent advancements in AI agent implementations, with a focus on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities. The primary objectives of this work are to a) communicate the current capabilities and limitations of existing AI agent implementations, b) share insights gained from our observations of these systems in action, and c) suggest important considerations for future developments in AI agent design. We achieve this by providing overviews of single-agent and multi-agent architectures, identifying key patterns and divergences in design choices, and evaluating their overall impact on accomplishing a provided goal. Our contribution outlines key themes when selecting an agentic architecture, the impact of leadership on agent systems, agent communication styles, and key phases for planning, execution, and reflection that enable robust AI agent systems.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e854.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e854.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent method that interleaves explicit reasoning ('thoughts') and actions, recording the thought process so the model can act and observe in loops; improves trustworthiness and reduces hallucination on QA-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ReAct agent/method</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-agent pattern where an LLM alternates between producing a 'thought' (reasoning) and performing an 'action' (tool call or environment interaction); records the chain of thought and observations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>6% hallucination rate on HotpotQA (compared to 14% with Chain-of-Thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>interleaved reasoning-and-action loop; explicit thought trace; supports tool/action calls</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change (agent loop / prompting pattern)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Interleave explicit 'thought' outputs with 'action' invocations so the model reasons, acts, and observes iteratively, increasing transparency and reducing hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reduced hallucination on HotpotQA to 6% compared to 14% for Chain-of-Thought; reported improved effectiveness over zero-shot prompting on language and decision-making tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e854.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e854.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step intermediate reasoning from LLMs to improve multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt engineering strategy instructing the model to produce intermediate reasoning steps (a 'chain of thought') before producing final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Reported 14% hallucination rate on HotpotQA in the survey's comparison</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Elicit stepwise reasoning within the LLM response to improve multi-step reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improves reasoning on some benchmarks versus zero-shot, but in the survey had higher hallucination on HotpotQA compared to ReAct.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e854.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e854.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAISE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAISE (survey reference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ReAct-derived single-agent approach that augments the agent with short-term and long-term memory components and demonstrates improved context retention and output quality; fine-tuning was reported to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>RAISE agent</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Extends the ReAct loop with a scratchpad for short-term memory and a dataset of similar past examples for long-term memory; supports fine-tuning of the LM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Reported to outperform ReAct in both efficiency and output quality (no numeric metrics provided in the survey)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>short-term scratchpad; long-term example dataset; memory augmentation; supports fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>fine-tuning (reported as improving performance for their task)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change + training (memory modules and fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Add persistent and scratch memory stores to maintain longer context and fine-tune the model on task-relevant data to reduce role hallucinations and improve focused behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improved efficiency and output quality vs ReAct as reported by authors; no numeric before/after metrics provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Struggles with complex logic and role-consistency (hallucination about persona/skills); limited reasoning capacity rather than pure memory is implicated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e854.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e854.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language Agents with Verbal Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-agent approach where the agent uses LLM-based self-evaluation and linguistic feedback (verbal RL) to iteratively refine behaviors, improving success rates and lowering hallucination compared to some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language Agents with Verbal Reinforcement Learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Reflexion agent</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses self-reflection via an LLM evaluator (verbal reinforcement learning) with metrics like success state, trajectory, and persistent memory to provide targeted feedback and corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Survey reports improved success rate and reduced hallucination compared to Chain-of-Thought and ReAct, but provides no numeric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>self-reflection via LLM evaluator; sliding-window long-term memory (token-limited)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>verbal reinforcement learning (linguistic feedback loop)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change + training paradigm (self-evaluation / verbal RL)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Use an LLM to evaluate past trajectories and provide linguistic corrective feedback to the agent, iteratively refining behavior and reducing hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reported improved success and reduced hallucination relative to CoT and ReAct; no numerical before/after metrics given in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Limited long-term memory (sliding window bounded by token limit) and susceptibility to local non-optimal solutions; thus constrained by memory and exploration needs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e854.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e854.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGPT+P</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoGPT+P: Affordance-based Task Planning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach for embodied robotic tasks that couples LLM-generated goals/steps with classical PDDL planners, object detection, and object affordance mapping to improve planning accuracy over purely LM-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoGPT+P: Affordance-based Task Planning with Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>AutoGPT+P</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines vision (object detection), object affordance mapping (OAM), an LLM for generating candidate goals/steps, and a classical PDDL planner to execute plans; includes four tool modes for planning and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>robotic task planning / embodied manipulation planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning / embodied navigation / tool selection</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported to significantly improve over purely language-model-based planning approaches; no numeric success rates given in the survey. Also reports variable tool selection accuracy and occasional illogical exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>hybrid architecture: LLM + classical planner (PDDL); object detection; affordance mapping; multiple planning/exploration tools</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>hybrid approach (architectural change)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Delegate plan execution to a classical planner while using the LLM to propose goals/steps and tool choices; combine perception affordances with planner to overcome LLMs' constrained planning abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Significant improvement over pure LLM planning reported by authors in the survey; no quantified before/after provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>LLMs 'constrained reasoning capabilities' limit their ability to translate natural language instructions directly into executable multi-step robotic plans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e854.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e854.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LATS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Agent Tree Search (LATS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-agent method that uses a tree-search (MCTS-inspired) representation of states and LM-based heuristics plus self-reflection to unify planning, acting, and reasoning, yielding strong results on simple QA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LATS</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Represents states as tree nodes and actions as transitions; uses LM heuristics to search candidate actions and an LM-based evaluator for self-reflection/error-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>simple question-answering benchmarks (unnamed in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Survey reports 'extremely well' on various simple QA benchmarks but provides no numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>tree search (MCTS-like), LM heuristics, self-reflection/state evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change (search + reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Integrate an explicit tree search over possible action sequences with LM-guided heuristics and a reflection step to detect and correct reasoning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Dramatic performance improvement on simple QA benchmarks per the cited work; increased computational cost and time.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e854.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e854.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework that reduces unproductive agent chatter by requiring structured outputs and using a publish-subscribe mechanism so agents only read relevant information; reported outperforming single-agent baselines on coding benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>MetaGPT multi-agent framework</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-agent architecture where agents produce structured documents/diagrams rather than unstructured chat; includes a publish-subscribe mechanism to filter messages and reduce noise.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>HumanEval, MBPP (code generation benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Survey states MetaGPT demonstrates 'significantly better results' than single-agent architectures on HumanEval and MBPP, but no numeric values provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>code generation / collaborative programming</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>multi-agent collaboration; structured outputs requirement; publish-subscribe message filtering</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change (multi-agent structuring + message filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Force agents to output structured artifacts and use publish-subscribe to ensure agents only access relevant information, reducing unproductive chatter and improving coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Reported significant improvement on HumanEval and MBPP versus single-agent baselines; no numeric before/after in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Unstructured multi-agent communication ('chatter') and lack of intelligent message filtering degrade interactive/collaborative performance; structured comms mitigate this.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e854.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e854.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DyLAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic LLM-Agent Network (DyLAN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A horizontal multi-agent framework that dynamically re-evaluates agent contributions and advances top contributors to subsequent rounds, improving performance on arithmetic and general reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DyLAN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dynamic multi-agent network where agents share information horizontally; after each round contributions are scored and top contributors move to next round to optimize team performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>arithmetic and general reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Survey reports improved performance on arithmetic and general reasoning benchmarks; no numeric metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / code generation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>dynamic team construction; contribution scoring/ranking; horizontal information sharing</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change (dynamic team optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Rank agent contributions each round and only advance top contributors for subsequent execution rounds to concentrate effort and improve outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improved benchmark performance reported by authors; no explicit numeric before/after values in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e854.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e854.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embodied Teams (Guo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied LLM Agents Learn to Cooperate in Organized Teams</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study showing that multi-agent teams with a designated leader and criticize-reflect evaluation cycles complete embodied tasks faster and with lower communication cost than leaderless teams.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Embodied LLM Agents Learn to Cooperate in Organized Teams.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Embodied multi-agent teams with leader</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vertical/hybrid multi-agent architecture where a lead agent organizes tasks and supports criticize-reflect steps for planning, evaluation and reorganization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>embodied cooperative tasks / team-based task completion</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>sequential decision-making / collaborative planning and execution</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Teams with an organized leader complete tasks nearly 10% faster than teams without a leader (survey-cited result).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>designated leader; criticize-reflect planning/evaluation; dynamic team reorganization</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change (leadership, team dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Introduce a clear leader role and include a criticize-reflect step to generate/evaluate plans, provide feedback, and reorganize teams dynamically to improve efficiency and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Approximately 10% faster task completion with leadered teams; dynamic leadership reduces time-to-completion and communication cost.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Lack of leadership causes inefficient communication where agents spend time giving orders rather than exchanging useful information, degrading interactive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e854.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e854.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentBench / agent-specific benchmarks (survey reference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent-oriented benchmarks that evaluate LLM-based agents in interactive environments (web browsing, CLI, video games) to better measure reasoning, planning, and tool-calling generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Benchmark for LLMs as Intelligent Agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>AgentBench (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A suite for evaluating agents across interactive environments like web browsing, command-line, and games, reporting success rate, human-similarity, and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>web browsing, command-line interfaces, video games (as testbeds)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / web navigation / embodied interaction</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>benchmarking (evaluation methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Provide agent-focused evaluation metrics (success rate, output similarity, efficiency) in interactive environments to better capture agent capabilities beyond static QA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Static, single-iteration QA benchmarks (e.g., MMLU, GSM8K) fail to capture multi-step tool use and planning; data contamination further inflates QA scores relative to real interactive ability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models. <em>(Rating: 2)</em></li>
                <li>Reflexion: Language Agents with Verbal Reinforcement Learning. <em>(Rating: 2)</em></li>
                <li>AutoGPT+P: Affordance-based Task Planning with Large Language Models. <em>(Rating: 2)</em></li>
                <li>Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. <em>(Rating: 2)</em></li>
                <li>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. <em>(Rating: 2)</em></li>
                <li>Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. <em>(Rating: 2)</em></li>
                <li>Embodied LLM Agents Learn to Cooperate in Organized Teams. <em>(Rating: 2)</em></li>
                <li>Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key? <em>(Rating: 2)</em></li>
                <li>A Benchmark for LLMs as Intelligent Agents. <em>(Rating: 2)</em></li>
                <li>AgentBench <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-854",
    "paper_id": "paper-269187633",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "brief_description": "An agent method that interleaves explicit reasoning ('thoughts') and actions, recording the thought process so the model can act and observe in loops; improves trustworthiness and reduces hallucination on QA-style tasks.",
            "citation_title": "ReAct: Synergizing Reasoning and Acting in Language Models.",
            "mention_or_use": "mention",
            "model_or_agent_name": "ReAct agent/method",
            "model_description": "Single-agent pattern where an LLM alternates between producing a 'thought' (reasoning) and performing an 'action' (tool call or environment interaction); records the chain of thought and observations.",
            "model_size": null,
            "qa_task_name": "HotpotQA",
            "qa_performance": "6% hallucination rate on HotpotQA (compared to 14% with Chain-of-Thought prompting)",
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": false,
            "architectural_features": "interleaved reasoning-and-action loop; explicit thought trace; supports tool/action calls",
            "training_method": null,
            "intervention_type": "architectural change (agent loop / prompting pattern)",
            "intervention_description": "Interleave explicit 'thought' outputs with 'action' invocations so the model reasons, acts, and observes iteratively, increasing transparency and reducing hallucination.",
            "intervention_effect": "Reduced hallucination on HotpotQA to 6% compared to 14% for Chain-of-Thought; reported improved effectiveness over zero-shot prompting on language and decision-making tasks.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e854.0",
            "source_info": {
                "paper_title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A prompting technique that elicits step-by-step intermediate reasoning from LLMs to improve multi-step problem solving.",
            "citation_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "mention_or_use": "mention",
            "model_or_agent_name": "Chain-of-Thought prompting",
            "model_description": "Prompt engineering strategy instructing the model to produce intermediate reasoning steps (a 'chain of thought') before producing final answers.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": "Reported 14% hallucination rate on HotpotQA in the survey's comparison",
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": "chain-of-thought prompting",
            "training_method": "prompting only",
            "intervention_type": "prompting strategy",
            "intervention_description": "Elicit stepwise reasoning within the LLM response to improve multi-step reasoning capability.",
            "intervention_effect": "Improves reasoning on some benchmarks versus zero-shot, but in the survey had higher hallucination on HotpotQA compared to ReAct.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e854.1",
            "source_info": {
                "paper_title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "RAISE",
            "name_full": "RAISE (survey reference)",
            "brief_description": "A ReAct-derived single-agent approach that augments the agent with short-term and long-term memory components and demonstrates improved context retention and output quality; fine-tuning was reported to improve performance.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "RAISE agent",
            "model_description": "Extends the ReAct loop with a scratchpad for short-term memory and a dataset of similar past examples for long-term memory; supports fine-tuning of the LM.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": "Reported to outperform ReAct in both efficiency and output quality (no numeric metrics provided in the survey)",
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "short-term scratchpad; long-term example dataset; memory augmentation; supports fine-tuning",
            "training_method": "fine-tuning (reported as improving performance for their task)",
            "intervention_type": "architectural change + training (memory modules and fine-tuning)",
            "intervention_description": "Add persistent and scratch memory stores to maintain longer context and fine-tune the model on task-relevant data to reduce role hallucinations and improve focused behavior.",
            "intervention_effect": "Improved efficiency and output quality vs ReAct as reported by authors; no numeric before/after metrics provided in the survey.",
            "hypothesized_cause_of_gap": "Struggles with complex logic and role-consistency (hallucination about persona/skills); limited reasoning capacity rather than pure memory is implicated.",
            "uuid": "e854.2",
            "source_info": {
                "paper_title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "brief_description": "A single-agent approach where the agent uses LLM-based self-evaluation and linguistic feedback (verbal RL) to iteratively refine behaviors, improving success rates and lowering hallucination compared to some baselines.",
            "citation_title": "Reflexion: Language Agents with Verbal Reinforcement Learning.",
            "mention_or_use": "mention",
            "model_or_agent_name": "Reflexion agent",
            "model_description": "Uses self-reflection via an LLM evaluator (verbal reinforcement learning) with metrics like success state, trajectory, and persistent memory to provide targeted feedback and corrections.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": "Survey reports improved success rate and reduced hallucination compared to Chain-of-Thought and ReAct, but provides no numeric scores.",
            "interactive_task_name": null,
            "interactive_task_type": "multi-step reasoning / sequential decision-making",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "self-reflection via LLM evaluator; sliding-window long-term memory (token-limited)",
            "training_method": "verbal reinforcement learning (linguistic feedback loop)",
            "intervention_type": "architectural change + training paradigm (self-evaluation / verbal RL)",
            "intervention_description": "Use an LLM to evaluate past trajectories and provide linguistic corrective feedback to the agent, iteratively refining behavior and reducing hallucination.",
            "intervention_effect": "Reported improved success and reduced hallucination relative to CoT and ReAct; no numerical before/after metrics given in the survey.",
            "hypothesized_cause_of_gap": "Limited long-term memory (sliding window bounded by token limit) and susceptibility to local non-optimal solutions; thus constrained by memory and exploration needs.",
            "uuid": "e854.3",
            "source_info": {
                "paper_title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "AutoGPT+P",
            "name_full": "AutoGPT+P: Affordance-based Task Planning with Large Language Models",
            "brief_description": "A hybrid approach for embodied robotic tasks that couples LLM-generated goals/steps with classical PDDL planners, object detection, and object affordance mapping to improve planning accuracy over purely LM-based planning.",
            "citation_title": "AutoGPT+P: Affordance-based Task Planning with Large Language Models.",
            "mention_or_use": "mention",
            "model_or_agent_name": "AutoGPT+P",
            "model_description": "Combines vision (object detection), object affordance mapping (OAM), an LLM for generating candidate goals/steps, and a classical PDDL planner to execute plans; includes four tool modes for planning and exploration.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "robotic task planning / embodied manipulation planning",
            "interactive_task_type": "planning / embodied navigation / tool selection",
            "interactive_performance": "Reported to significantly improve over purely language-model-based planning approaches; no numeric success rates given in the survey. Also reports variable tool selection accuracy and occasional illogical exploration.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "hybrid architecture: LLM + classical planner (PDDL); object detection; affordance mapping; multiple planning/exploration tools",
            "training_method": null,
            "intervention_type": "hybrid approach (architectural change)",
            "intervention_description": "Delegate plan execution to a classical planner while using the LLM to propose goals/steps and tool choices; combine perception affordances with planner to overcome LLMs' constrained planning abilities.",
            "intervention_effect": "Significant improvement over pure LLM planning reported by authors in the survey; no quantified before/after provided here.",
            "hypothesized_cause_of_gap": "LLMs 'constrained reasoning capabilities' limit their ability to translate natural language instructions directly into executable multi-step robotic plans.",
            "uuid": "e854.4",
            "source_info": {
                "paper_title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LATS",
            "name_full": "Language Agent Tree Search (LATS)",
            "brief_description": "A single-agent method that uses a tree-search (MCTS-inspired) representation of states and LM-based heuristics plus self-reflection to unify planning, acting, and reasoning, yielding strong results on simple QA benchmarks.",
            "citation_title": "Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models.",
            "mention_or_use": "mention",
            "model_or_agent_name": "LATS",
            "model_description": "Represents states as tree nodes and actions as transitions; uses LM heuristics to search candidate actions and an LM-based evaluator for self-reflection/error-correction.",
            "model_size": null,
            "qa_task_name": "simple question-answering benchmarks (unnamed in survey)",
            "qa_performance": "Survey reports 'extremely well' on various simple QA benchmarks but provides no numeric metrics.",
            "interactive_task_name": null,
            "interactive_task_type": "planning / multi-step reasoning",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": "tree search (MCTS-like), LM heuristics, self-reflection/state evaluator",
            "training_method": null,
            "intervention_type": "architectural change (search + reflection)",
            "intervention_description": "Integrate an explicit tree search over possible action sequences with LM-guided heuristics and a reflection step to detect and correct reasoning errors.",
            "intervention_effect": "Dramatic performance improvement on simple QA benchmarks per the cited work; increased computational cost and time.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e854.5",
            "source_info": {
                "paper_title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "MetaGPT",
            "name_full": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
            "brief_description": "A multi-agent framework that reduces unproductive agent chatter by requiring structured outputs and using a publish-subscribe mechanism so agents only read relevant information; reported outperforming single-agent baselines on coding benchmarks.",
            "citation_title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework.",
            "mention_or_use": "mention",
            "model_or_agent_name": "MetaGPT multi-agent framework",
            "model_description": "Multi-agent architecture where agents produce structured documents/diagrams rather than unstructured chat; includes a publish-subscribe mechanism to filter messages and reduce noise.",
            "model_size": null,
            "qa_task_name": "HumanEval, MBPP (code generation benchmarks)",
            "qa_performance": "Survey states MetaGPT demonstrates 'significantly better results' than single-agent architectures on HumanEval and MBPP, but no numeric values provided.",
            "interactive_task_name": null,
            "interactive_task_type": "code generation / collaborative programming",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "multi-agent collaboration; structured outputs requirement; publish-subscribe message filtering",
            "training_method": null,
            "intervention_type": "architectural change (multi-agent structuring + message filtering)",
            "intervention_description": "Force agents to output structured artifacts and use publish-subscribe to ensure agents only access relevant information, reducing unproductive chatter and improving coordination.",
            "intervention_effect": "Reported significant improvement on HumanEval and MBPP versus single-agent baselines; no numeric before/after in the survey.",
            "hypothesized_cause_of_gap": "Unstructured multi-agent communication ('chatter') and lack of intelligent message filtering degrade interactive/collaborative performance; structured comms mitigate this.",
            "uuid": "e854.6",
            "source_info": {
                "paper_title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "DyLAN",
            "name_full": "Dynamic LLM-Agent Network (DyLAN)",
            "brief_description": "A horizontal multi-agent framework that dynamically re-evaluates agent contributions and advances top contributors to subsequent rounds, improving performance on arithmetic and general reasoning benchmarks.",
            "citation_title": "Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization.",
            "mention_or_use": "mention",
            "model_or_agent_name": "DyLAN",
            "model_description": "Dynamic multi-agent network where agents share information horizontally; after each round contributions are scored and top contributors move to next round to optimize team performance.",
            "model_size": null,
            "qa_task_name": "arithmetic and general reasoning benchmarks",
            "qa_performance": "Survey reports improved performance on arithmetic and general reasoning benchmarks; no numeric metrics provided.",
            "interactive_task_name": null,
            "interactive_task_type": "multi-step reasoning / code generation",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": false,
            "architectural_features": "dynamic team construction; contribution scoring/ranking; horizontal information sharing",
            "training_method": null,
            "intervention_type": "architectural change (dynamic team optimization)",
            "intervention_description": "Rank agent contributions each round and only advance top contributors for subsequent execution rounds to concentrate effort and improve outcomes.",
            "intervention_effect": "Improved benchmark performance reported by authors; no explicit numeric before/after values in the survey.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e854.7",
            "source_info": {
                "paper_title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Embodied Teams (Guo)",
            "name_full": "Embodied LLM Agents Learn to Cooperate in Organized Teams",
            "brief_description": "A study showing that multi-agent teams with a designated leader and criticize-reflect evaluation cycles complete embodied tasks faster and with lower communication cost than leaderless teams.",
            "citation_title": "Embodied LLM Agents Learn to Cooperate in Organized Teams.",
            "mention_or_use": "mention",
            "model_or_agent_name": "Embodied multi-agent teams with leader",
            "model_description": "Vertical/hybrid multi-agent architecture where a lead agent organizes tasks and supports criticize-reflect steps for planning, evaluation and reorganization.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "embodied cooperative tasks / team-based task completion",
            "interactive_task_type": "sequential decision-making / collaborative planning and execution",
            "interactive_performance": "Teams with an organized leader complete tasks nearly 10% faster than teams without a leader (survey-cited result).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "designated leader; criticize-reflect planning/evaluation; dynamic team reorganization",
            "training_method": null,
            "intervention_type": "architectural change (leadership, team dynamics)",
            "intervention_description": "Introduce a clear leader role and include a criticize-reflect step to generate/evaluate plans, provide feedback, and reorganize teams dynamically to improve efficiency and accuracy.",
            "intervention_effect": "Approximately 10% faster task completion with leadered teams; dynamic leadership reduces time-to-completion and communication cost.",
            "hypothesized_cause_of_gap": "Lack of leadership causes inefficient communication where agents spend time giving orders rather than exchanging useful information, degrading interactive performance.",
            "uuid": "e854.8",
            "source_info": {
                "paper_title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "AgentBench",
            "name_full": "AgentBench / agent-specific benchmarks (survey reference)",
            "brief_description": "Agent-oriented benchmarks that evaluate LLM-based agents in interactive environments (web browsing, CLI, video games) to better measure reasoning, planning, and tool-calling generalization.",
            "citation_title": "A Benchmark for LLMs as Intelligent Agents.",
            "mention_or_use": "mention",
            "model_or_agent_name": "AgentBench (benchmark)",
            "model_description": "A suite for evaluating agents across interactive environments like web browsing, command-line, and games, reporting success rate, human-similarity, and efficiency.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "web browsing, command-line interfaces, video games (as testbeds)",
            "interactive_task_type": "tool use / web navigation / embodied interaction",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": "benchmarking (evaluation methodology)",
            "intervention_description": "Provide agent-focused evaluation metrics (success rate, output similarity, efficiency) in interactive environments to better capture agent capabilities beyond static QA benchmarks.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Static, single-iteration QA benchmarks (e.g., MMLU, GSM8K) fail to capture multi-step tool use and planning; data contamination further inflates QA scores relative to real interactive ability.",
            "uuid": "e854.9",
            "source_info": {
                "paper_title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models.",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Reflexion: Language Agents with Verbal Reinforcement Learning.",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "AutoGPT+P: Affordance-based Task Planning with Large Language Models.",
            "rating": 2,
            "sanitized_title": "autogptp_affordancebased_task_planning_with_large_language_models"
        },
        {
            "paper_title": "Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models.",
            "rating": 2,
            "sanitized_title": "language_agent_tree_search_unifies_reasoning_acting_and_planning_in_language_models"
        },
        {
            "paper_title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework.",
            "rating": 2,
            "sanitized_title": "metagpt_meta_programming_for_a_multiagent_collaborative_framework"
        },
        {
            "paper_title": "Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization.",
            "rating": 2,
            "sanitized_title": "dynamic_llmagent_network_an_llmagent_collaboration_framework_with_agent_team_optimization"
        },
        {
            "paper_title": "Embodied LLM Agents Learn to Cooperate in Organized Teams.",
            "rating": 2,
            "sanitized_title": "embodied_llm_agents_learn_to_cooperate_in_organized_teams"
        },
        {
            "paper_title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
            "rating": 2,
            "sanitized_title": "rethinking_the_bounds_of_llm_reasoning_are_multiagent_discussions_the_key"
        },
        {
            "paper_title": "A Benchmark for LLMs as Intelligent Agents.",
            "rating": 2,
            "sanitized_title": "a_benchmark_for_llms_as_intelligent_agents"
        },
        {
            "paper_title": "AgentBench",
            "rating": 1,
            "sanitized_title": "agentbench"
        }
    ],
    "cost": 0.021532,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>THE LANDSCAPE OF EMERGING AI AGENT ARCHITECTURES FOR REASONING, PLANNING, AND TOOL CALLING: A SURVEY</p>
<p>Tula Masterman tula.masterman@neudesic.com 
Neudesic, an IBM Company</p>
<p>Sandi Besen sandi.besen@ibm.com 
Neudesic, an IBM Company</p>
<p>Alex Chao achao@microsoft.com 
Neudesic, an IBM Company</p>
<p>THE LANDSCAPE OF EMERGING AI AGENT ARCHITECTURES FOR REASONING, PLANNING, AND TOOL CALLING: A SURVEY
6B364B13B6694A855E8DF497B6D8854F
This survey paper examines the recent advancements in AI agent implementations, with a focus on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities.The primary objectives of this work are to a) communicate the current capabilities and limitations of existing AI agent implementations, b) share insights gained from our observations of these systems in action, and c) suggest important considerations for future developments in AI agent design.We achieve this by providing overviews of single-agent and multi-agent architectures, identifying key patterns and divergences in design choices, and evaluating their overall impact on accomplishing a provided goal.Our contribution outlines key themes when selecting an agentic architecture, the impact of leadership on agent systems, agent communication styles, and key phases for planning, execution, and reflection that enable robust AI agent systems.</p>
<p>Introduction</p>
<p>Since the launch of ChatGPT, many of the first wave of generative AI applications have been a variation of a chat over a corpus of documents using the Retrieval Augmented Generation (RAG) pattern.While there is a lot of activity in making RAG systems more robust, various groups are starting to build what the next generation of AI applications will look like, centralizing on a common theme: agents.</p>
<p>Beginning with investigations into recent foundation models like GPT-4 and popularized through open-source projects like AutoGPT and BabyAGI, the research community has experimented with building autonomous agent-based systems [19,1].</p>
<p>As opposed to zero-shot prompting of a large language model where a user types into an open-ended text field and gets a result without additional input, agents allow for more complex interaction and orchestration.In particular, agentic systems have a notion of planning, loops, reflection and other control structures that heavily leverage the model's inherent reasoning capabilities to accomplish a task end-to-end.Paired with the ability to use tools, plugins, and function calling, agents are empowered to do more general-purpose work.</p>
<p>Among the community, there is a current debate on whether single or multi-agent systems are best suited for solving complex tasks.While single agent architectures excel when problems are well-defined and feedback from other</p>
<p>The opinions expressed in this paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.arXiv:2404.11584v1[cs.AI] 17 Apr 2024 agent-personas or the user is not needed, multi-agent architectures tend to thrive more when collaboration and multiple distinct execution paths are required.Agents.AI agents are language model-powered entities able to plan and take actions to execute goals over multiple iterations.AI agent architectures are either comprised of a single agent or multiple agents working together to solve a problem.</p>
<p>Typically, each agent is given a persona and access to a variety of tools that will help them accomplish their job either independently or as part of a team.Some agents also contain a memory component, where they can save and load information outside of their messages and prompts.In this paper, we follow the definition of agent that consists of "brain, perception, and action" [31].These components satisfy the minimum requirements for agents to understand, reason, and act on the environment around them.</p>
<p>Agent Persona.An agent persona describes the role or personality that the agent should take on, including any other instructions specific to that agent.Personas also contain descriptions of any tools the agent has access to.They make the agent aware of their role, the purpose of their tools, and how to leverage them effectively.Researchers have found that "shaped personality verifiably influences Large Language Model (LLM) behavior in common downstream (i.e.subsequent) tasks, such as writing social media posts" [21].Solutions that use multiple agent personas to solve problems also show significant improvements compared to Chain-of-Thought (CoT) prompting where the model is asked to break down its plans step by step [28,29].</p>
<p>Tools.In the context of AI agents, tools represent any functions that the model can call.They allow the agent to interact with external data sources by pulling or pushing information to that source.An example of an agent persona and associated tools is a professional contract writer.The writer is given a persona explaining their role and the types of tasks it must accomplish.It is also given tools related to adding notes to a document, reading an existing document, or sending an email with a final draft.</p>
<p>Single Agent Architectures.These architectures are powered by one language model and will perform all the reasoning, planning, and tool execution on their own.The agent is given a system prompt and any tools required to complete their task.In single agent patterns there is no feedback mechanism from other AI agents; however, there may be options for humans to provide feedback that guides the agent.</p>
<p>Multi-Agent Architectures.These architectures involve two or more agents, where each agent can utilize the same language model or a set of different language models.The agents may have access to the same tools or different tools.Each agent typically has their own persona.</p>
<p>Multi-agent architectures can have a wide variety of organizations at any level of complexity.In this paper, we divide them into two primary categories: vertical and horizontal.It is important to keep in mind that these categories represent two ends of a spectrum, where most existing architectures fall somewhere between these two extremes.</p>
<p>Vertical Architectures.In this structure, one agent acts as a leader and has other agents report directly to them.Depending on the architecture, reporting agents may communicate exclusively with the lead agent.Alternatively, a leader may be defined with a shared conversation between all agents.The defining features of vertical architectures include having a lead agent and a clear division of labor between the collaborating agents.</p>
<p>Horizontal Architectures.In this structure, all the agents are treated as equals and are part of one group discussion about the task.Communication between agents occurs in a shared thread where each agent can see all messages from the others.Agents also can volunteer to complete certain tasks or call tools, meaning they do not need to be assigned by a leading agent.Horizontal architectures are generally used for tasks where collaboration, feedback and group discussion are key to the overall success of the task [2].</p>
<p>2 Key Considerations for Effective Agents</p>
<p>Overview</p>
<p>Agents are designed to extend language model capabilities to solve real-world challenges.Successful implementations require robust problem-solving capabilities enabling agents to perform well on novel tasks.To solve real-world problems effectively, agents require the ability to reason and plan as well as call tools that interact with an external environment.In this section we explore why reasoning, planning, and tool calling are critical to agent success.</p>
<p>The Importance of Reasoning and Planning</p>
<p>Reasoning is a fundamental building block of human cognition, enabling people to make decisions, solve problems, and understand the world around us.AI agents need a strong ability to reason if they are to effectively interact with complex environments, make autonomous decisions, and assist humans in a wide range of tasks.This tight synergy between "acting" and "reasoning" allows new tasks to be learned quickly and enables robust decision making or reasoning, even under previously unseen circumstances or information uncertainties [32].Additionally, agents need reasoning to adjust their plans based on new feedback or information learned.</p>
<p>If agents lacking reasoning skills are tasked with acting on straightforward tasks, they may misinterpret the query, generate a response based on a literal understanding, or fail to consider multi-step implications.</p>
<p>Planning, which requires strong reasoning abilities, commonly falls into one of five major approaches: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning [12].These approaches allow the model to either break the task down into sub tasks, select one plan from many generated options, leverage a preexisting external plan, revise previous plans based on new information, or leverage external information to improve the plan.</p>
<p>Most agent patterns have a dedicated planning step which invokes one or more of these techniques to create a plan before any actions are executed.For example, Plan Like a Graph (PLaG) is an approach that represents plans as directed graphs, with multiple steps being executed in parallel [15,33].This can provide a significant performance increase over other methods on tasks that contain many independent subtasks that benefit from asynchronous execution.</p>
<p>The Importance of Effective Tool Calling</p>
<p>One key benefit of the agent abstraction over prompting base language models is the agents' ability to solve complex problems by calling multiple tools.These tools enable the agent to interact with external data sources, send or retrieve information from existing APIs, and more.Problems that require extensive tool calling often go hand in hand with those that require complex reasoning.</p>
<p>Both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps.Many methods use multiple iterations of reasoning, memory, and reflection to effectively and accurately complete problems [16,23,32].They often do this by breaking a larger problem into smaller subproblems, and then solving each one with the appropriate tools in sequence.</p>
<p>Other works focused on advancing agent patterns highlight that while breaking a larger problem into smaller subproblems can be effective at solving complex tasks, single agent patterns often struggle to complete the long sequence required [22,6].</p>
<p>Multi-agent patterns can address the issues of parallel tasks and robustness since individual agents can work on individual subproblems.Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks.Then, each agent works independently on solving each task using their own independent set of tools.</p>
<p>3 Single Agent Architectures</p>
<p>Overview</p>
<p>In this section, we highlight some notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, and LATS.Each of these methods contain a dedicated stage for reasoning about the problem before any action is taken to advance the goal.We selected these methods based on their contributions to the reasoning and tool calling capabilities of agents.</p>
<p>Key Themes</p>
<p>We find that successful goal execution by agents is contingent upon proper planning and self-correction [32,16,23,1].Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations [32].We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent [22].</p>
<p>Examples</p>
<p>ReAct.In the ReAct (Reason + Act) method, an agent first writes a thought about the given task.It then performs an action based on that thought, and the output is observed.This cycle can repeat until the task is complete [32].When applied to a diverse set of language and decision-making tasks, the ReAct method demonstrates improved effectiveness compared to zero-shot prompting on the same tasks.It also provides improved human interoperability and trustworthiness because the entire thought process of the model is recorded.When evaluated on the HotpotQA dataset, the ReAct method only hallucinated 6% of the time, compared to 14% using the chain of thought (CoT) method [29,32].However, the ReAct method is not without its limitations.While intertwining reasoning, observation, and action improves trustworthiness, the model can repetitively generate the same thoughts and actions and fail to create new thoughts to provoke finishing the task and exiting the ReAct loop.Incorporating human feedback during the execution of the task would likely increase its effectiveness and applicability in real-world scenarios.</p>
<p>RAISE.</p>
<p>The RAISE method is built upon the ReAct method, with the addition of a memory mechanism that mirrors human short-term and long-term memory [16].It does this by using a scratchpad for short-term storage and a dataset of similar previous examples for long-term storage.</p>
<p>By adding these components, RAISE improves upon the agent's ability to retain context in longer conversations.The paper also highlights how fine-tuning the model results in the best performance for their task, even when using a smaller model.They also showed that RAISE outperforms ReAct in both efficiency and output quality.</p>
<p>While RAISE significantly improves upon existing methods in some respects, the researchers also highlighted several issues.First, RAISE struggles to understand complex logic, limiting its usefulness in many scenarios.Additionally, RAISE agents often hallucinated with respect to their roles or knowledge.For example, a sales agent without a clearly defined role might retain the ability to code in Python, which may enable them to start writing Python code instead of focusing on their sales tasks.These agents might also give the user misleading or incorrect information.This problem was addressed by fine-tuning the model, but the researchers still highlighted hallucination as a limitation in the RAISE implementation.</p>
<p>Reflexion.Reflexion is a single-agent pattern that uses self-reflection through linguistic feedback [23].By utilizing metrics such as success state, current trajectory, and persistent memory, this method uses an LLM evaluator to provide</p>
<p>Example Pool
 &lt; Q1, A1 &gt;  &lt; Q2, A2 &gt;  &lt; Q3, A3 &gt;  . . .  &lt; Q N , A N &gt; Working Memory  System Prompt  Profile  Task Instruction  . . .  Conversation History  Scratchpad  Retrieved Examples  Task Trajectory LLMs  API-based LLMs  GPT-4 GPT-3.5  Claude  . . .  Open-sourced LLMs  Llama  Qwen  Baichuan  . . .</p>
<p>Agent Loop</p>
<p>Figure 3: A diagram showing the RAISE method [16] specific and relevant feedback to the agent.This results in an improved success rate as well as reduced hallucination compared to Chain-of-Thought and ReAct.</p>
<p>Despite these advancements, the Reflexion authors identify various limitations of the pattern.Primarily, Reflexion is susceptible to "non-optimal local minima solutions".It also uses a sliding window for long-term memory, rather than a database.This means that the volume of long-term memory is limited by the token limit of the language model.Finally, the researchers identify that while Reflexion surpasses other single-agent patterns, there are still opportunities to improve performance on tasks that require a significant amount of diversity, exploration, and reasoning.</p>
<p>AUTOGPT + P. AutoGPT + P (Planning) is a method that addresses reasoning limitations for agents that command robots in natural language [1].AutoGPT+P combines object detection and Object Affordance Mapping (OAM) with a planning system driven by a LLM.This allows the agent to explore the environment for missing objects, propose alternatives, or ask the user for assistance with reaching its goal.</p>
<p>AutoGPT+P starts by using an image of a scene to detect the objects present.A language model then uses those objects to select which tool to use, from four options: Plan Tool, Partial Plan Tool, Suggest Alternative Tool, and Explore Tool.These tools allow the robot to not only generate a full plan to complete the goal, but also to explore the environment, make assumptions, and create partial plans.</p>
<p>However, the language model does not generate the plan entirely on its own.Instead, it generates goals and steps to work aside a classical planner which executes the plan using Planning Domain Definition Language (PDDL).The paper found that "LLMs currently lack the ability to directly translate a natural language instruction into a plan for executing robotic tasks, primarily due to their constrained reasoning capabilities" [1].By combining the LLM planning capabilities with a classical planner, their approach significantly improves upon other purely language model-based approaches to robotic planning.</p>
<p>As with most first of their kind approaches, AutoGPT+P is not without its drawbacks.Accuracy of tool selection varies, with certain tools being called inappropriately or getting stuck in loops.In scenarios where exploration is required, the tool selection sometimes leads to illogical exploration decisions like looking for objects in the wrong place.The framework also is limited in terms of human interaction, with the agent being unable to seek clarification and the user being unable to modify or terminate the plan during execution.LATS.Language Agent Tree Search (LATS) is a single-agent method that synergizes planning, acting, and reasoning by using trees [36].This technique, inspired by Monte Carlo Tree Search, represents a state as a node and taking an action as traversing between nodes.It uses LM-based heuristics to search for possible options, then selects an action using a state evaluator.</p>
<p>When compared to other tree-based methods, LATS implements a self-reflection reasoning step that dramatically improves performance.When an action is taken, both environmental feedback as well as feedback from a language model is used to determine if there are any errors in reasoning and propose alternatives.This ability to self-reflect combined with a powerful search algorithm makes LATS perform extremely well on various tasks.</p>
<p>However, due to the complexity of the algorithm and the reflection steps involved, LATS often uses more computational resources and takes more time to complete than other single-agent methods [36].The paper also uses relatively simple question answering benchmarks and has not been tested on more robust scenarios that involve involving tool calling or complex reasoning.</p>
<p>4 Multi Agent Architectures</p>
<p>Overview</p>
<p>In this section, we examine a few key studies and sample frameworks with multi-agent architectures, such as Embodied LLM Agents Learn to Cooperate in Organized Teams, DyLAN, AgentVerse, and MetaGPT.We highlight how these implementations facilitate goal execution through inter-agent communication and collaborative plan execution.This is not intended to be an exhaustive list of all agent frameworks, our goal is to provide broad coverage of key themes and examples related to multi-agent patterns.</p>
<p>Key Themes</p>
<p>Multi-agent architectures create an opportunity for both the intelligent division of labor based on skill and helpful feedback from a variety of agent personas.Many multi-agent architectures work in stages where teams of agents are created and reorganized dynamically for each planning, execution, and evaluation phase [2,9,18].This reorganization provides superior results because specialized agents are employed for certain tasks, and removed when they are no longer needed.By matching agents roles and skills to the task at hand, agent teams can achieve greater accuracy and decrease time to meet the goal.Key features of effective multi-agent architectures include clear leadership in agent teams, dynamic team construction, and effective information sharing between team members so that important information does not get lost in superfluous chatter.</p>
<p>Examples</p>
<p>Embodied LLM Agents Learn to Cooperate in Organized Teams.Research by Guo et al. demonstrates the impact of a lead agent on the overall effectiveness of the agent team [9].This architecture contains a vertical component through the leader agent, as well as a horizontal component from the ability for agents to converse with other agents besides the leader.The results of their study demonstrate that agent teams with an organized leader complete their tasks nearly 10% faster than teams without a leader.</p>
<p>Furthermore, they discovered that in teams without a designated leader, agents spent most of their time giving orders to one another (~50% of communication), splitting their remaining time between sharing information, or requesting guidance.Conversely, in teams with a designated leader, 60% of the leader's communication involved giving directions, prompting other members to focus more on exchanging and requesting information.Their results demonstrate that agent teams are most effective when the leader is a human.</p>
<p>Figure 5: Agent teams with a designated leader achieve superior performance [9] .</p>
<p>Beyond team structure, the paper emphasizes the importance of employing a "criticize-reflect" step for generating plans, evaluating performance, providing feedback, and re-organizing the team [9].Their results indicate that agents with a dynamic team structure with rotating leadership provide the best results, with both the lowest time to task completion and the lowest communication cost on average.Ultimately, leadership and dynamic team structures improve the overall team's ability to reason, plan, and perform tasks effectively.</p>
<p>DyLAN.The Dynamic LLM-Agent Network (DyLAN) framework creates a dynamic agent structure that focuses on complex tasks like reasoning and code generation [18].DyLAN has a specific step for determining how much each agent has contributed in the last round of work and only moves top contributors the next round of execution.This method is horizontal in nature since agents can share information with each other and there is no defined leader.DyLAN shows improved performance on a variety of benchmarks which measure arithmetic and general reasoning capabilities.This highlights the impact of dynamic teams and demonstrates that by consistently re-evaluating and ranking agent contributions, we can create agent teams that are better suited to complete a given task.</p>
<p>AgentVerse.Multi-agent architectures like AgentVerse demonstrate how distinct phases for group planning can improve an AI agent's reasoning and problem-solving capabilities [2].AgentVerse contains four primary stages for task execution: recruitment, collaborative decision making, independent action execution, and evaluation.This can be repeated until the overall goal is achieved.By strictly defining each phase, AgentVerse helps guide the set of agents to reason, discuss, and execute more effectively.</p>
<p>As an example, the recruitment step allows agents to be removed or added based on the progress towards the goal.This helps ensure that the right agents are participating at any given stage of problem solving.The researchers found that horizontal teams are generally best suited for collaborative tasks like consulting, while vertical teams are better suited for tasks that require clearer isolation of responsibilities for tool calling.Actions:</p>
<p>==</p>
<p>Agents:</p>
<p>x N rounds</p>
<p>x M turns MetaGPT.Many multi-agent architectures allow agents to converse with one another while collaborating on a common problem.This conversational capability can lead to chatter between the agents that is superfluous and does not further the team goal.MetaGPT addresses the issue of unproductive chatter amongst agents by requiring agents to generate structured outputs like documents and diagrams instead of sharing unstructured chat messages [11].</p>
<p>Additionally, MetaGPT implements a "publish-subscribe" mechanism for information sharing.This allows all the agents to share information in one place, but only read information relevant to their individual goals and tasks.This streamlines the overall goal execution and reduces conversational noise between agents.When compared to single-agent architectures on the HumanEval and MBPP benchmarks, MetaGPT's multi-agent architecture demonstrates significantly better results.</p>
<p>5 Discussion and Observations</p>
<p>Overview</p>
<p>In this section we discuss the key themes and impacts of the design choices exhibited in the previously outlined agent patterns.These patterns serve as key examples of the growing body of research and implementation of AI agent architectures.Both single and multi-agent architectures seek to enhance the capabilities of language models by giving them the ability to execute goals on behalf of or alongside a human user.Most observed agent implementations broadly follow the plan, act, and evaluate process to iteratively solve problems.</p>
<p>We find that both single and multi-agent architectures demonstrate compelling performance on complex goal execution.We also find that across architectures clear feedback, task decomposition, iterative refinement, and role definition yield improved agent performance.</p>
<p>Key Findings</p>
<p>Typical Conditions for Selecting a Single vs Multi-Agent Architecture.Based on the aforementioned agent patterns, we find that single-agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined.Single agents are also typically easier to implement since only one agent and set of tools needs to be defined.Additionally, single agent architectures do not face limitations like poor feedback from other agents or distracting and unrelated chatter from other team members.However, they may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities are not robust.</p>
<p>Multi-agent architectures are generally well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task.For example, document generation may benefit from a multi-agent architecture where one agent provides clear feedback to another on a written section of the document.Multi-agent systems are also useful when parallelization across distinct tasks or workflows is required.Crucially, Wang et.al finds that multi-agent patterns perform better than single agents in scenarios when no examples are provided [26].By nature, multi-agent systems are more complex and often benefit from robust conversation management and clear leadership.</p>
<p>While single and multi-agent patterns have diverging capabilities in terms of scope, research finds that "multi-agent discussion does not necessarily enhance reasoning when the prompt provided to an agent is sufficiently robust" [26].This suggests that those implementing agent architectures should decide between single or multiple agents based on the broader context of their use case, and not based on the reasoning capabilities required.</p>
<p>Agents and Asynchronous Task Execution.While a single agent can initiate multiple asynchronous calls simultaneously, its operational model does not inherently support the division of responsibilities across different execution threads.This means that, although tasks are handled asynchronously, they are not truly parallel in the sense of being autonomously managed by separate decision-making entities.Instead, the single agent must sequentially plan and execute tasks, waiting for one batch of asynchronous operations to complete before it can evaluate and move on to the next step.Conversely, in multi-agent architectures, each agent can operate independently, allowing for a more dynamic division of labor.This structure not only facilitates simultaneous task execution across different domains or objectives but also allows individual agents to proceed with their next steps without being hindered by the state of tasks handled by others, embodying a more flexible and parallel approach to task management.</p>
<p>Impact of Feedback and Human Oversight on Agent Systems.When solving a complex problem, it is extremely unlikely that one provides a correct, robust solution on their first try.Instead, one might pose a potential solution before criticizing it and refining it.One could also consult with someone else and receive feedback from another perspective.The same idea of iterative feedback and refinement is essential for helping agents solve complex problems.This is partially because language models tend to commit to an answer earlier in their response, which can cause a 'snowball effect' of increasing diversion from their goal state [34] .By implementing feedback, agents are much more likely to correct their course and reach their goal.</p>
<p>Additionally, the inclusion of human oversight improves the immediate outcome by aligning the agent's responses more closely with human expectations, mitigating the potential for agents to delve down an inefficient or invalid approach to solving a task.As of today, including human validation and feedback in the agent architecture yields more reliable and trustworthy results [4,9].</p>
<p>Language models also exhibit sycophantic behavior, where they "tend to mirror the user's stance, even if it means forgoing the presentation of an impartial or balanced viewpoint" [20].Specifically, the AgentVerse paper describes how agents are susceptible to feedback from other agents, even if the feedback is not sound.This can lead the agent team to generate a faulty plan which diverts them from their objective [2].Robust prompting can help mitigate this, but those developing agent applications should be aware of the risks when implementing user or agent feedback systems.</p>
<p>Challenges with Group Conversations and Information Sharing.One challenge with multi-agent architectures lies in their ability to intelligently share messages between agents.Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like "how are you", while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage.The extraneous dialogue in multi-agent systems can impair both the agent's ability to reason effectively and execute the right tools, ultimately distracting the agents from the task and decreasing team efficiency.This is especially true in a horizontal architecture, where agents typically share a group chat and are privy to every agent's message in a conversation.Message subscribing or filtering improves multi-agent performance by ensuring agents only receive information relevant to their tasks.</p>
<p>In vertical architectures, tasks tend to be clearly divided by agent skill which helps reduce distractions in the team.However, challenges arise when the leading agent fails to send critical information to their supporting agents and does not realize the other agents aren't privy to necessary information.This failure can lead to confusion in the team or hallucination in the results.One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.</p>
<p>Impact of Role Definition and Dynamic Teams.Clear role definition is critical for both single and multi-agent architectures.In single-agent architectures role definition ensures that the agent stays focused on the provided task, executes the proper tools, and minimizes hallucination of other capabilities.Similarly, role definition in multi-agent architectures ensures each agent knows what it's responsible for in the overall team and does not take on tasks outside of their described capabilities or scope.Beyond individual role definition, establishing a clear group leader also improves the overall performance of multi-agent teams by streamlining task assignment.Furthermore, defining a clear system prompt for each agent can minimize excess chatter by prompting the agents not to engage in unproductive communication.</p>
<p>Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective.This ensures that all agents participating in the planning or execution of tasks are fit for that round of work.</p>
<p>Summary</p>
<p>Both single and multi-agent patterns exhibit strong performance on a variety of complex tasks involving reasoning and tool execution.Single agent patterns perform well when given a defined persona and set of tools, opportunities for human feedback, and the ability to work iteratively towards their goal.When constructing an agent team that needs to collaborate on complex goals, it is beneficial to deploy agents with at least one of these key elements: clear leader(s), a defined planning phase and opportunities to refine the plan as new information is learned, intelligent message filtering, and dynamic teams whose agents possess specific skills relevant to the current sub-task.If an agent architecture employs at least one of these approaches it is likely to result in increased performance compared to a single agent architecture or a multi-agent architecture without these tactics.</p>
<p>6 Limitations of Current Research and Considerations for Future Research</p>
<p>Overview</p>
<p>In this section we examine some of the limitations of agent research today and identify potential areas for improving AI agent systems.While agent architectures have significantly enhanced the capability of language models in many ways, there are some major challenges around evaluations, overall reliability, and issues inherited from the language models powering each agent.</p>
<p>Challenges with Agent Evaluation</p>
<p>While LLMs are evaluated on a standard set of benchmarks designed to gauge their general understanding and reasoning capabilities, the benchmarks for agent evaluation vary greatly.</p>
<p>Many research teams introduce their own unique agent benchmarks alongside their agent implementation which makes comparing multiple agent implementations on the same benchmark challenging.Additionally, many of these new agent-specific benchmarks include a hand-crafted, highly complex, evaluation set where the results are manually scored [2].This can provide a high-quality assessment of a method's capabilities, but it also lacks the robustness of a larger dataset and risks introducing bias into the evaluation, since the ones developing the method are also the ones writing and scoring the results.Agents can also have problems generating a consistent answer over multiple iterations, due to variability in the models, environment, or problem state.This added randomness poses a much larger problem to smaller, complex evaluation sets.</p>
<p>Impact of Data Contamination and Static Benchmarks</p>
<p>Some researchers evaluate their agent implementations on the typical LLM benchmarks.Emerging research indicates that there is significant data contamination in the model's training data, supported by the observation that a model's performance significantly worsens when benchmark questions are modified [8,38,37].This raises doubts on the authenticity of benchmark scores for both the language models and language model powered agents.</p>
<p>Furthermore, researchers have found that "As LLMs progress at a rapid pace, existing datasets usually fail to match the models' ever-evolving capabilities, because the complexity level of existing benchmarks is usually static and fixed" [37].To address this, work has been done to create dynamic benchmarks that are resistant to simple memorization [38,37].Researchers have also explored the idea of generating an entirely synthetic benchmark based on a user's specific environment or use case [14,27].While these techniques can help with contamination, decreasing the level of human involvement can pose additional risks regarding correctness and the ability to solve problems.</p>
<p>Benchmark Scope and Transferability</p>
<p>Many language model benchmarks are designed to be solved in a single iteration, with no tool calls, such as MMLU or GSM8K [3,10].While these are important for measuring the abilities of base language models, they are not good proxies for agent capabilities because they do not account for agent systems' ability to reason over multiple steps or access outside information.StrategyQA improves upon this by assessing models' reasoning abilities over multiple steps, but the answers are limited to Yes/No responses [7].As the industry continues to pivot towards agent focused use-cases additional measures will be needed to better assess the performance and generalizability of agents to tasks involving tools that extend beyond their training data.</p>
<p>Some agent specific benchmarks like AgentBench evaluate language model-based agents in a variety of different environments such as web browsing, command-line interfaces, and video games [17].This provides a better indication for how well agents can generalize to new environments, by reasoning, planning, and calling tools to achieve a given task.Benchmarks like AgentBench and SmartPlay introduce objective evaluation metrics designed to evaluate the implementation's success rate, output similarity to human responses, and overall efficiency [17,30].While these objective metrics are important to understanding the overall reliability and accuracy of the implementation, it is also important to consider more nuanced or subjective measures of performance.Metrics such as efficiency of tool use, reliability, and robustness of planning are nearly as important as success rate but are much more difficult to measure.Many of these metrics require evaluation by a human expert, which can be costly and time consuming compared to LLM-as-judge evaluations.</p>
<p>Real-world Applicability</p>
<p>Many of the existing benchmarks focus on the ability of Agent systems to reason over logic puzzles or video games [17].While evaluating performance on these types of tasks can help get a sense of the reasoning capabilities of agent systems, it is unclear whether performance on these benchmarks translates to real-world performance.Specifically, real-world data can be noisy and cover a much wider breadth of topics that many common benchmarks lack.</p>
<p>One popular benchmark that uses real-world data is WildBench, which is sourced from the WildChat dataset of 570,000 real conversations with ChatGPT [35].Because of this, it covers a huge breadth of tasks and prompts.While WildBench covers a wide range of topics, most other real-world benchmarks focus on a specific task.For example, SWE-bench is a benchmark that uses a set of real-world issues raised on GitHub for software engineering tasks in Python [13].This can be very helpful when evaluating agents designed to write Python code and provides a sense for how well agents can reason about code related problems; however, it is less informative when trying to understand agent capabilities involving other programming languages.</p>
<p>Bias and Fairness in Agent Systems</p>
<p>Language Models have been known to exhibit bias both in terms of evaluation as well as in social or fairness terms [5].Moreover, agents have specifically been shown to be "less robust, prone to more harmful behaviors, and capable of generating stealthier content than LLMs, highlighting significant safety challenges" [25].Other research has found "a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives" [24].This tendency can lead to faulty reasoning in any agent-based implementation.</p>
<p>As the complexity of tasks and agent involvement increases, more research is needed to identify and address biases within these systems.This poses a very large challenge to researchers, since scalable and novel benchmarks often involve some level of LLM involvement during creation.However, a truly robust benchmark for evaluating bias in LLM-based agents must include human evaluation.</p>
<p>Conclusion and Future Directions</p>
<p>The AI agent implementations explored in this survey demonstrate the rapid enhancement in language model powered reasoning, planning, and tool calling.Single and multi-agent patterns both show the ability to tackle complex multi-step problems that require advanced problem-solving skills.The key insights discussed in this paper suggest that the best agent architecture varies based on use case.Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning-execution -evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering.Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.</p>
<p>While the current state of AI-driven agents is promising, there are notable limitations and areas for future improvement.Challenges around comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases will need to be addressed in the near-term to enable reliable agents.By examining the progression from static language models to more dynamic, autonomous agents, this survey aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures.</p>
<p>Figure 1 :
1
Figure 1: A visualization of single and multi-agent architectures with their underlying features and abilities</p>
<p>Figure 2 :
2
Figure 2: An example of the ReAct method compared to other methods[32]</p>
<p>Figure 4 :
4
Figure 4: A diagram of the AutoGPT+P method [1]</p>
<p>Figure 6 :
6
Figure 6: A diagram of the AgentVerse method [2]</p>
<p>AutoGPT+P: Affordance-based Task Planning with Large Language Models. Timo Birr, arXiv:2402.10778[cs]version:1Feb. 2024</p>
<p>Weize Chen, arXiv:2308.10848Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. </p>
<p>Karl Cobbe, arXiv:2110.14168Training Verifiers to Solve Math Word Problems. </p>
<p>Large Language Model-based Human-Agent Collaboration for Complex Task Solving. Xueyang Feng, arXiv:2402.12914[cs.CL]2024</p>
<p>Bias and Fairness in Large Language Models: A Survey. Isabel O Gallegos, arXiv:2309.00770</p>
<p>Efficient Tool Use with Chain-of-Abstraction Reasoning. Silin Gao, arXiv:2401.17464Feb. 2024</p>
<p>Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Mor Geva, arXiv:2101.02235</p>
<p>Shahriar Golchin, Mihai Surdeanu, arXiv:2308.08493[cs]version:3Time Travel in LLMs: Tracing Data Contamination in Large Language Models. Feb. 2024</p>
<p>Embodied LLM Agents Learn to Cooperate in Organized Teams. Xudong Guo, arXiv:2403.12482[cs.AI]2024</p>
<p>Dan Hendrycks, arXiv:2009.03300Measuring Massive Multitask Language Understanding. </p>
<p>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. Sirui Hong, arXiv:2308.00352[cs.AI]2023</p>
<p>Understanding the planning of LLM agents: A survey. Xu Huang, arXiv:2402.02716[cs.AI]2024</p>
<p>SWE-bench: Can Language Models Resolve Real-World GitHub Issues?. Carlos E Jimenez, arXiv:2310.06770</p>
<p>S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models. Fangyu Lei, arXiv:2310.15147</p>
<p>Graph-enhanced Large Language Models in Asynchronous Plan Reasoning. Fangru Lin, arXiv:2402.02805</p>
<p>From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. Na Liu, arXiv:2401.02777</p>
<p>Xiao Liu, arXiv:2308.03688Evaluating LLMs as Agents. </p>
<p>Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. Zijun Liu, arXiv:2310.02170[cs.CL]2023</p>
<p>. Yohei, Nakajima. yoheinakajima/babyagi. original-date: 2023-04-03T00:40:27ZApr. 2024</p>
<p>AI Deception: A Survey of Examples, Risks, and Potential Solutions. Peter S Park, arXiv:2308.14752</p>
<p>Personality Traits in Large Language Models. Greg Serapio-Garca, arXiv:2307.00184[cs.CL]2023</p>
<p>Zhengliang Shi, arXiv:2403.03031Learning to Use Tools via Cooperative and Interactive Agents. </p>
<p>Reflexion: Language Agents with Verbal Reinforcement Learning. Noah Shinn, arXiv:2303.11366</p>
<p>Amir Taubenfeld, arXiv:2402.04049Systematic Biases in LLM Simulations of Debates. Feb. 2024</p>
<p>Evil Geniuses: Delving into the Safety of LLM-based Agents. Yu Tian, arXiv:2311.11855Feb. 2024</p>
<p>Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?. Qineng Wang, arXiv:2402.18272Feb. 2024</p>
<p>Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation. Siyuan Wang, arXiv:2402.11443Feb. 2024</p>
<p>Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. Zhenhailong Wang, arXiv:2307.05300[cs.AI]2024</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, arXiv:2201.11903</p>
<p>Yue Wu, arXiv:2310.01557A Benchmark for LLMs as Intelligent Agents. </p>
<p>The Rise and Potential of Large Language Model Based Agents: A Survey. Zhiheng Xi, arXiv:2309.07864[cs.AI]2023</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. Shunyu Yao, arXiv:2210.03629</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Shunyu Yao, arXiv:2305.10601</p>
<p>How Language Model Hallucinations Can Snowball. Muru Zhang, arXiv:2305.13534May 2023</p>
<p>(InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild. Wenting Zhao, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. Andy Zhou, arXiv:2310.04406</p>
<p>DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents. Kaijie Zhu, arXiv:2402.14865Feb. 2024</p>
<p>DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks. Kaijie Zhu, arXiv:2309.17167</p>            </div>
        </div>

    </div>
</body>
</html>
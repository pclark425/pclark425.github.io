<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3530 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3530</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3530</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-c585207e32bc3de19cab4e9f8f6e7d9805861dc9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c585207e32bc3de19cab4e9f8f6e7d9805861dc9" target="_blank">Neural Program Induction for KBQA Without Gold Programs or Query Annotations</a></p>
                <p><strong>Paper Venue:</strong> International Joint Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> A noise-resilient NPI model, Stable Sparse Reward based Programmer (SSRP) that evades noise-induced instability through continual retrospection and its comparison with current learning behavior is proposed.</p>
                <p><strong>Paper Abstract:</strong> Neural Program Induction (NPI) is a paradigm for decomposing high-level tasks such as complex question-answering over knowledge bases (KBQA) into executable programs by employing neural models. Typically, this involves two key phases: i) inferring input program variables from the high-level task description, and ii) generating the correct program sequence involving these variables. Here we focus on NPI for Complex KBQA with only the final answer as supervision, and not gold programs. This raises major challenges; namely, i) noisy query annotation in the absence of any supervision can lead to catastrophic forgetting while learning, ii) reward becomes extremely sparse owing to the noise. To deal with these, we propose a noise-resilient NPI model, Stable Sparse Reward based Programmer (SSRP) that evades noise-induced instability through continual retrospection and its comparison with current learning behavior. On complex KBQA datasets, SSRP performs at par with hand-crafted rule-based models when provided with gold program input, and in the noisy settings outperforms state-of-the-art models by a significant margin even with a noisier query annotator.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3530.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3530.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SSRP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stable Sparse Reward based Programmer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A noise-resilient neural program induction (NPI) model that learns to generate executable programs for complex KBQA using only final-answer supervision; it wraps a Sparse Reward based Programmer with a Reference Programmer snapshot to stabilize learning under noisy input and extremely sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SSRP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent NPI that encodes the query with a GRU, maintains a dynamic variable memory and operator prototype matrices, samples operators and feasible argument-variable instantiations, executes generated programs on a KB and learns from discrete rewards. SSRP augments SRP with a Reference Programmer snapshot to retrospectively scale gradient updates based on program distance (Jaccard on operator sequences) and reward differences, thereby reducing catastrophic forgetting under noisy input.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Complex KBQA (WebQuestionsSP, CQA-12K subset of CSQA)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-hop knowledge-base question answering requiring strict logical reasoning over KB relations, types and entities (inference chains up to 2 hops on WebQuestionsSP and up to 3–5 steps for complex/logical/quantitative questions in CQA-12K), often with additional constraints and quantitative counting.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Noise-resilient wrapper around a REINFORCE-trained NPI: (i) Reference Programmer (periodic snapshot) for retrospection, (ii) compute program-distance (Jaccard on operator sequences) and reward differences to produce confidence weights c_i that scale gradients, (iii) other techniques: beam search, feasibility checks constrained by KB, two-phase program decomposition, auxiliary rewards for desired output types, penalties for wrong termination/generate-short programs, entropy/dropout regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>WebQuestionsSP (noisy input): overall Test F1 = 72.61%. Per-category (selected rows in paper Table 1): e.g. some categories show lower/higher; on CQA-12K noisy setting (Answerable subset) SSRP F1: Simple 76.38%, Logical 48.13%, QCount 47.01%, Combined 44.7%. On full CQA-12K (Full columns) SSRP F1: Simple 14.54%, Logical 2.98%, QCount 7.03%, Combined 4.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared baselines in paper on noisy CQA-12K: SRP (no SSRP wrapper) Answerable: Simple 54.76%, Logical 33.13%, QCount 51.25%, Combined 33.6%; A2C Answerable: Simple 82.5%, Logical 52.4%, QCount 35.4%, Combined 31.7%; NSM Answerable: Simple 31.49%, Logical 7.35%, QCount 2.76%, Combined 19.35%. On WebQuestionsSP, SRP (with gold input) matched a rule-based system (SRP 82.85% vs rule-based 82.59%), while SSRP on noisy input reached 72.61%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Against SRP (same architecture without Reference Programmer) SSRP improves substantially in noisy settings (e.g., Combined Answerable: SRP 33.6% -> SSRP 44.7% on CQA-12K Answerable). Against NSM on noisy CQA-12K, SSRP shows large gains (e.g., Logical: NSM 7.35% -> SSRP 48.13% on Answerable). Compared to A2C, SSRP underperforms on some simpler categories but outperforms on quantitative-count/generalization (e.g., QCount Answerable: A2C 35.4% vs SSRP 47.01%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Learning is vulnerable to extreme reward sparsity and noisy ERT linking: with noisy inputs only ~10–20% of train/test questions are exactly answerable, limiting achievable performance. Even with SSRP, performance on the full (noisy) CQA-12K set is low (single-digit F1s) because most instances are unanswerable given noisy candidates. SRP without SSRP suffers unlearning in noisy regimes; A2C is better on simple/logical classes but worse on quantitative/generalization. The approach depends on beam search and feasibility filters and still requires effective ERT candidate generation; poor linking quality is a key failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper ablates and analyzes: (i) SRP trained with gold ERT vs SRP/SSRP with noisy ERT — SRP with gold input substantially outperforms NSM (gold-input SRP: Simple 96.52% vs NSM 78.38%; Logical 87.72% vs 35.40%; QCount 51.33% vs 12.38%), showing architecture capacity; (ii) SRP vs SSRP demonstrates the Reference Programmer stabilizes training under noise (SSRP > SRP in noisy CQA-12K combined and discussion shows SRP succumbs to unlearning whereas SSRP improves steadily); (iii) A2C (advantage actor-critic) baseline comparison shows standard RL stabilization helps on simpler categories but auxiliary rewards and SSRP's retrospective scaling help in extremely sparse/complex categories. Additional analyses: effect of auxiliary rewards, program decomposition, feasibility constraints, beam size K and sampling hyperparameters (n_p, n_v), and the caution hyperparameter alpha controlling conservativeness of SSRP updates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Program Induction for KBQA Without Gold Programs or Query Annotations', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3530.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3530.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SRP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Reward based Programmer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The core neural program induction model used in the paper that samples operators and feasible variable arguments, writes new variables to a dynamic memory, executes candidate programs on the KB and learns from sparse, discrete rewards using REINFORCE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SRP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GRU-based recurrent NPI with operator and variable-type prototype matrices, dynamic variable memory, feasibility checking against the KB, beam search for multiple candidate programs, and training via REINFORCE with auxiliary rewards and programmatic constraints to mitigate reward sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Complex KBQA (WebQuestionsSP, CQA-12K)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same multi-hop KBQA tasks as SSRP requiring strict logical chaining of operators over entities/relations/types, and quantitative counting in some categories.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Core method: REINFORCE-trained program induction with beam search, feasibility constraints from KB, auxiliary rewards (bias toward correct output variable types), penalties for wrong termination/short programs, entropy/dropout regularization, and two-phase program decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With gold ERT linking (WebQuestionsSP/CQA-12K gold input): SRP matches or exceeds strong baselines (WebQuestionsSP overall Test F1 82.85%, comparable to hand-crafted rule-based 82.59%). On CQA-12K with gold ERT linking (Table 3): Simple 96.52%, Logical 87.72%, QCount 51.33%. On noisy input (no SSRP wrapper), CQA-12K Answerable Combined F1 = 33.6% (SRP Answerable Combined), Full Combined = 3.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to NSM when given gold input, SRP much higher (e.g., Logical 87.72% SRP vs 35.40% NSM; Simple 96.52% vs 78.38% NSM). Under noisy input, SRP is outperformed by SSRP and A2C on some categories.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>SRP (with gold input) substantially outperforms NSM across categories, demonstrating better multi-step program exploration; but under noisy input SRP degrades and SSRP wrapper is necessary to regain robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>SRP trained with REINFORCE experiences instability and catastrophic forgetting under noisy ERT candidates, leading to unlearning in training; extreme reward sparsity and exploded program space with noisy inputs reduce positive reward signals severely.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Analysis contrasts SRP with SSRP (showing SSRP stability gains), with A2C baseline, and with NSM; also shows effect of gold vs noisy ERT linking on performance, highlighting that gold input vastly simplifies the search and raises performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Program Induction for KBQA Without Gold Programs or Query Annotations', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3530.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3530.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Symbolic Machines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that learns to translate natural language queries to executable logical forms over Freebase with only final-answer supervision using a neural programmer-interpreter-like architecture; used as the main baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NSM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A neural semantic parsing framework that decodes programs token-by-token into logical forms executable on a KB; trained with weak supervision (final-answer feedback) and a REINFORCE-style algorithm in prior work. In the context of this paper, NSM is used as a baseline (original NSM used a near-oracle entity linker in its experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>KBQA (WebQuestionsSP; weakly-supervised semantic parsing)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Translate questions into logical forms/programs executed on Freebase to answer multi-hop KB questions; requires compositional semantic parsing and constrained logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Token-by-token decoding of logical forms; weak supervision from answers; uses a strong entity linker in prior work to reduce input noise (in-house linker with reported ~94% entity linking accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in this paper as a baseline on noisy CQA-12K: Answerable F1 scores (Table 2) Simple 31.49%, Logical 7.35%, QCount 2.76%, Combined 19.35%; on full set: Combined 2.0% F1. With gold ERT linking (Table 3) NSM: Simple 78.38%, Logical 35.40%, QCount 12.38% (substantially lower than SRP with gold input).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>This paper's SRP (with gold input) and SSRP (with noisy input) substantially outperform NSM across most categories, especially for logical and quantitative questions (e.g., Logical with gold input SRP 87.72% vs NSM 35.40%; on noisy input SSRP Logical 48.13% vs NSM 7.35%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>NSM decodes token-by-token and cannot easily enforce high-level programmatic structure; it relies on high-quality entity linking (prior work used a near-oracle linker) and suffers greatly when ERT linking is noisy, leading to poor performance on complex/noisy KBQA in this paper's evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper discusses structural differences: NSM filters bad programs post-generation and thus wastes exploration; SRP/SSRP generate line-of-code atomic actions allowing programmatic constraints. Comparative experiments show NSM lags significantly when ERT linking is noisy or when questions require deeper logical/quantitative reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Program Induction for KBQA Without Gold Programs or Query Annotations', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3530.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3530.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A2C-baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Advantage Actor-Critic baseline (A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard actor-critic reinforcement learning baseline (advantage actor-critic) applied to the SRP architecture to reduce variance in policy gradient updates and stabilize learning under noisy rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On actor-critic algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>A2C (actor-critic applied to SRP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same SRP architecture trained using an Advantage Actor-Critic algorithm: an auxiliary network estimates the value/advantage function and gradients are scaled using the advantage to reduce variance relative to vanilla REINFORCE.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Complex KBQA (CQA-12K noisy evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above: multi-step KB reasoning, including simple, logical (multi-hop) and quantitative counting queries.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Use of actor-critic RL (A2C) instead of REINFORCE to stabilize learning; standard environment rewards used (no auxiliary rewards in A2C baseline per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On CQA-12K noisy Answerable subset (Table 2): Simple 82.5%, Logical 52.4%, QCount 35.4%, Combined 31.7%. On full set (Full columns) A2C: Simple 4.982%, Logical 3.3%, QCount 5.2%, Combined 2.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>A2C outperforms SRP and sometimes SSRP on simpler categories (e.g., Answerable Simple: A2C 82.5% > SSRP 76.38%; Logical: A2C 52.4% > SSRP 48.13%), indicating that actor-critic stabilization helps in less-sparse settings. However, SSRP and SRP outperform A2C on quantitative count and combined-generalization, showing A2C's limits under extremely sparse/complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>A2C performs worse than SSRP on quantitative counting and on generalization across multiple categories; when auxiliary rewards and programmatic constraints are important for exploration in very sparse reward regimes, pure A2C (without those auxiliary signals) underperforms.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper compares A2C to SRP and SSRP to evaluate whether standard RL variance-reduction (A2C) can substitute SSRP's retrospective reference mechanism. Empirically, A2C helps on simpler categories, but SSRP's auxiliary rewards and retrospection are more effective on complex/quantitative categories and combined modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Program Induction for KBQA Without Gold Programs or Query Annotations', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3530.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3530.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rule-based</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hand-crafted rule-based KBQA parser</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A manually derived semantic parser that converts queries into program-like inference rules (hand-crafted semantic parses) and executes them on the KB to obtain answers; used as a high-quality reference baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Rule-based semantic parser</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Human-engineered rules/semantic parses encoding inference chains and exact constraints necessary to answer queries; serves as a strong interpretability-focused baseline (not a learned neural model).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>WebQuestionsSP (and comparable KBQA tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Deterministic program-like rules capturing the correct inference chain/constraints to answer questions on Freebase; requires precise logical specification of relations and constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Manual semantic parsing and rule construction encoding correct inference chains and constraints for each question type.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On WebQuestionsSP (Table 1, gold-input comparisons) overall Test F1 = 82.59% (comparable to SRP with gold input 82.85%). Performance varies by category (e.g., high for simple single-hop no-constraint, lower for constrained categories).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>SRP (with gold input) matches or slightly exceeds the rule-based parser; however, rule-based system is not robust to noisy input and does not scale without manual effort.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires human annotation and hand-crafting; not scalable to arbitrary novel question types and KB coverage; inapplicable when gold ERT annotations are not available.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Used as a reference upper-baseline when SRP is given gold program input; demonstrates that learned SRP can reach hand-crafted performance when provided clean inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Program Induction for KBQA Without Gold Programs or Query Annotations', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural symbolic machines: Learning semantic parsers on freebase with weak supervision <em>(Rating: 2)</em></li>
                <li>On actor-critic algorithms <em>(Rating: 1)</em></li>
                <li>Neural programmer-interpreters <em>(Rating: 2)</em></li>
                <li>Leveraging grammar and reinforcement learning for neural program synthesis <em>(Rating: 2)</em></li>
                <li>Learning a natural language interface with neural programmer <em>(Rating: 2)</em></li>
                <li>Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph <em>(Rating: 2)</em></li>
                <li>Question answering on freebase via relation extraction and textual evidence <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3530",
    "paper_id": "paper-c585207e32bc3de19cab4e9f8f6e7d9805861dc9",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "SSRP",
            "name_full": "Stable Sparse Reward based Programmer",
            "brief_description": "A noise-resilient neural program induction (NPI) model that learns to generate executable programs for complex KBQA using only final-answer supervision; it wraps a Sparse Reward based Programmer with a Reference Programmer snapshot to stabilize learning under noisy input and extremely sparse rewards.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SSRP",
            "model_description": "Recurrent NPI that encodes the query with a GRU, maintains a dynamic variable memory and operator prototype matrices, samples operators and feasible argument-variable instantiations, executes generated programs on a KB and learns from discrete rewards. SSRP augments SRP with a Reference Programmer snapshot to retrospectively scale gradient updates based on program distance (Jaccard on operator sequences) and reward differences, thereby reducing catastrophic forgetting under noisy input.",
            "model_size": null,
            "reasoning_task_name": "Complex KBQA (WebQuestionsSP, CQA-12K subset of CSQA)",
            "reasoning_task_description": "Multi-hop knowledge-base question answering requiring strict logical reasoning over KB relations, types and entities (inference chains up to 2 hops on WebQuestionsSP and up to 3–5 steps for complex/logical/quantitative questions in CQA-12K), often with additional constraints and quantitative counting.",
            "method_or_intervention": "Noise-resilient wrapper around a REINFORCE-trained NPI: (i) Reference Programmer (periodic snapshot) for retrospection, (ii) compute program-distance (Jaccard on operator sequences) and reward differences to produce confidence weights c_i that scale gradients, (iii) other techniques: beam search, feasibility checks constrained by KB, two-phase program decomposition, auxiliary rewards for desired output types, penalties for wrong termination/generate-short programs, entropy/dropout regularization.",
            "performance": "WebQuestionsSP (noisy input): overall Test F1 = 72.61%. Per-category (selected rows in paper Table 1): e.g. some categories show lower/higher; on CQA-12K noisy setting (Answerable subset) SSRP F1: Simple 76.38%, Logical 48.13%, QCount 47.01%, Combined 44.7%. On full CQA-12K (Full columns) SSRP F1: Simple 14.54%, Logical 2.98%, QCount 7.03%, Combined 4.6%.",
            "baseline_performance": "Compared baselines in paper on noisy CQA-12K: SRP (no SSRP wrapper) Answerable: Simple 54.76%, Logical 33.13%, QCount 51.25%, Combined 33.6%; A2C Answerable: Simple 82.5%, Logical 52.4%, QCount 35.4%, Combined 31.7%; NSM Answerable: Simple 31.49%, Logical 7.35%, QCount 2.76%, Combined 19.35%. On WebQuestionsSP, SRP (with gold input) matched a rule-based system (SRP 82.85% vs rule-based 82.59%), while SSRP on noisy input reached 72.61%.",
            "improvement_over_baseline": "Against SRP (same architecture without Reference Programmer) SSRP improves substantially in noisy settings (e.g., Combined Answerable: SRP 33.6% -&gt; SSRP 44.7% on CQA-12K Answerable). Against NSM on noisy CQA-12K, SSRP shows large gains (e.g., Logical: NSM 7.35% -&gt; SSRP 48.13% on Answerable). Compared to A2C, SSRP underperforms on some simpler categories but outperforms on quantitative-count/generalization (e.g., QCount Answerable: A2C 35.4% vs SSRP 47.01%).",
            "limitations_or_failures": "Learning is vulnerable to extreme reward sparsity and noisy ERT linking: with noisy inputs only ~10–20% of train/test questions are exactly answerable, limiting achievable performance. Even with SSRP, performance on the full (noisy) CQA-12K set is low (single-digit F1s) because most instances are unanswerable given noisy candidates. SRP without SSRP suffers unlearning in noisy regimes; A2C is better on simple/logical classes but worse on quantitative/generalization. The approach depends on beam search and feasibility filters and still requires effective ERT candidate generation; poor linking quality is a key failure mode.",
            "ablation_or_analysis": "Paper ablates and analyzes: (i) SRP trained with gold ERT vs SRP/SSRP with noisy ERT — SRP with gold input substantially outperforms NSM (gold-input SRP: Simple 96.52% vs NSM 78.38%; Logical 87.72% vs 35.40%; QCount 51.33% vs 12.38%), showing architecture capacity; (ii) SRP vs SSRP demonstrates the Reference Programmer stabilizes training under noise (SSRP &gt; SRP in noisy CQA-12K combined and discussion shows SRP succumbs to unlearning whereas SSRP improves steadily); (iii) A2C (advantage actor-critic) baseline comparison shows standard RL stabilization helps on simpler categories but auxiliary rewards and SSRP's retrospective scaling help in extremely sparse/complex categories. Additional analyses: effect of auxiliary rewards, program decomposition, feasibility constraints, beam size K and sampling hyperparameters (n_p, n_v), and the caution hyperparameter alpha controlling conservativeness of SSRP updates.",
            "uuid": "e3530.0",
            "source_info": {
                "paper_title": "Neural Program Induction for KBQA Without Gold Programs or Query Annotations",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "SRP",
            "name_full": "Sparse Reward based Programmer",
            "brief_description": "The core neural program induction model used in the paper that samples operators and feasible variable arguments, writes new variables to a dynamic memory, executes candidate programs on the KB and learns from sparse, discrete rewards using REINFORCE.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SRP",
            "model_description": "GRU-based recurrent NPI with operator and variable-type prototype matrices, dynamic variable memory, feasibility checking against the KB, beam search for multiple candidate programs, and training via REINFORCE with auxiliary rewards and programmatic constraints to mitigate reward sparsity.",
            "model_size": null,
            "reasoning_task_name": "Complex KBQA (WebQuestionsSP, CQA-12K)",
            "reasoning_task_description": "Same multi-hop KBQA tasks as SSRP requiring strict logical chaining of operators over entities/relations/types, and quantitative counting in some categories.",
            "method_or_intervention": "Core method: REINFORCE-trained program induction with beam search, feasibility constraints from KB, auxiliary rewards (bias toward correct output variable types), penalties for wrong termination/short programs, entropy/dropout regularization, and two-phase program decomposition.",
            "performance": "With gold ERT linking (WebQuestionsSP/CQA-12K gold input): SRP matches or exceeds strong baselines (WebQuestionsSP overall Test F1 82.85%, comparable to hand-crafted rule-based 82.59%). On CQA-12K with gold ERT linking (Table 3): Simple 96.52%, Logical 87.72%, QCount 51.33%. On noisy input (no SSRP wrapper), CQA-12K Answerable Combined F1 = 33.6% (SRP Answerable Combined), Full Combined = 3.7%.",
            "baseline_performance": "Compared to NSM when given gold input, SRP much higher (e.g., Logical 87.72% SRP vs 35.40% NSM; Simple 96.52% vs 78.38% NSM). Under noisy input, SRP is outperformed by SSRP and A2C on some categories.",
            "improvement_over_baseline": "SRP (with gold input) substantially outperforms NSM across categories, demonstrating better multi-step program exploration; but under noisy input SRP degrades and SSRP wrapper is necessary to regain robustness.",
            "limitations_or_failures": "SRP trained with REINFORCE experiences instability and catastrophic forgetting under noisy ERT candidates, leading to unlearning in training; extreme reward sparsity and exploded program space with noisy inputs reduce positive reward signals severely.",
            "ablation_or_analysis": "Analysis contrasts SRP with SSRP (showing SSRP stability gains), with A2C baseline, and with NSM; also shows effect of gold vs noisy ERT linking on performance, highlighting that gold input vastly simplifies the search and raises performance.",
            "uuid": "e3530.1",
            "source_info": {
                "paper_title": "Neural Program Induction for KBQA Without Gold Programs or Query Annotations",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "NSM",
            "name_full": "Neural Symbolic Machines",
            "brief_description": "Prior work that learns to translate natural language queries to executable logical forms over Freebase with only final-answer supervision using a neural programmer-interpreter-like architecture; used as the main baseline in the paper.",
            "citation_title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision",
            "mention_or_use": "use",
            "model_name": "NSM",
            "model_description": "A neural semantic parsing framework that decodes programs token-by-token into logical forms executable on a KB; trained with weak supervision (final-answer feedback) and a REINFORCE-style algorithm in prior work. In the context of this paper, NSM is used as a baseline (original NSM used a near-oracle entity linker in its experiments).",
            "model_size": null,
            "reasoning_task_name": "KBQA (WebQuestionsSP; weakly-supervised semantic parsing)",
            "reasoning_task_description": "Translate questions into logical forms/programs executed on Freebase to answer multi-hop KB questions; requires compositional semantic parsing and constrained logical reasoning.",
            "method_or_intervention": "Token-by-token decoding of logical forms; weak supervision from answers; uses a strong entity linker in prior work to reduce input noise (in-house linker with reported ~94% entity linking accuracy).",
            "performance": "Reported in this paper as a baseline on noisy CQA-12K: Answerable F1 scores (Table 2) Simple 31.49%, Logical 7.35%, QCount 2.76%, Combined 19.35%; on full set: Combined 2.0% F1. With gold ERT linking (Table 3) NSM: Simple 78.38%, Logical 35.40%, QCount 12.38% (substantially lower than SRP with gold input).",
            "baseline_performance": null,
            "improvement_over_baseline": "This paper's SRP (with gold input) and SSRP (with noisy input) substantially outperform NSM across most categories, especially for logical and quantitative questions (e.g., Logical with gold input SRP 87.72% vs NSM 35.40%; on noisy input SSRP Logical 48.13% vs NSM 7.35%).",
            "limitations_or_failures": "NSM decodes token-by-token and cannot easily enforce high-level programmatic structure; it relies on high-quality entity linking (prior work used a near-oracle linker) and suffers greatly when ERT linking is noisy, leading to poor performance on complex/noisy KBQA in this paper's evaluations.",
            "ablation_or_analysis": "Paper discusses structural differences: NSM filters bad programs post-generation and thus wastes exploration; SRP/SSRP generate line-of-code atomic actions allowing programmatic constraints. Comparative experiments show NSM lags significantly when ERT linking is noisy or when questions require deeper logical/quantitative reasoning.",
            "uuid": "e3530.2",
            "source_info": {
                "paper_title": "Neural Program Induction for KBQA Without Gold Programs or Query Annotations",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "A2C-baseline",
            "name_full": "Advantage Actor-Critic baseline (A2C)",
            "brief_description": "A standard actor-critic reinforcement learning baseline (advantage actor-critic) applied to the SRP architecture to reduce variance in policy gradient updates and stabilize learning under noisy rewards.",
            "citation_title": "On actor-critic algorithms",
            "mention_or_use": "use",
            "model_name": "A2C (actor-critic applied to SRP)",
            "model_description": "Same SRP architecture trained using an Advantage Actor-Critic algorithm: an auxiliary network estimates the value/advantage function and gradients are scaled using the advantage to reduce variance relative to vanilla REINFORCE.",
            "model_size": null,
            "reasoning_task_name": "Complex KBQA (CQA-12K noisy evaluation)",
            "reasoning_task_description": "As above: multi-step KB reasoning, including simple, logical (multi-hop) and quantitative counting queries.",
            "method_or_intervention": "Use of actor-critic RL (A2C) instead of REINFORCE to stabilize learning; standard environment rewards used (no auxiliary rewards in A2C baseline per paper).",
            "performance": "On CQA-12K noisy Answerable subset (Table 2): Simple 82.5%, Logical 52.4%, QCount 35.4%, Combined 31.7%. On full set (Full columns) A2C: Simple 4.982%, Logical 3.3%, QCount 5.2%, Combined 2.4%.",
            "baseline_performance": null,
            "improvement_over_baseline": "A2C outperforms SRP and sometimes SSRP on simpler categories (e.g., Answerable Simple: A2C 82.5% &gt; SSRP 76.38%; Logical: A2C 52.4% &gt; SSRP 48.13%), indicating that actor-critic stabilization helps in less-sparse settings. However, SSRP and SRP outperform A2C on quantitative count and combined-generalization, showing A2C's limits under extremely sparse/complex tasks.",
            "limitations_or_failures": "A2C performs worse than SSRP on quantitative counting and on generalization across multiple categories; when auxiliary rewards and programmatic constraints are important for exploration in very sparse reward regimes, pure A2C (without those auxiliary signals) underperforms.",
            "ablation_or_analysis": "Paper compares A2C to SRP and SSRP to evaluate whether standard RL variance-reduction (A2C) can substitute SSRP's retrospective reference mechanism. Empirically, A2C helps on simpler categories, but SSRP's auxiliary rewards and retrospection are more effective on complex/quantitative categories and combined modeling.",
            "uuid": "e3530.3",
            "source_info": {
                "paper_title": "Neural Program Induction for KBQA Without Gold Programs or Query Annotations",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Rule-based",
            "name_full": "Hand-crafted rule-based KBQA parser",
            "brief_description": "A manually derived semantic parser that converts queries into program-like inference rules (hand-crafted semantic parses) and executes them on the KB to obtain answers; used as a high-quality reference baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Rule-based semantic parser",
            "model_description": "Human-engineered rules/semantic parses encoding inference chains and exact constraints necessary to answer queries; serves as a strong interpretability-focused baseline (not a learned neural model).",
            "model_size": null,
            "reasoning_task_name": "WebQuestionsSP (and comparable KBQA tasks)",
            "reasoning_task_description": "Deterministic program-like rules capturing the correct inference chain/constraints to answer questions on Freebase; requires precise logical specification of relations and constraints.",
            "method_or_intervention": "Manual semantic parsing and rule construction encoding correct inference chains and constraints for each question type.",
            "performance": "On WebQuestionsSP (Table 1, gold-input comparisons) overall Test F1 = 82.59% (comparable to SRP with gold input 82.85%). Performance varies by category (e.g., high for simple single-hop no-constraint, lower for constrained categories).",
            "baseline_performance": null,
            "improvement_over_baseline": "SRP (with gold input) matches or slightly exceeds the rule-based parser; however, rule-based system is not robust to noisy input and does not scale without manual effort.",
            "limitations_or_failures": "Requires human annotation and hand-crafting; not scalable to arbitrary novel question types and KB coverage; inapplicable when gold ERT annotations are not available.",
            "ablation_or_analysis": "Used as a reference upper-baseline when SRP is given gold program input; demonstrates that learned SRP can reach hand-crafted performance when provided clean inputs.",
            "uuid": "e3530.4",
            "source_info": {
                "paper_title": "Neural Program Induction for KBQA Without Gold Programs or Query Annotations",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision",
            "rating": 2
        },
        {
            "paper_title": "On actor-critic algorithms",
            "rating": 1
        },
        {
            "paper_title": "Neural programmer-interpreters",
            "rating": 2
        },
        {
            "paper_title": "Leveraging grammar and reinforcement learning for neural program synthesis",
            "rating": 2
        },
        {
            "paper_title": "Learning a natural language interface with neural programmer",
            "rating": 2
        },
        {
            "paper_title": "Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph",
            "rating": 2
        },
        {
            "paper_title": "Question answering on freebase via relation extraction and textual evidence",
            "rating": 1
        }
    ],
    "cost": 0.01426925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Neural Program Induction for KBQA Without Gold Programs or Query Annotations</h1>
<p>Ghulam Ahmed Ansari ${ }^{<em> 1}$, Amrita Saha ${ }^{</em> 1}$, Vishwajeet Kumar ${ }^{2}$, Mohan Bhambhani ${ }^{1}$, Karthik Sankaranarayanan ${ }^{1}$ and Soumen Chakrabarti ${ }^{2}$<br>${ }^{1}$ IBM Research, India<br>${ }^{2}$ Indian Institute of Technology Bombay, India<br>{ansarigh, amrsaha4, mbhamb09, kartsank}@in.ibm.com<br>{vishwajeetkumar86, soumen.chakrabarti} @ gmail.com</p>
<h4>Abstract</h4>
<p>Neural Program Induction (NPI) is a paradigm for decomposing high-level tasks such as complex question-answering over knowledge bases (KBQA) into executable programs by employing neural models. Typically, this involves two key phases: i) inferring input program variables from the highlevel task description, and ii) generating the correct program sequence involving these variables. Here we focus on NPI for Complex KBQA with only the final answer as supervision, and not gold programs. This raises major challenges; namely i) noisy query annotation in the absence of any supervision can lead to catastrophic forgetting while learning, ii) reward becomes extremely sparse owing to the noise. To deal with these, we propose a noise-resilient NPI model, Stable Sparse Reward based Programmer (SSRP) that evades noise-induced instability through continual retrospection and its comparison with current learning behavior. On complex KBQA datasets, SSRP performs at par with hand-crafted rule-based models when provided with gold program input, and in the noisy settings outperforms state-of-the-art models by a significant margin even with a noisier query annotator.</p>
<h2>1 Introduction</h2>
<p>Recently, the neural program induction (NPI) paradigm has gained significant interest for approaching complex tasks through programmatic decomposition, i.e., generating a sequence of atomic operations on program variables, which upon execution yield the answer. This provides a practical approach to solving complex tasks while providing better interpretability than one-shot inferences using deep networks. Consequently, NPI techniques have been employed for variety of tasks such as Addition, Sorting [Reed and De Freitas, 2015], GridWorld navigation [Bunel et al., 2018], Tabular QA [Neelakantan et al., 2016; He et al., 2018], Math word problems [Bosnjak et al., 2017] or KBQA [Liang et al., 2017]. However, two critical assumptions have been commonly made by them to keep the problem tractable:</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>1) Either the input program variables to the NPI process is known or trivial to infer, or the inference is outsourced to other pretrained models. For example, in learning addition or sorting, or for navigating the GridWorld, the input program variables are directly provided. For tabular QA, the question annotation is still manageable as the query-words need to be linked to table cell and column values of a quite small (typically $10 \times 5$ sized) table.
2) Either gold programs or program skeletons are used to train the NPI model, or training is constrained to relatively simple programs. But realistically, for various complex applications, these assumptions may not hold, or may be too expensive to ensure. E.g., complex KBQA requires as input to the NPI, an annotation of the query with KB entities (E), relations (R) and types (T), a.k.a. ERT linking using huge KBs containing millions of entities. This annotation is particularly non-trivial and noisy in an unsupervised setting. Further, the complex nature of the questions require generation of long, multi-line programs, for which providing gold program supervision is expensive. Neural Symbolic Machine (NSM) by [Liang et al., 2017] is the only complex KBQA system that can be trained with only final answer as supervision and noisy query annotation as input. But it is not a true NPI model, as it decodes the program token-by-token and is unable to incorporate highlevel structure and programmatic styles when decoding.</p>
<p>The primary contribution of our work is a noise-resilient NPI model that is distant-supervised by the final answer alone. It can handle noise in the program input so extreme, that only for $10-20 \%$ of the training or test questions the exact gold answer can be reached. We next elaborate on the steep challenges faced in this problem setting.</p>
<h2>Effect of Noise on Program Space</h2>
<p>The accuracy of the initial step of inferring program variables has severe repercussions on the program induction process. Even with the availability of gold input-variables, the search space of programs blows up to an exponential size. In the noisy data setting, the problem becomes compounded, since instead of the gold input program variables there are multiple possible candidates for each of the program input. E.g., to generate a 3 -line program in the noisy setting with 7 operators and 5 possible candidate values for each input program variable, the program space explodes to $6.7 \times 10^{8}$ programs.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Complex KBQA by identifying the input program variables through joint ERT linking, then learning to induce programs from it, which on execution leads to the answer with associated reward. (Note that the <em>Gold Program</em> is not available during training or test)</p>
<h3>Effect of Noise on Reward Sparsity</h3>
<p>NPI models typically learn through reinforce-style updates [Williams, 1992], owing to the discrete nature of the feedback (or rewards) obtained by evaluating the induced programs. But inducing programs without gold programs for training suffers from an extremely sparse reward space, i.e., only a negligible number of programs in the entire exponential program space can yield rewards. Out of the enormous number of possible programs, only one or very few having a correct operator sequence invoked on the correct set of input and intermediate program variables get any reward. This renders the NPI problem extremely hard in the noisy setting and also requires the query annotation to be highly accurate.</p>
<h3>Effect of Extreme Reward Sparsity on Learning</h3>
<p>This nature of extreme sparsity in the reward space poses learning challenges by making the model unstable and prone to local optima problems. Further, the noise in the input has a more debilitating effect, by increasing the variance in the system. It can sometimes even fatally cripple the learner by confounding the model as it can receive counter-intuitive rewards even upon exploring the correct operator sequence.</p>
<p>Figure 1 illustrates the two phases of the complex KBQA task for a question <em>Which academy award was Henry Fonda chosen for but did not win?</em> which requires multi-step inference using logical reasoning over the KB. The first step of ERT linking requires a joint inference to resolve <em>Henry Fonda</em> as the American actor and identify phrases like <em>academy award</em> to be a potential KB-type or words like <em>win</em> or <em>chosen</em> to be linked to KB-relations <em>nominated</em> or <em>awarded</em> or <em>casted</em>. However, this joint reasoning, based on semantic similarity and KB connectivity, may not be always sufficient to make the correct inference. The situation escalates in the absence of any gold ERT linking supervision. E.g., in Figure 1, <em>chosen</em> has been linked to both <em>casted:P161</em> and <em>nominated:P1411</em> as potential candidates and <em>Academy Award</em> to a radio series and the film award. The output of the first phase is the input program variables memory consisting of <em>all</em> potential ERT candidates, which is fed into the NPI model, along with the gold answer. The NPI model then explores different programs that are logically consistent and compliant with the task description (i.e. KB, query, answer-type) and obtains a positive reward if it is able to reach the correct answer on execution. For e.g., the true program for the query in Figure 1 is the 2nd one and not the 1st, despite having the same operator sequence.</p>
<h3>Complex KBQA Datasets</h3>
<p>1) <em>WebQuestionsSP</em> [Yih <em>et al.</em>, 2016] provided a complex KBQA dataset having around 5K questions answerable from Freebase. While the natural language form of the questions look simple, for e.g. <em>Which team does David Beckham play for?</em>, it often requires up to 2-hop inference chains, sometimes with additional constraints.</p>
<p>2) <em>Complex Sequential QA (CSQA)</em> [Saha <em>et al.</em>, 2018] provided a dataset with 1.5M complex conversational QA pairs, requiring logical and quantitative reasoning over WikiData. For simplicity, we use the publicly available subset CQA-12K (comparable in size to WebQuestionsSP) consisting of 12K QA pairs from each of the seven question categories, where the questions do not depend on the previous context. We use the smaller dataset because the initial step of unsupervised query annotation is very time-consuming. The diverse categories of complex questions and the massive scale KB makes CQA-12K particularly suited to study Complex KBQA.</p>
<h1>2 Related Work</h1>
<h3>2.1 Joint Entity, Relation, Type (ERT) Linking</h3>
<p>While entity, relation and type linking has been around for decades, either the task is focused on (i) linking corpus mentions to KB, leveraging distant supervision signals from large scale corpus, for e.g. [Wu <em>et al.</em>, 2018; Wang <em>et al.</em>, 2018; Titov and Le, 2018; Radhakrishnan <em>et al.</em>, 2018] or (ii) linking short query text to KB artifacts by using gold linking data which is often expensive to obtain [Dubey <em>et al.</em>, 2018; Yu <em>et al.</em>, 2017; Yih <em>et al.</em>, 2015; Sorokin and Gurevych, 2017]. We are, however, interested in an unsupervised joint ERT linking on a query without leveraging any corpus.</p>
<h3>2.2 Neural Program Induction</h3>
<p>First generation semantic parsing systems, e.g., [Yih <em>et al.</em>, 2016], for modularizing complex tasks like complex KBQA using hand-crafted rules, have been popular for decades. Of the more recent neural methods of task decomposition into a program, the most relevant is Neural Symbolic Machines (NSM) [Liang <em>et al.</em>, 2017]. They learn to translate the query to a program like logical form executable on the KB, with only answer as supervision. Their choice of KBQA task and answer-supervised program induction setting makes NSM the</p>
<p>closest comparable work to ours. The two main distinctive factors of our work with respect to NSM are: i) NSM was applied to simpler questions and has several limitations which makes generalization to more complex programs hard, and ii) the input program variable creation was outsourced to an in-house annotator [Iyyer et al., 2017] trained on gold entity, relation and type linking data and having near-oracle accuracy ( $94 \%$ on entity linking). This greatly alleviates the noise-related issues arising in the program induction itself.</p>
<p>In contrast, the core challenge in our work arises from the absence of gold ERT linking data and the downstream NPI model having to handle realistic levels of labeling noise shown by ERT linkers on complex questions. This level of noisy-input in complex program induction for QA on a large scale KB, has not been addressed before.</p>
<h2>3 Model Overview</h2>
<p>Following are the main components of the model:</p>
<p>1) The ERT linker is an unsupervised query annotator that links the query spans with KB entities, relations and types and pre-populates them as input variables to the NPI model.
2) The programmer generates programs using the natural language query, KB, and pre-populated variables in memory as input. A program is a sequence of operators invoked with past instantiated variables as their arguments and by generating new variables which are written into memory.
3) The interpreter executes the generated program using the KB and outputs an answer, which upon comparison with the gold answer yields a reward. During training this reward is sent back to the programmer to update its parameters through a REINFORCE objective [Williams, 1992].</p>
<h2>4 Entity Relation Type (ERT) Linking</h2>
<p>Step 1: Independent E,R,T Linking generates separate candidate-lists for each of the KB entity, relation and type in the query. This step annotates all the longest possible query n-grams or shallow-parsed chunks that have a GloVe cosine score $&gt;0.3$ with any KB entity, relation or type.
Step 2: Joint E,R,T linking in an unsupervised setting ranks the extracted entities, relations and types using the sum of the following scores:</p>
<ul>
<li>Semantic or surface-match score from Step 1.</li>
<li>KB-connectivity score of a candidate entity, relation or type for a given query-span based on its hop-distance with candidate entities or relations or types associated with other spans or the entity-relation-type or type-relation-type connection with other relations or types.</li>
</ul>
<h2>5 Stable Sparse Reward Based Programmer</h2>
<p>In this section, we describe in detail the proposed NPI model which we call Stable Sparse Reward based Programmer (SSRP). The candidate entities, relations and KB-types obtained from the previous step and the query are provided as input program variables to SSRP which then induces the program. SSRP consists of the following two components: (i) the core program induction algorithm whose sole supervision is from rewards obtained by evaluating induced programs with respect to the final answer, which we call Sparse</p>
<p>Reward based Programmer (SRP) and is described in Section 5.1, and (ii) a generic noise-resilient wrapper over SRP which counteracts the noise encountered in the complex KBQA setting and stabilizes SSRP, which is described in Section 5.2.</p>
<h3>5.1 Sparse Reward Based Programmer</h3>
<p>We first describe the building blocks of SRP.</p>
<h2>Seven Variable-Types</h2>
<ul>
<li>KB artifacts: ent, rel, type</li>
<li>Base data types: int, bool, None (for padding)</li>
<li>Composite data types: set i.e. Set of KB entities</li>
</ul>
<h2>Seven Operators</h2>
<ul>
<li>gen_set(ent, rel, type) $\rightarrow$ set</li>
<li>verify(ent, rel, ent) $\rightarrow$ bool</li>
<li>set_{union/intersec/diff}(set, set) $\rightarrow$ set</li>
<li>set_count $($ set $) \rightarrow$ int</li>
<li>terminate()</li>
</ul>
<p>The core part of the NPI is a recurrent model which can access the following at each step: i) embedding matrices encoding a vocabulary of operators and variable types, ii) operator prototype matrices $M^{\text {op.org }}$ and $M^{\text {op.out }}$ for storing the argument and output variable type information respectively for each operator, and iii) variable memory matrix which is a query-specific scratch (dynamic) memory for storing new program variables as they get created. For each variable type it learns separate key and value embedding matrices respectively, for looking up a variable in memory and accessing its information. Additionally it also has a matrix to store the attention over the variables declared of each type.</p>
<h2>SRP Pipeline and Training</h2>
<p>The algorithm in Fig. 2 outlines the pseudocode of the SRP invoking the following steps. The figure on its left, illustrates the same process, and in the following description we refer to both.</p>
<ul>
<li>SRP encodes the input query into a fixed dimensional representation $q$ using a GRU network.</li>
<li>The other essential input to SRP is the scratch memory for storing program variables, which is initialized with the input variables ( KB entities, relations and types), when the programmer starts the induction process.</li>
<li>The action taken by SRP at time-step $t$ is markovially conditioned on the environment state $e_{t}$ and the hidden representation of the current program state $h_{t}$, each of which are encoded by recurrent networks initialized with the query encoding. Given the environment and program state, the NPI model then generates an output program which is a sequence of actions, each of which involves an operator invoked over previously defined variables in memory. This process creates a new variable, at each time-step which is added to the memory to be used later.</li>
<li>At each time-step, conditional to program state $h_{t}$, the model first samples a set of $n_{p}$ operators using OperatorSampler. For e.g. gen_set at first step in Fig 2</li>
<li>Then for each of the sampled operators, for e.g. $p$, SRP understands the feasibility of all possible variable instantiations for its $m$ arguments by invoking FeasibleVariable. Feasibility of a variable instantiation for a given operator $p$ is a boolean vector ( $V_{p}^{\text {feas }}$ in the pseu-</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Query Encoding: $q=G R U($ Query $)$
Initialization: $e_{1}, h_{1}=\operatorname{Linear}(q), A=[]$
for $t \in 1, \cdots, T$ do
$P_{t}=$ OperatorSampler $\left(h_{t}, n_{p}\right), \quad C={}$
for $p \in P_{t}$ do
$V_{p}^{\text {type }}=\left[v_{p, 1}^{\text {type }}, \cdots, v_{p, n}^{\text {type }}\right]=M^{o p, a r g}[p]$
$V_{p}^{\text {feas }}=$ FeasibleVariable $(p)$
$V_{p}=$ ArgVarSampler $\left(h_{t}, V_{p}^{\text {type }}, V_{p}^{\text {feas }}, n_{v}\right)$
for $V \in V_{p}$ do
$C=C \bigcup\left(p, V, V_{p}^{\text {type }}\right)$
$\left(p, V, V_{p}^{\text {type }}\right)=\arg \max (C)$
$u_{p}^{\text {key }}, u_{p}^{\text {val }}=\operatorname{OutVarGen}\left(p, V_{p}^{\text {type }}, V\right), u_{p}^{\text {type }}=M^{o p, a u t}[p]$
WriteVarToMem $\left(u_{p}^{\text {key }}, u_{p}^{\text {val }}, u_{p}^{\text {type }}\right)$
$e_{t+1}=G R U\left(e_{t}, u_{p}^{\text {val }}\right), h_{t+1}=G R U\left(e_{t}, u_{p}^{\text {val }}, h_{t}\right)$
$A . \operatorname{append}((p, V))$
Output: $A$
$\triangleright$ Generated action sequence</p>
<p>Figure 2: SRP model architecture and pseudocode: Starting with the query encoding, at each time-step the model samples an action based on the environment $e_{t}$ and program state $h_{t}$. Each action involves sampling the operator, and previously defined variables as its arguments and creating the output variable which is then added to the scratch memory. Figure outlines the pipeline with solid line for the t'th timestep.
decode), governed by the KB, to ensure that no variable is created that is inconsistent with the KB. For e.g. gen_set('Henry Fonda', 'nominated', 'Academy award') (where Academy award refers to the CBS series) is not valid as per the KB. Other programmatic paradigms like ensuring non-repetition of lines of code are also used to determine feasibility.</p>
<ul>
<li>For the operator $p$, ArgVarSampler then samples $n_{v}$ (hyperparameter) feasible variable instantiations by attending over all variables of the corresponding types stored in memory. The variable type of the arguments $V_{p}^{\text {type }}$ is obtained by looking up the operator prototype matrix $M^{o p, a r g}$. For e.g. in the fig. the sampled action is gen_set('Henry Fonda, 'nominated', 'Academy awards') at $t=1$ ( $n_{v}=1$ for simplicity). The pseudocode shows the sampled variable instantiations $V_{p}$.</li>
<li>After applying the sampled operator over the sampled variable instantiations, the OutVarGen module generates the key and value embedding of the new variable $u_{p}^{\text {key }}$ and $u_{p}^{\text {val }}$, which are functions of the embedding of the sampled operator $p$ and operand variables $V$.</li>
<li>The WriteVarToMem module writes the key and value embedding of the newly generated variable to the dynamic scratch memory and correspondingly updates the attention over the memory variables of that type.</li>
<li>The actions are sampled till either the terminate is sampled or for the maximum allowable timesteps, thus generating the program to be evaluated by the interpreter.</li>
</ul>
<p>In order to get feedback from multiple candidate programs using single training instance, SRP employs a beam search i.e. at each step, out of the total number of candidates generated as above, $K$ (typically 20) most-likely actions are sampled for the $K$ beams and the corresponding newly generated variables written into memory. Thus the algorithm progresses till $T$ steps to finally output $K$ candidate programs each of which feed the model back with some reward. To learn from the discrete rewards, the REINFORCE objective is used.</p>
<h2>Mitigating Sparsity in SRP</h2>
<p>The following steps make SRP's exploration of the program space more efficient and mitigate the sparsity problem</p>
<ul>
<li>Decomposing the program generation into two predetermined phases, first one involving only operations over input program variables, and second phase operating on the variables created by the first phase.</li>
<li>Generating semantically correct programs by (a) Incorporating programmatic paradigms like disallowing repeating or useless actions (e.g intersection of a set with itself) (b) Biasing the model towards generating answers of the desired variable type using auxiliary rewards.</li>
<li>Penalizing for terminating in the wrong variable type as answer or generating shorter programs.</li>
<li>Entropy and dropout based regularization.</li>
</ul>
<h3>5.2 Noise-stabilizing Wrapper of SSRP</h3>
<p>We now discuss the multiple challenges in the program induction process caused by the presence of noise in the input program variable. In the noise-free setting itself, the program space is exponential with the number of gold input program variables. The noisy input data blows up the program variable space furthermore, with the manifold combinations of the candidate inputs. Additionally, in the absence of gold programs, the noisy setting raises another serious issue of exacerbating the reward space sparsity, where, even with gold input data, only a handful of programs in the exponential space could have yielded a positive reward. The program explosion compounded with extreme reward sparsity can easily render the learning unstable, even more so in reinforce-style algorithms by increasing variance. For instance, by following the same good operator sequence the model will get a positive reward, if it had used the correct input variable candidates but no reward on selecting the wrong input data, which is bound to confound the model.</p>
<p>To counteract this phenomenon, we propose SSRP, having a noise-resiliency wrapper over SRP that introduces a con-</p>
<p>Algorithm 1 SSRP Algorithm
$\theta, \theta^{r e f}$ denotes parameters of the Current/Reference Programmer
for $n \in 1, \cdots, N$ do $\quad \triangleright$ loop over mini-batch $\bar{P}^{r e f}:\left[P_{0}^{r u r} \cdots P_{K}^{r u r}\right] \leftarrow$ CurrentProgrammer $(q, \theta)$
$\bar{P}^{r e f}:\left[P_{0}^{r e f} \cdots P_{K}^{r e f}\right] \leftarrow$ ReferenceProgrammer $\left(q, \theta^{r e f}\right)$
for $\forall i \in 1 \cdots K$ and $\forall j \in 1 \cdots K$ do
$c_{i j}=1$
$d_{i j} \leftarrow$ Distance $\left(P_{i}^{r u r}, P_{j}^{r e f}\right)$
$r_{i}^{r u r} \leftarrow$ Reward_Func $\left(P_{i}^{r u r}, q\right)$
$r_{j}^{r e f} \leftarrow$ Reward_Func $\left(P_{j}^{r e f}, q\right)$
if $r_{i}^{r u r} \leq 0$ then
if $r_{i}^{r u r}-r_{j}^{r e f}+\epsilon&gt;0$ then
$\delta_{i j} \leftarrow r_{i}^{r u r}-r_{j}^{r e f}$
else
$\delta_{i j} \leftarrow 0$
$c_{i j} \leftarrow e^{-\alpha d_{i j}} \delta_{i j} \quad \triangleright \alpha$ is a hyperparameter
$c_{i} \leftarrow \max <em i="i" j="j">{j}\left(c</em>\right)$
if $n \bmod$ Update_Freq $==0$ then
$\theta^{r e f} \leftarrow \theta$
Output: $\bar{c}:\left[c_{0} \cdots c_{i} \cdots c_{K}\right]$
cept of a Reference Programmer. The Reference Programmer is a snapshot (periodically refreshed) of an older version of the SRP model that is currently undergoing training, which we will call Current Programmer. In order to avoid unlearning, for a given training instance, the model retrospectively takes a decision about extent of backpropagation, based on two factors: i) distance between its current programs $P_{0}^{r u r} \cdots P_{K}^{r u r}$ ( $K$ being beam size) and the programs $P_{0}^{r e f} \cdots P_{K}^{r e f}$ seen by the Reference Programmer, and ii) difference between the rewards obtained by the reference and the current model. The intuition here being that when the current reward is non-positive but higher than the reference reward, the confidence in backpropagation $\left(\bar{c}=\left[c_{0} \cdots c_{K}\right]\right)$ increases proportional to the reward difference but exponentially decreases with the program distance. Here, jaccard distance between the operator sequence is used as program distance. $\alpha$ is a cautiousness hyperparameter, for which, higher the value, more conservative the model is in updating itself, when the current model's exploration diverges from its past behavior. Also, as the reward difference decreases, the need for the model to update itself diminishes, as it has not improved much beyond its old version. This sort of retrospective controlling of gradient, by multiplying with the confidence vector $\bar{c}$ helps stabilize the model when dealing with crippling noise in the data.</p>
<p>Comparison with Advantage Actor-Critic methods. We also empirically compare our SSRP ${ }^{1}$ with an SRP version trained with the more stable RL algorithms like Advantage Actor Critic(A2C)[Konda and Tsitsiklis, 2003] that can handle noise by reducing variability in the system. For our $A 2 C$ baseline, we use the standard rewards (and no auxiliary rewards, refer end of Sec 5.1) provided by the environment for optimization. In this version, an additional network in SRP</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup> estimates the advantage function used in the objective.</p>
<h2>6 Experiments</h2>
<p>In this section we describe separate experimental setup having gold or noisy program input to extensively evaluate the proposed model SSRP with NSM [Liang et al., 2017] the closest known baseline, as well as a hand-crafted rule-based model. We also compare SSRP with SRP to study the impact</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question Type</th>
<th style="text-align: left;">Rule- <br> Based</th>
<th style="text-align: left;">SRP</th>
<th style="text-align: left;">SSRP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Inference-chain-len=1, no constraint</td>
<td style="text-align: left;">89.03</td>
<td style="text-align: left;">89.09</td>
<td style="text-align: left;">83.81</td>
</tr>
<tr>
<td style="text-align: left;">Inference-chain-len=1 with constraint</td>
<td style="text-align: left;">46.52</td>
<td style="text-align: left;">79.94</td>
<td style="text-align: left;">48.12</td>
</tr>
<tr>
<td style="text-align: left;">Inference-chain-len=2, no constraint</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">88.69</td>
<td style="text-align: left;">77.41</td>
</tr>
<tr>
<td style="text-align: left;">Inference-chain-len=2, with <br> nontemporal constraint</td>
<td style="text-align: left;">71.31</td>
<td style="text-align: left;">63.07</td>
<td style="text-align: left;">40.65</td>
</tr>
<tr>
<td style="text-align: left;">Inference-chain-len $&lt;=2$, with <br> temporal constraint</td>
<td style="text-align: left;">83.83</td>
<td style="text-align: left;">48.86</td>
<td style="text-align: left;">57.42</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: left;">82.59</td>
<td style="text-align: left;">82.85</td>
<td style="text-align: left;">72.61</td>
</tr>
</tbody>
</table>
<p>Table 1: WebQuestionsSP Test F1 Scores(\%) of rule-based model and proposed SRP on gold program input and SSRP on noisy program input. Competing model NSM, on noisy program input is reported to have $69 \%$ on overall Test
of employing the noise-resiliency wrapper. We evaluate on two datasets: i) the popularly used WebQuestionsSP which requires upto 2-hop inferences over KB, sometimes with additional constraint satisfaction requirements, and ii) the recently introduced CSQA for complex KBQA. For experimental simplicity, we selected the publicly available subset CQA-12K of the original CSQA dataset, which contains 12 K QA pairs from each question category and is comparable in size to WebQuestionsSP. Out of the 12K QA pairs, 10K pairs are used for training and 1 K each for development and test. Since addressing the NPI in the absence of gold programs and on noisy input is a hard problem, we focus our evaluation on three query categories, requiring simple (single-step programs), logical (typically requiring 3 steps, for e.g. in Fig 1), and quantitative reasoning (requiring 4-5 steps - for e.g, How many rivers originate in China and flow through Tibet?).</p>
<h3>6.1 Results on WebQuestionsSP</h3>
<p>With gold input data. We first train a SRP model with the gold program input and compare this model's performance with a rule-based model based human annotated semantic parsed form of the query. The latter knows the inference chain of relations and the exact constraints that need to be additionally applied to reach the answer. This inference rule that was manually derived, can be written out in a program form, which on execution will give the final answer. Table 1 shows that SRP performs at-par with the manually crafted parser even without supervision of gold programs.
With noisy input. Here we follow the training setup of NSM to train our SSRP model and test it on the noisy program input. One notable difference here is that while NSM uses a proprietary in-house entity linker reported to have a $94 \%$ recall, we can only use the best possible publicly available linker, [Xu et al., 2016] having precision and recall respectively of $(96.7 \%, 86 \%)$ for entity linking and $(53.2 \%$,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Answerable</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Full</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Simple <br> (83)</td>
<td style="text-align: center;">Logical <br> (228)</td>
<td style="text-align: center;">QCount <br> (96)</td>
<td style="text-align: center;">Combined <br> (407)</td>
<td style="text-align: center;">Simple <br> (1000)</td>
<td style="text-align: center;">Logical <br> (1000)</td>
<td style="text-align: center;">QCount <br> (1000)</td>
<td style="text-align: center;">Combined <br> (3000)</td>
</tr>
<tr>
<td style="text-align: center;">SSRP</td>
<td style="text-align: center;">76.38</td>
<td style="text-align: center;">48.13</td>
<td style="text-align: center;">47.01</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">14.54</td>
<td style="text-align: center;">2.98</td>
<td style="text-align: center;">7.03</td>
<td style="text-align: center;">4.6</td>
</tr>
<tr>
<td style="text-align: center;">SRP</td>
<td style="text-align: center;">54.76</td>
<td style="text-align: center;">33.13</td>
<td style="text-align: center;">51.25</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">10.42</td>
<td style="text-align: center;">2.20</td>
<td style="text-align: center;">7.33</td>
<td style="text-align: center;">3.7</td>
</tr>
<tr>
<td style="text-align: center;">A2C</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">4.982</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">2.4</td>
</tr>
<tr>
<td style="text-align: center;">NSM</td>
<td style="text-align: center;">31.49</td>
<td style="text-align: center;">7.35</td>
<td style="text-align: center;">2.76</td>
<td style="text-align: center;">19.35</td>
<td style="text-align: center;">6.11</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">2.0</td>
</tr>
</tbody>
</table>
<p>Table 2: F1 score (in \%) of baseline NSM and A2C and the proposed models SRP and SSRP in the noisy setting. Boldfaced numbers indicate the best model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">SRP</th>
<th style="text-align: center;">NSM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Simple</td>
<td style="text-align: center;">96.52</td>
<td style="text-align: center;">78.38</td>
</tr>
<tr>
<td style="text-align: center;">Logical</td>
<td style="text-align: center;">87.72</td>
<td style="text-align: center;">35.40</td>
</tr>
<tr>
<td style="text-align: center;">QCount</td>
<td style="text-align: center;">51.33</td>
<td style="text-align: center;">12.38</td>
</tr>
</tbody>
</table>
<p>Table 3: F1-score (\%) of NSM and SRP using gold ERT linking data as program input.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ERT</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">E</td>
<td style="text-align: center;">$(19.79)$</td>
<td style="text-align: center;">$(17.40)$</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">$(15.47)$</td>
<td style="text-align: center;">$(16.44)$</td>
</tr>
<tr>
<td style="text-align: center;">T</td>
<td style="text-align: center;">$(16.61)$</td>
<td style="text-align: center;">$(24.51)$</td>
</tr>
</tbody>
</table>
<p>Table 3: F1-score (\%) of NSM and SRP using gold ERT linking data as program input.</p>
<h2>6.3 Discussion</h2>
<p>We make a few key observations on the results:</p>
<ul>
<li>On using gold input to the NPI, the significant margin of upto $2 \times$ improvement in F1 showcased by SRP over NSM indicate that the proposed method can indeed handle complex multi-step inferencing pragmatically and explore the search space more efficiently.</li>
<li>The significant margin of performance between SSRP and SRP evinces that reference network for selfassessment in SSRP considerably helps in alleviating catastrophic forgetting. SSRP's performance is remarkable, as the average noise level is high, with only 10 $20 \%$ of the train and test set being exactly answerable.</li>
<li>Table 2 shows that $A 2 C$ performs better than SSRP and SRP on the simpler classes (Simple and Logical). However, on the more complex category (Quantitative Count) or when generalizing to multiple categories with a single model, both SRP and SSRP beat A2C. This shows that the auxiliary rewards incorporated in SRP and SSRP facilitate better exploration in extremely sparse settings.</li>
<li>From the training trend in Fig 3, it is evident that while SRP succumbs to unlearning, in the noisy setting, the training performance of SSRP steadily improves.</li>
<li>Also, in contrast to NSM, both SSRP and SRP adapt fairly well to the exploded action space, because of the noisy input candidates, as can be seen in the scores obtained by the respective models. Only on the combined data, NSM performs somewhat better than SRP, presumably because it can afford to widen the exploration with double the beam-size owing to its model simplicity.</li>
<li>While NSM decodes token-by-token requiring multistep decoding for even simple queries, SRP and SSRP's atomic actions are each line-of-code. This altogether avoids generating logically inconsistent programs or incorporate high-level programmatic paradigms. Whereas, NSM only filters out "bad" programs post generatation, thus still wasting most explorations.</li>
</ul>
<h2>7 Conclusion</h2>
<p>In this work, we proposed a noise resilient NPI model SSRP that tackles complex KBQA significantly better than state-ofthe art models, with the two notable distinctions from other existing NPI models, i) it learns program induction in absence of supervision from gold programs, and ii) both during training and evaluation, it has to handle noise in the query annotation, so severe that it renders $80-90 \%$ of the questions unanswerable. In future, we plan to study more realistic applications like complex visual QA or algebraic problem solving.</p>
<h2>References</h2>
<p>[Bosnjak et al., 2017] Matko Bosnjak, Tim Rocktäschel, Jason Naradowsky, and Sebastian Riedel. Programming with a differentiable forth interpreter. In ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 547-556, 2017.
[Bunel et al., 2018] Rudy Bunel, Matthew J. Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging grammar and reinforcement learning for neural program synthesis. CoRR, abs/1805.04276, 2018.
[Dubey et al., 2018] Mohnish Dubey, Debayan Banerjee, Debanjan Chaudhuri, and Jens Lehmann. EARL: joint entity and relation linking for question answering over knowledge graphs. CoRR, abs/1801.03825, 2018.
[He et al., 2018] Xuanli He, Gholamreza Haffari, and Mohammad Norouzi. Sequence to sequence mixture model for diverse machine translation. In Proceedings of the 22nd Conference on Computational Natural Language Learning, CoNLL 2018, Brussels, Belgium, October 31 - November 1, 2018, pages 583-592, 2018.
[Iyyer et al., 2017] Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. Search-based neural structured learning for sequential question answering. In $A C L$, volume 1, 2017.
[Konda and Tsitsiklis, 2003] Vijay R. Konda and John N. Tsitsiklis. On actor-critic algorithms. SIAM J. Control Optim., 42(4):1143-1166, April 2003.
[Liang et al., 2017] Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus, and Ni Lao. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 23-33, 2017.
[Neelakantan et al., 2016] Arvind Neelakantan, Quoc V Le, Martin Abadi, Andrew McCallum, and Dario Amodei. Learning a natural language interface with neural programmer. arXiv preprint arXiv:1611.08945, 2016.
[Radhakrishnan et al., 2018] Priya Radhakrishnan, Partha Talukdar, and Vasudeva Varma. Elden: Improved entity linking using densified knowledge graphs. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 1844-1853, 2018.
[Reed and De Freitas, 2015] Scott Reed and Nando De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279, 2015.
[Saha et al., 2018] Amrita Saha, Vardaan Pahuja, Mitesh M. Khapra, Karthik Sankaranarayanan, and Sarath Chandar. Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph. In AAAI, 2018.
[Sorokin and Gurevych, 2017] Daniil Sorokin and Iryna Gurevych. Context-aware representations for knowledge base relation extraction. In Proceedings of EMNLP, 2017.
[Titov and Le, 2018] Ivan Titov and Phong Le. Improving entity linking by modeling latent relations between mentions. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, 2018.
[Wang et al., 2018] William Yang Wang, Weiran Xu, and Pengda Qin. Robust distant supervision relation extraction via deep reinforcement learning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2137-2147, 2018.
[Williams, 1992] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning, pages 5-32. Springer, 1992.
[Wu et al., 2018] Zeqiu Wu, Xiang Ren, Frank F. Xu, Ji Li, and Jiawei Han. Indirect supervision for relation extraction using question-answer pairs. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018, pages 646-654, 2018.
[Xu et al., 2016] Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang, and Dongyan Zhao. Question answering on freebase via relation extraction and textual evidence. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 712, 2016, Berlin, Germany, Volume 1: Long Papers, 2016.
[Yih et al., 2015] Wen-tau Yih, Ming-Wei Chang, Xiaodong He , and Jianfeng Gao. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 1321-1331, 2015.
[Yih et al., 2016] Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang, and Jina Suh. The value of semantic parse labeling for knowledge base question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 201-206, 2016.
[Yu et al., 2017] Mo Yu, Wenpeng Yin, Kazi Saidul Hasan, Cícero Nogueira dos Santos, Bing Xiang, and Bowen Zhou. Improved neural relation detection for knowledge base question answering. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, 2017.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ code and supplementary material available at https://github.com/CIPITR/SSRP&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
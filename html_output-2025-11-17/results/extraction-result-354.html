<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-354 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-354</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-354</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-271571227</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.21762v1.pdf" target="_blank">ReplanVLM: Replanning Robotic Tasks With Visual Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have gained increasing popularity in robotic task planning due to their exceptional abilities in text analytics and generation, as well as their broad knowledge of the world. However, they fall short in decoding visual cues. LLMs have limited direct perception of the world, which leads to a deficient grasp of the current state of the world. By contrast, the emergence of visual language models (VLMs) fills this gap by integrating visual perception modules, which can enhance the autonomy of robotic task planning. Despite these advancements, VLMs still face challenges, such as the potential for task execution errors, even when provided with accurate instructions. To address such issues, this letter proposes a ReplanVLM framework for robotic task planning. In this study, we focus on error correction interventions. An internal error correction mechanism and an external error correction mechanism are presented to correct errors under corresponding phases. A replan strategy is developed to replan tasks or correct error codes when task execution fails. Experimental results on real robots and in simulation environments have demonstrated the superiority of the proposed framework, with higher success rates and robust error correction capabilities in open-world tasks.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e354.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e354.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReplanVLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReplanVLM: Replanning Robotic Tasks with Visual Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLM-based robotic task planning framework built on GPT-4V that integrates a Decision Bot (plan/code generation), an Inner Bot (internal error correction), an Extra Bot (external error correction), and an object-detection pipeline to iteratively plan, execute, detect failures, and replan for multi-step manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V (as VLM backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses GPT-4V as a vision-language model backbone to ingest images + text and produce natural language plans and executable code; combined with an external Yolov8 detector that provides object identity and 3D poses via RGB bounding boxes and depth fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Seven multi-step manipulation and arrangement tasks (Tasks 1–7)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A set of seven embodied tasks on a real robot and in simulation, including: (1) food demand recognition (identify and fetch an apple), (2) grasping a block occluded by other blocks (remove occluders then grasp), (3) stacking blocks according to a reference image, (4) sequential arrangement on a moving conveyor belt, (5) categorization and transport of fruits to plates, (6) grabbing an object inside a drawer (requires opening), and (7) toy recognition by attribute (e.g., 'evil' toy). The agent receives textual instructions and visual observations (RGB+depth).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning; object manipulation; instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretrained VLM (GPT-4V) with in-context prompts/CoT, visual input from camera (RGB+depth) processed by Yolov8, and prompt engineering (role-playing, code repository, examples)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>prompt-based role assignment, chain-of-thought (CoT) reasoning, code generation (Decision Bot), internal regeneration/verification (Inner Bot), and image-difference checks (Extra Bot)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit multimodal embeddings inside GPT-4V combined with explicit artifacts: natural language task plans, generated executable code, and numeric object poses computed from bounding boxes+depth (no explicit global spatial map beyond per-object poses and scene images)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task success rate (percentage) per task; error detection rate and error correction rate for error experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Average success rate 94.2% on real robots and 94.2% in simulation across seven tasks; task-specific success: Task1 100%, Task2 90%, Task3 80%, Task4 100%, Task5 100%, Task6 90%, Task7 100%. Error detection rate average 95.7%; error correction rate average 80.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Succeeded at combining visual recognition with planning to (a) interpret vague/semantic instructions (e.g., 'I'm hungry' → identify apple), (b) reason about object-relational constraints (identify occluding objects and plan to remove them before grasp), (c) classify and route objects (categorization and transport), and (d) operate in dynamic scenes (arrangement on conveyor). The system used image-grounded prompts plus code-generation to realize procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Worst performance on precise spatial stacking (Task 3, 80% success) indicating limitations in fine-grained spatial placement and long-horizon stacking stability; failures attributed to limits in VLM visual processing, absence of non-visual modalities (touch), and occasional hallucinated or incorrect code/plans without correction cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to SayCan (10% average success) and ProgPrompt (31.4% average success) on the same tasks, ReplanVLM outperformed both by a large margin.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing internal correction (ReplanVLM-internal) reduced average success to 87.1%; removing external correction (ReplanVLM-external) reduced to 78.5%; removing both (ReplanVLM-both) reduced to 68.6%. External correction had the largest single impact.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating a VLM (GPT-4V) with explicit perception (Yolov8 pose estimation), plus internal (code/plan verification) and external (before/after image comparison) error-correction loops, enables reliable use of spatial, procedural and object-relational knowledge for embodied manipulation; the knowledge is represented both implicitly in model weights and explicitly as generated code and numeric object poses, and error-correction loops are critical to overcome hallucinations and perceptual failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReplanVLM: Replanning Robotic Tasks With Visual Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e354.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e354.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V (vision-enabled GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal vision-language model used as the VLM backbone in ReplanVLM to interpret images and text jointly, produce natural-language reasoning, and generate executable code for robotic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language transformer that accepts image+text inputs and produces language outputs; here used with prompt engineering (role-playing, CoT, examples) to generate high-level plans and robot-executable code.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used across Tasks 1–7 within ReplanVLM</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-step manipulation tasks described in ReplanVLM; GPT-4V is the reasoning/plan-generation component that fuses visual observations and textual instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning; manipulation; instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational (via multimodal grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on multimodal corpora (implicit) plus in-context examples and prompts provided by the Decision/Inner/Extra Bots</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>prompting with CoT, role-playing prompts, example-driven in-context learning, and code generation prompts</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit multimodal embeddings that ground visual features to linguistic tokens; outputs explicit natural-language plans and code; does not produce an explicit symbolic spatial map in the framework beyond per-object pose readouts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>part of ReplanVLM evaluation (success rate, error detection/correction)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>As the VLM backbone of ReplanVLM, contributed to the observed 94.2% average success rate and 95.7% error detection rate / 80.0% correction rate in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>When supplied with images and prompts, GPT-4V could identify object identities, infer occlusion relations, and propose multi-step procedures (e.g., remove occluders then grasp target).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Limitations in precise spatial placement (stacking) and occasional hallucinated or syntactically/semantically incorrect code that required Inner Bot verification; performance depends on image quality and completeness of perceptual outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used in ReplanVLM; baseline systems (SayCan, ProgPrompt) did not employ GPT-4V and performed substantially worse.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not ablated independently in paper; removing error-correction modules while keeping GPT-4V reduced overall system performance substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4V enables grounding of textual instructions into visually-grounded procedural plans and code, but its outputs need verification (internal checks) and outcome validation (external checks) to robustly realize embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReplanVLM: Replanning Robotic Tasks With Visual Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e354.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e354.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inner Bot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Internal Error Correction Mechanism (Inner Bot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLM-driven internal verifier that inspects generated plans and code for format, semantic matching, and functional equivalence by regenerating plans/codes and comparing functionality to prevent hallucination and unsafe actions before execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V (component)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompts a VLM to play a verifier role performing format checks, plan-code alignment checks, and independent regeneration/verification of plans and code given environment and codebase info.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Internal verification step within all Tasks 1–7 prior to execution</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Checks Decision Bot outputs (plans + code) for syntactic validity, semantic alignment with task requirements, and functional equivalence by regenerating alternatives and comparing; if checks fail, feedback triggers re-generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>plan verification; safety checking; procedural correctness</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+object-relational+spatial (to the extent plans reference object relations or poses)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>in-context prompting and environment/codebase information supplied to the Inner Bot; relies on the VLM's pretrained knowledge and the current visual/environmental context</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>prompt-based verification, plan/code regeneration and comparison; internal feedback loop (up to five cycles)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Natural-language critiques, regenerated plan/code artifacts, and comparisons of code functionality (symbolic/code-level representation); not a learned explicit symbolic verifier but a prompt-driven VLM process</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>system-level success rate impact (ablation) and prevention of execution-time hallucinations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Ablation: removing Inner Bot reduced average success from 94.2% to 87.1% (ReplanVLM-internal), indicating a substantial contribution to reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Detected format and alignment errors in generated plans/code, prevented hazardous or logically incorrect actions (e.g., trying to grab a target without removing occluder), and prompted corrective re-generation before execution.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Inner Bot feedback itself may be inaccurate; framework treats its feedback as advisory, and persistent disagreement after ~5 cycles leads to declared failure; cannot substitute for tactile feedback or fine-grained spatial correction.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared indirectly in ablation vs full system; no external baseline exactly matching this component.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removal reduces mean success to 87.1%, showing internal checking contributes ~7.1 percentage points to average performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language-model-based internal verification (regenerate-and-compare) is effective at reducing hallucinated or ill-formed plans/code and thereby increases safe and successful executions, encoding procedural correctness as explicit code artifacts and natural-language critiques rather than separate symbolic representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReplanVLM: Replanning Robotic Tasks With Visual Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e354.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e354.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Extra Bot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>External Error Correction Mechanism (Extra Bot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLM-based post-execution evaluator that compares before/after environment images and task requirements to determine success, identify causes of failure (e.g., occlusion not removed), and send corrective feedback to the Decision Bot for replanning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V (component)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompts GPT-4V to analyze image pairs (pre/post execution), the original plan/code, and task requirements to determine completion status and to hypothesize causes of failure for replanning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Post-execution evaluation across Tasks 1–7</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given pre- and post-execution images plus plan/code and requirements, decide if task completed; if not, produce a diagnosis (e.g., 'red cube did not move; yellow cube still on top') and feed this back for replanning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>outcome validation; diagnostic reasoning; replanning trigger</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational+procedural (identifies spatial changes and links them to procedural steps that failed)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>visual inputs (before/after images) processed by VLM prompts and object poses from Yolov8; in-context instructions and the Decision Bot's plan/code</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>image-pair analysis via VLM prompting, generation of failure hypotheses, and feedback loop to Decision Bot (replan)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Natural-language failure diagnoses tied to per-object pose observations (explicit numeric poses from detector) rather than an internal symbolic state machine</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>error detection rate and error correction rate; contribution to overall success rate (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Average error detection rate 95.7%; average error correction rate 80.0%. Ablation removing Extra Bot reduced average success to 78.5% (from 94.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Highly effective at detecting when objects did not move as intended and at suggesting corrective procedural changes (e.g., remove occluder first), enabling replanning to succeed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Less able to correct some failures when visual evidence is ambiguous or when non-visual failure modes (gripper slip, unseen damage) occur; cannot sense tactile contact.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Indirect via ablation: presence of Extra Bot produces ~15.7 percentage point improvement over removing it.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing Extra Bot caused the largest single drop in average success (to 78.5%), indicating external outcome validation is critical for embodied reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Outcome validation using image-difference analysis and VLM-based diagnosis is a powerful way to supply procedural corrective knowledge for replanning; spatial and object-relational knowledge is used by the language model to map perceived changes to corrective procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReplanVLM: Replanning Robotic Tasks With Visual Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e354.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e354.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yolov8 Detector</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yolov8 object detection and pose estimation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision module that detects objects in RGB images, provides bounding-box pixel coordinates and tracks objects, and fuses those coordinates with depth data to compute object poses used by the VLM components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yolov8</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A real-time object detector used to extract object identities and pixel bounding boxes; pixel coordinates combined with depth camera (Intel Realsense D435i) yield per-object pose estimates and movement endpoints for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Perception submodule for Tasks 1–7</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detect and track objects on the workspace or conveyor, return object poses to the planning pipeline, and provide before/after pose data for the Extra Bot's comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>perception for manipulation; spatial pose estimation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational (object identity + 3D position/pose)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>vision model trained on object detection datasets and run-time RGB+depth sensor fusion</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>standard object-detection inference producing bounding boxes and tracking; depth fusion to compute pose</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Explicit numeric poses (x,y,z, orientation) per detected object and symbolic labels (object class/name)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>used to compute pose accuracy implicitly; system-level success rates reflect adequacy of perception</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>No separate quantitative detector metrics provided; detector enabled per-task success rates reported for ReplanVLM (average 94.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Provided reliable object identity and pose estimation sufficient for most tasks including occluder detection and conveyor manipulation; tracking supported dynamic scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Perceptual limitations contributed to errors in precise stacking and some failed executions; incomplete visual cues (e.g., invisible objects inside closed drawer) require procedural steps to reveal them (opening drawer) before detection.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared to other detectors in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not ablated separately; system-level failures partially attributed to visual processing limits.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit per-object pose representation provided by Yolov8+depth is the primary sensory grounding that enables the VLM to reason about spatial and object-relational relations; however, precise fine-grained spatial control still challenged the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReplanVLM: Replanning Robotic Tasks With Visual Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e354.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e354.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs without vision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models operating without direct sensory input</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Text-only LLMs (discussed in related work) have strong textual/world knowledge and planning ability but limited understanding of current perceptual state (physical forms, spatial locations, affordances) when used alone for robotic planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generic LLMs (e.g., GPT-4, GPT-3 family) as discussed in related work</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer-based LLMs trained on text corpora; can perform chain-of-thought reasoning and generate plans/code but lack built-in visual grounding unless augmented with external perception modules.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mentioned as prior approaches for task planning and generation (e.g., SayCan, ProgPrompt pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used by prior work to synthesize plans or code for robot tasks often relying on external perception modules for grounding; when used without direct sensory input they operate from language-only context or symbolic state.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning; instruction generation (when used without sensors)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+object-relational (text-derived), limited spatial grounding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora; in-context examples; symbolic affordances or external heuristics when combined with other modules</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot or few-shot prompting, chain-of-thought, program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit procedural and commonsense knowledge in model weights and generated natural-language plans; lacks direct perceptual state encoding when not supplied with sensor inputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Reported via baselines (SayCan, ProgPrompt) on the paper's tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Baseline methods that rely on LLMs without integrated VLM perception performed poorly on the evaluated visual/manipulation tasks: SayCan average 10% success; ProgPrompt average 31.4% success.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Can produce high-level procedural plans and code fragments, useful for symbolic planning and program synthesis in known/simulated domains.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Failed to ground actions to the current visual state, missed spatial relations (e.g., occluders), and produced unsafe or non-executable plans when visual context was required.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Directly compared: SayCan (10%), ProgPrompt (31.4%) vs ReplanVLM (94.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not applicable within paper; discussed as motivation for integrating VLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs without sensory grounding are insufficient for real-world embodied tasks that require up-to-date spatial and object-relational knowledge; visual grounding (VLMs + detectors) and outcome verification are necessary to bridge the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReplanVLM: Replanning Robotic Tasks With Visual Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>VoxPoser: Composable 3d value maps for robotic manipulation with language models <em>(Rating: 2)</em></li>
                <li>Physically grounded vision-language models for robotic manipulation <em>(Rating: 2)</em></li>
                <li>Look before you leap: Unveiling the power of GPT-4V in robotic vision-language planning <em>(Rating: 2)</em></li>
                <li>Replan: Robotic replanning with perception and language models <em>(Rating: 2)</em></li>
                <li>Vision-language interpreter for robot task planning <em>(Rating: 2)</em></li>
                <li>HuggingGPT: Solving AI tasks with ChatGPT and its friends in hugging face <em>(Rating: 1)</em></li>
                <li>Tidybot: Personalized robot assistance with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-354",
    "paper_id": "paper-271571227",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "ReplanVLM",
            "name_full": "ReplanVLM: Replanning Robotic Tasks with Visual Language Models",
            "brief_description": "A VLM-based robotic task planning framework built on GPT-4V that integrates a Decision Bot (plan/code generation), an Inner Bot (internal error correction), an Extra Bot (external error correction), and an object-detection pipeline to iteratively plan, execute, detect failures, and replan for multi-step manipulation tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4V (as VLM backbone)",
            "model_size": null,
            "model_description": "Uses GPT-4V as a vision-language model backbone to ingest images + text and produce natural language plans and executable code; combined with an external Yolov8 detector that provides object identity and 3D poses via RGB bounding boxes and depth fusion.",
            "task_name": "Seven multi-step manipulation and arrangement tasks (Tasks 1–7)",
            "task_description": "A set of seven embodied tasks on a real robot and in simulation, including: (1) food demand recognition (identify and fetch an apple), (2) grasping a block occluded by other blocks (remove occluders then grasp), (3) stacking blocks according to a reference image, (4) sequential arrangement on a moving conveyor belt, (5) categorization and transport of fruits to plates, (6) grabbing an object inside a drawer (requires opening), and (7) toy recognition by attribute (e.g., 'evil' toy). The agent receives textual instructions and visual observations (RGB+depth).",
            "task_type": "multi-step planning; object manipulation; instruction following",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pretrained VLM (GPT-4V) with in-context prompts/CoT, visual input from camera (RGB+depth) processed by Yolov8, and prompt engineering (role-playing, code repository, examples)",
            "has_direct_sensory_input": true,
            "elicitation_method": "prompt-based role assignment, chain-of-thought (CoT) reasoning, code generation (Decision Bot), internal regeneration/verification (Inner Bot), and image-difference checks (Extra Bot)",
            "knowledge_representation": "Implicit multimodal embeddings inside GPT-4V combined with explicit artifacts: natural language task plans, generated executable code, and numeric object poses computed from bounding boxes+depth (no explicit global spatial map beyond per-object poses and scene images)",
            "performance_metric": "task success rate (percentage) per task; error detection rate and error correction rate for error experiments",
            "performance_result": "Average success rate 94.2% on real robots and 94.2% in simulation across seven tasks; task-specific success: Task1 100%, Task2 90%, Task3 80%, Task4 100%, Task5 100%, Task6 90%, Task7 100%. Error detection rate average 95.7%; error correction rate average 80.0%.",
            "success_patterns": "Succeeded at combining visual recognition with planning to (a) interpret vague/semantic instructions (e.g., 'I'm hungry' → identify apple), (b) reason about object-relational constraints (identify occluding objects and plan to remove them before grasp), (c) classify and route objects (categorization and transport), and (d) operate in dynamic scenes (arrangement on conveyor). The system used image-grounded prompts plus code-generation to realize procedures.",
            "failure_patterns": "Worst performance on precise spatial stacking (Task 3, 80% success) indicating limitations in fine-grained spatial placement and long-horizon stacking stability; failures attributed to limits in VLM visual processing, absence of non-visual modalities (touch), and occasional hallucinated or incorrect code/plans without correction cycles.",
            "baseline_comparison": "Compared to SayCan (10% average success) and ProgPrompt (31.4% average success) on the same tasks, ReplanVLM outperformed both by a large margin.",
            "ablation_results": "Removing internal correction (ReplanVLM-internal) reduced average success to 87.1%; removing external correction (ReplanVLM-external) reduced to 78.5%; removing both (ReplanVLM-both) reduced to 68.6%. External correction had the largest single impact.",
            "key_findings": "Integrating a VLM (GPT-4V) with explicit perception (Yolov8 pose estimation), plus internal (code/plan verification) and external (before/after image comparison) error-correction loops, enables reliable use of spatial, procedural and object-relational knowledge for embodied manipulation; the knowledge is represented both implicitly in model weights and explicitly as generated code and numeric object poses, and error-correction loops are critical to overcome hallucinations and perceptual failures.",
            "uuid": "e354.0",
            "source_info": {
                "paper_title": "ReplanVLM: Replanning Robotic Tasks With Visual Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V (vision-enabled GPT-4)",
            "brief_description": "A multimodal vision-language model used as the VLM backbone in ReplanVLM to interpret images and text jointly, produce natural-language reasoning, and generate executable code for robotic tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4V",
            "model_size": null,
            "model_description": "Vision-language transformer that accepts image+text inputs and produces language outputs; here used with prompt engineering (role-playing, CoT, examples) to generate high-level plans and robot-executable code.",
            "task_name": "Used across Tasks 1–7 within ReplanVLM",
            "task_description": "Same multi-step manipulation tasks described in ReplanVLM; GPT-4V is the reasoning/plan-generation component that fuses visual observations and textual instructions.",
            "task_type": "multi-step planning; manipulation; instruction following",
            "knowledge_type": "spatial+procedural+object-relational (via multimodal grounding)",
            "knowledge_source": "pretraining on multimodal corpora (implicit) plus in-context examples and prompts provided by the Decision/Inner/Extra Bots",
            "has_direct_sensory_input": true,
            "elicitation_method": "prompting with CoT, role-playing prompts, example-driven in-context learning, and code generation prompts",
            "knowledge_representation": "Implicit multimodal embeddings that ground visual features to linguistic tokens; outputs explicit natural-language plans and code; does not produce an explicit symbolic spatial map in the framework beyond per-object pose readouts.",
            "performance_metric": "part of ReplanVLM evaluation (success rate, error detection/correction)",
            "performance_result": "As the VLM backbone of ReplanVLM, contributed to the observed 94.2% average success rate and 95.7% error detection rate / 80.0% correction rate in experiments.",
            "success_patterns": "When supplied with images and prompts, GPT-4V could identify object identities, infer occlusion relations, and propose multi-step procedures (e.g., remove occluders then grasp target).",
            "failure_patterns": "Limitations in precise spatial placement (stacking) and occasional hallucinated or syntactically/semantically incorrect code that required Inner Bot verification; performance depends on image quality and completeness of perceptual outputs.",
            "baseline_comparison": "Used in ReplanVLM; baseline systems (SayCan, ProgPrompt) did not employ GPT-4V and performed substantially worse.",
            "ablation_results": "Not ablated independently in paper; removing error-correction modules while keeping GPT-4V reduced overall system performance substantially.",
            "key_findings": "GPT-4V enables grounding of textual instructions into visually-grounded procedural plans and code, but its outputs need verification (internal checks) and outcome validation (external checks) to robustly realize embodied tasks.",
            "uuid": "e354.1",
            "source_info": {
                "paper_title": "ReplanVLM: Replanning Robotic Tasks With Visual Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Inner Bot",
            "name_full": "Internal Error Correction Mechanism (Inner Bot)",
            "brief_description": "A VLM-driven internal verifier that inspects generated plans and code for format, semantic matching, and functional equivalence by regenerating plans/codes and comparing functionality to prevent hallucination and unsafe actions before execution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4V (component)",
            "model_size": null,
            "model_description": "Prompts a VLM to play a verifier role performing format checks, plan-code alignment checks, and independent regeneration/verification of plans and code given environment and codebase info.",
            "task_name": "Internal verification step within all Tasks 1–7 prior to execution",
            "task_description": "Checks Decision Bot outputs (plans + code) for syntactic validity, semantic alignment with task requirements, and functional equivalence by regenerating alternatives and comparing; if checks fail, feedback triggers re-generation.",
            "task_type": "plan verification; safety checking; procedural correctness",
            "knowledge_type": "procedural+object-relational+spatial (to the extent plans reference object relations or poses)",
            "knowledge_source": "in-context prompting and environment/codebase information supplied to the Inner Bot; relies on the VLM's pretrained knowledge and the current visual/environmental context",
            "has_direct_sensory_input": true,
            "elicitation_method": "prompt-based verification, plan/code regeneration and comparison; internal feedback loop (up to five cycles)",
            "knowledge_representation": "Natural-language critiques, regenerated plan/code artifacts, and comparisons of code functionality (symbolic/code-level representation); not a learned explicit symbolic verifier but a prompt-driven VLM process",
            "performance_metric": "system-level success rate impact (ablation) and prevention of execution-time hallucinations",
            "performance_result": "Ablation: removing Inner Bot reduced average success from 94.2% to 87.1% (ReplanVLM-internal), indicating a substantial contribution to reliability.",
            "success_patterns": "Detected format and alignment errors in generated plans/code, prevented hazardous or logically incorrect actions (e.g., trying to grab a target without removing occluder), and prompted corrective re-generation before execution.",
            "failure_patterns": "Inner Bot feedback itself may be inaccurate; framework treats its feedback as advisory, and persistent disagreement after ~5 cycles leads to declared failure; cannot substitute for tactile feedback or fine-grained spatial correction.",
            "baseline_comparison": "Compared indirectly in ablation vs full system; no external baseline exactly matching this component.",
            "ablation_results": "Removal reduces mean success to 87.1%, showing internal checking contributes ~7.1 percentage points to average performance.",
            "key_findings": "Language-model-based internal verification (regenerate-and-compare) is effective at reducing hallucinated or ill-formed plans/code and thereby increases safe and successful executions, encoding procedural correctness as explicit code artifacts and natural-language critiques rather than separate symbolic representations.",
            "uuid": "e354.2",
            "source_info": {
                "paper_title": "ReplanVLM: Replanning Robotic Tasks With Visual Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Extra Bot",
            "name_full": "External Error Correction Mechanism (Extra Bot)",
            "brief_description": "A VLM-based post-execution evaluator that compares before/after environment images and task requirements to determine success, identify causes of failure (e.g., occlusion not removed), and send corrective feedback to the Decision Bot for replanning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4V (component)",
            "model_size": null,
            "model_description": "Prompts GPT-4V to analyze image pairs (pre/post execution), the original plan/code, and task requirements to determine completion status and to hypothesize causes of failure for replanning.",
            "task_name": "Post-execution evaluation across Tasks 1–7",
            "task_description": "Given pre- and post-execution images plus plan/code and requirements, decide if task completed; if not, produce a diagnosis (e.g., 'red cube did not move; yellow cube still on top') and feed this back for replanning.",
            "task_type": "outcome validation; diagnostic reasoning; replanning trigger",
            "knowledge_type": "spatial+object-relational+procedural (identifies spatial changes and links them to procedural steps that failed)",
            "knowledge_source": "visual inputs (before/after images) processed by VLM prompts and object poses from Yolov8; in-context instructions and the Decision Bot's plan/code",
            "has_direct_sensory_input": true,
            "elicitation_method": "image-pair analysis via VLM prompting, generation of failure hypotheses, and feedback loop to Decision Bot (replan)",
            "knowledge_representation": "Natural-language failure diagnoses tied to per-object pose observations (explicit numeric poses from detector) rather than an internal symbolic state machine",
            "performance_metric": "error detection rate and error correction rate; contribution to overall success rate (ablation)",
            "performance_result": "Average error detection rate 95.7%; average error correction rate 80.0%. Ablation removing Extra Bot reduced average success to 78.5% (from 94.2%).",
            "success_patterns": "Highly effective at detecting when objects did not move as intended and at suggesting corrective procedural changes (e.g., remove occluder first), enabling replanning to succeed.",
            "failure_patterns": "Less able to correct some failures when visual evidence is ambiguous or when non-visual failure modes (gripper slip, unseen damage) occur; cannot sense tactile contact.",
            "baseline_comparison": "Indirect via ablation: presence of Extra Bot produces ~15.7 percentage point improvement over removing it.",
            "ablation_results": "Removing Extra Bot caused the largest single drop in average success (to 78.5%), indicating external outcome validation is critical for embodied reliability.",
            "key_findings": "Outcome validation using image-difference analysis and VLM-based diagnosis is a powerful way to supply procedural corrective knowledge for replanning; spatial and object-relational knowledge is used by the language model to map perceived changes to corrective procedures.",
            "uuid": "e354.3",
            "source_info": {
                "paper_title": "ReplanVLM: Replanning Robotic Tasks With Visual Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Yolov8 Detector",
            "name_full": "Yolov8 object detection and pose estimation pipeline",
            "brief_description": "A vision module that detects objects in RGB images, provides bounding-box pixel coordinates and tracks objects, and fuses those coordinates with depth data to compute object poses used by the VLM components.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Yolov8",
            "model_size": null,
            "model_description": "A real-time object detector used to extract object identities and pixel bounding boxes; pixel coordinates combined with depth camera (Intel Realsense D435i) yield per-object pose estimates and movement endpoints for manipulation.",
            "task_name": "Perception submodule for Tasks 1–7",
            "task_description": "Detect and track objects on the workspace or conveyor, return object poses to the planning pipeline, and provide before/after pose data for the Extra Bot's comparison.",
            "task_type": "perception for manipulation; spatial pose estimation",
            "knowledge_type": "spatial+object-relational (object identity + 3D position/pose)",
            "knowledge_source": "vision model trained on object detection datasets and run-time RGB+depth sensor fusion",
            "has_direct_sensory_input": true,
            "elicitation_method": "standard object-detection inference producing bounding boxes and tracking; depth fusion to compute pose",
            "knowledge_representation": "Explicit numeric poses (x,y,z, orientation) per detected object and symbolic labels (object class/name)",
            "performance_metric": "used to compute pose accuracy implicitly; system-level success rates reflect adequacy of perception",
            "performance_result": "No separate quantitative detector metrics provided; detector enabled per-task success rates reported for ReplanVLM (average 94.2%).",
            "success_patterns": "Provided reliable object identity and pose estimation sufficient for most tasks including occluder detection and conveyor manipulation; tracking supported dynamic scenes.",
            "failure_patterns": "Perceptual limitations contributed to errors in precise stacking and some failed executions; incomplete visual cues (e.g., invisible objects inside closed drawer) require procedural steps to reveal them (opening drawer) before detection.",
            "baseline_comparison": "Not compared to other detectors in paper.",
            "ablation_results": "Not ablated separately; system-level failures partially attributed to visual processing limits.",
            "key_findings": "Explicit per-object pose representation provided by Yolov8+depth is the primary sensory grounding that enables the VLM to reason about spatial and object-relational relations; however, precise fine-grained spatial control still challenged the pipeline.",
            "uuid": "e354.4",
            "source_info": {
                "paper_title": "ReplanVLM: Replanning Robotic Tasks With Visual Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "LLMs without vision",
            "name_full": "Large Language Models operating without direct sensory input",
            "brief_description": "Text-only LLMs (discussed in related work) have strong textual/world knowledge and planning ability but limited understanding of current perceptual state (physical forms, spatial locations, affordances) when used alone for robotic planning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Generic LLMs (e.g., GPT-4, GPT-3 family) as discussed in related work",
            "model_size": null,
            "model_description": "Pretrained transformer-based LLMs trained on text corpora; can perform chain-of-thought reasoning and generate plans/code but lack built-in visual grounding unless augmented with external perception modules.",
            "task_name": "Mentioned as prior approaches for task planning and generation (e.g., SayCan, ProgPrompt pipelines)",
            "task_description": "Used by prior work to synthesize plans or code for robot tasks often relying on external perception modules for grounding; when used without direct sensory input they operate from language-only context or symbolic state.",
            "task_type": "multi-step planning; instruction generation (when used without sensors)",
            "knowledge_type": "procedural+object-relational (text-derived), limited spatial grounding",
            "knowledge_source": "pre-training on text corpora; in-context examples; symbolic affordances or external heuristics when combined with other modules",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot or few-shot prompting, chain-of-thought, program synthesis",
            "knowledge_representation": "Implicit procedural and commonsense knowledge in model weights and generated natural-language plans; lacks direct perceptual state encoding when not supplied with sensor inputs",
            "performance_metric": "Reported via baselines (SayCan, ProgPrompt) on the paper's tasks",
            "performance_result": "Baseline methods that rely on LLMs without integrated VLM perception performed poorly on the evaluated visual/manipulation tasks: SayCan average 10% success; ProgPrompt average 31.4% success.",
            "success_patterns": "Can produce high-level procedural plans and code fragments, useful for symbolic planning and program synthesis in known/simulated domains.",
            "failure_patterns": "Failed to ground actions to the current visual state, missed spatial relations (e.g., occluders), and produced unsafe or non-executable plans when visual context was required.",
            "baseline_comparison": "Directly compared: SayCan (10%), ProgPrompt (31.4%) vs ReplanVLM (94.2%).",
            "ablation_results": "Not applicable within paper; discussed as motivation for integrating VLMs.",
            "key_findings": "LLMs without sensory grounding are insufficient for real-world embodied tasks that require up-to-date spatial and object-relational knowledge; visual grounding (VLMs + detectors) and outcome verification are necessary to bridge the gap.",
            "uuid": "e354.5",
            "source_info": {
                "paper_title": "ReplanVLM: Replanning Robotic Tasks With Visual Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "VoxPoser: Composable 3d value maps for robotic manipulation with language models",
            "rating": 2,
            "sanitized_title": "voxposer_composable_3d_value_maps_for_robotic_manipulation_with_language_models"
        },
        {
            "paper_title": "Physically grounded vision-language models for robotic manipulation",
            "rating": 2,
            "sanitized_title": "physically_grounded_visionlanguage_models_for_robotic_manipulation"
        },
        {
            "paper_title": "Look before you leap: Unveiling the power of GPT-4V in robotic vision-language planning",
            "rating": 2,
            "sanitized_title": "look_before_you_leap_unveiling_the_power_of_gpt4v_in_robotic_visionlanguage_planning"
        },
        {
            "paper_title": "Replan: Robotic replanning with perception and language models",
            "rating": 2,
            "sanitized_title": "replan_robotic_replanning_with_perception_and_language_models"
        },
        {
            "paper_title": "Vision-language interpreter for robot task planning",
            "rating": 2,
            "sanitized_title": "visionlanguage_interpreter_for_robot_task_planning"
        },
        {
            "paper_title": "HuggingGPT: Solving AI tasks with ChatGPT and its friends in hugging face",
            "rating": 1,
            "sanitized_title": "hugginggpt_solving_ai_tasks_with_chatgpt_and_its_friends_in_hugging_face"
        },
        {
            "paper_title": "Tidybot: Personalized robot assistance with large language models",
            "rating": 1,
            "sanitized_title": "tidybot_personalized_robot_assistance_with_large_language_models"
        }
    ],
    "cost": 0.01479375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ReplanVLM: Replanning Robotic Tasks with Visual Language Models
31 Jul 2024</p>
<p>Aoran Mei 
Guo-Niu Zhu 
Member, IEEEHuaxiang Zhang 
Zhongxue Gan 
ReplanVLM: Replanning Robotic Tasks with Visual Language Models
31 Jul 20246FC70D32FA47FEE724D8B22A3E94E19AarXiv:2407.21762v1[cs.RO]
Large language models (LLMs) have gained increasing popularity in robotic task planning due to their exceptional abilities in text analytics and generation, as well as their broad knowledge of the world.However, they fall short in decoding visual cues.LLMs have limited direct perception of the world, which leads to a deficient grasp of the current state of the world.By contrast, the emergence of visual language models (VLMs) fills this gap by integrating visual perception modules, which can enhance the autonomy of robotic task planning.Despite these advancements, VLMs still face challenges, such as the potential for task execution errors, even when provided with accurate instructions.To address such issues, this paper proposes a ReplanVLM framework for robotic task planning.In this study, we focus on error correction interventions.An internal error correction mechanism and an external error correction mechanism are presented to correct errors under corresponding phases.A replan strategy is developed to replan tasks or correct error codes when task execution fails.Experimental results on real robots and in simulation environments have demonstrated the superiority of the proposed framework, with higher success rates and robust error correction capabilities in open-world tasks.Videos of our experiments are available at https://youtu.be/NPk2pWKazJc.</p>
<p>I. INTRODUCTION</p>
<p>Large language models (LLMs) have received widespread attention for their remarkable reasoning and understanding capabilities and have been applied in various scenarios, such as human-robot interaction [1], action planning [2], visual target navigation [3], and task planning [4].With prompt engineering techniques like chain-of-thought (CoT), LLMs are empowered with powerful reasoning abilities for complex task planning and execution [5].For example, Raman et al. [6] proposed a prompt-based strategy for extracting executable plans from an LLM.Their work introduced a corrective re-prompting technique to extract executable corrective actions to achieve the intended goal.Silver et al. [7] used GPT-4 to synthesize Python programs and generate task plans in the planning domain definition language.However, handling the dynamic interaction with the environment remains challenging in robotic task planning.It requires not only a precise perception of environmental information but also the prediction of the causal effects of their actions on the environment.Current LLMs have limitations in understanding the physical state of the world, such as physical forms, spatial locations, and physical properties, which are essential for robot task planning and execution.*This work was supported by Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0103 and Key Project of Comprehensive Prosperity Plan of Fudan University under Grant XM06231744.(Corresponding author: Guo-Niu Zhu and Zhongxue Gan)</p>
<p>All authors are with the Academy for Engineering and Technology, Fudan University, Shanghai 200433, China (e-mail: guoniu zhu@fudan.edu.cn).</p>
<p>To overcome these challenges, recent studies tend to incorporate external vision models to improve the perception of the environment [8].Under this framework, LLMs assimilate insights from vision models for task planning.Methods such as HuggingGPT [9] and Tidybot [10] have adopted this technique.Even though this strategy enables the model to understand the environment better and shows potential in robotic task planning, it suffers from serious issues.The performance of such models depends on external vision models, whereas the information provided by the vision model might be incomplete.</p>
<p>On the other hand, the development of vision language models (VLMs) has provided a novel solution to the challenges faced by LLMs in environmental interaction [11].VLMs integrate techniques from computer vision and natural language processing to deeply understand text information related to visual content and generate corresponding descriptions.By incorporating visual perception capabilities, VLMs can effectively decode visual cues in task planning, not only capturing environmental information but also predicting the causal impact of each action on the environment.</p>
<p>Although VLMs outperform LLMs in dealing with complex scenes, long-term task planning remains challenging.It involves predicting multiple future steps, each of which may depend on the outcome of the previous step.The complexity of such scenarios requires the model to have a high degree of adaptability and predictive capability to deal with the uncertainty and diversity of the environment.To address such issues, this study proposes a ReplanVLM, a novel robotic task planning paradigm.This framework is developed based on GPT-4V.An internal error correction mechanism and an external error correction mechanism are presented to enhance the performance of the framework in error detection and correction.A replan strategy is suggested to replan tasks when task execution fails.The main contributions of this paper are summarized as follows.</p>
<p>(1) VLM-Enhanced ReplanVLM Framework: We propose a ReplanVLM, a robotic task planning framework based on VLMs.This framework achieves a deep understanding of the environment and task requirements.The introduction of VLMs not only enables the robot to process visual and textual information but also improves the accuracy and efficiency of task planning and execution.</p>
<p>(2) Internal and External Error Correction Mechanisms: We present an internal error correction mechanism to inspect codes, environments, and task requirements to prevent errors caused by hallucinations or misunderstandings.We put forward an external error correction mechanism that reevaluates the environmental state post-interaction between the robot and its surroundings to guarantee the adaptability of task execution to environmental changes.</p>
<p>(3) Experimental Validation and Application: Multiple experiments on real robots and in simulations are conducted to evaluate the efficacy of the proposed ReplanVLM framework for robotic task planning.Experimental results and comparative analysis show that our framework substantially reduces errors in task execution and enhances the robot's autonomy and adaptability in intricate environments.</p>
<p>II. RELATED WORK</p>
<p>LLMs and VLMs are becoming increasingly popular in the field of robotic task planning [12], [13].Lots of studies were proposed to use LLMs and VLMs to generate task plans [14].For example, Luan et al. [15] presented a multi-layer LLM to enhance the robot's proficiency in handling complex tasks.A VLM and a LLM were integrated to tackle the challenges associated with task planning and execution.Huang et al. [16] proposed a VoxPoser framework for extracting affordances and constraints for manipulation tasks in the real world.By leveraging their code-writing capabilities, LLMs were used to interact with VLMs to construct 3D value maps to enable zero-shot task trajectory generation for robots.Zhang et al. [17] introduced a visually-grounded symbolic planning framework.They used VLMs to detect action failures and verify action affordances towards enabling successful plan execution.Although VLMs can provide visual perception information to LLMs, it might be incomplete.For example, VLMs may only provide object names while ignoring the spatial relationship between objects, which could lead to the failure of robotic task execution or even damage equipment.</p>
<p>To address this issue, some studies directly used VLMs to generate robotic task plans.Hu et al. [18] put forward a robotic vision-language planning (ViLA) framework.They utilized LLMs to generate a sequence of actionable steps.By integrating perceptual data directly into the reasoning and planning processes, the ViLA allows robots to understand common sense knowledge in the visual world, including spatial layouts and object attributes.Skreta et al. [19] employed VLMs to understand world state information to empower robots to replan in real-time in the event of action plan failures.Shirai et al. [20] introduced a visual-language interpreter (ViLaIn) framework for robot task planning.They used LLM and VLM to generate problem descriptions and drive symbolic planners in a language-guided framework.Wake et al. [21] presented a multimodal robot task planning pipeline using GPT-4V.By integrating observations from human demonstration, their proposed GPT-4V(ion) succeeded in converting human actions from videos into robot-executable programs.Even if these methods made significant advances in generating task plans, they do not have error correction mechanisms.When VLMs misunderstand instructions or environments, they may generate incorrect plans.</p>
<p>To bridge this gap, this paper proposes a ReplanVLM framework, which introduces internal and external error correction mechanisms to handle internal (e.g., code generation errors) and external errors (e.g., task failures due to environmental changes during execution), respectively.</p>
<p>III. REPLANVLM: MODEL STRUCTURE AND DETAILS</p>
<p>A. Framework of the proposed ReplanVLM Fig. 1 shows an overview of the ReplanVLM framework, which is developed based on GPT-4V.To start with, the model takes textual instructions as input, e.g., "help me get an apple."It can also handle abstract requests, like "I'm a bit thirsty."In this case, the model is capable of making decisions to grab a cup or an apple based on the recognition results of environmental information.In addition to the textual instructions, the input can be a combination of images and text, e.g., "arrange the blocks in reality according to the sequence shown in the picture."After the initial input stage, a Decision Bot is introduced to plan the task based on the provided input and generate code.The code is then passed to an Inner Bot for review.If the Inner Bot finds errors (i.e., "No"), such as code mistakes or task planning issues, the Inner Bot will send the reason back to the Decision Bot to regenerate new task plans and codes.Otherwise (i.e., "Yes"), the robot executes the task plan.Once the execution is completed, an Extra Bot is suggested to compare the current environment with its initial state based on the task requirements.If the task is identified as a failure (i.e., "No"), the Extra Bot will send the failure information back to the Decision Bot to replan the task.Otherwise (i.e., "Yes"), the iteration would be terminated.</p>
<p>This framework is proposed for various tasks and objects even though Fig. 1 uses cubes as examples.In line with the framework, pseudocode (see Algorithm 1) is provided to illustrate the overall process, where ℓ represents the tasks to be completed, t is a certain moment, and x t is the state of the environment at time t.</p>
<p>Algorithm 1 ReplanVLM</p>
<p>1: Initial time t = 0, initialize environmental information x t , initial instructions ℓ ▷ Initialize the required data 2: while True do  end if 16: end while</p>
<p>B. Internal Error Correction Mechanism</p>
<p>The internal error correction mechanism is proposed to mitigate the hallucination problem in LLMs during the task generation process.Hallucination is a challenging issue in the process of generating task plans.Incorrect codes may be produced even when the task planning is correct.Another significant role of the internal error correction mechanism is to utilize VLMs to predict the potential impact that their actions might have on the surrounding environment.Consider a scenario where the objective is to "grab the red block," but a yellow block rests on the red one.In some cases, the model might attempt to grab the red block directly, disregarding the yellow block above it.This action would potentially cause harm to the objects or even damage the robots.To tackle such issues, this study presents an internal error correction mechanism to prevent these problems to enhance the safety and reliability of task execution.The internal error correction mechanism involves an Inner Bot.Based on an internal feedback strategy, the internal error correction mechanism can assess and correct the task plans and codes generated by the Decision Bot.It can be conducted as follows.</p>
<p>Format Check: The Inner Bot first verifies whether the task plans and codes generated by the Decision Bot meet the format requirements.This step prevents the generation of unsuitable text (e.g., blanks) or pseudocode, which can occur even with correct prompts.</p>
<p>Matching Check: Then, Inner Bot checks the alignment of the generated plans and codes.This step helps avoid errors caused by discrepancies between codes and task plans.</p>
<p>Plan and Code Verification: To verify the accuracy of the task plans and codes generated by Decision Bot, Inner Bot regenerates the task plans and codes based on the environment and codebase information, and compares whether the two sets of codes have the same functionality.Such a comparison enhances the framework to detect and tolerate errors.</p>
<p>If any of the above three points are not satisfied, the feedback generated by the Inner Bot will be passed to the Decision Bot.Given that the Inner Bot's response might contain inaccuracies, this feedback only serves as a reference for the Decision Bot, not a decisive factor.Subsequently, the Decision Bot will regenerate plans and codes based on the known information.If their opinions remain inconsistent after five cycles, the task is deemed a failure.If all three points are satisfied, the robot will execute the task according to the plans and codes generated by the Decision Bot.Fig. 2 illustrates an example of the internal error correction mechanism in Task 1 (details can be found in Section IV-C).</p>
<p>C. External Error Correction Mechanism</p>
<p>The external error correction mechanism is presented to assess the completion status of tasks.During the task execution, robots often encounter unpredictable challenges that can result in task failure.These challenges include but are not limited to, sudden changes in the environment or failures in the robot system.In this context, the external error correction mechanism is crucial.It refers to an Extra Bot, which is utilized to determine whether the task is completed.This mechanism analyzes environmental changes, compares these changes with task requirements, identifies potential causes of task failure, and records such information for task replanning, which is conducted as follows.</p>
<p>The Extra Bot evaluates images of the environment taken before and after task execution.Using these images, along with task requirements, and the task plans and codes generated by the Decision Bot, it assesses whether the task has been successfully completed.If the task is deemed incomplete, the Extra Bot identifies the perceived error reasons and Fig. 2.An example of the internal error correction mechanism.For the task requirement "I am hungry" in Task 1, the Decision Bot generates plans and codes to grab "food."The Inner Bot recommends specifying the object to be grabbed as an "apple" and then relays this suggestion back to the Decision Bot.Finally, the Decision Bot creates a new and accurate plan.</p>
<p>sends them to the Decision Bot.The Decision Bot then uses this feedback and the current environmental information to regenerate task plans and codes.If the Extra Bot confirms task completion, the ReplanVLM framework terminates.</p>
<p>By providing such external feedback techniques, the external error correction mechanism can not only enhance the system's ability to understand failure scenarios but also optimize task execution strategies.Fig. 3 depicts an example of the external error correction mechanism in Task 2 (details can be found in Section IV-C).Fig. 3.An example of the external error correction mechanism.For the task requirement "give me the red cube" in Task 2, the Extra Bot compares the position of the red cube before and after task execution and finds that it has not moved.Then, the Extra Bot sends this information to the Decision Bot, suggesting that the yellow cube be removed first before grabbing the red cube.Based on this feedback, the Decision Bot generates correct plans and codes.</p>
<p>D. Object Detection</p>
<p>In this study, Yolov8 algorithm is used for object detection.Firstly, it identifies the object's pixel coordinates based on the bounding box in RGB images.Then, the object's pose information is gained by integrating these pixel coordinates and the depth information.Besides, this algorithm has tracking capabilities.It can accurately detect objects and give their pose information even if they move.In some tasks, such as "give me()," the movement endpoints are predetermined.While in other tasks, like "put('plate')," the robot dynamically chooses a suitable placement based on environmental settings.</p>
<p>IV. EXPERIMENTS A. Experimental Platform</p>
<p>As illustrated in Fig. 4, the experimental setup consists of a 6 degrees of freedom robotic arm (i.e., JAKA Zu 7), an Intel Realsense D435i depth camera, a Rochu pneumatic gripper, and a conveyor belt.A couple of toys and simulated fruits are introduced as objects for the experiment.</p>
<p>B. Prompt</p>
<p>For each component of the ReplanVLM framework, we propose different prompts.As shown in Fig. 5, the prompt of the Decision Bot consists of role-playing, error messages, code repository, CoT, and examples.By contrast, the prompts of the Inner Bot and Extra Bot comprise role-playing, images, and information from the Decision Bot.For each task undertaken by these components, the prompts would vary slightly to accommodate the specifics of the task, e.g., the code repository and role-playing.</p>
<p>C. Task Design</p>
<p>A series of tasks are proposed to evaluate the effectiveness of the ReplanVLM framework on real robots and in simulation environments.As shown in Table I, these tasks are designed based on the following criteria.</p>
<p>Semantic Understanding (SU): the ability to comprehend and interpret the meaning of a language in a way that is akin to human understanding.For example, given the task "I'm a little hungry," the robot can identify from environmental clues that it should grab an apple.</p>
<p>Spatial Relationship (SR): the capacity to understand the relative spatial position between objects, such as the ability to stack blocks in order according to a given picture.External Spatial Constraints (EC): the ability to recognize various constraints present in the environment.For instance, in the task "Help me grab the red block," the robot needs to remove the blue and yellow blocks located above the red one.</p>
<p>Understanding of Special Attributes (UA): the ability to comprehend the attributes of objects, such as "kind," "evil," "dangerous," "safe," "edible," etc.</p>
<p>Adaptability to Dynamic Scenes (AS): assessing the framework's adaptability to dynamic scenes.</p>
<p>Minimum Operation Steps (MS): the minimum operation steps (no error correction process) include robotic arm movement, gripper closure, and gripper opening.Based on these criteria, seven tasks are proposed as follows.</p>
<p>Task 1: Food Demand Recognition.In this task, a variety of objects, including food, are placed on a table.This task aims to assess whether the robot can accurately understand the user's needs upon receiving the instruction "I'm hungry" and successfully identify and grasp the apple on the table.</p>
<p>Task 2: Grasping the Block.In this task, three colored blocks are arranged in a specific order-blue on top, yellow in the middle, and red at the bottom.Before grabbing the red block and placing it into a cardboard box, the robot needs to remove the blue and yellow blocks.</p>
<p>Task 3: Stacking Blocks.By providing the VLM with a picture showing the order of block stacking, the robot is required to stack the blocks in the same order.</p>
<p>Task 4: Sequential Arrangement on a Conveyor Belt.This task requires the robot to reorder the objects in a designated order on a moving conveyor belt.This task aims to examine the robot's response to dynamic environments, especially its ability to monitor and adjust the arrangement of objects in dynamic scenarios.</p>
<p>Task 5: Categorization and Transport.In this task, the robot is required to classify the fruits on the conveyor belt and place them into different plates.This task intends to test the robot's ability to recognize and transport items, assessing its efficiency in handling objects with various physical properties.</p>
<p>Task 6: Grabbing Invisible Objects.This task requires the robot to identify and grab a block inside a drawer.Since the visual detection system cannot directly identify the block, the robot must open the drawer and then grab the object.</p>
<p>Task 7: Toy Recognition.In this task, the robot is asked to identify and grab toys with given attributes, e.g., grabbing a toy with evil attributes.This task aims to evaluate the VLM's ability to recognize intricate features, such as moral attributes.</p>
<p>In the experiment, the robot runs ten rounds on each task.The average success rate of the ten rounds will be taken as the evaluation metric to indicate the system performance on the corresponding task.</p>
<p>D. Ablation Experiment</p>
<p>To evaluate the relative importance of each module within the ReplanVLM framework, ablation studies are conducted to assess the contribution of the internal and external error correction mechanisms on the overall performance of the ReplanVLM.Accordingly, four models are developed in the ablation experiment, i.e., the ReplanVLM, the ReplanVLM without internal error correction mechanism (ReplanVLMinternal), the ReplanVLM without external error correction mechanism (ReplanVLM-external), and the ReplanVLM without error correction mechanisms (ReplanVLM-both).Each model will be tested on the tasks as presented in IV-C.</p>
<p>E. Error Correction Experiment</p>
<p>Furthermore, error correction experiments are introduced to test the robustness of the ReplanVLM framework when faced with external disturbances.Specifically, this experiment focuses on the model's ability to detect and correct errors after a failed operation.In this experiment, intentional manual interference is applied to cause a failure during the robot's grabbing task.Then, the robot needs to determine whether the task is completed.If the task is deemed incomplete, a correction mechanism is triggered.An error detection rate and an error correction rate are introduced as evaluation metrics in this experiment.Likewise, the experiment will be run in ten rounds to get statistical metrics.</p>
<p>F. Simulation Experiment</p>
<p>Besides the experiments on real robots, simulations are conducted to evaluate the ReplanVLM framework.Cop-peliaSim robotic simulator is introduced to perform the seven tasks described in IV-C.As depicted in Fig. 6, a UR5 robotic arm and a RG2 gripper are set up to execute these tasks in the simulation environment.</p>
<p>V. RESULTS AND DISCUSSION</p>
<p>A. Experimental Results</p>
<p>Table II illustrates the experimental results on real robots and in simulations.As shown in Table II, the proposed ReplanVLM achieves an average success rate of 94.2% on real robots.The success rate can even reach up to 100% in the task of food demand recognition (Task 1), sequential arrangement on a conveyor belt (Task 4), categorization and transport (Task 5), and toy recognition (Task 7).In task 3 (visual assistance in block stacking), the ReplanVLM framework performs the worst, with a success rate of 80%.In most of the tasks, the success rate is higher than 90%.Overall, the ReplanVLM performs well across various scenarios, especially in tasks that require the recognition of complex attributes, the discovery of potential constraints, and the use of world knowledge.The performance across diverse tasks demonstrates the effectiveness of the proposed ReplanVLM in robot task planning.In addition, the proposed framework achieves an average success rate of 94.2% in simulations, which is identical to the results on real robots.Specifically, the success rate of Task 1, Task 2, and Task 5 can reach up to 100%, while the other four tasks get a success rate of 90%.Therefore, the ReplanVLM framework not only performs well in a single task but also maintains a high level of reliability in multiple scenarios in simulated environments.Moreover, the results of the ReplanVLM framework show a high consistency in both real-world and simulated environments.</p>
<p>B. Comparative Experiment Analysis</p>
<p>To evaluate the performance of the proposed Replan-VLM, SayCan [22] and ProgPrompt [23] are introduced as baseline methods.As listed in Table III, the proposed framework achieves an average success rate of 94.2%, while SayCan gets 10% and ProgPrompt gains 31.4%.Obviously, the average success rate of our framework is much higher than baseline methods.The proposed framework outperforms baseline methods on all testing tasks.Notably, both the SayCan and ProgPrompt obtain nearly zero success rates in tasks that require a deep understanding of visual information, such as scene analysis with visual constraints and handling objects with complex attributes (e.g., distinguishing between monsters and Ultraman toys).Compared with baseline methods, the proposed framework shows great potential in intricate environments, especially in scenarios that require scene understanding and decision-making capabilities.</p>
<p>C. Ablation Experiment Analysis</p>
<p>Corresponding to the protocol of the ablation study discussed in IV-D, the results of the ablation experiment are given in Table IV.As illustrated in Table IV, the average success rates of the four methods are obtained as 94.2% (Replan-VLM), 87.1% (ReplanVLM-internal), 78.5% (ReplanVLMexternal), and 68.6% (ReplanVLM-both).Specifically, the ReplanVLM outperforms the other three methods on all testing tasks.With the internal and external error correction mechanisms, the ReplanVLM achieves the highest average success rate.On the other hand, the average success rate of the ReplanVLM-internal is slightly lower than the Replan-VLM.Although the ReplanVLM-internal performs as well as the ReplanVLM on some tasks, it lags behind ReplanVLM on most tasks.In addition, the average success rate of the ReplanVLM-external is even lower than the ReplanVLMinternal, while the ReplanVLM-both performs the worst.</p>
<p>From the results of ablation experiments, it can be concluded that both the internal and external error correction mechanisms contribute substantially to the overall performance of the ReplanVLM framework.By removing any one of the error correction mechanisms, it will lead to a significant decrease in the success rate.By comparison, the impact of the external error correction mechanism on the overall framework is more pronounced than the internal error correction strategy.</p>
<p>D. Error Correction Experiment Analysis</p>
<p>Table V shows the results of the error correction experiment.As listed in Table V, the proposed framework gains an average error detection rate of 95.7%, with an average error correction rate of 80%.The error detection rate can reach up to 100% in the majority of tasks.Fig. 7 shows an example of the correction experiment.As it can be seen from the experimental results, the ReplanVLM framework can effectively detect errors and correct them during the task execution.Although there is a fluctuation in error correction rate across different tasks, the proposed framework shows strong potential in enhancing the accuracy and stability of robot task execution, especially in intricate environments.</p>
<p>E. Discussion</p>
<p>With the aid of the internal and external error correction mechanisms, the proposed ReplanVLM framework demonstrates an impressive performance on real robots and in simulation environments.It outperforms baseline methods in the comparative studies, with a better performance on all testing tasks.Either the internal error correction mechanism or the external error correction mechanism contributes significantly to the overall performance of the ReplanVLM framework.</p>
<p>Despite the prominent performance, this study has some limitations.First of all, this study only uses text and visual information for robot task planning.It cannot perceive other modal information such as touch and hearing.Secondly, although the LLM possesses broad world knowledge, it is difficult to handle complex manipulation tasks, such as cooking, through simple programming.Thirdly, the success rate of the task planning and execution still suffers from certain errors, such as the limitations in the VLM's visual processing and defects with the error correction mechanisms.In addition, the network latency caused by using the GPT-4V deteriorates the real-time performance of the overall system.The framework cannot apply to real-time systems.</p>
<p>VI. CONCLUSIONS</p>
<p>This paper proposed a ReplanVLM framework to enhance the success rate of robots in task planning and execution.By incorporating VLM techniques in robotic task planning, the ReplanVLM framework introduces internal and external error correction mechanisms that can effectively identify and rectify errors encountered during task execution.Extensive experiments in real-world and simulated environments were conducted to evaluate the effectiveness and robustness of the proposed method in various scenarios.Experimental results and comparative analysis demonstrate that the ReplanVLM framework can reliably detect and correct errors in diverse situations.The success rate of task planning and execution is substantially enhanced.</p>
<p>In the future, we will introduce some other perceptual information, such as tactile information, to enhance the framework to better understand the environment so that the task plans can be generated more accurately.In addition, we will investigate the potential of multimodal LLMs in completing complex manipulation tasks.</p>
<p>Fig. 1 .
1
Fig.1.ReplanVLM overview.We propose a ReplanVLM framework, which is composed of an internal error correction mechanism and an external error correction mechanism.The internal error correction mechanism refers to an Inner Bot, which is developed to assess and correct the task plans and codes generated by the Decision Bot.The external error correction mechanism involves an Extral Bot, which is used to determine whether the task is completed.If the task is deemed incomplete, the Extra Bot identifies the cause of the failure and sends it back to the Decision Bot to replan the task.</p>
<p>Fig. 4 .
4
Fig. 4. Overview of the system setup.</p>
<p>Fig. 5 .
5
Fig. 5. Example prompts in the ReplanVLM framework.</p>
<p>Fig. 6 .
6
Fig. 6.Experimental setup in the simulation environment.</p>
<p>Fig. 7 .
7
Fig. 7. Correction process.Taking Task 3 as an example, the image on the left shows the final position of the blocks.During the process of the robot placing the blue block, manual interference resulted in the blue block not being placed on top of the red block (Step 4).Subsequently, the VLM detected and corrected the error, ultimately completing the task.</p>
<p>TABLE IV RESULTS
IV
OF THE ABLATION EXPERIMENT Method Task 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7 Average
ReplanVLM100%90%100%80%100%90%100%94.2%ReplanVLM-internal90%90%100%60%90%90%90%87.1%ReplanVLM-external90%80%100%80%80%80%70%78.5%ReplanVLM-both90%60%90%60%80%60%40%68.6%TABLE VRESULTS OF THE ERROR CORRECTION EXPERIMENTTaskError detection rate Error correction rateTask 1100%90%Task 2100%80%Task 390%60%Task 480%60%Task 5100%100%Task 6100%80%Task 7100%90%Average95.7%80.0%</p>
<p>Large language models as zero-shot human models for human-robot interaction. B Zhang, H Soh, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023</p>
<p>Interactively robot action planning with uncertainty analysis and active questioning by large language model. K Hori, K Suzuki, T Ogata, 2024 IEEE/SICE International Symposium on System Integration (SII). IEEE2024</p>
<p>L3MVN: Leveraging large language models for visual target navigation. B Yu, H Kasaei, M Cao, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023</p>
<p>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. L Guan, K Valmeekam, S Sreedharan, S Kambhampati, Advances in Neural Information Processing Systems. 202436</p>
<p>A smart interactive camera robot based on large language models. Z Bao, G.-N Zhu, W Ding, Y Guan, W Bai, Z Gan, 2023 IEEE International Conference on Robotics and Biomimetics (ROBIO). IEEE2023</p>
<p>Planning with large language models via corrective re-prompting. S S Raman, V Cohen, E Rosen, I Idrees, D Paulius, S Tellex, NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022</p>
<p>Generalized planning in PDDL domains with pretrained large language models. T Silver, S Dan, K Srinivas, J B Tenenbaum, L P Kaelbling, M Katz, arXiv:2305.110142023arXiv preprint</p>
<p>Chat with the environment: Interactive multimodal perception using large language models. X Zhao, M Li, C Weber, M B Hafez, S Wermter, arXiv:2303.082682023arXiv preprint</p>
<p>HuggingGPT: Solving AI tasks with ChatGPT and its friends in hugging face. Y Shen, K Song, X Tan, D Li, W Lu, Y Zhuang, Advances in Neural Information Processing Systems. 202436</p>
<p>Tidybot: Personalized robot assistance with large language models. J Wu, R Antonova, A Kan, M Lepert, A Zeng, S Song, J Bohg, S Rusinkiewicz, T Funkhouser, arXiv:2305.056582023arXiv preprint</p>
<p>Physically grounded vision-language models for robotic manipulation. J Gao, B Sarkar, F Xia, T Xiao, J Wu, B Ichter, A Majumdar, D Sadigh, arXiv:2309.025612023arXiv preprint</p>
<p>Large language models for robotics: A survey. F Zeng, W Gan, Y Wang, N Liu, P S Yu, arXiv:2311.072262023arXiv preprint</p>
<p>GameVLM: A decisionmaking framework for robotic task planning based on visual language models and zero-sum games. A Mei, J Wang, G.-N Zhu, Z Gan, arXiv:2405.137512024arXiv preprint</p>
<p>Interactive task planning with language models. B Li, P Wu, P Abbeel, J Malik, arXiv:2310.106452023arXiv preprint</p>
<p>Enhancing robot task planning and execution through multi-layer large language models. Z Luan, Y Lai, R Huang, S Bai, Y Zhang, H Zhang, Q Wang, Sensors. 24516872024</p>
<p>VoxPoser: Composable 3d value maps for robotic manipulation with language models. W Huang, C Wang, R Zhang, Y Li, J Wu, F.-F Li, arXiv:2307.059732023arXiv preprint</p>
<p>Grounding classical task planners via vision-language models. X Zhang, Y Ding, S Amiri, H Yang, A Kaminski, C Esselink, S Zhang, arXiv:2304.085872023arXiv preprint</p>
<p>Look before you leap: Unveiling the power of GPT-4V in robotic vision-language planning. Y Hu, F Lin, T Zhang, L Yi, Y Gao, arXiv:2311.178422023arXiv preprint</p>
<p>Replan: Robotic replanning with perception and language models. M Skreta, Z Zhou, J L Yuan, K Darvish, A Aspuru-Guzik, A Garg, arXiv:2401.041572024arXiv preprint</p>
<p>Vision-language interpreter for robot task planning. K Shirai, C C Beltran-Hernandez, M Hamaya, A Hashimoto, S Tanaka, K Kawaharazuka, K Tanaka, Y Ushiku, S Mori, arXiv:2311.009672023arXiv preprint</p>
<p>N Wake, A Kanehira, K Sasabuchi, J Takamatsu, K Ikeuchi, arXiv:2311.12015GPT-4V (ision) for robotics: Multimodal task planning from human demonstration. 2023arXiv preprint</p>
<p>Do as I can, not as I say: Grounding language in robotic affordances. A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog, D Ho, J Ibarz, A Irpan, E Jang, R Julian, Conference on Robot Learning. PMLR2023</p>
<p>PROGPROMPT: Generating situated robot task plans using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023530</p>            </div>
        </div>

    </div>
</body>
</html>
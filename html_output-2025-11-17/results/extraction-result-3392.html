<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3392 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3392</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3392</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-89d857fdd842e2a632b76a8f38cf8d0f03aef46f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/89d857fdd842e2a632b76a8f38cf8d0f03aef46f" target="_blank">IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes IsoBench, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games, and presents two prompting techniques, $\textit{IsoCombination}$ and $\textit{IsoScratchPad}$, which improve model performance by considering combinations of, and translations between, different input representations.</p>
                <p><strong>Paper Abstract:</strong> Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 points worse. Finally, we present two prompting techniques, $\textit{IsoCombination}$ and $\textit{IsoScratchPad}$, which improve model performance by considering combinations of, and translations between, different input representations.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3392.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3392.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / GPT-4V / GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (including GPT-4V / GPT-4 Turbo multimodal variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>API-access multimodal foundation model family from OpenAI; instruction-tuned LLM backbone combined with a vision encoder in multimodal variants, evaluated zero-shot on IsoBench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (GPT-4V / GPT-4 Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative Pre-trained Transformer 4 family; the paper treats GPT-4 as the general term covering both the language-only GPT-4 and the multimodal GPT-4V/GPT-4 Turbo variants. These are instruction-tuned foundation models accessible via API, combining a large language model backbone with a vision encoder for image+text inputs. Models were zero-shot prompted on IsoBench.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Chess (winner identification and chess puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Two chess tasks: (1) Winner identification from final board states (three-way: White win / Black win / draw), and (2) chess puzzles asking for the best first move in a given position. Solving requires spatial reasoning about piece locations, move legality, tactical motifs (checks, captures), and board geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Graphical Board (image of the chessboard) and multiple textual isomorphic formats (Algebraic Notation Layout (ANL), Portable Game Notation (PGN), Forsyth-Edwards Notation (FEN)).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompting (models are instruction-tuned and evaluated zero-shot). The paper also explores IsoCombination (providing multiple isomorphic representations together) and IsoScratchPad (two-step: describe image -> solve from generated text) as augmentation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>IsoBench analysis shows GPT-4 multimodal variants perform substantially worse on image (graphical board) inputs than on isomorphic textual board encodings (FEN/PGN/ANL). The paper attributes failures to visual recognition errors (misreading pieces/positions), counting/comparison failures, and insufficient exploitation of low-level visual features due to coarse visual tokenization and suboptimal multimodal fusion; additionally, image-prompted responses are often more cursory and lack chain-of-thought reasoning compared to text prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as accuracy aggregated across the Games subject (which includes the two chess tasks). For Games (Image) GPT-4 Turbo: 27.6% accuracy; Games (Text) GPT-4 / GPT-4 variants: 42.8% accuracy. Dataset sizes for the chess tasks: winner identification = 257 games, chess puzzles = 200 puzzles (per Appendix A.3). The paper reports these aggregate image vs text accuracies rather than per-subtask breakdown in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Significant degradation on image-board inputs: misrecognition of pieces/locations, failure to count or compare multiple visual elements, omission of critical visual details in model-generated descriptions (when using IsoSP), and absence or truncation of chain-of-thought when given images. The multimodal fusion pipeline appears insufficient for eliciting capabilities that the LLM shows on textual encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Compared to other evaluated models, GPT-4 Turbo is the best performer on image representations in IsoBench's Games domain, but still substantially worse on images than on texts. Across models, textual representations outperform image representations; this runs counter to human picture-superiority effects (humans typically find visual board states easier), indicating multimodal models prefer textual encodings (e.g., FEN/PGN) for spatial-game reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3392.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3392.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 1.0 Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 1.0 Pro (Google multimodal foundation model family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal foundation model (Gemini Pro variant) evaluated via API on IsoBench; instruction-tuned to accept image+text inputs and text-only inputs, evaluated zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 1.0 Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gemini 1.0 Pro is a multimodal foundation model from Google that combines a large language model backbone with visual encoders in its vision-capable variants. In IsoBench it is evaluated via API in a zero-shot setting on both image and isomorphic text representations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Chess (winner identification and chess puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same chess tasks as above: determine game outcome from a final board or propose the best first move for a chess puzzle; tasks require spatial reasoning about piece placement, tactical patterns, and move legality.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Graphical Board (image) and textual encodings (ANL, PGN, FEN).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompts; the paper also evaluates IsoCombination and IsoScratchPad strategies generally (though reported IsoCB/IsoSP improvements are given for several domains).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Gemini shows the same modality gap: better performance on textual chess encodings than on graphical boards. The paper highlights visual recognition and multimodal fusion limitations (e.g., coarse image tokenization, loss of low-level features) as likely causes. Qualitatively, image-prompted outputs are less detailed and contain more vision-driven errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregated Games accuracies reported in Table 1: Games (Image) Gemini 1.0 Pro: 22.9% accuracy; Games (Text) Gemini 1.0 Pro: 34.9% accuracy. These are aggregated across the Games domain (winner identification + puzzles).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Fails more often on image-board inputs due to vision errors and incomplete visual descriptions; struggles with counting/comparing pieces visually and with eliciting chain-of-thought from image prompts. Multimodal fusion limitations degrade spatial reasoning from raw images.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Per IsoBench, Gemini underperforms GPT-4 Turbo on image representations and is outperformed by Claude-3 Opus on textual representations; like other models it prefers textual encodings, contrary to human picture-superiority expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3392.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3392.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3 Opus (Anthropic multimodal foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>API-access multimodal foundation model family from Anthropic; instruction-tuned and evaluated zero-shot on IsoBench image and text representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Claude-3 Opus is a multimodal foundation model from Anthropic combining an LLM backbone with vision capabilities in its multimodal variants; evaluated zero-shot on IsoBench across modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Chess (winner identification and chess puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Chess winner identification and chess puzzles requiring spatial understanding of piece positions, move generation, and evaluation of tactical patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Graphical chessboard images and textual representations (ANL, PGN, FEN).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompts; paper additionally explores IsoCombination and IsoScratchPad as strategies to improve image->text performance.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Claude-3 Opus exhibits a marked modality gap: performs substantially better on text encodings of chess states than on images. The paper documents vision-driven failure modes (visual recognition errors, missing visual features in generated descriptions), and notes that text prompts elicit more elaborate chain-of-thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregated Games accuracies in Table 1: Games (Image) Claude-3 Opus: 21.0% accuracy; Games (Text) Claude-3 Opus: 33.4% accuracy. Across all IsoBench problems, Claude-3 Opus shows the largest average text-image gap (+28.7 percentage points) among evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Visual misrecognition (pieces/locations), failure to count/compare pieces, omission of crucial visual details when attempting to describe images (IsoSP can lose information), and tendency toward curt answers (lack of chain-of-thought) on image prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Claude-3 Opus attains the best text-based performance on IsoBench overall but has larger degradation on images vs text than some other models; humans, by contrast, typically benefit from visual board representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3392.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3392.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral (Mixtral-8x7B open/accessible model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representative open-source/external model evaluated in IsoBench (Mixtral-8x7B); included for baseline comparisons alongside API-access models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An open / externally released mixture-of-experts style model (referenced in paper as Mixtral-8x7B) evaluated on IsoBench; instruction-tuned and assessed zero-shot where available.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Chess (winner identification and chess puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same chess tasks (winner identification, chess puzzles) requiring spatial reasoning over board positions, move legality, and tactics.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Textual representations (ANL/PGN/FEN) and in some cases image (graphical board) inputs — note: Table 1 shows Mixtral evaluated on text representations for Games (35.5%) and image column was not filled for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Mixtral shows poorer overall performance relative to API-access models and generally exhibits the same modality preference (text > image) when image evaluations are available; open-source models also tend to produce more default / superficial answers rather than detailed reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 1 reports Games (Text) Mixtral-8x7B: 35.5% accuracy on Games aggregated; image result for Games shown as '-' in Table 1 (not reported/evaluated or not available in that table cell).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Lower absolute performance, tendency to output default answers independent of input sample, and visual understanding failures where evaluated on images. Overall weaker chain-of-thought/explanatory generation compared to API-access models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Mixtral underperforms top API models (GPT-4 Turbo, Claude-3) on Games text evaluations and generally follows the same degraded image performance trend; humans still outperform all evaluated models on tasks requiring robust visual-spatial reasoning from raw board images.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MathVerse: Does your multi-modal llm truly see the diagrams in visual math problems? <em>(Rating: 2)</em></li>
                <li>How far are we from intelligent visual deductive reasoning? <em>(Rating: 2)</em></li>
                <li>MathVista: Evaluating mathematical reasoning of foundation models in visual contexts <em>(Rating: 1)</em></li>
                <li>MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3392",
    "paper_id": "paper-89d857fdd842e2a632b76a8f38cf8d0f03aef46f",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "GPT-4 / GPT-4V / GPT-4 Turbo",
            "name_full": "GPT-4 (including GPT-4V / GPT-4 Turbo multimodal variants)",
            "brief_description": "API-access multimodal foundation model family from OpenAI; instruction-tuned LLM backbone combined with a vision encoder in multimodal variants, evaluated zero-shot on IsoBench.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (GPT-4V / GPT-4 Turbo)",
            "model_description": "Generative Pre-trained Transformer 4 family; the paper treats GPT-4 as the general term covering both the language-only GPT-4 and the multimodal GPT-4V/GPT-4 Turbo variants. These are instruction-tuned foundation models accessible via API, combining a large language model backbone with a vision encoder for image+text inputs. Models were zero-shot prompted on IsoBench.",
            "model_size": null,
            "puzzle_name": "Chess (winner identification and chess puzzles)",
            "puzzle_description": "Two chess tasks: (1) Winner identification from final board states (three-way: White win / Black win / draw), and (2) chess puzzles asking for the best first move in a given position. Solving requires spatial reasoning about piece locations, move legality, tactical motifs (checks, captures), and board geometry.",
            "input_representation": "Graphical Board (image of the chessboard) and multiple textual isomorphic formats (Algebraic Notation Layout (ANL), Portable Game Notation (PGN), Forsyth-Edwards Notation (FEN)).",
            "prompting_method": "Zero-shot instruction prompting (models are instruction-tuned and evaluated zero-shot). The paper also explores IsoCombination (providing multiple isomorphic representations together) and IsoScratchPad (two-step: describe image -&gt; solve from generated text) as augmentation strategies.",
            "spatial_reasoning_analysis": "IsoBench analysis shows GPT-4 multimodal variants perform substantially worse on image (graphical board) inputs than on isomorphic textual board encodings (FEN/PGN/ANL). The paper attributes failures to visual recognition errors (misreading pieces/positions), counting/comparison failures, and insufficient exploitation of low-level visual features due to coarse visual tokenization and suboptimal multimodal fusion; additionally, image-prompted responses are often more cursory and lack chain-of-thought reasoning compared to text prompts.",
            "performance_metrics": "Reported as accuracy aggregated across the Games subject (which includes the two chess tasks). For Games (Image) GPT-4 Turbo: 27.6% accuracy; Games (Text) GPT-4 / GPT-4 variants: 42.8% accuracy. Dataset sizes for the chess tasks: winner identification = 257 games, chess puzzles = 200 puzzles (per Appendix A.3). The paper reports these aggregate image vs text accuracies rather than per-subtask breakdown in the main text.",
            "limitations_or_failure_modes": "Significant degradation on image-board inputs: misrecognition of pieces/locations, failure to count or compare multiple visual elements, omission of critical visual details in model-generated descriptions (when using IsoSP), and absence or truncation of chain-of-thought when given images. The multimodal fusion pipeline appears insufficient for eliciting capabilities that the LLM shows on textual encodings.",
            "comparison_to_other_models_or_humans": "Compared to other evaluated models, GPT-4 Turbo is the best performer on image representations in IsoBench's Games domain, but still substantially worse on images than on texts. Across models, textual representations outperform image representations; this runs counter to human picture-superiority effects (humans typically find visual board states easier), indicating multimodal models prefer textual encodings (e.g., FEN/PGN) for spatial-game reasoning.",
            "uuid": "e3392.0",
            "source_info": {
                "paper_title": "IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Gemini 1.0 Pro",
            "name_full": "Gemini 1.0 Pro (Google multimodal foundation model family)",
            "brief_description": "A multimodal foundation model (Gemini Pro variant) evaluated via API on IsoBench; instruction-tuned to accept image+text inputs and text-only inputs, evaluated zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini 1.0 Pro",
            "model_description": "Gemini 1.0 Pro is a multimodal foundation model from Google that combines a large language model backbone with visual encoders in its vision-capable variants. In IsoBench it is evaluated via API in a zero-shot setting on both image and isomorphic text representations.",
            "model_size": null,
            "puzzle_name": "Chess (winner identification and chess puzzles)",
            "puzzle_description": "Same chess tasks as above: determine game outcome from a final board or propose the best first move for a chess puzzle; tasks require spatial reasoning about piece placement, tactical patterns, and move legality.",
            "input_representation": "Graphical Board (image) and textual encodings (ANL, PGN, FEN).",
            "prompting_method": "Zero-shot instruction prompts; the paper also evaluates IsoCombination and IsoScratchPad strategies generally (though reported IsoCB/IsoSP improvements are given for several domains).",
            "spatial_reasoning_analysis": "Gemini shows the same modality gap: better performance on textual chess encodings than on graphical boards. The paper highlights visual recognition and multimodal fusion limitations (e.g., coarse image tokenization, loss of low-level features) as likely causes. Qualitatively, image-prompted outputs are less detailed and contain more vision-driven errors.",
            "performance_metrics": "Aggregated Games accuracies reported in Table 1: Games (Image) Gemini 1.0 Pro: 22.9% accuracy; Games (Text) Gemini 1.0 Pro: 34.9% accuracy. These are aggregated across the Games domain (winner identification + puzzles).",
            "limitations_or_failure_modes": "Fails more often on image-board inputs due to vision errors and incomplete visual descriptions; struggles with counting/comparing pieces visually and with eliciting chain-of-thought from image prompts. Multimodal fusion limitations degrade spatial reasoning from raw images.",
            "comparison_to_other_models_or_humans": "Per IsoBench, Gemini underperforms GPT-4 Turbo on image representations and is outperformed by Claude-3 Opus on textual representations; like other models it prefers textual encodings, contrary to human picture-superiority expectations.",
            "uuid": "e3392.1",
            "source_info": {
                "paper_title": "IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Claude-3 Opus",
            "name_full": "Claude-3 Opus (Anthropic multimodal foundation model)",
            "brief_description": "API-access multimodal foundation model family from Anthropic; instruction-tuned and evaluated zero-shot on IsoBench image and text representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude-3 Opus",
            "model_description": "Claude-3 Opus is a multimodal foundation model from Anthropic combining an LLM backbone with vision capabilities in its multimodal variants; evaluated zero-shot on IsoBench across modalities.",
            "model_size": null,
            "puzzle_name": "Chess (winner identification and chess puzzles)",
            "puzzle_description": "Chess winner identification and chess puzzles requiring spatial understanding of piece positions, move generation, and evaluation of tactical patterns.",
            "input_representation": "Graphical chessboard images and textual representations (ANL, PGN, FEN).",
            "prompting_method": "Zero-shot instruction prompts; paper additionally explores IsoCombination and IsoScratchPad as strategies to improve image-&gt;text performance.",
            "spatial_reasoning_analysis": "Claude-3 Opus exhibits a marked modality gap: performs substantially better on text encodings of chess states than on images. The paper documents vision-driven failure modes (visual recognition errors, missing visual features in generated descriptions), and notes that text prompts elicit more elaborate chain-of-thought reasoning.",
            "performance_metrics": "Aggregated Games accuracies in Table 1: Games (Image) Claude-3 Opus: 21.0% accuracy; Games (Text) Claude-3 Opus: 33.4% accuracy. Across all IsoBench problems, Claude-3 Opus shows the largest average text-image gap (+28.7 percentage points) among evaluated models.",
            "limitations_or_failure_modes": "Visual misrecognition (pieces/locations), failure to count/compare pieces, omission of crucial visual details when attempting to describe images (IsoSP can lose information), and tendency toward curt answers (lack of chain-of-thought) on image prompts.",
            "comparison_to_other_models_or_humans": "Claude-3 Opus attains the best text-based performance on IsoBench overall but has larger degradation on images vs text than some other models; humans, by contrast, typically benefit from visual board representations.",
            "uuid": "e3392.2",
            "source_info": {
                "paper_title": "IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Mixtral-8x7B",
            "name_full": "Mixtral (Mixtral-8x7B open/accessible model)",
            "brief_description": "Representative open-source/external model evaluated in IsoBench (Mixtral-8x7B); included for baseline comparisons alongside API-access models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7B",
            "model_description": "An open / externally released mixture-of-experts style model (referenced in paper as Mixtral-8x7B) evaluated on IsoBench; instruction-tuned and assessed zero-shot where available.",
            "model_size": "8x7B",
            "puzzle_name": "Chess (winner identification and chess puzzles)",
            "puzzle_description": "Same chess tasks (winner identification, chess puzzles) requiring spatial reasoning over board positions, move legality, and tactics.",
            "input_representation": "Textual representations (ANL/PGN/FEN) and in some cases image (graphical board) inputs — note: Table 1 shows Mixtral evaluated on text representations for Games (35.5%) and image column was not filled for some tasks.",
            "prompting_method": "Zero-shot instruction prompting.",
            "spatial_reasoning_analysis": "Mixtral shows poorer overall performance relative to API-access models and generally exhibits the same modality preference (text &gt; image) when image evaluations are available; open-source models also tend to produce more default / superficial answers rather than detailed reasoning.",
            "performance_metrics": "Table 1 reports Games (Text) Mixtral-8x7B: 35.5% accuracy on Games aggregated; image result for Games shown as '-' in Table 1 (not reported/evaluated or not available in that table cell).",
            "limitations_or_failure_modes": "Lower absolute performance, tendency to output default answers independent of input sample, and visual understanding failures where evaluated on images. Overall weaker chain-of-thought/explanatory generation compared to API-access models.",
            "comparison_to_other_models_or_humans": "Mixtral underperforms top API models (GPT-4 Turbo, Claude-3) on Games text evaluations and generally follows the same degraded image performance trend; humans still outperform all evaluated models on tasks requiring robust visual-spatial reasoning from raw board images.",
            "uuid": "e3392.3",
            "source_info": {
                "paper_title": "IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MathVerse: Does your multi-modal llm truly see the diagrams in visual math problems?",
            "rating": 2
        },
        {
            "paper_title": "How far are we from intelligent visual deductive reasoning?",
            "rating": 2
        },
        {
            "paper_title": "MathVista: Evaluating mathematical reasoning of foundation models in visual contexts",
            "rating": 1
        },
        {
            "paper_title": "MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi",
            "rating": 1
        }
    ],
    "cost": 0.013634,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations</h1>
<p>Deqing Fu ${ }^{\text {® }}$ Ruohao Guo ${ }^{\text {® }}$ Ghazal Khalighinejad ${ }^{\text {® }}$ Ollie Liu ${ }^{\text {® }}$,<br>Bhuwan Dhingra ${ }^{\text {A }}$ Dani Yogatama ${ }^{\text {® }}$ Robin Jia ${ }^{\text {® }}$ Willie Neiswanger ${ }^{\text {® }}$<br>${ }^{\text {® }}$ USC ${ }^{\text {® }}$ Georgia Tech ${ }^{\text {® }}$ Duke</p>
<h4>Abstract</h4>
<p>Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose IsoBench, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple isomorphic representations of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 points worse. Finally, we present two prompting techniques, IsoCombination and IsoScratchPad, which improve model performance by considering combinations of, and translations between, different input representations.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Do multimodal foundation models treat every modality equally? In this example, a model is provided with either an image representation or a text representation isomorphic to the image, where the instructions are kept identical. Surprisingly, multimodal models often give different responses for these isomorphic inputs (e.g., in the figure above, only the response to the text representation is correct). In IsoBench, we scale such examples into four domains (Math, Science, Algorithms, Games) and find a consistent preference towards text across many popular multimodal foundation models.</p>
<h2>1 Introduction</h2>
<p>On the heels of the large language model (LLM) revolution, we are currently witnessing a second revolution of multimodal foundation models. Exemplified by models like GPT4V (OpenAI, 2023b), Claude (Anthropic, 2024), and Gemini (Google, 2023), these models</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: IsoBench contains four major domains: Mathematical Functions, Science Questions, Graph Algorithms, and Chess Games. For each domain, there are two or three subtasks. All examples within IsoBench are provided with one image representation and several textual representations that are isomorphic to each other.
combine a large language model backbone with a vision encoder that enables them to accept either pure text inputs or a combination of text and images. The rapid rise of these multimodal models necessitates new benchmarks that accurately assess their capabilities.
In this work, we study whether these models exhibit the same capabilities when processing text and image inputs. Our key insight is that many problems consisting of both image and text prompts can be equivalently formulated with text-only prompts. By studying how well models do on both representations of the same problem, we can evaluate whether the multimodal fusion components of these models truly empower the model with the same capabilities when reasoning about images and text. While many benchmarks already exist for testing multimodal foundation models (Yue et al., 2023; Lu et al., 2024; Zhang et al., 2024a; Fan et al., 2024; Zhang et al., 2024b), none of these measure models' performance discrepancies between semantically equivalent inputs in different modalities.
We create IsoBench, a broad-coverage benchmark for evaluating multimodal foundation models on 10 different tasks with multiple isomorphic representations. IsoBench includes four major subjects: mathematical functions, algorithmic problems, science questions, and chess games. For each test example, IsoBench provides one image representation and one or more alternative textual representations. We evaluate a suite of multimodal foundation models, including GPT-4 ${ }^{1}$, Gemini, and Claude-3, on IsoBench, and find that all tested models perform substantially better when given textual representations compared with visual representations. This bias in favor of textual representations runs counter to human cognition: humans are known to exhibit a picture superiority effect (Defeyter et al., 2009), a preference towards visual representations, and humans generate visual images internally regardless of whether the required task is visual or verbal (Amit et al., 2017).
Finally, we design two simple mechanisms to further study the performance of multimodal foundation models on tasks with isomorphic representations: IsoCombination (IsoCB), which feeds multiple isomorphic representations to the model, and IsoScratchPad (IsoSP), which uses a scratchpad to translate between visual and text representations. We find that both</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Subject</th>
<th style="text-align: left;">Representation</th>
<th style="text-align: left;">GPT-4 <br> Turbo</th>
<th style="text-align: left;">Gemini 1.0 <br> Pro</th>
<th style="text-align: left;">Claude-3 <br> Opus</th>
<th style="text-align: left;">Mixtral- <br> 8x7B</th>
<th style="text-align: left;">Random <br> Guess</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Science</td>
<td style="text-align: left;">Image</td>
<td style="text-align: left;">72.0</td>
<td style="text-align: left;">66.7</td>
<td style="text-align: left;">71.3</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">38.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Text</td>
<td style="text-align: left;">86.7</td>
<td style="text-align: left;">69.3</td>
<td style="text-align: left;">89.3</td>
<td style="text-align: left;">68.0</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Mathematics</td>
<td style="text-align: left;">Image</td>
<td style="text-align: left;">46.9</td>
<td style="text-align: left;">36.4</td>
<td style="text-align: left;">41.7</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">44.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Text</td>
<td style="text-align: left;">76.6</td>
<td style="text-align: left;">73.9</td>
<td style="text-align: left;">85.8</td>
<td style="text-align: left;">66.1</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Algorithms</td>
<td style="text-align: left;">Image</td>
<td style="text-align: left;">54.2</td>
<td style="text-align: left;">37.0</td>
<td style="text-align: left;">39.1</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">34.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Text</td>
<td style="text-align: left;">69.5</td>
<td style="text-align: left;">43.8</td>
<td style="text-align: left;">73.7</td>
<td style="text-align: left;">49.5</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Games</td>
<td style="text-align: left;">Image</td>
<td style="text-align: left;">27.6</td>
<td style="text-align: left;">22.9</td>
<td style="text-align: left;">21.0</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">18.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Text</td>
<td style="text-align: left;">42.8</td>
<td style="text-align: left;">34.9</td>
<td style="text-align: left;">33.4</td>
<td style="text-align: left;">35.5</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: left;">Image</td>
<td style="text-align: left;">50.2</td>
<td style="text-align: left;">40.7</td>
<td style="text-align: left;">43.3</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">33.9</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Text</td>
<td style="text-align: left;">68.9</td>
<td style="text-align: left;">55.6</td>
<td style="text-align: left;">72.0</td>
<td style="text-align: left;">57.2</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Delta (Text - Image)</td>
<td style="text-align: left;">+18.7</td>
<td style="text-align: left;">+14.9</td>
<td style="text-align: left;">+28.7</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 1: IsoBench results. Scores are evaluated as accuracy. Within each subject, the best scores in processing image representations are highlighted in blue, and the best in text representations are in red. Overall, GPT-4 Turbo is the best for images and Claude-3 Opus is the best for text. Across four subjects within IsoBench, multimodal foundation models have a strong preference for text modalities. The gap in accuracy between the best text representation and its isomorphic image representation can be as large as $28.7 \%$.
methods improve aspects of multimodal models, with IsoCB improving model performance on graph algorithm problems by up to 9.4 points compared with the best single representation, and IsoSP improving performance on science problems by up to 14.4 points.
In full, we summarize our contributions as follows:</p>
<ul>
<li>We introduce IsoBench, a test dataset consisting of 1,887 samples $^{2}$ spanning diverse domains such as discrete and applied mathematics, physics, chemistry, and chess. For each sample, we evaluate multiple isomorphic input representations containing the same information-one with a visual representation and others with domain-specific textual representations-to facilitate multimodal performance assessments.</li>
<li>We benchmark eight popular foundation models, and find that each of the tested multimodal models perform substantially better on text-only prompts than imagebased prompts, in contrast with known human preferences for images over text.</li>
<li>To bridge performance discrepancies between different input modalities, we introduce IsoCombination (IsoCB) and IsoScratchPad (IsoSP), two procedures that, respectively, fuse input modalities and transform visual input into textual representation at inference time. In certain settings, we find that IsoCB and IsoSP can improve the performance of multimodal foundation models by nearly 10 percentage points.</li>
</ul>
<h1>2 IsoBench</h1>
<p>IsoBench is a collection of problems of the form $\left(\mathcal{P},\left{\mathcal{R}<em m="m">{1}, \cdots, \mathcal{R}</em>}\right}\right)$, where $\mathcal{P}$ is a natural language instruction of task, and $\mathcal{R<em i="i">{i}$ 's are different representations of the sample. Ideally, these representations are isomorphic to each other, i.e. for any pairs of input $\mathcal{R}</em>}, \mathcal{R<em i="i">{j}$, there exists a bijective function $\phi$ such that $\phi\left(R</em>$ is selected). We now detail the construction of IsoBench.}\right)=R_{j}$. As illustrated in Figure 1, the prompts to foundation models share the same instructions except for the problem descriptions (where one of the representations $\mathcal{R}_{j</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Image</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.7</td>
</tr>
<tr>
<td style="text-align: center;">Even/Odd <br> Function</td>
<td style="text-align: center;">${ }^{\text {I }}$ ITEX</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Code</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Image</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">35.2</td>
</tr>
<tr>
<td style="text-align: center;">Convex/Concave <br> Function</td>
<td style="text-align: center;">${ }^{\text {I }}$ ITEX</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Code</td>
<td style="text-align: center;">66.02</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Image</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: center;">Count <br> Breakpoints</td>
<td style="text-align: center;">${ }^{\text {I }}$ ITEX</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Code</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 2: Benchmark results on Mathematics problems. We report accuracy scores. For each model, the best score of each task is highlighted in bold.</p>
<h1>2.1 Mathematics</h1>
<p>Our first evaluation suite consists of problems in continuous mathematics. Foundation models are increasingly deployed as assistants for data science applications (Cheng et al., 2023), and they must perform competently on plot understanding to provide helpful insights. We evaluate this capability by prompting foundation models to extract mathematical properties from continuous functions.</p>
<p>Formats. We consider three input formats: Image, Text with ${ }^{\text {I }}$ ITEX, and Text with Code. We first sample candidate functions and convert them to their ${ }^{\text {I }}$ ITEXand sympy representations. Then, we use matplotlib to plot the functions. In practice, it can be difficult to exactly map these images back to their corresponding functions, so the authors manually filtered data to ensure that the properties to be tested are clearly visible in the images.</p>
<p>Tasks. We consider three tasks: classifying parity (i.e. even/odd/neither) functions, convex/concave functions, as well as counting the number of breakpoints in piecewise linear functions (either 2 or 3). These problems are designed to be visually trivial for humans to solve, but requires some mathematical maturity to answer analytically. We sample 128 samples for each class, culminating to 896 total samples. We refer readers to Appendix A. 1 for additional details and Appendix B for sample prompts and responses.</p>
<h3>2.2 Games</h3>
<p>A critical area for enhancing the strategic capabilities of foundation models lies in understanding game strategies. We focus on two tasks in the game of chess: winner identification and chess puzzle solving. For both tasks, we use data from the Lichess database ${ }^{3}$.</p>
<p>Formats. For our chess game dataset, we consider four representations: Graphical Board (Image), Algebraic Notation Layout (ANL), Portable Game Notation (PGN), and ForsythEdwards Notation (FEN). The Graphical Board is an image of the chessboard, whereas ANL, PGN, and FEN are all text-based representations of the game state. For additional details, please see Appendix A.3.</p>
<p>Tasks. We consider two tasks: winner identification from final board states and chess puzzles. In the winner identification task, the objective is to analyze a given representation of a chess game and determine the outcome: whether the game resulted in a win for White, a win for Black, or a draw. Our dataset for this task includes 257 games that lasted more than nine rounds in February 2024. In the chess puzzles task, a chess puzzle is given and the objective is to find the best first move. The dataset for this task comprises 200 puzzles.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Table 3: Benchmark results on chess Games problems. We report accuracy scores. For each model, the best score of each task is highlighted in bold.</p>
<h1>2.3 Algorithms</h1>
<p>To provide concrete planning advice, foundation models need to reason algorithmically. We focus on graph algorithms, which are relevant in many realistic scenarios. For example, if asked to purchase flight tickets from Los Angeles to Vienna, foundation models need to (1) enumerate all possible choices with possible connection stops in between, and (2) check the lowest ticket prices among all choices. Fundamentally, the first task is a graph connectivity problem and the second is a weighted shortest path problem.
In this section, we investigate three main graph algorithm problems with increasing complexity: graph connectivity, maximum flow, and graph isomorphism.</p>
<p>Formats. For graph algorithms, we consider three representations: 1. Image, where graphs are visualized with networkx package with random styles; 2. Text with $\mathbf{I S T}_{\mathbf{E}} \mathbf{X}$, where the adjacency matrix is chosen to be the mathematical representation of graphs; 3. Text with story or description, where the graph problems are described as a story-telling version of the scenario (e.g., formulating the graph connectivity problem as determining whether it is possible to drive from one city to another).</p>
<p>Tasks. We consider three tasks. Graph Connectivity requires deciding whether two query nodes are connected in an undirected graph. Maximum Flow requires computing the maximum flow one can send from a given source node to a given sink node, within a weighed directed graph. Finally, Graph Isomorphism requires determining whether two provided graphs with the same number of nodes are isomorphic (i.e., whether there exists a bijection between nodes that preserves all edges). Each task contains 128 problems with details in Appendix A.2.</p>
<h3>2.4 Science</h3>
<p>Formats. We consider two input formats for science questions: Image and Text. For the Image representation, each sample includes a textual question and multiple choices, along with a figure that provides additional context. To get the corresponding isomorphic Text inputs, one author manually wrote descriptions for each figure. ${ }^{4}$ The annotator avoided introducing any extra reasoning or information beyond what is depicted in the figures. Isomorphic examples of image and text representation are provided in Appendix Table 7.</p>
<p>Tasks. We compiled a dataset consisting of 75 Chemistry and 75 Physics questions. We first selected questions from the ScienceQA (Lu et al., 2022) test set, choosing only questions</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 4: Benchmark results on graph Algorithms problems. For connectivity and isomorphism, binary classification accuracy is reported. For Maxflow, an exact match accuracy of total flows is reported. For each model, the best score of each task is highlighted in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Image</td>
<td style="text-align: left;">74.7</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">61.3</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">70.7</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">42.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Text</td>
<td style="text-align: left;">$\mathbf{8 6 . 7}$</td>
<td style="text-align: left;">84.0</td>
<td style="text-align: left;">$\mathbf{6 9 . 3}$</td>
<td style="text-align: left;">76.0</td>
<td style="text-align: left;">$\mathbf{8 9 . 3}$</td>
<td style="text-align: left;">68.0</td>
<td style="text-align: left;">61.3</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Physics</td>
<td style="text-align: left;">Image</td>
<td style="text-align: left;">69.3</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">72.0</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">72.0</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">45.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Text</td>
<td style="text-align: left;">$\mathbf{9 4 . 7}$</td>
<td style="text-align: left;">76.0</td>
<td style="text-align: left;">$\mathbf{8 5 . 3}$</td>
<td style="text-align: left;">84.0</td>
<td style="text-align: left;">$\mathbf{9 0 . 7}$</td>
<td style="text-align: left;">65.3</td>
<td style="text-align: left;">73.3</td>
<td style="text-align: left;">-</td>
</tr>
</tbody>
</table>
<p>Table 5: Benchmark results on Science problems. We report accuracy scores. For each model, the best score of each task is highlighted in bold.
whose answers rely on understanding the corresponding figures. This process resulted in a total of 50 chemistry and 60 physics questions. To enhance the diversity of the science dataset, we manually added 25 new chemistry questions and 15 new physics questions, introducing three new categories distinct from those in the ScienceQA dataset. The category distribution of our entire science dataset is detailed in Appendix Figure 4, illustrating the wide range of science questions covered.</p>
<h1>3 Performance Analysis</h1>
<p>We evaluate IsoBench on API-access multimodal foundation models, such as GPT-4V (gpt-4-0125-preview and gpt-4-vision-preview), Claude 3 (claude-3-opus-20240229), and Gemini Pro (gemini-1.0-pro and gemini-pro-vision). We also benchmark on APIaccess single-modal large language models for reference, such as GPT-3.5 Turbo (OpenAI, 2023a, gpt-3.5-turbo-0125) and PaLM-2 (Anil et al., 2023, text-bison-001). We evaluted representative open-source models such as Mixtral 8x7B (Jiang et al., 2024), LLaMa-2 70B (Touvron et al., 2023), and LLaVA-1.5 7B (Liu et al., 2023a) as well. All models we evaluate are instruction-tuned models and are zero-shot prompted. We present fine-grained results on these tasks in Tables 2 to 5, and provide sample responses in Figures 5 to 17.</p>
<p>Overview. We observe that, across all models that admit multimodal representations, language-only prompts almost always outperform vision-language prompts. On language prompts, API-access models exhibit strong reasoning capabilities (e.g. Figure 13). They generally outperform open-sourced models, which exhibit a tendency to generate a default</p>
<p>answer regardless of input samples. We now delineate several intriguing observations. We present aggregated IsoBench results in Table 1 with details on more models in Table 8.</p>
<p>Vision models may be insufficient. In tasks that require explicit enumeration of objects such as breakpoint and chemistry (e.g. Figures 12 and 17), we observe significant performance degradation due to counting errors, which has been a long-standing problem for vision models (Liu et al., 2022). In fact, leveraging a sub-optimal feature extractor for one modality may hurt the performance of a multimodal model; and the design of an optimal model requires nuanced understanding of the interaction effects between modalities (Liang et al., 2024). Thomason et al. (2018) also showed unimodal models can even outperform their multimodal counterparts, and it suggests that models can often learn a shortcut only relying on language model part and perform weaker when forced to use only images. Overall, visual recognition errors, such as Figures 6, 8 and 12, are prevalent in failure cases, and often misdirect natural language generation towards incorrect conclusions.</p>
<p>Models cannot utilize low-level visual features for generation. Perhaps surprisingly, vision-language foundation models are far from perfect for convexity problems, which mostly feature simple, smooth curves, even though curve detectors (Olah et al., 2020) are among the most observed and well-studied phenomena in the mechanistic understanding of vision models. This inconsistency suggests that, even if performing certain tasks is within a vision model's capabilities, the current multimodal fusion scheme is insufficient for eliciting them. We posit that popular approaches such as image-tokenization only offer a coarse-grained representation that summarizes high-level features, and may be unsuitable for detailed analysis such as plot and chart understanding.</p>
<p>LLM backbones may not be exempt from blame. Recent works (Gonen et al., 2022; Razeghi et al., 2022; Han \&amp; Tsvetkov, 2022) have reported language models to prefer textual formats that are more common in the pre-training data, which echo our observation in performance variability between different language-only inputs. While formats do not fully explain this discrepancy, we hypothesize that the imbalance between visual and input data, and in general the sub-optimal robustness of LLMs in the face of less common distribution can contribute to the performance gap between visual and language inputs.</p>
<p>Lastly, we observe generated responses of vision-language prompts to be cursory compared to those of language-only prompts; and they sometimes contain a direct answer instead of a reasoning process, despite being instructed to perform chain-of-thought reasoning. To mitigate this difference, we discuss two simple strategies that aims to augment a visionlanguage prompt with better reasoning capabilities of language prompts, presented next.</p>
<h1>4 IsoCombination and IsoScratchPad</h1>
<p>In many scientific fields, the images are complicated, researchers usually provide extra text inputs to help the model. This is also a common practice in scientific fields, where figures and tables in most papers are coupled with captions (for example, this paper), providing descriptions and takeaways. Motivated by these and based on the observations made above on IsoBench, we can design two simple and deliberately contrived mechanisms to further study the performance of multimodal foundation models on tasks with isomorphic representations: IsoCombination (IsoCB) and IsoScratchPad (IsoSP). IsoCB feeds multiple isomorphic representations to the multimodal model simultaneously in order to see the combined effect of using multiple representations on performance. Alternatively, IsoSP uses a scratchpad to explicitly translate from visual to text representations (and then uses this text representation to solve a task), taking advantage of the empirically higher-performing text representations for visual data. In particular, for IsoSP we employ a two-step prompting strategy: initially, the model receives a prompt featuring a visual representation, which it is tasked to translate into a textual format; then, the model is prompted with this generated text representation to predict the output. We illustrate both methods in Figure 3.</p>
<p>We show results for IsoCB and IsoSP in Table 6, along with results for image-only and text-only representations. Interestingly, we find that IsoSP, which converts from image to</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Illustration of IsoCombination (IsoCB) and IsoScratchPad (IsoSP). IsoCB combines all representations provided by a user and constructs one unified prompt for a foundation model. IsoSP is a two-step prompting method, where a foundation model first describes an image and then uses the textual description as the sole representation for a given task.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-4 Turbo</th>
<th style="text-align: center;">Gemini 1.0 Pro</th>
<th style="text-align: center;">Claude-3 Opus</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Maxflow</td>
<td style="text-align: center;">Image</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">12.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Text (Best)</td>
<td style="text-align: center;">$56.3(+19.5)$</td>
<td style="text-align: center;">$19.5(+6.3)$</td>
<td style="text-align: center;">$73.4(+60.9)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IsoCB</td>
<td style="text-align: center;">$65.6(+28.9)$</td>
<td style="text-align: center;">$21.9(+8.6)$</td>
<td style="text-align: center;">$75.0(+62.5)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IsoSP</td>
<td style="text-align: center;">$52.3(+15.6)$</td>
<td style="text-align: center;">$19.5(+6.3)$</td>
<td style="text-align: center;">$22.7(+10.2)$</td>
</tr>
<tr>
<td style="text-align: center;">Connectivity</td>
<td style="text-align: center;">Image</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">53.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">$95.1(+19.3)$</td>
<td style="text-align: center;">$75.0(+25.8)$</td>
<td style="text-align: center;">$94.5(+41.4)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IsoCB</td>
<td style="text-align: center;">$83.6(+7.8)$</td>
<td style="text-align: center;">$52.3(+1.6)$</td>
<td style="text-align: center;">$85.2(+32.0)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IsoSP</td>
<td style="text-align: center;">$82.0(+6.3)$</td>
<td style="text-align: center;">$51.6(+0.8)$</td>
<td style="text-align: center;">$63.3(+10.1)$</td>
</tr>
<tr>
<td style="text-align: center;">Physics QA</td>
<td style="text-align: center;">Image</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">70.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">$86.7(+12.0)$</td>
<td style="text-align: center;">$69.3(+8.0)$</td>
<td style="text-align: center;">$89.3(+18.7)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IsoCB</td>
<td style="text-align: center;">$88.0(+13.3)$</td>
<td style="text-align: center;">$70.7(+9.3)$</td>
<td style="text-align: center;">$88.0(+17.3)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IsoSP</td>
<td style="text-align: center;">$84.0(+9.3)$</td>
<td style="text-align: center;">$76.0(+14.7)$</td>
<td style="text-align: center;">$78.7(+8.0)$</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry QA</td>
<td style="text-align: center;">Image</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">72.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">$94.7(+25.3)$</td>
<td style="text-align: center;">$85.3(+13.3)$</td>
<td style="text-align: center;">$90.7(+18.8)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IsoCB</td>
<td style="text-align: center;">$92.0(+22.7)$</td>
<td style="text-align: center;">$82.7(+20.7)$</td>
<td style="text-align: center;">$89.3(+27.3)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IsoSP</td>
<td style="text-align: center;">$88.0(+18.7)$</td>
<td style="text-align: center;">$84.0(+12.0)$</td>
<td style="text-align: center;">$73.3(+1.3)$</td>
</tr>
</tbody>
</table>
<p>Table 6: IsoCombination and IsoScratchPad results. Best methods are highlighted in red and improvements over image-only prompts are in (green). We find that both methods improve performance in comparison with image representations, and for certain domains, IsoCombination additionally improves performance relative to text representations.
text, typically outperforms image-only representations (and thus it could potentially be used to improve performance given only image representations for some tasks). However, compared to direct text representations, IsoSP tends to fall short. Our manual review of IsoSP and text results provides a possible explanation: the model has difficulty understanding images, especially in tasks that require counting items or comparing numbers visually. The larger the performance gap between IsoSP and Text representations, the harder it becomes for the model to understand images. Another possible reason for IsoSP's underperformance relative to Text representations could be that, the model-generated interpretation of images tends to miss critical information for reasoning. For example, to determine which pair of</p>
<p>magnets in the given image has a larger magnetic force in a physics question, it is essential to consider both the distance between the magnets and their sizes. Yet, the descriptions generated by models like Claude-3 recognize only the distance, neglecting the magnet sizes, and thus leading to an incorrect answer. Additionally, we find that IsoCB, which combines representations, typically outperforms image-only representations, and even text-only representations on certain tasks.</p>
<h1>5 Related Works</h1>
<p>Vision-Language Foundation Models. There has been a plethora of recent developments in vision-language foundation models; they can be broadly categorized by their methods for representing visual modalities. A representative approach involves tokenizing visual inputs to be jointly trained with language inputs (Yu et al., 2023; Google, 2023; McKinzie et al., 2024; Chen et al., 2022; 2023; Team, 2024, inter alia). Another line of work processes continuous visual features by directly projecting them to the language embedding space via a learnable function (Liu et al., 2023b;a; 2024a; Bavishi et al., 2023). While the design choices of some API-access models (OpenAI, 2023b; Anthropic, 2024; Google, 2023; Reka, 2024) remain largely unknown, most performant models use early fusion, the practice of integrating visual and language features at the input level of an autoregressive model. At the core of these design choices is the hardness in representing visual features, which has been reported by several early studies (McKinzie et al., 2024) to be the key bottleneck towards performant vision-language foundation models. IsoBench aims to supplement prior works, which have mostly focused on analyzing modeling methods, with a data-centric view by measuring performance discrepancies incurred by multimodal representations.</p>
<p>Multimodal Datasets. Many vision-language datasets have recently been curated to assess the capabilities of multimodal foundation models (Yue et al., 2023; Lu et al., 2024; Zhang et al., 2024a; Fan et al., 2024; Zhang et al., 2024b). They often consist of a large collection of problems with varying levels of difficulty and usage of the visual inputs. As a result, they are suitable for assessing VLMs as a whole, but cannot attribute these performances to capabilities of their LLM backbones and improvements from downstream vision-language training. While several contemporaneous works (Fan et al., 2024; Zhang et al., 2024a;b) introduce a language-only subset of their vision-language prompts, they focus primarily on assessing certain areas of foundation models, such as deductive reasoning and problems of beyond-polynomial complexity. IsoBench is the first of its kind that offers a holistic and fine-grained analysis of the performance gap induced by variations in input modalities. For instance, a contemporaneous work, MathVerse (Zhang et al., 2024a), studies redundant, but not necessarily equivalent input representations on math problems whilst IsoBench studies the impact of isomorphic representations on a broader set of tasks.</p>
<p>Sensitivity of Language Model to Perturbations. Similar to what IsoBench observes on multimodal foundation models' sensitivity to input modalities, uni-modal language models also show sensitivity to input perturbation. For example, LLMs are shown to be sensitive to subtle changes in zero-shot and few-show settings (Sclar et al., 2023; Chang \&amp; Jia, 2023). Meanwhile, the sensitivity of Transformers, the backbone of foundation models, is also broadly studied (Bombari \&amp; Mondelli, 2024; Bhattamishra et al., 2023; Vasudeva et al., 2024; Kong et al., 2024). To address the sensitivity issues, methods including prompt design (Yoo et al., 2021; Mishra et al., 2021; Le et al., 2023; Liu et al., 2023c) and prompting-based training strategies (Jain et al., 2023; Guo et al., 2023) have been employed. IsoBench is one of the pioneers to study model sensitivity to input representations and modalities.</p>
<p>Foundation Models for Algorithmic Reasoning Recent work has shown that Transformers, the backbone of multimodal foundation models, are capable of implementing various algorithms after fine-tuning on particular tasks (Khalighinejad et al., 2023; Garg et al., 2022; von Oswald et al., 2022; Hanna et al., 2023; Fu et al., 2023; Lee et al., 2023; Zhou et al., 2024) or with proper prompting schemes (Wei et al., 2023; Yao et al., 2023; Liu et al., 2024b). However, the extent to which foundation models are capable of solving problems involving complex reasoning, algorithms, or scientific awareness remains unclear.</p>
<h1>6 Conclusion</h1>
<p>In this paper, we developed IsoBench, a broad-coverage benchmark for evaluating multimodal foundation models on a varied set of quantitative tasks, where each example input is provided with multiple isomorphic representations. IsoBench includes four major subjects (mathematical functions, algorithmic problems, science questions, and chess games) in a dataset consisting of over 1,630 examples. Within each domain, we evaluated multiple input representations-one with visual input and others with domain-specific, isomorphic textual inputs-to facilitate multimodal performance assessments. We found that all tested models perform substantially better on text-only prompts than image-based prompts, in contrast with known human preferences for images over text. To bridge performance discrepancies between different input modalities, we also introduced IsoCombination and IsoScratchPad, two procedures that, respectively, fuse input modalities and transforms visual input to textual representation at inference time. In full, we hope that IsoBench can provide fine-grained feedback to diagnose performance gaps caused by the form of a given input representation, and help us better understand how the capabilities of multimodal foundation models change depending on the input modality that is provided.</p>
<h2>Acknowledgement</h2>
<p>We would like to thank the USC NLP, Duke NLP Group, and an Open Philanthropy University Organizer Fellowship for providing computing resources. DF and RJ were supported by an Open Philanthropy research grant and a Google Research Scholar Award. DF would like to thank Vatsal Sharan and Qilin Ye for their discussions on graphs. OL would like to thank Xinyan Velocity Yu, Bill Yuchen Lin, and Corby Rosset for their helpful discussions.</p>
<h1>References</h1>
<p>Elinor Amit, Caitlyn Hoeflin, Nada Hamzah, and Evelina Fedorenko. An asymmetrical relationship between verbal and visual thinking: Converging evidence from behavior and fmri. Neurolmage, 152:619-627, 2017. ISSN 1053-8119. doi: https://doi.org/10.1016/j. neuroimage.2017.03.029. URL https://www.sciencedirect.com/science/article/pii/ 51053811917302379. 2</p>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023. 6</p>
<p>Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024. URL https://www-cdn.a nthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf. 1,9</p>
<p>Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar. Introducing our multimodal models, 2023. URL https: //www. adept. ai/blog/fuyu-8b. 9</p>
<p>Satwik Bhattamishra, Arkil Patel, Varun Kanade, and Phil Blunsom. Simplicity bias in transformers and their ability to learn sparse Boolean functions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, 2023. 9</p>
<p>Simone Bombari and Marco Mondelli. Towards understanding the word sensitivity of attention layers: A study via random features, 2024. 9</p>
<p>Ting-Yun Chang and Robin Jia. Data curation alone can stabilize in-context learning. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8123-8144, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.452. URL https://aclanthology.org/2023.acl-long.452. 9</p>
<p>Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointlyscaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022. 9</p>
<p>Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a multilingual vision and language model. arXiv preprint arXiv:2305.18565, 2023. 9</p>
<p>Liying Cheng, Xingxuan Li, and Lidong Bing. Is gpt-4 a good data analyst? arXiv preprint arXiv:2305.15038, 2023. 4</p>
<p>Margaret Anne Defeyter, Riccardo Russo, and Pamela Louise McPartlin. The picture superiority effect in recognition memory: A developmental study using the response signal procedure. Cognitive Development, 24(3):265-273, 2009. ISSN 0885-2014. doi: https://doi.org/10.1016/j.cogdev.2009.05.002. URL https://www.sciencedirect.com/sc ience/article/pii/S0885201409000471. 2</p>
<p>Lizhou Fan, Wenyue Hua, Xiang Li, Kaijie Zhu, Mingyu Jin, Lingyao Li, Haoyang Ling, Jinkui Chi, Jindong Wang, Xin Ma, et al. Nphardeval4v: A dynamic reasoning benchmark of multimodal large language models. arXiv preprint arXiv:2403.01777, 2024. 2, 9</p>
<p>Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order optimization methods for in-context learning: A study with linear models, 2023. 9</p>
<p>Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. ArXiv, abs/2208.01066, 2022. 9</p>
<p>Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. arXiv preprint arXiv:2212.04037, 2022. 7</p>
<p>Gemini Team Google. Gemini: A family of highly capable multimodal models, 2023. 1, 9
Ruohao Guo, Wei Xu, and Alan Ritter. Instruction tuning with lexicons for zero-shot style classification. arXiv preprint arXiv:2305.14592, 2023. 9</p>
<p>Xiaochuang Han and Yulia Tsvetkov. Orca: Interpreting prompted language models via locating supporting data evidence in the ocean of pretraining data. arXiv preprint arXiv:2205.12600, 2022. 7</p>
<p>Michael Hanna, Ollie Liu, and Alexandre Variengien. How does GPT-2 compute greaterthan?: Interpreting mathematical abilities in a pre-trained language model. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https://openrevi ew.net/forum?id=p4PckNQR8k. 9</p>
<p>Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al. Neftune: Noisy embeddings improve instruction finetuning. arXiv preprint arXiv:2310.05914, 2023. 9</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. 6</p>
<p>Ghazal Khalighinejad, Ollie Liu, and Sam Wiseman. Approximating cky with transformers. arXiv preprint arXiv:2305.02386, 2023. 9</p>
<p>Xianghao Kong, Ollie Liu, Han Li, Dani Yogatama, and Greg Ver Steeg. Interpretable diffusion via information decomposition. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=X6tNkN6ate. 9</p>
<p>Duong Minh Le, Ruohao Guo, Wei Xu, and Alan Ritter. Improved instruction ordering in recipe-grounded conversation. arXiv preprint arXiv:2305.17280, 2023. 9</p>
<p>Nayoung Lee, Kartik K. Sreenivasan, Jason D. Lee, Kangwook Lee, and Dimitris Papailiopoulos. Teaching arithmetic to small transformers. ArXiv, abs/2307.03381, 2023. 9</p>
<p>Paul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard Chen, Zihao Deng, Nicholas Allen, Randy Auerbach, Faisal Mahmood, et al. Quantifying \&amp; modeling multimodal interactions: An information decomposition framework. Advances in Neural Information Processing Systems, 36, 2024. 7</p>
<p>Chang Liu, Yujie Zhong, Andrew Zisserman, and Weidi Xie. Countr: Transformer-based generalised visual counting. arXiv preprint arXiv:2208.13721, 2022. 7</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023a. 6, 9</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023b. 9</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https://llava-vl.github.io/blog/2024-01-30-llava-next/. 9</p>
<p>Ollie Liu, Deqing Fu, Dani Yogatama, and Willie Neiswanger. DeLLMa: A framework for decision making under uncertainty with large language models, 2024b. 9</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35, 2023c. 9</p>
<p>Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. 5</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024. 2, 9</p>
<p>Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis \&amp; insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. 9</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing instructional prompts to gptk's language. arXiv preprint arXiv:2109.07830, 2021. 9</p>
<p>Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill.00024.001. https: / distill.pub/2020/circuits/zoom-in. 7</p>
<p>OpenAI. Gpt 3.5 turbo. openai.com, 2023a. URL https://help.openai.com/en/articles/85 55514-gpt-3-5-turbo-updates. 6</p>
<p>OpenAI. Gpt-4 technical report, 2023b. 1, 9
Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022. 7</p>
<p>Reka. Reka flash: An efficient and capable multimodal language model. 2024. URL https: //reka.ai/reka-flash-an-efficient-and-capable-multimodal-language-model/. 9</p>
<p>Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. arXiv preprint arXiv:2310.11324, 2023. 9</p>
<p>Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 9</p>
<p>Jesse Thomason, Daniel Gordon, and Yonatan Bisk. Shifting the baseline: Single modality performance on visual navigation \&amp; qa. North American Chapter of the Association for Computational Linguistics, abs/1811.00613, 2018. 7</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 6</p>
<p>Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang, and Vatsal Sharan. Simplicity bias of transformers to learn low sensitivity functions, 2024. 9</p>
<p>Johannes von Oswald, Eyvind Niklasson, E. Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, 2022. 9</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. 9</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023. 9</p>
<p>Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woomyeong Park. Gpt3mix: Leveraging large-scale language models for text augmentation. arXiv preprint arXiv:2104.08826, 2021. 9</p>
<p>Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multimodal models: Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2023. 9</p>
<p>Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. 2, 9</p>
<p>Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024a. 2, 9</p>
<p>Yizhe Zhang, He Bai, Ruixiang Zhang, Jiatao Gu, Shuangfei Zhai, Josh Susskind, and Navdeep Jaitly. How far are we from intelligent visual deductive reasoning? arXiv preprint arXiv:2403.04732, 2024b. 2, 9</p>
<p>Tianyi Zhou, Deqing Fu, Vatsal Sharan, and Robin Jia. Pre-trained large language models use fourier features to compute addition, 2024. URL https://arxiv.org/abs/2406.03445. 9</p>
<h1>Appendix</h1>
<h2>A Dataset Generation</h2>
<p>We detail procedures to curate the IsoBench dataset. Please refer to the supplementary materials for concrete samples and further implementation details.</p>
<h2>A. 1 Mathematics</h2>
<ol>
<li>Image: the model is prompted with a (textual) problem statement and the image visualization of the function. We generate these plots with matplotlib.</li>
<li>Text ( $\mathbf{I S T}<em _mathrm_E="\mathrm{E">{\mathbf{E}} \mathbf{X}$ ): in addition to the problem statement, we provide $\mathrm{IST}</em>$ definition of the function as textual inputs.}} \mathrm{X</li>
<li>Text (Code): we replace the $\mathrm{IST}_{\mathrm{E}} \mathrm{X}$ definition of function in the previous representation with its sympy definition. All other parts of the prompt are kept identical.</li>
</ol>
<p>Even \&amp; Odd Function. We sample a dataset of rational functions - the quotients of two polynomials - and extract their even and odd parts as samples. Each prompt formulates a 3 -way classification problem for the model to choose among even, odd, and neither. Each class consists of 128 samples.</p>
<p>Convex \&amp; Concave Function. We sample convex (resp. concave) functions by sampling operations from $\left{x^{p},|x|,-\log x, \exp x\right}$ (resp. the negative of these operations) as well as multiplicative weights. Each class consists of 128 samples; and we prompt the model with their appropriate domains to ensure correctness of either of the properties.</p>
<p>Counting Breakpoints. We sample piecewise linear functions with 2 or 3 breakpoints, i.e. sudden changes in slope at intersections of linear functions. We manually audit to ensure that the changes in slope are visible. Each class consists of 128 samples.
Images for math problems are generated with 300 DPI in JPEG format.</p>
<h2>A. 2 Algorithms</h2>
<ol>
<li>Image: the model is prompted with an instruction of the problem to solve and the image visualization of the graph. For graphs, we use networkx to visualize.</li>
<li>Text with $\mathbf{I S T}_{\mathbf{E}} \mathbf{X}$ : the model is prompted with an instruction of the problem to solve and the mathematical expression of the graphs, and here we choose to use the adjacency matrix as the mathematical representation.</li>
<li>Text with story or description: the model is prompted with an instruction of the problem to solve and the story-telling description of the scenario. For example, formulate the graph connectivity problem as the possibility of driving from one city to another.</li>
</ol>
<p>Graph Connectivity. We sample 128 undirected graphs using Erdos-Renyi random graph generation method. Each pair of nodes has a probability of $p$ to be connected. The query nodes are also sampled at random and we balance the benchmark to have $50 \%$ of the sample having query nodes connected and the rest disconnected.</p>
<p>Maximum Flow. We sample 128 weighted directed graphs. Each edge has a random integer flow uniformly sampled from 0 to 9 and a fairly sampled direction. Maxflow is a fairly hard problem so we restrict the graphs to have 3,4 or 5 nodes.</p>
<p>Graph Isomorphism. We sample 128 pairs of graphs, with 64 pairs isomorphic and 64 of them not isomorphic. In generating isomorphic pairs $G$ and $H$, we sample a random permutation matrix $\Pi$, and the adjacency matrix corresponding to graph $H$ is $A_{H}=\Pi A_{G} \Pi^{\top}$ where $A_{G}$ is the adjacency matrix of graph $G$. In generating non-isomorphic pairs, $A_{G}$ and</p>
<p>$A_{H}$ are sampled individually. To avoid models using simple shortcuts such as counting the number of nodes, graphs $G$ and $H$ in each pair are guaranteed to have the same number of nodes and the same number of edges.</p>
<p>Images for algorithm problems are generated with 300 DPI in JPEG format.</p>
<h1>A. 3 Games</h1>
<ol>
<li>Graphical Board: the model is prompted with an instruction of the problem to solve and the visual representation of the chess board and its pieces in a graphical format (PNG) is given.</li>
<li>Algebraic Notation Layout (ANL): this refers to a text-based representation of the current position of pieces on the chess board, using algebraic notation for the squares.</li>
<li>Portable Game Notation (PGN): this is a textual representation of the chess game's moves in standard algebraic notation. PGN is widely used for the recording of games, allowing for both humans and computers to read the game history. It captures the entire sequence of moves from the beginning to the current state or the end of the game.</li>
<li>Forsyth-Edwards Notation (FEN): this is a compact way to describe the current position of a game of chess in textual format.</li>
</ol>
<p>Winner Identification from Checkmate The task is to analyze a given chess game representation and identify the winner: whether the game resulted in a win for White or Black, or if it ended without a winner. Our dataset for this task contains the games that lasted more than nine rounds in February 2024.</p>
<p>Chess Puzzles For this task, we consider chess puzzles that consist of a chess position, along with a sequence of optimal moves necessary to resolve the puzzle. In our evaluation, the primary objective is to predict the initial move.</p>
<p>Images for Chess are the same as Lichess, and are converted to JPEG formats.</p>
<h2>A. 4 Sciences</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Distribution of science problem categories. We introduced three new categories that are absent from the ScienceQA dataset: Electric Circuit, Organic Compound, and Chemical equations.</p>
<ol>
<li>Image: Each sample contains the textual questions, choices, and an accompanying figure providing additional context to the question.</li>
<li>Text: Instead of prompting LLMs to describe the image content, we asked one author to manually write descriptions for each figure. The annotator was instructed to avoid including additional reasoning or information beyond what is present in the figure, focusing solely on describing the content.</li>
</ol>
<p>Images for Chess are the same as ScienceQA, and are converted to JPEG formats.</p>
<h1>B Sample Responses for Math, Algorithms, Games, and Science Problems</h1>
<p>We show sample responses for Math, Algorithms, Games, and Science Problems in the figures below, for multiple multimodal foundation models.</p>
<h2>PROMPT</h2>
<p>You are given a real-valued, scalar function $f(x)$.
YOUR TASK is to determine whether $f(x)$ is an even function, an odd function, or neither.</p>
<ul>
<li>Definition of an even function: A function such that</li>
</ul>
<p>$$
f(x)=f(-x)
$$</p>
<p>where the value remains unchanged if the sign of the independent variable is reversed.</p>
<ul>
<li>Definition of an odd function: A function such that</li>
</ul>
<p>$$
f(-x)=-f(x)
$$</p>
<p>where the sign is reversed but the absolute value remains the same if the sign of the independent variable is reversed.</p>
<ul>
<li>A function is neither even nor odd if it does not satisfy either condition.</li>
</ul>
<p>Here is the expression of $f(x)$ :</p>
<p>$$
f(x)=\frac{7 x^{6}-3 x^{5}+x^{2}-21.76}{2 x^{6}+4 x^{4}+3 x^{2}+11.34}
$$</p>
<p>Respond with 'even', 'odd', 'neither' first on whether the function $f(x)$ is even, odd, or neither, based on the definitions and your observation of the function.
Please parse it to JSON format. Put the result (even, odd, or neither) under key 'answer', and put the explanations and reasonings under key 'explanation' (Do not summarize, paraphrase or change meanings).</p>
<h2>GPT-4 RESPONSE</h2>
<p>to determine whether the function</p>
<p>$$
f(x)=\frac{7 x^{6}-3 x^{5}+x^{2}-21.76}{2 x^{6}+4 x^{4}+3 x^{2}+11.34}
$$</p>
<p>is even, odd, or neither, we substitute $-x$ for $x$ and compare the resulting expression to the original function. for an even function, we should find that</p>
<p>$$
f(x)=f(-x)
$$</p>
<p>and for an odd function, we should find that</p>
<p>$$
f(-x)=-f(x)
$$</p>
<p>substituting $-x$ into the function, we get</p>
<p>$$
f(-x)=\frac{7(-x)^{6}-3(-x)^{5}+(-x)^{2}-21.76}{2(-x)^{6}+4(-x)^{4}+3(-x)^{2}+11.34}=\frac{7 x^{6}+3 x^{5}+x^{2}-21.76}{2 x^{6}+4 x^{4}+3 x^{2}+11.34}
$$</p>
<p>this expression is not equal to the original function $f(x)$, indicating that the function is not even. additionally, the expression is not the negative of the original function, indicating that the function is not odd. the presence of the $x^{5}$ term, which changes sign when $x$ is replaced with $-x$, but does not satisfy the condition for an odd function due to the other terms, is a key factor in determining that the function is neither even nor odd.</p>
<p>Figure 5: Sample response from GPT-4 to a parity problem in IsoBench. GPT-4 was able to analyze the parity of the rational function with the correct reasoning and computation.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<h1>PROMPT</h1>
<p>You are given a plot of a real-valued, scalar function $f(x)$.
YOUR TASK is to determine whether $f(x)$ is an even function, an odd function, or neither.</p>
<ul>
<li>Definition of an even function: A function such that</li>
</ul>
<p>$$
f(x)=f(-x)
$$</p>
<p>where the value remains unchanged if the sign of the independent variable is reversed.</p>
<ul>
<li>Definition of an odd function: A function such that</li>
</ul>
<p>$$
f(-x)=-f(x)
$$</p>
<p>where the sign is reversed but the absolute value remains the same if the sign of the independent variable is reversed</p>
<ul>
<li>A function is neither even nor odd if it does not satisfy either definitions.</li>
</ul>
<p>Respond with 'even', 'odd', 'neither' first on whether the function $f(x)$ is even, odd, or neither, based on the definition above. For any of your conclusions (even, odd, or neither), please explain in details based on your observation of the plot. Answer (start with 'even', 'odd', or 'neither'):</p>
<h2>GPT-4 RESPONSE</h2>
<p>The plot of the function $f(x)$ shows symmetry about the $y$-axis. This means that for every point $(x, f(x))$ on the right side of the $y$-axis, there is a corresponding point $(-x, f(x))$ on the left side of the $y$-axis with the same function value. This symmetry indicates that</p>
<p>$$
f(x)=f(-x)
$$</p>
<p>for all $x$ in the domain of the function, which is the definition of an even function. Therefore, based on the plot, $f(x)$ is an even function.</p>
<p>Figure 6: Sample response from GPT-4V to the same parity problem as in Figure 5. GPT-4V cannot distinguish the nuanced asymmetry at the top-rigth and top-left corners of the plot. We observe the same failure mode for Gemini-Pro and LLaVA.</p>
<p>PROMPT
You are given a real-valued, scalar function $f(x)$.
YOUR TASK is to determine whether $f(x)$ is an even function, an odd function, or neither.</p>
<ul>
<li>Definition of an even function: A function such that</li>
</ul>
<p>$$
f(x)=f(-x)
$$</p>
<p>where the value remains unchanged if the sign of the independent variable is reversed.</p>
<ul>
<li>Definition of an odd function: A function such that</li>
</ul>
<p>$$
f(-x)=-f(x)
$$</p>
<p>where the sign is reversed but the absolute value remains the same if the sign of the independent variable is reversed.</p>
<ul>
<li>A function is neither even nor odd if it does not satisfy either condition.</li>
</ul>
<p>Here is the expression of $f(x)$ :</p>
<p>$$
f(x)=\frac{4 x^{3}\left(x^{2}-1\right)}{16 x^{10}+8 x^{8}+10 x^{2}+13.98}
$$</p>
<p>Respond with 'even', 'odd', 'neither' first on whether the function $f(x)$ is even, odd, or neither, based on the definitions and your observation of the function.
Please parse it to JSON format. Put the result (even, odd, or neither) under key 'answer', and put the explanations and reasonings under key 'explanation' (Do not summarize, paraphrase or change meanings).</p>
<h1>GPT-4 RESPONSE</h1>
<p>to determine whether the function</p>
<p>$$
f(x)=\frac{4 x^{3}\left(x^{2}-1\right)}{16 x^{10}+8 x^{8}+10 x^{2}+13.98}
$$</p>
<p>is even, odd, or neither, we substitute $-x$ for $x$ and compare the result to the original function. substituting $-x$ into the function gives</p>
<p>$$
f(-x)=\frac{4(-x)^{3}\left((-x)^{2}-1\right)}{16(-x)^{10}+8(-x)^{8}+10(-x)^{2}+13.98}=\frac{-4 x^{3}\left(x^{2}-1\right)}{16 x^{10}+8 x^{8}+10 x^{2}+13.98}
$$</p>
<p>, which simplifies to $-f(x)$. this means that the function satisfies the condition for being an odd function, which is</p>
<p>$$
f(-x)=-f(x)
$$</p>
<p>. the function does not satisfy the condition for being an even function, which would require</p>
<p>$$
f(x)=f(-x)
$$</p>
<p>. therefore, the function is odd.
Figure 7: Sample response from GPT-4 to a parity problem in IsoBench. GPT-4 was able to analyze the parity of the rational function with the correct reasoning and computation.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<h1>PROMPT</h1>
<p>You are given a plot of a real-valued, scalar function $f(x)$.
YOUR TASK is to determine whether $f(x)$ is an even function, an odd function, or neither.</p>
<ul>
<li>Definition of an even function: A function such that</li>
</ul>
<p>$$
f(x)=f(-x)
$$</p>
<p>where the value remains unchanged if the sign of the independent variable is reversed.</p>
<ul>
<li>Definition of an odd function: A function such that</li>
</ul>
<p>$$
f(-x)=-f(x)
$$</p>
<p>where the sign is reversed but the absolute value remains the same if the sign of the independent variable is reversed</p>
<ul>
<li>A function is neither even nor odd if it does not satisfy either definitions.</li>
</ul>
<p>Respond with 'even', 'odd', 'neither' first on whether the function $f(x)$ is even, odd, or neither, based on the definition above. For any of your conclusions (even, odd, or neither), please explain in details based on your observation of the plot. Answer (start with 'even', 'odd', or 'neither'):</p>
<h2>GPT-4 RESPONSE</h2>
<p>neither
The function $f(x)$ depicted in the plot does not satisfy the conditions for being an even function nor an odd function. For a function to be even, the graph of the function should be symmetric with respect to the $y$-axis, meaning that the part of the graph in the positive $x$-region should be a mirror image of the part in the negative $x$-region. This function does not exhibit such symmetry; the shapes and values of the function on either side of the $y$-axis are not mirror images of each other.
Similarly, for a function to be odd, the graph should have rotational symmetry about the origin, which means that rotating the graph 180 degrees around the origin should result in the same graph. This is not the case for the given function either, as the positive and negative sides of the graph do not match when one side is rotated 180 degrees.
Since the function does not meet the criteria for being even or odd, we conclude that the function is neither even nor odd.
Figure 8: Sample response from GPT-4V to the same parity problem as in Figure 7. GPT-4V cannot reason with the symmetry at origin. We observe the same failure mode for all other vision models.</p>
<p>PROMPT
You are given a real-valued, scalar function $f(x)$.
YOUR TASK is to determine whether $f(x)$ is an convex function or an concave function</p>
<ul>
<li>Definition of a convex function: A function such that for all $x, y$, and $\varnothing \ll t&lt;1$</li>
</ul>
<p>$$
f(t x+(1-t) y) \leq t f(x)+(1-t) f(y)
$$</p>
<ul>
<li>Definition of a concave function: A function such that for all $x, y$, and $\varnothing \ll t&lt;1$</li>
</ul>
<p>$$
f(t x+(1-t) y) \geq t f(x)+(1-t) f(y)
$$</p>
<p>Here is the expression of $f(x)$, defined for all $x&gt;0$ :</p>
<p>$$
f(x)=0.31 x-1.73 \log (x)+1.13|x|-0.63
$$</p>
<p>Respond with 'convex' or 'concave' first on whether the function $f(x)$ is convex or concave, based on the definitions and your observation of the function.
Please parse it to JSON format. Put the result (convex or concave) under key 'answer', and put the explanations and reasonings under key 'explanation' (Do not summarize, paraphrase or change meanings).</p>
<h1>Claude 3 RESPONSE</h1>
<p>The given function $f(x)$ is a convex function. This can be determined by examining the individual terms in the expression:</p>
<ol>
<li>The term $0.31 x$ is a linear function, which is both convex and concave.</li>
<li>The term $-1.73 \log (x)$ is a concave function, as the logarithm function is concave for $x&gt;0$.</li>
<li>The term $1.13|x|$ is a convex function, as the absolute value function is convex.</li>
<li>The constant term $\sim \varnothing .63$ does not affect the convexity or concavity of the function.</li>
</ol>
<p>Since the sum of a convex function and a concave function is not guaranteed to be either convex or concave, we need to consider the dominant term. In this case, the convex term $1.13|x|$ grows faster than the concave term $-1.73 \log (x)$ as $x$ increases. Therefore, for sufficiently large values of $x$, the convex term will dominate, making the overall function $f(x)$ convex.</p>
<p>Figure 9: Sample response from Claude 3 to a convexity problem in IsoBench. Claude 3 was able to analyze the convexity of the function with the correct reasoning and computation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ In preliminary experiments, we leveraged GPT-4 to generate image descriptions. However, we observed that these descriptions often include reasoning and additional knowledge, leading to biased "improved" performance.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
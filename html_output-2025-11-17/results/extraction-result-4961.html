<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4961 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4961</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4961</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-254877753</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.findings-acl.67.pdf" target="_blank">Towards Reasoning in Large Language Models: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4961.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4961.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that includes intermediate natural-language reasoning steps (rationales) in demonstrations to elicit multi-step reasoning from large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 / PaLM (and other LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based autoregressive language models (e.g., GPT-3 family and PaLM) that are pretrained on large text corpora and used in few-shot or zero-shot prompting regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (GPT-3 example) / >100B (PaLM and other LLMs mentioned); null when unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Arithmetic, symbolic, commonsense multi-step reasoning benchmarks (e.g., GSM8K, Last Letter Concatenation, Coin Flip, StrategyQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks requiring multi-step informal deductive and arithmetic reasoning, symbolic manipulation, or commonsense chains of inference producing a final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Replace standard few-shot ⟨input,output⟩ exemplars with ⟨input, chain of thought, output⟩ triples so the model generates intermediate reasoning steps before the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: reported large improvements on arithmetic, symbolic, and some commonsense reasoning tasks relative to standard few-shot prompting; can produce dramatic gains for sufficiently large models (reporting is survey-level; no single numeric accuracy given in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Generated rationales can be incorrect or inconsistent; CoT elicits reasoning-like outputs but does not prove internal systematic logical reasoning; struggles on more complex compositional tasks and can be sensitive to exemplar choice.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms standard few-shot prompting on many multi-step tasks for large models; effect typically emerges at large model scale (>100B parameters); outperforms fully supervised finetuning in some OOD robustness settings per cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Survey cites ablations showing CoT effect depends on exemplar relevance and ordering (Wang et al.); Saparov & He find models may produce valid individual proof steps but still choose wrong steps when multiple exist; performance scales with model size and is much stronger in very large models (Wei et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4961.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4961.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought ("Let's think step by step")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minimal prompting variant that elicits step-by-step reasoning from LLMs with a generic instruction (e.g., "Let's think step by step") without few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 and similar large LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Very large autoregressive LLMs used in zero-shot prompt settings; architecture: transformer decoder, pretrained on text.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>e.g., GPT-3 175B referenced; null if unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Arithmetic and commonsense reasoning tasks (same benchmarks as CoT applications, e.g., GSM8K, StrategyQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring multi-step reasoning where conventional zero-shot prompting fails but adding a simple instruction attempts to induce stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Append a short instruction like "Let's think step by step" to the prompt to induce the model to produce intermediate reasoning and then the final answer, without demonstration exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: reported to enable nontrivial zero-shot improvements on several reasoning tasks compared to plain zero-shot prompting, but typically smaller gains than few-shot CoT; exact numbers not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not universally effective across models and tasks; still limited by model scale and training distribution; can produce plausible-looking but incorrect rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Sits between plain zero-shot prompting and few-shot CoT in effectiveness; simpler to use (no exemplar construction) but generally less effective than high-quality few-shot CoT for the largest gains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Survey references studies showing prompt wording and model pretraining (e.g., training on code) modulate effectiveness; no single numeric ablation values given here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4961.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4961.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency Decoding / Voting over Diverse Rationales</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding strategy that samples multiple diverse chain-of-thought rationales from a model and selects the most consistent final answer by plurality/voting, marginalizing over rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large LLMs (applied to models used in CoT experiments, e.g., GPT-3 / PaLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLMs used with stochastic decoding (sampling) to produce multiple reasoning trajectories which are then aggregated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>null (survey-level references to large models)</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Arithmetic and multi-step reasoning benchmarks (e.g., GSM8K and related CoT-evaluated tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where multiple valid reasoning trajectories may lead to the same correct answer; aggregation increases robustness to sampling noise.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Use stochastic decoding to sample many chain-of-thought outputs, extract final answers, and pick the most frequent answer (or marginalize answers) as final prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: reported to improve accuracy over single greedy chain-of-thought decoding often substantially; specific numeric gains are referenced in original paper but not reproduced numerically in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Increases compute (multiple decodings); if model samples diverse but systematically biased wrong rationales, voting may reinforce incorrect majority; doesn't fix rationale validity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Improves over greedy CoT decoding; complementary to exemplar design and rationale refinement methods.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Survey cites evidence that marginilizing across sampled rationales improves final-answer accuracy and out-of-distribution robustness versus single-run CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4961.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4961.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Least-to-most</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-Most Prompting (Problem Decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decomposition prompting strategy that breaks complex problems into a sequence of simpler subproblems solved in order, conditioning later steps on earlier answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Least-to-most prompting enables complex reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large LLMs (CoT-capable models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs prompted iteratively to produce and solve subquestions; leverages in-context decomposition rather than monolithic CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>null (survey does not provide explicit parameter counts for experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Compositional and multi-step reasoning tasks, semantic parsing and complex arithmetic requiring compositional generalization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring decomposition into intermediate subgoals (e.g., multi-step word problems, compositional semantic parsing).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Prompt the model to decompose a complex question into ordered subproblems (least complex first) and solve them sequentially, passing intermediate answers forward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: reported improvements on compositional tasks that standard CoT struggles with; helps with length/generalization in some studies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires good decomposition strategy; failures in decomposition lead to cascading errors; not guaranteed to scale to arbitrary task complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Often improves over monolithic CoT for compositional tasks; related approaches include dynamic least-to-most, decomposed prompting, and successive prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Survey cites follow-up work showing dynamic exemplar selection and syntactic parsing to support decomposition further improve results; specific ablations are in cited papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4961.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4961.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STaR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Taught Reasoner (STaR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bootstrapping/iterative fine-tuning pipeline where an LLM generates chain-of-thought rationales, selects rationales that lead to correct answers, and then fine-tunes on those successful rationales to improve reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>STar: Bootstrapping reasoning with reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs used as both generator and training target (e.g., GPT-family style models in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained autoregressive LLMs that are iteratively fine-tuned on their own high-quality chain-of-thought outputs to self-improve.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>null (survey-level description)</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>General multi-step reasoning benchmarks where CoT is applicable (e.g., arithmetic and commonsense reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where generating and then filtering correct rationales can create additional training data to refine the model's reasoning abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Generate CoT outputs, filter those that produce correct final answers, fine-tune the model on successful ⟨input, CoT, output⟩ triples, and iterate.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: reported improvements in subsequent model ability to produce correct rationales and answers; survey notes STaR as an effective self-improvement approach (quantitative metrics are in the original STaR paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relies on an initial strong model to produce sufficient correct rationales; risk of overfitting to self-generated artifacts; quality of filtering crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Offers alternative to supervised finetuning on human-written rationales; can outperform baseline few-shot CoT when ample successful self-generated rationales exist.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Survey references Huang et al. (2022a) and Zelikman et al. (2022) demonstrating self-improvement without supervised data in some settings; performance depends on filtering heuristics and initial model quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4961.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4961.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trained Verifier for Rationale Selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Train a separate verifier model to score candidate rationales and final answers generated by a reasoning model and select the highest-scoring solution to improve final-answer correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training verifiers to solve math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Primary LLM (generator) plus a separate verifier model (typically a classifier/LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline where a generative LLM produces multiple rationale+solution candidates and an auxiliary verifier (trained discriminatively or via LLM scoring) ranks candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>null (survey level)</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Math word problems and numerical reasoning benchmarks where correctness can be judged (e.g., GSM8K-style problems)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Problems where selecting the correct final answer from multiple candidate reasoning traces improves accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Generate candidate CoT traces and answers, use a trained verifier to assign scores and pick the highest-scoring solution; can also use the LLM itself as verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: reported to improve final-answer selection and reduce errors from incorrect single rationales; numeric gains referenced in original cited work (Cobbe et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires labeled data or reliable criteria to train verifier; verifier can inherit biases; training verifier adds complexity and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Improves over naive single-run CoT by filtering; complementary to self-consistency and rationale exploration strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Survey cites experiments where verifier selection increases accuracy compared to greedy CoT and where LLM-based verifiers can serve as an alternative to trained discriminative verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4961.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4961.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FOLIO (First-Order Logic Inference/Ontology dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset designed to test first-order logic reasoning capabilities of LLMs using natural-language premises and conclusions derived from formal first-order formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Folio: Natural language reasoning with firstorder logic</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs evaluated in referenced studies (survey mentions LLMs generally; specific model experiments in FOLIO paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Benchmarks for LLMs to evaluate formal first-order logical entailment / validity when premises and hypotheses are expressed in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>First-order logic reasoning (validity of conclusions from premises; formal deductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Determine correctness/validity of a conclusion given a set of premises mapped from first-order logic statements to natural language; assesses formal logical inference capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluation dataset enabling formal stepwise analysis of model reasoning; used to probe models' ability to perform deductive first-order reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: survey states LLMs still struggle with such formal tasks; Han et al. find weaknesses and incomplete proofs in many cases (specific metrics are in the FOLIO paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Models often fail at complex first-order reasoning, produce incomplete or incorrect proofs, and struggle when multiple derivation choices exist.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Shows gap between informal CoT-style success and strict formal logical reasoning; models that perform well on informal tasks often underperform on FOLIO-style formal logic benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Survey references FOLIO as a targeted diagnostic; ablations and per-category performance analyses are in the original FOLIO paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4961.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4961.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PrOntoQA (Synthetic Ontology-based QA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic dataset generated from (real or fictional) ontologies with unique proofs per example, designed to enable formal, step-by-step evaluation of reasoning traces produced by LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs evaluated in the referenced work (survey-level; specific models in original PrOntoQA paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Synthetic task generator that maps formal proofs to natural-language facts/sentences so that each example has a unique ground-truth proof trace.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Step-by-step proof generation and verification; synthetic deductive reasoning grounded in ontologies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess whether a model can produce the unique correct chain of inference (proof) from premises to conclusion; enables analysis of individual reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Dataset creation and diagnostic evaluation; used to test faithfulness and correctness of model-produced rationales at the step level.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: survey reports models can produce valid individual proof steps in some cases but may select wrong steps when multiple exist; detailed numbers are in original PrOntoQA work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Models sometimes choose incorrect proof steps among alternatives leading to incomplete or wrong proofs; synthetic nature may differ from natural tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Provides a stricter diagnostic than end-task accuracy; highlights discrepancies between correct final answer and correct internal proof.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Survey notes PrOntoQA enables formal analysis of each reasoning step; specific ablation details are in the originating paper (citation omitted in survey references).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4961.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4961.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROSCOE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROSCOE (Suite of Step-by-Step Reasoning Metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of interpretable evaluation metrics for scoring step-by-step reasoning along multiple perspectives, including semantic alignment, logical inference, and coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Roscoe: A suite of metrics for scoring step-by-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Any LLM whose chain-of-thought outputs are evaluated (survey-level)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Metric suite applied to CoT/rationale outputs to quantify the quality of intermediate reasoning steps beyond end-task accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Evaluation/analysis task rather than a single reasoning benchmark; applied where step-level rationales exist.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure semantic and logical quality of generated rationales: alignment to gold steps, correctness of inferences, and linguistic coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Automated and interpretable scoring metrics to analyze and diagnose chain-of-thought outputs in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not applicable (ROSCOE is an evaluation methodology); enables deeper analysis that often reveals rationale errors even when final answers are correct.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Score design depends on available references; may require gold step annotations; not a replacement for end-task metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Provides more granular insight than single-number accuracy metrics; used in survey as an example of deeper analysis tools.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Survey cites ROSCOE as enabling multi-faceted analysis; concrete ablations and metric breakdowns are in the ROSCOE paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4961.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4961.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code/pretraining on code</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training on Code (Codex / Code-pretrained LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation that models pretrained or fine-tuned on code perform better on certain reasoning tasks when reasoning is framed as code generation or program execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating large language models trained on code</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex and code-pretrained variants of LLMs (e.g., models from Chen et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs pretrained on code (source code, APIs, etc.) and natural language, enabling them to perform program-like precise computations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Numerical reasoning, algorithmic tasks, and other reasoning tasks reframed as code generation/program execution</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where strict computation or procedural reasoning benefits from representing reasoning as executable code rather than free-form natural-language rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Frame reasoning tasks as program generation or use code-like intermediate representations (e.g., 'program-of-thoughts' or scratchpad code) so models leverage code pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: reported to improve performance on many reasoning tasks (especially numerical and algorithmic) compared to natural-language-only LMs; survey notes improvements but does not give precise metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Depends on code-pretraining coverage and on model's ability to generate correct runnable code; not all reasoning tasks can be naturally codified.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Often outperforms purely language-pretrained models for algorithmic/numerical tasks when using code-style reasoning; complementary to CoT approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Survey cites experiments showing models trained on code do better with code-framed reasoning; additional analyses are in cited code-pretraining papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Reasoning in Large Language Models: A Survey', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 2)</em></li>
                <li>STar: Bootstrapping reasoning with reasoning <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Folio: Natural language reasoning with firstorder logic <em>(Rating: 2)</em></li>
                <li>Roscoe: A suite of metrics for scoring step-by-step reasoning <em>(Rating: 2)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 1)</em></li>
                <li>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4961",
    "paper_id": "paper-254877753",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A prompting technique that includes intermediate natural-language reasoning steps (rationales) in demonstrations to elicit multi-step reasoning from large language models.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "GPT-3 / PaLM (and other LLMs)",
            "model_description": "Large transformer-based autoregressive language models (e.g., GPT-3 family and PaLM) that are pretrained on large text corpora and used in few-shot or zero-shot prompting regimes.",
            "model_size": "175B (GPT-3 example) / &gt;100B (PaLM and other LLMs mentioned); null when unspecified",
            "logical_reasoning_task": "Arithmetic, symbolic, commonsense multi-step reasoning benchmarks (e.g., GSM8K, Last Letter Concatenation, Coin Flip, StrategyQA)",
            "task_description": "Benchmarks requiring multi-step informal deductive and arithmetic reasoning, symbolic manipulation, or commonsense chains of inference producing a final answer.",
            "method_or_approach": "Replace standard few-shot ⟨input,output⟩ exemplars with ⟨input, chain of thought, output⟩ triples so the model generates intermediate reasoning steps before the final answer.",
            "performance": "Qualitative: reported large improvements on arithmetic, symbolic, and some commonsense reasoning tasks relative to standard few-shot prompting; can produce dramatic gains for sufficiently large models (reporting is survey-level; no single numeric accuracy given in this paper).",
            "limitations_or_failure_cases": "Generated rationales can be incorrect or inconsistent; CoT elicits reasoning-like outputs but does not prove internal systematic logical reasoning; struggles on more complex compositional tasks and can be sensitive to exemplar choice.",
            "comparison": "Outperforms standard few-shot prompting on many multi-step tasks for large models; effect typically emerges at large model scale (&gt;100B parameters); outperforms fully supervised finetuning in some OOD robustness settings per cited works.",
            "ablation_or_analysis_results": "Survey cites ablations showing CoT effect depends on exemplar relevance and ordering (Wang et al.); Saparov & He find models may produce valid individual proof steps but still choose wrong steps when multiple exist; performance scales with model size and is much stronger in very large models (Wei et al.).",
            "uuid": "e4961.0",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Zero-shot-CoT",
            "name_full": "Zero-shot Chain-of-Thought (\"Let's think step by step\")",
            "brief_description": "A minimal prompting variant that elicits step-by-step reasoning from LLMs with a generic instruction (e.g., \"Let's think step by step\") without few-shot exemplars.",
            "citation_title": "Large language models are zero-shot reasoners",
            "mention_or_use": "use",
            "model_name": "GPT-3 and similar large LLMs",
            "model_description": "Very large autoregressive LLMs used in zero-shot prompt settings; architecture: transformer decoder, pretrained on text.",
            "model_size": "e.g., GPT-3 175B referenced; null if unspecified",
            "logical_reasoning_task": "Arithmetic and commonsense reasoning tasks (same benchmarks as CoT applications, e.g., GSM8K, StrategyQA)",
            "task_description": "Tasks requiring multi-step reasoning where conventional zero-shot prompting fails but adding a simple instruction attempts to induce stepwise reasoning.",
            "method_or_approach": "Append a short instruction like \"Let's think step by step\" to the prompt to induce the model to produce intermediate reasoning and then the final answer, without demonstration exemplars.",
            "performance": "Qualitative: reported to enable nontrivial zero-shot improvements on several reasoning tasks compared to plain zero-shot prompting, but typically smaller gains than few-shot CoT; exact numbers not reported in survey.",
            "limitations_or_failure_cases": "Not universally effective across models and tasks; still limited by model scale and training distribution; can produce plausible-looking but incorrect rationales.",
            "comparison": "Sits between plain zero-shot prompting and few-shot CoT in effectiveness; simpler to use (no exemplar construction) but generally less effective than high-quality few-shot CoT for the largest gains.",
            "ablation_or_analysis_results": "Survey references studies showing prompt wording and model pretraining (e.g., training on code) modulate effectiveness; no single numeric ablation values given here.",
            "uuid": "e4961.1",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Self-consistency",
            "name_full": "Self-Consistency Decoding / Voting over Diverse Rationales",
            "brief_description": "A decoding strategy that samples multiple diverse chain-of-thought rationales from a model and selects the most consistent final answer by plurality/voting, marginalizing over rationales.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "Large LLMs (applied to models used in CoT experiments, e.g., GPT-3 / PaLM)",
            "model_description": "Transformer-based LLMs used with stochastic decoding (sampling) to produce multiple reasoning trajectories which are then aggregated.",
            "model_size": "null (survey-level references to large models)",
            "logical_reasoning_task": "Arithmetic and multi-step reasoning benchmarks (e.g., GSM8K and related CoT-evaluated tasks)",
            "task_description": "Tasks where multiple valid reasoning trajectories may lead to the same correct answer; aggregation increases robustness to sampling noise.",
            "method_or_approach": "Use stochastic decoding to sample many chain-of-thought outputs, extract final answers, and pick the most frequent answer (or marginalize answers) as final prediction.",
            "performance": "Qualitative: reported to improve accuracy over single greedy chain-of-thought decoding often substantially; specific numeric gains are referenced in original paper but not reproduced numerically in this survey.",
            "limitations_or_failure_cases": "Increases compute (multiple decodings); if model samples diverse but systematically biased wrong rationales, voting may reinforce incorrect majority; doesn't fix rationale validity.",
            "comparison": "Improves over greedy CoT decoding; complementary to exemplar design and rationale refinement methods.",
            "ablation_or_analysis_results": "Survey cites evidence that marginilizing across sampled rationales improves final-answer accuracy and out-of-distribution robustness versus single-run CoT.",
            "uuid": "e4961.2",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Least-to-most",
            "name_full": "Least-to-Most Prompting (Problem Decomposition)",
            "brief_description": "A decomposition prompting strategy that breaks complex problems into a sequence of simpler subproblems solved in order, conditioning later steps on earlier answers.",
            "citation_title": "Least-to-most prompting enables complex reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Large LLMs (CoT-capable models)",
            "model_description": "Transformer LLMs prompted iteratively to produce and solve subquestions; leverages in-context decomposition rather than monolithic CoT.",
            "model_size": "null (survey does not provide explicit parameter counts for experiments)",
            "logical_reasoning_task": "Compositional and multi-step reasoning tasks, semantic parsing and complex arithmetic requiring compositional generalization",
            "task_description": "Tasks requiring decomposition into intermediate subgoals (e.g., multi-step word problems, compositional semantic parsing).",
            "method_or_approach": "Prompt the model to decompose a complex question into ordered subproblems (least complex first) and solve them sequentially, passing intermediate answers forward.",
            "performance": "Qualitative: reported improvements on compositional tasks that standard CoT struggles with; helps with length/generalization in some studies.",
            "limitations_or_failure_cases": "Requires good decomposition strategy; failures in decomposition lead to cascading errors; not guaranteed to scale to arbitrary task complexity.",
            "comparison": "Often improves over monolithic CoT for compositional tasks; related approaches include dynamic least-to-most, decomposed prompting, and successive prompting.",
            "ablation_or_analysis_results": "Survey cites follow-up work showing dynamic exemplar selection and syntactic parsing to support decomposition further improve results; specific ablations are in cited papers.",
            "uuid": "e4961.3",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "STaR",
            "name_full": "Self-Taught Reasoner (STaR)",
            "brief_description": "A bootstrapping/iterative fine-tuning pipeline where an LLM generates chain-of-thought rationales, selects rationales that lead to correct answers, and then fine-tunes on those successful rationales to improve reasoning performance.",
            "citation_title": "STar: Bootstrapping reasoning with reasoning",
            "mention_or_use": "use",
            "model_name": "LLMs used as both generator and training target (e.g., GPT-family style models in cited work)",
            "model_description": "Pretrained autoregressive LLMs that are iteratively fine-tuned on their own high-quality chain-of-thought outputs to self-improve.",
            "model_size": "null (survey-level description)",
            "logical_reasoning_task": "General multi-step reasoning benchmarks where CoT is applicable (e.g., arithmetic and commonsense reasoning tasks)",
            "task_description": "Tasks where generating and then filtering correct rationales can create additional training data to refine the model's reasoning abilities.",
            "method_or_approach": "Generate CoT outputs, filter those that produce correct final answers, fine-tune the model on successful ⟨input, CoT, output⟩ triples, and iterate.",
            "performance": "Qualitative: reported improvements in subsequent model ability to produce correct rationales and answers; survey notes STaR as an effective self-improvement approach (quantitative metrics are in the original STaR paper).",
            "limitations_or_failure_cases": "Relies on an initial strong model to produce sufficient correct rationales; risk of overfitting to self-generated artifacts; quality of filtering crucial.",
            "comparison": "Offers alternative to supervised finetuning on human-written rationales; can outperform baseline few-shot CoT when ample successful self-generated rationales exist.",
            "ablation_or_analysis_results": "Survey references Huang et al. (2022a) and Zelikman et al. (2022) demonstrating self-improvement without supervised data in some settings; performance depends on filtering heuristics and initial model quality.",
            "uuid": "e4961.4",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Verifier",
            "name_full": "Trained Verifier for Rationale Selection",
            "brief_description": "Train a separate verifier model to score candidate rationales and final answers generated by a reasoning model and select the highest-scoring solution to improve final-answer correctness.",
            "citation_title": "Training verifiers to solve math word problems",
            "mention_or_use": "use",
            "model_name": "Primary LLM (generator) plus a separate verifier model (typically a classifier/LM)",
            "model_description": "Pipeline where a generative LLM produces multiple rationale+solution candidates and an auxiliary verifier (trained discriminatively or via LLM scoring) ranks candidates.",
            "model_size": "null (survey level)",
            "logical_reasoning_task": "Math word problems and numerical reasoning benchmarks where correctness can be judged (e.g., GSM8K-style problems)",
            "task_description": "Problems where selecting the correct final answer from multiple candidate reasoning traces improves accuracy.",
            "method_or_approach": "Generate candidate CoT traces and answers, use a trained verifier to assign scores and pick the highest-scoring solution; can also use the LLM itself as verifier.",
            "performance": "Qualitative: reported to improve final-answer selection and reduce errors from incorrect single rationales; numeric gains referenced in original cited work (Cobbe et al.).",
            "limitations_or_failure_cases": "Requires labeled data or reliable criteria to train verifier; verifier can inherit biases; training verifier adds complexity and compute.",
            "comparison": "Improves over naive single-run CoT by filtering; complementary to self-consistency and rationale exploration strategies.",
            "ablation_or_analysis_results": "Survey cites experiments where verifier selection increases accuracy compared to greedy CoT and where LLM-based verifiers can serve as an alternative to trained discriminative verifiers.",
            "uuid": "e4961.5",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "FOLIO",
            "name_full": "FOLIO (First-Order Logic Inference/Ontology dataset)",
            "brief_description": "A dataset designed to test first-order logic reasoning capabilities of LLMs using natural-language premises and conclusions derived from formal first-order formulas.",
            "citation_title": "Folio: Natural language reasoning with firstorder logic",
            "mention_or_use": "mention",
            "model_name": "LLMs evaluated in referenced studies (survey mentions LLMs generally; specific model experiments in FOLIO paper)",
            "model_description": "Benchmarks for LLMs to evaluate formal first-order logical entailment / validity when premises and hypotheses are expressed in natural language.",
            "model_size": null,
            "logical_reasoning_task": "First-order logic reasoning (validity of conclusions from premises; formal deductive reasoning)",
            "task_description": "Determine correctness/validity of a conclusion given a set of premises mapped from first-order logic statements to natural language; assesses formal logical inference capabilities.",
            "method_or_approach": "Evaluation dataset enabling formal stepwise analysis of model reasoning; used to probe models' ability to perform deductive first-order reasoning.",
            "performance": "Qualitative: survey states LLMs still struggle with such formal tasks; Han et al. find weaknesses and incomplete proofs in many cases (specific metrics are in the FOLIO paper).",
            "limitations_or_failure_cases": "Models often fail at complex first-order reasoning, produce incomplete or incorrect proofs, and struggle when multiple derivation choices exist.",
            "comparison": "Shows gap between informal CoT-style success and strict formal logical reasoning; models that perform well on informal tasks often underperform on FOLIO-style formal logic benchmarks.",
            "ablation_or_analysis_results": "Survey references FOLIO as a targeted diagnostic; ablations and per-category performance analyses are in the original FOLIO paper.",
            "uuid": "e4961.6",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "PrOntoQA",
            "name_full": "PrOntoQA (Synthetic Ontology-based QA)",
            "brief_description": "A synthetic dataset generated from (real or fictional) ontologies with unique proofs per example, designed to enable formal, step-by-step evaluation of reasoning traces produced by LMs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs evaluated in the referenced work (survey-level; specific models in original PrOntoQA paper)",
            "model_description": "Synthetic task generator that maps formal proofs to natural-language facts/sentences so that each example has a unique ground-truth proof trace.",
            "model_size": null,
            "logical_reasoning_task": "Step-by-step proof generation and verification; synthetic deductive reasoning grounded in ontologies.",
            "task_description": "Assess whether a model can produce the unique correct chain of inference (proof) from premises to conclusion; enables analysis of individual reasoning steps.",
            "method_or_approach": "Dataset creation and diagnostic evaluation; used to test faithfulness and correctness of model-produced rationales at the step level.",
            "performance": "Qualitative: survey reports models can produce valid individual proof steps in some cases but may select wrong steps when multiple exist; detailed numbers are in original PrOntoQA work.",
            "limitations_or_failure_cases": "Models sometimes choose incorrect proof steps among alternatives leading to incomplete or wrong proofs; synthetic nature may differ from natural tasks.",
            "comparison": "Provides a stricter diagnostic than end-task accuracy; highlights discrepancies between correct final answer and correct internal proof.",
            "ablation_or_analysis_results": "Survey notes PrOntoQA enables formal analysis of each reasoning step; specific ablation details are in the originating paper (citation omitted in survey references).",
            "uuid": "e4961.7",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "ROSCOE",
            "name_full": "ROSCOE (Suite of Step-by-Step Reasoning Metrics)",
            "brief_description": "A set of interpretable evaluation metrics for scoring step-by-step reasoning along multiple perspectives, including semantic alignment, logical inference, and coherence.",
            "citation_title": "Roscoe: A suite of metrics for scoring step-by-step reasoning",
            "mention_or_use": "mention",
            "model_name": "Any LLM whose chain-of-thought outputs are evaluated (survey-level)",
            "model_description": "Metric suite applied to CoT/rationale outputs to quantify the quality of intermediate reasoning steps beyond end-task accuracy.",
            "model_size": null,
            "logical_reasoning_task": "Evaluation/analysis task rather than a single reasoning benchmark; applied where step-level rationales exist.",
            "task_description": "Measure semantic and logical quality of generated rationales: alignment to gold steps, correctness of inferences, and linguistic coherence.",
            "method_or_approach": "Automated and interpretable scoring metrics to analyze and diagnose chain-of-thought outputs in detail.",
            "performance": "Not applicable (ROSCOE is an evaluation methodology); enables deeper analysis that often reveals rationale errors even when final answers are correct.",
            "limitations_or_failure_cases": "Score design depends on available references; may require gold step annotations; not a replacement for end-task metrics.",
            "comparison": "Provides more granular insight than single-number accuracy metrics; used in survey as an example of deeper analysis tools.",
            "ablation_or_analysis_results": "Survey cites ROSCOE as enabling multi-faceted analysis; concrete ablations and metric breakdowns are in the ROSCOE paper.",
            "uuid": "e4961.8",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Code/pretraining on code",
            "name_full": "Training on Code (Codex / Code-pretrained LLMs)",
            "brief_description": "Observation that models pretrained or fine-tuned on code perform better on certain reasoning tasks when reasoning is framed as code generation or program execution.",
            "citation_title": "Evaluating large language models trained on code",
            "mention_or_use": "mention",
            "model_name": "Codex and code-pretrained variants of LLMs (e.g., models from Chen et al.)",
            "model_description": "Transformer LLMs pretrained on code (source code, APIs, etc.) and natural language, enabling them to perform program-like precise computations.",
            "model_size": null,
            "logical_reasoning_task": "Numerical reasoning, algorithmic tasks, and other reasoning tasks reframed as code generation/program execution",
            "task_description": "Tasks where strict computation or procedural reasoning benefits from representing reasoning as executable code rather than free-form natural-language rationales.",
            "method_or_approach": "Frame reasoning tasks as program generation or use code-like intermediate representations (e.g., 'program-of-thoughts' or scratchpad code) so models leverage code pretraining.",
            "performance": "Qualitative: reported to improve performance on many reasoning tasks (especially numerical and algorithmic) compared to natural-language-only LMs; survey notes improvements but does not give precise metrics.",
            "limitations_or_failure_cases": "Depends on code-pretraining coverage and on model's ability to generate correct runnable code; not all reasoning tasks can be naturally codified.",
            "comparison": "Often outperforms purely language-pretrained models for algorithmic/numerical tasks when using code-style reasoning; complementary to CoT approaches.",
            "ablation_or_analysis_results": "Survey cites experiments showing models trained on code do better with code-framed reasoning; additional analyses are in cited code-pretraining papers.",
            "uuid": "e4961.9",
            "source_info": {
                "paper_title": "Towards Reasoning in Large Language Models: A Survey",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 2,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "STar: Bootstrapping reasoning with reasoning",
            "rating": 2,
            "sanitized_title": "star_bootstrapping_reasoning_with_reasoning"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Folio: Natural language reasoning with firstorder logic",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "Roscoe: A suite of metrics for scoring step-by-step reasoning",
            "rating": 2,
            "sanitized_title": "roscoe_a_suite_of_metrics_for_scoring_stepbystep_reasoning"
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 1,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        },
        {
            "paper_title": "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
            "rating": 1,
            "sanitized_title": "language_models_are_greedy_reasoners_a_systematic_formal_analysis_of_chainofthought"
        }
    ],
    "cost": 0.01735875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Reasoning in Large Language Models: A Survey</p>
<p>Jie Huang 
Department of Computer Science
University of Illinois at Urbana-Champaign</p>
<p>Kevin Chen 
Department of Computer Science
University of Illinois at Urbana-Champaign</p>
<p>Chuan Chang kcchang@illinois.edu 
Department of Computer Science
University of Illinois at Urbana-Champaign</p>
<p>Towards Reasoning in Large Language Models: A Survey
CA8B2B910B2B9C57AD835F2672E95E7E
Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking.In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large.However, it is not yet clear to what extent LLMs are capable of reasoning.This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions.Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work. 1</p>
<p>Introduction</p>
<p>Reasoning is a cognitive process that involves using evidence, arguments, and logic to arrive at conclusions or make judgments.It plays a central role in many intellectual activities, such as problem solving, decision making, and critical thinking.The study of reasoning is important in fields like psychology (Wason and Johnson-Laird, 1972), philosophy (Passmore, 1961), and computer science (Huth and Ryan, 2004), as it helps individuals make decisions, solve problems, and think critically.</p>
<p>Recently, large language models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;Chung et al., 2022;OpenAI, 2022, inter alia) such as Chat-GPT have made significant advancements in natural language processing and related fields.It has been shown that these models exhibit emergent behaviors, including the ability to "reason", when they are large enough (Wei et al., 2022a).For example, by providing the models with "chain of thoughts", i.e., reasoning exemplars, or a simple prompt "Let's think step by step", these models are able to answer questions with explicit reasoning steps (Wei et al., 2022b;Kojima et al., 2022), e.g., "all whales are mammals, all mammals have kidneys; therefore, all whales have kidneys."This has sparked considerable interest in the community since reasoning ability is a hallmark of human intelligence that is frequently considered missed in current artificial intelligence systems (Marcus, 2020;Russin et al., 2020;Mitchell, 2021;Bommasani et al., 2021).</p>
<p>However, despite the strong performance of LLMs on certain reasoning tasks, it remains unclear whether LLMs are actually reasoning and to what extent they are capable of reasoning.For example, Kojima et al. (2022) claim that "LLMs are decent zero-shot reasoners (p.1)", while Valmeekam et al. (2022) conclude that "LLMs are still far from achieving acceptable performance on common planning/reasoning tasks which pose no issues for humans to do (p.2)."This limitation is also stated by Wei et al. (2022b):</p>
<p>"we qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually reasoning (p.9)."Therefore, in this paper, we aim to provide a comprehensive overview and engage in an insightful discussion on the current state of knowledge on this fast-evolving topic.We initiate our exploration with a clarification of the concept of reasoning ( §2).Subsequently, we turn our attention to the techniques for enhancing/eliciting reasoning in LLMs ( §3), the methods and benchmarks for evaluating reasoning in LLMs ( §4), and the key findings and implications in this field ( §5).Finally, we reflect on and discuss the current state of the field ( §6). 2 What is Reasoning?</p>
<p>Reasoning is the process of thinking about something in a logical and systematic way, using evidence and past experiences to reach a conclusion or make a decision (Wason and Johnson-Laird, 1972;Wason, 1968;Galotti, 1989;Fagin et al., 2004;McHugh and Way, 2018).Reasoning involves making inferences, evaluating arguments, and drawing logical conclusions based on available information.Although "reasoning" is a term that is commonly used in literature and daily life, it is also an abstract concept that can refer to many things.To help the reader better understand this concept, we summarize several main categories of reasoning that are commonly recognized: Deductive reasoning.Deductive reasoning is a type of reasoning in which a conclusion is drawn based on the truth of the premises.In deductive reasoning, the conclusion must necessarily follow from the premises, meaning that if the premises are true, the conclusion must also be true.For example:</p>
<p>• Premise: All mammals have kidneys.</p>
<p>• Premise: All whales are mammals.</p>
<p>• Conclusion: All whales have kidneys.</p>
<p>Inductive reasoning.Inductive reasoning is a type of reasoning in which a conclusion is drawn based on observations or evidence.The conclusion is likely to be true based on the available evidence, but it is not necessarily certain.For example:</p>
<p>• Observation: Every time we see a creature with wings, it is a bird.• Observation: We see a creature with wings.</p>
<p>• Conclusion: The creature is likely to be a bird.</p>
<p>Abductive reasoning.Abductive reasoning is a type of reasoning in which a conclusion is drawn based on the best explanation for a given set of observations.The conclusion is the most likely explanation based on the available evidence, but it is not necessarily certain.For example:</p>
<p>• Observation: The car cannot start and there is a puddle of liquid under the engine.• Conclusion: The most likely explanation is that the car has a leak in the radiator.</p>
<p>Other types of reasoning include analogical reasoning, which involves making comparisons between two or more things in order to make inferences or arrive at conclusions; causal reasoning, which involves identifying and understanding the causes and effects of events or phenomena; and probabilistic reasoning, which involves making decisions or arriving at conclusions based on the likelihood or probability of certain outcomes.</p>
<p>Formal Reasoning vs Informal Reasoning.Formal reasoning is a systematic and logical process that follows a set of rules and principles, often used in mathematics and logic.Informal reasoning is a less structured approach that relies on intuition, experience, and common sense to draw conclusions and solve problems, and is often used in everyday life.Formal reasoning is more structured and reliable, while informal reasoning is more adaptable and open-ended, but may also be less reliable.We refer the reader to Galotti (1989); Bronkhorst et al. (2020) for a detailed distinction between them.</p>
<p>Reasoning in Language Models.The concept of reasoning in language models has been around for some time, but there is not a clear definition of what it entails.In the literature, the term "reasoning" is often used to refer to informal reasoning, although it is not always explicitly stated that it is informal (Cobbe et al., 2021;Wei et al., 2022b, inter alia).Different forms of reasoning may be used depending on the task, benchmark, or method being used, e.g., deductive reasoning (Cobbe et al., 2021;Creswell et al., 2022;Han et al., 2022b, inter alia), inductive reasoning (Yang et al., 2022;Misra et al., 2022, inter alia) or abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia).In this paper, we encompass various forms of reasoning, with a particular focus on "informal deductive reasoning" in large language models since it is a widely used form in which the conclusion is guaranteed to be true as long as the premises are true.</p>
<p>Towards Reasoning in Large Language Models</p>
<p>Reasoning, particularly multi-step reasoning, is often seen as a weakness in language models and other NLP models (Bommasani et al., 2021;Rae et al., 2021;Valmeekam et al., 2022).Recent research has suggested that reasoning ability may emerge in language models at a certain scale, such as models with over 100 billion parameters (Wei et al., 2022a,b;Cobbe et al., 2021).In this paper, we follow Wei et al. (2022a) in considering reasoning as an ability that is rarely present in smallscale models like GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2019), and therefore focus on techniques applicable to improving or eliciting "reasoning"2 in LLMs such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022).</p>
<p>Fully Supervised Finetuning</p>
<p>Before discussing reasoning in large language models, it is worth mentioning there is research working on eliciting/improving reasoning in small language models through fully supervised finetuning on specific datasets.For example, Rajani et al.</p>
<p>(2019) finetune a pretrained GPT model (Radford et al., 2018) to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks (Talmor et al., 2019).Talmor et al. (2020) train RoBERTa (Liu et al., 2019)  There are two major limitations of fully supervised finetuning.First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create.Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions.</p>
<p>Prompting &amp; In-Context Learning</p>
<p>Large language models such as GPT-3 (Brown et al., 2020) have demonstrated remarkable fewshot performance across a variety of tasks through in-context learning.These models can be prompted with a question and a few ⟨input, output⟩ exemplars to potentially solve a problem through "reasoning", either implicitly or explicitly.However, research has shown that these models still fall short when it comes to tasks that require multiple steps of reasoning to solve (Bommasani et al., 2021;Rae et al., 2021;Valmeekam et al., 2022).This may be due to a lack of exploration into the full capabilities of these models, as recent studies have suggested.</p>
<p>Chain of Thought and Its Variants</p>
<p>To encourage LLMs to engage in reasoning rather than simply providing answers directly, we may guide LLMs to generate "reasoning" explicitly.One approach for doing this is chain-of-thought prompting, proposed by Wei et al. (2022b).This approach involves providing a few examples of "chain of thought" (CoT), which are intermediate natural language reasoning steps, in the prompt to LLMs (Figure 2).Specifically, in CoT prompting, ⟨input, output⟩ demonstrations are replaced with ⟨input, chain of thought, output⟩ triples, e.g., "[input] Roger has 5 tennis balls.He buys 2 more cans of tennis balls.Each can has 3 tennis balls.How many tennis balls does he have now?[chain of thought] Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls.5 + 6 = 11.[output] The answer is 11."In this way, given a target question, the model learns to generate explicit ratio- nale before producing the final answer.Experimental results show that this simple idea can improve LLMs' few-shot performance on arithmetic, symbolic, and commonsense reasoning tasks, sometimes to a striking degree.</p>
<p>There are several variants of chain-of-thought prompting that have been proposed in the literature, in a different form or to solve a specific problem.</p>
<p>Different Form: Kojima et al. (2022) introduce Zero-shot-CoT, in which LLMs are simply prompted with the phrase "Let's think step by step" after the input, in order to elicit reasoning without the need for few-shot demonstrations.Madaan et al. (2022); Gao et al. (2022); Chen et al. (2022) find that LLMs trained with code, e.g., Codex (Chen et al., 2021), can achieve better performance on reasoning tasks by framing reasoning as code generation.Wang et al. (2022a) propose to iteratively prompt chain of thought.He et al. (2023) attempt to retrieve external knowledge in CoT to improve faithfulness of reasoning.</p>
<p>Specific Problem/Setting: Before chain of thought, Nye et al. (2022) also try to use intermediate computations, named "scratchpads", to improve language models' reasoning performance in both finetuning and few-shot regimes, with a particular focus on programs.Shi et al. (2022) attempt to solve multilingual reasoning tasks with CoT in the native language, CoT in English (regardless of the problem language), and CoT in English (with the problem translated to English).Chen (2022) apply CoT to table-based reasoning, finding that LLMs can achieve strong performance on table tasks with only one exemplar.Prystawski et al. (2022) demonstrate that CoT can improve LLMs' performance on paraphrase selection for metaphors.Lu et al. (2022) apply chain of thought to solve multimodal science questions.</p>
<p>Rationale Engineering</p>
<p>The original version of chain-of-thought prompting, proposed by Wei et al. (2022b), relies on manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation.Rationale engineering aims to more effectively elicit or utilize reasoning in LLMs.This can be achieved through rationale refinement, which involves creating more effective examples of reasoning steps, or through rationale exploration and rationale verification, which involve exploring and verifying the rationales produced by LLMs.A summary of raltionale engineering is illustrated in Figure 2.</p>
<p>Rationale refinement.The choice of exemplars can significantly affect the few-shot performance of LLMs, as demonstrated in research such as Liu et al. (2022b), which also appears in chain-of-thought prompting.Rationale refinement aims to create and refine rationale examples that are better able to elicit reasoning in LLMs.Fu et al. (2022b) propose complexity-based prompting to create rationales with more reasoning steps.Their experiments show that the performance of LLMs improves with the increased rationale complexity.Similarly, Zhou et al. (2022c) propose algorithmic prompting, which suggests that providing more thorough examples of solutions can help improve reasoning performance on some simple math calculations.Zhang et al. (2022b) design Auto-CoT to automatically construct exemplars by partitioning questions from a given dataset into clusters and then using Zero-Shot-CoT (Kojima et al., 2022) to generate the rationale for a representative question from each cluster.The analysis shows that making exemplars diverse is important in prompting LLMs to produce better rationales.</p>
<p>Rationale exploration.In addition to providing better exemplars, we can allow LLMs to fully explore various ways of reasoning to improve their performance on reasoning tasks, named rationale exploration.Based on the idea that complex problems often admit multiple ways of thinking that can lead to their unique correct answer, Wang et al. (2022c) present a decoding strategy called selfconsistency to improve upon the traditional greedy decoding used in chain-of-thought prompting.This strategy involves sampling a diverse set of rationales, rather than just the greedy one, and selecting the most consistent answer by marginalizing out the sampled rationales.The idea is also used in Fu et al. (2022b) to vote over the top complex rationales.To further improve performance, Li et al. (2022b) suggest providing different demonstrations for each question by sampling exemplars from an exemplar base, in order to increase the diversity of the sampled rationales.</p>
<p>Rationale verification.Ensuring that the rationales produced by LLMs are valid is critical, as incorrect rationales can lead to incorrect final predictions (Ye and Durrett, 2022).To address this issue, the process of rationale verification aims to verify whether the rationales produced by LLMs lead to the correct final answers.Cobbe et al. (2021) propose augmenting LLMs with a trained verifier that assigns a score to each rationale and solution generated by the LLM, selecting the highest-ranked solution as the final answer when solving math word problems.Li et al. (2022b) also use this technique to guide rationale selection, in conjunction with the process of rationale exploration.Different from the above methods that train an external verifier to verify the rationales, Weng et al. (2022) suggest using LLMs themselves as the verifiers.</p>
<p>Problem Decomposition</p>
<p>Chain-of-thought prompting, while effective for eliciting reasoning in LLMs, can struggle with complex tasks, e.g., tasks that require compositional generalization (Lake and Baroni, 2018; Keysers et al., 2020).To solve a complex problem, it is helpful to first break it down into smaller, more manageable subproblems.By solving each of these subproblems, we can effectively solve the complex problem.This technique is called problem decom-position or divide and conquer (Talmor and Berant, 2018;Min et al., 2019;Perez et al., 2020).</p>
<p>Based on this idea, Zhou et al. (2022a) propose least-to-most prompting, which consists of two steps: decomposing the complex problem into subproblems and solving these subproblems in a specific order, with each subproblem being facilitated by the answers obtained from previously solved subproblems.As follow-up work, Drozdov et al. (2022) introduce dynamic least-to-most prompting, which is designed to solve more realistic semantic parsing problems by decomposing the problems with prompting-based syntactic parsing and dynamically selecting exemplars based on the decomposition.In addition, Khot et al. (2022) design decomposed prompting, which breaks down a complex problem into subproblems that can be handled by a shared library of prompting-based LLMs, each specialized in a particular subproblem.Furthermore, Dua et al. (2022) develop successive prompting, which iteratively decomposes a complex problem into a simple problem, with the next subproblem prediction having access to the answers to the previous subproblems.While the above methods decompose or solve compositional questions with multiple forward passes, Press et al. (2022) suggest decomposing and solving the input question in one forward pass using CoT prompting.Overall, these techniques show promise for helping LLMs to solve complex tasks by decomposing the problem into more manageable subproblems.</p>
<p>Others</p>
<p>There are other techniques that have been developed to facilitate reasoning in LLMs for specific tasks or settings.For instance, Creswell et al. (2022); Creswell and Shanahan (2022) introduce a selection-inference framework that uses LLMs as modules to select and infer reasoning steps from a set of facts that culminate in the final answer.Kazemi et al. ( 2022) suggest using backward chaining, i.e., from goal to the set of facts that support it, instead of forward chaining like Creswell et al. (2022); Creswell and Shanahan (2022).In addition, Jung et al. (2022) propose a method for solving binary questions by prompting LLMs abductively and recursively to rationalize each option.Zhou et al. (2022b) design a technique for performing numerical reasoning on complex numbers by replacing the complex numbers with simple numbers to produce simpler expressions, and then using these expressions to perform calculations on the complex numbers.There are also efforts to distill reasoning from LLMs into smaller models, such as the work by Li et al. (2022a); Shridhar et al. (2022);Magister et al. (2022).Finally, we refer the reader to Dohan et al. (2022)'s position paper on language model cascade, which presents a unifying framework for understanding chain-of-thought prompting and research in this line.</p>
<p>Hybrid Method</p>
<p>While "prompting" techniques can help elicit or better utilize reasoning in large language models to solve reasoning tasks, they do not actually improve the reasoning capabilities of the LLMs themselves, as the parameters of the models remain unchanged.In contrast, the "hybrid approach" aims to simultaneously improve the reasoning capabilities of LLMs and make better use of these models in order to solve complex problems.This approach involves both enhancing the reasoning capabilities of the LLMs and using techniques such as prompting to effectively utilize these capabilities.</p>
<p>Reasoning-Enhanced Training and Prompting</p>
<p>One approach to improving the reasoning capabilities of LLMs is to pretrain or finetune the models on datasets that include "reasoning".Lewkowycz et al. (2022); Taylor et al. (2022) find that LLMs trained on datasets containing scientific and mathematical data can achieve better performance on reasoning tasks like quantitative reasoning problems when using CoT prompting3 .Pi et al. (2022) show that continually pretraining with SQL data can boost the performance of language models, e.g., T5 (Raffel et al., 2020), on natural language reasoning such as numerical reasoning and logical reasoning.finetuning and scratchpad prompting results in a significant improvement in LLMs' ability to generalize to longer problems, while this phenomenon is not observed in the standard fully supervised finetuning paradigm.</p>
<p>Bootstrapping &amp; Self-Improving</p>
<p>Instead of finetuning LLMs on pre-built datasets that include reasoning, there are studies that have explored the idea of using LLMs to self-improve their reasoning abilities through a process known as bootstrapping.One example of this is the Self-Taught Reasoner (STaR) introduced by Zelikman et al. ( 2022), in which a LLM is trained and refined on its own output iteratively.Specifically, with CoT prompting, the model first generates initial rationales.And then, the model is finetuned on rationales that lead to correct answers.This process can be repeated, with each iteration resulting in an improved model that can generate better training data, which in turn leads to further improvements.As a follow-up to this work, Huang et al. (2022a) show that LLMs are able to self-improve their reasoning abilities without the need for supervised data by leveraging the self-consistency of reasoning (Wang et al., 2022c).</p>
<p>Measuring Reasoning in Large Language Models</p>
<p>We summarize methods and benchmarks for evaluating reasoning abilities of LLMs in this section.</p>
<p>End Task Performance</p>
<p>One way to measure reasoning abilities of LLMs is to report their performance, e.g., accuracy, on end tasks that require reasoning.We list some common benchmarks as follows.</p>
<p>Arithmetic Reasoning.Arithmetic reasoning is the ability to understand and apply mathematical concepts and principles in order to solve problems involving arithmetic operations.This involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems.</p>
<p>Representative benchmarks for arithmetic reasoning include GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), AS-Div (Miao et al., 2020), AQuA (Ling et al., 2017), and MAWPS (Roy and Roth, 2015).It is worth mentioning that Anil et al. (2022) generate the Parity Datasets and the Boolean Variable Assignment Dataset for analyzing the length generalization capabilities of LLMs ( §3.3.1).</p>
<p>Commonsense Reasoning.Commonsense Reasoning is the use of everyday knowledge and understanding to make judgments and predictions about new situations.It is a fundamental aspect of human intelligence that enables us to navigate our environment, understand others, and make decisions with incomplete information.Benchmarks that can be used for testing commonsense reasoning abilities of LLMs include CSQA (Talmor et al., 2019), StrategyQA (Geva et al., 2021), and ARC (Clark et al., 2018).We refer the reader to Bhargava and Ng ( 2022)'s survey for more work in this domain.</p>
<p>Symbolic Reasoning.Symbolic reasoning is a form of reasoning that involves the manipulation of symbols according to formal rules.In symbolic reasoning, we use abstract symbols to represent concepts and relationships, and then manipulate those symbols according to precise rules in order to draw conclusions or solve problems.Two benchmarks of symbolic reasoning are presented in Wei et al. (2022b), including Last Letter Concatenation and Coin Flip.</p>
<p>Others.In practice, there are many benchmarks that can be used to evaluate reasoning abilities of LLMs (indirectly), as long as the downstream task involves reasoning.BIG-bench (Srivastava et al., 2022), for example, includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement.Other benchmarks, such as SCAN (Lake and Baroni, 2018) and the one proposed by Anil et al. (2022), focus on evaluating generalization ability.LLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA (Pasupat and Liang, 2015), FetaQA (Nan et al., 2022), as suggested by Chen (2022).In addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as CommonGen (Lin et al., 2020;Liu et al., 2022a) and Open Relation Modeling (Huang et al., 2022b,d).</p>
<p>Analysis on Reasoning</p>
<p>Although LLMs have demonstrated impressive performance on various reasoning tasks, the extent to which their predictions are based on true reasoning or simple heuristics is not always clear.This is because most existing evaluations focus on their accuracy on end tasks, rather than directly assessing their reasoning steps.While some error analysis has been conducted on the generated rationales of LLMs (Wei et al., 2022b;Kojima et al., 2022, inter alia), this analysis has often been limited in depth.</p>
<p>There have been some efforts to develop metrics and benchmarks that enable a more formal/deep analysis of reasoning in LLMs.Golovneva et al. (2022) design ROSCOE, a set of interpretable, detailed step-by-step evaluation metrics covering various perspectives including semantic alignment, logical inference, semantic similarity, and language coherence.Saparov and He (2022) create a synthetic dataset called PrOntoQA that is generated from real or fictional ontologies.Each example in the dataset has a unique proof, which can be converted to simple sentences and back again, allowing for a formal analysis of each reasoning step.Han et al. (2022a) introduce a dataset called FO-LIO to test the first-order logic reasoning capabilities of LLMs.FOLIO contains first-order logic reasoning problems that require models to determine the correctness of conclusions given a set of premises.In addition, Wang et al. (2022b) conduct ablation experiments on CoT and find that LLMs may also perform reasoning while prompting with invalid rationals.Their study also suggests that being relevant to the query and correctly ordering the reasoning steps are important for CoT prompting.</p>
<p>In summary, most existing studies primarily report the performance of the models on downstream reasoning tasks, without a detailed examination of the quality of the rationales produced.This leaves open the question of whether the models are actually able to reason in a way that is similar to human reasoning, or whether they are simply able to achieve good performance on the tasks through other means.Further research is needed to more formally analyze the reasoning abilities of LLMs.</p>
<p>Findings and Implications</p>
<p>In this section, we summarize the important findings and implications of studies on reasoning in large language models.</p>
<p>Reasoning seems an emergent ability of LLMs.Wei et al. (2022a,b); Suzgun et al. (2022) show that reasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters).This suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks.However, the reason for this emergent ability is not yet fully understood.We refer the reader to Wei et al. (2022a); Fu et al. (2022a) for some potential explanations.</p>
<p>Chain of thought elicits "reasoning" of LLMs.The use of chain-of-thought (CoT) prompts (Wei et al., 2022b) has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of Wei et al. (2022a,b); Suzgun et al. (2022).Additionally, Saparov and He (2022) ( §4.2) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual.However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs.Moreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement.In addition to these benefits, the use of CoT prompts has been shown to improve the out-ofdistribution robustness of LLMs (Wei et al., 2022b;Zhou et al., 2022a;Anil et al., 2022, inter alia), an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms.</p>
<p>LLMs show human-like content effects on reasoning.According to Dasgupta et al. (2022), LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature.For example, the models' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions.These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well.This provides some evidence that language models may "reason" in a way that is similar to human reasoning.</p>
<p>LLMs are still unskilled at complex reasoning.Although LLMs seem to possess impressive reasoning capabilities with the techniques described in §3, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as Valmeekam et al. (2022); Han et al. (2022a); Ruis et al. (2022).For instance, Valmeekam et al. (2022) find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022) struggle to perform effectively.These findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard.</p>
<p>6 Reflection, Discussion, and Future Directions</p>
<p>Why reasoning?Reasoning is the process of thinking about something in a logical and systematic way, and it is a key aspect of human intelligence.By incorporating reasoning capabilities into language models, we can enable them to perform tasks that require more complex and nuanced thinking, such as problem solving, decision making, and planning (Huang et al., 2022e,f;Song et al., 2022).This can improve the performance of these models on downstream tasks and increase their out-ofdistribution robustness (Wei et al., 2022a,b;Suzgun et al., 2022;Zhou et al., 2022a;Anil et al., 2022).</p>
<p>In addition, reasoning can make language models more explainable and interpretable, as it provides explicit rationales for their predictions.</p>
<p>Right task/application?As Valmeekam et al. (2022) point out, current benchmarks may not adequately reflect the reasoning capabilities of LLMs.</p>
<p>In addition, tasks such as solving simple math problems and concatenating letters in strings ( §4.1) are artificial and do not accurately reflect real-world situations.To truly understand the reasoning ability of LLMs, it is important to consider more realistic and meaningful applications such as decision making (Edwards, 1954), legal reasoning (Levi, 2013), and scientific reasoning (Zimmerman, 2000).Our ultimate goal should not be to enable LLMs to solve simple math problems, which can be simply done with other programs.When conducting relevant research, it is essential to ask whether the specific task being tackled is meaningful and whether the proposed method can be generalized to more realistic tasks and applications.</p>
<p>Are language models really able to reason?</p>
<p>There are several indications that LLMs are able to reason, including 1) high performance on various tasks requiring reasoning (Suzgun et al., 2022);</p>
<p>2) the ability to reason step-by-step with chainof-thought prompting (Wei et al., 2022b); and 3) the reflection of human-like content effects on reasoning (Dasgupta et al., 2022).However, these findings are not sufficient to conclude that LLMs can truly reason.For 1), it is not clear whether the models are making predictions based on reasoning or heuristics (Patel et al., 2021).For many existing benchmarks on reasoning, actually, we can design a program with heuristic rules to achieve very high performance.We usually do not think a program relying on heuristic rules is capable of reasoning.</p>
<p>For 2), although the models seem to reason stepby-step, the generated rationales may be incorrect and inconsistent.It is possible that the models are "generating reasoning-like response" rather than "reasoning step-by-step".For 3), while LLMs display some human-like reasoning patterns, this does not necessarily mean that they behave like humans.Additionally, there are several observations that suggest LLMs may not be capable of reasoning: 1) LLMs still struggle with tasks that require complex reasoning (Valmeekam et al., 2022;Han et al., 2022a;Ruis et al., 2022).If LLMs are really decent reasoners, they should handle tasks that can be simply solved by humans through reasoning; 2) LLMs make mistakes in their reasoning, as explained above; 3) #4 The performance of LLMs on downstream tasks has been found to be sensitive to the frequency of certain terms, such as numbers, in the training data (Razeghi et al., 2022;Jung et al., 2022), which would not be expected if the models were solving mathematical problems through reasoning; 4) # Language models have been found to struggle with associating relevant information that they have memorized (Huang et al., 2022c).</p>
<p>Overall, it is still too early to draw a conclusion about the proposed question.In fact, there is also an ongoing debate about whether language models can actually understand language or capture meaning (Bender and Koller, 2020;Li et al., 2021;Manning, 2022;Piantasodi and Hill, 2022).Further in-depth analysis of factors such as training data, model architecture, and optimization objectives is needed, as well as the development of better benchmarks for measuring the reasoning capabilities of LLMs.However, it is clear that the current models are not yet capable of robust reasoning.</p>
<p>Improving reasoning capabilities of LLMs.</p>
<p>While techniques like chain-of-thought prompting (Wei et al., 2022b) may help to elicit reasoning abilities in large language models, they cannot enable the models to solve tasks beyond their current capabilities.To truly enhance reasoning in LLMs, we need to utilize training data, model architecture, and optimization objectives that are designed to encourage reasoning.For example, finetuning a model with a dataset including CoT data has been shown to improve reasoning (Chung et al., 2022), and models can also self-improve through the process of bootstrapping their reasoning (Zelikman et al., 2022;Huang et al., 2022a).There is still much research that needs to be done in this area, and we look forward to future progress in improving reasoning in large language models.</p>
<p>Conclusion</p>
<p>In this paper, we have provided a detailed and upto-date review of the current state of knowledge on reasoning in large language models.We have discussed techniques for improving and eliciting reasoning in LLMs, methods and benchmarks for evaluating reasoning abilities, and the findings and implications of previous studies in this topic.While LLMs have made significant progress in natural language processing and related fields, it remains unclear to what extent they are capable of true reasoning or whether they are simply using memorized patterns and heuristics to solve problems.Further research is needed to fully understand the reasoning abilities of LLMs, improve LLMs' reasoning capabilities, and determine their potential for use in a variety of applications.We hope that this paper will serve as a useful overview of the current state of the field and stimulate further discussion and research on this interesting and important topic.</p>
<p>Limitations</p>
<p>In this paper, we provide an overview of the current state of knowledge on reasoning in large language models.Reasoning is a broad concept that encompasses various forms, making it impractical to summarize all related work in a single paper.Therefore, we focus on deductive reasoning, as it is the most commonly studied in the literature.Other forms of reasoning such as inductive reasoning (Yang et al., 2022;Misra et al., 2022, inter alia) and abductive reasoning (Wiegreffe et al., 2022;Lampinen et al., 2022;Jung et al., 2022, inter alia) may not be discussed in depth.</p>
<p>Additionally, given the rapid evolution and significance of reasoning within large language models, it is crucial to note that new contributions may have emerged in the field concurrent with the writing of this paper.An additional resource to consider is a parallel survey by Qiao et al. (2022), which emphasizes reasoning via language model prompting.Our coverage may not extend to papers released during or after 2023 such as evaluation on Chat-GPT (Bang et al., 2023;Zheng et al., 2023).As such, we recommend readers to check the papers that cite this survey for a more comprehensive and updated understanding of this field.</p>
<p>C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?Not applicable.Left blank.</p>
<p>C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?Not applicable.Left blank.</p>
<p>C4.If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?Not applicable.Left blank.</p>
<p>D Did you use human annotators (e.g., crowdworkers) or research with human participants?Left blank.</p>
<p>D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?Not applicable.Left blank.</p>
<p>D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?Not applicable.Left blank.</p>
<p>D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating?For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?Not applicable.Left blank.</p>
<p>D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?Not applicable.Left blank.</p>
<p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?Not applicable.Left blank.</p>
<p>Figure 1 :
1
Figure 1: The structure of the paper.</p>
<p>Figure 2 :
2
Figure 2: An illustration of Chain-of-Thought Prompting and Rationale Engineering, where asterisk (*) denotes the target problem to be solved.</p>
<p>Paperlist can be found at https://github.com/ jeffhj/LM-reasoning.
It is important to note that the term "reasoning" in this paper does not necessarily imply that LLMs are truly capable of reasoning or that they are able to reason in the same way that humans do. We will discuss this issue in more detail in §6.
This may also be true for models trained with code(Chen et al., 2021;Fu et al., 2022a).</p>
<h1>indicates the finding has not been carefully examined in language models with more than 100 billion parameters.</h1>
<p>AcknowledgementsWe would like to thank Jason Wei (OpenAI)  and Denny Zhou (Google DeepMind) for their valuable advice and constructive feedback on this work.This material is based upon work supported by the National Science Foundation IIS 16-19302 and IIS 16-33755, Zhejiang University ZJU Research 083650, IBM-Illinois Center for Cognitive Computing Systems Research (C3SR) and IBM-Illinois Discovery Accelerator Institute (IIDAI), gift grants from eBay and Microsoft Azure, UIUC OVCR CCIL Planning Grant 434S34, UIUC CSBS Small Grant 434C8U, and UIUC New Frontiers Initiative.Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the funding agencies.We wrote part of the appendix with ChatGPT assistance (e.g., to generate an initial description for commonsense reasoning).The generated text is carefully revised and examined by the authors.BDid you use or create scientific artifacts?Not applicable.Left blank.B1. Did you cite the creators of artifacts you used?Not applicable.Left blank.B2. Did you discuss the license or terms for use and / or distribution of any artifacts?Not applicable.Left blank.B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified?For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?Not applicable.Left blank.B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?Not applicable.Left blank.B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?Not applicable.Left blank.B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created?Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results.For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.Not applicable.Left blank.C Did you run computational experiments?Left blank.C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?No response.The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.
MathQA: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, 10.18653/v1/N19-1245Proceedings of the 2019 Conference of the North American Chapter. the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American ChapterMinneapolis, MinnesotaAssociation for Computational Linguistics20191Long and Short Papers</p>
<p>Exploring length generalization in large language models. Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, Behnam Neyshabur, ArXiv preprint, abs/2207.049012022</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, ArXiv preprint, abs/2302.040232023</p>
<p>Climbing towards NLU: On meaning, form, and understanding in the age of data. Emily M Bender, Alexander Koller, 10.18653/v1/2020.acl-main.463Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Commonsense knowledge reasoning and generation with pretrained language models: A survey. Prajjwal Bhargava, Vincent Ng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, abs/2108.07258ArXiv. 2021</p>
<p>Logical reasoning in formal and everyday reasoning tasks. Hugo Bronkhorst, Gerrit Roorda, International Journal of Science and Mathematics Education. 1882020Cor Suhre, and Martin Goedhart</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>Evaluating large language models trained on code. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, preprint, abs/2107.033742021</p>
<p>Large language models are few (1)-shot table reasoners. Wenhu Chen, abs/2210.067102022ArXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, abs/2211.125882022ArXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, ArXiv preprint, abs/2204.023112022</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, ArXiv preprint, abs/2210.114162022</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, ArXiv preprint, abs/1803.054572018</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, abs/2110.14168ArXiv preprint. 2021</p>
<p>Faithful reasoning using large language models. Antonia Creswell, Murray Shanahan, abs/2208.142712022ArXiv preprint</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, abs/2205.097122022ArXiv preprint</p>
<p>Language models show human-like content effects on reasoning. Ishita Dasgupta, Stephanie Cy Andrew K Lampinen, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, preprint, abs/2207.070512022</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>. David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Jascha Sohl-Dickstein. et al. 2022. Language model cascades. ArXiv preprint, abs/2207.10342</p>
<p>Compositional semantic parsing with large language models. Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou, abs/2209.15003ArXiv preprint. 2022</p>
<p>Successive prompting for decomposing complex questions. Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner, abs/2212.040922022ArXiv preprint</p>
<p>The theory of decision making. Ward Edwards, Psychological bulletin. 5143801954</p>
<p>Ronald Fagin, Yoram Joseph Y Halpern, Moshe Moses, Vardi, Reasoning about knowledge. MIT press2004</p>
<p>How does gpt obtain its ability? tracing emergent abilities of language models to their sources. Yao Fu, Hao Peng, Tushar Khot, 2022a</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, abs/2210.00720ArXiv preprint. 2022b</p>
<p>Approaches to studying formal and everyday reasoning. Kathleen M Galotti, Psychological bulletin. 10533311989</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, abs/2211.10435ArXiv preprint. 2022</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 10.1162/tacl_a_00370Transactions of the Association for Computational Linguistics. 92021</p>
<p>Roscoe: A suite of metrics for scoring step-by-step reasoning. Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, abs/2212.07919ArXiv preprint. 2022</p>
<p>Folio: Natural language reasoning with firstorder logic. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, abs/2209.00840ArXiv preprint. 2022a</p>
<p>Human-like property induction is a challenge for large language models. Simon Jerome, Han , Keith Ransom, Andrew Perfors, Charles Kemp, 2022b</p>
<p>Rethinking with retrieval: Faithful large language model inference. Hangfeng He, Hongming Zhang, Dan Roth, abs/2301.003032023ArXiv preprint</p>
<p>Reasoning with transformer-based models: Deep learning, but shallow reasoning. Chadi Helwe, Chloé Clavel, Fabian M Suchanek, 3rd Conference on Automated Knowledge Base Construction. 2021</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. the Neural Information Processing Systems Track on Datasets and Benchmarks20211</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, abs/2210.11610ArXiv preprint. 2022a</p>
<p>Open relation modeling: Learning to define relations between entities. Jie Huang, Kevin Chang, Jinjun Xiong, Wen-Mei Hwu, 10.18653/v1/2022.findings-acl.26Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022b</p>
<p>Are large pre-trained language models leaking your personal information?. Jie Huang, Hanyin Shao, Kevin Chen, -Chuan Chang, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022c</p>
<p>DEER: Descriptive knowledge graph for explaining entity relationships. Jie Huang, Kerui Zhu, Kevin Chen-Chuan, Jinjun Chang, Wen-Mei Xiong, Hwu, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022d</p>
<p>Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine LearningPMLR2022e162</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, 2022 Conference on Robot Learning. 2022f</p>
<p>Michael Huth, Mark Ryan, Logic in Computer Science: Modelling and reasoning about systems. Cambridge university press2004</p>
<p>Maieutic prompting: Logically consistent reasoning with recursive explanations. Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi, The 2022 Conference on Empirical Methods for Natural Language Processing. 2022</p>
<p>Lambada: Backward chaining for automated reasoning in natural language. Najoung Seyed Mehran Kazemi, Deepti Kim, Xin Bhatia, Deepak Xu, Ramachandran, abs/2212.13894ArXiv preprint. 2022</p>
<p>Measuring compositional generalization: A comprehensive method on realistic data. Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc Van Zee, Olivier Bousquet, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, abs/2210.02406ArXiv preprint. 2022</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. 2022</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. M Brenden, Marco Lake, Baroni, Proceedings of the 35th International Conference on Machine Learning, ICML 2018. the 35th International Conference on Machine Learning, ICML 2018Stockholmsmässan, Stockholm, SwedenPMLR2018. July 10-15, 201880of Proceedings of Machine Learning Research</p>
<p>Can language models learn from explanations in context?. Ishita Andrew K Lampinen, Dasgupta, C Y Stephanie, Kory Chan, Michael Henry Matthewson, Antonia Tessler, James L Creswell, Jane X Mcclelland, Felix Wang, Hill, 2022In Findings of the Association for Computational Linguistics: EMNLP 2022</p>
<p>An introduction to legal reasoning. Levi Edward, 2013University of Chicago Press</p>
<p>Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag. ArXiv preprint</p>
<p>Implicit representations of meaning in neural language models. Belinda Z Li, Maxwell Nye, Jacob Andreas, 10.18653/v1/2021.acl-long.143Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Explanations from large language models make small reasoners better. Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, ArXiv preprint, abs/2210.067262022a</p>
<p>On the advance of making language models better reasoners. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, abs/2206.02336ArXiv preprint. 2022b</p>
<p>CommonGen: A constrained text generation challenge for generative commonsense reasoning. Wangchunshu Bill Yuchen Lin, Ming Zhou, Pei Shen, Chandra Zhou, Yejin Bhagavatula, Xiang Choi, Ren, 10.18653/v1/2020.findings-emnlp.165Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Dimongen: Diversified generative commonsense reasoning for explaining concept relationships. Chenzhengyi Liu, Jie Huang, Kerui Zhu, Kevin Chen, -Chuan Chang, abs/2212.105452022aArXiv preprint</p>
<p>What makes good in-context examples for GPT-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.10The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Dublin, Ireland and OnlineAssociation for Computational Linguistics2022b. DeeLIO 2022Proceedings of Deep Learning Inside Out</p>
<p>Roberta: A robustly optimized bert pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, ArXiv preprint. 2019. 1907.11692</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Advances in Neural Information Processing Systems. 2022</p>
<p>Language models of code are few-shot commonsense learners. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)2022</p>
<p>Teaching small language models to reason. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn, abs/2212.08410ArXiv preprint. 2022</p>
<p>Human language understanding &amp; reasoning. D Christopher, Manning, Daedalus. 15122022</p>
<p>The next decade in ai: four steps towards robust artificial intelligence. Gary Marcus, abs/2002.061772020ArXiv preprint</p>
<p>What is reasoning? Mind. Conor Mchugh, Jonathan Way, 2018127</p>
<p>A diverse corpus for evaluating and developing English math word problem solvers. Chao-Chun Shen-Yun Miao, Keh-Yih Liang, Su, 10.18653/v1/2020.acl-main.92Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Multi-hop reading comprehension through question decomposition and rescoring. Sewon Min, Victor Zhong, Luke Zettlemoyer, Hannaneh Hajishirzi, 10.18653/v1/P19-1613Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>A property induction framework for neural language models. Kanishka Misra, Julia Taylor Rayz, Allyson Ettinger, abs/2205.069102022ArXiv preprint</p>
<p>Abstraction and analogymaking in artificial intelligence. Melanie Mitchell, Annals of the New York Academy of Sciences. 150512021</p>
<p>Caiming Xiong, Dragomir Radev, and Dragomir Radev. Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryściński, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, 10.1162/tacl_a_00446Transactions of the Association for Computational Linguistics. 102022FeTaQA: Free-form table question answering</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena, Deep Learning for Code Workshop. 2022</p>
<p>Chatgpt: Optimizing language models for dialogue. 2022OpenAI</p>
<p>Philosophical reasoning. Panupong Pasupat and Percy Liang. John Arthur, Passmore , 10.3115/v1/P15-1142Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational Linguistics1961. 20151Compositional semantic parsing on semi-structured tables</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Unsupervised question decomposition for question answering. Ethan Perez, Patrick Lewis, Wen-Tau Yih, Kyunghyun Cho, Douwe Kiela, 10.18653/v1/2020.emnlp-main.713Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Reasoning like program executors. Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, Weizhu Chen, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)2022</p>
<p>Meaning without reference in large language models. T Steven, Felix Piantasodi, Hill, abs/2208.029572022ArXiv preprint</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, abs/2210.03350ArXiv preprint. 2022</p>
<p>Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models. Ben Prystawski, Paul Thibodeau, Noah Goodman, abs/2209.08141ArXiv preprint. 2022</p>
<p>Reasoning with language model prompting: A survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, abs/2212.09597ArXiv preprint. 2022</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, ArXiv preprint, abs/2112.114462021</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 211402020</p>
<p>Explain yourself! leveraging language models for commonsense reasoning. Nazneen Fatema Rajani, Bryan Mccann, Caiming Xiong, Richard Socher, 10.18653/v1/P19-1487Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Impact of pretraining term frequencies on few-shot reasoning. Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, abs/2202.07206ArXiv preprint. 2022</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, 10.18653/v1/D15-1202Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Large language models are not zero-shot communicators. Laura Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim Rocktäschel, Edward Grefenstette, abs/2210.14986ArXiv preprint. 2022</p>
<p>Deep learning needs a prefrontal cortex. Jacob Russin, C O' Randall, Yoshua Reilly, Bengio, Work Bridging AI Cogn Sci. 1072020</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, abs/2210.01240ArXiv preprint. 2022</p>
<p>Bloom: A 176b-parameter open-access multilingual language model. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, ArXiv preprint, abs/2211.051002022</p>
<p>Language models are multilingual chain-of-thought reasoners. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, abs/2210.03057ArXiv preprint. 2022</p>
<p>Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions. Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan, abs/2212.001932022ArXiv preprint</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, ArXiv preprint, abs/2212.040882022</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, 2022ArXiv preprint, abs/2206.04615</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, ArXiv preprint, abs/2210.092612022</p>
<p>The web as a knowledge-base for answering complex questions. Alon Talmor, Jonathan Berant, 10.18653/v1/N18-1059Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLouisianaNew Orleans20181Association for Computational Linguistics</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. NeurIPS2020. 2020. 2020. December 6-12, 2020</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, preprint, abs/2211.09085Galactica: A large language model for science. 2022</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022</p>
<p>Iteratively prompt pre-trained language models for chain of thought. Boshi Wang, Xiang Deng, Huan Sun, The 2022 Conference on Empirical Methods for Natural Language Processing. 2022a</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, abs/2212.10001ArXiv preprint. 2022b</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, abs/2203.11171ArXiv preprint. 2022c</p>
<p>Reasoning about a rule. Peter C Wason, Quarterly journal of experimental psychology. 2031968</p>
<p>Psychology of reasoning: Structure and content. Peter Cathcart Wason and Philip Nicholas Johnson-Laird1972Harvard University Press86</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Transactions on Machine Learning Research. 2022a</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b</p>
<p>Large language models are reasoners with self-verification. Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, Jun Zhao, abs/2212.095612022ArXiv preprint</p>
<p>Reframing human-AI collaboration for generating free-text explanations. Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, Yejin Choi, 10.18653/v1/2022.naacl-main.47Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, abs/2212.10923ArXiv preprint. 2022</p>
<p>The unreliability of explanations in few-shot prompting for textual reasoning. Xi Ye, Greg Durrett, Advances in neural information processing systems. 2022</p>
<p>Alert: Adapting language models to reasoning tasks. Ping Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab, Asli Celikyilmaz, abs/2212.08286ArXiv preprint. 2022</p>
<p>STar: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 2022</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, ArXiv preprint, abs/2205.010682022a</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, abs/2210.034932022bArXiv preprint</p>
<p>Why does chatgpt fall short in providing truthful answers?. Shen Zheng, Jie Huang, Kevin Chen, -Chuan Chang, ArXiv preprint, abs/2304.105132023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, Ed Chi, ArXiv preprint, abs/2205.106252022a</p>
<p>Reflection of thought: Inversely eliciting numerical reasoning in language models via solving linear systems. Fan Zhou, Haoyu Dong, Qian Liu, Zhoujun Cheng, Shi Han, Dongmei Zhang, abs/2210.05075ArXiv preprint. 2022b</p>
<p>Teaching algorithmic reasoning via incontext learning. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, Hanie Sedghi, 2022cArXiv preprint, abs/2211.09066</p>
<p>The development of scientific reasoning skills. Corinne Zimmerman, Developmental review. 2012000</p>            </div>
        </div>

    </div>
</body>
</html>
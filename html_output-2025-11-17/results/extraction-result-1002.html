<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1002 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1002</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1002</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-24c3e71af21109b23ed9c8d1af13a5921c4f8020</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/24c3e71af21109b23ed9c8d1af13a5921c4f8020" target="_blank">Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data</a></p>
                <p><strong>Paper Venue:</strong> CLEaR</p>
                <p><strong>Paper TL;DR:</strong> This work proposes Amortized Causal Discovery, a novel framework that leverages shared dynamics to learn to infer causal relations from time-series data, and enables a single, amortized model to be trained that infers causal relations across samples with different underlying causal graphs.</p>
                <p><strong>Paper Abstract:</strong> Standard causal discovery methods must fit a new model whenever they encounter samples from a new underlying causal graph. However, these samples often share relevant information - for instance, the dynamics describing the effects of causal relations - which is lost when following this approach. We propose Amortized Causal Discovery, a novel framework that leverages such shared dynamics to learn to infer causal relations from time-series data. This enables us to train a single, amortized model that infers causal relations across samples with different underlying causal graphs, and thus makes use of the information that is shared. We demonstrate experimentally that this approach, implemented as a variational model, leads to significant improvements in causal discovery performance, and show how it can be extended to perform well under hidden confounding.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1002.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1002.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Amortized Causal Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that learns a shared dynamics model across multiple time-series samples while amortizing per-sample causal-graph inference via an encoder; separates graph prediction (encoder) from dynamics modeling (decoder) so it can generalize across samples with different underlying causal graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Amortized Causal Discovery (ACD)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>ACD trains an amortized encoder f_phi that maps a multivariate time-series sample x_s to a distribution over summary graphs (edges), and a dynamics decoder f_theta that predicts next time-steps conditional on the encoded graph; training minimizes a reconstruction (negative log-likelihood) term plus a graph regularizer (KL to a prior) across many samples, implemented variationally with a Gumbel-Softmax relaxation for discrete edges. Inference can be done by (a) directly using the encoder's predicted graph or (b) Test-Time Adaptation (TTA): optimizing a graph to minimize the learned decoder loss on the test sample. The decoder uses a zero-function edge-type to implement 'no-edge' so that predicted zero-edges imply no Granger-causality in the model.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated time-series environments (Particles, Kuramoto, Netsim simulated fMRI)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Batch of simulated datasets rather than an interactive/closed-loop lab: (1) Kuramoto phase-coupled oscillators (5 1-D series), (2) Particles in 2D with spring couplings (5 agents with vector states), (3) Netsim simulated fMRI (15 regions). These are simulator-generated datasets (not active/interactive experiments), but ACD is designed to leverage multiple related samples with varying graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Models hidden confounders via augmenting the amortized encoder to predict latent variables (proxies) and uses structural biases to feed these latents into the decoder; uses KL regularization on latent posteriors to impose priors; uses zero-edge masking in the decoder to remove spurious edges; uses TTA to re-optimize graph to better explain observed dynamics and thereby remove spurious edges.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Hidden confounding (global latent variables like temperature; unobserved time-series behaving like observed variables), measurement/observation noise, irrelevant/absent edges.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Encoder predicts a posterior over graph edges and additional latent variables; the latent posterior is encouraged (via KL to a prior) to capture confounders — detection is implicit via the encoder's latent posterior and by monitoring decoder prediction fit (poor fit suggests model mismatch). For MPIR-style baselines (compared methods) detection occurs by measuring predictive performance changes when inputs are perturbed (see MPIR).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Decoder uses a learned zero-edge-type function (explicit masking) so edges predicted as 'no-edge' propagate zero messages; KL regularizer pushes posterior graphs/latents toward prior (sparsity or uniform), reducing spurious edges or latent overfitting; TTA optimizes graphs to maximize predictive likelihood, effectively downweighting graphs that explain data poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Test-Time Adaptation (optimize graph to minimize decoder loss on test sample) is used to search for graphs that better explain the dynamics, thereby refuting encoder-predicted spurious edges; theoretical argument (Appendix A) shows that a zero-edge implies no Granger-causality in the learned model, providing a refutation criterion for edges.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>High AUROC on simulated tasks when using the full ACD setup: Kuramoto AUROC 0.952 ± 0.003, Particles AUROC ≈ 0.999, Netsim AUROC 0.688 ± 0.051; with latent-variable extensions to handle hidden confounding ACD outperforms baselines (figures shown); temperature-prediction R^2=0.888, AUROC=0.966, accuracy=0.644 for α=2 for the latent-temperature variant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline (non-amortized or models ignoring latents) perform substantially worse: e.g., classical baselines on Kuramoto range from AUROC 0.502 (MPIR) to 0.647 (linear Granger), and ACD without latent modeling or without TTA degrades (plots show encoder overfitting in low-data regimes). For hidden confounder experiments, baselines that impute mean or ignore latent (Mean/None) perform worse than ACD+Latent (figures reported).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ACD can leverage shared dynamics across samples to dramatically improve Granger-causal discovery relative to per-sample refitting baselines; explicit latent-variable extensions let the amortized encoder model global (temperature) or dynamic (unobserved time-series) confounders and recover substantially better edge AUROC than baselines that ignore or impute the missing signal; TTA reduces encoder amortization gap and overfitting, improving performance in low-data regimes; zero-edge masking yields a rigorous connection to Granger non-causation for the learned model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1002.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1002.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACD-VI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Probabilistic Implementation of ACD</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specific probabilistic instantiation of ACD using a variational encoder q_phi(z|x) (GNN) and decoder p_theta(x|z) trained with a variational lower bound and Gumbel-Softmax relaxation for discrete edges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Variational ACD (Gumbel-Softmax encoder + probabilistic decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The encoder is a Graph Neural Network that outputs logits ψ_ij, which (after Gumbel noise and temperature τ) produce relaxed categorical samples z_ij via the Concrete / Gumbel-Softmax; the decoder aggregates edge-specific neural messages h_ij^t = Σ_e>0 z_{ij,e} f_e([x_i^t, x_j^t]) and predicts Δx via f_v, with Gaussian output. Loss is ELBO: E_q[log p_theta(x|z)] - KL[q_phi(z|x) || p(z)]. Regularizer r enforces priors (e.g., sparsity) on graphs. During test-time the relaxed distribution is replaced by categorical samples.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same simulated datasets (Particles, Kuramoto, Netsim)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline simulated time-series datasets; decoder trained to predict multiple steps ahead (teacher-forcing or autoregressive prediction used), not interactive.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Augmented encoder outputs additional latent variables (continuous or discrete) intended to model confounders; KL term on latent posterior guides those latents toward a chosen prior (e.g., uniform on a range for temperature), which imposes structural bias and regularizes the latent to capture global variations rather than overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Global unobserved variables (temperature-like), unobserved dynamic time-series, observation noise.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>The variational posterior over extra latent variables is trained to explain residual variations in decoder prediction; success is judged by improved decoder likelihood and downstream AUROC — implicit detection rather than an explicit statistical test.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>KL regularization on q_phi for latent posteriors and on graph posteriors to a chosen prior reduces the influence of over-flexible latents/edges; the zero-edge type in the decoder effectively zeroes messages from non-edges.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Optimizing ELBO and optionally performing TTA (optimize graph posterior or discrete graph to improve decoder likelihood) serves to reject graph/latent configurations that induce poor predictive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When the variational latent is used to model temperature, ACD+Latent outperforms baselines (Mean and None) across temperature regimes (figure 5). Latent-temperature prediction stats: R^2=0.888, AUROC=0.966, accuracy=0.644 for α=2 (demonstrates the encoder can recover meaningful latent proxies).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Without the latent extension the decoder and encoder conflate graph/dynamics and perform worse under heterogenous-sample confounding; exact numeric drop is shown in figures (ACD-None and ACD-Mean lines perform worse than ACD+Latent).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The variational implementation enables explicit latent-variable modeling (proxy learning) for confounders; KL priors and architectural structural bias (how the predicted latent is used by decoder) are essential to coax the encoder to represent meaningful confounders that improve causal-edge recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1002.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1002.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACD-LatentTemp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ACD with Latent Temperature (global confounder proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ACD variant that adds an encoder-predicted latent representing a global temperature per sample which modulates dynamics strength; trained with a KL penalty to a uniform prior and a decoder that conditions on predicted temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Latent-Temperature Confounder Modeling (ACD)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For each sample x_s the amortized encoder additionally outputs a posterior q(c | x_s) over a scalar temperature c (modeled as a uniform-like posterior); a KL term pushes q towards a uniform prior on [0,4α]. The decoder is provided this temperature and is designed (inductive bias) to match the true dynamics when conditioned on c and the graph. This lets the model attribute global amplitude/strength variations to the latent rather than spurious edges.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Particles simulator with temperature variation per sample</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline simulation where each sample's dynamics are generated with a scalar temperature drawn from a categorical distribution (three possible magnitudes scaled by α); the environment is non-interactive and the confounder is global per sample and constant over time.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Latent proxy prediction plus KL regularization and decoder structural bias (explicit conditioning on the predicted temperature) — effectively a proxy-variable approach to capture the global confounder and prevent it from inducing spurious pairwise edges.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Global confounding variable that scales causal interaction strengths (systematic confounder across all observed variables).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Encoder attempts to explain systematic variation in observed dynamics via its posterior q(c|x); success is evaluated by decoder likelihood and downstream edge AUROC; no separate hypothesis test is used.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>By explaining global variations through the learned c, the model avoids attributing those variations to edges (i.e., edges get lower posterior mass), effectively downweighting spurious edge explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Comparative evaluation vs baselines (None: no explicit c; Mean: decoder with mean temperature) demonstrates that graphs that attribute dynamics to edges rather than c yield worse decoder fit and are disfavored by ELBO; TTA can further refine the graph to reject spurious edges.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>ACD+Latent outperforms baselines across temperature regimes (figure 5); for α=2 the learned latent predicts temperature with R^2=0.888 and AUROC=0.966 and accuracy=0.644 — indicating good recovery of the confounder and improved causal-edge AUROC relative to Mean/None baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline variants that either fix temperature to mean (Mean) or do not model it (None) yield substantially worse causal-AUROC across tested α values (plots in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>1 global latent (temperature) per sample</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly modeling a global latent confounder that modulates interaction strengths lets the encoder+decoder separate systematic sample-level effects from pairwise causal effects and substantially improves edge recovery compared to ignoring or naively imputing the confounder.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1002.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1002.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACD-LatentTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ACD with Latent Unobserved Time-series</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ACD extension that augments the encoder with an additional latent trajectory z_u^t representing an unobserved dynamic time-series; the encoder uses its trajectory for graph inference and the decoder conditions on its current value when predicting dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Latent Unobserved Time-Series Modeling (ACD)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>One original time-series is treated as unobserved; the amortized encoder outputs a latent trajectory z_u^{1:T} representing that missing series. That latent's whole trajectory is used in encoder computations for graph posterior prediction, and its current timestep value is fed to the decoder as if it were an observed variable. The latent is learned jointly with graph posterior q_phi and decoder p_theta to explain dynamics under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Particles simulator with one removed/unobserved time-series</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline simulator where one of the true time-series trajectories is removed from observations and acts like a hidden agent that still influences observed agents through the same dynamics; experiments vary how many observed series are directly influenced by the missing series.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Latent trajectory imputation (proxy) learned by the encoder; architectural structural bias ensures the latent is used equivalently to observed series and is therefore able to capture direct influences of the missing signal.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Unobserved dynamic confounders (missing variables that interact with observed variables and can induce spurious pairwise associations).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>The encoder attempts to capture unexplained cross-series dependencies by allocating capacity to the latent trajectory; improved decoder likelihood and preserved AUROC relative to baselines are used to verify detection.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>By explaining interactions via the latent trajectory rather than spurious observed-to-observed edges, the model reduces posterior mass on incorrect edges (implicit downweighting rather than an explicit weight schedule).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Comparative performance: baselines that ignore (None) or impute mean (Mean) show severe AUROC degradation as more observed series are influenced by the hidden one, while ACD+Latent stays close to the fully observed upper bound; this empirical refutation demonstrates that attributing effects to the latent rejects spurious direct edges.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>As the number of observed series influenced by the unobserved one increases, ACD+Latent maintains much higher AUROC and better future-trajectory prediction than None/Mean baselines (figures 6 and 7); quantitative AUROC values are shown in plots (ACD+Latent tracks Observed baseline closely).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baselines ignoring the missing series (None) or imputing its value (Mean) deteriorate strongly as more observed series are influenced by the hidden series (plots show steep AUROC drop).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>1 unobserved time-series (experiments vary how many observed series it influences)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning a latent trajectory proxy for a missing dynamic variable substantially mitigates the spurious edge problem that arises from unobserved interacting variables; the architectural choice to treat the latent like an observed series is crucial for effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1002.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1002.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TTA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-Time Adaptation (graph optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A test-time optimization procedure that, given a pre-trained decoder f_theta, searches over graph candidates to find the graph that minimizes the decoder loss on the specific test sample, thereby improving per-sample graph estimates and reducing encoder amortization error.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Test-Time Adaptation (TTA) for graph refinement</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Instantiate a non-amortized variational distribution q(z) over graphs for a test sample and optimize q (or discrete graph variables) to minimize the decoder loss L(x_test, z, f_theta*). Initialization can be random or seeded from the encoder's prediction; run gradient-based optimization on the (learned) decoder loss to find a graph that best explains the observed sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same simulated datasets (Kuramoto, Particles, Netsim)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline per-sample optimization; not an interactive experiment-selection strategy but a test-time inference refinement technique.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>By optimizing graph structure to maximize predictive fit, TTA effectively downweights or removes edges that would otherwise be spuriously predicted by the amortized encoder; it can also close the amortization gap that causes encoder mispredictions.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Encoder overfitting-induced spurious edges, non-generalizable graph predictions, sample-specific idiosyncrasies that lead to wrong edge assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Edges that produce poor predictive likelihood under the decoder are discovered (i.e., detection is by loss-based evidence that an edge is not needed to explain data).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Optimization reduces posterior mass on edges that hinder predictive performance, effectively downweighting spurious edges in the final graph.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Edges predicted by the encoder but that do not improve decoder likelihood are removed by the optimization; empirical results show TTA improves AUROC in low-data regimes and reduces encoder overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>TTA improves performance in low-data regimes — e.g., on Kuramoto, with <50 training samples TTA allows ACD to outperform prior approaches; in experiments TTA (and Enc+TTA) outperform encoder-only in low-data settings (fig. 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Encoder-only (no TTA) performs better in the high-data regime, but suffers from an amortization gap and overfitting in low-data settings (training curves in Appendix B.2 show encoder overfit while decoder generalizes better).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TTA closes the amortization gap and reduces encoder overfitting by adapting graph predictions to individual test samples; combining encoder initialization with TTA (Enc+TTA) can yield the best results depending on dataset and training size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1002.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1002.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPIR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Minimum Predictive Information Regularization (MPIR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline causal-discovery method that detects causal links by measuring changes in predictive performance when noise is added to input variables; used in comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discovering nonlinear relations with minimum predictive information regularization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MPIR (predictive-information perturbation test)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Determines causal links by examining the change in predictive performance (on a decoder or predictor) when noise is added to a candidate input variable; large drop in predictive ability indicates the input is causally relevant for the output.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Applied as a baseline on Kuramoto and Netsim experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline evaluation on simulated time-series; not interactive; used as a comparator for ACD.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Perturbation-based detection: by corrupting individual inputs and measuring effect on predictive performance, MPIR can detect variables that are only spuriously correlated (which will not strongly affect predictive performance) versus those that are predictive (causal).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations that are not predictive when the input is randomized (i.e., correlation vs. causal signal), measurement noise.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Measure change in predictive performance when noise is injected into candidate input variable; significant change implies causal relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Not an explicit downweighting mechanism; edges scored by sensitivity to perturbation and thresholded to decide edges.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation is performed via empirical perturbation: if adding noise to a variable does not degrade prediction, it may be ruled out as causal.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Reported as baseline on Kuramoto: AUROC 0.502 ± 0.006 (Table 1), and on Netsim AUROC 0.484 ± 0.017 (Table 2) in the paper's experimental comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MPIR is relatively weak on the simulated datasets in this paper (low AUROC) compared to ACD; its perturbation-based detection is a direct method for identifying variables whose predictive effect is spurious or negligible but did not match the performance of amortized, dynamics-aware approaches here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1002.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1002.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Proxy-Latent Methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proxy-based latent-variable causal methods (e.g., Louizos et al. 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that use encoder-style models to learn latent proxies for unobserved confounders and thereby improve causal effect estimation under hidden confounding; referenced as inspiration for ACD's latent modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal effect inference with deep latent-variable models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Proxy-based deep latent-variable causal inference</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Encoder models map observed proxies to a latent representation meant to capture hidden confounders; the latent is then used in downstream causal estimation; identification and success require sufficient proxy richness and structural assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General observational datasets; referred as conceptual inspiration; not evaluated in this paper's simulations directly.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Typically non-interactive observational datasets (the Louizos et al. work focused on treatment-effect estimation tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Learn latent proxies from observed variables and condition downstream models on these latents; regularize via priors (e.g., KL) to avoid degenerate solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Hidden confounding and missing covariates that induce spurious associations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit via encoder's ability to explain residual dependence; theoretical identifiability results require sufficient proxy structure.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Conditioning on latents reduces spurious associations attributed to observed variables; regularization reduces overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Model selection and improved predictive fit when conditioning on latents is used as indirect evidence that spurious edges are being explained by the latent rather than true direct causation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Prior work (Louizos et al.) reported improved causal-effect estimation under hidden confounding when informative proxies are available; in this paper, analogous latent strategies improved graph AUROC versus baselines in simulated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Proxy-based latent modeling is a viable strategy to mitigate hidden confounding; ACD adopts this idea for temporal causal discovery by training an encoder to output latent proxies (temperature, missing time-series) and conditioning the decoder accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural relational inference for interacting systems <em>(Rating: 2)</em></li>
                <li>Causal effect inference with deep latent-variable models <em>(Rating: 2)</em></li>
                <li>Neural granger causality for nonlinear time series <em>(Rating: 2)</em></li>
                <li>Discovering nonlinear relations with minimum predictive information regularization <em>(Rating: 2)</em></li>
                <li>Neural relational inference with fast modular meta-learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1002",
    "paper_id": "paper-24c3e71af21109b23ed9c8d1af13a5921c4f8020",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "ACD",
            "name_full": "Amortized Causal Discovery",
            "brief_description": "A framework that learns a shared dynamics model across multiple time-series samples while amortizing per-sample causal-graph inference via an encoder; separates graph prediction (encoder) from dynamics modeling (decoder) so it can generalize across samples with different underlying causal graphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Amortized Causal Discovery (ACD)",
            "method_description": "ACD trains an amortized encoder f_phi that maps a multivariate time-series sample x_s to a distribution over summary graphs (edges), and a dynamics decoder f_theta that predicts next time-steps conditional on the encoded graph; training minimizes a reconstruction (negative log-likelihood) term plus a graph regularizer (KL to a prior) across many samples, implemented variationally with a Gumbel-Softmax relaxation for discrete edges. Inference can be done by (a) directly using the encoder's predicted graph or (b) Test-Time Adaptation (TTA): optimizing a graph to minimize the learned decoder loss on the test sample. The decoder uses a zero-function edge-type to implement 'no-edge' so that predicted zero-edges imply no Granger-causality in the model.",
            "environment_name": "Simulated time-series environments (Particles, Kuramoto, Netsim simulated fMRI)",
            "environment_description": "Batch of simulated datasets rather than an interactive/closed-loop lab: (1) Kuramoto phase-coupled oscillators (5 1-D series), (2) Particles in 2D with spring couplings (5 agents with vector states), (3) Netsim simulated fMRI (15 regions). These are simulator-generated datasets (not active/interactive experiments), but ACD is designed to leverage multiple related samples with varying graphs.",
            "handles_distractors": true,
            "distractor_handling_technique": "Models hidden confounders via augmenting the amortized encoder to predict latent variables (proxies) and uses structural biases to feed these latents into the decoder; uses KL regularization on latent posteriors to impose priors; uses zero-edge masking in the decoder to remove spurious edges; uses TTA to re-optimize graph to better explain observed dynamics and thereby remove spurious edges.",
            "spurious_signal_types": "Hidden confounding (global latent variables like temperature; unobserved time-series behaving like observed variables), measurement/observation noise, irrelevant/absent edges.",
            "detection_method": "Encoder predicts a posterior over graph edges and additional latent variables; the latent posterior is encouraged (via KL to a prior) to capture confounders — detection is implicit via the encoder's latent posterior and by monitoring decoder prediction fit (poor fit suggests model mismatch). For MPIR-style baselines (compared methods) detection occurs by measuring predictive performance changes when inputs are perturbed (see MPIR).",
            "downweighting_method": "Decoder uses a learned zero-edge-type function (explicit masking) so edges predicted as 'no-edge' propagate zero messages; KL regularizer pushes posterior graphs/latents toward prior (sparsity or uniform), reducing spurious edges or latent overfitting; TTA optimizes graphs to maximize predictive likelihood, effectively downweighting graphs that explain data poorly.",
            "refutation_method": "Test-Time Adaptation (optimize graph to minimize decoder loss on test sample) is used to search for graphs that better explain the dynamics, thereby refuting encoder-predicted spurious edges; theoretical argument (Appendix A) shows that a zero-edge implies no Granger-causality in the learned model, providing a refutation criterion for edges.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "High AUROC on simulated tasks when using the full ACD setup: Kuramoto AUROC 0.952 ± 0.003, Particles AUROC ≈ 0.999, Netsim AUROC 0.688 ± 0.051; with latent-variable extensions to handle hidden confounding ACD outperforms baselines (figures shown); temperature-prediction R^2=0.888, AUROC=0.966, accuracy=0.644 for α=2 for the latent-temperature variant.",
            "performance_without_robustness": "Baseline (non-amortized or models ignoring latents) perform substantially worse: e.g., classical baselines on Kuramoto range from AUROC 0.502 (MPIR) to 0.647 (linear Granger), and ACD without latent modeling or without TTA degrades (plots show encoder overfitting in low-data regimes). For hidden confounder experiments, baselines that impute mean or ignore latent (Mean/None) perform worse than ACD+Latent (figures reported).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "ACD can leverage shared dynamics across samples to dramatically improve Granger-causal discovery relative to per-sample refitting baselines; explicit latent-variable extensions let the amortized encoder model global (temperature) or dynamic (unobserved time-series) confounders and recover substantially better edge AUROC than baselines that ignore or impute the missing signal; TTA reduces encoder amortization gap and overfitting, improving performance in low-data regimes; zero-edge masking yields a rigorous connection to Granger non-causation for the learned model.",
            "uuid": "e1002.0",
            "source_info": {
                "paper_title": "Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "ACD-VI",
            "name_full": "Variational Probabilistic Implementation of ACD",
            "brief_description": "A specific probabilistic instantiation of ACD using a variational encoder q_phi(z|x) (GNN) and decoder p_theta(x|z) trained with a variational lower bound and Gumbel-Softmax relaxation for discrete edges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Variational ACD (Gumbel-Softmax encoder + probabilistic decoder)",
            "method_description": "The encoder is a Graph Neural Network that outputs logits ψ_ij, which (after Gumbel noise and temperature τ) produce relaxed categorical samples z_ij via the Concrete / Gumbel-Softmax; the decoder aggregates edge-specific neural messages h_ij^t = Σ_e&gt;0 z_{ij,e} f_e([x_i^t, x_j^t]) and predicts Δx via f_v, with Gaussian output. Loss is ELBO: E_q[log p_theta(x|z)] - KL[q_phi(z|x) || p(z)]. Regularizer r enforces priors (e.g., sparsity) on graphs. During test-time the relaxed distribution is replaced by categorical samples.",
            "environment_name": "Same simulated datasets (Particles, Kuramoto, Netsim)",
            "environment_description": "Offline simulated time-series datasets; decoder trained to predict multiple steps ahead (teacher-forcing or autoregressive prediction used), not interactive.",
            "handles_distractors": true,
            "distractor_handling_technique": "Augmented encoder outputs additional latent variables (continuous or discrete) intended to model confounders; KL term on latent posterior guides those latents toward a chosen prior (e.g., uniform on a range for temperature), which imposes structural bias and regularizes the latent to capture global variations rather than overfitting.",
            "spurious_signal_types": "Global unobserved variables (temperature-like), unobserved dynamic time-series, observation noise.",
            "detection_method": "The variational posterior over extra latent variables is trained to explain residual variations in decoder prediction; success is judged by improved decoder likelihood and downstream AUROC — implicit detection rather than an explicit statistical test.",
            "downweighting_method": "KL regularization on q_phi for latent posteriors and on graph posteriors to a chosen prior reduces the influence of over-flexible latents/edges; the zero-edge type in the decoder effectively zeroes messages from non-edges.",
            "refutation_method": "Optimizing ELBO and optionally performing TTA (optimize graph posterior or discrete graph to improve decoder likelihood) serves to reject graph/latent configurations that induce poor predictive performance.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "When the variational latent is used to model temperature, ACD+Latent outperforms baselines (Mean and None) across temperature regimes (figure 5). Latent-temperature prediction stats: R^2=0.888, AUROC=0.966, accuracy=0.644 for α=2 (demonstrates the encoder can recover meaningful latent proxies).",
            "performance_without_robustness": "Without the latent extension the decoder and encoder conflate graph/dynamics and perform worse under heterogenous-sample confounding; exact numeric drop is shown in figures (ACD-None and ACD-Mean lines perform worse than ACD+Latent).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "The variational implementation enables explicit latent-variable modeling (proxy learning) for confounders; KL priors and architectural structural bias (how the predicted latent is used by decoder) are essential to coax the encoder to represent meaningful confounders that improve causal-edge recovery.",
            "uuid": "e1002.1",
            "source_info": {
                "paper_title": "Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "ACD-LatentTemp",
            "name_full": "ACD with Latent Temperature (global confounder proxy)",
            "brief_description": "An ACD variant that adds an encoder-predicted latent representing a global temperature per sample which modulates dynamics strength; trained with a KL penalty to a uniform prior and a decoder that conditions on predicted temperature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Latent-Temperature Confounder Modeling (ACD)",
            "method_description": "For each sample x_s the amortized encoder additionally outputs a posterior q(c | x_s) over a scalar temperature c (modeled as a uniform-like posterior); a KL term pushes q towards a uniform prior on [0,4α]. The decoder is provided this temperature and is designed (inductive bias) to match the true dynamics when conditioned on c and the graph. This lets the model attribute global amplitude/strength variations to the latent rather than spurious edges.",
            "environment_name": "Particles simulator with temperature variation per sample",
            "environment_description": "Offline simulation where each sample's dynamics are generated with a scalar temperature drawn from a categorical distribution (three possible magnitudes scaled by α); the environment is non-interactive and the confounder is global per sample and constant over time.",
            "handles_distractors": true,
            "distractor_handling_technique": "Latent proxy prediction plus KL regularization and decoder structural bias (explicit conditioning on the predicted temperature) — effectively a proxy-variable approach to capture the global confounder and prevent it from inducing spurious pairwise edges.",
            "spurious_signal_types": "Global confounding variable that scales causal interaction strengths (systematic confounder across all observed variables).",
            "detection_method": "Encoder attempts to explain systematic variation in observed dynamics via its posterior q(c|x); success is evaluated by decoder likelihood and downstream edge AUROC; no separate hypothesis test is used.",
            "downweighting_method": "By explaining global variations through the learned c, the model avoids attributing those variations to edges (i.e., edges get lower posterior mass), effectively downweighting spurious edge explanations.",
            "refutation_method": "Comparative evaluation vs baselines (None: no explicit c; Mean: decoder with mean temperature) demonstrates that graphs that attribute dynamics to edges rather than c yield worse decoder fit and are disfavored by ELBO; TTA can further refine the graph to reject spurious edges.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "ACD+Latent outperforms baselines across temperature regimes (figure 5); for α=2 the learned latent predicts temperature with R^2=0.888 and AUROC=0.966 and accuracy=0.644 — indicating good recovery of the confounder and improved causal-edge AUROC relative to Mean/None baselines.",
            "performance_without_robustness": "Baseline variants that either fix temperature to mean (Mean) or do not model it (None) yield substantially worse causal-AUROC across tested α values (plots in paper).",
            "has_ablation_study": true,
            "number_of_distractors": "1 global latent (temperature) per sample",
            "key_findings": "Explicitly modeling a global latent confounder that modulates interaction strengths lets the encoder+decoder separate systematic sample-level effects from pairwise causal effects and substantially improves edge recovery compared to ignoring or naively imputing the confounder.",
            "uuid": "e1002.2",
            "source_info": {
                "paper_title": "Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "ACD-LatentTS",
            "name_full": "ACD with Latent Unobserved Time-series",
            "brief_description": "An ACD extension that augments the encoder with an additional latent trajectory z_u^t representing an unobserved dynamic time-series; the encoder uses its trajectory for graph inference and the decoder conditions on its current value when predicting dynamics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Latent Unobserved Time-Series Modeling (ACD)",
            "method_description": "One original time-series is treated as unobserved; the amortized encoder outputs a latent trajectory z_u^{1:T} representing that missing series. That latent's whole trajectory is used in encoder computations for graph posterior prediction, and its current timestep value is fed to the decoder as if it were an observed variable. The latent is learned jointly with graph posterior q_phi and decoder p_theta to explain dynamics under partial observability.",
            "environment_name": "Particles simulator with one removed/unobserved time-series",
            "environment_description": "Offline simulator where one of the true time-series trajectories is removed from observations and acts like a hidden agent that still influences observed agents through the same dynamics; experiments vary how many observed series are directly influenced by the missing series.",
            "handles_distractors": true,
            "distractor_handling_technique": "Latent trajectory imputation (proxy) learned by the encoder; architectural structural bias ensures the latent is used equivalently to observed series and is therefore able to capture direct influences of the missing signal.",
            "spurious_signal_types": "Unobserved dynamic confounders (missing variables that interact with observed variables and can induce spurious pairwise associations).",
            "detection_method": "The encoder attempts to capture unexplained cross-series dependencies by allocating capacity to the latent trajectory; improved decoder likelihood and preserved AUROC relative to baselines are used to verify detection.",
            "downweighting_method": "By explaining interactions via the latent trajectory rather than spurious observed-to-observed edges, the model reduces posterior mass on incorrect edges (implicit downweighting rather than an explicit weight schedule).",
            "refutation_method": "Comparative performance: baselines that ignore (None) or impute mean (Mean) show severe AUROC degradation as more observed series are influenced by the hidden one, while ACD+Latent stays close to the fully observed upper bound; this empirical refutation demonstrates that attributing effects to the latent rejects spurious direct edges.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "As the number of observed series influenced by the unobserved one increases, ACD+Latent maintains much higher AUROC and better future-trajectory prediction than None/Mean baselines (figures 6 and 7); quantitative AUROC values are shown in plots (ACD+Latent tracks Observed baseline closely).",
            "performance_without_robustness": "Baselines ignoring the missing series (None) or imputing its value (Mean) deteriorate strongly as more observed series are influenced by the hidden series (plots show steep AUROC drop).",
            "has_ablation_study": true,
            "number_of_distractors": "1 unobserved time-series (experiments vary how many observed series it influences)",
            "key_findings": "Learning a latent trajectory proxy for a missing dynamic variable substantially mitigates the spurious edge problem that arises from unobserved interacting variables; the architectural choice to treat the latent like an observed series is crucial for effectiveness.",
            "uuid": "e1002.3",
            "source_info": {
                "paper_title": "Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "TTA",
            "name_full": "Test-Time Adaptation (graph optimization)",
            "brief_description": "A test-time optimization procedure that, given a pre-trained decoder f_theta, searches over graph candidates to find the graph that minimizes the decoder loss on the specific test sample, thereby improving per-sample graph estimates and reducing encoder amortization error.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Test-Time Adaptation (TTA) for graph refinement",
            "method_description": "Instantiate a non-amortized variational distribution q(z) over graphs for a test sample and optimize q (or discrete graph variables) to minimize the decoder loss L(x_test, z, f_theta*). Initialization can be random or seeded from the encoder's prediction; run gradient-based optimization on the (learned) decoder loss to find a graph that best explains the observed sequence.",
            "environment_name": "Same simulated datasets (Kuramoto, Particles, Netsim)",
            "environment_description": "Offline per-sample optimization; not an interactive experiment-selection strategy but a test-time inference refinement technique.",
            "handles_distractors": true,
            "distractor_handling_technique": "By optimizing graph structure to maximize predictive fit, TTA effectively downweights or removes edges that would otherwise be spuriously predicted by the amortized encoder; it can also close the amortization gap that causes encoder mispredictions.",
            "spurious_signal_types": "Encoder overfitting-induced spurious edges, non-generalizable graph predictions, sample-specific idiosyncrasies that lead to wrong edge assignments.",
            "detection_method": "Edges that produce poor predictive likelihood under the decoder are discovered (i.e., detection is by loss-based evidence that an edge is not needed to explain data).",
            "downweighting_method": "Optimization reduces posterior mass on edges that hinder predictive performance, effectively downweighting spurious edges in the final graph.",
            "refutation_method": "Edges predicted by the encoder but that do not improve decoder likelihood are removed by the optimization; empirical results show TTA improves AUROC in low-data regimes and reduces encoder overfitting.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "TTA improves performance in low-data regimes — e.g., on Kuramoto, with &lt;50 training samples TTA allows ACD to outperform prior approaches; in experiments TTA (and Enc+TTA) outperform encoder-only in low-data settings (fig. 3).",
            "performance_without_robustness": "Encoder-only (no TTA) performs better in the high-data regime, but suffers from an amortization gap and overfitting in low-data settings (training curves in Appendix B.2 show encoder overfit while decoder generalizes better).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "TTA closes the amortization gap and reduces encoder overfitting by adapting graph predictions to individual test samples; combining encoder initialization with TTA (Enc+TTA) can yield the best results depending on dataset and training size.",
            "uuid": "e1002.4",
            "source_info": {
                "paper_title": "Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "MPIR",
            "name_full": "Minimum Predictive Information Regularization (MPIR)",
            "brief_description": "A baseline causal-discovery method that detects causal links by measuring changes in predictive performance when noise is added to input variables; used in comparisons in this paper.",
            "citation_title": "Discovering nonlinear relations with minimum predictive information regularization",
            "mention_or_use": "mention",
            "method_name": "MPIR (predictive-information perturbation test)",
            "method_description": "Determines causal links by examining the change in predictive performance (on a decoder or predictor) when noise is added to a candidate input variable; large drop in predictive ability indicates the input is causally relevant for the output.",
            "environment_name": "Applied as a baseline on Kuramoto and Netsim experiments",
            "environment_description": "Offline evaluation on simulated time-series; not interactive; used as a comparator for ACD.",
            "handles_distractors": true,
            "distractor_handling_technique": "Perturbation-based detection: by corrupting individual inputs and measuring effect on predictive performance, MPIR can detect variables that are only spuriously correlated (which will not strongly affect predictive performance) versus those that are predictive (causal).",
            "spurious_signal_types": "Spurious correlations that are not predictive when the input is randomized (i.e., correlation vs. causal signal), measurement noise.",
            "detection_method": "Measure change in predictive performance when noise is injected into candidate input variable; significant change implies causal relevance.",
            "downweighting_method": "Not an explicit downweighting mechanism; edges scored by sensitivity to perturbation and thresholded to decide edges.",
            "refutation_method": "Refutation is performed via empirical perturbation: if adding noise to a variable does not degrade prediction, it may be ruled out as causal.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Reported as baseline on Kuramoto: AUROC 0.502 ± 0.006 (Table 1), and on Netsim AUROC 0.484 ± 0.017 (Table 2) in the paper's experimental comparison.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "MPIR is relatively weak on the simulated datasets in this paper (low AUROC) compared to ACD; its perturbation-based detection is a direct method for identifying variables whose predictive effect is spurious or negligible but did not match the performance of amortized, dynamics-aware approaches here.",
            "uuid": "e1002.5",
            "source_info": {
                "paper_title": "Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Proxy-Latent Methods",
            "name_full": "Proxy-based latent-variable causal methods (e.g., Louizos et al. 2017)",
            "brief_description": "Approaches that use encoder-style models to learn latent proxies for unobserved confounders and thereby improve causal effect estimation under hidden confounding; referenced as inspiration for ACD's latent modeling.",
            "citation_title": "Causal effect inference with deep latent-variable models",
            "mention_or_use": "mention",
            "method_name": "Proxy-based deep latent-variable causal inference",
            "method_description": "Encoder models map observed proxies to a latent representation meant to capture hidden confounders; the latent is then used in downstream causal estimation; identification and success require sufficient proxy richness and structural assumptions.",
            "environment_name": "General observational datasets; referred as conceptual inspiration; not evaluated in this paper's simulations directly.",
            "environment_description": "Typically non-interactive observational datasets (the Louizos et al. work focused on treatment-effect estimation tasks).",
            "handles_distractors": true,
            "distractor_handling_technique": "Learn latent proxies from observed variables and condition downstream models on these latents; regularize via priors (e.g., KL) to avoid degenerate solutions.",
            "spurious_signal_types": "Hidden confounding and missing covariates that induce spurious associations.",
            "detection_method": "Implicit via encoder's ability to explain residual dependence; theoretical identifiability results require sufficient proxy structure.",
            "downweighting_method": "Conditioning on latents reduces spurious associations attributed to observed variables; regularization reduces overfitting.",
            "refutation_method": "Model selection and improved predictive fit when conditioning on latents is used as indirect evidence that spurious edges are being explained by the latent rather than true direct causation.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Prior work (Louizos et al.) reported improved causal-effect estimation under hidden confounding when informative proxies are available; in this paper, analogous latent strategies improved graph AUROC versus baselines in simulated tasks.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Proxy-based latent modeling is a viable strategy to mitigate hidden confounding; ACD adopts this idea for temporal causal discovery by training an encoder to output latent proxies (temperature, missing time-series) and conditioning the decoder accordingly.",
            "uuid": "e1002.6",
            "source_info": {
                "paper_title": "Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural relational inference for interacting systems",
            "rating": 2
        },
        {
            "paper_title": "Causal effect inference with deep latent-variable models",
            "rating": 2
        },
        {
            "paper_title": "Neural granger causality for nonlinear time series",
            "rating": 2
        },
        {
            "paper_title": "Discovering nonlinear relations with minimum predictive information regularization",
            "rating": 2
        },
        {
            "paper_title": "Neural relational inference with fast modular meta-learning",
            "rating": 1
        }
    ],
    "cost": 0.020153249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data</h1>
<p>Sindy Löwe<br>UvA-Bosch Delta Lab, University of Amsterdam<br>David Madras<br>University of Toronto, Vector Institute<br>Richard Zemel<br>University of Toronto, Vector Institute<br>Max Welling<br>UvA-Bosch Delta Lab, University of Amsterdam</p>
<p>LOEWE.SINDY @ GMAIL.COM</p>
<p>MADRAS@CS.TORONTO.EDU</p>
<p>ZEMEL@CS.TORONTO.EDU</p>
<p>WELLING.MAX@GMAIL.COM</p>
<p>Editors: Bernhard Schölkopf, Caroline Uhler and Kun Zhang</p>
<h4>Abstract</h4>
<p>On time-series data, most causal discovery methods fit a new model whenever they encounter samples from a new underlying causal graph. However, these samples often share relevant information which is lost when following this approach. Specifically, different samples may share the dynamics which describe the effects of their causal relations. We propose Amortized Causal Discovery, a novel framework that leverages such shared dynamics to learn to infer causal relations from timeseries data. This enables us to train a single, amortized model that infers causal relations across samples with different underlying causal graphs, and thus leverages the shared dynamics information. We demonstrate experimentally that this approach, implemented as a variational model, leads to significant improvements in causal discovery performance, and show how it can be extended to perform well under added noise and hidden confounding.</p>
<p>Keywords: Causal Discovery, Granger Causality, Hidden Confounding, Noisy Observations, Amortization, Time-Series, Graph Neural Networks</p>
<h2>1. Introduction</h2>
<p>Inferring causal relations in observational time-series is central to many fields of scientific inquiry (Spirtes et al., 2000; Berzuini et al., 2012). Suppose you want to analyze fMRI data, which measures the activity of different brain regions over time - how can you infer the (causal) influence of one brain region on another? This question is addressed by the field of causal discovery (Glymour et al., 2019). Methods within this field allow us to infer causal relations from observational data - when interventions (e.g. randomized trials) are infeasible, unethical or too expensive.</p>
<p>In time-series, the assumption that causes temporally precede their effects enables us to discover causal relations in observational data (Peters et al., 2017); with approaches relying on conditional independence tests (Entner and Hoyer, 2010), scoring functions (Chickering, 2002), or deep learning (Tank et al., 2018). All of these methods assume that samples share a single underlying causal graph and refit a new model whenever this assumption does not hold. However, samples with different underlying causal graphs may share relevant information, such as the dynamics describing the effects of causal relations. For instance, we may want to infer synaptic connections (i.e. causal relations) between neurons based on their spiking behavior, from a set of recordings of neuronal</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Amortized Causal Discovery. We propose to train a single model that predicts causal relations across samples with different underlying causal graphs but shared dynamics (Eq. (2)). This allows us to generalize across samples and to improve our performance with additional training data. In contrast, previous approaches (Section 2) fit a new model for every sample with a different underlying causal graph.
firing. Despite recording different populations of neurons with different wiring, the dynamics of how neurons connected by synapses influence one another may stay the same. This principle occurs in a number of areas: fMRI test subjects may have varying brain connectivity but the same underlying neurochemistry; social networks may have differing structure but comparable interpersonal relationships; different stocks may relate differently to one another but obey similar market forces. Despite a range of relevant applications, inferring causal relations across samples with different underlying causal graphs is as of yet largely unexplored.</p>
<p>In this paper, we propose a novel causal discovery framework for time-series that embraces this aspect: Amortized Causal Discovery (Fig. 1). In this framework, we learn to infer causal relations across samples with different underlying causal graphs but shared dynamics. We achieve this by separating the causal relation prediction from the modeling of their dynamics: an amortized encoder predicts the edges in the causal graph, and a decoder models the dynamics of the system under the predicted causal relations. This setup allows us to pool statistical strength across samples and to achieve significant improvements in performance with additional training data. It also allows us to infer causal relations in previously unseen samples without refitting our model. Additionally, we show that Amortized Causal Discovery can improve robustness under hidden confounding by modeling the unobserved variables with the amortized encoder. Our contributions are as follows:</p>
<ul>
<li>We formalize Amortized Causal Discovery (ACD), a novel framework for causal discovery in time-series, in which we learn to infer causal relations from samples with different underlying causal graphs but shared dynamics (Eq. (2)).</li>
<li>We propose a variational model for ACD, applicable to multi-variate, non-linear data.</li>
<li>We present experiments demonstrating the effectiveness of this model on a range of causal discovery datasets, in the fully observed setting, with added noise, and under hidden confounding.</li>
</ul>
<h1>2. Background: Granger Causality</h1>
<p>Granger causality (Granger, 1969) is one of the most commonly used approaches to infer causal relations from observational time-series data. Its central assumption is that causes precede their effects: if the prediction of the future of time-series $Y$ can be improved by knowing past elements of time-series $X$, then $X$ "Granger causes" $Y$. Originally, Granger causality was defined for linear relations; we follow the more recent definition of Tank et al. (2018) for non-linear Granger causality:</p>
<p>Definition 1 Non-Linear Granger Causality: Given $N$ stationary time-series $\boldsymbol{x}=\left{\boldsymbol{x}<em N="N">{1}, \ldots \boldsymbol{x}</em>$, such that}\right}$ across time-steps $t={1, \ldots, T}$ and a non-linear autoregressive function $g_{j</p>
<p>$$
\boldsymbol{x}<em j="j">{j}^{t+1}=g</em>}\left(\boldsymbol{x<em N="N">{1}^{\leq t}, \ldots, \boldsymbol{x}</em>
$$}^{\leq t}\right)+\boldsymbol{\varepsilon}_{j}^{t+1</p>
<p>where $\boldsymbol{x}<em j="j">{j}^{\leq t}=\left(\ldots, \boldsymbol{x}</em>}^{t-1}, \boldsymbol{x<em j="j">{j}^{t}\right)$ denotes the present and past of series $j$ and $\boldsymbol{\varepsilon}</em>}^{t+1}$ represents independent noise. In this setup, time-series $i$ Granger causes $j$, if $g_{j}$ depends on $\boldsymbol{x<em i="i">{i}^{\leq t}$, i.e. if $\exists \boldsymbol{x}</em>}^{\prime \leq t} \neq \boldsymbol{x<em j="j">{i}^{\leq t}: g</em>}\left(\boldsymbol{x<em i="i">{1}^{\leq t}, \ldots, \boldsymbol{x}</em>}^{\prime \leq t}, \ldots, \boldsymbol{x<em j="j">{N}^{\leq t}\right) \neq g</em>}\left(\boldsymbol{x<em i="i">{1}^{\leq t}, \ldots, \boldsymbol{x}</em>\right)$.}^{\leq t}, \ldots \boldsymbol{x}_{N}^{\leq t</p>
<p>Granger causal relations are equivalent to causal relations in the underlying directed acyclic graph (DAG) if all relevant variables are observed and no instantaneous ${ }^{1}$ connections exist (Peters et al., 2013, 2017, Theorem 10.1).</p>
<p>Many methods for Granger causal discovery, including vector autoregressive (Hyvärinen et al., 2010) and more recent deep learning-based approaches (Tank et al., 2018; Khanna and Tan, 2019; Wu et al., 2020), can be encapsulated by a particular framework:</p>
<ol>
<li>Define a function $f_{\theta}$ (an MLP in Tank et al. (2018), a linear model in Hyvärinen et al. (2010)), which learns to predict the next time-step of a given test sequence $\boldsymbol{x}_{\text {test }}$.</li>
<li>Fit $f_{\theta}$ to $\boldsymbol{x}<em _star="\star">{\text {test }}$ by minimizing some loss $\mathcal{L}: \theta</em>}=\operatorname{argmin<em _test="{test" _text="\text">{\theta} \mathcal{L}\left(\boldsymbol{x}</em>\right)$.}}, f_{\theta</li>
<li>Apply some fixed function $h$ (e.g. thresholding) to the learned parameters to produce the Granger causal graph estimate for $\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">{\text {test }}: \hat{\mathcal{G}}</em><em _star="\star">{\text {test }}}=h\left(\theta</em>$ between time-series $i$ and $j$ are zero, then $i$ does not Granger-cause $j$.}\right)$. For instance, Tank et al. (2018) infer the Granger causal relations through examination of the weights $\theta_{\star}$ : if all outgoing weights $\boldsymbol{w}_{i j</li>
</ol>
<p>The shortcoming of this approach is that, when we have $S$ samples $\boldsymbol{x}<em S="S">{1}, \ldots, \boldsymbol{x}</em>$ with different underlying causal graphs, the parameters $\theta$ must be optimized separately for each of them. As a result, methods within this framework cannot take advantage of the information that might be shared between samples. This motivates us to question: can we amortize this process?</p>
<h2>3. Amortized Causal Discovery</h2>
<p>We propose Amortized Causal Discovery (ACD), a framework in which we learn to infer causal relations across samples with different underlying causal graphs but shared dynamics. To illustrate, we return to the example from Section 1: suppose you want to infer synaptic connections (i.e. causal relations) between neurons based on their spiking behavior. You are given a set of $S$ recordings (i.e. samples), each containing $N$ time-series representing the firing of $N$ individual neurons. Even</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>though you might record across different populations of neurons with different wiring, the dynamics of how neurons connected by synapses influence one another stays the same. ACD takes advantage of such shared dynamics to improve the prediction of causal relations. Given a training set $\boldsymbol{X}<em _test="{test" _text="\text">{\text {train }}$ and test sequence $\boldsymbol{x}</em>$, it can be summarized as follows:}</p>
<ol>
<li>Define an encoding function $f_{\phi}$ which learns to infer Granger causal relations of any sample $\boldsymbol{x}<em _text="\text" _train="{train">{i}$ in $\boldsymbol{X}</em>$ which learns to predict the next time-step of the samples under the inferred causal relations.}}$. Define a decoding function $f_{\theta</li>
<li>Fit $f_{\phi}$ and $f_{\theta}$ to $\boldsymbol{X}<em>{\text {train }}$ by minimizing some loss $\mathcal{L}: f</em>{\phi_{<em>}}, f_{\theta_{</em>}}=\operatorname{argmin}<em _phi="\phi">{f</em>}, f_{\theta}} \mathcal{L}\left(\boldsymbol{X<em _phi="\phi">{\text {train }}, f</em>\right)$.}, f_{\theta</li>
<li>For a test sequence $\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">{\text {test }}$, simply output the Granger causal graph estimate $\hat{\mathcal{G}}</em><em _boldsymbol_x="\boldsymbol{x">{\text {test }}}$ : $\hat{\mathcal{G}}</em><em _phi__="\phi_{*">{\text {test }}}=f</em>\right)$}}\left(\boldsymbol{x}_{\text {test }</li>
</ol>
<p>By dividing the model into two parts, an encoder and a decoder, ACD can use the activations of $f_{\phi_{<em>}}$ to infer causal structure. This increases the flexibility of our approach greatly compared to methods that use the learned weights $\theta_{</em>}$ such as the prior Granger causal discovery methods described in Section 2. In this section, we describe our framework in more detail, and provide a probabilistic implementation thereof. We also extend our approach to model hidden confounders.</p>
<p>Preliminaries We begin with a dataset $\boldsymbol{X}=\left{\boldsymbol{x}<em s="1">{s}\right}</em>}^{S}$ of $S$ samples, where each sample $\boldsymbol{x<em s="s">{s}$ consists of $N$ stationary time-series $\boldsymbol{x}</em>}=\left{\boldsymbol{x<em N="N" s_="s,">{s, 1}, \ldots, \boldsymbol{x}</em>}\right}$ across time-steps $t={1, \ldots, T}$. We denote the $t$-th time-step of the $i$-th time-series of $\boldsymbol{x<em i="i" s_="s,">{s}$ as $\boldsymbol{x}</em>}^{t}$ (sometimes omitting $s$ for brevity). We assume there is an associated directed acyclic graph $\mathcal{G<em s="s">{s}^{1: T}=\left{\mathcal{V}</em>}^{1: T}, \mathcal{E<em i="i" s_="s,">{s}^{1: T}\right}$ underlying the generative process of each sample. This is a structural causal model (SCM) (Pearl, 2009). Its endogenous (observed) variables are vertices $v</em>}^{t} \in \mathcal{V<em i="i" s_="s,">{s}^{1: T}$ for each time-series $i$ and each time-step $t$. Every set of incoming edges to an endogenous variable defines inputs to a deterministic function $g</em>}^{t}$ which determines that variable's value ${ }^{2}$. The edges are defined by ordered pairs of vertices $\mathcal{E<em i="i" s_="s,">{s}^{1: T}=\left{\left(v</em>\right)\right}$, which we make two assumptions about:}^{t}, v_{s, j}^{t^{\prime}</p>
<ol>
<li>No edges are instantaneous $\left(t=t^{\prime}\right)$ or go back in time. Thus, $t&lt;t^{\prime}$ for all edges.</li>
<li>Edges are invariant to time. Thus, if $\left(v_{s, i}^{t}, v_{s, j}^{t+k}\right) \in \mathcal{E}<em i="i" s_="s,">{s}^{1: T}$, then $\forall 1 \leq t^{\prime} \leq T-k:\left(v</em>}^{t^{\prime}}, v_{s, j}^{t^{\prime}+k}\right) \in$ $\mathcal{E<em i="i" s_="s,">{s}^{1: T}$. The associated structural equations $g</em>$.}^{t}$ are invariant to time as well, i.e. $g_{s, i}^{t}=g_{s, i}^{t^{\prime}} \forall t, t^{\prime</li>
</ol>
<p>The first assumption states that causes temporally precede their effects and makes causal relations identifiable from observational data, when no hidden confounders are present (Peters et al., 2013, 2017, Theorem 10.1). The second simplifies modeling: it is a fairly general assumption that allows us to define dynamics that govern all time-steps (Eq. (2)).</p>
<p>Throughout this paper, we are interested in discovering the summary graph $\mathcal{G}<em s="s">{s}=\left{\mathcal{V}</em>}, \mathcal{E<em i="i" s_="s,">{s}\right}$ (Peters et al., 2017). It consists of vertices $v</em>} \in \mathcal{V<em s="s">{s}$ for each time-series $i$ in sample $s$, and has directed edges whenever they exist in $\mathcal{E}</em>}^{1: T}$ at any time-step, i.e. $\mathcal{E<em i="i" s_="s,">{s}=\left{\left(v</em>}, v_{s, j}\right) \mid \exists t, t^{\prime}:\left(v_{s, i}^{t}, v_{s, j}^{t^{\prime}}\right) \in \mathcal{E<em s="s">{s}^{1: T}\right}$. Note that while $\mathcal{G}</em>$ may contain (self-)cycles.}^{1: T}$ is acyclic (due to the first assumption above), the summary graph $\mathcal{G}_{s</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Amortized Causal Discovery The key assumption for Amortized Causal Discovery is that there exists some fixed function $g$ that describes the dynamics of all samples $\boldsymbol{x}<em s="s">{s} \in \boldsymbol{X}$ given their past observations $\boldsymbol{x}</em>$ :}^{\leq t}$ and their underlying causal graph $\mathcal{G}_{s</p>
<p>$$
\boldsymbol{x}<em s="s">{s}^{t+1}=g\left(\boldsymbol{x}</em>}^{\leq t}, \mathcal{G<em s="s">{s}\right)+\boldsymbol{\varepsilon}</em>
$$}^{t+1</p>
<p>There are two variables in this data-generating process that we would like to model: the causal graph $\mathcal{G}<em s="s">{s}$ that is specific to sample $\boldsymbol{x}</em>}$, and the dynamics $g$ that are shared across all samples. This separation between the causal graph and the dynamics allows us to divide our model accordingly: we introduce an amortized causal discovery encoder $f_{\phi}$ which learns to infer a causal graph $\mathcal{G<em s="s">{s}$ given the sample $\boldsymbol{x}</em>$ that learns to approximate $g$ :}$, and a dynamics decoder $f_{\theta</p>
<p>$$
\boldsymbol{x}<em _theta="\theta">{s}^{t+1} \approx f</em>}\left(\boldsymbol{x<em _phi="\phi">{s}^{\leq t}, f</em>\right)\right)
$$}\left(\boldsymbol{x}_{s</p>
<p>We formalize Amortized Causal Discovery (ACD) as follows. Let $\mathbb{G}$ be the domain of all possible summary graphs on $\boldsymbol{x}<em s="s">{s}: \mathcal{G}</em>} \in \mathbb{G}$. Let $\mathbb{X}$ be the domain of any single step, partial or full, observed sequence: $\boldsymbol{x<em s="s">{s}^{t}, \boldsymbol{x}</em>}^{\leq t}, \boldsymbol{x<em _phi="\phi">{s} \in \mathbb{X}$. The model consists of two components: a causal discovery encoder $f</em>}: \mathbb{X} \rightarrow \mathbb{G}$ which infers a causal graph for each input sample, and a decoder $f_{\theta}: \mathbb{X} \times \mathbb{G} \rightarrow \mathbb{X}$ which models the dynamics. This model is optimized with a sample-wise loss $\ell: \mathbb{X} \times \mathbb{X} \rightarrow \mathbb{R}$ which scores how well the decoder models the true dynamics of $\boldsymbol{x<em s="s">{s}$, and a regularization term $r: \mathbb{G} \rightarrow \mathbb{R}$ on the inferred graphs. For example, this function $r$ may enforce sparsity by penalizing graphs with more edges. Note, that our formulation of the graph prediction problem is unsupervised: we do not have access to the true underlying graph $\mathcal{G}</em>$ with $S$ samples, we optimize:}$. Then, given some dataset $\boldsymbol{X}_{\text {train }</p>
<p>$$
\begin{aligned}
f_{\phi_{\star}}, f_{\theta_{\star}} &amp; =\operatorname{argmin}<em _phi="\phi">{f</em>}, f_{\theta}} \mathcal{L}\left(\boldsymbol{X<em _phi="\phi">{\text {train }}, f</em>\right) \
\text { where } \mathcal{L}\left(\boldsymbol{X}}, f_{\theta<em s="1">{\text {train }}, \phi, \theta\right) &amp; =\sum</em>}^{S} \sum_{t=1}^{T-1} \ell\left(\boldsymbol{x<em _theta="\theta">{s}^{t+1}, f</em>}\left(\boldsymbol{x<em _phi="\phi">{s}^{\leq t}, f</em>}\left(\boldsymbol{x<em _phi="\phi">{s}\right)\right)\right)+r\left(f</em>\right)\right)
\end{aligned}
$$}\left(\boldsymbol{x}_{s</p>
<p>Once we have completed optimization, we can perform causal graph prediction on any new input test sample $\boldsymbol{x}<em _test="{test" _text="\text">{\text {test }}$ in two ways - we can feed $\boldsymbol{x}</em>}}$ into the amortized encoder and take its output as the predicted edges (Eq. 6); or we can instantiate our estimate $\hat{\mathcal{G}<em _test="{test" _text="\text">{\text {test }} \in \mathbb{G}$ which will be our edge predictions, and find the edges which best explain the observed sequence $\boldsymbol{x}</em>$, which we term Test-Time Adaptation (TTA) (Eq. 7):}}$ by minimizing the (learned) decoding loss with respect to $\hat{\mathcal{G}}_{\text {test }</p>
<p>$$
\begin{aligned}
\hat{\mathcal{G}}^{\mathrm{Enc}} &amp; =f_{\phi_{\star}}\left(\boldsymbol{x}<em _hat_mathcal_G="\hat{\mathcal{G">{\text {test }}\right) \
\hat{\mathcal{G}}^{\mathrm{TTA}} &amp; =\operatorname{argmin}</em>}<em _test="{test" _text="\text">{\text {test }} \in \mathbb{G}} \mathcal{L}\left(\boldsymbol{x}</em>}}, \hat{\mathcal{G}<em _theta__star="\theta_{\star">{\text {test }}, f</em>\right)
\end{aligned}
$$}</p>
<p>By separating the prediction of causal relations from the modeling of their dynamics, ACD yields a number of benefits. ACD can learn to infer causal relations across samples with different underlying causal graphs, and it can infer causal relations in previously unseen test samples without refitting (Eq. (6)). By generalizing across samples, it can improve causal discovery performance with increasing training data size. We can replace either $f_{\phi}$ or $f_{\theta}$ with ground truth annotations, or simulate the outcome of counterfactual causal relations. Additionally, ACD can be applied in the standard causal discovery setting, where only a single causal graph underlies all samples, by replacing the amortized encoder $f_{\phi}$ with an estimated graph $\hat{\mathcal{G}}$ (or distribution over $\mathbb{G}$ ) in Eq. (4).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A Probabilistic Implementation of ACD. An amortized encoder $q_{\phi}(\boldsymbol{z} \mid \boldsymbol{x})$ predicts the causal relations between the input time-series $\boldsymbol{x}$. A decoder $p_{\theta}(\boldsymbol{x} \mid \boldsymbol{z})$ learns to predict the next time-step of the time-series $\boldsymbol{x}^{t+1}$ given their current values $\boldsymbol{x}^{t}$ and the predicted relations $\boldsymbol{z}$. This separation between causal relation prediction and modeling lets us train the model across samples with different underlying causal graphs but shared dynamics (Eq. (2)).</p>
<h1>3.1. A Probabilistic Implementation of ACD</h1>
<p>We take a probabilistic approach to ACD and model the functions $f_{\phi}$ and $f_{\theta}$ using variational inference (Fig. 2). We amortize the encoder $f_{\phi}$ with an encoding function $q_{\phi}(\boldsymbol{z} \mid \boldsymbol{x})$, which outputs a distribution over $\boldsymbol{z}$ representing the predicted edges $\hat{\mathcal{E}}<em _theta="\theta">{E n c}$ in the causal graph; and we learn a decoder $p</em>$ is a variational lower bound:}(\boldsymbol{x} \mid \boldsymbol{z})$ which probabilistically models the dynamics of the time-series under the predicted causal relations. We choose a negative log-likelihood for the decoder loss $\ell$ and a KL-Divergence to a prior distribution over $\mathbb{G}$ for the regularizer $r$. As a result, our loss function $\mathcal{L</p>
<p>$$
\mathcal{L}=\mathbb{E}<em _phi="\phi">{q</em>)\right]
$$}(\boldsymbol{z} \mid \boldsymbol{x})}\left[\log p_{\theta}(\boldsymbol{x} \mid \boldsymbol{z})\right]-\mathrm{KL}\left[q_{\phi}(\boldsymbol{z} \mid \boldsymbol{x}) | p(\boldsymbol{z</p>
<p>Encoder The encoder $q_{\phi}(\boldsymbol{z} \mid \boldsymbol{x})$ applies a graph neural network $f_{\text {enc, } \phi}$ (Scarselli et al., 2008; Li et al., 2016; Gilmer et al., 2017; Kipf and Welling, 2017) to the input, which propagates information across a fully connected graph $\mathcal{G}={\mathcal{V}, \mathcal{E}}$. This graph includes vertices $v_{i} \in \mathcal{V}$ for each time-series $i$, and each pair of vertices $\left(v_{i}, v_{j}\right)$ is connected by an edge.</p>
<p>$$
\begin{aligned}
\boldsymbol{\psi}<em _enc="{enc" _text="\text">{i j} &amp; =f</em>)}, \phi}(\boldsymbol{x<em _phi="\phi">{i j} \
q</em>}\left(\boldsymbol{z<em i="i" j="j">{i j} \mid \boldsymbol{x}\right) &amp; =\operatorname{Softmax}\left(\boldsymbol{\psi}</em> / \tau\right)
\end{aligned}
$$</p>
<p>To enable us to backpropagate through the samples of the discrete distribution $q_{\phi}\left(\boldsymbol{z}_{i j} \mid \boldsymbol{x}\right)$, during training, we relax it by adding Gumbel distributed noise $\boldsymbol{g}$ (Maddison et al., 2017; Jang et al., 2017):</p>
<p>$$
\boldsymbol{z}<em i="i" j="j">{i j} \sim \operatorname{Softmax}\left(\left(\boldsymbol{\psi}</em>\right) / \tau\right)
$$}+\boldsymbol{g</p>
<p>The output $\boldsymbol{z}<em _Enc="{Enc" _text="\text">{i j}$ of the encoder represents the predicted edges $\hat{\mathcal{E}}</em>}}$ in the causal graph $\hat{\mathcal{G}<em _mathcal_E="\mathcal{E">{\text {Enc }}$. We consider the possibility that there are $n</em>\right}$.}}$ different edge-types expressing causal relationships; for instance, inhibitory or excitatory synaptic connections. Then, more specifically, $z_{i j, e}=1$ expresses that there is a directed edge of type $e$ from time-series $i$ to $j$, where $e \in\left{1, \ldots, n_{\mathcal{E}</p>
<p>Decoder The decoder $p_{\theta}(\boldsymbol{x} \mid \boldsymbol{z})$ models the dynamics of the time-series under the predicted causal relations. It uses both the predicted causal relations $\boldsymbol{z}<em 1="1">{i j}$ and the feature vectors of the time-series at the current time-step $t, \boldsymbol{x}^{t}=\left{\boldsymbol{x}</em>\right}$ as its input. First, it propagates information along the}^{t}, \ldots \boldsymbol{x}_{N}^{t</p>
<p>predicted edges by applying a neural network $f_{e}$, using the zero function for $f_{0}$ :</p>
<p>$$
\boldsymbol{h}<em e_0="e&gt;0">{i j}^{t}=\sum</em>} z_{i j, e} f_{e}\left(\left[\boldsymbol{x<em j="j">{i}^{t}, \boldsymbol{x}</em>\right]\right)
$$}^{t</p>
<p>Then, the decoder accumulates the incoming messages to each node and applies a neural network $f_{v}$ to predict the change between the current and the next time-step:</p>
<p>$$
\begin{aligned}
\boldsymbol{\mu}<em j="j">{j}^{t+1} &amp; =\boldsymbol{x}</em>}^{t}+f_{v}\left(\left[\sum_{i \neq j} \boldsymbol{h<em j="j">{i j}^{t}, \boldsymbol{x}</em>\right]\right) \
p_{\theta}\left(\boldsymbol{x}}^{t<em j="j">{j}^{t+1} \mid \boldsymbol{x}^{t}, \boldsymbol{z}\right) &amp; =\mathcal{N}\left(\boldsymbol{\mu}</em>\right)
\end{aligned}
$$}^{t+1}, \sigma^{2} \mathbb{I</p>
<p>In other words, the decoder predicts $\Delta \hat{\boldsymbol{x}}^{t}$, which is added to the current value of the time-series to yield the prediction for the next time-step $\hat{\boldsymbol{x}}^{t+1}=\boldsymbol{x}^{t}+\Delta \hat{\boldsymbol{x}}^{t}$. Note that this approach assumes the existence of self-edges: that is, the value of $\boldsymbol{x}<em j="j">{j}^{t}$ always affects the value of $\boldsymbol{x}</em>$.}^{t+1</p>
<p>Prediction of Causal Relations In order to align our model with the philosophy of Granger Causality, we include a "no edge"-type edge function: If the encoder predicts the "no edge"-type edge $e=0$ by setting $z_{i j, 0}=1$, the decoder uses the zero function and no information is propagated from time-series $i$ to $j$ (Eq. (12)). Due to this, time-series $i$ will Granger cause the decoder-predicted time-series $j$ only when the edge is predicted to exist (see Appendix A). Hence, by the same logic that justifies prior Granger causal work (Section 2), we expect the predicted edges to correspond to Granger causal relations. Finally, since we assume no hidden confounders and no instantaneous edges, these Granger causal relations will correspond to relations in the underlying SCM (Peters et al., 2017, Theorem 10.3).</p>
<h1>3.2. Hidden Confounding</h1>
<p>Hidden confounders are a critical problem in the time-series context: when they exist, Granger causality is not guaranteed to correspond to the true causal graph anymore (Peters et al., 2017, Theorem 10.3) ${ }^{3}$. Inspired by proxy-based methods from causal inference (e.g. Louizos et al. (2017), see Section 4), we present a method for applying ACD to the hidden confounding setting. First, we extend the amortized encoder $q_{\phi}(\boldsymbol{z} \mid \boldsymbol{x})$ to predict an additional variable. Then, we encourage this variable to model the hidden confounder by applying a structural bias - depending on the type of unobserved variable that we want to model, its predicted value is utilized differently by the remaining model. The decoder remains responsible for modeling the dynamics, and now also processes the predictions for the unobserved variable. While this setup might not allow us to identify the hidden confounders, and it is still true that the predicted Granger causal relations may not correspond to the underlying SCM, the data-driven approach underlying ACD can benefit our model: by training across samples with different underlying causal graphs, our model has access to substantially more information about the causal dynamics at hand and we show empirically that it can learn to mitigate the effects of the hidden confounders.</p>
<p>We consider two types of hidden confounders which were chosen to cover a wide range of potential confounders as one might encounter in practice. First, we introduce a temperature variable</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>that confounds all observed variables by influencing the strength of their causal relations. This temperature is sampled separately for each sample, and remains constant throughout each sample. This is a realistic example of a global variable that may influence physical dynamics. Second, we introduce a hidden variable that behaves just like the observed variables, i.e. it may affect or be affected by the observed variables through the same causal relations, and its value changes across time. This confounder is inspired by the introductory example where one wants to infer the causal relations between neurons based on their spiking pattern. In this scenario, it is virtually impossible to record all relevant neurons. The resulting unobserved neurons behave and influence the observed neurons in a similar fashion as the unobserved time-series confounder introduced here. In both scenarios, we extend the encoder to predict this hidden variable, and feed that prediction into the decoder. We provide more details in Section 5.2.</p>
<h1>4. Related Work</h1>
<p>A range of approaches to causal discovery in both temporal and non-temporal data exist (Heinze-Deml et al., 2018; Spirtes et al., 2000; Peters et al., 2017). One common class is constraint-based, relying on conditional independence testing to uncover an underlying DAG structure or equivalence class (Spirtes et al., 2000). These methods predict a single graph $\mathcal{G}$ (or equivalence class) for all samples. There is no notion of fitting a dynamics model for time-series methods in this class (Entner and Hoyer, 2010). Another common class of methods for causal discovery is score-based (Chickering, 2002; Bengio et al., 2019). Here, a score function $h$ is chosen, and the methods perform a search through graph space to optimize this score, i.e. $\mathcal{G}=\operatorname{argmin}<em _theta__="\theta_{*">{\mathcal{G}} h(\mathcal{G})$. Our proposed decoder-based inference (Eq. (7)) can be seen as score-based causal discovery with a learned score function $\mathcal{L} \circ f</em>$. A third class of methods fits a (possibly regularized) dynamics model $f$ and then analyzes its form to produce a causal graph estimate, by using linear dynamics (Hyvärinen et al., 2010), recurrent models (Tank et al., 2018; Khanna and Tan, 2019; Nauta et al., 2019), or other deep-learning based approaches (Wu et al., 2020; Lachapelle et al., 2019; Zheng et al., 2020). See Section 2 for discussion.}</p>
<p>The range of approaches to causal discovery relevant to ours include Li et al. (2018), who propose to learn a linear mixed effects model across samples; concurrent work explores amortized deep learning of differing types of causal structure (Li et al., 2020; Ke et al., 2020) and the relationship between GNNs and causal effect inference (Zečević et al., 2021). Additionally relevant are other approaches for temporal data: works such as Peters et al. (2013) or Eichler (2012) use independence or additivity assumptions. Another adjacent category of causal discovery work explores the idea of jointly learned causal structure across examples, including in the setting where a number of related datasets are collected (Dhir and Lee, 2020; Huang et al., 2019, 2020b; Shimizu, 2012; Tillman and Eberhardt, 2014; Huang et al., 2020a).</p>
<p>There is little systematic study of hidden confounding in the time-series setting. Some empirical work in the non-temporal domain shows that encoder-based models with enough proxies (variables caused by hidden confounders) can improve causal inference under hidden confounding (Louizos et al., 2017; Parbhoo et al., 2020), and theoretical work proves the identifiability of latent variables from proxies under some assumptions (Kruskal, 1977; Allman et al., 2009).</p>
<p>Several works have used graph neural networks (Battaglia et al., 2016; Santoro et al., 2017; Kipf et al., 2018) or attention mechanisms (Vaswani et al., 2017; Fuchs et al., 2019; Goyal et al., 2019; Van Steenkiste et al., 2018) to infer relations between time-series. Alet et al. (2019) propose a meta-learning algorithm to additionally model unobserved variables. While these approaches</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Causal discovery performance (in AUROC) on the particles dataset (A-left) and Kuramoto (B-right). ACD improves with more training data, outperforming previous approaches with as few as 50 available training samples on Kuramoto. In the high-data regime, encoder inference (Enc) is best, while test-time adaptation (TTA and Enc+TTA) is superior in low-data settings.
model object relations in a number of ways, they are not explicitly designed to infer causal graphical structure.</p>
<p>The probabilistic implementation of ACD is based on the Neural Relational Inference (NRI) model (Kipf et al., 2018). We extend this work by new inference methods using test-time adaptation and new algorithms to handle hidden confounders. Moreover, we provide a proof that relates the zero-edge function to Granger causality, which allows for a causal interpretation of the inferred edges. Subsequently, we apply our model to a different problem than NRI, namely (Granger) causal discovery, and show that it outperforms the current state of the art for this type of problem. Last but not least, we show that our model achieves strong causal discovery performance even under noise and hidden confounding, an accomplishment that is - to the best of our knowledge - new in this field.</p>
<h1>5. Experiments</h1>
<p>Implementation We measure causal discovery performance by the area under the receiver operator curve (AUROC) of predicted edge probabilities over test samples. We compare to recurrent models (Tank et al. (2018); Khanna and Tan (2019)), a mutual-information (MI) based model by Wu et al. (2020) and several baselines implemented by those authors, including MI (unmodified), transfer entropy (Schreiber, 2000), and linear Granger causality. More details in Appendix B, our code is available at github.com/loeweX/AmortizedCausalDiscovery.</p>
<h3>5.1. Fully Observed Amortized Causal Discovery</h3>
<p>We test ACD on three datasets: two fully-observed physics simulations (Kuramoto and Particles) and the Netsim dataset of simulated fMRI data (Smith et al., 2011). Note, in contrast to the physics simulations used in Kipf et al. (2018), we generate data with asymmetric connectivity matrices to represent causal relations.</p>
<p>First, we test our method on the Kuramoto dataset, which contains five 1-D time-series of phase-coupled oscillators (Kuramoto, 1975). We find that ACD greatly outperforms all approaches for Granger causal discovery that we compare against (Table 1). In contrast to these approaches, ACD achieves this result without fitting to the test samples.</p>
<p>Additionally, we find that ACD can indeed utilize samples with different underlying causal graphs - its performance improves steadily with increasing training data size (Fig. 3). Nonetheless, it is also applicable to the lowdata regime: when applying ACD with testtime adaptation (TTA), it requires less than 50 training samples to outperform all previous approaches. We note that the baseline performance here is worse than presented elsewhere in the literature - this is because we do not evaluate the prediction of self-connectivity, which is the easiest connectivity to predict.</p>
<table>
<thead>
<tr>
<th style="text-align: right;">Method</th>
<th style="text-align: center;">AUROC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">MPIR (Wu et al., 2020)</td>
<td style="text-align: center;">$0.502 \pm 0.006$</td>
</tr>
<tr>
<td style="text-align: right;">Transfer Entropy (Schreiber, 2000)</td>
<td style="text-align: center;">$0.560 \pm 0.005$</td>
</tr>
<tr>
<td style="text-align: right;">NGC (Tank et al., 2018)</td>
<td style="text-align: center;">$0.574 \pm 0.018$</td>
</tr>
<tr>
<td style="text-align: right;">eSRU (Khanna and Tan, 2019)</td>
<td style="text-align: center;">$0.607 \pm 0.001$</td>
</tr>
<tr>
<td style="text-align: right;">Mutual Information</td>
<td style="text-align: center;">$0.616 \pm 0.000$</td>
</tr>
<tr>
<td style="text-align: right;">Linear Granger Causality</td>
<td style="text-align: center;">$0.647 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: right;">Amortized Causal Discovery</td>
<td style="text-align: center;">$\mathbf{0 . 9 5 2} \pm \mathbf{0 . 0 0 3}$</td>
</tr>
</tbody>
</table>
<p>Table 1: AUROC for causal discovery on Kuramoto dataset. $95 \%$ confidence interval shown.</p>
<p>In our second experiment, we apply ACD to the particles dataset. This dataset models five particles that move around a two-dimensional space, with some particles influencing others unidirectionally by pulling them with a spring. Since all previous methods were intended for onedimensional time-series, we were unable to evaluate them in this domain. ACD, on the other hand, is readily applicable to higher-dimensional data, and performs almost perfectly on this dataset with 0.999 AUROC.</p>
<p>In both experiments, causal relation prediction with the learned encoder (Enc - Eq. (6)) performs best in the high-data regime, while test-time adaptation (TTA - Eq. (7)) improves the performance in low-data settings (Fig. 3). This benefit of TTA can be largely attributed to two effects. First, TTA closes the amortization gap of the encoder (Cremer et al., 2018). Second, TTA overcomes the encoder's overfitting on the training data (as seen in the training curves in Appendix B.2) by adapting to the individual test samples. On the particles dataset, initializing TTA with the encoder's prediction (Enc+TTA) improves over a random initialization (TTA) as the encoder improves; but we do not observe this effect on the Kuramoto dataset.</p>
<p>Finally, we apply ACD to the Netsim dataset (Smith et al., 2011) of simulated fMRI data. Here, the task is to infer the underlying connectivity between 15 brain regions across 50 samples. A single graph underlies all samples, allowing us to demonstrate ACD's applicability to the classical setting. We replace the amortized encoder $q_{\phi}(\boldsymbol{z} \mid \boldsymbol{x})$ with a global latent distribution $q(\boldsymbol{z})$, optimize it through the decoder, and then use test-time adaptation (TTA). Even though our model cannot benefit from its data-driven design here, it performs comparably to methods that are intended for use in the single-graph setting (Table 2).</p>
<table>
<thead>
<tr>
<th style="text-align: right;">Method</th>
<th style="text-align: center;">AUROC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">MPIR (Wu et al., 2020)</td>
<td style="text-align: center;">$0.484 \pm 0.017$</td>
</tr>
<tr>
<td style="text-align: right;">Transfer Entropy (Schreiber, 2000)</td>
<td style="text-align: center;">$0.543 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: right;">NGC (Tank et al., 2018)</td>
<td style="text-align: center;">$0.624 \pm 0.020$</td>
</tr>
<tr>
<td style="text-align: right;">eSRU (Khanna and Tan, 2019)</td>
<td style="text-align: center;">$0.670 \pm 0.015$</td>
</tr>
<tr>
<td style="text-align: right;">Mutual Information</td>
<td style="text-align: center;">$\mathbf{0 . 7 2 8} \pm \mathbf{0 . 0 0 2}$</td>
</tr>
<tr>
<td style="text-align: right;">Linear Granger Causality</td>
<td style="text-align: center;">$0.503 \pm 0.004$</td>
</tr>
<tr>
<td style="text-align: right;">Amortized Causal Discovery</td>
<td style="text-align: center;">$\mathbf{0 . 6 8 8} \pm \mathbf{0 . 0 5 1}$</td>
</tr>
</tbody>
</table>
<p>Table 2: AUROC for causal discovery on Netsim dataset. $95 \%$ confidence interval shown.</p>
<h1>5.1.1. Noisy Data</h1>
<p>Handling noisy data is a key challenge in causal discovery. Here, we show that ACD is robust to a certain amount of observational noise in both the Particles and Kuramoto tasks.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Causal discovery performance (in AUROC) on the particles dataset (A-left) and Kuramoto (B-right) for different levels of observation noise across five seeds. For comparison, the standard deviation of both noiseless datasets is about 0.6 .
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: AUROC with unobserved temperature. ACD with a latent variable outperforms a baseline which imputes a mean temperature, and a learned fixed-temperature decoder (None).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: AUROC with unobserved time-series. As more time-series are influenced by the unobserved one (x-axis), the benefit of using an additional latent variable for modeling its effects grows.</p>
<p>We test ACD's performance for different levels of observation noise added to the Particles and Kuramoto datasets (see Fig. 8 in the Appendix for visual examples). We sample this additive noise from a zero-mean normal distribution with standard deviations varying between 0.0 and 0.2 . For comparison, the input samples in both the Particles and Kuramoto datasets have a standard deviation of 0.6. For each dataset and each standard deviation, we train across five random seeds. In Fig. 4, we show the average AUROC on a test set with the same noise scheme applied. ACD's performance degrades gracefully when stronger observation noise is added. In the highest noise scenario, when the noise is sampled from a normal distribution with a standard deviation of 0.2 , the performance of ACD matches the performance of the baselines from Table 1 on noiseless data.</p>
<h1>5.2. Amortized Causal Discovery under Hidden Confounding</h1>
<h3>5.2.1. LATENT TEMPERATURE</h3>
<p>In this experiment, we use the particles dataset and vary an unobserved temperature variable, which modulates how strongly the particles exert force on each other - higher temperatures result in stronger forces and a more chaotic system. For each $\boldsymbol{x}_{s}$, we sample an independent temperature $c \sim \operatorname{Categorical}\left(\left[\frac{\alpha}{2}, \alpha, 2 \alpha\right]\right)$ from a categorical distribution with $\alpha \in \mathbb{R}$ and equal probabilities. We predict this unobserved temperature by extending the amortized encoder with an additional latent variable, which models a uniform distribution. Then, we add a KL-Divergence between this posterior and a uniform prior on the interval $[0,4 \alpha]$ to our variational loss. To allow for learning in this setting, we introduce an inductive bias: we use a decoder which matches the true dynamics $g$ given the predicted temperature and causal relations. See Appendix C. 1 for more details and additional results.</p>
<p>Results Fig. 5 shows the causal discovery results across different values of $\alpha$. ACD enhanced with an additional latent variable (Latent) outperforms both tested baselines across all temperatures: Mean, which uses the same ground-truth decoder as Latent and fixes the decoder temperature to be the mean of the categorical distribution, and None, which does not model $c$ explicitly and trains an MLP decoder. Additionally, this method achieves high predictive performance on the unobserved temperature variable: for $\alpha=2$, temperature prediction obtains $0.888 R^{2}, 0.966$ AUROC and 0.644 accuracy. These results indicate that we can model an unobserved temperature variable, and thus improve robustness under hidden confounding.</p>
<h3>5.2.2. UnObSERVED TiME-SERIES</h3>
<p>Here, we treat one of the original time-series in the particles dataset as unobserved. It exhibits the same dynamics as the observed time-series, evolving and causally influencing others the same way as before. This challenging setting has received little attention in the literature; Alet et al. (2019) tackled it with mixed success. We model the unobserved time-series by extending the amortized encoder with an additional latent variable and applying a suitable structural bias: the latent prediction $\boldsymbol{z}_{u}^{t}$ for timesteps $t={1, \ldots, T}$ is treated in the same way as the observed time-series $\boldsymbol{x}$. Its entire trajectory is used by the encoder to predict causal relations, and its value at the current time-step is fed into the decoder. See Appendix C. 2 for more details and additional results.</p>
<p>Results Fig. 6 shows how the causal discovery AUROC depends on the number of observed time-series directly influenced by the unobserved one. When this number is zero, all tested approaches perform the same. With growing numbers of influenced time-series, the baselines that either ignore the missing time-series (None) or</p>
<p>impute its value with the average of the observed time-series over time (Mean) deteriorate strongly. In contrast, the proposed ACD with a Latent variable stays closer to the performance of the fully Observed baseline. As shown in Fig. 7, it also improves the future trajectory prediction of the observed time-series. A Supervised baseline, that uses the (usually unavailable) ground-truth trajectories to optimize the prediction of the unobserved time-series, improves only slightly over our approach. These results indicate that ACD can use latent variables to improve robustness to unobserved time-series.</p>
<h1>6. Conclusion</h1>
<p>We introduce ACD, a framework for causal discovery in time-series data which can leverage the information that is shared across samples. We provide a probabilistic implementation of this framework, and demonstrate significant performance gains over the existing literature when predicting causal relations, both in the fully observed setting and with noise and hidden confounding.</p>
<p>Despite this improvement in performance over previous work, several limitations remain. Our assumptions from Sec. 3 regarding shared dynamics and edges in the graph, are not verifiable in practice; however, this is standard in causal inference, which frequently relies on untestable assumptions such as ignorability or consistency.</p>
<p>We conducted all our experiments on simulated data; as a result, they are not particularly realistic: real-world data is more complex and potentially misspecified. This limitation is in line with the data used in related works (e.g. Khanna and Tan (2019)), and it remains an exciting direction for future work to improve modeling and experimentation in more realistic settings.</p>
<p>Finally, our contribution is primarily empirical: we propose and attack a novel version of the causal discovery problem - where samples have different underlying causal graphs but shared dynamics - and demonstrate that ACD outperforms prior methods by a large margin in this setting. It remains an important future direction of inquiry to understand the conditions under which ACD, as well as other Granger causal discovery approaches that inspired ACD (e.g. Tank et al. (2018); Khanna and Tan (2019)), can be guaranteed to identify the correct causal structures.</p>
<h2>Acknowledgments</h2>
<p>Thanks to Thomas Kipf for helpful discussions and to Sara Magliacane, Marco Federici, Gabriele Bani, Joop Pascha, Patrick Forre, Pascal Esser, Maja Rudolph, Elliot Creager, Taylor Killian, and Renjie Liao for their feedback on the manuscript. David Madras was supported by an NSERC Alexander Graham Bell Canada Graduate Scholarship-Doctoral (CGS-D). Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute (www. vectorinstitute.ai/#partners).</p>
<h2>References</h2>
<p>Ferran Alet, Erica Weng, Tomás Lozano-Pérez, and Leslie Pack Kaelbling. Neural relational inference with fast modular meta-learning. In Advances in Neural Information Processing Systems, pages 11804-11815, 2019.</p>
<p>Elizabeth S Allman, Catherine Matias, John A Rhodes, et al. Identifiability of parameters in latent structure models with many observed variables. The Annals of Statistics, 37(6A):3099-3132, 2009 .</p>
<p>Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in Neural Information Processing Systems, pages 4502-4510, 2016.</p>
<p>Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Nan Rosemary Ke, Sebastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle causal mechanisms. In International Conference on Learning Representations, 2019.</p>
<p>Carlo Berzuini, Philip Dawid, and Luisa Bernardinell. Causality: Statistical perspectives and applications. John Wiley \&amp; Sons, 2012.</p>
<p>David Maxwell Chickering. Optimal structure identification with greedy search. Journal of Machine Learning Research, 3(Nov):507-554, 2002.</p>
<p>Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoencoders. In International Conference on Machine Learning, pages 1078-1086, 2018.</p>
<p>Anish Dhir and Ciarán M Lee. Integrating overlapping datasets using bivariate causal discovery. In AAAI, pages 3781-3790, 2020.</p>
<p>Michael Eichler. Causal inference in time series analysis. Wiley Online Library, 2012.
Doris Entner and Patrik O Hoyer. On causal discovery from time series data using FCI. Probabilistic Graphical Models, pages 121-128, 2010.</p>
<p>Fabian B Fuchs, Adam R Kosiorek, Li Sun, Oiwi Parker Jones, and Ingmar Posner. End-to-end recurrent multi-object tracking and trajectory prediction with relational reasoning. arXiv preprint arXiv:1907.12887, 2019.</p>
<p>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, pages 1263-1272, 2017.</p>
<p>Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graphical models. Frontiers in Genetics, 10, 2019.</p>
<p>Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard Schölkopf. Recurrent independent mechanisms. arXiv preprint arXiv:1909.10893, 2019 .</p>
<p>Clive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods. Econometrica: Journal of the Econometric Society, pages 424-438, 1969.</p>
<p>Christina Heinze-Deml, Marloes H Maathuis, and Nicolai Meinshausen. Causal structure learning. Annual Review of Statistics and Its Application, 5:371-391, 2018.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 1735-1780, 1997.</p>
<p>Biwei Huang, Kun Zhang, Pengtao Xie, Mingming Gong, Eric P Xing, and Clark Glymour. Specific and shared causal relation modeling and mechanism-based clustering. In Advances in Neural Information Processing Systems, pages 13510-13521, 2019.</p>
<p>Biwei Huang, Kun Zhang, Mingming Gong, and Clark Glymour. Causal discovery from multiple data sets with non-identical variable sets. In AAAI, pages 10153-10161, 2020a.</p>
<p>Biwei Huang, Kun Zhang, Jiji Zhang, Joseph D Ramsey, Ruben Sanchez-Romero, Clark Glymour, and Bernhard Schölkopf. Causal discovery from heterogeneous/nonstationary data. J. Mach. Learn. Res., 21(89):1-53, 2020b.</p>
<p>Aapo Hyvärinen, Kun Zhang, Shohei Shimizu, and Patrik O. Hoyer. Estimation of a structural vector autoregression model using non-gaussianity. Journal of Machine Learning Research, 11(56): $1709-1731,2010$.</p>
<p>Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparametrization with gumble-softmax. In International Conference on Learning Representations, 2017.</p>
<p>Nan Rosemary Ke, Jane Wang, Jovana Mitrovic, Martin Szummer, Danilo J Rezende, et al. Amortized learning of neural causal representations. arXiv preprint arXiv:2008.09301, 2020.</p>
<p>Saurabh Khanna and Vincent YF Tan. Economy statistical recurrent units for inferring nonlinear granger causality. International Conference on Learning Representations, 2019.</p>
<p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.</p>
<p>Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017.</p>
<p>Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In International Conference on Machine Learning, pages 2688-2697, 2018.</p>
<p>Joseph B Kruskal. Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics. Linear Algebra and its Applications, 18(2):95-138, 1977.</p>
<p>Yoshiki Kuramoto. Self-entrainment of a population of coupled non-linear oscillators. In International Symposium on Mathematical Problems in Theoretical Physics, pages 420-422. Springer, 1975 .</p>
<p>Sébastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradient-based neural dag learning. In International Conference on Learning Representations, 2019.</p>
<p>Xiang Li, Shanghong Xie, Peter McColgan, Sarah J Tabrizi, Rachael I Scahill, Donglin Zeng, and Yuanjia Wang. Learning subject-specific directed acyclic graphs with mixed effects structural equation models from observational data. Frontiers in genetics, 9:430, 2018.</p>
<p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. In International Conference on Learning Representations, 2016.</p>
<p>Yunzhu Li, Antonio Torralba, Animashree Anandkumar, Dieter Fox, and Animesh Garg. Causal discovery in physical systems from videos. arXiv preprint arXiv:2007.00631, 2020.</p>
<p>Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. In International Conference on Learning Representations, 2017.</p>
<p>Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling. Causal effect inference with deep latent-variable models. In Advances in Neural Information Processing Systems, pages 6446-6456, 2017.</p>
<p>Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. In International Conference on Learning Representations, 2017.</p>
<p>Meike Nauta, Doina Bucur, and Christin Seifert. Causal discovery with attention-based convolutional neural networks. Machine Learning and Knowledge Extraction, 1(1):312-340, 2019.</p>
<p>Sonali Parbhoo, Mario Wieser, Aleksander Wieczorek, and Volker Roth. Information bottleneck for estimating treatment effects with systematically missing covariates. Entropy, 22(4):389, 2020.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pages 8024-8035, 2019.</p>
<p>Judea Pearl. Causality. Cambridge University Press, 2009.
Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Causal inference on time series using restricted structural equation models. In Advances in Neural Information Processing Systems, pages 154-162, 2013.</p>
<p>Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. MIT Press, 2017.</p>
<p>Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In Advances in Neural Information Processing Systems, pages 4967-4976, 2017.</p>
<p>Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.</p>
<p>Thomas Schreiber. Measuring information transfer. Physical Review Letters, 85(2):461, 2000.
Shohei Shimizu. Joint estimation of linear non-gaussian acyclic models. Neurocomputing, 81: 104-107, 2012.</p>
<p>Stephen M Smith, Karla L Miller, Gholamreza Salimi-Khorshidi, Matthew Webster, Christian F Beckmann, Thomas E Nichols, Joseph D Ramsey, and Mark W Woolrich. Network modelling methods for fMRI. Neuroimage, 54(2):875-891, 2011.</p>
<p>Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction, and search. MIT Press, 2000.</p>
<p>Alex Tank, Ian Covert, Nicholas Foti, Ali Shojaie, and Emily Fox. Neural granger causality for nonlinear time series. arXiv preprint arXiv:1802.05842, 2018.</p>
<p>Robert E Tillman and Frederick Eberhardt. Learning causal structure from multiple datasets with similar variable sets. Behaviormetrika, 41(1):41-64, 2014.</p>
<p>Sjoerd Van Steenkiste, Michael Chang, Klaus Greff, and Jürgen Schmidhuber. Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. In International Conference on Learning Representations, 2018.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998-6008, 2017.</p>
<p>Tailin Wu, Thomas Breuel, Michael Skuhersky, and Jan Kautz. Discovering nonlinear relations with minimum predictive information regularization. arXiv preprint arXiv:2001.01885, 2020.</p>
<p>Matej Zečević, Devendra Singh Dhami, Petar Veličković, and Kristian Kersting. Relating graph neural networks to structural causal models. arXiv preprint arXiv:2109.04173, 2021.</p>
<p>Xun Zheng, Chen Dan, Bryon Aragam, Pradeep Ravikumar, and Eric Xing. Learning sparse nonparametric dags. In International Conference on Artificial Intelligence and Statistics, pages 3414-3425. PMLR, 2020.</p>
<h1>Appendix A. Granger Causality of ACD</h1>
<p>Here, we show that when constraining the edge-type $e=0$ to be the zero function, time series $i$ does not Granger cause the model prediction of $j$ in ACD. In the noiseless setting, at the global optimum of sufficiently expressive model classes, this is equivalent to saying that we recover the true Granger causal relations.</p>
<p>Claim. If $z_{i j, 0}=1, i$ does not Granger cause the model prediction of $j$ in ACD.
Proof. According to Theorem 1, time-series $i$ does not cause $j$, if $g_{j}$ is invariant to $\boldsymbol{x}<em j="j">{i}^{\leq t}$. In our model, the decoder represents this non-linear model $g</em>$, and $i$ does not Granger cause these predictions.}$ and consists of two functions. First, it propagates information across edges using Eq. (12). This function returns a value of zero, if $z_{i j, 0}=1$. This output is used for the second function, described in Eq. (1), which does not introduce any new terms that depend on $i$. Thus, if $z_{i j, 0}=1$, the decoder's prediction for $j$ is invariant to $\boldsymbol{x}_{i}^{\leq t</p>
<h2>Appendix B. Fully Observed Amortized Causal Discovery</h2>
<h2>B.1. Experimental Details</h2>
<h2>B.1.1. DATASETS</h2>
<p>Physics Simulations To generate these simulations, we follow the description of the underlying physics of Kipf et al. (2018) for the phase-coupled oscillators (Kuramoto) (Kuramoto, 1975) and the particles connected by springs. In contrast to their simulations, however, we allow the connectivity matrix, which describes which time-series influences another, to be asymmetric. This way, it describes causal relations instead of correlations.</p>
<p>For both datasets, we generate 50,000 training and 10,000 validation samples. We restrict the number of test samples to 200, since the previous methods we compare to must be refit for each individual sample. We simulate systems with $N=5$ time-series. Our training and validation samples consist of $T=49$ time-steps, while the test-samples are $T=99$ time-steps long. This increased length allows us to infer causal relations on the first half of the data, and to test the future prediction performance on the second half (with $k={1, \ldots, 49}$ ). In Figure 8, we show some examples of both the particles and Kuramoto datasets in the noiseless setting and with added noise.</p>
<p>Netsim The Netsim dataset simulates blood-oxygen-level-dependent (BOLD) imaging data across different regions within the human brain and is described in Smith et al. (2011). The task is to infer the directed connections, i.e. causal relations, between different brain areas.</p>
<p>The Netsim dataset includes simulations with different numbers of brain regions and different underlying connectivity matrices. In our experiments, we use the data from the third simulation Sim-3.mat as provided by Khanna and Tan (2019). It consists of samples from 50 subjects, each with the same underlying causal graph, each of length $T=200$ and including $N=15$ different brain regions. Note, that we report worse results than Khanna and Tan (2019), since we assume self-connectivity for all time-series and only evaluate the causal discovery performance between different time-series.</p>
<p>The dataset is very small ( 50 samples) and due to this, we do not use a training/validation/test split, but use the same 50 points at each phase instead. While this is not standard machine learning practice, it still facilitates a fair comparison to the other methods, each of which is fit to individual</p>
<p>test points. The purpose of including experiments on this dataset is not to demonstrate generalization ability, but rather to show that our method is flexible enough to work reasonably well even in the classical causal discovery setting (with one shared causal graph, and fitting the model on the test set).</p>
<h1>B.1.2. ARCHITECTURE AND HYPERPARAMETERS</h1>
<p>Our model is implemented in PyTorch (Paszke et al., 2019). We did no hyperparameter optimization for model training, but used the settings as described for the NRI model (Kipf et al., 2018). The latent dimension throughout the model is set to size 256. We optimize our model using ADAM (Kingma and $\mathrm{Ba}, 2015$ ) with a learning rate of 0.0005 . In the experiments on the particles dataset, the learning rate is decayed by a factor of 0.5 every 200 epochs. We set our batch-size to 128 and train for 500 epochs. The temperature of the Gumbel-Softmax is set to $\tau=0.5$. During testing, this concrete distribution is replaced by a categorical distribution to obtain discrete edge predictions.</p>
<p>There was no thorough hyperparameter optimization done for test-time adaptation (TTA). Since there was no pre-existing implementation, some hand-tuning was performed. We use a learning rate of 0.1 for the Kuramoto and particles datasets and 0.01 for Netsim. For each, we run 1000 iterations.</p>
<p>Encoder In our experiments, the amortized encoder applies a graph neural network $f_{e n c, \phi}$ on the input. It implements two edge-propagation steps along the causal graph:</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{\psi}<em _mathrm_emb="\mathrm{emb">{j}^{1}=f</em>}}\left(\boldsymbol{x<em i="i" j="j">{j}\right) \
&amp; \boldsymbol{\psi}</em>}^{1}=f_{v}^{1}\left(\left[\boldsymbol{\psi<em j="j">{i}^{1}, \boldsymbol{\psi}</em>\right]\right) \
&amp; \boldsymbol{\psi}}^{1<em v="v">{j}^{2}=f</em>}^{2}\left(\sum_{i \neq j} \boldsymbol{\psi<em i="i" j="j">{i j}^{1}\right) \
&amp; \boldsymbol{\psi}</em>}=f_{e}^{2}\left(\left[\boldsymbol{\psi<em j="j">{i}^{2}, \boldsymbol{\psi}</em>\right]\right)
\end{aligned}
$$}^{2</p>
<p>$f_{e}^{1}, f_{e}^{2}$ and $f_{e}^{2}$ are fully-connected networks (MLPs). On both the particles dataset and Netsim, $f_{\text {emb }}$ is an MLP as well (MLP Encoder); on the Kuramoto dataset, we use a 1D CNN with attentive pooling (Lin et al., 2017) instead (CNN Encoder).</p>
<p>When conducting test-time adaptation as described in Eq. (7), we remove the encoder and model a distribution over $\mathbb{G}$ using a non-amortized variational distribution $q(\boldsymbol{z})$ with its initial values sampled from a unit Gaussian.</p>
<p>Decoder The decoder implements a single edge-propagation step according to equations 12-14. It uses MLPs for both $f_{e}$ and $f_{v}$. To improve performance, we train the decoder to predict several time-steps into the future. For this, we replace the true input $\boldsymbol{x}^{t}$ with the predicted $\boldsymbol{\mu}^{t}$ for $k=10$ steps.</p>
<p>Following our causal formulation of the NRI model, we implement Eq. (12) by masking out the values of the corresponding edges. Thus, the ordering of the edge types is not arbitrary in our setting.</p>
<p>We note that while this implementation of the decoder assumes a full-time graph of Markov order 1, the full ACD framework does not, and could be implemented using a recurrent architecture to remove this assumption.</p>
<p>Since our physics simulations are differentiable, we can replace the decoder with the ground-truth dynamics and backpropagate through them. We call this setup the simulation decoder.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Example trajectories with different levels of observation noise for the Particles dataset (left) and Kuramoto (right). The observation noise is sampled from a zero-mean Gaussian distribution with standard deviation of (from top to bottom) $0.0,0.1$ and 0.2 , respectively.</p>
<p>Variance When we report the variance on the ACD results, we collected these across five different random seeds. Baselines in Kuramoto/Netsim use three seeds each, except for NGC, which uses only one due to a longer runtime (the confidence intervals shown for NGC are confidence intervals on the AUROC itself, whereas all other confidence intervals are based on variance of AUROC across seeds).</p>
<h1>B.1.3. BASELINES</h1>
<p>We compare ACD against several baselines:
Neural Granger Causality From Tank et al. (2018), we optimized an MLP or LSTM to do next step prediction on a sample. We found that the MLP worked best. The causal links are wherever an input weight is non-zero. We used ADAM and then line search to find exact zeros. In this method, we calculate AUROC by running with a range of sparsity hyperparameters $(\lambda=[0,0.1,0.2,0.4,0.8]$</p>
<p>for Kuramoto and $\lambda=[0,0.1,0.15,0.2,0.3,0.4,0.5,0.6,1]$ for Netsim). As in Tank et al. (2018), we calculate a score $s$ for each edge, where $s=\min \left{\lambda: z_{i j, 0}=1\right}$, and use that score to calculate AUROC. Code was used from https://github.com/icc2115/Neural-GC.</p>
<p>ESRU Khanna and Tan (2019) take a similar approach to Tank et al. (2018), but they use economy statistical recurrent units (eSRU), instead of LSTMs. We found one layer worked best, and used their hyperparameters otherwise. We use sparsity hyperparameters $[0.1,0.2,0.3,0.4,0.5]$ for Kuramoto, and $[0,0.1,0.15,0.2,0.3,0.4,0.5,0.6,1]$ for Netsim. Code was used from https://github. com/sakhanna/SRU_for_GCI.</p>
<p>MPIR Wu et al. (2020) determine where causal links exist by examining the predictive performance change when noise is added to an input variable. Code for this method and the baselines below was used from https://github.com/tailintalent/causal.</p>
<p>Transfer Entropy Schreiber (2000) suggest this entropy-like measure between two variables to produce a metric which is likely to be higher when a causal connection exists. We use the implementation by Wu et al. (2020).</p>
<p>Mutual Information Using the implementation by Wu et al. (2020), we calculate the mutual information between every pair of time series.</p>
<p>Linear Granger Causality Using the implementation by Wu et al. (2020), this is a linear version of Granger causality where non-zero linear weights are taken as greater causal importance.</p>
<p>We did not run the baselines on the particles dataset since it is two-dimensional and most baselines did not provide an obvious way for handling multi-dimensional time series. When training ACD on the particles and Kuramoto datasets, we additionally input the velocity (and phase for Kuramoto) of the time-series. Since our chosen NRI encoders and decoders are not recurrent we cannot recover this information in any other way in this model. This enables a more fair comparison to the recurrent methods, which are able to aggregate this information over several time steps.</p>
<h1>B.2. Additional Experimental Result - Training Curves</h1>
<p>Fig. 9 shows the training curves when training on 100 training samples of the particles dataset. We observe that the encoder overfits on the training samples, as indicated by the AUROC performance. In contrast, the decoder shows less overfitting as indicated by the negative log-likelihood (NLL) performance.</p>
<h2>Appendix C. Amortized Causal Discovery with Unobserved Variables</h2>
<h2>C.1. Temperature Experiments</h2>
<p>Implementation Details In this experiment, we use the CNN encoder and a simulation decoder matching the true generative ODE process. Our optimization scheme is the same as before.</p>
<p>For modeling the latent temperature, we output a uniform distribution as our posterior $q_{\phi_{v}}(c \mid \boldsymbol{x})$. One tricky aspect about this is the KL-Divergence:</p>
<p>$$
K L\left(q_{\phi_{v}}(c \mid \boldsymbol{x}) | p(c)\right)=-\int q_{\phi_{v}}(c \mid \boldsymbol{x}) \log \frac{q_{\phi_{v}}(c \mid \boldsymbol{x})}{p(c)} d \boldsymbol{z}
$$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>For instance, if an unobserved time-series $\hat{U}$ causes both time-series $X$ and $Y$, then the past of $X$ can help predict the future of $Y$, even though there is no causal link between them.</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
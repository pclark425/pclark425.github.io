<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3321 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3321</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3321</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-bcf2c7e3f4ed64c8294c35a59220a26dd4f40060</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bcf2c7e3f4ed64c8294c35a59220a26dd4f40060" target="_blank">OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work presents OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam, and implements a comprehensive assessment methodology to accurately evaluate model responses.</p>
                <p><strong>Paper Abstract:</strong> Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.97% on OlympiadBench, with a mere 10.74% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at \url{https://github.com/OpenBMB/OlympiadBench}</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3321.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3321.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V (GPT-4Vision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source multimodal variant of GPT-4 that accepts images and text and is evaluated on OlympiadBench for multimodal scientific reasoning; used in experiments in this paper and analyzed for error modes (hallucination, calculation, conceptual confusion).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal GPT-4 variant (closed-source) that accepts image + text inputs; evaluated via API in zero-shot setting using a standardized prompt template; shows capabilities in visual reasoning but exhibits hallucinations, algebraic simplification errors, and classification-discussion failures.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot step-by-step prompting (explicit answer formatting)', 'visual (multimodal) grounding / image understanding', 'implicit stepwise natural-language reasoning (free-form)', 'symbolic verification via external scoring (SymPy used by authors, not the model)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluated in a zero-shot setting using a unified prompt template that enforces stepwise solutions and explicit final-answer formatting; relies on its internal chain of reasoning in natural language when given images (multimodal); the paper uses SymPy-based automatic scoring to check symbolic/numeric outputs externally. No special chain-of-thought prompt was applied to GPT-4V in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar style: primarily zero-shot step-by-step natural-language reasoning with multimodal inputs; does not use specialized diverse prompting methods in the experiments (except typical stepwise prompting enforced by template).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OlympiadBench (bilingual multimodal Olympiad-level math & physics benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>8,476 open-ended math and physics problems (many with images), expert step-by-step solutions provided, evaluated via an automatic scoring pipeline for numeric and symbolic answers and manual sampling for theorem proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported performance (zero-shot multimodal evaluation using the paper's prompt template): Math average 21.70% (breakdown: En_COMP 27.18%, Zh_COMP 14.87%, Zh_CEE 21.27%), Physics average 10.74% (En_COMP 11.42%, Zh_CEE 10.45%), Overall average 17.97%. In text-only experiments (using text-only inputs), GPT-4V achieved higher scores: overall 29.07%, Math avg 31.01%, Physics avg 16.24%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to GPT-4 (text-only), GPT-4V (multimodal) performs better on long-context physics problems (long-PhO subset: GPT-4V 18.47% vs GPT-4 14.92%) but worse on normal short-context physics problems (GPT-4V 1.35% vs GPT-4 4.05%), indicating multimodal training may improve long-context reasoning but not uniformly. Multimodal evaluation (with images) is much harder; text-only runs of GPT-4V show improved absolute scores versus multimodal runs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4V, despite being the best model tested, attains low absolute accuracy on OlympiadBench (17.97% multimodal); uses stepwise natural-language reasoning with multimodal inputs constrained by the zero-shot prompt; multimodal training appears to modify long-context reasoning capabilities; common failure modes include hallucination, algebraic manipulation errors, insufficient case analysis, and underuse of image details.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Multimodal training does not always yield better text-only performance (paper notes multimodal training slightly hurts text-only results in some splits). GPT-4V often hallucinated or used incorrect theorems; success on simple answers with incorrect processes (i.e., correct final answer despite wrong steps) shows that diverse internal reasoning does not guarantee correct reasoning trace.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3321.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3321.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (text-only variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Text-only GPT-4 (closed-source) evaluated on OlympiadBench text-only questions as a baseline to compare with the multimodal GPT-4V, showing similar high-level capabilities in text-only settings and differing behavior on long-context physics problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large language model (text-only interface) used via API; evaluated zero-shot with the same prompt template (without images) to compare against GPT-4V's multimodal performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot step-by-step prompting (explicit answer formatting)', 'text-only internal natural-language reasoning (chain-of-thought-like behavior emerges but not explicitly prompted in this study)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluated in zero-shot on text-only inputs with the same prompt template that enforces stepwise outputs and final-answer formatting; no special CoT prompt applied for GPT-4 in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar style: zero-shot stepwise textual reasoning; no explicit diverse prompting methods applied in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OlympiadBench (text-only subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Subset of OlympiadBench questions that are text-only (no images), evaluated with the same automatic scoring pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Text-only experiment results: GPT-4 overall average 29.93%, Math avg 32.00%, Physics avg 16.24% (breakdowns: En_COMP Math 30.42%, Zh_COMP Math 16.42%, Zh_CEE Math 37.98%; Physics En_COMP 12.29%, Zh_CEE 24.35%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to GPT-4V, GPT-4 (text-only) slightly outperforms GPT-4V on normal short-context physics problems but underperforms on long-context (long-PhO) problems; overall text-only scores are higher than multimodal-evaluation GPT-4V scores, indicating differences introduced by multimodal training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text-only GPT-4 attains substantially higher accuracy on text-only questions than GPT-4V's multimodal evaluation, highlighting that multimodal model evaluation with images increases difficulty; GPT-4 and GPT-4V differences suggest multimodal training alters long-context reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>GPT-4's advantage is not universal: GPT-4V surpasses GPT-4 on long-context physics subsets, showing that neither text-only nor multimodal alone guarantees dominance across all reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3321.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3321.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeekMath-7B-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeekMath-7B-RL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7B model specialized in mathematical reasoning, pre-trained on math-related corpora and fine-tuned with reinforcement learning to improve chain-of-thought style reasoning, used here as a strong baseline on text-only OlympiadBench problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeekMath-7B-RL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 7B language model pre-trained with ~120B math-related data and RL-based tuning to enhance chain-of-thought (CoT) reasoning; requires an explicit chain-of-thought prompt appended during evaluation as per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['explicit chain-of-thought prompting', 'RL fine-tuning to improve CoT reasoning', 'zero-shot with dedicated CoT prompt (for evaluation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Model was pre-trained on large math-specific corpora and further trained via reinforcement learning to encourage high-quality chain-of-thought reasoning; the authors appended a specific chain-of-thought prompt for DeepSeekMath during evaluation to trigger its CoT behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse relative to other tested models: explicitly trained and prompted for chain-of-thought reasoning (distinct from the zero-shot stepwise prompts used for other LMMs/LLMs in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OlympiadBench (text-only evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Text-only subset of OlympiadBench; DeepSeekMath evaluated with its required CoT prompt in zero-shot to leverage its RL-enhanced chain-of-thought capability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Text-only experiment results: overall average 17.02%, Math avg 18.09% (En_COMP 19.44%, Zh_COMP 2.70%, Zh_CEE 22.42%), Physics avg 9.97% (En_COMP 6.78%, Zh_CEE 16.52%). In the paper DeepSeekMath-7B-RL outperforms or matches some larger closed-source LMMs on text-only math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to LMMs and general LLMs, DeepSeekMath's specialized CoT training gives it strong text-only math performance relative to its size; the paper highlights that DeepSeekMath (despite smaller size) outperforms or is on par with Gemini-ProVision and Qwen-VL-Max on text-only parts, illustrating that targeted CoT training can substitute for mere model scale in some reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Specialized chain-of-thought training (via RL) plus explicit CoT prompting yields competitive performance on difficult math benchmarks even at modest parameter sizes; demonstrates that using a distinct reasoning method (CoT with RL) can close gaps versus larger generalist models.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Despite CoT-focused training, DeepSeekMath's overall accuracy remains low on OlympiadBench (17.02% text-only), showing CoT alone does not solve the benchmark. Its performance is uneven across languages and subsets (e.g., low Zh_COMP score), indicating limits to specialization and prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3321.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3321.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-VL-Max</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-VL-Max</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large closed-source vision-language model (from Alibaba) trained on large bilingual image-text data; evaluated as a strong multilingual LMM baseline with advantages on Chinese problems in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-VL-Max</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large vision-language model trained on multilingual image-text pairs; evaluated both multimodally and in text-only mode (text-model counterpart) in the paper. Noted to have relative strength on Chinese due to training data composition.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot step-by-step prompting', 'multimodal visual-text reasoning (vision + language)', 'bilingual image-text pretraining (affects reasoning in Chinese)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Relies on multimodal pretraining and zero-shot prompting with the standardized prompt template; no explicit chain-of-thought prompting used in the experiments. The modelâ€™s bilingual visual-text pretraining biases it toward better Chinese multimodal performance according to the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar style: zero-shot multimodal reasoning; no specialized CoT prompting applied in experiments (except text-model counterpart runs for text-only).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OlympiadBench (multimodal & text-only comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Evaluated on both the multimodal OlympiadBench and the text-only subset to compare effect of multimodal training and bilingual data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Multimodal block: Overall avg 10.09% (Math avg 12.65%, Physics avg 5.09%). Text-only block: Overall avg 18.27% (Math avg 19.70%, Physics avg 8.83%). Notably stronger on Chinese math subsets (e.g., Zh_COMP and Zh_CEE) in multimodal runs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Qwen-VL-Max performs substantially better in text-only runs than in multimodal runs on OlympiadBench (18.27% vs 10.09% overall), supporting the paper's observation that multimodal training can slightly hurt text-only performance; it performs better on Chinese-language subsets compared to English, consistent with its training data mix.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multimodal LMMs trained on bilingual data (like Qwen-VL-Max) show relative strength on Chinese multimodal problems but overall lower accuracy on this difficult benchmark; performance improves in text-only mode, indicating that modality and input formatting materially affect reasoning results.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Qwen-VL-Max's multimodal performance is notably lower than some text-only results (i.e., text-only run outperforms multimodal), showing that more modalities do not necessarily produce better performance on complex reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3321.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3321.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-Pro-Vision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-Pro-Vision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source multimodal model in the Gemini family (Google/DeepMind); evaluated as a second-tier closed-source LMM baseline in the paper and found to underperform the top closed-source model on OlympiadBench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-Pro-Vision</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A member of the Gemini family of multimodal models (closed-source); used via the accessible Gemini-Pro API in experiments in place of unavailable Gemini-Ultra. Evaluated zero-shot with the shared prompt template.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot multimodal stepwise prompting', 'multimodal pretraining (vision + language)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Applied in the paper via zero-shot prompts for stepwise solutions; relied upon multimodal pretraining for image understanding but no specialized CoT prompting was used.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar style: standard zero-shot multimodal reasoning; no distinct or specialized chain-of-thought prompting in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OlympiadBench (multimodal & text-only comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Evaluated on both multimodal and text-only subsets (text-only used the corresponding text-model API) to contrast modality effects on reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Multimodal block: Overall avg 4.22% (Math avg 5.14%, Physics avg 2.45%). Text-only block: Overall avg 7.34% (Math avg 7.63%, Physics avg 5.41%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Gemini-Pro-Vision's performance is substantially lower than GPT-4V and Qwen-VL-Max on OlympiadBench; the paper notes Gemini-ProVision (second-tier closed-source) is 'much less compatible' with extremely hard tasks like OlympiadBench despite being a multimodal model.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Being a multimodal closed-source model does not guarantee superior performance on extremely challenging reasoning benchmarks; Gemini-Pro-Vision underperforms both larger closed-source and some open-source specialized models on OlympiadBench.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Despite being a recent multimodal model, Gemini-Pro-Vision scored only slightly above some open-source models and well below GPT-4V, indicating that model family/scale and training regimen matter more than mere multimodality for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3321.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3321.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yi-VL-34B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yi-VL-34B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 34B multimodal model trained with Chinese image-text pairs, evaluated on OlympiadBench and noted for better Chinese performance due to its training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi-VL-34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 34B vision-language model trained on bilingual image-text data (with significant Chinese content); evaluated multimodally (single image limit noted) and in text-only mode via its text-model counterpart (Yi-34B-Chat).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot multimodal stepwise prompting', 'bilingual visual-text pretraining']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Evaluated using the paper's zero-shot prompt template; its bilingual visual-text training gives it relative strengths on Chinese multimodal tasks; accepts only single-image input in tested checkpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar style: zero-shot multimodal reasoning (no specialized CoT prompting in experiments); limited by single-image input for some problems.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OlympiadBench (multimodal & text-only comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Evaluated on multimodal OlympiadBench and text-only subsets to analyze bilingual and multimodal effects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Multimodal block: Overall avg 3.42% (Math avg 4.23%, Physics avg 1.46%). Text-only block: Overall avg 5.72% (Math avg 6.24%, Physics avg 2.28%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Yi-VL-34B shows better relative performance on Chinese questions due to training data composition but overall performance remains low versus top closed-source models; text-only runs modestly outperform multimodal runs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Open-source large multimodal models with bilingual pretraining can show language-specific advantages, but still achieve low absolute accuracy on Olympiad-level reasoning; modality constraints (single-image input) limit handling of multi-image progressive physics problems.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Single-image input limitation and generally low scores show that being large and bilingual is not sufficient to solve OlympiadBench; multimodal capability did not translate to high performance here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3321.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3321.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-NeXT-34B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-NeXT-34B (and its base LLM Nous-Hermes-2-Yi-34B used for text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 34B multimodal model claiming improvements in reasoning, OCR, and world knowledge; evaluated multimodally and via its base LLM for text-only questions in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-NeXT-34B (multimodal) / Nous-Hermes-2-Yi-34B (text base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 34B multimodal model with reported improvements in reasoning and OCR; in this paper the multimodal checkpoint (limited to single-image input) is used for multimodal evaluation, while its base LLM Nous-Hermes-2-Yi-34B was used for text-only evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot multimodal stepwise prompting', 'improved OCR and world-knowledge integration (claimed)', 'text-only evaluation via base LLM']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Applied in zero-shot with the unified prompt; multimodal checkpoint had single-image input constraint and was compared to text-only base LLM runs to measure modality effects.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar style: zero-shot multimodal/text-only reasoning; no specialized CoT prompting in the reported evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>OlympiadBench (multimodal & text-only comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Evaluated on multimodal OlympiadBench and a text-only subset (base LLM) to compare multimodal capabilities versus text-only reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Multimodal block: Overall avg 3.65% (Math avg 4.30%, Physics avg 2.08%). Text-only block: Overall avg 5.87% (Math avg 6.29%, Physics avg 3.13%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Text-only runs of the base LLM improve performance over the multimodal checkpoint in this evaluation; the single-image constraint and multimodal overhead may reduce effectiveness on large multimodal scientific problems.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLaVA-NeXT-34B's claimed improvements (OCR, reasoning) did not yield high accuracy on OlympiadBench; text-only base LLM shows modest gains over the multimodal model, consistent with the paper's observation that multimodal training can sometimes reduce text-only performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Despite claims of improved reasoning/OCR, LLaVA-NeXT-34B scored low on OlympiadBench; single-image input limitation impaired handling of multi-image or progressive physics problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepseekmath: Pushing the limits of mathematical reasoning in open language models <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 2)</em></li>
                <li>GPT-4V(ision) system card <em>(Rating: 2)</em></li>
                <li>Qwen-vl: A frontier large vision-language model with versatile abilities <em>(Rating: 2)</em></li>
                <li>Gemini: A family of highly capable multimodal models <em>(Rating: 2)</em></li>
                <li>Solving olympiad geometry without human demonstrations <em>(Rating: 2)</em></li>
                <li>Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts <em>(Rating: 1)</em></li>
                <li>Learn to explain: Multimodal reasoning via thought chains for science question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3321",
    "paper_id": "paper-bcf2c7e3f4ed64c8294c35a59220a26dd4f40060",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V (GPT-4Vision)",
            "brief_description": "A closed-source multimodal variant of GPT-4 that accepts images and text and is evaluated on OlympiadBench for multimodal scientific reasoning; used in experiments in this paper and analyzed for error modes (hallucination, calculation, conceptual confusion).",
            "citation_title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
            "mention_or_use": "use",
            "model_name": "GPT-4V",
            "model_description": "Multimodal GPT-4 variant (closed-source) that accepts image + text inputs; evaluated via API in zero-shot setting using a standardized prompt template; shows capabilities in visual reasoning but exhibits hallucinations, algebraic simplification errors, and classification-discussion failures.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot step-by-step prompting (explicit answer formatting)",
                "visual (multimodal) grounding / image understanding",
                "implicit stepwise natural-language reasoning (free-form)",
                "symbolic verification via external scoring (SymPy used by authors, not the model)"
            ],
            "reasoning_methods_description": "Evaluated in a zero-shot setting using a unified prompt template that enforces stepwise solutions and explicit final-answer formatting; relies on its internal chain of reasoning in natural language when given images (multimodal); the paper uses SymPy-based automatic scoring to check symbolic/numeric outputs externally. No special chain-of-thought prompt was applied to GPT-4V in the study.",
            "diversity_of_methods": "Similar style: primarily zero-shot step-by-step natural-language reasoning with multimodal inputs; does not use specialized diverse prompting methods in the experiments (except typical stepwise prompting enforced by template).",
            "reasoning_task_name": "OlympiadBench (bilingual multimodal Olympiad-level math & physics benchmark)",
            "reasoning_task_description": "8,476 open-ended math and physics problems (many with images), expert step-by-step solutions provided, evaluated via an automatic scoring pipeline for numeric and symbolic answers and manual sampling for theorem proofs.",
            "performance_by_method": "Reported performance (zero-shot multimodal evaluation using the paper's prompt template): Math average 21.70% (breakdown: En_COMP 27.18%, Zh_COMP 14.87%, Zh_CEE 21.27%), Physics average 10.74% (En_COMP 11.42%, Zh_CEE 10.45%), Overall average 17.97%. In text-only experiments (using text-only inputs), GPT-4V achieved higher scores: overall 29.07%, Math avg 31.01%, Physics avg 16.24%.",
            "comparison_of_methods": "Compared to GPT-4 (text-only), GPT-4V (multimodal) performs better on long-context physics problems (long-PhO subset: GPT-4V 18.47% vs GPT-4 14.92%) but worse on normal short-context physics problems (GPT-4V 1.35% vs GPT-4 4.05%), indicating multimodal training may improve long-context reasoning but not uniformly. Multimodal evaluation (with images) is much harder; text-only runs of GPT-4V show improved absolute scores versus multimodal runs.",
            "key_findings": "GPT-4V, despite being the best model tested, attains low absolute accuracy on OlympiadBench (17.97% multimodal); uses stepwise natural-language reasoning with multimodal inputs constrained by the zero-shot prompt; multimodal training appears to modify long-context reasoning capabilities; common failure modes include hallucination, algebraic manipulation errors, insufficient case analysis, and underuse of image details.",
            "counter_examples_or_negative_results": "Multimodal training does not always yield better text-only performance (paper notes multimodal training slightly hurts text-only results in some splits). GPT-4V often hallucinated or used incorrect theorems; success on simple answers with incorrect processes (i.e., correct final answer despite wrong steps) shows that diverse internal reasoning does not guarantee correct reasoning trace.",
            "uuid": "e3321.0",
            "source_info": {
                "paper_title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (text-only variant)",
            "brief_description": "Text-only GPT-4 (closed-source) evaluated on OlympiadBench text-only questions as a baseline to compare with the multimodal GPT-4V, showing similar high-level capabilities in text-only settings and differing behavior on long-context physics problems.",
            "citation_title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Closed-source large language model (text-only interface) used via API; evaluated zero-shot with the same prompt template (without images) to compare against GPT-4V's multimodal performance.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot step-by-step prompting (explicit answer formatting)",
                "text-only internal natural-language reasoning (chain-of-thought-like behavior emerges but not explicitly prompted in this study)"
            ],
            "reasoning_methods_description": "Evaluated in zero-shot on text-only inputs with the same prompt template that enforces stepwise outputs and final-answer formatting; no special CoT prompt applied for GPT-4 in main experiments.",
            "diversity_of_methods": "Similar style: zero-shot stepwise textual reasoning; no explicit diverse prompting methods applied in experiments.",
            "reasoning_task_name": "OlympiadBench (text-only subset)",
            "reasoning_task_description": "Subset of OlympiadBench questions that are text-only (no images), evaluated with the same automatic scoring pipeline.",
            "performance_by_method": "Text-only experiment results: GPT-4 overall average 29.93%, Math avg 32.00%, Physics avg 16.24% (breakdowns: En_COMP Math 30.42%, Zh_COMP Math 16.42%, Zh_CEE Math 37.98%; Physics En_COMP 12.29%, Zh_CEE 24.35%).",
            "comparison_of_methods": "Compared to GPT-4V, GPT-4 (text-only) slightly outperforms GPT-4V on normal short-context physics problems but underperforms on long-context (long-PhO) problems; overall text-only scores are higher than multimodal-evaluation GPT-4V scores, indicating differences introduced by multimodal training.",
            "key_findings": "Text-only GPT-4 attains substantially higher accuracy on text-only questions than GPT-4V's multimodal evaluation, highlighting that multimodal model evaluation with images increases difficulty; GPT-4 and GPT-4V differences suggest multimodal training alters long-context reasoning.",
            "counter_examples_or_negative_results": "GPT-4's advantage is not universal: GPT-4V surpasses GPT-4 on long-context physics subsets, showing that neither text-only nor multimodal alone guarantees dominance across all reasoning styles.",
            "uuid": "e3321.1",
            "source_info": {
                "paper_title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "DeepSeekMath-7B-RL",
            "name_full": "DeepSeekMath-7B-RL",
            "brief_description": "An open-source 7B model specialized in mathematical reasoning, pre-trained on math-related corpora and fine-tuned with reinforcement learning to improve chain-of-thought style reasoning, used here as a strong baseline on text-only OlympiadBench problems.",
            "citation_title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
            "mention_or_use": "use",
            "model_name": "DeepSeekMath-7B-RL",
            "model_description": "Open-source 7B language model pre-trained with ~120B math-related data and RL-based tuning to enhance chain-of-thought (CoT) reasoning; requires an explicit chain-of-thought prompt appended during evaluation as per the paper.",
            "model_size": "7B",
            "reasoning_methods": [
                "explicit chain-of-thought prompting",
                "RL fine-tuning to improve CoT reasoning",
                "zero-shot with dedicated CoT prompt (for evaluation)"
            ],
            "reasoning_methods_description": "Model was pre-trained on large math-specific corpora and further trained via reinforcement learning to encourage high-quality chain-of-thought reasoning; the authors appended a specific chain-of-thought prompt for DeepSeekMath during evaluation to trigger its CoT behavior.",
            "diversity_of_methods": "Diverse relative to other tested models: explicitly trained and prompted for chain-of-thought reasoning (distinct from the zero-shot stepwise prompts used for other LMMs/LLMs in this paper).",
            "reasoning_task_name": "OlympiadBench (text-only evaluation)",
            "reasoning_task_description": "Text-only subset of OlympiadBench; DeepSeekMath evaluated with its required CoT prompt in zero-shot to leverage its RL-enhanced chain-of-thought capability.",
            "performance_by_method": "Text-only experiment results: overall average 17.02%, Math avg 18.09% (En_COMP 19.44%, Zh_COMP 2.70%, Zh_CEE 22.42%), Physics avg 9.97% (En_COMP 6.78%, Zh_CEE 16.52%). In the paper DeepSeekMath-7B-RL outperforms or matches some larger closed-source LMMs on text-only math problems.",
            "comparison_of_methods": "Compared to LMMs and general LLMs, DeepSeekMath's specialized CoT training gives it strong text-only math performance relative to its size; the paper highlights that DeepSeekMath (despite smaller size) outperforms or is on par with Gemini-ProVision and Qwen-VL-Max on text-only parts, illustrating that targeted CoT training can substitute for mere model scale in some reasoning tasks.",
            "key_findings": "Specialized chain-of-thought training (via RL) plus explicit CoT prompting yields competitive performance on difficult math benchmarks even at modest parameter sizes; demonstrates that using a distinct reasoning method (CoT with RL) can close gaps versus larger generalist models.",
            "counter_examples_or_negative_results": "Despite CoT-focused training, DeepSeekMath's overall accuracy remains low on OlympiadBench (17.02% text-only), showing CoT alone does not solve the benchmark. Its performance is uneven across languages and subsets (e.g., low Zh_COMP score), indicating limits to specialization and prompting.",
            "uuid": "e3321.2",
            "source_info": {
                "paper_title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Qwen-VL-Max",
            "name_full": "Qwen-VL-Max",
            "brief_description": "A large closed-source vision-language model (from Alibaba) trained on large bilingual image-text data; evaluated as a strong multilingual LMM baseline with advantages on Chinese problems in this paper.",
            "citation_title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
            "mention_or_use": "use",
            "model_name": "Qwen-VL-Max",
            "model_description": "Closed-source large vision-language model trained on multilingual image-text pairs; evaluated both multimodally and in text-only mode (text-model counterpart) in the paper. Noted to have relative strength on Chinese due to training data composition.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot step-by-step prompting",
                "multimodal visual-text reasoning (vision + language)",
                "bilingual image-text pretraining (affects reasoning in Chinese)"
            ],
            "reasoning_methods_description": "Relies on multimodal pretraining and zero-shot prompting with the standardized prompt template; no explicit chain-of-thought prompting used in the experiments. The modelâ€™s bilingual visual-text pretraining biases it toward better Chinese multimodal performance according to the authors.",
            "diversity_of_methods": "Similar style: zero-shot multimodal reasoning; no specialized CoT prompting applied in experiments (except text-model counterpart runs for text-only).",
            "reasoning_task_name": "OlympiadBench (multimodal & text-only comparisons)",
            "reasoning_task_description": "Evaluated on both the multimodal OlympiadBench and the text-only subset to compare effect of multimodal training and bilingual data.",
            "performance_by_method": "Multimodal block: Overall avg 10.09% (Math avg 12.65%, Physics avg 5.09%). Text-only block: Overall avg 18.27% (Math avg 19.70%, Physics avg 8.83%). Notably stronger on Chinese math subsets (e.g., Zh_COMP and Zh_CEE) in multimodal runs.",
            "comparison_of_methods": "Qwen-VL-Max performs substantially better in text-only runs than in multimodal runs on OlympiadBench (18.27% vs 10.09% overall), supporting the paper's observation that multimodal training can slightly hurt text-only performance; it performs better on Chinese-language subsets compared to English, consistent with its training data mix.",
            "key_findings": "Multimodal LMMs trained on bilingual data (like Qwen-VL-Max) show relative strength on Chinese multimodal problems but overall lower accuracy on this difficult benchmark; performance improves in text-only mode, indicating that modality and input formatting materially affect reasoning results.",
            "counter_examples_or_negative_results": "Qwen-VL-Max's multimodal performance is notably lower than some text-only results (i.e., text-only run outperforms multimodal), showing that more modalities do not necessarily produce better performance on complex reasoning benchmarks.",
            "uuid": "e3321.3",
            "source_info": {
                "paper_title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Gemini-Pro-Vision",
            "name_full": "Gemini-Pro-Vision",
            "brief_description": "A closed-source multimodal model in the Gemini family (Google/DeepMind); evaluated as a second-tier closed-source LMM baseline in the paper and found to underperform the top closed-source model on OlympiadBench.",
            "citation_title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
            "mention_or_use": "use",
            "model_name": "Gemini-Pro-Vision",
            "model_description": "A member of the Gemini family of multimodal models (closed-source); used via the accessible Gemini-Pro API in experiments in place of unavailable Gemini-Ultra. Evaluated zero-shot with the shared prompt template.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot multimodal stepwise prompting",
                "multimodal pretraining (vision + language)"
            ],
            "reasoning_methods_description": "Applied in the paper via zero-shot prompts for stepwise solutions; relied upon multimodal pretraining for image understanding but no specialized CoT prompting was used.",
            "diversity_of_methods": "Similar style: standard zero-shot multimodal reasoning; no distinct or specialized chain-of-thought prompting in experiments.",
            "reasoning_task_name": "OlympiadBench (multimodal & text-only comparisons)",
            "reasoning_task_description": "Evaluated on both multimodal and text-only subsets (text-only used the corresponding text-model API) to contrast modality effects on reasoning.",
            "performance_by_method": "Multimodal block: Overall avg 4.22% (Math avg 5.14%, Physics avg 2.45%). Text-only block: Overall avg 7.34% (Math avg 7.63%, Physics avg 5.41%).",
            "comparison_of_methods": "Gemini-Pro-Vision's performance is substantially lower than GPT-4V and Qwen-VL-Max on OlympiadBench; the paper notes Gemini-ProVision (second-tier closed-source) is 'much less compatible' with extremely hard tasks like OlympiadBench despite being a multimodal model.",
            "key_findings": "Being a multimodal closed-source model does not guarantee superior performance on extremely challenging reasoning benchmarks; Gemini-Pro-Vision underperforms both larger closed-source and some open-source specialized models on OlympiadBench.",
            "counter_examples_or_negative_results": "Despite being a recent multimodal model, Gemini-Pro-Vision scored only slightly above some open-source models and well below GPT-4V, indicating that model family/scale and training regimen matter more than mere multimodality for these tasks.",
            "uuid": "e3321.4",
            "source_info": {
                "paper_title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Yi-VL-34B",
            "name_full": "Yi-VL-34B",
            "brief_description": "An open-source 34B multimodal model trained with Chinese image-text pairs, evaluated on OlympiadBench and noted for better Chinese performance due to its training data.",
            "citation_title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
            "mention_or_use": "use",
            "model_name": "Yi-VL-34B",
            "model_description": "Open-source 34B vision-language model trained on bilingual image-text data (with significant Chinese content); evaluated multimodally (single image limit noted) and in text-only mode via its text-model counterpart (Yi-34B-Chat).",
            "model_size": "34B",
            "reasoning_methods": [
                "zero-shot multimodal stepwise prompting",
                "bilingual visual-text pretraining"
            ],
            "reasoning_methods_description": "Evaluated using the paper's zero-shot prompt template; its bilingual visual-text training gives it relative strengths on Chinese multimodal tasks; accepts only single-image input in tested checkpoint.",
            "diversity_of_methods": "Similar style: zero-shot multimodal reasoning (no specialized CoT prompting in experiments); limited by single-image input for some problems.",
            "reasoning_task_name": "OlympiadBench (multimodal & text-only comparisons)",
            "reasoning_task_description": "Evaluated on multimodal OlympiadBench and text-only subsets to analyze bilingual and multimodal effects.",
            "performance_by_method": "Multimodal block: Overall avg 3.42% (Math avg 4.23%, Physics avg 1.46%). Text-only block: Overall avg 5.72% (Math avg 6.24%, Physics avg 2.28%).",
            "comparison_of_methods": "Yi-VL-34B shows better relative performance on Chinese questions due to training data composition but overall performance remains low versus top closed-source models; text-only runs modestly outperform multimodal runs.",
            "key_findings": "Open-source large multimodal models with bilingual pretraining can show language-specific advantages, but still achieve low absolute accuracy on Olympiad-level reasoning; modality constraints (single-image input) limit handling of multi-image progressive physics problems.",
            "counter_examples_or_negative_results": "Single-image input limitation and generally low scores show that being large and bilingual is not sufficient to solve OlympiadBench; multimodal capability did not translate to high performance here.",
            "uuid": "e3321.5",
            "source_info": {
                "paper_title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaVA-NeXT-34B",
            "name_full": "LLaVA-NeXT-34B (and its base LLM Nous-Hermes-2-Yi-34B used for text-only)",
            "brief_description": "An open-source 34B multimodal model claiming improvements in reasoning, OCR, and world knowledge; evaluated multimodally and via its base LLM for text-only questions in this paper.",
            "citation_title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
            "mention_or_use": "use",
            "model_name": "LLaVA-NeXT-34B (multimodal) / Nous-Hermes-2-Yi-34B (text base)",
            "model_description": "Open-source 34B multimodal model with reported improvements in reasoning and OCR; in this paper the multimodal checkpoint (limited to single-image input) is used for multimodal evaluation, while its base LLM Nous-Hermes-2-Yi-34B was used for text-only evaluation.",
            "model_size": "34B",
            "reasoning_methods": [
                "zero-shot multimodal stepwise prompting",
                "improved OCR and world-knowledge integration (claimed)",
                "text-only evaluation via base LLM"
            ],
            "reasoning_methods_description": "Applied in zero-shot with the unified prompt; multimodal checkpoint had single-image input constraint and was compared to text-only base LLM runs to measure modality effects.",
            "diversity_of_methods": "Similar style: zero-shot multimodal/text-only reasoning; no specialized CoT prompting in the reported evaluations.",
            "reasoning_task_name": "OlympiadBench (multimodal & text-only comparisons)",
            "reasoning_task_description": "Evaluated on multimodal OlympiadBench and a text-only subset (base LLM) to compare multimodal capabilities versus text-only reasoning.",
            "performance_by_method": "Multimodal block: Overall avg 3.65% (Math avg 4.30%, Physics avg 2.08%). Text-only block: Overall avg 5.87% (Math avg 6.29%, Physics avg 3.13%).",
            "comparison_of_methods": "Text-only runs of the base LLM improve performance over the multimodal checkpoint in this evaluation; the single-image constraint and multimodal overhead may reduce effectiveness on large multimodal scientific problems.",
            "key_findings": "LLaVA-NeXT-34B's claimed improvements (OCR, reasoning) did not yield high accuracy on OlympiadBench; text-only base LLM shows modest gains over the multimodal model, consistent with the paper's observation that multimodal training can sometimes reduce text-only performance.",
            "counter_examples_or_negative_results": "Despite claims of improved reasoning/OCR, LLaVA-NeXT-34B scored low on OlympiadBench; single-image input limitation impaired handling of multi-image or progressive physics problems.",
            "uuid": "e3321.6",
            "source_info": {
                "paper_title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "GPT-4V(ision) system card",
            "rating": 2
        },
        {
            "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities",
            "rating": 2
        },
        {
            "paper_title": "Gemini: A family of highly capable multimodal models",
            "rating": 2
        },
        {
            "paper_title": "Solving olympiad geometry without human demonstrations",
            "rating": 2
        },
        {
            "paper_title": "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts",
            "rating": 1
        },
        {
            "paper_title": "Learn to explain: Multimodal reasoning via thought chains for science question answering",
            "rating": 1
        }
    ],
    "cost": 0.018967499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems</h1>
<p>Chaoqun $\mathbf{H e}^{1}$, Renjie Luo ${ }^{2}$, Yuzhuo Bai ${ }^{1}$, Shengding $\mathbf{H u}^{1}$, Zhen Leng Thai ${ }^{1}$<br>Junhao Shen ${ }^{1}$, Jinyi $\mathbf{H u}^{1}$, Xu Han ${ }^{1 <em>}$, Yujie Huang ${ }^{1}$, Yuxiang Zhang ${ }^{3}$<br>Jie Liu ${ }^{3}$, Lei Qi ${ }^{3}$, Zhiyuan Liu ${ }^{1 </em>}$, Maosong Sun ${ }^{1}$<br>${ }^{1}$ Dept. of Comp. Sci. \&amp; Tech., Institute for AI, Tsinghua University, Beijing, China<br>${ }^{2}$ Institute of Artificial Intelligence, Beihang University, China<br>${ }^{3}$ Wisdom Way AI Lab, China<br>{hcq21, byz22}@mails.tsinghua.edu.cn, renjie.luo@outlook.com, {hanxu2022,liuzy}@tsinghua.edu.cn</p>
<h4>Abstract</h4>
<p>Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of $17.97 \%$ on OlympiadBench, with a mere $10.74 \%$ in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT4 V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at https: //github.com/OpenBMB/OlympiadBench</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks such as text generation (Zhao et al., 2023), code generation (Zan et al., 2023) and mathematical reasoning (Lu et al., 2023; Zhou et al., 2023), garnering significant attention from both academia and industry (Wei et al., 2022; Zhao et al., 2023; Bubeck et al., 2023). The most powerful models</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Question: Find all triples $(x, y, z)$ of positive integers such that $x \leq y \leq$ $z$ and $x^{2}\left(y^{2}+z^{2}\right)=2012(x y z+2)$.
Solution: First note that $z$ divides $2012 \cdot 2=2^{3} \cdot 503$. If 503 | $z$ then the right-hand side of the equation is divisible by $503^{3}$, and it follows that $503^{2} \mid x y z+2$. This is false as 503 | $x$. Hence $x=2^{m}$ with $m \in{0,1,2,3}$. If $m \geq 2$ then $2^{n} \mid 2012(x y z+2)$. However the highest powers of 2 dividing 2012 and $x y z+2=$ $2^{m} y z+2$ are $2^{3}$ and $2^{2}$ respectively. So $x=1$ or $x=2$, yielding the two equations</p>
<p>$$
\begin{gathered}
y^{3}+z^{3}=2012(y z+2) \
y^{3}+z^{3}=503(y z+1)
\end{gathered}
$$</p>
<p>In both cases ...... It follows that $y \equiv-x(\bmod 503)$ as claimed. Therefore $y+z=503 k$ with $k \geq 1$. In view of $y^{3}+$ $z^{3}=(y+z)((y-z)^{3}+y z)$ the two equations take the form</p>
<p>$$
\begin{gathered}
k(y-z)^{3}+(k-4) y z=8 \
k(y-z)^{2}+(k-1) y z=1
\end{gathered}
$$</p>
<p>In (1) we have $(k-4) y z \leq 8$, which implies $k \leq 4 \ldots \ldots$ Therefore (1) has no integer solutions.
Equation (2) implies $0 \leq(k-1) y z \leq 1$, so that $k=1$ or $k=2$. Also $0 \leq k(y-z)^{2} \leq 1$, hence $k=2$ only if $y=z$. However then $y=z=1$, which is false in view of $y+z \geq 503$. Therefore $k=1$ and (2) takes the form $(y-z)^{2}=1$, yielding $z-y=(y-z)=1$. Continued with $k=1$ and $y+z=503 k$, this leads to $y=251, z=252$.
In summary the triple $(2,251,252)$ is the only solution.
Final answer: $(2,251,252)$
Subfield: Number theory
Answer type: Triple
Question type: Open-ended
Figure 1: An example of IMO in OlympiadBench. Solving this example requires AI systems to span different mathematical domains and conduct advanced reasoning.
such as GPT-4 (OpenAI, 2023a) and Gemini Ultra (Team, 2023) have even surpassed oridinary human level on a wide variety of benchmarks such as MMLU (Hendrycks et al., 2020), MMMU (Yue et al., 2023), and even surpassing human expert in many area. These results show a promising future that LLMs can serve as proficient assistants for human scientists (Nguyen, 2023; Qiu et al., 2023). Among the array of expert-level skills exhibited by LLMs, scientific reasoning consistently emerges as one of the most brilliant, showcasing some of the most distinguished intellectual properties that experts possess. Therefore, this paper primarily focuses on mathematical and physical reasoning.</p>
<p>In recent years, several benchmarks related to mathematics have been proposed, such as the</p>
<p>dataset GSM8K (Cobbe et al., 2021) as well as the dataset MATH (Hendrycks et al., 2021). However, these benchmarks, are primarily developed before the advent of highly capable LLMs, and now lack sufficient challenge for the latest models. For instance, GPT-4 with prompting techniques(Zhou et al., 2023) has achieved a 97.0% success rate on GSM8K and 84.3% on MATH. The rapid evolution of LLMs may soon lead to saturated results on these benchmarks. Concurrently, LLMs are not yet fully equipped to assist mathematicians in solving complex problems (Collins et al., 2023; Zhang et al., 2023), nor are they capable of performing expert-level mathematical reasoning independently. This discrepancy underscores the need for more challenging datasets to benchmark future advancements of LLMs in this domain. Similarly, physics presents comparable challenges for AI to those found in mathematics. Nevertheless, existing benchmarks related to physics (Lu et al., 2022; Arora et al., 2023; Wang et al., 2024) are characterized by their relatively low difficulty and limited scope. There is also a significant lack of a rigorous and challenging benchmark in physics.</p>
<p>In addition to the issue regarding the benchmark difficulty, it is important to note that these benchmarks predominantly focus on text. This presents a significant limitation, as a wide range of scientific reasoning contexts require multimodal reasoning abilities. For example, grasping geometry reasoning in mathematics or understanding experiments designs in physics are scenarios where multimodal reasoning capabilities are crucial. Notably, various large multimodal models (LMMs) have been developed (Team, 2023; Liu et al., 2023) and demonstrate proficiency on a variety of tasks (Lu et al., 2022; Yue et al., 2023; Zhang et al., 2024b; Lu et al., 2024), offering the potential for multimodal scientific reasoning. Nevertheless, there is still a lack of sufficient benchmarks to prove whether these LMMs are capable of handling scientific problems. Consequently, a challenging multimodal benchmark is essential for advancing scientific reasoning tasks(Zhang et al., 2024a; Lu et al., 2023).</p>
<p>To address the aforementioned inadequacies, we introduce OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark. This collection comprises 8,476 math and physics problems sourced from International Olympiads, Chinese Olympiads, and the most challenging segments of the Chinese College Entrance Exam (GaoKao). We download PDF data from official websites and utilize Mathpix for OCR parsing. We meticulously inspect, clean, and revise the data, and further adopt LLMs for deduplication. Finally, we annotate the data with crucial information such as answer types and subfields, yielding a dataset that is clean, accurate, and detailed. As shown in Figure 1, OlympiadBench features numerous distinct characteristics such as difficulty, free-form generation, expert-level solution annotation, detailed labeling of difficulty, wide-coverage of modality and language, etc. These features are summarized more clearly from Table 1.</p>
<p>We conduct an evaluation of current state-of-the-art LLMs and LMMs on the OlympiadBench. The best-performing model, GPT-4V, is a multimodal version of GPT-4 developed by OpenAI that can understand images. Despite its advanced capabilities, GPT-4V achieves a score of only 17.97% on OlympiadBench, with individual scores of 21.70% in mathematics and 10.74% in physics.</p>
<p>Importantly, the experiment results show that LMMs still struggle in computational error, incorrect reasoning or induction. For the process involved in the correct responses, the process occasionally includes hallucinated reasoning, or choosing a more complex solution when a simpler solution exists. All these results highlight the substantial challenge OlympiadBench presents to contemporary large models and point the direction of future efforts.</p>
<p>OlympiadBench is inspired by the significant advances made by DeepMind AlphaGeometry (Trinh et al., 2024), which nearly matches the proficiency of International Mathematical Olympiad (IMO) gold medalists in geometry proofs. It is clear that OlympiadBench, along with other challenging datasets like the AI-MO challenge, will witness and benchmark the swift progress towards expert-level AI assistants for solving scientific problems.</p>
<h2>2 Related Work</h2>
<p>This section gives an overview of the existing datasets in solving mathematics and physics problems as well as multimodal datasets.</p>
<p>Mathematics Benchmarks. Solving mathematics problems and proving theorems in natural languages has been a key research focus in machine learning and natural language processing since the 1960s (Bobrow et al., 1964). Previous bench-</p>
<p>https://mathpix.com/
https://aimoprize.com/</p>
<p>marks (Koncel-Kedziorski et al., 2016; Wang et al., 2017; Ling et al., 2017; Amini et al., 2019; Cobbe et al., 2021; Wei et al., 2023) focus predominantly on math word problems (WMPs) which involve four basic arithmetic operations with single or multiple operation steps (Lu et al., 2023). Typically, the GSM8K (Cobbe et al., 2021) dataset targets elementary-level questions within 8 steps of basic arithmetic operations. However, these problems are typically text-only (Lu et al., 2023) and of lower difficulty, with reasoning limited to a few computations. As the complexity of the problems rises, some works (Hendrycks et al., 2021; Frieder et al., 2023; Arora et al., 2023) introduce competitionlevel problems integrating mathematical logic and background knowledge. Yet, these challenging datasets are increasingly being surmounted (Zhou et al., 2023). Theorem proving is a problem to demonstrate the truth of a mathematical claim (a theorem) through a sequence of logical arguments (a proof) (Lu et al., 2023). Earlier efforts mainly focused on translating natural language proofs into formal representations, facing significant expertise and labor challenges (Zheng et al., 2022; Welleck et al., 2021). The emergence of LLMs has facilitated notable advancements in the domain of natural language proof (Jiang et al., 2023). OlympiadBench presents mathematical reasoning and theoretical proofs all in natural language with detailed solution annotations.</p>
<p>Physics Benchmarks. Physics questions in SciQ (Welbl et al., 2017), ScienceQA (Lu et al., 2022) and E-EVAL (Hou et al., 2024) are mainly elementary and high school level multiple-choice questions, lacking complex reasoning and computational tasks. In MMLU-STEM (Hendrycks et al., 2020) and C-Eval-STEM (Huang et al., 2023), physics questions also adopt a multiple-choice format. JEEBench (Arora et al., 2023) extends this format to include multistep reasoning with physics knowledge, yet it is limited in scope and purely text-only. TheoremQA (Chen et al., 2023) is the first theorem-driven question-answering dataset. Curated by domain experts, it contains 800 highquality questions that encompass 350 theorems from Mathematics, Physics, Electrical Engineering and Computer Science (EE\&amp;CS), and Finance. SciEval (Sun et al., 2023) consists of a total of about 18,000 challenging scientific questions, spanning three important basic science fields: chemistry, physics and biology. SciBench (Wang et al., 2024) and OCWCourses (Lewkowycz et al., 2022) of-
fer college-level physics questions in free-response formats, where SciBench contains multimodal information. In contrast, OlympiadBench escalates in difficulty, diversifies in question types, and surpasses in volume, setting a new benchmark for complexity and variety in the domain.</p>
<p>Multimodal Benchmarks. For assessing multimodal capability, works such as Geometry3K (Lu et al., 2021), GeoQA (Chen et al., 2021), GeoQA+ (Cao and Xiao, 2022), and UniGeo (Chen et al., 2022) have employed multimodal information for tackling geometric problems, integrating natural language descriptions with diagrams. ScienceQA (Lu et al., 2022), MMMU (Yue et al., 2023), CMMMU (Zhang et al., 2024b) and CMMU (He et al., 2024) are multimodal, multidiscipline evaluation sets, encompassing a broad range of subjects. MathVista (Lu et al., 2024) integrates 28 existing and 3 newly constructed multimodal datasets involving mathematics, aiming to establish a benchmark that encapsulates challenges from a variety of mathematical and visual tasks. However, it does not concentrate on delving into the complexity of mathematics problems.</p>
<p>In summary, we introduce a new benchmark to address these gaps. Table 1 presents a comparison between OlympiadBench and several related benchmarks, highlighting the significant advantages of OlympiadBench across all aspects.</p>
<h2>3 The OlympiadBench Dataset</h2>
<p>To evaluate the reasoning abilities of LLMs and LMMs in mathematics and physics problems, we have created OlympiadBench, a bilingual and multimodal scientific benchmark at the competition level. This section provides a detailed account of the construction process of OlympiadBench. Summarized statistics of the dataset is shown in Table 2, and more detailed statistics are in Appendix A.2.</p>
<h3>3.1 Design Principle</h3>
<p>The motivation behind OlympiadBench is to establish a benchmark that represents the pinnacle of human intellectual achievement, thereby encouraging researchers of large models to push the boundaries of mathematical and physical reasoning capabilities. We focus on curating challenges that epitomize the highest level of competition worldwide. Specifically, OlympiadBench includes:</p>
<ol>
<li>Inclusion of Olympiad-Level Problems. We collect mathematics and physics problems</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">Subject</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Multi- <br> modal</th>
<th style="text-align: center;">Detailed <br> solution</th>
<th style="text-align: center;">Difficulty <br> level</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Answer <br> type</th>
<th style="text-align: center;">Language <br> type</th>
<th style="text-align: center;">Question <br> type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Maths</td>
<td style="text-align: center;">Physics</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Maths</td>
<td style="text-align: center;">Physics</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SciBench</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">COL</td>
<td style="text-align: center;">217</td>
<td style="text-align: center;">295</td>
<td style="text-align: center;">Num</td>
<td style="text-align: center;">EN</td>
<td style="text-align: center;">OE</td>
</tr>
<tr>
<td style="text-align: center;">MMMU</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">COL</td>
<td style="text-align: center;">540</td>
<td style="text-align: center;">443</td>
<td style="text-align: center;">Num</td>
<td style="text-align: center;">EN</td>
<td style="text-align: center;">MC,OE</td>
</tr>
<tr>
<td style="text-align: center;">MathVista</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Num</td>
<td style="text-align: center;">EN</td>
<td style="text-align: center;">MC,OE</td>
</tr>
<tr>
<td style="text-align: center;">ScienceQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">H</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">617</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">EN</td>
<td style="text-align: center;">MC</td>
</tr>
<tr>
<td style="text-align: center;">SciEval</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1,657</td>
<td style="text-align: center;">Num</td>
<td style="text-align: center;">EN</td>
<td style="text-align: center;">MC,FB,J</td>
</tr>
<tr>
<td style="text-align: center;">JEEBench</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">CEE</td>
<td style="text-align: center;">236</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">Num</td>
<td style="text-align: center;">EN</td>
<td style="text-align: center;">MC,OE</td>
</tr>
<tr>
<td style="text-align: center;">MMLU</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">COL</td>
<td style="text-align: center;">948</td>
<td style="text-align: center;">548</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">EN</td>
<td style="text-align: center;">MC</td>
</tr>
<tr>
<td style="text-align: center;">AGIEval</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CEE</td>
<td style="text-align: center;">953</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">Num</td>
<td style="text-align: center;">EN,ZH</td>
<td style="text-align: center;">MC,FB,OE</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">1,319</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Num</td>
<td style="text-align: center;">EN</td>
<td style="text-align: center;">OE</td>
</tr>
<tr>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">COMP</td>
<td style="text-align: center;">5,000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Num,Exp,Tup</td>
<td style="text-align: center;">EN</td>
<td style="text-align: center;">OE</td>
</tr>
<tr>
<td style="text-align: center;">OlympiadBench</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">COMP</td>
<td style="text-align: center;">6,142</td>
<td style="text-align: center;">2,334</td>
<td style="text-align: center;">ALL</td>
<td style="text-align: center;">EN,ZH</td>
<td style="text-align: center;">OE</td>
</tr>
</tbody>
</table>
<p>Table 1: For difficulty level, COMP: Competition, COL: College, CEE: College Entrance Examination, H: High School, E: Elementary School, and we picked the highest level; For answer type, Num: Numeric value, Exp: Expression, Equ: Equation, Int: Interval, Tup: Tuple; For language type, EN: English, ZH: Chinese; For question type, OE: Open-ended, MC: Multiple-choice, FB: Fill-in-the-blank, J: Judgement. For the statistical analysis of quantity and relevant metrics in AGIEval, we exclude 1,000 questions from the MATH benchmark to facilitate a more accurate comparison. The "-" indicates that it cannot be confirmed. Upon comparison, OlympiadBench leads in all aspects.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Statistics</th>
<th style="text-align: right;">Number</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Total Problems</td>
<td style="text-align: right;">8,476</td>
</tr>
<tr>
<td style="text-align: left;">* Problems with images</td>
<td style="text-align: right;">$4,869(57 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">* Problems with solutions</td>
<td style="text-align: right;">$8,476(100 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Difficulties (CEE: COMP)</td>
<td style="text-align: right;">$66 \%: 34 \%$</td>
</tr>
<tr>
<td style="text-align: left;">EN: ZH</td>
<td style="text-align: right;">$2,125: 6,351$</td>
</tr>
<tr>
<td style="text-align: left;">Open-ended Questions</td>
<td style="text-align: right;">$6,728(79 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Theorem Proving</td>
<td style="text-align: right;">$1,748(21 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Math: Physics</td>
<td style="text-align: right;">$6,142: 2,334$</td>
</tr>
<tr>
<td style="text-align: left;">* Maths with images</td>
<td style="text-align: right;">2,911</td>
</tr>
<tr>
<td style="text-align: left;">* Physics with images</td>
<td style="text-align: right;">1,958</td>
</tr>
<tr>
<td style="text-align: left;">Average question tokens</td>
<td style="text-align: right;">253</td>
</tr>
<tr>
<td style="text-align: left;">Max question tokens</td>
<td style="text-align: right;">3,745</td>
</tr>
<tr>
<td style="text-align: left;">Average solution tokens</td>
<td style="text-align: right;">347</td>
</tr>
<tr>
<td style="text-align: left;">Max solution tokens</td>
<td style="text-align: right;">4,223</td>
</tr>
</tbody>
</table>
<p>Table 2: Statistics of OlympiadBench. When calculating tokens, images are not included.
from the International Olympiad competitions, which cater to the most outstanding high school students in a region. These problems are open-ended, differing from traditional multiple-choice or fill-in-the-blank formats. This selection aims to more accurately reflect the complexity of advanced scientific reasoning, providing insight into the actual reasoning process of the models.
2. Provision of Detailed Solutions. Given the advanced difficulty of these problems, which may exceed the comprehension of individuals
without a specialized background in mathematics, each problem is accompanied by expertly crafted solutions that detail the reasoning steps involved. This approach can not only reduces the difficulty of annotation and evaluation but also enhances the accuracy of the solutions provided. Furthermore, detailed expertlevel solutions are valuable for research in model reasoning.
3. Incorporation of Visuals. Recognizing the crucial role of visual information in conveying complex ideas, OlympiadBench incorporates problems that require understanding images, identifying spatial relationships, and other advanced reasoning tasks. This inclusion aims to assess the model's capabilities in interpreting visual data as part of its reasoning process.
4. Minimization of Data Leakage Risks. To minimize the risk of data leakage, we have sourced problems from official Olympiad competitions, converting them from their original PDF files provided by official websites to the markdown format required. This strategy is aimed at reducing the likelihood of the data being inadvertently incorporated into the pre-training corpora of models.</p>
<p>Through these carefully designed criteria, OlympiadBench aims to not only challenge but also significantly advance the capabilities of</p>
<p>models in mathematical and physical reasoning.</p>
<h3>3.2 Data Processing</h3>
<p>The data processing pipeline is structured into three distinct phases: data collection, format conversion $\&amp;$ deduplication, and classification labeling.</p>
<p>Data Collection. OlympiadBench is meticulously compiled from three primary sources: Global Mathematics and Physics Olympiad Problems, Regional and National Chinese Math Competitions, and Gaokao Mock Questions for Mathematics and Physics ${ }^{3}$. Each chosen for its distinct advantages in creating a robust and comprehensive benchmark for evaluating LLMs and LMMs in mathematical and scientific reasoning. Their challenges progressively increase in difficulty, not only distinguishing the reasoning capabilities of models of various sizes but also offering guidance on scaling laws <em>Kaplan et al. (2020)</em> for specialized models in these domains.</p>
<p>Format Conversion and Deduplication. After collecting all PDF files, we utilize the Mathpix tool for OCR recognition and convert them into markdown format. However, no conversion process is flawless, necessitating manual verification by our team members between the original PDF files and the converted Markdown texts. The Markdown texts are further structured into a format akin to "Problemâ€”Solutionâ€”Answer", employing its markup language for text organization. Subsequently, we leverage a specialized small-scale language model ${ }^{4}$ trained on mathematical symbol corpora for vectorizing the data and performing deduplication based on cosine similarity.</p>
<p>Classification Labeling. We note that both mathematics and physics problems predominantly comprise two types of questions: the open-ended problems and the theorem proving problems. We also note that the dataset, enriched by both Olympiad and national examination questions, covers a broad spectrum of subfields, as illustrated in Figure 2. Therefore, we manually annotate each question with topic and problem type annotations.</p>
<h3>3.3 Data Characteristics</h3>
<p>In contrast to previous benchmarks, OlympiadBench unveils two unique characteristics within its dataset: the incorporation of Progressive Prob-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Subfields Distribution of OlympiadBench</p>
<table>
<thead>
<tr>
<th>Answer type</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Numeric</td>
<td>$1 / 4$</td>
</tr>
<tr>
<td>Expression</td>
<td>$x=(1 / 2) a t^{2}$</td>
</tr>
<tr>
<td>Equation</td>
<td>$x^{2}+y^{2}=1$</td>
</tr>
<tr>
<td>Tuple</td>
<td>$(x, y, z)=(0,0,0)$</td>
</tr>
<tr>
<td>Interval</td>
<td>$(-\infty,-1) \cup(1,+\infty)$</td>
</tr>
</tbody>
</table>
<p>Table 3: Examples of the five answer types lems in Physics and the categorization of answers to most open-ended questions into a limited number of types.</p>
<p>Progressive Problems in Physics. In physics competitions such as the International Physics Olympiad (IPhO), problems are often structured around a common material or scenario, with subsequent questions potentially relying on the answers or information from previous questions. One example is given in Figure 10 and Figure 11. This design characteristic is commonly referred to as "progressive problems". By linking a series of questions together, progressive problems require participants to apply their knowledge and skills comprehensively to gradually solve more complex issues. This type of question design aims to test students' depth of understanding, application capabilities, and innovative thinking, rather than just basic knowledge. To better utilize this feature, we compile the material, questions, and their answers into the 'context' field for each set of progressive problems.</p>
<p>Answer Type Classification. Whether in mathematics or physics, the answers to problem requiring</p>
<p>definitive response can largely be categorized into the following types: numeric, expression, equation, interval, and tuple. Simple examples of these can be seen in Table 3.</p>
<h3>3.4 Automatic Scoring Pipeline</h3>
<p>We design an automated scoring pipeline (see Algorithm 1) to evaluate model-generated answers across complex fields like mathematics and physics, where answers vary from numbers to equations. This method simplifies answers into two categories: numeric values, handled through floating-point operations, and symbolic expressions, requiring symbolic computation.</p>
<p>For equations, we ensure all terms are on one side before dividing to check for mathematical equivalence. Intervals and tuples are compared by extracting and evaluating each element. Numeric answers are verified against a small tolerance of error, defaulting to $1 \mathrm{e}-8$ but adjustable for physics problems to allow for a specific error margin. For expressions, we use the SymPy ${ }^{5}$ library to confirm if the subtraction of two expressions approaches zero, indicating correctness.</p>
<h2>4 Experiments</h2>
<h3>4.1 Settings</h3>
<p>We conduct evaluations of open-source and closedsourced LMMs that have been selected with consideration of their comprehensive capabilities on OlympiadBench. At the same time, we have selected LLMs with strong mathematical and logical abilities for evaluation on text-only questions.</p>
<p>As no accurate automatic evaluation method for theorem proving exists, we run full experiment on the automatic-scoring-available open-ended problems with answer type included in the Table 3, which is discussed in this section. We do manual sampling check of GPT-4V for theorem proving problems with analysis reported at Section 5.1.</p>
<h3>4.1.1 Prompts</h3>
<p>We evaluate the models in a zero-shot setting. Due to the high difficulty of the OlympiadBench questions, there should be considerable randomness in the results when using small batch data as the validation set, so we directly use a specific prompt template for all models instead of conducting promptengineering for each model respectively. The</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>prompt template for English and Chinese openended questions is shown in the figure 3. To ensure the most complete extraction of the model's final results, we explicitly prescribe the types and formats of the answers in the prompt to promote the accuracy of the machine's automatic scoring. In order to test the native mathematical and physical abilities of the models, the prompts used in the test do not introduce knowledge points and other extra information contained in the dataset, but this information can be applied in subsequent research. Note that deepseek-math-7B-RL (Shao et al., 2024) requires the addition of a specific chain-of-thought prompt at the end of the input, which we adhered to during the evaluation.</p>
<h3>4.1.2 Evaluation Workflow</h3>
<p>We first apply each model to generate answers for questions in OlympiadBench using prompts formed by prompt template, with open-source models running on NVIDIA A800 GPUs. Then, we run the automatic scoring pipeline to judge the correctness of the answers as described in subsection 3.4. Finally, we calculate the micro-average accuracy as the comparing metric. The code of the whole workflow is provided in the supplementary material.</p>
<h3>4.2 Baselines</h3>
<p>In our study, we evaluate the performance of current leading bilingual large multimodal models (LMMs), as well as bilingual large language models (LLMs) that has strong mathematical and reasoning abilities. We take both open- and closedsource models into consideration, using either the largest and latest released checkpoints or the bestperforming official APIs available.</p>
<p>For LMMs, we select GPT-4V(GPT-4Vision) (OpenAI, 2023b), Gemini-ProVision (Team, 2023), Qwen-VL-Max (Bai et al., 2023) for closed-source models, while Yi-VL-34B (01-ai, 2024) and LLaVA-NeXT-34B (Liu et al., 2024) for open-source models. For models that demand compulsory image input, we take their LMM counterpart (corresponding text-model API or base LLM) for evaluation. Specifically, for LLaVA-NeXT-34B, we use its base LLM, Nous-Hermes-2-Yi-34B (NousResearch, 2023). The text model corresponding to Yi-VL-34B is Yi-34B-Chat (01-ai, 2023). Similarly, for the Gemini-Pro-Vision, we utilize the Gemini-Pro API interface. To examine the impact of replacing LMM with base LLM for processing text-only</p>
<table>
<thead>
<tr>
<th style="text-align: center;">The following is a question from an International <subject> competition:</th>
<th style="text-align: center;"><img alt="img-1.jpeg" src="img-1.jpeg" /></th>
<th style="text-align: center;">etäº†è¶³ä¸­çš„<subject>ç«žèµ›ä¸­çš„è§£ç­”é¢˜:</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">The answer of the question should be <ans_type>.</td>
<td style="text-align: center;">Single answer</td>
<td style="text-align: center;">ç­”æ¡ˆæ˜¯åž‹ä¸º<ans_type>.</td>
</tr>
<tr>
<td style="text-align: center;">The question has multiple answers, each of them should be <ans_type>.</td>
<td style="text-align: center;">Multiple answers of single type</td>
<td style="text-align: center;">é¡Œç›®å‰å¤šä¸ªç­”æ¡ˆ, ç­”æ¡ˆæ˜¯åž‹å‡ä¸º<ans_type>.</td>
</tr>
<tr>
<td style="text-align: center;">The question has multiple answers, with the answers in order being <ans_type>, ...<ans_type>.</td>
<td style="text-align: center;">Multiple answers of the same type</td>
<td style="text-align: center;">é¡Œç›®å‰å¤šä¸ªç­”æ¡ˆ, ç­”æ¡ˆæ˜¯åž‹åˆç§°ä¸º<ans_type>, ...<ans_type>.</td>
</tr>
<tr>
<td style="text-align: center;">Please calculate the answer according to the given requirements and the information provided. Please use LeToX format to represent the variables and formulas used in the solution process and results. Please end your solution with</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">æŒæ ¹è·¯é¢˜ç›®çš„è¦åœ¨å’Œæ‰€æä¾›çš„ä¿¡æ¯è®¡ç®—å¾—å‡ºç­”æ¡ˆ, è§£ç­”è¿‡ç¨‹å’Œç»“æžœä¸­ä½¿</td>
</tr>
<tr>
<td style="text-align: center;">"So the final answer is 'browd [answer]."</td>
<td style="text-align: center;">Single answer</td>
<td style="text-align: center;">ç„¶ä½•åŒ…å«å’Œå…¬å¼ä¾¿ä½¿ç”¨LeToXæ ¼å¼è¡¨ç¤ºã€‚</td>
</tr>
<tr>
<td style="text-align: center;">"So the final answer is 'browd [multiple answers connected with commas]."</td>
<td style="text-align: center;">Multiple answers</td>
<td style="text-align: center;">è¯·åœ¨æœ€åŽä»¥</td>
</tr>
<tr>
<td style="text-align: center;">and give the result explicitly.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">"æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯â€˜browd [ç­”æ¡ˆ]. "</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">"æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯â€˜browd [å‰æœ€å¤§è¿žå·è¿žæŽ¥çš„å¤šä¸ªç­”æ¡ˆ]. "</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ä½†å¼å´å‡ºç»“æžœã€‚</td>
</tr>
</tbody>
</table>
<p>Figure 3: The template of the construction of the prompt for English(left) and Chinese(right) open-ended questions, among which <subject>, <ans_type>, and whether there are multiple answers can all be obtained from the data items in OlympiadBench dataset.
data, we subsequently compare the performance differences between GPT-4V and GPT-4 ${ }^{6}$ on text-only questions in OlympiadBench.</p>
<p>For LLMs, we select DeepSeekMath-7BRL (Shao et al., 2024) as the primary baseline for text-only questions, and report the results of the selected LMMs (or their LLM counterparts) on the text-only questions for comparison, and additionally evaluate GPT-4 as described above.</p>
<h3>4.3 Main Results</h3>
<p>The overall experiment result is shown in table 4. Based on the results, our key findings can be summarized as the following:</p>
<p>OlympiadBench is more challenging than existing benchmarks, which provides new perspective to compare LMMs. As shown in table 7, the most advanced model only achieves an average accuracy of $17.97 \%$ on OlympiadBench, which is much lower than that of existing benchmarks. Moreover, the gap between the models has been widened, thereby becoming more significant, which helps people to compare the differences in capabilities between different models more accurately.</p>
<p>There still exists a huge difference between the most powerful closed-source models and open-source models, but a large model size is needed. The average accuracy of GPT-4V is more than 5 times larger than the best-performing open-source model (Yi-VL-34B). But Gemini-ProVision, being closed-source models of the secondtier size, is much less compatible on complicated tasks such as OlympiadBench, for it achieves an average accuracy that is only slightly higher than open-source model.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>The challenge lies more on question-withimages, Physics and none-English text. The model performance on text-only questions is significantly above average, showing the challenging spirit of multi-modal questions. Meanwhile, Physics questions, especially Physics questions with images, are more challenging than math questions, as they require knowledge of the laws of Physics as well as other world knowledge besides mathematical abilities such as calculation and reasoning. Moreover, LMMs with a focus on bilingual image-text training data, such as Qwen-VL-Max and Yi-VL-34B, perform better on Chinese questions then English questions.</p>
<p>Open source LLMs is catching at fast speed in the area of maths and physics. Although with a relatively small size, DeepSeekMath-7BRL outperforms or is on par with Gemini-ProVision and Qwen-VL-Max on the text-only part of OlympiadBench, especially in Math problems, showing promising future of open-source model of pre-training and fine-tuning on fine-grained mathematical and reasoning data.</p>
<p>Multi-modal training slightly hurts performance on text-only math and physics tasks, but may also bring some improvement. The text-only version GPT-4 performs slightly better on all datasets of OlympiadBench, except for the En_COMP dataset. We hypothesis that the improvement in the En_COMP dataset shows an enhancement of long-context text reasoning capabilities, which is discussed in Appendix B.2.</p>
<h2>5 Analysis</h2>
<p>In this section, we conduct analysis on the GPT4 V 's answers of specific open-ended questions that have been sampled, as well as giving preliminary</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Maths</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Physics</th>
<th></th>
<th></th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>En_COMP</td>
<td>Zh_COMP</td>
<td>Zh_CEE</td>
<td>Avg.</td>
<td></td>
<td>En_COMP</td>
<td>Zh_CEE</td>
<td>Avg.</td>
<td></td>
</tr>
<tr>
<td>LLaVA-NeXT-34Bâ€ </td>
<td>3.98</td>
<td>2.60</td>
<td>4.64</td>
<td>4.30</td>
<td>-</td>
<td>1.36</td>
<td>2.32</td>
<td>2.08</td>
<td>3.65</td>
</tr>
<tr>
<td>Yi-VL-34Bâ€ </td>
<td>4.22</td>
<td>3.68</td>
<td>4.31</td>
<td>4.23</td>
<td>-</td>
<td>0.91</td>
<td>1.64</td>
<td>1.46</td>
<td>3.42</td>
</tr>
<tr>
<td>Gemini-Pro-Vision</td>
<td>6.92</td>
<td>2.59</td>
<td>5.05*</td>
<td>5.14</td>
<td>-</td>
<td>3.19*</td>
<td>2.12</td>
<td>2.45</td>
<td>4.22</td>
</tr>
<tr>
<td>Qwen-VL-Max</td>
<td>10.68</td>
<td>13.21*</td>
<td>13.08</td>
<td>12.65</td>
<td>-</td>
<td>3.76*</td>
<td>5.64*</td>
<td>5.09</td>
<td>10.09</td>
</tr>
<tr>
<td>GPT-4V</td>
<td>27.18</td>
<td>14.87</td>
<td>21.27</td>
<td>21.70</td>
<td>-</td>
<td>11.42</td>
<td>10.45</td>
<td>10.74</td>
<td>17.97</td>
</tr>
<tr>
<td>Experiment with text-only</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaVA-NeXT-34B</td>
<td>4.15</td>
<td>2.94</td>
<td>8.55</td>
<td>6.29</td>
<td>-</td>
<td>2.12</td>
<td>5.22</td>
<td>3.13</td>
<td>5.87</td>
</tr>
<tr>
<td>Yi-VL-34B</td>
<td>4.45</td>
<td>3.68</td>
<td>8.06</td>
<td>6.24</td>
<td>-</td>
<td>0.85</td>
<td>5.22</td>
<td>2.28</td>
<td>5.72</td>
</tr>
<tr>
<td>DeepSeekMath-7B-RL</td>
<td>19.44</td>
<td>2.70</td>
<td>22.42</td>
<td>18.09</td>
<td>-</td>
<td>6.78</td>
<td>16.52</td>
<td>9.97</td>
<td>17.02</td>
</tr>
<tr>
<td>Gemini-Pro-Vision</td>
<td>7.57</td>
<td>2.94</td>
<td>9.20*</td>
<td>7.63</td>
<td>-</td>
<td>4.66</td>
<td>6.96</td>
<td>5.41</td>
<td>7.34</td>
</tr>
<tr>
<td>Qwen-VL-Max</td>
<td>11.57</td>
<td>14.29</td>
<td>25.89</td>
<td>19.70</td>
<td>-</td>
<td>4.24</td>
<td>18.26</td>
<td>8.83</td>
<td>18.27</td>
</tr>
<tr>
<td>GPT-4V</td>
<td>28.93</td>
<td>15.93</td>
<td>37.10</td>
<td>31.01</td>
<td>-</td>
<td>12.71</td>
<td>23.48</td>
<td>16.24</td>
<td>29.07</td>
</tr>
<tr>
<td>GPT-4</td>
<td>30.42</td>
<td>16.42</td>
<td>37.98</td>
<td>32.00</td>
<td>-</td>
<td>12.29</td>
<td>24.35</td>
<td>16.24</td>
<td>29.93</td>
</tr>
</tbody>
</table>
<p>Table 4: Experimental results. En_COMP: COMP problems in English, Zh_COMP: COMP problems in Chinese, Zh_CEE: CEE problems in Chinese. For closed-source models, the responses for some problems are not available, we mark the results with * (all of the proportion of missing answers are less than $1 \%$, except for the result of Qwen-VL-Max in Physics-En_COMP, where 26 questions exceed maximum input length). The causes are further described in Appendix B.3. Moreover, LLaVA-NeXT-34B and Yi-VL-34B only accepts input with single image, we mark results from only one image input with $\dagger$.</p>
<p>examination of theorem proving questions.</p>
<h3>5.1 Examination of Theorem Proving Questions</h3>
<p>For GPT-4V, we do manual sampling check to evaluate the mathematical theorem proving questions. In the questions drawn according to the knowledge point distribution, GPT-4V only answers 6 out of 81 questions correctly in Math-Zh_COMP, all of which are relatively simple and classic conclusions (e.g. AM-GM inequality), or involved only simple computational derivations, and was basically unable to complete the proof within the token limitation in Math-En_COMP, indicating that existing models still cannot effectively solve lengthy reasoning and proofs, which is consistent with the conclusions in existing papers (Trinh et al., 2024).</p>
<p>In solving proof problems, GPT-4V exposes several important issues, including: inability to fully utilize image information (figure 9 as an example); tending to make mistakes in simplifying and transforming algebraic expressions; proposing simple, basic incorrect conclusions;struggling with classification discussions, etc. Detailed examples can be found in the Appendix C.</p>
<h3>5.2 Mistake Analysis of GPT-4V</h3>
<p>We manually sample and check 97 maths (55 for English and 42 for Chinese) and 67 physics Olympiad-level open-ended problems that GPT-
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Distribution of the error occurring in GPT4V's solving process of 164 sampled Olympic-level open-ended problems.</p>
<p>4 V fails, and analyze the type of mistakes, the overall results are shown in figure 4. In maths problems, the typical errors of GPT-4V include: insufficient classification discussion, especially in combinatorial problems; poor performance in problems requiring large calculations (e.g. conic curve problems), manifests as a lack of logic in the calculation process, resulting in the model being unable to provide a reasonable answer. However, we also found that GPT-4V has strong abilities in solving quadratic equations and derivative problems. In physics problems, GPT-4V tends to fall in conceptual confusion, or introduce unnecessary variables or concepts, but its capability to simplify and transform algebraic expressions is stronger than in purely mathematical situations, with nearly no numerical calculation errors.</p>
<h2>6 Discussion and Future Work</h2>
<p>Here, we discuss the automated evaluation of theorem problems and the future research directions in advanced disciplines.</p>
<p>Evaluation of Theorem's Proof Problems. The inability to automatically evaluate theorem's proof problems remains a significant challenge today. Currently, mainstream methods for the automatic evaluation of proofs require formalization, necessitating domain expertise and background knowledge. Exploring how to automatically evaluate natural language proofs represents an important research direction. Our dataset includes expert-level comprehensive annotations in a fusion of natural language and LaTeX formats, making it a highquality resource for research in natural language proof problems and fostering further development.</p>
<p>Expansion of Disciplines. Mathematics, much like physics, serves as a litmus test for artificial intelligence, requiring a strong foundational knowledge, rigorous high-level computations, and precise logical reasoning. Currently, large models still face significant challenges in mathematics and physics, which are critical hurdles that must be overcome for the development of AGI. In our future work, we will integrate additional fields such as geography, biology, and chemistry to provide a more innovative and comprehensive evaluation of a model's reasoning capabilities.</p>
<h2>7 Conclusion</h2>
<p>We create OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark to assess the capabilities of large models in mathematics and physics reasoning. Each problem is detailed with expert-level annotations for step-by-step reasoning. In our benchmarking, we provide a detailed analysis of model performance, pinpointing prevalent error types and potential areas for enhancement. This significant and challenging effort fills a notable void, and we intend to open-source the benchmark to advance AGI and scientific reasoning research. Future efforts will focus on gathering more challenging questions and broadening the range of subjects to further develop rigorous scientific benchmarks.</p>
<h2>Acknowledgements</h2>
<p>This work is supported by the National Key R\&amp;D Program of China (No.2022ZD0116312), National Natural Science Foundation of China (No.</p>
<p>62236004), Quan Cheng Laboratory (Grant No. QCLZD202301) and Institute Guo Qiang at Tsinghua University.</p>
<h2>Ethical Considerations</h2>
<p>In this paper, we introduce OlympiadBench, a highly challenging bilingual, multimodal scientific benchmark aimed at evaluating the mathematical and physical reasoning of large models now and AGI in the future. The paper outlines the dataset construction, including data gathering, OCR processing, cleansing, deduplication, and detailed annotation. OlympiadBench's data, derived exclusively from official sources, substantially reduces the likelihood of pre-training data leakage. We offer precise annotations for each problem and have implemented an exhaustive evaluation script for more accurate model performance assessment. Additionally, being bilingual and providing expert-level reasoning annotations for every question, OlympiadBench serves as a crucial resource for propelling AGI's prowess in scientific reasoning. Committed to environmental sustainability, we intend to release the dataset and accompanying scripts publicly to cut down on unnecessary carbon footprint. In experiments, we comply with all licenses for models and data.</p>
<h2>Limitations</h2>
<p>In pursuit of understanding the logical reasoning abilities of LLMs and LMMs within the multimodal domains of mathematics and physics, we develop OlympiadBench, a challenging bilingual multimodal scientific benchmark. Despite filling a notable void, this work acknowledges inherent limitations. First, in the OlympiadBench, some questions feature answers that require categorical discussions or textual descriptions, such as proofs, which currently cannot be assessed using regular expressions or tools like SymPy at the code level and necessitate manual review. However, this data holds significant research value. Secondly, the automated scoring system we propose cannot perform specific analysis based on the particulars of each question. It makes logical judgments solely based on the two symbols or numerical expressions inputted, without integrating any special constraints that may exist within the actual problem context. What's more, the development of datasets for multimodal scientific reasoning requires extensive manual effort in gathering and annotating data,</p>
<p>which constrains the diversity and difficulty of multimodal scientific challenges. As a result, this hampers AI's capacity to learn from and address more intricate scenarios.</p>
<h2>References</h2>
<p>01-ai. 2023. Yi-34b-chat model card.
01-ai. 2024. Yi-vl-34b model card.
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357-2367, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Daman Arora, Himanshu Gaurav Singh, and Mausam. 2023. Have llms advanced enough? a challenging problem solving benchmark for large language models.</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966.</p>
<p>Daniel Bobrow et al. 1964. Natural language input for a computer problem solving system.</p>
<p>SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4.</p>
<p>Jie Cao and Jing Xiao. 2022. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th International Conference on Computational Linguistics, pages 1511-1520, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.</p>
<p>Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. 2022. UniGeo: Unifying geometry logical reasoning via reformulating mathematical expression. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3313-3323, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. 2021. GeoQA: A geometric question answering benchmark towards multimodal numerical reasoning. In Findings of
the Association for Computational Linguistics: ACLIJCNLP 2021, pages 513-523, Online. Association for Computational Linguistics.</p>
<p>Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023. TheoremQA: A theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7889-7901, Singapore. Association for Computational Linguistics.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Katherine M Collins, Albert Q Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B Tenenbaum, William Hart, et al. 2023. Evaluating language models for mathematics through interactions. arXiv preprint arXiv:2306.01694.</p>
<p>Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, and Julius Berner. 2023. Mathematical capabilities of chatgpt.</p>
<p>Zheqi He, Xinya Wu, Pengfei Zhou, Richeng Xuan, Guang Liu, Xi Yang, Qiannan Zhu, and Hua Huang. 2024. Cmmu: A benchmark for chinese multi-modal multi-type question understanding and reasoning.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.</p>
<p>Jinchang Hou, Chang Ao, Haihong Wu, Xiangtao Kong, Zhigang Zheng, Daijia Tang, Chengming Li, Xiping Hu, Ruifeng Xu, Shiwen Ni, and Min Yang. 2024. E-eval: A comprehensive chinese k-12 education evaluation benchmark for large language models.</p>
<p>Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.</p>
<p>Albert Q. Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, TimothÃ©e Lacroix, Yuhuai Wu, and Guillaume Lample. 2023. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157, San Diego, California. Association for Computational Linguistics.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024. Llavanext: Improved reasoning, ocr, and world knowledge.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. arXiv preprint arXiv:2304.08485.</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. 2024. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR).</p>
<p>Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. 2021. Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 67746786, Online. Association for Computational Linguistics.</p>
<p>Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS).</p>
<p>Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. 2023. A survey of deep learning for mathematical reasoning.</p>
<p>Ha-Thanh Nguyen. 2023. A brief report on lawgpt 1.0: A virtual legal assistant based on gpt-3. arXiv preprint arXiv:2302.05729.</p>
<p>NousResearch. 2023. Nous-hermes-2-yi-34b model card.</p>
<p>OpenAI. 2023a. Gpt-4 technical report.
OpenAI. 2023b. Gpt-4v(ision) system card.
Jielin Qiu, William Han, Jiacheng Zhu, Mengdi Xu, Michael Rosenberg, Emerson Liu, Douglas Weber, and Ding Zhao. 2023. Transfer knowledge from natural language to electrocardiography: Can we detect cardiovascular disease through language models? arXiv preprint arXiv:2301.09017.</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.</p>
<p>Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2023. Scieval: A multi-level large language model evaluation benchmark for scientific research.</p>
<p>Gemini Team. 2023. Gemini: A family of highly capable multimodal models.</p>
<p>Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. 2024. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476-482.</p>
<p>Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2024. Scibench: Evaluating college-level scientific problem-solving abilities of large language models.</p>
<p>Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. Deep neural solver for math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 845-854, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models.</p>
<p>Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. 2023. Cmath: Can your language model pass chinese elementary school math test?</p>
<p>Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 94-106, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. 2021. Naturalproofs: Mathematical theorem proving in natural language.</p>
<p>Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.</p>
<p>Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, and JianGuang Lou. 2023. Large language models meet NL2Code: A survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 74437464, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Cedegao E Zhang, Katherine M Collins, Adrian Weller, and Joshua B Tenenbaum. 2023. Ai for mathematics: A cognitive science perspective. arXiv preprint arXiv:2310.13021.</p>
<p>Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu. 2024a. Mmllms: Recent advances in multimodal large language models.</p>
<p>Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, Wenhu Chen, and Jie Fu. 2024b. Cmmmu: A chinese massive multidiscipline multimodal understanding benchmark.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models.</p>
<p>Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. 2022. Minif2f: a cross-system benchmark for formal olympiad-level mathematics.</p>
<p>Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. 2023. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification.</p>
<h2>A Dataset Details</h2>
<h2>A. 1 Data Sources</h2>
<p>Our data sources can be split into the following three parts:</p>
<ol>
<li>Global Mathematics and Physics Olympiad Problems. The Mathematics and Physics Olympiad problems are globally recognized for their complexity and quality. These problems often require multiple methods of solution and the ability to integrate sub-disciplines from within the broader fields of mathematics and physics. The participants in these competitions represent some of the most proficient individuals worldwide in logical reasoning within mathematics and physics. This not only sets a high standard for problem-solving but also fosters a diverse set of analytical skills that are crucial for the advancement of large models.</li>
<li>Regional and National Chinese Mathematics Competitions. In addition to maintaining a high level of difficulty, regional competitions and the CMO introduce elements specific to the Chinese context. This inclusion is instrumental in furthering the development and research of Chinese-oriented and multilingual large models. By encompassing a wide array of mathematics and physics problems, these competitions provide a unique opportunity to develop models that are adaptable and proficient across different mathematical queries, enhancing their versatility and effectiveness.</li>
<li>Gaokao Mock Questions for Mathematics and Physics. Given that the resolution of Olympiad-level problems typically necessitates models with substantial parameter sizes, we also incorporate Gaokao simulation problems to evaluate smaller models' capabilities in answering free-form mathematics and physics questions.</li>
</ol>
<p>The integration of data from Gaokao simulation problems, regional and national competitions, to the global Olympiads constructs a smooth difficulty transition curve. This methodology not only distinguishes the mathematical and physical problemsolving capabilities of different models but also provides guidance on the scaling laws for models specialized in these domains.</p>
<table>
<thead>
<tr>
<th>Subject</th>
<th>Source</th>
<th>Coverage Years</th>
<th>Number</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maths</td>
<td>IMO</td>
<td>2006-2022</td>
<td>509</td>
</tr>
<tr>
<td></td>
<td>RMM</td>
<td>2011, 2013, 2015-2019, 2021, 2023</td>
<td>53</td>
</tr>
<tr>
<td></td>
<td>ARML</td>
<td>2009-2014, 2019, 2023</td>
<td>505</td>
</tr>
<tr>
<td></td>
<td>EMC</td>
<td>1998-2023</td>
<td>364</td>
</tr>
<tr>
<td></td>
<td>EGMO</td>
<td>2013, 2015-2023</td>
<td>64</td>
</tr>
<tr>
<td>Physics</td>
<td>IPhO</td>
<td>1984, 1986-1990, 2008-2012, 2014-2016, 2018-2019, 2021</td>
<td>381</td>
</tr>
<tr>
<td></td>
<td>APhO</td>
<td>2013-2015</td>
<td>200</td>
</tr>
<tr>
<td></td>
<td>EPhO</td>
<td>2019-2022</td>
<td>17</td>
</tr>
<tr>
<td></td>
<td>USAPhO</td>
<td>2017-2021</td>
<td>113</td>
</tr>
<tr>
<td></td>
<td>PUPC</td>
<td>2020-2022</td>
<td>65</td>
</tr>
<tr>
<td></td>
<td>OPhO</td>
<td>2020-2023</td>
<td>132</td>
</tr>
</tbody>
</table>
<p>Table 5: Summary of Problems in Math and Physics Competitions, with full acronyms listed in the Table 6</p>
<h3>A.2 Data Curation Process</h3>
<p>Our initial step involves a comprehensive survey of well-known Olympiad competitions, and the list of which is accessible through the AoPS community platform ${ }^{7}$. We cataloged these competitions based on several criteria: difficulty level, volume of questions, availability of materials in public domains, language, discipline, and coverage years. Following the design principles outlined in Section 3.1, we meticulously select specific contests and years that not only adhere to our dataset design criteria but also try to span the widest possible range of years (Table 5).</p>
<p>In the format conversion phase, we also manually annotated the subfield of each question in maths or physics, with their distribution detailed in Table 9.</p>
<h3>A.3 Example of Progressive Problem in Physics</h3>
<p>Figures 10 and 11 present a sequential challenge from the International Physics Olympiad (IPhO) 2021, illustrating the intricacies of progressive problem-solving in a competitive context. This particular problem set exemplifies a common trait in advanced physics competitions: the dependency of many questions on the solutions and materials of preceding ones. These dependencies are sometimes explicit, but most are implicit, weaving a complex web of interconnected knowledge and reasoning.</p>
<p>An explicit instance of this dependency can be observed in problem C.2, where the prompt directly requires the use of the symbol $\beta$ defined in B. 1 for the calculation of an unknown quantity. This re-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>quirement not only tests the participants' ability to understand and apply physical concepts but also assesses their skill in navigating through and linking various parts of a problem set. Such explicit instructions are crucial for guiding participants through the logical progression of the problems, yet the majority of dependencies remain implicit, demanding a deeper level of comprehension and integration of the material.</p>
<p>This structure of problem-solving reflects a realistic scientific inquiry, where discoveries and solutions often rely on previously established knowledge. The explicit mention of $\beta$ in C. 2 as derived from B. 1 is emblematic of this educational approach, aiming to foster a holistic understanding and the ability to build upon existing information to solve complex problems. It underscores the importance of thorough comprehension of earlier sections for successful problem-solving in later sections, simulating real-world scientific challenges where new solutions are often predicated on a foundation of established knowledge.</p>
<h2>B Evaluation Details</h2>
<h2>B. 1 Details of the Evaluated Models</h2>
<h2>B.1.1 LMMs</h2>
<p>We have selected current mainstream LMMs that have performed the best on past scientific multimodal datasets for evaluation.</p>
<p>The closed-source models include: GPT4V (OpenAI, 2023b), developed by OpenAI, which is currently the most powerful multimodal model. Gemini (Team, 2023) is the LMM series developed by Google Deepmind, with Gemini-UltraVision being purported to have surpassed GPT-4V</p>
<table>
<thead>
<tr>
<th>Subject</th>
<th>Acronym</th>
<th>Full name</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maths</td>
<td>IMO</td>
<td>International Mathematical Olympiad</td>
</tr>
<tr>
<td></td>
<td>RMM</td>
<td>Romanian Master of Mathematics</td>
</tr>
<tr>
<td></td>
<td>ARML</td>
<td>American Regions Mathematics League</td>
</tr>
<tr>
<td></td>
<td>EMC</td>
<td>Euclid Mathematics Competition</td>
</tr>
<tr>
<td></td>
<td>EGMO</td>
<td>European Girlsâ€™ Mathematical Olympiad</td>
</tr>
<tr>
<td>Physics</td>
<td>IPhO</td>
<td>International Physics Olympiad</td>
</tr>
<tr>
<td></td>
<td>APhO</td>
<td>Asian Physics Olympiad</td>
</tr>
<tr>
<td></td>
<td>EPhO</td>
<td>European Physics Olympiad</td>
</tr>
<tr>
<td></td>
<td>USAPhO</td>
<td>USA Physics Olympiad</td>
</tr>
<tr>
<td></td>
<td>PUPC</td>
<td>Princeton University Physics Competition</td>
</tr>
<tr>
<td></td>
<td>OPhO</td>
<td>Online Physics Olympiad</td>
</tr>
</tbody>
</table>
<p>Table 6: Full names of all competitions'acronyms used in this paper
on datasets like MMMU. However the unavailability of Googleâ€™s API for Gemini Ultra, we test the accessible Gemini-Pro-Vision as an alternative. Qwen-VL-Max (Bai et al., 2023), developed by Alibaba, is the largest LMM, and stands on par with GPT-4V and Gemini-Ultra in multi-modal tasks. Due to the large proportion of Chinese data used in its training, Qwen-VL-Max has a certain advantage in Chinese language ability.</p>
<p>The open-source models include: Yi-VL34B (01-ai, 2024) is the first open-source 34B multi-modal model that has demonstrated satisfying performance on several latest datasets. With Chinese text-image pairs included in the training process, Yi-VL-34B offers adequate multilingual support. LLaVA-NeXT-34B (Liu et al., 2024) claims to be the strongest open-source LMM, with enhancements in reasoning, OCR, and world knowledge. Despite being trained exclusively with English multi-modal data, it demonstrates an emergent zero-shot Chinese multi-modal capability on Chinese benchmarks.</p>
<p>It should be noted that an image must be passed for Gemini-Pro-Vision, LLaVA-NeXT, and Yi-VL during inference. Therefore, for the text-only questions in OlympiadBench dataset, we use the corresponding text-model api (for closed-source models), or their base LLM (for open-source models). To examine the impact of replacing LMM with base LLM for processing text-only data, we subsequently compare the performance differences between GPT-4V and GPT-4 on text-only questions in OlympiadBench.</p>
<h3>B.1.2 LLMs</h3>
<p>The field of LLM starts early in scientific areas such as mathematics and physics, with models specifically trained occurring. We select DeepSeekMath-7B-RL (Shao et al., 2024) as the primary baseline for text-only questions. DeepSeekMath-7BRL is pre-trained on 120B math-related data and enhanced chain-of-thought (CoT) reasoning capabilities using reinforcement learning, in the result scoring close to GPT-4 and Gemini-Ultra on the MATH (Hendrycks et al., 2021) dataset. We report the results of the selected LMMs (or their LLM counterparts) on the text-only questions for comparison, and additionally evaluate GPT-4 in order to compare with GPT-4V ${ }^{8}$.</p>
<h3>B.2 Detailed Experiment Result</h3>
<p>The comparison of the performance of mainstream closed-ended models on different datasets are shown in Table 7.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: center;">GPT-4(V)</th>
<th style="text-align: center;">Qwen <br> VL-Max</th>
<th style="text-align: center;">Gemini <br> Pro</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MATH</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">32.6</td>
</tr>
<tr>
<td style="text-align: left;">MathVista(testmini)</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">45.2</td>
</tr>
<tr>
<td style="text-align: left;">OlympiadBench</td>
<td style="text-align: center;">17.97</td>
<td style="text-align: center;">10.09</td>
<td style="text-align: center;">4.22</td>
</tr>
</tbody>
</table>
<p>Table 7: Comparison of Performance on Different Benchmarks. The values for MATH and MathVista are obtained from Gemini and Qwen's report.</p>
<p>To further discuss the performance difference between GPT-4 and GPT-4V on the PhysicsEn_COMP, we split the En_COMP dataset into two sub-datasets, with normal-PhO being normal</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>PhO questions, and long-PhO being PhO questions that show in a relational series, therefore having long context. As shown in table 8, GPT-4 keeps performing slightly better on normal-PhO, but lags much behind on long-PhO, which may indicate improvement of long-context text reasoning capabilities after multimodal training.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">long-PhO <br> $\mathbf{( 1 5 7 )}$</th>
<th style="text-align: center;">normal-PhO <br> $\mathbf{( 7 4 )}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4V</td>
<td style="text-align: center;">18.47</td>
<td style="text-align: center;">1.35</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">14.92</td>
<td style="text-align: center;">4.05</td>
</tr>
</tbody>
</table>
<p>Table 8: Average accuracy of GPT-4V and GPT-4 for the En_COMP dataset</p>
<h2>B. 3 Unavailable Responses for Closed-Source Models</h2>
<p>As described in table 4, the response for some problems are not available, the main causes are as follows:</p>
<ol>
<li>Exceeding input limit: Some of the context of the problems are too long, which exceed the input token limitation for the API. This case mainly occurs in Physics-En_COMP that contains long-context problems of over 6,000 tokens.</li>
<li>Inappropriate response: Some problems trigger inappropriate response, which are banned by the API to return.</li>
<li>No response: Some problems continuously get no or empty response from the API.</li>
<li>Request timed out: Some problems continuously fail to get a response.</li>
</ol>
<p>We removed the problems with unavailable response when calculating the accuracy.</p>
<h2>C Additional Analysis and Examples</h2>
<h2>C. 1 Performance analysis of GPT-4V</h2>
<p>We analyzed GPT-4V's performance (accuracy on open-ended problems) on different knowledge points based on the knowledge point labels in OlympiadBench, the results can be found at Figure 5 .</p>
<p>For Math problems, GPT-4V has poor performance in geometry, with the lowest scoring knowledge points being almost exclusively geometryrelated. This may show the need of improving the
ability of understanding and imaging plane or 3d situations. GPT-4V also performs poorly on knowledge points that are more computationally intensive such as conic curves; and struggles to give a complete and comprehensive classification discussion, therefore prone to making mistakes on combinatorial problems. However, GPT-4V is stronger in knowledge related to derivatives and complex numbers.</p>
<p>As for Physics problems, none of the knowledge points surpass an accuracy of $16 \%$, and GPT-4V struggles more in thermodynamics and mechanics.</p>
<h2>C. 2 Detailed Description of the Error Types in GPT-4V's solving or proving process</h2>
<p>The error types are as follows:</p>
<ol>
<li>Question Misunderstanding: GPT-4V sometimes misunderstands the intention or settings of the question.</li>
<li>Value Calculation Error: GPT-4V make simple calculation mistakes sometimes, such as outputting $\frac{b}{2}+7=\frac{b+7}{2}$, these mistakes appears more in Chinese and Math contents.</li>
<li>Expression Calculation Error: Similar to value calculation error, but happens when transforming between two expressions.</li>
<li>Logical Reasoning / Induction Error / Conceptual Confusion: GPT-4V sometimes makes false reasoning or induction, as well as encounters conceptual confusion (see Figure 7 for example).</li>
<li>Introducing Unnecessary variables or concepts: GPT-4V sometimes suddenly introduce variables or try to use concepts that have no contribution to solving the problem, which not only makes the output longer, but also may confuse GPT-4V itself and leads to incorrect output.</li>
<li>Conclusion Hallucination: GPT-4V sometimes hallucinates for a conclusion that is not reached in former output, or hallucinates a theorem that does not really exist (for example, when solving geometric proving problem, GPT-4V always mention "The Power Theorem", which does not exist, and all the proof thereafter will lost their logic).</li>
<li>Unfinished Answering: GPT-4V sometimes says the question have confliction in settings</li>
</ol>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: An analysis of GPT-4V's accuracy on different knowledge points, blue for Math and red for Physics.
(which is not true), or degenerates after some tokens.
8. Insufficient Classification Discussions: When doing classification discussion, GPT-4V may miss some possible situation, or have overlapped discussion (see Figure 6 for example).
9. Incorrect Judging: Sometimes GPT-4V gives the right answer, but is judged as incorrect due to the limitation of the automated scoring system: One important problem is that many problems, especially Physics problems, accept answers that fall in a specific range due to rounding up, rather than a fixed numerical answer, so a precision is needed for automatically calculating the range, which was not given in many cases. Using relative precision (such as a percentage of $1 \%$ ), may leads to accepting answers with unacceptable error when the origin answer is big, so we manually decide the precision for OlympiadBench problems if there is no official precision given, which may leads to incorrect judging. For example, in a Physics question (Physics-En_COMP#995), the precision is manually set as $1 e 1$, with the official answer being $\sqrt{2} \times 3970 \approx 5614.43$. GPT-</p>
<p>4 V calculates the answer as $\sqrt{2} \times 3970 \approx$ $1.414 \times 3970 \approx 5615.88$, which falls out of the acceptable range, but is actually correct. Another problem is the current automated scoring system can't judge the equality between expressions such as $a \sqrt{b}$ and $\sqrt{a^{2} b}$ with $a \geq 0$. These flaws call for future work such as automatically deciding required precision of the answer, and automatically judging equality between expressions.</p>
<h2>C. 3 Other Observation for Open-Ended Problems</h2>
<ol>
<li>Given a simple solution, GPT-4V may choose a more complex method to solve the problem (see Figure 8).</li>
<li>Models may give correct answers with a false process. Mainly observed for problems with a simple answer, such as the variables takes 0 as the answer.</li>
<li>GPT-4V may success in giving correct overall idea, but fail in calculation (such as solving quadratic equations with extra negative signs), which leads to a wrong answer.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">GPT-4V's Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">A die, with the numbers $1,2,3,4,6$, and 8 on its six faces, is rolled. After this roll, if an odd number appears on the top face, all odd numbers on the die are doubled. If an even number appears on the top face, all the even numbers are halved. If the given die changes in this way, what is the probability that a 2 will appear on the second roll of the die?</td>
<td style="text-align: center;">Let's first determine the possible outcomes after the first roll and the corresponding changes to the die. <br> If an odd number appears on the top face after the first roll (which can be either 1, 3, or 6), all odd numbers are doubled. So the new faces of the die will be: <br> - If 1 appears, it becomes 2. (3 should also be doubled.) <br> - If 3 appears, it becomes 6. (1 should also be doubled.) <br> - The number 6 is even, so it remains unchanged. (Incorrectly classifying 6 into odd numbers, which leads to failure in thorough discussion of even numbers below.) <br> If an even number appears on the top face after the first roll (which can be either 2, 4, or 8), all even numbers are halved. So the new faces of the die will be: <br> - If 2 appears, it becomes 1. (Same problem.) <br> - If 4 appears, it becomes 2 . <br> - If 8 appears, it becomes 4 . <br> Now, let's calculate the probability of getting a 2 on the second roll based on the first roll: <br> - If the first roll is 1 , the die will have faces $2,2,6,4,6,8$. The probability of rolling a $2 i \varepsilon \frac{2}{6}$.</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">- If the first roll is 8 , the die will have faces $1,2,3,2,3,4$. The probability of rolling a 2 is $\frac{2}{6}$. <br> Now, we need to calculate the total probability of rolling a 2 on the second roll. Since each face has an equal probability of appearing on the first roll, we can average the probabilities: <br> Total probability $\frac{1}{6} \times\left(\frac{2}{6}+\cdots+\frac{2}{6}\right)=\cdots=\frac{1}{4}$. <br> So the final answer is $\left\backslash\right.$ boxed $\left\lceil\frac{3}{4}\right\rceil$.</td>
</tr>
</tbody>
</table>
<p>Figure 6: An example of GPT-4V making insufficient classification discussions in a combinatorial problem. The red text is the wrong part of GPT-4V's answer, and the purple text is our further illustration.
4. GPT-4V may not fully utilize the information
from the image (see Figure 9).</p>
<h1>D Automatic Scoring Pipeline</h1>
<p>The pipeline workflow is shown in Algorithm 1.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">Auto</span><span class="w"> </span><span class="nt">Scoring</span><span class="w"> </span><span class="nt">Judge</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">GroundTruth</span><span class="o">,</span><span class="w"> </span><span class="nt">ModelOutput</span><span class="o">;</span>
<span class="w">    </span><span class="nt">Output</span><span class="o">:</span><span class="w"> </span><span class="nt">Boolean</span><span class="w"> </span><span class="nt">value</span><span class="w"> </span><span class="nt">indicating</span><span class="w"> </span><span class="nt">match</span><span class="o">;</span>
<span class="w">    </span><span class="nt">Preprocess</span><span class="w"> </span><span class="nt">GroundTruth</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">ModelOutput</span><span class="o">;</span>
<span class="w">    </span><span class="nt">if</span><span class="w"> </span><span class="nt">GroundTruth</span><span class="w"> </span><span class="nt">equals</span><span class="w"> </span><span class="nt">ModelOutput</span><span class="w"> </span><span class="nt">then</span>
<span class="w">        </span><span class="nt">return</span><span class="w"> </span><span class="nt">True</span><span class="o">;</span>
<span class="w">    </span><span class="nt">else</span>
<span class="w">        </span><span class="nt">if</span><span class="w"> </span><span class="nt">GroundTruth</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">ModelOutput</span><span class="w"> </span><span class="nt">are</span><span class="w"> </span><span class="nt">intervals</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">tuples</span><span class="w"> </span><span class="nt">then</span>
<span class="w">            </span><span class="nt">if</span><span class="w"> </span><span class="nt">GroundTruth</span><span class="w"> </span><span class="nt">equals</span><span class="w"> </span><span class="nt">ModelOutput</span><span class="w"> </span><span class="nt">as</span><span class="w"> </span><span class="nt">intervals</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">tuples</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                </span><span class="nt">return</span><span class="w"> </span><span class="nt">True</span><span class="o">;</span>
<span class="w">            </span><span class="nt">else</span>
<span class="w">                </span><span class="nt">return</span><span class="w"> </span><span class="nt">False</span><span class="o">;</span>
<span class="w">            </span><span class="nt">end</span>
<span class="w">            </span><span class="nt">else</span>
<span class="w">                </span><span class="nt">if</span><span class="w"> </span><span class="nt">GroundTruth</span><span class="w"> </span><span class="nt">equals</span><span class="w"> </span><span class="nt">ModelOutput</span><span class="w"> </span><span class="nt">numerically</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                    </span><span class="nt">return</span><span class="w"> </span><span class="nt">True</span><span class="o">;</span>
<span class="w">                </span><span class="nt">else</span>
<span class="w">                    </span><span class="nt">if</span><span class="w"> </span><span class="nt">Both</span><span class="w"> </span><span class="nt">GroundTruth</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">ModelOutput</span><span class="w"> </span><span class="nt">contain</span><span class="w"> </span><span class="s2">&quot;=&quot;</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                        </span><span class="nt">return</span><span class="w"> </span><span class="nt">EquationEqual</span><span class="o">(</span><span class="nt">GroundTruth</span><span class="o">,</span><span class="w"> </span><span class="nt">ModelOutput</span><span class="o">);</span>
<span class="w">                    </span><span class="nt">else</span>
<span class="w">                        </span><span class="nt">return</span><span class="w"> </span><span class="nt">ExpressionEqual</span><span class="o">(</span><span class="nt">GroundTruth</span><span class="o">,</span><span class="w"> </span><span class="nt">ModelOutput</span><span class="o">);</span>
<span class="w">                    </span><span class="nt">end</span>
<span class="w">            </span><span class="nt">end</span>
<span class="w">        </span><span class="nt">end</span>
<span class="w">    </span><span class="nt">end</span>
</code></pre></div>

<table>
<thead>
<tr>
<th style="text-align: center;">Context</th>
<th style="text-align: center;">GPT-4V's Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3. Block on an Incline Plane Consider a block of mass, $M$, and charge, $Q&gt;0$, sliding down an incline plane at an angle $\alpha$ with the horizontal with initial position $\left(x_{0}, y_{0}\right)$. <br> The system is exposed to an upward electric field given by $E(y)=\beta\left(y_{0}-y\right)$ with $\beta&gt;0$. <br> Figure 2: An incline plane submerged in a medium.</td>
<td style="text-align: center;">To find the position $y_{e}$ where the block experiences net zero vertical force, we need to consider the forces acting on the block in the vertical direction. There are two forces to consider: the gravitational force and the electric force. <br> The gravitational force acting on the block in the vertical direction can be found by considering the component of the weight of the block that acts perpendicular to the incline. The weight of the block is Mg , where $g$ is the acceleration due to gravity. The component of this force in the vertical direction is $\mathrm{M} g \sin \alpha$. (The vertical gravity force should be Mg itself, and the component that is vertical to the plane should be $\mathrm{M} g \cos \alpha$, so GPT-4V gets wrong in any way.) <br> The electric force acting on the block is due to the electric field $E(y)=\beta\left(y_{0}-y\right)$. Since the charge of the block is $Q$, the electric force is $F_{e}=Q E(y)=Q \beta\left(y_{0}-y\right)$. <br> At the position $y_{e}$ where the net vertical force is zero, these two forces must balance each other: $\mathrm{Mg} \sin \alpha=Q \beta\left(y_{0}-y\right)$. Now we can solve for $y_{e}$ : <br> $\qquad$</td>
</tr>
</tbody>
</table>
<p>Figure 7: An example of GPT-4V getting confused between the concept of "vertical to the plane" and "vertical(to the ground)".</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">GPT-4V's Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">å·²çŸ¥æ­£æ•° $x, y$ æ»¡è¶³: $x+y=1$.æ±‚ $\frac{1}{x}+\frac{4}{y}$ çš„æœ€å°å€¼.</td>
<td style="text-align: center;">ç”±å·²çŸ¥æ¡ä»¶ $x+y=1$, æˆ‘ä»¬å¯ä»¥å°† $\frac{1}{x}+\frac{4}{y}$ è¡¨è¾¾ä¸º $x$ å’Œ $y$çš„å‡½æ•°ã€‚é¦–å…ˆ, æˆ‘ä»¬å°† $y$ ç”¨ $x$ è¡¨ç¤ºå‡ºæ¥, å³ $y=1-x$ ã€‚ç„¶åŽå°† $y$</td>
</tr>
<tr>
<td style="text-align: center;">Standard Solution</td>
<td style="text-align: center;">ä»£å…¥ $\frac{1}{x}+\frac{4}{y}$ ä¸­, å¾—åˆ°: $\frac{1}{x}+\frac{4}{y}=\frac{1}{x}+\frac{4}{1-x}$</td>
</tr>
<tr>
<td style="text-align: center;">ç”± $a+b-$ $2 \sqrt{a b}=(\sqrt{a}-\sqrt{b})^{2} \geq 0$, æ•… $a+$ $b \geq 2 \sqrt{a b}$. æ‰€ç¤º $\frac{1}{x}+\frac{4}{y}=$ $\left(\frac{1}{x}+\frac{4}{y}\right)(x+y)=1+4+\frac{y}{x}+$ $\frac{3 x}{y} \geq 5+2 \sqrt{\frac{y}{x}} \cdot \frac{4 x}{y}=9$, ç­‰å·åœ¨ $x=\frac{1}{3}, y=\frac{2}{3}$ å¤„å–åˆ°, æ•…æœ€å°å€¼ä¸º 9 .</td>
<td style="text-align: center;">ä¸ºäº†æ‰¾åˆ°è¿™ä¸ªè¡¨è¾¾å¼çš„æœ€å°å€¼, æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¾®åˆ†æ³•ã€‚é¦–å…ˆ, æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¾®åˆ†æ³•ç‚¹ã€‚ æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ±‚æ ¹å…¬å¼æ¥è§£å®ƒ: $3 x^{2}+2 x-1=0$. è¿™æ˜¯ä¸€ä¸ªäºŒæ¬¡æ–¹ç¨‹, æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ±‚æ ¹å…¬å¼æ¥è§£å®ƒ: $x=\frac{-b \pm \sqrt{b^{2}-4 a c}}{2 a}$, å…¶ä¸­ $a=3, b=2, c=-1$, ä»£å…¥æ±‚æ ¹å…¬å¼å¾—åˆ°: $x=$ $\cdots=\frac{-2 \pm 4}{6}$ æˆ‘ä»¬å¾—åˆ°ä¸¤ä¸ªè§£, åˆ†åˆ«æ˜¯: $x=\frac{1}{3}$ åŠ $x=-1$.ç”±äºŽ $x$ æ˜¯æ­£æ•°, æˆ‘ä»¬æŽ’é™¤ $x=-1$ çš„è§£, æ‰€ä»¥ $x=\frac{1}{3}$. å°† $x$ çš„å€¼ä»£å…¥ $y=1-x$, å¾—åˆ° $\cdots$ ã€‚çŽ°åœ¨æˆ‘ä»¬å¯ä»¥è®¡ç®— $\frac{1}{x}+\frac{4}{y}$ çš„å€¼: $\frac{1}{x}+\frac{4}{y}=\frac{1}{3}+\frac{4}{2}=\cdots=9$.æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯ 9 ã€‚</td>
</tr>
</tbody>
</table>
<p>Figure 8: An example of GPT-4V giving a solution that is much complex than that of the standard solution, although reaching the correct answer eventually.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 9: An example of GPT-4V's failure in utilizing image information from Math-Zh_COMP. GPT-4V starts proving with "we have $\angle D E F=\angle F B C=59^{\circ}$ ", which is an error that can evidently be identified from the image, showing insufficient comprehension of the given plane geometry figure.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Subset</th>
<th style="text-align: center;">Subfield</th>
<th style="text-align: center;">Number</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CEE Math</td>
<td style="text-align: center;">Derivative</td>
<td style="text-align: center;">334</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Conic Sections</td>
<td style="text-align: center;">350</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sequence</td>
<td style="text-align: center;">258</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Trigonometric Functions</td>
<td style="text-align: center;">236</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Set Theory</td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Logic</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Elementary Functions</td>
<td style="text-align: center;">158</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Inequality</td>
<td style="text-align: center;">138</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PC\&amp;PE</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Probability and Statistics</td>
<td style="text-align: center;">758</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Plane Geometry</td>
<td style="text-align: center;">790</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Solid Geometry</td>
<td style="text-align: center;">1322</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Complex Numbers</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">COMP Math</td>
<td style="text-align: center;">Combinatorics</td>
<td style="text-align: center;">369</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Algebra</td>
<td style="text-align: center;">525</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Number Theory</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Geometry</td>
<td style="text-align: center;">535</td>
</tr>
<tr>
<td style="text-align: center;">CEE\&amp;COMP Physics</td>
<td style="text-align: center;">Mechanics</td>
<td style="text-align: center;">1010</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Electromagnetism</td>
<td style="text-align: center;">714</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Thermodynamics</td>
<td style="text-align: center;">248</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Optics</td>
<td style="text-align: center;">153</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Modern Physics</td>
<td style="text-align: center;">209</td>
</tr>
</tbody>
</table>
<p>Table 9: Statistics of subfield in Mathematics and Physics. PC\&amp;PE stands for Polar Coordinates and Parametric Equations.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ The version of GPT-4 is "0125-preview" and GPT-4V is "1106-vision-preview".&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
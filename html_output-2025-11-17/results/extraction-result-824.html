<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-824 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-824</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-824</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-273638677</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.19727v1.pdf" target="_blank">FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning</a></p>
                <p><strong>Paper Abstract:</strong> Financial intelligence generation from vast data sources has typically relied on traditional methods of knowledge-graph construction or database engineering. Recently, fine-tuned financial domain-specific Large Language Models (LLMs), have emerged. While these advancements are promising, limitations such as high inference costs, hallucinations, and the complexity of concurrently analyzing high-dimensional financial data, emerge. This motivates our invention FISHNET (Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert swarming, and Task planning), an agentic architecture that accomplishes highly complex analytical tasks for more than 98,000 regulatory filings that vary immensely in terms of semantics, data hierarchy, or format. FISHNET shows remarkable performance for financial insight generation (61.8% success rate over 5.0% Routing, 45.6% RAG R-Precision). We conduct rigorous ablations to empirically prove the success of FISHNET, each agent's importance, and the optimized performance of assembling all agents. Our modular architecture can be leveraged for a myriad of use-cases, enabling scalability, flexibility, and data integrity that are critical for financial tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e824.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e824.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FISHNET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular multi-agent architecture for extracting and composing financial intelligence from heterogeneous regulatory filings using sub-querying, task planning, a central harmonizer, expert per-document agents (swarm), hierarchical retrieval, and neural-conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>FISHNET multi-agent system</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Modular agentic system composed of: Sub-querying Agent (query decomposition + HalluciBot filtering), Task Planning Agent (few-shot / ICL plan generation, long-term memory), Harmonizer Agent (aggregates/swaps numeric operations and executes plans), and many Expert Agents (one per filing type). Retrieval uses FAISS vector DB with hierarchical embedding indices; routing uses embedding-based and generative routing combined with simulated swarming.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Retrieval (R-Precision on filing data)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Retrieval reported via R-Precision ablations: agent-level 45.65% and table-level 52.2% (embedding index ablations aggregated across questions).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Multi-filing, multi-entity financial query answering (agentic end-to-end task)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning / multi-step reasoning / sequential decision-making (agent orchestration + tool-like DB access)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>End-to-end agentic success rate: 61.8% overall (Easy questions aggregate 81.0%; Hard questions aggregate 28.3%). Per-filing success rates reported in Table 5 (e.g., NPORT 46.4% overall, NMFP 95.6%, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>sub-querying, task planning module, central harmonizer (orchestrator), expert agents (tool-like specialists), hierarchical retrieval (global/agent/table embeddings), FAISS vector DB, long-term memory for plans, simulated swarming (multi-agent communication), HalluciBot hallucination filter, neural-conditioning (priming with positive/negative examples).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>few-shot learning and in-context learning (ICL) for planner and sub-query generator; prompting and query-rewriting using gpt-3.5-turbo variants; no task-specific fine-tuning of large closed-source LLMs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change; hybrid approach (retrieval + multi-agent orchestration + prompting/neural-conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Combines hierarchical embedding retrieval (table/agent/global), explicit sub-query decomposition, a task planning agent that iterates with a harmonizer, specialized expert agents per filing type (acting as tools), simulated swarming for collective decision-making, and neural-conditioning (priming agents with curated positive/negative examples and using HalluciBot to filter/repair queries). This replaces or supplements monolithic fine-tuning by orchestrating multiple smaller, specialized workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improved end-to-end success relative to baseline retrieval and routing approaches: FISHNET achieved 61.8% agentic success vs. reported baseline metrics (RAG baseline retrieval R-Precision ≈45.56% and reported RAG routing accuracy 5.0%; Generative Routing routing accuracy ≈55.7%). Table-level hierarchical embeddings improved R-Precision from agent-level 45.65% to table-level 52.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The paper attributes lower performance on interactive/procedural (agentic) tasks to: difficulty of routing queries across many heterogeneous agents/tables, hierarchical/semantically complex data (especially N-PORT/N-CSR), query vagueness/hallucination risk, and the need for sequential planning and reconciliation across documents — capabilities that simple retrieval or single-turn QA setups do not provide.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e824.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e824.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (Embedding-based retrieval baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (hierarchical embedding ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Embedding-based retrieval baseline used in ablations: 5M datapoints embedded at global / agent / table scopes and retrieved with FAISS; coupled with generation for answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>RAG with hierarchical embedding indices (global / agent / table)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Embedding of line items and metadata using text-embedding-ada-002; FAISS Flat L2 exact KNN retrieval; three index granularities compared (global, agent, table). Retrieval results fed to generation (standard RAG pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Retrieval (R-Precision)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Reported R-Precision (ablation): overall agent-level 45.65%; table-level 52.2% (table-level outperforms agent/global in many filings). Per-filing R-Precision listed in Table 3 (e.g., NPORT agent-level 35.0% vs table-level 44.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported routing accuracy for the RAG routing baseline: 5.0% routing accuracy (paper cites RAG baseline routing accuracy in contributions/abstract).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>hierarchical embedding indices (global/agent/table), FAISS-based exact KNN retrieval, RAG pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>embedding of precomputed text embeddings (text-embedding-ada-002); no end-to-end fine-tuning reported for retrieval/generation coupling in this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Flat or coarse retrieval indices (global) introduce distractors; without specialized routing/orchestration, retrieval alone is insufficient to solve multi-agent, multi-step financial queries — causing low routing accuracy and lower end-to-end success on interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e824.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e824.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative routing (LLM-based routing)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative agent/table routing using semantic generation (gpt-3.5/gpt-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Routing approach that uses generative LLMs (gpt-3.5-turbo-16k, gpt-4) to semantically map queries to the correct agent or table rather than using embedding similarity alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Generative routing (gpt-3.5-turbo-16k / gpt-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM-based semantic classification/generation to select agent and sub-table for a query (no embedding matching). Implemented as LangChain chained Retriever QA agents but selection is done by model generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Pathway routing (agent/table selection) for multi-agent pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>routing / planning initialization (semantic mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported generative routing routing accuracy ≈55.7% (paper reports generative routing baseline and states best performing model for pathway routing was gpt-3.5-turbo-16k-0613). Per-split easy questions: agent selection 82.7%, table selection 99.1%, overall route 82.0%; hard questions show low agent identification (24.9%) but reasonable table identification (74.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>LLM-based semantic routing; used in combination with retrieval; can seed swarming processes; best performing variant identified (gpt-3.5-turbo-16k-0613).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting, few-shot examples, in-context learning; generative decisions made at inference time (no fine-tuning reported).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy / generative routing</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Use generative LLMs to map queries to agents/tables and optionally simulate swarm deliberations (5-agent simulation over 2–3 timesteps) to converge on routing decisions rather than relying solely on embedding similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Generative routing achieves substantially higher pathway routing accuracy compared to the (reported) RAG routing baseline (≈55.7% vs 5.0% reported for RAG routing); gpt-3.5-turbo-16k variant is reported as best for routing.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Generative models can leverage semantics/context to improve routing for many query types, but still fail on hard hierarchical routing because agent identification requires deep schema/ontology understanding across heterogeneous filings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e824.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e824.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hierarchical Embedding Indexing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Global / Agent / Table level embedding indices (hierarchical retrieval ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embedding-index design that partitions the retrieval space at different granularities (global single index; agent-level index per filing type; table-level index per subtable) to study retrieval discriminative power in complex datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Hierarchical embedding indices (global / agent / table)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>5M datapoints indexed into three scopes using text-embedding-ada-002 and FAISS Flat L2 exact KNN: (1) global unified index, (2) agent-level partitions, (3) hyper-local table-level indices per sub-table.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Retrieval (R-Precision)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Table-level indexing improved aggregated R-Precision from agent-level 45.65% to table-level 52.2% (per Table 3). Per-filing gains: e.g., NPORT agent-level 35.0% -> table-level 44.1%; NCSR agent-level 16.8% -> table-level 54.9% (large gain).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Component of agentic pipeline (retrieval used by expert agents and harmonizer)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool-use / retrieval support for multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>granular retrieval indices, FAISS exact KNN, embedding-based retrieval integrated into agent pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>precomputed embeddings (text-embedding-ada-002) and index construction; no model fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change (data/index structure)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Partition retrieval indices at the table-level to reduce distractors and increase discriminative power of KNN retrieval for highly heterogeneous/tabular filings; used in fallback when routing is perfect or near-perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>R-Precision gains reported: overall from ~45.65% (agent-level) to 52.2% (table-level); very large improvement for certain unstructured filings (e.g., NCSR). This improved retrieval quality contributes to better downstream agentic performance though exact end-to-end before/after agentic metrics are not isolated to this intervention alone.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>A global index floods the search with distractors across filing types and tables; partitioning reduces irrelevant candidates and improves retrieval, but does not by itself solve routing or multi-step orchestration challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e824.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e824.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural-conditioning (priming)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural-conditioning / psychological priming of agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conditioning LLM agents via curated positive/negative examples (psychological priming) and grounding/context to steer generation and planning behavior, used to reduce hallucination and guide agent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Neural-conditioning (priming with positive/negative examples and HalluciBot filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agents are 'psychologically primed' with positive and negative examples and ground-truth context; HalluciBot is used to classify hallucination probability and prompt iterative query repair; approach inspired by prior studies (FlowMind, emotional stimulus works).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Agent behavior shaping for multi-step planning and execution</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>prompting-based behavior conditioning for planning / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>priming examples, hallucination detector (HalluciBot), in-context example conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting-based priming and in-context learning rather than supervised fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy / behavioral conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Apply positive and negative examples and a hallucination-probability filter (HalluciBot) at query time to improve the quality of sub-queries, reduce vagueness/hallucination, and better condition downstream planning and agent actions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Paper claims priming and HalluciBot filtering are essential for improving downstream process but provides no isolated quantitative before/after metric; described qualitatively as improving query quality and hence downstream effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Vague or poorly worded queries increase hallucination and reduce effectiveness of multi-stage agent pipelines; neural-conditioning / priming reduces hallucination risk and yields clearer sub-queries for planning and routing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Expert Router: Orchestrating Efficient Language Model Inference through Prompt Classification <em>(Rating: 2)</em></li>
                <li>Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration <em>(Rating: 2)</em></li>
                <li>ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs <em>(Rating: 2)</em></li>
                <li>FlowMind: Automatic Workflow Generation with LLMs <em>(Rating: 2)</em></li>
                <li>Understanding the planning of LLM agents: A survey <em>(Rating: 2)</em></li>
                <li>Conversational Swarm Intelligence, a Pilot Study <em>(Rating: 1)</em></li>
                <li>Artificial Swarm Intelligence, a Human-in-the-Loop Approach to A.I. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-824",
    "paper_id": "paper-273638677",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "FISHNET",
            "name_full": "Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning",
            "brief_description": "A modular multi-agent architecture for extracting and composing financial intelligence from heterogeneous regulatory filings using sub-querying, task planning, a central harmonizer, expert per-document agents (swarm), hierarchical retrieval, and neural-conditioning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "FISHNET multi-agent system",
            "model_description": "Modular agentic system composed of: Sub-querying Agent (query decomposition + HalluciBot filtering), Task Planning Agent (few-shot / ICL plan generation, long-term memory), Harmonizer Agent (aggregates/swaps numeric operations and executes plans), and many Expert Agents (one per filing type). Retrieval uses FAISS vector DB with hierarchical embedding indices; routing uses embedding-based and generative routing combined with simulated swarming.",
            "model_size": null,
            "qa_task_name": "Retrieval (R-Precision on filing data)",
            "qa_performance": "Retrieval reported via R-Precision ablations: agent-level 45.65% and table-level 52.2% (embedding index ablations aggregated across questions).",
            "interactive_task_name": "Multi-filing, multi-entity financial query answering (agentic end-to-end task)",
            "interactive_task_type": "planning / multi-step reasoning / sequential decision-making (agent orchestration + tool-like DB access)",
            "interactive_performance": "End-to-end agentic success rate: 61.8% overall (Easy questions aggregate 81.0%; Hard questions aggregate 28.3%). Per-filing success rates reported in Table 5 (e.g., NPORT 46.4% overall, NMFP 95.6%, etc.).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "sub-querying, task planning module, central harmonizer (orchestrator), expert agents (tool-like specialists), hierarchical retrieval (global/agent/table embeddings), FAISS vector DB, long-term memory for plans, simulated swarming (multi-agent communication), HalluciBot hallucination filter, neural-conditioning (priming with positive/negative examples).",
            "training_method": "few-shot learning and in-context learning (ICL) for planner and sub-query generator; prompting and query-rewriting using gpt-3.5-turbo variants; no task-specific fine-tuning of large closed-source LLMs reported.",
            "intervention_type": "architectural change; hybrid approach (retrieval + multi-agent orchestration + prompting/neural-conditioning)",
            "intervention_description": "Combines hierarchical embedding retrieval (table/agent/global), explicit sub-query decomposition, a task planning agent that iterates with a harmonizer, specialized expert agents per filing type (acting as tools), simulated swarming for collective decision-making, and neural-conditioning (priming agents with curated positive/negative examples and using HalluciBot to filter/repair queries). This replaces or supplements monolithic fine-tuning by orchestrating multiple smaller, specialized workflows.",
            "intervention_effect": "Improved end-to-end success relative to baseline retrieval and routing approaches: FISHNET achieved 61.8% agentic success vs. reported baseline metrics (RAG baseline retrieval R-Precision ≈45.56% and reported RAG routing accuracy 5.0%; Generative Routing routing accuracy ≈55.7%). Table-level hierarchical embeddings improved R-Precision from agent-level 45.65% to table-level 52.2%.",
            "hypothesized_cause_of_gap": "The paper attributes lower performance on interactive/procedural (agentic) tasks to: difficulty of routing queries across many heterogeneous agents/tables, hierarchical/semantically complex data (especially N-PORT/N-CSR), query vagueness/hallucination risk, and the need for sequential planning and reconciliation across documents — capabilities that simple retrieval or single-turn QA setups do not provide.",
            "uuid": "e824.0",
            "source_info": {
                "paper_title": "FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "RAG (Embedding-based retrieval baseline)",
            "name_full": "Retrieval-Augmented Generation (hierarchical embedding ablations)",
            "brief_description": "Embedding-based retrieval baseline used in ablations: 5M datapoints embedded at global / agent / table scopes and retrieved with FAISS; coupled with generation for answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "RAG with hierarchical embedding indices (global / agent / table)",
            "model_description": "Embedding of line items and metadata using text-embedding-ada-002; FAISS Flat L2 exact KNN retrieval; three index granularities compared (global, agent, table). Retrieval results fed to generation (standard RAG pipeline).",
            "model_size": null,
            "qa_task_name": "Retrieval (R-Precision)",
            "qa_performance": "Reported R-Precision (ablation): overall agent-level 45.65%; table-level 52.2% (table-level outperforms agent/global in many filings). Per-filing R-Precision listed in Table 3 (e.g., NPORT agent-level 35.0% vs table-level 44.1%).",
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": "Reported routing accuracy for the RAG routing baseline: 5.0% routing accuracy (paper cites RAG baseline routing accuracy in contributions/abstract).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "hierarchical embedding indices (global/agent/table), FAISS-based exact KNN retrieval, RAG pipeline",
            "training_method": "embedding of precomputed text embeddings (text-embedding-ada-002); no end-to-end fine-tuning reported for retrieval/generation coupling in this baseline.",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Flat or coarse retrieval indices (global) introduce distractors; without specialized routing/orchestration, retrieval alone is insufficient to solve multi-agent, multi-step financial queries — causing low routing accuracy and lower end-to-end success on interactive tasks.",
            "uuid": "e824.1",
            "source_info": {
                "paper_title": "FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Generative routing (LLM-based routing)",
            "name_full": "Generative agent/table routing using semantic generation (gpt-3.5/gpt-4)",
            "brief_description": "Routing approach that uses generative LLMs (gpt-3.5-turbo-16k, gpt-4) to semantically map queries to the correct agent or table rather than using embedding similarity alone.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Generative routing (gpt-3.5-turbo-16k / gpt-4)",
            "model_description": "LLM-based semantic classification/generation to select agent and sub-table for a query (no embedding matching). Implemented as LangChain chained Retriever QA agents but selection is done by model generation.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Pathway routing (agent/table selection) for multi-agent pipeline",
            "interactive_task_type": "routing / planning initialization (semantic mapping)",
            "interactive_performance": "Reported generative routing routing accuracy ≈55.7% (paper reports generative routing baseline and states best performing model for pathway routing was gpt-3.5-turbo-16k-0613). Per-split easy questions: agent selection 82.7%, table selection 99.1%, overall route 82.0%; hard questions show low agent identification (24.9%) but reasonable table identification (74.9%).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "LLM-based semantic routing; used in combination with retrieval; can seed swarming processes; best performing variant identified (gpt-3.5-turbo-16k-0613).",
            "training_method": "prompting, few-shot examples, in-context learning; generative decisions made at inference time (no fine-tuning reported).",
            "intervention_type": "prompting strategy / generative routing",
            "intervention_description": "Use generative LLMs to map queries to agents/tables and optionally simulate swarm deliberations (5-agent simulation over 2–3 timesteps) to converge on routing decisions rather than relying solely on embedding similarity.",
            "intervention_effect": "Generative routing achieves substantially higher pathway routing accuracy compared to the (reported) RAG routing baseline (≈55.7% vs 5.0% reported for RAG routing); gpt-3.5-turbo-16k variant is reported as best for routing.",
            "hypothesized_cause_of_gap": "Generative models can leverage semantics/context to improve routing for many query types, but still fail on hard hierarchical routing because agent identification requires deep schema/ontology understanding across heterogeneous filings.",
            "uuid": "e824.2",
            "source_info": {
                "paper_title": "FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Hierarchical Embedding Indexing",
            "name_full": "Global / Agent / Table level embedding indices (hierarchical retrieval ablation)",
            "brief_description": "An embedding-index design that partitions the retrieval space at different granularities (global single index; agent-level index per filing type; table-level index per subtable) to study retrieval discriminative power in complex datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Hierarchical embedding indices (global / agent / table)",
            "model_description": "5M datapoints indexed into three scopes using text-embedding-ada-002 and FAISS Flat L2 exact KNN: (1) global unified index, (2) agent-level partitions, (3) hyper-local table-level indices per sub-table.",
            "model_size": null,
            "qa_task_name": "Retrieval (R-Precision)",
            "qa_performance": "Table-level indexing improved aggregated R-Precision from agent-level 45.65% to table-level 52.2% (per Table 3). Per-filing gains: e.g., NPORT agent-level 35.0% -&gt; table-level 44.1%; NCSR agent-level 16.8% -&gt; table-level 54.9% (large gain).",
            "interactive_task_name": "Component of agentic pipeline (retrieval used by expert agents and harmonizer)",
            "interactive_task_type": "tool-use / retrieval support for multi-step reasoning",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "granular retrieval indices, FAISS exact KNN, embedding-based retrieval integrated into agent pipeline",
            "training_method": "precomputed embeddings (text-embedding-ada-002) and index construction; no model fine-tuning",
            "intervention_type": "architectural change (data/index structure)",
            "intervention_description": "Partition retrieval indices at the table-level to reduce distractors and increase discriminative power of KNN retrieval for highly heterogeneous/tabular filings; used in fallback when routing is perfect or near-perfect.",
            "intervention_effect": "R-Precision gains reported: overall from ~45.65% (agent-level) to 52.2% (table-level); very large improvement for certain unstructured filings (e.g., NCSR). This improved retrieval quality contributes to better downstream agentic performance though exact end-to-end before/after agentic metrics are not isolated to this intervention alone.",
            "hypothesized_cause_of_gap": "A global index floods the search with distractors across filing types and tables; partitioning reduces irrelevant candidates and improves retrieval, but does not by itself solve routing or multi-step orchestration challenges.",
            "uuid": "e824.3",
            "source_info": {
                "paper_title": "FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Neural-conditioning (priming)",
            "name_full": "Neural-conditioning / psychological priming of agents",
            "brief_description": "Conditioning LLM agents via curated positive/negative examples (psychological priming) and grounding/context to steer generation and planning behavior, used to reduce hallucination and guide agent behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Neural-conditioning (priming with positive/negative examples and HalluciBot filtering)",
            "model_description": "Agents are 'psychologically primed' with positive and negative examples and ground-truth context; HalluciBot is used to classify hallucination probability and prompt iterative query repair; approach inspired by prior studies (FlowMind, emotional stimulus works).",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Agent behavior shaping for multi-step planning and execution",
            "interactive_task_type": "prompting-based behavior conditioning for planning / multi-step reasoning",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": null,
            "architectural_features": "priming examples, hallucination detector (HalluciBot), in-context example conditioning",
            "training_method": "prompting-based priming and in-context learning rather than supervised fine-tuning",
            "intervention_type": "prompting strategy / behavioral conditioning",
            "intervention_description": "Apply positive and negative examples and a hallucination-probability filter (HalluciBot) at query time to improve the quality of sub-queries, reduce vagueness/hallucination, and better condition downstream planning and agent actions.",
            "intervention_effect": "Paper claims priming and HalluciBot filtering are essential for improving downstream process but provides no isolated quantitative before/after metric; described qualitatively as improving query quality and hence downstream effectiveness.",
            "hypothesized_cause_of_gap": "Vague or poorly worded queries increase hallucination and reduce effectiveness of multi-stage agent pipelines; neural-conditioning / priming reduces hallucination risk and yields clearer sub-queries for planning and routing.",
            "uuid": "e824.4",
            "source_info": {
                "paper_title": "FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Expert Router: Orchestrating Efficient Language Model Inference through Prompt Classification",
            "rating": 2,
            "sanitized_title": "expert_router_orchestrating_efficient_language_model_inference_through_prompt_classification"
        },
        {
            "paper_title": "Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration",
            "rating": 2,
            "sanitized_title": "leeroo_orchestrator_elevating_llms_performance_through_model_integration"
        },
        {
            "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
            "rating": 2,
            "sanitized_title": "toolllm_facilitating_large_language_models_to_master_16000_realworld_apis"
        },
        {
            "paper_title": "FlowMind: Automatic Workflow Generation with LLMs",
            "rating": 2,
            "sanitized_title": "flowmind_automatic_workflow_generation_with_llms"
        },
        {
            "paper_title": "Understanding the planning of LLM agents: A survey",
            "rating": 2,
            "sanitized_title": "understanding_the_planning_of_llm_agents_a_survey"
        },
        {
            "paper_title": "Conversational Swarm Intelligence, a Pilot Study",
            "rating": 1,
            "sanitized_title": "conversational_swarm_intelligence_a_pilot_study"
        },
        {
            "paper_title": "Artificial Swarm Intelligence, a Human-in-the-Loop Approach to A.I.",
            "rating": 1,
            "sanitized_title": "artificial_swarm_intelligence_a_humanintheloop_approach_to_ai"
        }
    ],
    "cost": 0.015997499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning
25 Oct 2024</p>
<p>J P Morgan 0000-0001-5516-262X
AI New York
NYUSA</p>
<p>AI New York
NYUSA</p>
<p>AI New York
NYUSA</p>
<p>AI New York
NYUSA</p>
<p>FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning
25 Oct 20243CF99CE462D729081EC4243555F85CCC10.1145/3677052.3698597arXiv:2410.19727v1[cs.AI]CCS ConceptsApplied computing → Economics• Information systems → Information retrieval query processingInformation extraction LLM Agents, Swarming, Harmonizing, Planning, Sub-querying
Financial intelligence generation from vast data sources has typically relied on traditional methods of knowledge-graph construction or database engineering.Recently, fine-tuned financial domainspecific Large Language Models (LLMs), have emerged.While these advancements are promising, limitations such as high inference costs, hallucinations, and the complexity of concurrently analyzing high-dimensional financial data, emerge.This motivates our invention FISHNET (Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert swarming, and Task planning), an agentic architecture that accomplishes highly complex analytical tasks for more than 98,000 regulatory filings that vary immensely in terms of semantics, data hierarchy, or format.FISHNET shows remarkable performance for financial insight generation (61.8% success rate over 5.0% Routing, 45.6% RAG R-Precision).We conduct rigorous ablations to empirically prove the success of FISH-NET, each agent's importance, and the optimized performance of assembling all agents.Our modular architecture can be leveraged for a myriad of use-cases, enabling scalability, flexibility, and data integrity that are critical for financial tasks.</p>
<p>Figure 1: Traditional Swarm Intelligence (SI) vs. FISHNET: SI relies on highly capable individual agents collectively working towards an efficient solution that exceeds the respective capabilities of those agents.SI typically excludes the presence of a central entity that harmonizes the agents' actions.Recently, a plethora of studies have explored an LLM's capability to orchestrate actions.In FISHNET, we combine these two approaches together; a central harmonizer can orchestrate while the expert agents also communicate.</p>
<p>Introduction</p>
<p>Agent-Based Modeling (ABM) in finance has traditionally focused on simulating the interactions amongst market participants or simulating economic scenarios [21].In the realm of financial insight generation, traditional methods of knowledge graph construction have proliferated the research arena [11].In this context, ABM to generate financial intelligence from multiple knowledge bases is a relatively unexplored realm of research for AI in finance.Moreover, What is Bank X's wallet share in equity swaps for U.S. '40Act funds in the latest quarter of 2024?</p>
<p>Harmonizer 1) Reconciles period between NPORT/NMFP/NCSR to generate total equity swap volume 2) Generates equity swaps volume for Bank A using NCEN fund mapping 3) Divides NPORT, NMFP, NCSR, NCEN</p>
<p>Identify the total volume of equity swaps for the latest quarter</p>
<p>Identify Bank A's volume of equity swaps for the same period Calculate wallet: Divide Bank A's volume by the total volume Query (Q) Subquery Generation (q)</p>
<p>Assessment Hallucination Probability</p>
<p>Iterate</p>
<p>Task Plan q 1 q 2 q 3 q 1 NPORT Agents q 2</p>
<p>NPORT, NCEN Agents q 3 Harmonizer Routes &amp; Divides Finds Nothing Finds Bank A's Funds Finds all '40Acts Funds, excluding MMFs, with equity swaps for q1 '24 Finds all Money Market Funds (MMFs) with equity swaps for q1 '24 Finds all '40Acts with equity swaps for full year 2023</p>
<p>Harmonization and Swarming Experts</p>
<p>Revised Plan
q 1 q 2 q 3
NPORT, NMFP, NCSR Bank X's share is 13.7%The plan is communicated to the expert agents and kicks off the swarming process -each expert agent starts its search.(5) The harmonizer synthesizes the information gathered via Swarm Intelligence (SI) and communicates it back to the task planning agent.(6) This enables the task planning agent to revise and optimize its previous plan.(7) The revised plan is then executed by the harmonizer agent and results are communicated back to the user.</p>
<p>swarming Large Language Models (LLMs) in finance is a novel arena that explores the collaboration of multiple LLM agents to achieve complex tasks [12].In the beginning, artificial Swarm Intelligence (SI) as a study drew its motivation by observing emergent behaviors in nature such as swarming bees or schooling fish [29].</p>
<p>In nature, the formation of closed-loop systems among groups of independent agents engenders high-level intelligence which exceeds the capacity of the individual agents [28] (Figure 1).Recently, with the rapid development of LLMs, studies on developing customized tools for expert usage [6], leveraging LLMs as a central orchestrating agent [27], or capitalizing on an LLM's planning capabilities [36] have proliferated the research arena.While these streams are highly promising, firstly, there is a dearth of research into swarming LLMs to gather unknown intelligence from large financial datasets.Secondly, there is also a lack of studies that combine swarming, planning, orchestrating, or sub-querying into an agentic system, especially for use-cases in finance.</p>
<p>Therefore, we propose a novel methodology, FISHNET (Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert swarming, and Task planning), that amalgamates these existing methodologies to generate financial insights, during inference, from a myriad of highly complex financial documents.Instead of fine-tuning LLMs for financial insight generation which can be very costly or prohibitive due to closed-source models [43], FISHNET proposes an effective alternative approach that achieves highly promising results (61.8% successful execution rate) benchmarked against baseline scenarios such as Retrieval Augmented Generation (RAG; 5.0% routing accuracy, 45.56% expected recovery rate) and Generative Routing (55.7% routing accuracy).We conduct rigorous empirical experiments, analyzing 98,034 U.S. regulatory filings submitted to the the Securities and Exchange Commission's (SEC) electronic database, EDGAR (Electronic Data Gathering, Analysis, and Retrieval) [1] and the IAPD (Investment Advisor Public Disclosure) website [2].Instead of the traditional workflow of humans randomly searching for data points within documents or fine-tuning an LLM on these filings, FISHNET leverages the best of all worlds in swarming, harmonizing, neural-conditioning, and task planning for financial documents.This enables FISHNET to generate highly sophisticated responses to any complex query that is imposed such as "Calculate Bank A's wallet share for single-stock swaps for the latest quarter" (Figure 2).</p>
<p>Contributions.In summary, our contributions are as follows:</p>
<p>(1) We propose a novel agentic system to generate highly sophisticated financial intelligence from complex databases, providing flexibility compared to fine-tuning large opensource models for financial analysis.(2) Through our rigorous ablations, we empirically prove the importance of each agent and component involved in FISHNET.Moreover, FISHNET achieves a 61.8% success rate in discovering, routing, and generating a solution for multi-filing, multi-entity queries.(3) FISHNET is moreover, a novel means of employing ABM for database management or intelligence generation, contrary to traditional research of using ABM for market simulation.</p>
<p>2 Related Work 2.0.1 Swarm Intelligence.Multiple studies have delved into the subject of Swarm Intelligence (SI).In particular, AFSA (Artificial Fish Swarm Algorithm) strives to mimic the behavior of fish via a stochastic method that searches for a myriad of solutions in a randomized procedure [25].AFSA emulates three main behaviors that schools of fish exhibit: the first behavior is preying, the second is swarming, where fish tend to avoid danger by assembling in a group, and the third is executing without explicit leadership, via a biological organ that detects infinitesimal pressure changes in neighboring environments [30].Thus, AFSA initializes n randomly distributed artificial fish across the search space, where n is the population size.Each fish searches for positions with higher fitness values -ultimately, at the end of each iteration, the position with the best fitness value is compared with the previously archived solution in AFSA's bulletin board.If the new position is better, the bulletin will be updated.This iteration persists until the stop criterion is met [25].Rosenberg et al. [30] dives deeper in his recent study on collective super intelligence enabled via LLM agents.These agents serve as the organ that can participate in simultaneous actions and communicate with each other.A key characteristic of SI is that there is no central entity that orchestrates task management.</p>
<p>2.0.2LLMs and Harmonization/Orchestration.Pichlmeier et al. [24] introduces the highly modular architecture of Expert Router that orchestrates the performance of multiple experts -diving deep into the capabilities of an LLM to serve as the central harmonizer of experts.Similarly, Mohammadshahi et al. [19] investigates the abilities of an LLM-based orchestrator that can effectively pick the expert agents for optimized task execution with regards to the input query.It is important to highlight that orchestration diverges from Mixture of Experts (MoE) modeling as the latter requires the router to select experts from the MoE layer; the MoE layer can then be distributed through multiple GPUs via Model Parallelism or Expert Parallelism techniques [10].In contrast, orchestration does not imperatively demand that the experts be housed in a single layer.</p>
<p>The efficient role of LLMs as an orchestrator will be leveraged in FISHNET.</p>
<p>2.0.3LLMs and Neural-Conditioning. Li et al. [15] has delved into the emotional intelligence capabilities of LLMs and how contextual prompts can serve as a stimuli to significantly increase the performance of generative tasks.In contrast, Wang et al. [37] investigates the effects of negative emotional stimuli that also enhances the performance of LLMs.On a slightly different note, in Zeng et al. [45], FlowMind empirically proves how grounding the LLM with context also helps improve its performance.These studies spawn the potential of neurally conditioning or psychological priming an LLM to act in certain way.In FISHNET, we psychologically prime our agents via positive and negative examples to act within expectations.</p>
<p>LLMs and Expert</p>
<p>Agents.The usage of tools or expert agents is a heavily explored arena of current academic research on LLMs [26].For example, Qin et al. [26] introduces ToolLLaMA which has amalgamated 16,464 Application Programming Interfaces (APIs) that are leveraged in the construction of a valid solution.ToolL-LaMA has proven to be effective in not only successfully solving complex tasks but also generalizing to new APIs.Zeng et al. [45] has empirically proven the remarkable performance of LLMs leveraging domain-specific tools or APIs.Liu et al. [17] estimates the usage of graphs in which each node is a tool -this invention, ToolNet, is scalable to thousands of tools and performs well on a myriad of multi-hop reasoning tasks.Schimanski et al. [31] investigates the deployment of LLM specialists for Evidence-Based Question-Answering.This highly prevalent area of utilizing experts or tools will be expanded on in FISHNET, as we construct expert agents for each type of document.</p>
<p>2.0.5 LLMs and Task Management.Amongst LLMs' emergent capabilities that are not directly related to their original task of word sequence completion, their ability to conduct task management or sequential decision making is one of the prevalent areas of study [36].Huang et al. [8] conducts a comprehensive study into current research that investigates LLMs' planning capabilities.This paper categorizes planning abilities into one of the following five areastask decomposition, multi-plan selection, external planner-aided planning, reflection and refinement, and memory-augmented planning.In this context, Pallagani et al. [23] studies the integration of LLMs into Automated Planning and Scheduling (APS), leveraging the highly nuanced language capabilities of LLMs in traditional APS scenarios.</p>
<p>2.0.6 LLMs and Finance.Generative AI has been applied to various NLP tasks within the financial sector [5], from predictive modeling to sentiment analysis [3, 40-42, 47, 47].Recently, BloombergGPT was released, trained on a myriad of documents, encompassing news, filings, press releases, and social media from Bloomberg archives [43].While impressive in its asserted capabilities, the model is entirely proprietary and very costly to train.A number of LLMs have been fine-tuned on finance-related tasks [4,14,18,44,46]; however, rarely do they leverage LLMs for coding.Therefore, the motivation to derive financial insights from large amounts of data, without having to undergo the costly process of fine-tuning, remains strong.This serves as the backbone for our efforts to compose FISHNET.</p>
<p>2.0.7 LLMs and Coding.LLMs, especially in their potential for code generation, have seen considerable exploration and advancement [22].These studies have explored chain of thought through code, as demonstrated in [16] for robotic programs, web browsing [20], or table question-answering [39].Recently, LLMs have shown the ability to construct modular code for visual question answering based on abstractions of high-level APIs [32].FISHNET will leverage LLMs for coding workflow structures in a multi-agent framework.</p>
<p>Methodology 3.1 Sub-querying Agent</p>
<p>As a query is imposed to FISHNET, the sub-querying process consists of firstly assessing the query's quality or probability of hallucination, leveraging HalluciBot [38,40].We utilize HalluciBot's binary classifications of hallucinatory or non-hallucinatory queries to continuously iterate towards a positive class transition.This process is essential since vague or poorly-worded queries, especially in searching across multiple databases, can significantly hinder the effectiveness of the downstream process [9].Then, the subquerying process takes a query Q and outputs a set of q sub-queries: Q = { 0 ,  1 , . . .,   }.To accomplish this, we leverage in-context learning (ICL) with an LLM to generate a sub-query q that is optimized for the task management or planning stage.</p>
<p>Task Planning Agent</p>
<p>The Task Planning Agent's primary objective is to initialize the planning stage for each sub-query, collaborate with the Harmonizer Agent (Section 3.3), and update FISHNET's long-term memory with an optimized plan by leveraging the Swarm Intelligence (SI) generated by Expert Agents (Section 3.4).Firstly, the Task Planning Agent takes the sub-queries q and starts devising a plan that will accomplish the resolution of each sub-query.The first iteration of this planning process is done solely by the Task Planning Agent, that has been given few-shot examples and In-Context Learning (ICL) to have a basic understanding of what each Expert Agent can accomplish.Secondly, the Task Planning Agent communicates with the Harmonizer Agent which synthesizes the SI compiled by multiple Expert Agents.The SI can encompass heretofore unknown information for the Task Planning Agent -as the latter has only been trained with few-shot learning whilst the Expert Agents are comprehensive and detailed experts for their respective filings.Therefore, the Task Planning Agent can revise its initial plan and devise an optimized plan, that will also be stored in long-term memory for future reference.</p>
<p>Harmonizer Agent</p>
<p>The Harmonizer Agent's prime focus is to communicate with each Expert Agent, synthesize the SI, and communicate it back to the Task Planning Agent; therefore, the initial plan can be refined using SI.The Harmonizer's next key role is to execute the optimized plan.In other words, while the Expert Agents retrieve the relevant data from their respective databases, the Harmonizer can aggregate, multiply, divide, or perform any arithmetic actions, by following the optimized plan.</p>
<p>Expert Agents</p>
<p>We assign an Expert Agent for each type of U.S. regulatory filing in the document ingestion pipeline.Each Expert will be fully capable of understanding all data fields in the filing, requirements, format, frequency of submission, and any other minute details.As regulatory filings differ immensely in all these aspects, a specialized agent for each filing will enable a truly modularized and expertdriven approach to assembling insights.For all filings, amendments to original filings are submitted on an ad-hoc basis -therefore, reconciliation across different timelines is key.</p>
<p>3.4.1 N-PORT Agent.The N-PORT Agent fully comprehends and manipulates every single data field, ontology, hierarchy, filing entity (whether it is a fund, trust, or asset manager), and filing frequency for Form N-PORT.Form N-PORT or the Monthly Portfolio Investments Report is a mandatory filing for all registered management investment companies ('40Acts) or an Exchange-Traded Fund(ETF) that is organized as a Unit-Investment Trust (UIT).It is critical to note that this filing is not required for Money-Market Funds (MMFs).While filed every month, Form N-PORT is released every quarter regarding each fund's portfolio and each investment holding within the portfolio as of the last business day, or calendar day, of the last month in the quarter [1].</p>
<p>N-PORT Dataset</p>
<p>We ingest more than a full year's submission of N-PORT filings, starting from the 1st quarter of 2023 to the 2nd quarter of 2024.</p>
<p>We have ingested a grand total of 40,372 original filings and 1,076 amendments that covers all portfolio holdings for all '40Act funds.We scrape these reports at a maximum throughput of 10 reports (SEC site limits for a single host).Once the filings are downloaded, we index each filing according to its unique ID.</p>
<p>N-MFP Dataset</p>
<p>Similarly to N-PORT, we ingest six quarters of filings into our data pipeline for N-MFP.This amounts to a grand total of 3,574 original filings and amendments.We extract and process the data into tables via indexing and chronological ordering.</p>
<p>ADV Dataset</p>
<p>We crawl the IAPD website to ingest Form ADV for the past year, counting 15,292 filings or more than 685,000 PDF pages in total.</p>
<p>N-CEN Dataset</p>
<p>We ingest 2,909 N-CEN filings for the past 6 quarters.N-CEN is an annual filing that can also be filed on a group level.Therefore, we reconcile the unique trust-level identifiers so every filing is indexed in a relational database.</p>
<p>N-CSR Dataset</p>
<p>2,756 N-CSR filings are ingested in our dataset for the past six quarters.Form N-CSR varies greatly from filer to filer and is a highly unstructured type of filing.The data ingestion pipeline for N-CSR is one of most robust, leveraging multiple Natural Language Processing (NLP) and computer vision techniques to process the data from these filings.</p>
<p>13F Dataset</p>
<p>For the past six quarters, we ingest 25,544 13F Holdings Reports, 5,353 13F Notices, 1,122 13F Holdings Reports Amendments and 36 13F Notice Amendments.This amounts to 32,055 unique filings and amendments.We create citational knowledge graphs to track the relationships between filers.</p>
<p>Metrics</p>
<p>We evaluate our experiments across three dimensions:</p>
<p>(1) Retrieval -We use R-Precision across all queries to account for variable retrieval sizes.(2) Routing -We use accuracy to assess the LLM's effectiveness to route queries to the correct agents/tables in a zero-shot setting.</p>
<p>(3) Agentic -We use the success rate of creating an accurate solution to judge the full system architecture.</p>
<p>R-Precision</p>
<p>R-Precision is a metric used to evaluate the performance of an information retrieval system.It is defined as the precision at , where  is the number of relevant documents for a given query.</p>
<p>R-Precision = |Relevant Docs.∩ Retrieved Docs. at |  In the context of embedding-based retrieval, let  be the total number of relevant documents for a query.Then R-Precision is calculated as the proportion of relevant documents retrieved in the top  positions of the ranked list of retrieved documents.Therefore, R-Precision is equivalent to both the precision at the -th position (@) and the recall at the -th position.</p>
<p>Accuracy of Pathway Routing for Agents</p>
<p>In evaluating the performance of pathway routing for agents, it is essential to consider the conditional accuracy of correctly identifying the agent and subsequently selecting the appropriate subtable.This two-step accuracy measure ensures that the overall routing process is evaluated comprehensively.</p>
<p>Definition. The accuracy of pathway routing is evaluated in two steps:</p>
<p>(1) Agent Identification Accuracy: The proportion of instances where the correct agent, A, is identified, as   =  () (2) Sub-table Identification Accuracy: Given the correct agent, A, is identified, the proportion of instances where the correct sub-table, S, is selected, as
𝐴𝑐𝑐 𝑠𝑢𝑏𝑡𝑎𝑏𝑙𝑒 = 𝑃 (𝑆 | 𝐴)
The overall accuracy   can be considered as the product of these two accuracies, reflecting the conditional nature of the</p>
<p>Question Creation &amp; Augmentation</p>
<p>We define 7 easy and 4 hard question templates and their canonical code solution to generate random samples.Each sample is curated such that a complete answer is guaranteed -the sampling source is from the total space of valid inputs that yields solutions.The original templates are useful for measuring the repeatability and reliability of our agent framework.To test the robustness of our system architecture, we inject noise into the templates by using gpt-3.5-turboas a query re-writer to generate 2 variations.Each question is tagged with the appropriate route for the agent and table.Finally, each canonical solution is executed to produce a solution and the relevant records required for it.</p>
<p>Ablations and Results</p>
<p>6.</p>
<p>3.1 Embedding Based (RAG).We embedded 5 million datapoints into 3 variable scopes: global, agent, and table level search spaces to test the most reliable retrieval for data discovery.R-Precision, our retrieval metric, demonstrates that hierarchical embedding data is preferable to a flat global structure (Table 3).The highest gain in R-Precision is drawn from NCSR, as a bespoke embedding space allows for a cleaner KNN retrieval on a single table (vs. 3 tables at the agent level).</p>
<p>(1) Global -each data row is indexed into one unified index.By inundating the search space with millions of distractor points, global measures the accretive value of segmented indexes by agents or tables.However, given that our agents and tables are subsets of the global embedding space, we do not directly ablate this split and hypothesize that, at best, it will perform equivalent to the table-level ablation.</p>
<p>(2) Agent -each data row is indexed according to its respective agent.This partitions the search space to a single filing, measuring the intra-discriminative power of embeddings to retrieve when isolated from other filing and agent types.(3) Table -each data row is indexed per sub-table.Therefore, this hyper-local search space experiments with how well any RAG system could retrieve data within a single table, an alternative to SQL querying.In cases of perfect routing, this ablation explores the discriminative power of embeddings to discover valid records from irrelevant ones.</p>
<p>6.3.2Routing.We explore routing based methods, both embedding and generative.The Harmonizer agent (Section 3.3) would route to the correct expert agent leveraging a FAISS vector database populated with the embeddings of descriptions (Section 6.1) for each agent.The Expert agent (Section 3.4) uses a database containing the embeddings of a list of tables and the schema affiliated with each agent to perform routing.They are implemented as LangChain1 chained Retriever QA agents that treats the vector database as as retriever.To empirically assess the accuracy of pathway routing, we employ a confusion matrix to identify agents and conditional probabilities to select sub-tables.Table 4 has a breakdown between Agents, Tables, and the Overall routing accuracy.</p>
<p>(1) Embedding -We route a query based on matching its embedding with that of the agents' personas or tables' descriptions in a RAG setting.(2) Generative -We use gpt-3.5-turbo-16kand gpt-4 to isolate and link a query to an agent or a table based exclusively on semantic generation.(3) Swarming -Using the generative guided query as a starting point, we simulate 5 agents performing collective-decision making over 2-3 timesteps to converge on a final answer using both the semantic generation of the query, but also all agents' reasoning history.</p>
<p>As expected, the easy questions score 82.7% at correctly selecting the agent, 99.1% correct in selecting the table, and 82.0%correct in determining the overall route.Easy questions, as single-agent focused or simple joins, had a high accuracy when juxtaposed to hard questions.These more difficult questions require more complex understanding of deeply hierarchical data in NPORT, leading to high accuracy in table identification (74.9%), yet low agent identification accuracy (24.9%).The best performing model for pathway routing was gpt-3.5-turbo-16k-0613.6.3.3Agentic.In Table 5, we see that FISHNET is able to achieve a success rate of agent, table, and data discovery of 61.8%.However, easy questions score in aggregate 81.0%, compared to hard questions at 28.3%.Furthermore, we see that variations actually perform slightly better, as the noise injected by gpt-3.5-turboallows for our experiments to have more flexibility in recovering the answer, rather than fail on the same subset of templates repeatedly.Furthermore, query rewriting allows for an LLM to clarify a question conditioned on the original question, creating better questions.</p>
<p>Conclusion</p>
<p>In conclusion, we propose FISHNET, a multi-agent system to generate sophisticated financial insights.FISHNET combines swarming, sub-querying, harmonizing, planning, and neural-conditioning in a single system, demonstrating robust performance of 61.8% in crafting accurate solutions while navigating a complex, hierarchical agent-table data structure.As a novel means of leveraging ABM for financial data analysis, FISHNET provides a tangible alternative to fine-tuning or database engineering.Future work on FISHNET can focus on fine-tuning for different types of datasets, possibly non-English financial datasets that differ greatly in format or semantics.</p>
<p>Disclaimer</p>
<p>This paper was prepared for informational purposes by the Artificial Intelligence Research group of JPMorgan Chase &amp; Co. and its affiliates ("JP Morgan") and is not a product of the Research Department of JP Morgan.JP Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein.This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful.</p>
<p>Figure 2 :
2
Figure 2: System Diagram of FISHNET: For any imposed query, (1) the query's hallucination probability is firstly calculated to improve on the query's quality.(2) Then, our sub-querying agent breaks down the query into different components.(3) Our task planning agent, leveraging basic In-Context Learning (ICL) and few-shot examples, devises the first iteration of the plan based on the sub-queries.(4) The plan is communicated to the expert agents and kicks off the swarming process -each expert agent starts its search.(5)The harmonizer synthesizes the information gathered via Swarm Intelligence (SI) and communicates it back to the task planning agent.(6)This enables the task planning agent to revise and optimize its previous plan.(7)The revised plan is then executed by the harmonizer agent and results are communicated back to the user.</p>
<ol>
<li>4 .
4
2 N-MFP Agent.The N-MFP Agent precisely understands all key requirements and data fields of Form N-MFP.Form N-MFP is the monthly public reporting form used by money market funds required by section 30(b) of the Act and rule 30b1-7 under the Act (17 CFR 270.30b1-7).Similarly to Form N-PORT, Form N-MFP must report information about the fund and its portfolio holdings as</li>
</ol>
<p>Table 1 :
1
Overview of Investment Filings and Expert Agents.We enumerate for each filing who is required to file, what fields of interest are included, and how often the form must be filed.All of our filings can be amended by the filer, therefore all records must be reconciled to replace deprecated information.We provide references to the legal regulation and database where these filings are recorded.
FilingWho?What?Frequency? Structured? Completeness? Amended? Source Regulation13FInvestment ManagersHoldingsQuarterlyXMLPartialYes[1][33]N-CSR'40 Act FundsAnnual ReportAnnualTextMulti-FundYes[1][35]N-CEN'40 Act FundsCensusAnnualXMLMulti-FundYes[1][35]N-PORT'40 Act FundsHoldingsQuarterlyXMLSingle-FundYes[1][35]N-MFPMoney Market FundsHoldingsMonthlyXMLSingle-FundYes[1][35]ADVInvestment Advisors Entity InformationAnnualTextYesYes[2][34]</p>
<p>Table 2 :
2
Overview of Question Types.Each question is linked to the required Agent, and the median/total number of data points required to correctly answer each sample question.Hard questions require either: a) extraction from highly unstructured reports (Q3), or b) multiple agents (Q0, Q1, Q2).
Question
3.4.3ADVAgent.The ADV Agent is skilled at understanding the Investment Adviser Public Disclosure (IAPD) website and the two different forms that comprise Form ADV.Form ADV is filed annually by an Investment Advisor (IA) that has to register with the SEC or state authorities.It houses different types of information from N-PORT or N-MFP -such as business ownership, employees, clients, or disciplinary information pertaining [2].3.4.4 N-CEN Agent.The N-CEN Agent is an expert of Form N-CEN, an annual filing required for all registered investment companies ('40Acts), other than face-amount certificate companies.The Agent understands the frequency of submission as well as the precise data fields that include provision of financial support, principal underwriters, fund type, or investments in foreign corporations[1].A notable point is that N-CEN can be submitted on a trust-level, with the trust referring to a group of funds.vestment managers which exercise discretion for accounts holding Section 13(f) securities with a a certain market value needs to file 13F [1].Our Agent is highly skilled at understanding not only the ontology of 13F but also its unique filing structure.For FISHNET's training dataset, we have ingested 98,034 U.S. regulatory filings for six different types of filings from EDGAR and IAPD.These six filing types differ immensely in terms of ontology, format, submission requirements, citations, and data hierarchies.Moreover, they serve as critical sources of information for retail investors, brokerage firms, asset managers, and corporate entities.</p>
<p>Table 3 :
3
Ablation Studies on Embedding Architectures.Metrics are R-Precision.We report the recovery rate per filing datapoints, aggregated on each of our questions.The best results for each model are underlined.Filing 13F only had a single table, therefore the Agent and Table level R-Precision are equivalent.
R-Precision (%, ↑)FilingCount Agent Level Table Level13F60048.748.7ADV60068.586.0NCEN30075.180.6NPORT1,80035.044.1NCSR30016.854.9NMFP58851.060.8Overall4,18845.652.2</p>
<p>Table 4 :
4
[7,13]on Studies on Routing Architecture.The best accuracy for each task, per split, is underlined.(E)refers to solely embedding-based strategies, while (G) refers to generative strategies.Hard questions that require more than two tables are evaluated sequentially and scored with partial credit for identifying at least one table right.We used text-embedding-ada-002 on each individual line item per table, as well on each table's and agent's metadata descriptions.Null value fields are dropped from the JSON representation.Across all agents and tables, this yields 5,052,421 vector embeddings at a dimensionality of 1,536 (31 GB of memory at float32 precision).Embeddings are indexed by FAISS[7,13]using a Flat Euclidean (L2) Index for exact k-Nearest Neighbors (KNN).
Accuracy (%, ↑)</p>
<p>Table 5 :
5
Agentic Success Rates (higher the better) on our dataset.We provide splits according to each filing, difficulty, and in aggregate.Note that when reporting on a filing-level, we evaluate the success rate equally if a query requires two filings.Therefore the total count is higher than the total actual number of queries(4,200 vs. 3,300).
Success Rate (%, ↑)SplitCount Templated Variegated BothFiling -Level13F60071.070.3 70.5ADV60070.572.3 71.6NCEN1,20080.289.3 86.3NCSR3003.025.0 17.7NMFP300100.093.4 95.6NPORT1,20039.350.0 46.4Easy QuestionsQ030078.067.5 71.0Q130064.073.0 70.0Q230085.077.5 80.0Q330094.099.0 97.3Q430056.067.0 63.3Q530085.092.5 90.0Q6300100.093.4 95.6All Easy2,10080.281.4 81.0Hard QuestionsQ03009.021.5 17.3Q130042.072.5 62.3Q230021.013.5 16.0Q33003.025.0 17.6All Hard1,20018.833.1 28.3Overall3,30057.863.8 61.8
https://www.langchain.com</p>
<p>Dogu Araci, arXiv:1908.10063[cs.CLFinBERT: Financial Sentiment Analysis with Pre-trained Language Models. 2019</p>
<p>BizGraphQA: A Dataset for Image-based Inference over Graph-structured Diagrams from Business Domains. Petr Babkin, William Watson, Zhiqiang Ma, Lucas Cecchi, Natraj Raman, Armineh Nourbakhsh, Sameena Shah, 10.1145/3539618.3591875Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information RetrievalTaipei, Taiwan; New York, NY, USAAssociation for Computing Machinery2023SIGIR '23)</p>
<p>Innovative Application of Artificial Intelligence Technology in Bank Credit Risk Management. International Journal of. Shuochen Bi, Wenqing Bao, 10.62051/ijgem.v2n3.08Global Economics and Management. 232024. April 2024</p>
<p>Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou, arXiv:2305.17126[cs.LGLarge Language Models as Tool Makers. 2024</p>
<p>Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, Hervé Jégou, arXiv:2401.08281[cs.LGThe Faiss library. 2024</p>
<p>Understanding the planning of LLM agents: A survey. Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, Enhong Chen, arXiv:2402.02716[cs.AI2024</p>
<p>Query Expansion by Prompting Large Language Models. Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, Michael Bendersky, arXiv:2305.03653[cs.IR2023</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Yang, arXiv:2401.04088[cs.LGMixtral of Experts. Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed, 2024</p>
<p>Xuhui Jiang, Chengjin Xu, Yinghan Shen, Xun Sun, Lumingyuan Tang, Saizhuo Wang, Zhongwu Chen, Yuanzhuo Wang, Jian Guo, arXiv:2310.04835[cs.AIOn the Evolution of Knowledge Graphs: A Survey and Perspective. 2023</p>
<p>Swarm-GPT: Combining Large Language Models with Safe Motion Planning for Robot Choreography Design. Aoran Jiao, Tanmay P Patel, Sanjmi Khurana, Anna-Mariya Korol, Lukas Brunke, K Vivek, Utku Adajania, Siqi Culha, Angela P Zhou, Schoellig, arXiv:2312.01059[cs.RO2023</p>
<p>Billion-Scale Similarity Search with GPUs. Jeff Johnson, Matthijs Douze, Hervé Jégou, 10.1109/TBDATA.2019.2921572IEEE Transactions on Big Data. 720213 (2021</p>
<p>Jean Lee, Nicholas Stevens, Soyeon , Caren Han, Minseok Song, arXiv:2402.02315[cs.CLA Survey of Large Language Models in Finance (FinLLMs). 2024</p>
<p>Large Language Models Understand and Can be Enhanced by Emotional Stimuli. Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, Xing Xie, arXiv:2307.11760[cs.CL2023</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Xukun Liu, Zhiyuan Peng, Xiaoyuan Yi, Xing Xie, Lirong Xiang, Yuchen Liu, Dongkuan Xu, arXiv:2403.00839[cs.AIToolNet: Connecting Large Language Models with Massive Tools via Tool Graph. 2024</p>
<p>Xiao-Yang Liu, Guoxuan Wang, Daochen Zha, arXiv:2307.10485FinGPT: Democratizing Internet-scale Data for Financial Large Language Models. 2023. 2023arXiv preprint</p>
<p>Alireza Mohammadshahi, Ali Shaikh, Majid Yazdani, arXiv:2401.13979[cs.CLLeeroo Orchestrator: Elevating LLMs Performance Through Model Integration. 2024</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, arXiv:2112.09332[cs.CL]WebGPT: Browser-assisted question-answering with human feedback. Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman, 2022</p>
<p>Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M Mulvey, H Vincent Poor, Qingsong Wen, Stefan Zohren, arXiv:2406.11903A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges. 2024qfin.GN</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong, arXiv:2203.13474Codegen: An open large language model for code with multi-turn program synthesis. 2022. 2022arXiv preprint</p>
<p>On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling. Vishal Pallagani, Kaushik Roy, Bharath Muppasani, Francesco Fabiano, Andrea Loreggia, Keerthiram Murugesan, Biplav Srivastava, Francesca Rossi, Lior Horesh, Amit Sheth, arXiv:2401.02500[cs.AI2024APS</p>
<p>Josef Pichlmeier, Philipp Ross, Andre Luckow, arXiv:2404.15153[cs.CLExpert Router: Orchestrating Efficient Language Model Inference through Prompt Classification. 2024</p>
<p>A review of artificial fish swarm algorithms: recent advances and applications. Farhad Pourpanah, Ran Wang, Chee Peng Lim, Xi-Zhao Wang, Danial Yazdani, 10.1007/s10462-022-10214-4Artificial Intelligence Review. 5632022. June 2022</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun, arXiv:2307.16789[cs.AIToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. 2023</p>
<p>Sumedh Rasal, E J Hauer, arXiv:2402.16713[cs.MANavigating Complexity: Orchestrated Problem Solving with Multi-Agent LLMs. 2024</p>
<p>Artificial Swarm Intelligence, a Human-in-the-Loop Approach to A.I. Louis Rosenberg, 10.1609/aaai.v30i1.9833Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2016. Mar. 2016301</p>
<p>Louis Rosenberg, Gregg Willcox, Hans Schumann, Miles Bader, Ganesh Mani, Kokoro Sagae, Devang Acharya, Yuxin Zheng, Andrew Kim, Jialing Deng, arXiv:2309.03220[cs.HCConversational Swarm Intelligence, a Pilot Study. 2023</p>
<p>Louis Rosenberg, Gregg Willcox, Hans Schumann, Ganesh Mani, arXiv:2401.15109[cs.HCTowards Collective Superintelligence: Amplifying Group IQ using Conversational Swarms. 2024</p>
<p>Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus Leippold, arXiv:2402.08277[cs.CLTowards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering. 2024</p>
<p>Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, Dan Klein, arXiv:2306.05392[cs.CL]Modular Visual Question Answering via Code Generation. 2023</p>
<p>U S Congress, Codified at 15 U.S.C. § 78a et seq.Securities Exchange Act of 1934. 1934</p>
<p>Investment Advisers Act of. U S Congress, 1940. 1940U.S.Cet seq.</p>
<p>Investment Company Act of. U S Congress, 1940. 1940U.S.Cet seq.</p>
<p>Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2305.15771[cs.AIOn the Planning Abilities of Large Language Models : A Critical Investigation. 2023</p>
<p>Xu Wang, Cheng Li, Yi Chang, Jindong Wang, Yuan Wu, arXiv:2405.02814[cs.CLNegative-Prompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli. 2024</p>
<p>HalluciBot: Is There No Such Thing as a Bad Question?. William Watson, Nicole Cho, arXiv:2404.12535v1[cs.LG2024</p>
<p>HiddenTables and PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies. William Watson, Nicole Cho, Tucker Balch, Manuela Veloso, 10.18653/v1/2023.emnlp-main.442Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2023</p>
<p>William Watson, Nicole Cho, Nishan Srishankar, arXiv:2404.12535[cs.LGIs There No Such Thing as a Bad Question? H4R: HalluciBot For Ratiocination, Rewriting, Ranking, and Routing. 2024</p>
<p>Financial table extraction in image documents. William Watson, Bo Liu, 10.1145/3383455.3422520Proceedings of the First ACM International Conference on AI in Finance. the First ACM International Conference on AI in FinanceNew York, New York; New York, NY, USA, ArticleAssociation for Computing Machinery202151ICAIF '20)</p>
<p>Directed Criteria Citation Recommendation and Ranking Through Link Prediction. William Watson, Lawrence Yong, arXiv:2403.18855[cs.SI2024</p>
<p>Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann, arXiv:2303.17564[cs.LGBloombergGPT: A Large Language Model for Finance. 2023</p>
<p>Hongyang Yang, Xiao-Yang Liu, Christina Dan Wang, arXiv:2306.06031FinGPT: Open-Source Financial Large Language Models. 2023. 2023arXiv preprint</p>
<p>Zhen Zeng, William Watson, Nicole Cho, Saba Rahimi, Shayleen Reynolds, Tucker Balch, Manuela Veloso, arXiv:2404.13050[cs.CLFlowMind: Automatic Workflow Generation with LLMs. 2024</p>
<p>Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models. Boyu Zhang, Hongyang Yang, Xiao-Yang Liu, arXiv:2306.126592023. 2023arXiv preprint</p>
<p>Ran Zmigrod, Dongsheng Wang, Mathieu Sibue, Yulong Pei, Petr Babkin, Ivan Brugere, Xiaomo Liu, Nacho Navarro, Antony Papadimitriou, William Watson, Zhiqiang Ma, arXiv:2404.04003[cs.CLArmineh Nourbakhsh, and Sameena Shah. 2024. BuD-DIE: A Business Document Dataset for Multi-task Information Extraction. </p>            </div>
        </div>

    </div>
</body>
</html>
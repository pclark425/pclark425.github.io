<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4477 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4477</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4477</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-281103687</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.03957v1.pdf" target="_blank">CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking</a></p>
                <p><strong>Paper Abstract:</strong> The effectiveness of large language models (LLMs) to fact-check misinformation remains uncertain, despite their growing use. To this end, we present CANDY , a benchmark designed to systematically evaluate the capabilities and limitations of LLMs in fact-checking Chinese misinformation. Specifically, we curate a care-fully annotated dataset of ∼ 20k instances. Our analysis shows that current LLMs exhibit limitations in generating accurate fact-checking conclusions, even when enhanced with chain-of-thought reasoning and few-shot prompting. To understand these limitations, we develop a taxonomy to categorize flawed LLM-generated explanations for their conclusions and identify factual fabrication as the most common failure mode. Although LLMs alone are unreliable for fact-checking, our findings indicate their considerable potential to augment human performance when deployed as assistive tools in scenarios. Our dataset and code can be accessed at https://github.com/SCUNLP/CANDY .</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4477.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4477.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CANDY</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CANDY benchmark for LLM fact-checking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comprehensive benchmark and evaluation framework introduced in this paper to systematically measure LLM capabilities, limitations, and assistive potential for fact-checking Chinese misinformation via three tasks (Fact-Checking Conclusion, Fact-Checking Explanation, LLM-Assisted Fact-Checking).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>CANDY three-task evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluates LLMs across three progressive tasks: (1) Fact-Checking Conclusion — binary veracity classification of claims; (2) Fact-Checking Explanation — generation and manual assessment of explanatory rationales for conclusions; (3) LLM-Assisted Fact-Checking — human subject experiments measuring how LLMs assist human decision-making. The framework uses contamination vs contamination-free splits (model cutoff-aware) and standard prompting variants (zero/few-shot, with/without Chain-of-Thought). It combines automated metrics for conclusions with large-scale manual annotation and a human study to probe both autonomous and assistive roles.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>For conclusions: accuracy and F1 (micro-averaged). For explanations: reliability judged by taxonomy dimensions (Factuality Hallucination, Reasoning Inadequacy, Faithfulness Hallucination) and per-category counts/proportions; for assistive role: human accuracy and efficiency (comparison across Human, Human+Web, Human+LLM, Human+LLM+Web). Also measures temporal awareness (contamination-free performance) and calibration/overconfidence.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Misinformation fact-checking (Chinese, multi-domain: politics, health, society, disasters, life, culture, science)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Explanations / veracity determinations (rationales supporting truth/falsity decisions); not formal scientific theories but explanatory/hypothesis-like outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Applied to 16 LLMs and 3 LRMs on CANDYSET (~20k instances). Top-performing model GPT-4o achieved ~76.2% accuracy and 76.1% F1 (contamination-free reported values; contamination results reported in parentheses across tables). Contamination-free evaluation led to an average drop of 6.9% accuracy and 7.5% F1 versus contamination evaluation. Across explanation analysis, 91.2% of incorrect conclusions were associated with flawed explanations; manual annotation found 4,891 flawed explanations (428 of these still produced correct conclusions). Domain variation observed: higher accuracy in knowledge-intensive domains (~80+%), lower in temporal-sensitive domains (society/disasters ~69–74%). Human study: Human+LLM and Human+LLM+Web outperformed Human-only and Human+Web across education groups (exact table values in Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated metrics (accuracy, F1, calibration metrics) for conclusion task and for model-run statistics; extensive human annotation for explanation quality (two annotators + adjudicator per item) and an IRB-approved human study to measure assistive potential.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Quality control via authoritative sources, sampling checks (3% sample re-annotated), inter-annotator agreement measured with Fleiss' Kappa (0.75 for label validation, 0.76 for explanation annotation); decision-tree annotation process and adjudication by third annotator for disagreements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Prompt sensitivity (evaluations depend on prompt design); explanation annotation was limited to 11 LLMs due to manual cost; dataset restricted to Chinese (limits cross-lingual generalizability); contamination-free evaluation highlights temporal knowledge cutoffs as a major challenge; CoT and few-shot prompting can increase overconfidence in some models; LLMs prone to factual fabrication and logical/contextual inconsistencies.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>CANDYSET — ~20k raw instances collected from authoritative Chinese fact-checking sources, ~4.9k manually annotated flawed LLM explanations, ~7k human-study samples; contamination splits by model cutoff dates; includes gold evidence and domain labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4477.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4477.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CANDYSET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CANDYSET dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The dataset assembled for the CANDY benchmark: ~20k Chinese claims (multi-domain) with gold evidence, temporal stamps, and a subset (~4.9k) of manually annotated LLM-generated explanations for error analysis; also split for contamination-free evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Contamination-aware dataset partitioning for evaluation (CANDYSET)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provides training/test splits and contamination-free holdout determined by model knowledge cutoff dates to avoid data leakage; includes gold evidence per claim for grounding explanation evaluation. Used both for closed-book style evaluation of embedded model knowledge (no retrieval) and for web-augmented retrieval conditions in human experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Dataset supports evaluation of conclusion accuracy (Acc., F1), explanation faithfulness (manual taxonomy annotation), temporal robustness (pre/post cutoff splits), and human-assist experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Misinformation / fact-checking (Chinese)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Explanations and veracity claims (evidence-backed fact-check rationales)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Served as the testbed for all reported metrics: model-level accuracies/F1s, domain breakdowns, annotation counts (4,891 flawed explanations found among 22,000 generated candidate explanations across 11 LLMs), and human study sample selection (140 questions used in human study).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Dataset used in both automated evaluation of model outputs and human manual annotation for explanation evaluation and human study.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Data crawled from authoritative Chinese fact-checking platforms; manual normalization and evidence summarization; 3% random sample re-annotated by three annotators, Fleiss' Kappa = 0.75 for label validation; gold evidence verified by annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Chinese-only; initial preprocessing used GPT-4o for assistance (then manually verified) which could introduce biases; limited annotated-explanation coverage due to annotation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>CANDYSET (described above)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4477.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4477.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Taxonomy (CANDY)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-grained taxonomy for flawed fact-checking explanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-dimension taxonomy categorizing deficiencies in LLM-generated explanations into Factuality Hallucination (Factual Fabrication, Factual Inconsistency), Reasoning Inadequacy (Overgeneralized Reasoning, Under-Informativeness), and Faithfulness Hallucination (Instruction Inconsistency, Logical Inconsistency, Context Inconsistency).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Taxonomy-based manual error analysis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Manually annotate LLM-generated explanations against gold evidence using the decision tree and taxonomy; classify flawed explanations into the listed categories to quantify failure modes, enable per-domain analysis, and link flawed rationales to incorrect conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Categorical error types capturing (a) factual fabrication vs factual contradiction, (b) insufficient or overgeneralized reasoning, and (c) faithfulness problems (instruction deviation, internal logical conflicts, inconsistency with context). These categories enable fine-grained measurement of explanation reliability and mechanism of failures.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Misinformation fact-checking (explanatory outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Explanatory rationales / causal plausibility statements</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Applied to ~4,891 flawed explanations: factual hallucination (factual fabrication) was identified as the most common failure mode (~30% on breaking-news samples and largest across domains); logical inconsistency and context inconsistency were common (logical errors often >90% numerical/miscalc pattern); contamination-free setting increased factual hallucination by ~9% relative to contamination evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based: manual annotation by ten annotators (two independent labels + third adjudicator for disagreements) following a decision tree.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Inter-annotator agreement measured via Fleiss' Kappa = 0.76; annotation protocol included double-labeling and adjudication.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Manual annotation is costly and limited coverage (subset of models/instances); potential subjective boundary cases between taxonomy categories; taxonomy designed for fact-check explanations (may require adaptation for other scientific-explanation contexts).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Annotated subset of CANDYSET (approx. 4,891 flawed explanations labeled according to the taxonomy).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4477.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4477.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contamination evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contamination vs contamination-free evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation framework that distinguishes between contamination (data possibly included in model training) and contamination-free (data after model knowledge cutoff) conditions to measure model temporal robustness and leakage effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Contamination-aware performance evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Split test instances by timestamps relative to each model's knowledge cutoff to form contamination and contamination-free subsets; evaluate models on both splits to quantify performance degradation due to temporal knowledge gaps and potential training-data leakage. Used to emphasize real-world temporal challenges for fact-checking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Comparison of accuracy and F1 across contamination vs contamination-free splits; additional analysis on explanation error-type prevalence across these conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Misinformation fact-checking (temporal reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Temporal factual claims / time-sensitive explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Average degradation in contamination-free evaluation: ~6.9% absolute drop in accuracy and ~7.5% drop in F1 compared to contamination evaluation. Contamination-free setting increases factual hallucination rate by ~9% and shows worse temporal inconsistency (sampled time-sensitive claims showed ~75% factual inconsistency for models lacking temporal awareness).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated splits with human-validated labels; explanation error rates assessed via manual annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Use of model cutoff dates and authoritative timestamps in CANDYSET; sampling and manual verification of evidence and labels; inter-annotator agreement checks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires accurate model cutoff metadata; some domain samples scarce post-cutoff; contamination detection is nontrivial for very large or continuously updated models.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>CANDYSET contains timestamped claims and is partitioned accordingly for contamination vs contamination-free evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4477.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4477.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting & Claim Reframing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting schemes (zero/few-shot, CoT) and claim reframing (interrogative reformulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of prompting strategies evaluated: Zero-shot and Few-shot, each with/without Chain-of-Thought (CoT); for explanations, few-shot CoT producing five rationales; claim reframing (reformulating claims into interrogatives) is used to reduce fabrication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Prompting scheme comparison and claim-framing intervention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compare LLM performance under four prompting schemes (zero-shot w/o CoT, zero-shot w/ CoT, few-shot w/o CoT, few-shot w/ CoT) for the conclusion task; for explanation generation, use few-shot CoT asking for five distinct rationales. Evaluate how different prompt designs affect accuracy, F1, calibration (Expected Calibration Error), and explanation quality; additionally test claim reframing (convert claims to questions or non-occurrence statements) to measure influence on factual fabrication rates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Conclusion accuracy/F1, explanation flaw rates under taxonomy categories, calibration (ECE), and change in factual fabrication proportion when claims are reformulated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Misinformation fact-checking / LLM behavior</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Generated explanations / analytic rationales</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Few-shot CoT setting used as primary evaluation for explanations. CoT and few-shot prompting sometimes increased overconfidence (measured by ECE) and could worsen misinformation detection in some models (Table 5 shows CoT/few-shot frequently lead to higher overconfidence). Reformulating claims into interrogatives substantially reduced factual fabrication in GPT-4o from higher baseline down to ~14% fabricated outputs on a sample; converting occurrence-to-nonoccurrence variants reduced certain fabrication types but some models remained prone to alignment with input claim.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated comparison of prompt conditions; manual annotation of generated explanations to measure effect on taxonomy categories.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Average performance reported across three prompt variants (to mitigate prompt single-case bias); decision to present averaged results; manual checks of explanation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Results sensitive to prompt design; reported prompts may not be optimal; CoT can increase apparent reasoning but may amplify hallucination or overconfidence in some models.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Prompting schemes evaluated on CANDYSET and the annotated explanation subset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4477.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4477.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Metrics & Validation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accuracy, F1, Expected Calibration Error, Fleiss' Kappa, micro-average</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of quantitative metrics used in CANDY to evaluate model conclusions, calibration, and annotation reliability: micro-averaged accuracy/F1 for classification, Expected Calibration Error for over/under-confidence, and Fleiss' Kappa for inter-annotator agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Standard classification and reliability metrics (Acc, F1, ECE, Fleiss' Kappa)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Accuracy and F1 (micro-average) measure correctness of binary veracity predictions; Expected Calibration Error (ECE) quantifies calibration/overconfidence differences across prompting schemes (used to show CoT/few-shot effects); Fleiss' Kappa measures agreement across multiple annotators in label and explanation annotation tasks, validating human-labeled ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness (Acc, F1), calibration (ECE), annotation reliability (Fleiss' Kappa threshold indicates substantial agreement ~0.75+), and macro/ micro-averaging where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Misinformation / LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Explanations/veracity judgments</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported values include Fleiss' Kappa = 0.75 for label validation and 0.76 for explanation annotation; model-level accuracies and F1s reported per-model (e.g., GPT-4o ~76.2% acc / 76.1% F1 in contamination-free); CoT/few-shot setups often increased ECE indicating higher overconfidence despite not always improving detection.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Metrics applied to both automated outputs and as validation measures for human annotation quality; ECE is automated, Fleiss' Kappa is derived from human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Inter-annotator adjudication and Kappa calculation for annotation tasks; ECE-based analyses for calibration; sample re-annotation (3%) for data validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Metrics like accuracy/F1 do not capture explanation faithfulness; ECE highlights calibration but not the content quality of hallucinations; Fleiss' Kappa depends on annotator pool and guidelines and may mask nuanced disagreement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4477.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4477.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Study (LLM-assisted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-LLM cooperation human study protocol</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>IRB-approved human subjects experiment evaluating how LLM assistance affects human fact-checking accuracy and efficiency across education levels and support conditions (Human-only, Human+Web, Human+LLM, Human+LLM+Web).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human-LLM assisted fact-checking experiment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Participants (groups by education: elementary, middle, undergraduate, masters; 12 per group) were randomly assigned to one of four conditions: independent human judgment, human judgment assisted by web search (Baidu), human judgment assisted by GPT-4o, and human judgment assisted by GPT-4o with web-augmented retrieval. Each participant judged 140 claims (10 per domain before and after GPT-4o cutoff). Accuracy and efficiency were recorded and compared across conditions to quantify assistive value.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human accuracy (proportion correct), decision efficiency (time / throughput), and improvement relative to baseline human-only condition; cross-condition comparisons (Human vs Human+LLM vs Human+Web vs Human+LLM+Web).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Misinformation fact-checking (human-in-the-loop evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Human judgement aided by LLM explanations for veracity; not pure scientific theories but evaluation of explanatory assistance</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human+LLM condition improved accuracy and efficiency across all education levels relative to Human-only and Human+Web; Human+LLM+Web achieved the highest overall performance (exact numbers summarized in Table 7). Study used 140 claims sampled relative to GPT-4o cutoff and followed IRB protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based: measured human performance under different assistance modalities; LLM outputs served as inputs to humans, but final judgments were human.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>IRB-approved protocol, balanced sampling across domains and cutoff dates, compensation and standardization of instructions; results compared across matched groups.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limited participant pool (12 per education group), potential platform/tool familiarity effects, and dependency on one LLM (GPT-4o) for assistance conditions; results may vary with different LLMs or retrieval tool implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>CHEF: A pilot chinese dataset for evidence-based fact-checking <em>(Rating: 2)</em></li>
                <li>How we re-fute claims: Automatic fact-checking through flaw identification and explanation <em>(Rating: 2)</em></li>
                <li>FlawCheck <em>(Rating: 1)</em></li>
                <li>Leveraging chat-gpt for efficient fact-checking <em>(Rating: 1)</em></li>
                <li>A richly annotated corpus for different tasks in automated factchecking <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4477",
    "paper_id": "paper-281103687",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "CANDY",
            "name_full": "CANDY benchmark for LLM fact-checking",
            "brief_description": "A comprehensive benchmark and evaluation framework introduced in this paper to systematically measure LLM capabilities, limitations, and assistive potential for fact-checking Chinese misinformation via three tasks (Fact-Checking Conclusion, Fact-Checking Explanation, LLM-Assisted Fact-Checking).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "CANDY three-task evaluation framework",
            "evaluation_method_description": "Evaluates LLMs across three progressive tasks: (1) Fact-Checking Conclusion — binary veracity classification of claims; (2) Fact-Checking Explanation — generation and manual assessment of explanatory rationales for conclusions; (3) LLM-Assisted Fact-Checking — human subject experiments measuring how LLMs assist human decision-making. The framework uses contamination vs contamination-free splits (model cutoff-aware) and standard prompting variants (zero/few-shot, with/without Chain-of-Thought). It combines automated metrics for conclusions with large-scale manual annotation and a human study to probe both autonomous and assistive roles.",
            "evaluation_criteria": "For conclusions: accuracy and F1 (micro-averaged). For explanations: reliability judged by taxonomy dimensions (Factuality Hallucination, Reasoning Inadequacy, Faithfulness Hallucination) and per-category counts/proportions; for assistive role: human accuracy and efficiency (comparison across Human, Human+Web, Human+LLM, Human+LLM+Web). Also measures temporal awareness (contamination-free performance) and calibration/overconfidence.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Misinformation fact-checking (Chinese, multi-domain: politics, health, society, disasters, life, culture, science)",
            "theory_type": "Explanations / veracity determinations (rationales supporting truth/falsity decisions); not formal scientific theories but explanatory/hypothesis-like outputs)",
            "human_comparison": true,
            "evaluation_results": "Applied to 16 LLMs and 3 LRMs on CANDYSET (~20k instances). Top-performing model GPT-4o achieved ~76.2% accuracy and 76.1% F1 (contamination-free reported values; contamination results reported in parentheses across tables). Contamination-free evaluation led to an average drop of 6.9% accuracy and 7.5% F1 versus contamination evaluation. Across explanation analysis, 91.2% of incorrect conclusions were associated with flawed explanations; manual annotation found 4,891 flawed explanations (428 of these still produced correct conclusions). Domain variation observed: higher accuracy in knowledge-intensive domains (~80+%), lower in temporal-sensitive domains (society/disasters ~69–74%). Human study: Human+LLM and Human+LLM+Web outperformed Human-only and Human+Web across education groups (exact table values in Table 7).",
            "automated_vs_human_evaluation": "Hybrid: automated metrics (accuracy, F1, calibration metrics) for conclusion task and for model-run statistics; extensive human annotation for explanation quality (two annotators + adjudicator per item) and an IRB-approved human study to measure assistive potential.",
            "validation_method": "Quality control via authoritative sources, sampling checks (3% sample re-annotated), inter-annotator agreement measured with Fleiss' Kappa (0.75 for label validation, 0.76 for explanation annotation); decision-tree annotation process and adjudication by third annotator for disagreements.",
            "limitations_challenges": "Prompt sensitivity (evaluations depend on prompt design); explanation annotation was limited to 11 LLMs due to manual cost; dataset restricted to Chinese (limits cross-lingual generalizability); contamination-free evaluation highlights temporal knowledge cutoffs as a major challenge; CoT and few-shot prompting can increase overconfidence in some models; LLMs prone to factual fabrication and logical/contextual inconsistencies.",
            "benchmark_dataset": "CANDYSET — ~20k raw instances collected from authoritative Chinese fact-checking sources, ~4.9k manually annotated flawed LLM explanations, ~7k human-study samples; contamination splits by model cutoff dates; includes gold evidence and domain labels.",
            "uuid": "e4477.0",
            "source_info": {
                "paper_title": "CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "CANDYSET",
            "name_full": "CANDYSET dataset",
            "brief_description": "The dataset assembled for the CANDY benchmark: ~20k Chinese claims (multi-domain) with gold evidence, temporal stamps, and a subset (~4.9k) of manually annotated LLM-generated explanations for error analysis; also split for contamination-free evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Contamination-aware dataset partitioning for evaluation (CANDYSET)",
            "evaluation_method_description": "Provides training/test splits and contamination-free holdout determined by model knowledge cutoff dates to avoid data leakage; includes gold evidence per claim for grounding explanation evaluation. Used both for closed-book style evaluation of embedded model knowledge (no retrieval) and for web-augmented retrieval conditions in human experiments.",
            "evaluation_criteria": "Dataset supports evaluation of conclusion accuracy (Acc., F1), explanation faithfulness (manual taxonomy annotation), temporal robustness (pre/post cutoff splits), and human-assist experiments.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Misinformation / fact-checking (Chinese)",
            "theory_type": "Explanations and veracity claims (evidence-backed fact-check rationales)",
            "human_comparison": false,
            "evaluation_results": "Served as the testbed for all reported metrics: model-level accuracies/F1s, domain breakdowns, annotation counts (4,891 flawed explanations found among 22,000 generated candidate explanations across 11 LLMs), and human study sample selection (140 questions used in human study).",
            "automated_vs_human_evaluation": "Dataset used in both automated evaluation of model outputs and human manual annotation for explanation evaluation and human study.",
            "validation_method": "Data crawled from authoritative Chinese fact-checking platforms; manual normalization and evidence summarization; 3% random sample re-annotated by three annotators, Fleiss' Kappa = 0.75 for label validation; gold evidence verified by annotators.",
            "limitations_challenges": "Chinese-only; initial preprocessing used GPT-4o for assistance (then manually verified) which could introduce biases; limited annotated-explanation coverage due to annotation cost.",
            "benchmark_dataset": "CANDYSET (described above)",
            "uuid": "e4477.1",
            "source_info": {
                "paper_title": "CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Taxonomy (CANDY)",
            "name_full": "Fine-grained taxonomy for flawed fact-checking explanations",
            "brief_description": "A three-dimension taxonomy categorizing deficiencies in LLM-generated explanations into Factuality Hallucination (Factual Fabrication, Factual Inconsistency), Reasoning Inadequacy (Overgeneralized Reasoning, Under-Informativeness), and Faithfulness Hallucination (Instruction Inconsistency, Logical Inconsistency, Context Inconsistency).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Taxonomy-based manual error analysis",
            "evaluation_method_description": "Manually annotate LLM-generated explanations against gold evidence using the decision tree and taxonomy; classify flawed explanations into the listed categories to quantify failure modes, enable per-domain analysis, and link flawed rationales to incorrect conclusions.",
            "evaluation_criteria": "Categorical error types capturing (a) factual fabrication vs factual contradiction, (b) insufficient or overgeneralized reasoning, and (c) faithfulness problems (instruction deviation, internal logical conflicts, inconsistency with context). These categories enable fine-grained measurement of explanation reliability and mechanism of failures.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Misinformation fact-checking (explanatory outputs)",
            "theory_type": "Explanatory rationales / causal plausibility statements",
            "human_comparison": false,
            "evaluation_results": "Applied to ~4,891 flawed explanations: factual hallucination (factual fabrication) was identified as the most common failure mode (~30% on breaking-news samples and largest across domains); logical inconsistency and context inconsistency were common (logical errors often &gt;90% numerical/miscalc pattern); contamination-free setting increased factual hallucination by ~9% relative to contamination evaluation.",
            "automated_vs_human_evaluation": "Human-based: manual annotation by ten annotators (two independent labels + third adjudicator for disagreements) following a decision tree.",
            "validation_method": "Inter-annotator agreement measured via Fleiss' Kappa = 0.76; annotation protocol included double-labeling and adjudication.",
            "limitations_challenges": "Manual annotation is costly and limited coverage (subset of models/instances); potential subjective boundary cases between taxonomy categories; taxonomy designed for fact-check explanations (may require adaptation for other scientific-explanation contexts).",
            "benchmark_dataset": "Annotated subset of CANDYSET (approx. 4,891 flawed explanations labeled according to the taxonomy).",
            "uuid": "e4477.2",
            "source_info": {
                "paper_title": "CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Contamination evaluation",
            "name_full": "Contamination vs contamination-free evaluation framework",
            "brief_description": "An evaluation framework that distinguishes between contamination (data possibly included in model training) and contamination-free (data after model knowledge cutoff) conditions to measure model temporal robustness and leakage effects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Contamination-aware performance evaluation",
            "evaluation_method_description": "Split test instances by timestamps relative to each model's knowledge cutoff to form contamination and contamination-free subsets; evaluate models on both splits to quantify performance degradation due to temporal knowledge gaps and potential training-data leakage. Used to emphasize real-world temporal challenges for fact-checking.",
            "evaluation_criteria": "Comparison of accuracy and F1 across contamination vs contamination-free splits; additional analysis on explanation error-type prevalence across these conditions.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Misinformation fact-checking (temporal reasoning)",
            "theory_type": "Temporal factual claims / time-sensitive explanations",
            "human_comparison": false,
            "evaluation_results": "Average degradation in contamination-free evaluation: ~6.9% absolute drop in accuracy and ~7.5% drop in F1 compared to contamination evaluation. Contamination-free setting increases factual hallucination rate by ~9% and shows worse temporal inconsistency (sampled time-sensitive claims showed ~75% factual inconsistency for models lacking temporal awareness).",
            "automated_vs_human_evaluation": "Automated splits with human-validated labels; explanation error rates assessed via manual annotation.",
            "validation_method": "Use of model cutoff dates and authoritative timestamps in CANDYSET; sampling and manual verification of evidence and labels; inter-annotator agreement checks.",
            "limitations_challenges": "Requires accurate model cutoff metadata; some domain samples scarce post-cutoff; contamination detection is nontrivial for very large or continuously updated models.",
            "benchmark_dataset": "CANDYSET contains timestamped claims and is partitioned accordingly for contamination vs contamination-free evaluation.",
            "uuid": "e4477.3",
            "source_info": {
                "paper_title": "CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Prompting & Claim Reframing",
            "name_full": "Prompting schemes (zero/few-shot, CoT) and claim reframing (interrogative reformulation)",
            "brief_description": "Set of prompting strategies evaluated: Zero-shot and Few-shot, each with/without Chain-of-Thought (CoT); for explanations, few-shot CoT producing five rationales; claim reframing (reformulating claims into interrogatives) is used to reduce fabrication.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Prompting scheme comparison and claim-framing intervention",
            "evaluation_method_description": "Compare LLM performance under four prompting schemes (zero-shot w/o CoT, zero-shot w/ CoT, few-shot w/o CoT, few-shot w/ CoT) for the conclusion task; for explanation generation, use few-shot CoT asking for five distinct rationales. Evaluate how different prompt designs affect accuracy, F1, calibration (Expected Calibration Error), and explanation quality; additionally test claim reframing (convert claims to questions or non-occurrence statements) to measure influence on factual fabrication rates.",
            "evaluation_criteria": "Conclusion accuracy/F1, explanation flaw rates under taxonomy categories, calibration (ECE), and change in factual fabrication proportion when claims are reformulated.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Misinformation fact-checking / LLM behavior",
            "theory_type": "Generated explanations / analytic rationales",
            "human_comparison": false,
            "evaluation_results": "Few-shot CoT setting used as primary evaluation for explanations. CoT and few-shot prompting sometimes increased overconfidence (measured by ECE) and could worsen misinformation detection in some models (Table 5 shows CoT/few-shot frequently lead to higher overconfidence). Reformulating claims into interrogatives substantially reduced factual fabrication in GPT-4o from higher baseline down to ~14% fabricated outputs on a sample; converting occurrence-to-nonoccurrence variants reduced certain fabrication types but some models remained prone to alignment with input claim.",
            "automated_vs_human_evaluation": "Automated comparison of prompt conditions; manual annotation of generated explanations to measure effect on taxonomy categories.",
            "validation_method": "Average performance reported across three prompt variants (to mitigate prompt single-case bias); decision to present averaged results; manual checks of explanation quality.",
            "limitations_challenges": "Results sensitive to prompt design; reported prompts may not be optimal; CoT can increase apparent reasoning but may amplify hallucination or overconfidence in some models.",
            "benchmark_dataset": "Prompting schemes evaluated on CANDYSET and the annotated explanation subset.",
            "uuid": "e4477.4",
            "source_info": {
                "paper_title": "CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Evaluation Metrics & Validation",
            "name_full": "Accuracy, F1, Expected Calibration Error, Fleiss' Kappa, micro-average",
            "brief_description": "Set of quantitative metrics used in CANDY to evaluate model conclusions, calibration, and annotation reliability: micro-averaged accuracy/F1 for classification, Expected Calibration Error for over/under-confidence, and Fleiss' Kappa for inter-annotator agreement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Standard classification and reliability metrics (Acc, F1, ECE, Fleiss' Kappa)",
            "evaluation_method_description": "Accuracy and F1 (micro-average) measure correctness of binary veracity predictions; Expected Calibration Error (ECE) quantifies calibration/overconfidence differences across prompting schemes (used to show CoT/few-shot effects); Fleiss' Kappa measures agreement across multiple annotators in label and explanation annotation tasks, validating human-labeled ground truth.",
            "evaluation_criteria": "Correctness (Acc, F1), calibration (ECE), annotation reliability (Fleiss' Kappa threshold indicates substantial agreement ~0.75+), and macro/ micro-averaging where applicable.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Misinformation / LLM evaluation",
            "theory_type": "Explanations/veracity judgments",
            "human_comparison": false,
            "evaluation_results": "Reported values include Fleiss' Kappa = 0.75 for label validation and 0.76 for explanation annotation; model-level accuracies and F1s reported per-model (e.g., GPT-4o ~76.2% acc / 76.1% F1 in contamination-free); CoT/few-shot setups often increased ECE indicating higher overconfidence despite not always improving detection.",
            "automated_vs_human_evaluation": "Metrics applied to both automated outputs and as validation measures for human annotation quality; ECE is automated, Fleiss' Kappa is derived from human annotations.",
            "validation_method": "Inter-annotator adjudication and Kappa calculation for annotation tasks; ECE-based analyses for calibration; sample re-annotation (3%) for data validation.",
            "limitations_challenges": "Metrics like accuracy/F1 do not capture explanation faithfulness; ECE highlights calibration but not the content quality of hallucinations; Fleiss' Kappa depends on annotator pool and guidelines and may mask nuanced disagreement.",
            "uuid": "e4477.5",
            "source_info": {
                "paper_title": "CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Human Study (LLM-assisted)",
            "name_full": "Human-LLM cooperation human study protocol",
            "brief_description": "IRB-approved human subjects experiment evaluating how LLM assistance affects human fact-checking accuracy and efficiency across education levels and support conditions (Human-only, Human+Web, Human+LLM, Human+LLM+Web).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Human-LLM assisted fact-checking experiment",
            "evaluation_method_description": "Participants (groups by education: elementary, middle, undergraduate, masters; 12 per group) were randomly assigned to one of four conditions: independent human judgment, human judgment assisted by web search (Baidu), human judgment assisted by GPT-4o, and human judgment assisted by GPT-4o with web-augmented retrieval. Each participant judged 140 claims (10 per domain before and after GPT-4o cutoff). Accuracy and efficiency were recorded and compared across conditions to quantify assistive value.",
            "evaluation_criteria": "Human accuracy (proportion correct), decision efficiency (time / throughput), and improvement relative to baseline human-only condition; cross-condition comparisons (Human vs Human+LLM vs Human+Web vs Human+LLM+Web).",
            "model_name": "GPT-4o",
            "model_size": null,
            "scientific_domain": "Misinformation fact-checking (human-in-the-loop evaluation)",
            "theory_type": "Human judgement aided by LLM explanations for veracity; not pure scientific theories but evaluation of explanatory assistance",
            "human_comparison": true,
            "evaluation_results": "Human+LLM condition improved accuracy and efficiency across all education levels relative to Human-only and Human+Web; Human+LLM+Web achieved the highest overall performance (exact numbers summarized in Table 7). Study used 140 claims sampled relative to GPT-4o cutoff and followed IRB protocols.",
            "automated_vs_human_evaluation": "Human-based: measured human performance under different assistance modalities; LLM outputs served as inputs to humans, but final judgments were human.",
            "validation_method": "IRB-approved protocol, balanced sampling across domains and cutoff dates, compensation and standardization of instructions; results compared across matched groups.",
            "limitations_challenges": "Limited participant pool (12 per education group), potential platform/tool familiarity effects, and dependency on one LLM (GPT-4o) for assistance conditions; results may vary with different LLMs or retrieval tool implementations.",
            "uuid": "e4477.6",
            "source_info": {
                "paper_title": "CANDY: Benchmarking LLMs’ Limitations and Assistive Potential in Chinese Misinformation Fact-Checking",
                "publication_date_yy_mm": "2025-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "CHEF: A pilot chinese dataset for evidence-based fact-checking",
            "rating": 2,
            "sanitized_title": "chef_a_pilot_chinese_dataset_for_evidencebased_factchecking"
        },
        {
            "paper_title": "How we re-fute claims: Automatic fact-checking through flaw identification and explanation",
            "rating": 2,
            "sanitized_title": "how_we_refute_claims_automatic_factchecking_through_flaw_identification_and_explanation"
        },
        {
            "paper_title": "FlawCheck",
            "rating": 1
        },
        {
            "paper_title": "Leveraging chat-gpt for efficient fact-checking",
            "rating": 1,
            "sanitized_title": "leveraging_chatgpt_for_efficient_factchecking"
        },
        {
            "paper_title": "A richly annotated corpus for different tasks in automated factchecking",
            "rating": 1,
            "sanitized_title": "a_richly_annotated_corpus_for_different_tasks_in_automated_factchecking"
        }
    ],
    "cost": 0.0196475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking</p>
<p>Ruiling Guo ruiling_guo@stu.scu.edu 
School of Cyber Science and Engineering
Sichuan University
China</p>
<h1>Xinwei Yang</h1>
<p>College of Computer Science
Sichuan University
China</p>
<p>Chen Huang huang_chen@nus.edu.sg 
Tong Zhang 
College of Computer Science
Sichuan University
China</p>
<p>Yong Hu 
School of Cyber Science and Engineering
Sichuan University
China</p>
<p>Institute of Data Science
National University of Singapore
Singapore</p>
<p>CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking
4BABE8A961CC09E4BB1A00F00C25AC5B
The effectiveness of large language models (LLMs) to fact-check misinformation remains uncertain, despite their growing use.To this end, we present CANDY, a benchmark designed to systematically evaluate the capabilities and limitations of LLMs in fact-checking Chinese misinformation.Specifically, we curate a carefully annotated dataset of ∼20k instances.Our analysis shows that current LLMs exhibit limitations in generating accurate fact-checking conclusions, even when enhanced with chainof-thought reasoning and few-shot prompting.To understand these limitations, we develop a taxonomy to categorize flawed LLM-generated explanations for their conclusions and identify factual fabrication as the most common failure mode.Although LLMs alone are unreliable for fact-checking, our findings indicate their considerable potential to augment human performance when deployed as assistive tools in scenarios.Our dataset and code can be accessed at https://github.com/SCUNLP/CANDY.</p>
<p>Introduction</p>
<p>Misinformation, defined as "false or misleading information masquerading as legitimate news, regardless of intent" (Glockner et al., 2022).With the largest Internet user base, China produces vast amounts of such misinformation daily, making manual verification both labor-intensive and increasingly infeasible (ChinaDaily, 2025;piyao, 2025;Nakov et al., 2021).</p>
<p>Large language models (LLMs) possess extensive knowledge and explanatory capabilities, positioning them as promising tools for misinformation detection (Guan et al., 2023;Kang et al., 2024;Patil and Gudivada, 2024).However, concerns about their effectiveness persist, such as hallucination limitations (Wang et al., 2024a;Hu et al., CLAIM: A criminal case occurred in Jilin Province, where a butcher killed 12 people due to a land dispute.LLM RESPONSE: Not a rumor.A criminal case did occur in Jilin Province, attracting significant local attention.According to the police report, the suspect developed resentment from a land dispute, leading to the tragedy.Local government confirmed there were 12 victims...</p>
<p>ANNOTATION Domain: Society</p>
<p>Publish Date: 2024/8/14 Is Misinformation: Yes Gold Evidence: After investigation by the police from Jilin, it was found that no such case occurred at the location, and this information was entirely fabricated by ... Source: China Internet Joint Rumor Debunking Platform Error Type of LLM RESPONSE: Factual Fabrication Table 1: An example from the CANDYSET, showing LLMs' tendency to sycophantize misinformation and fabricate authoritative verification.2024; Augenstein et al., 2024;Fang et al., 2025b).This has spurred numerous researches aimed at evaluating LLMs' ability to identify misinformation (Hoes et al., 2023;Hsu et al., 2024;Cekinel and Karagoz, 2024;Kao and Yen, 2024;Vykopal et al., 2024).However, current evaluations are often superficial, primarily assessing the correctness of final answers (i.e., whether a statement is factual) while neglecting a deeper analysis of the characteristics and reasoning processes underlying LLMs' decisions.Consequently, it remains unclear precisely which types of misinformation LLMs can effectively identify, and what challenges they persistently face in this domain.Therefore, a comprehensive understanding of LLMs' capabilities and limitations in fact-checking Chinese misinformation remains uncertain, raising concerns about their practical utility.</p>
<p>To this end, we introduce CANDY, a benchmark designed to systematically evaluate the capabilities, limitations, and practical roles of LLMs in factchecking Chinese misinformation.To achieve this, we construct CANDYSET, a curated multi-domain dataset consisting of approximately 20k news instances collected from mainstream Chinese rumor refutation websites, with detailed sources provided in Table 9.The dataset is further partitioned based on the cut-off dates of different models to support contamination-free evaluation (i.e., evaluating performance on unseen data).</p>
<p>With CANDYSET, we further expand our benchmark analysis using three tasks including Fact-Checking Conclusion (i.e., if a statement is factual), Fact-Checking Explanation (if a LLM-generated explanation for its fact-checking conclusion is reliable), and LLM-Assisted Fact-Checking (to what extent can LLMs assist humans in fact-checking).</p>
<p>To better identify and expose the deficiency of LLMs, we develop a taxonomy that categorizes flawed LLM-generated explanations into three dimensions, which are further divided into seven finegrained categories (cf.Section 3.2).Using this taxonomy, we manually annotate around 5k LLMgenerated explanations to analyze explanation deficiencies, contributing a valuable component of our dataset (cf.Table 2).Finally, to evaluate the practical role of LLMs at real-world scenarios, our benchmark includes a human study to explore how LLMs can support users in fact-checking tasks.Our benchmark on sixteen LLMs and three large reasoning models (LRMs) reveals several challenges : (1) Despite employing techniques such as Chain-of-Thought reasoning and few-shot prompting, LLMs struggle with fact-checking conclusions, especially when addressing contaminationfree evaluation and time-sensitive events.Our analysis on the fact-checking conclusion task showed how LRM accuracy and timeliness often fall short during societal crises or disasters, especially since there is no specific event feature in the dataset.(2) Furthermore, our analysis on the explanation generation task revealed the main reason why LLMs often produce incorrect fact-checking conclusions: their tendency toward factual hallucination (particularly true for LRMs).In some cases, they even fabricate highly deceptive details to support false claims, which significantly limits their ability to serve as independent Chinese fact-checking decision makers.(3) Finally, our LLM-assisted fact-checking task showed that, under identical testing conditions, individuals from diverse educational backgrounds achieve higher accuracy and efficiency when assisted by LLMs.This suggests that LLMs are best positioned to serve as intelligent assistants or advisors in the fact-checking process, rather than as autonomous decision-makers.</p>
<p>In this paper, CANDY serves as a valuable resource for offering practical guidance and insights for LLM fact-checking.In conclusion, our contributions are as followings:</p>
<p>• We propose CANDY, the first benchmark provides a comprehensive evaluation and in-depth analysis of LLMs' ability to fact-check Chinese misinformation, as well as their applicability in practice.</p>
<p>• We introduce CANDYSET, a Chinese large-scale dataset.It comprises ~20k raw instances, ~5k carefully annotated LLM-generated explanations, and ~7k human study samples.This enables indepth evaluation.</p>
<p>• We introduce a fine-grained taxonomy for categorizing flawed LLM explanations, facilitating in-depth analysis of LLM deficiencies in factchecking misinformation.</p>
<p>• With CANDY, we experimentally benchmark sixteen LLMs (and three LRMs) using three progressive tasks to systematically evaluate the capabilities, limitations, and practical roles of LLMs in fact-checking real-world Chinese misinformation, offering practical guidance for future study.</p>
<p>Related Works</p>
<p>In recent years, with the proliferation of misinformation on social media, several works have developed various fact-checking benchmarks as shown in Table 3.For instance, COVID19-Health-Rumor (Yang et al., 2022), CHECKED (Yang et al., 2021) focus on rumors during public health crises, and  Weibo21 (Nan et al., 2021) and MCFEND (Li et al., 2024b) for multi-domain scenarios.However, except for CHEF (Hu et al., 2022), these benchmarks lack detailed explanations of supporting evidence, limiting their usefulness for deeper analysis.Additionally, of all the benchmarks, only PolitiFact(Kao and Yen, 2024) and FlawCheck (Hsu et al., 2024) strive to get LLMs to generate explanations during the fact-checking process.However, both of these benchmarks fall short in providing detailed human annotations and thorough analysis of the accuracy or shortcomings of these explanations.Moreover, none of the benchmarks consider testing LLMs in collaboration with real users in practical scenarios, which hinders their real-world applicability.Our work addresses these limitations by introducing (1) carefully annotated evidence explanations for each instance, (2) constructing a taxonomy to categorize errors in LLM-generated explanations, and</p>
<p>(3) introducing a real-world human study to examine the practical deployment of LLMs in authentic fact-checking scenarios.Fact-checking Explanation Annotation.To facilitate in-depth analysis of LLM fact-checking, we first required each LLM to generate explanations for its fact-checking decisions.We then manually compared these explanations with the collected evidence within our dataset to identify flaws.Due to the human cost involved, we considered explanations from eleven LLMs, each responsible for 2,000 randomly sampled entries from the raw dataset based on the original time and domain distribution.This resulted in 22,000 explanations, of which 4,891 were identified as flawed.Among these, only 428 led to correct LLM fact-checking outcomes, while the vast majority (4,463) resulted in incorrect outcomes.To provide more insights, we manually categorized these flawed explanations according to our pre-defined taxonomy (cf.Section 3.2).During the whole process, ten computer science master's students, fluent in Chinese and experienced in data annotation, were involved.Each explanation was independently labeled by two annotators, and discrepancies were resolved by a third annotator.A Fleiss' Kappa of 0.76 indicates substantial agreement among the annotators, confirming the reliability of the annotation process.More details are provided in Appendix B. Quality Control.CANDYSET's quality is rigorously controlled through two vetting layers: authoritative data sources, and human annotation and re-view process: (1) reliance on authoritative data sources, ensuring the credibility of news and evidence, and (2) a human annotation and review process, where LLM-generated fact-checking explanations are meticulously annotated and checked by multiple humans, resulting in substantial annotation agreement (Fleiss' Kappa of 0.76).</p>
<p>CANDY Benchmark</p>
<p>Taxonomy for Fact-checking Explanations</p>
<p>We propose a fine-grained taxonomy to group the deficiencies of LLM generated explanations across various expression tones, ranging from confident and definitive to speculative.It encompasses the following three dimensions.Factuality Hallucination refers to the LLM expressing reasons in a definitive tone that contradict real-world facts or are fabricated (Huang et al., 2023;Qin et al., 2024;Deng et al., 2024).We identify two subcategories:</p>
<p>• Factual Fabrication refers to the LLM's output that fabricates rationales for analysis without relying on any real-world information.For instance, it propagates misinformation by stating, "It was reported by reputable media outlets."• Factual Inconsistency refers to the LLM's output contains facts that can be grounded in real-world information, but present contradictions.For example: "China will host the World Cup in 2026."</p>
<p>Reasoning Inadequacy refers to the inability of an LLM to deliver high-quality and helpful reasoning when direct evidence is insufficient.</p>
<p>• Overgeneralized Reasoning refers to the tendency of a LLM to produce speculative rationales based on overly broad or superficial criteria.For example: Solely based on "the technology sector has indeed seen rapid advancements in recent years," concluding that "the new technology can increase battery life by ten times."</p>
<p>• Under Informativeness refers to the tendency of a LLM to exhibit excessive rigor or restraint, failing to provide more contextually valuable content.For example: "There is currently no conclusive scientific evidence proving that eating an apple a day is beneficial to health."</p>
<p>4 Benchmark Setup</p>
<p>Overview.We benchmark LLMs (and LRMs) using the following three tasks to systematically evaluate the capabilities, limitations, and practical roles of LLMs in fact-checking real-world Chinese misinformation: Fact-Checking Conclusion, Fact-Checing Explanation, and LLM-Assisted Fact-Checking, each detailed in the subsequent sections.</p>
<p>Baselines.Our study benchmarks sixteen LLMs, such as models from the OpenAI GPT family (Ope-nAI, 2023) and the Qwen family (Yang et al., 2024).Also, we consider three LRMs, including OpenAI O1-Mini (Jaech et al., 2024), DeepSeek-R1 (Guo et al., 2025) and Qwen-QwQ (Team, 2025).Following (Deng et al., 2023), we devise different prompting schemes for each baseline: 1) Zero-shot w/o CoT. 2) Zero-shot w/ CoT (Wei et al., 2022).</p>
<p>3) Few-shot w/o CoT (Dong et al., 2022a).4) Fewshot w/ CoT (Dong et al., 2022b).For detailed information about the models and prompt scheme design, please refer to Appendix C.2 and Appendix E, respectively.Evaluation Metrics &amp; Implementation Details.Specific metrics and implementation details for each task will be presented in corresponding sections.Further details are in Appendix C.</p>
<p>Task 1: Fact-Checking Conclusion</p>
<p>This section aims to evaluate the ability of LLMs to verify facts by assessing their performance in distinguishing factual statements from falsehoods.Following Huang et al. (2024), we adopt accuracy (Acc.) and F1 score as evaluation metrics.</p>
<p>Overall Evaluation</p>
<p>As shown in   robust utilization of extensive internal knowledge.Our detailed observations are as follows:</p>
<p>Current LLMs, even when employing methods like CoT reasoning and few-shot prompting, still struggle to accurately perform fact-checking tasks, particularly in contamination-free scenarios.While larger models such as Llama-3.2-70B,Qwen-2.5-72B, and GPT-4o demonstrate higher performance, even the top-performing GPT-4o only achieves moderate results (76.2% accuracy and 76.1% F1 score).Notably, LLM performance declines significantly when handling contaminationfree evaluation compared to contamination evaluation, with an average decrease of 6.9% in accuracy and 7.5% in F1 score.This performance gap highlights the complexities of contamination-free factchecking, which requires dynamic assessment of rapidly evolving information, unlike contamination fact-checking that often relies on static, pre-verified data.Smaller-scale open-source LLMs (e.g., Yi-1.5-6B,Llama-3.2-7B,Qwen-2.5-7B)exhibit even lower performance, often misclassifying truthful information as misinformation, leading to a considerable gap between accuracy and F1 (e.g., Yi-1.5-6B: 60.5% accuracy, 35.8% F1 in Table 4).The reason for this gap is illustrated in Figure 1, smallscale LLMs tend to classify most data instances as misinformation.Furthermore, methods like CoT reasoning and few-shot prompting may exacerbate overconfidence issues in small-scale open-source models, leading to adverse outcomes.Indeed, our analysis in</p>
<p>Fine-Grained Evaluation</p>
<p>We evaluate the fact-checking effectiveness across diverse domains of misinformation, categorized into three groups based on their inherent characteristics: 1) knowledge-intensive (e.g., politics, health, science, culture), requiring specialized expertise to verify; 2) temporal-sensitive (e.g., disasters, society), where accuracy depends heavily on contamination-free information; and 3) commonsense-sensitive (e.g., life-related topics).</p>
<p>LLMs exhibit varied cons and pros across domains.As shown in Table 6, in knowledgeintensive domains such as culture and politics, LLMs achieve high accuracy rates (82.77% and 80.25%, respectively), highlighting their strong knowledge base and feature extraction capabilities.However, the culture domain exhibits the lowest F1 score (60.03%) due to significant class imbalance and challenges in identifying incorrect samples.Performance declines in temporal-sensitive domains like society (73.85%) and disasters (69.01%), reflecting the difficulty LLMs face in adapting to rapidly evolving information.In the commonsensesensitive life domain, GPT-4o significantly outperforms its peers, exceeding the average accuracy by 18.82%, demonstrating its advanced flexibility and adaptability in handling informal scenarios and commonsense reasoning.Also, Qwen-2.5-72B and GLM4 showcase notable domain-specific expertise.</p>
<p>6 Task 2: Fact-Checking Explanation</p>
<p>On average, across all LLMs examined, a substantial proportion (91.2%) of fact-checking results leading to incorrect conclusions were associated with flawed LLM-generated explanations.In contrast, only 8.8% of the results leading to correct conclusions exhibited such flaws (cf.Table 2 for details).This marked difference underscores the detrimental impact of flawed explanations on factchecking performance, highlighting the need for a in-depth understanding of their nature and origin.</p>
<p>To this end, this section employs the taxonomy described in Section 3.2 to provide further insights.We focus on eleven representative LLMs, comprising eight closed-source models and three opensource models.Overall, the prevalence of unreliable explanations suggests that current LLMs are insufficiently reliable for real-world fact-checking, but also indicates that internal optimization strategies could potentially enhance task performance.</p>
<p>Overall Evaluation</p>
<p>The prevalence of flawed explanations highlights that inherent deficiencies within LLMs can significantly impact their fact-checking performance.As shown in Figure 2, flawed factchecking explanations persist across temporal scenarios, with factual hallucination being the predominant error type.The contamination-free evaluation reveals a 9% increase in factual hallucination, suggesting that LLMs are more inclined to generate coherent text based on statistical patterns when lacking direct factual evidence, rather than acknowledging knowledge gaps.Under contamination evaluation, logical inconsistency rises by 6%, suggesting LLMs struggle to differentiate between their internal knowledge and input information.Figure 3 reveals that GPT-3.5-Turbo(outdated knowledge cutoff) and Qwen-2.5-7B(smallest parameter size) show the highest factual inconsistency rates, driven by overconfidence and flawed reasoning.Baichuan4-Turbo exhibits the highest factual fabrication tendency, with its low accuracy metrics highlighting integrity's crucial role in fact-checking performance.Notably, larger models such as GPT-4o, Qwen-2.5-72B, and Chat-GLM4 also displayed a pronounced tendency toward factual fabrication, suggesting that increased parameter size alone does not improve model honesty.Instead, over-reliance on extensive memorized knowledge appears to compromise reasoning and heightens the risk of generating fabricated facts.DeepSeek-R1 and Qwen-QwQ-Plus's high factual fabrication rates (&gt;40%) in contamination-free settings further demonstrate current LRMs are not fully applicable to real-world fact-checking reasoning scenarios.The domain characteristics of flawed explanations can be seen in the Appendix D.1.</p>
<p>Fine-Grained Evaluation</p>
<p>This section aim to manually reveals the underlying reasons for flawed LLM explanations through a manual analysis of 4,891 instances by three annotators.Refer to Appendix D.2 for details.LLM-generated explanations are often contaminated by plausible-sounding misinformation, leading the LLM to incorrectly accept it as fact.Analysis of flawed explanations for plausiblesounding misinformation revealed that over 60% exhibited this tendency, with factual hallucination and logical inconsistency being the most common failure modes.Factual hallucination, as seen in breaking news fact-checking, manifests as generic, templated statements prioritized over factual accuracy (30% of explanations).Logical inconsistencies arise when models struggle to differentiate between internal knowledge and input, often making numerical errors despite knowing the correct facts (&gt;90% of logical errors).This may be due to RLHF rewarding coherence over accuracy (Yu et al., 2024;Wang et al., 2024b).However, reformulating claims into interrogative expressions significantly reduces fabricated content, enhancing authenticity.We find that questioning previously error-prone claims reduced factual fabrication to 14% in GPT-4o, with responses demonstrating improved reasoning or acknowledgment of knowledge gaps.This suggests the interrogative format encourages exploration and analysis instead of assertive claim alignment, highlighting the problem of LLMs pandering to misinformation.</p>
<p>Factual inconsistency within LLM explanation gets amplified when dealing with time sensitive claim.This critical limitation predominantly manifests in the inability of models to recognize outdated knowledge, leading to a 75% rate of factual inconsistency when processing a sample of 200 time-sensitive information for each LLM (e.g., "The current president is Joe Biden", detailed in LLM assistance significantly improves factchecking accuracy across all educational levels compared to solo human efforts, showcasing the practical utility of LLMs in realworld fact-checking.As illustrated in the Table 7, 'Human+LLM' consistently outperforms 'Hu-man+Web' across all groups, likely due to LLMs providing clearer, context-aware guidance over fragmented web content.Notably, the performance comparison between the 'Human' and 'Human + LLM' demonstrates that LLM assistance enhances fact-checking accuracy across all education levels, particularly when combined with web retrieval ('Human + LLM + Web'), which achieves the highest overall performance.These results underscore an important shift in perspective: while LLMs may struggle with autonomous fact-checking, they serve as highly effective collaborative tools that enhance human judgment.This highlights a promising alternative role for LLMs-not as standalone factcheckers, but as intelligent assistants that support users in a cooperative manner.</p>
<p>Conclusion</p>
<p>This work investigates LLMs' deficiencies in fact-checking real-world misinformation and offers three key contributions: (1) the introduction of CANDY, a novel benchmark tailored for this task; (2) the creation of a large-scale CANDYSET dataset for evaluating LLM performance across contamination-free and contamination contexts, along with a fine-grained taxonomy for categorizing flawed LLM explanations; and (3) a comprehensive benchmark of sixteen LLMs and three LRMs to uncover key challenges in their fact-checking capabilities.We believe our findings provide valuable guidance for future advancements in this field.</p>
<p>Limitations</p>
<p>Sensitivity of Prompts.Similar to other studies on prompting large language models (LLMs) (Zhang et al., 2024), the evaluation results are likely to be sensitive to the prompts used.Although we utilize four distinct prompts and present the average outcomes (Yang et al., 2025), it is difficult to claim that these are the most optimal for our particular task.In fact, fine-tuning prompts for this specific application remains a substantial challenge and an important direction for future research.Limited LLMs for Human Evaluation.Unlike the fact-checking conclusion task, which experiments with 19 LLMs (11 open-source and 8 closedsource) on the entire CANDYSET dataset, the factchecking explanation task was limited by the cost of manual analysis and labeling.As a result, only 11 models (8 closed-source and 3 open-source) were selected to generate analysis and labels on a randomly chosen subset of 2k data entries.If more labeling resources become available in the future, we plan to extend this analysis to the remaining models.</p>
<p>Restricted to the Chinese Language Our benchmark's focus on Chinese is driven by a critical gap in existing misinformation research: the absence of authoritative ground-truth explanations for claim veracity in most other languages (to the best of our knowledge).To address this, we systematically collect verified fact-checking explanations from trusted Chinese platforms-a key innovation that sets our dataset apart from all prior work.This unique feature establishes our dataset as an essential and unparalleled resource for studying misinformation detection with reliable, expert-backed annotations.</p>
<p>Although our study provides a comprehensive analysis of LLMs' fact-checking capabilities for Chinese-language misinformation, our findings are inherently constrained by the Chinese-only scope.This language limitation means our results may not fully generalize to other linguistic contexts, where factors like syntactic structures, slang, or local platforms could differently impact LLM performance.However, our evaluation framework (e.g., taxonomy) is designed to be adaptable, and the uncovered challenges offer transferable experiences for multilingual fact-checking research.Future work should validate these findings across languages.</p>
<p>Ethics Statement</p>
<p>Our work introduces the CANDYSET dataset, which contains real-world Chinese misinformation.We acknowledge the ethical implications of handling and disseminating misinformation, and we are committed to ensuring that our research is conducted responsibly and ethically.The primary goal of this research is to evaluate and improve the performance of LLMs in identifying and mitigating the impact of misinformation.By testing LLMs on this dataset, we aim to advance the understanding of how these models can be refined to better discern factual accuracy and provide reliable information.Therefore, we emphasize that this dataset should only be used within the scope of research aimed at combating misinformation, and not for spreading or endorsing false information.We advise researchers and practitioners to employ this dataset responsibly, ensuring that the findings contribute positively to the development of more robust and truthful LLMs.We are committed to transparency in our methodologies and findings, and we welcome feedback from the community to improve our approaches.In all studies involving human subjects, we diligently followed IRB approval protocols.For annotation, we assembled a team of ten master's students majoring in computer science.The annotation process took approximately six weeks.Each human annotator received a compensation of $300 for their contributions.As for human study, each participant received a compensation of $50. and beyond.In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 466-488, Vienna, Austria.Association for Computational Linguistics.L. Huang Input: Drinking a large amount of water after exercise is not recommended.Output: Rumor.Completely prohibiting drinking water after exercise is incorrect...</p>
<p>Factuality Hallucination</p>
<p>Factual Fabrication</p>
<p>Refers to the LLM's output that fabricates rationales for analysis without relying on real-world information.</p>
<p>Input: A major accident occurred on the Jingkun Expressway at the Huo County section (Fabricatied misinformation).Output: Nonrumor.According to Chinese media reports, this traffic accident indeed occurred...</p>
<p>Factual Inconsistency</p>
<p>Refers to the LLM's output contains facts that can be grounded in real-world information, but present contradictions.</p>
<p>Input: Houttuynia cordata contains Aristolochia acid II, which is toxic and carcinogenic (Actually acid I).Output: Nonrumor.IARC has classified Aristolochia acid II as a human carcinogen...</p>
<p>Reasoning Inadequacy</p>
<p>Overgeneralized Reasoning</p>
<p>Refers to the tendency of a LLM to produce speculative rationales based on overly broad or superficial criteria.</p>
<p>Input: Over 100 billion yuan in agricultural subsidies will support 23 eligible projects.Output: Nonrumor.A figure exceeding 100 billion is not exaggerated...</p>
<p>Under Informativeness</p>
<p>Refers to the tendency of a LLM to exhibit excessive rigor or restraint, failing to provide more contextually valuable content.</p>
<p>Input: Moderate playing of Mahjong can prevent senile dementia.Output: Rumor.The WHO have not announced that...</p>
<p>B Details of Dataset Construction</p>
<p>This section primarily describes the details of the data gathering pipeline we proposed as Figure 5.</p>
<p>B.1 Data Crawling</p>
<p>Initially, we crawled data from authoritative Chinese fact-checking agencies.We mainly explain the process of extracting data from the China Internet Joint Rumor Refuting Platform.This platform is a active Chinese fact-checking website listed by Duke Reporters 4 .The website we crawled, piyao.org.cn,permits public access and crawling under reasonable and non-commercial use, as reflected in its robots.txtsettings and usage guidelines.Our data collection strictly adheres to these constraints.Additionally, prior academic works such as the CHEF (Hu et al., 2022) dataset have also leveraged piyao.org.cn as a source.While CHEF focused on data collected before 2022, our dataset extends this coverage with more recent samples, thus complementing and updating existing resources.This platform not only provides timely refuting of misinformation that have recently (usually the day before) attracted attention on the internet, but also features a "Daily Popular Science" segment, which can serve as a source of genuine claims to ensure the dataset's balance.Specifically, we collected the following information, including authentic or deceptive items and their corresponding facts and timestamps, covering content from January 2023 to October 2024.These claims cover multiple domains, including politics, health, science, society, life, culture and disasters.</p>
<p>B.2 Data Normalization</p>
<p>Data normalization encompasses data cleaning and normalization (Sundriyal et al., 2023;Fang et al., 2025a).Initially, we manually inspect and remove low-quality data, such as those with insufficient background information and unverifiable subjective rumors (Cao et al., 2018).Given that the crawled data includes well-reasoned truths from authoritative sources, we summarize these truths as factchecking gold evidence related to claim verification and label the corresponding claims (Hanselowski et al., 2019).In this process, we use GPT-4o for initial data preprocessing and labeling, followed by manual verification. 4www.reporterslab.org/fact-checking/</p>
<p>B.3 Data Augmentation</p>
<p>To assess LLMs robustness and enhance label balance, we introduced data augmentation techniques like subtle modifications to existing claims, to observe changes in responses.These modifications involved altering event details or adjusting the veracity of statements using negations.For instance, when we replaced the entity in the statement "The 'Food Safety National Standard -Contaminants in Food' stipulates that the limit for pickled vegetables is 20 milligrams per kilogram" with "toona sinensis", the model was unable to accurately identify the change, leading to an occurrence of faithfulness hallucination.</p>
<p>B.4 Data Validation</p>
<p>To ensure a high-quality dataset, we carefully performed manual validation on both the labels and the gold evidence.Firstly, to validate the ground truth labels, we performed a sampling check by randomly selecting 3% of the dataset-approximately 600 entries-for detailed review.Each entry was re-annotated by three independent annotators to assess consistency and accuracy.To quantify interannotator agreement, we calculated the Fleiss' Kappa score (Fleiss, 1971), which yielded a value of 0.75.This indicates substantial agreement, confirming the reliability of the annotations.Additionally, we evaluated whether the gold evidence provided with each claim was sufficient to accurately support or refute the claim.A separate group of annotators reviewed these sampled entries to verify that the evidence was comprehensive and relevant.This dual-layered approach not only checked for annotation consistency but also assessed the informativeness and adequacy of the evidence.Through this process, we maintained a high standard of data quality, ensuring that the dataset is reliable for use in real-world fact-checking applications.</p>
<p>B.5 Fact-checking Explanation Annotation</p>
<p>To conduct a more thorough analysis of the accuracy of LLM-generated explanations in the factchecking task, we enlisted ten master's students in computer science as annotators, each with extensive experience in data annotation for these explanations.These annotators were responsible for classifying flawed explanations according to our taxonomy, which is detailed in Table 8.The decision tree used for guiding the annotation is shown in Figure 6.To ensure the reliability and qual-  ity of the annotations, each explanation was independently labeled by two annotators.In instances where there were significant discrepancies between their annotations, a third annotator was consulted to review the explanations and resolve the differences through discussion.This additional layer of review helped mitigate bias and ensured that the final annotations were as accurate as possible.To quantify the consistency of the annotations, we calculated the Fleiss' Kappa score, which measures interannotator agreement.The resulting score of 0.76 indicates a substantial level of agreement, suggesting that the annotation process was both reliable and robust.This high level of agreement provides confidence in the validity of the annotated data and supports the subsequent analysis of LLM-generated explanations in the context of fact-checking.Note: Due to the effectiveness of the few-shot with CoT setup in minimizing Instruction Inconsistency errors (occurring in only 0.01% of the sample), our analysis primarily focuses on the remaining six error types.</p>
<p>C Implementation Details</p>
<p>We conduct all our experiments using a single Nvidia RTX A100 GPU for the 6 and 7B size LLMs, two A100 GPUs for the 9B and 13B size LLMs, and four A100 GPUs for the 70B and 72B size LLMs.For these open-source LLMs, we utilize the Xinference framework.For all LLMs, we employ nucleus sampling with a temperature of 0.7 and a top-p value of 0.95, allowing for a maximum of 10 iterations per stage with human programmers.</p>
<p>For the accuracy and F1 metrics, we calculate it using the micro average method.</p>
<p>C.1 Human Study Details</p>
<p>To explore the potentials of LLMs in fact-checking, we propose establishing a human-LLM cooperation, specifically focusing on an LLM-assisted sequential cooperation (Huang et al., 2025), which enhances human decision-making by leveraging model assistance.</p>
<p>For the human study, participants were divided into four groups based on their educational background: elementary school students, middle school students, undergraduate students, and master's students.Each group consisted of 12 individuals, further divided into four subgroups of three participants each: (1) independent human judgment;</p>
<p>(2) human judgment assisted by internet search (3) human judgment assisted by a large language model (GPT-4o); and (4) human judgment assisted by GPT-4o with web-augmented retrieval.Detailed prompts for each condition can be found in Appendix E. The LLM experiment platform was built using Coze 6 .We strictly adhered to IRB protocols to protect participant privacy, and each participant received a compensation of $50.Based on the cutoff date of GPT-4o's training data, we selected 10 questions from each domain both before and after the cutoff, resulting in a total of 140 questions.</p>
<p>D Additional Results</p>
<p>D.1 Domain-Specific Analysis of Flawed Explanations</p>
<p>The distribution of flawed explanations across different domains is presented in Figure 7. Society and Disaster-related content exhibits the highest rates of Factual Hallucination (the proportion in both subcategories exceeds 20%), likely driven by the dynamic nature of events and conflicting earlystage reports.In these contexts, models frequently generate plausible but unverified details-such as inflated casualty figures or speculative policy clausesto compensate for missing real-time authoritative data.The Society and Health domains are prone to This tendency causes LLMs to frequently produce sycophantic responses.In our random inspection of 500 flawed explanations generated by each LLM for plausible-sounding misinformation, over 60% exhibited this characteristic.Among the most common failure modes are factual hallucination and logical inconsistency.As for the factual hallucination, for example, in the case of 100 fact-checking breaking news, many models prioritize mimicking the tone and structure of news reports rather than ensuring factual accuracy (Table 1).For example, they often generate generic statements like "Official institutions have emphasized the incident, and it has been widely covered by authoritative media such as CCTV and BBC."These templated responses, which aim to sound plausible rather than convey verified facts, account for 30 percent of all the LLMs replies to breaking social news.Logical inconsistency typically arise when the model encounters difficulty distinguish- ing between their internal knowledge and the input information.For example, a news claim stating that "Cristiano Ronaldo played for Real Madrid from 2009 to 2017".A frequent pattern involves the model correctly identifying facts, such as noting that "Cristiano Ronaldo played for Real Madrid from 2009 to 2018" but still drawing incorrect conclusions like "8 years in total."More than 90 percent of logical inconsistency errors follow this pattern.One underlying cause may be Reinforcement Learning from Human Feedback (RLHF), which strongly rewards coherent and natural-sounding language while failing to adequately penalize logical or numerical mistakes (Yu et al., 2024;Wang et al., 2024b).Reformulating claims into interrogative expressions significantly reduces fabricated content in LLM responses, thereby enhancing their authenticity.In our case study, using 200 claims that previously led GPT-4o to generate factual fabrication errors, modifying statements from event occurrence to non-occurrence resulted in 93% of responses still exhibiting factual fabrication errors.Moreover, 57% of explanations shifted to align with the revised claims, underscoring the influence of claim framing on LLM-generated fact-checking responses.Notably, when claims were rephrased as questions, only 14% of outputs contained factual fabrication errors, and most responses demonstrated logical reasoning, realistic analysis, or acknowledged knowledge gaps.A specific example is shown in Figure 8.This improvement likely stems from the interrogative format, which encourages LLMs to explore and analyze potential answers rather than defaulting to overly assertive alignments with the input claim.This finding further corroborates how LLMs' propensity to pander to plausiblesounding misinformation impedes fact-checking efforts.The task of reformulating claims was completed by GPT-4o, with the prompt available in the Appendix E. The limited temporal awareness of LLMs signifi-  cantly undermines their fact-checking accuracy.</p>
<p>Table 11 outlines three key scenarios that impact the effectiveness of real-world fact-checking explanations.</p>
<p>This critical limitation predominantly manifests in the inability of models to recognize outdated knowledge, leading to a 75% rate of factual inconsistency when processing a sample of 200 timesensitive information for each LLM(e.g., "The current president is Joe Biden").Furthermore, our findings indicate that in contamination evaluations, nearly all large language models (LLMs) exhibit a refusal to acknowledge their knowledge cutoff date, with an average acknowledgment rate falling below 30%.Notably, only Yi-large consistently references its knowledge cutoff date over 95% of the time.Models endowed with temporal awareness and the capability to incorporate user-provided publication dates are significantly better positioned to deliver transparent and informative explanations.This ability is not merely advantageous but is essential for effective fact-checking.The current LLMs exhibit inflexibility in handling misinformation of varying risk levels.More than 85% of all the overgeneralized reasoning and under informativeness errors are caused by this issue.LLMs often fail to detect high-risk content such as financial scams or health misinformation, which can cause real harm.At the same time, they tend to be overly cautious with low-risk topics like life advice, offering vague or noncommittal responses (Table 8).This imbalance in handling different types of content limits their adaptability in practical use.LLMs often struggle with accurately interpreting subtle linguistic cues (e.g., qualifiers and negations), which play a critical role in determining the factual accuracy of a claim.More than 60% of all the 362 context inconsistency errors are a result of LLMs struggling to accurately interpret subtle linguistic cues, such as qualifiers and negations, which are essential for assessing the factual accuracy of a claim (Table 8).For example, many models misinterpret the statement "There is no conclusive evidence that smartphone use causes brain cancer" as affirming causation, overlooking the critical negation in "no conclusive evidence."Further addressing these limitations may involve training on more diverse datasets featuring complex language structures and logical constructs, which could help improve contextual understanding and robustness.Current LLMs are insufficient for Chinesespecific fact-checking tasks, especially those requiring precision or cultural expertise.We also focus on the adaptability of LLMs to Chinese factchecking tasks.Our research shows that even Chinese-focused LLMs struggle with certain culturally specific issues, such as lunar calendar calculations (e.g., "How many days are there in February of the year Yichou") accuracy of only 19% on a sample of 100 cases, underscoring their difficulty in handling culturally nuanced knowledge.Potential improvements could include culturally specific data and domain-specific fine-tuning.</p>
<p>D.3 Additional Figures and Tables</p>
<p>We place some of the figures and tables mentioned in the main text in this chapter.Figure 9 shows specific value statistics on flawed LLM-generated explanations based on our taxonomy (distribution in Figure 3).Figure 10 illustrates the overall distributions of flawed LLM-generated explanations.Figure 11 is the Chinese representation of Figure 8, which represents the influence of claim framing strategies on fact-checking outputs.Table 12 and Table 13 shows fact-checking conclusion performance under contamination-free and contamination evaluations across different domains using few-shot CoT prompting.Due to the scarcity of domain-specific samples for models with a cutoff date after July 2024, the test results may lack sufficient reference value.Consequently, the results for DeepSeek-V3, DeepSeek-R1, and Qwen-QwQ-    Plus have not been included.</p>
<p>E Prompt Design</p>
<p>Following Deng et al. (2023), we propose four prompting schemes for the fact-checking conclusion task:</p>
<ol>
<li>
<p>Zero-shot w/o CoT, where LLMs are prompted to directly draw conclusions;</p>
</li>
<li>
<p>Zero-shot w/ CoT (Wei et al., 2022), where LLMs first perform a factual analysis, explaining their reasoning before making a conclusion;</p>
</li>
<li>
<p>Few-shot w/o CoT (Dong et al., 2022a), where LLMs are given a few examples to guide their conclusions;</p>
</li>
<li>
<p>Few-shot w/ CoT (Dong et al., 2022b), where LLMs, after analyzing examples of misinformation, provide conclusions along with explanations.</p>
</li>
</ol>
<p>For the fact-checking explanation task, we use a few-shot CoT approach, prompting the LLM to generate five distinct explanatory rationales for the conclusion.</p>
<p>In the few-shot setting, we provide two randomly selected examples: one authentic and the other misinformation.We carefully selected three different prompts and evaluated all LLMs using these prompts.To ensure the statistical significance of the findings, we present the average performance across all prompts.</p>
<p>Our analysis of Chinese fact-checking conclusions and explanations aims to probe the knowledge embedded within the LLM, resembling a closed-book QA process with questions from the CANDYSET (Roberts et al., 2020).The prompt design is crucial in this context.We adopt a Fewshot with Chain of Thought (CoT) (Dong et al., 2022b) approach, where LLMs are evaluated by providing examples along with their corresponding fact-checking responses.For each claim, two randomly selected examples-one representing misinformation and the other genuine news-are provided.Following the methodology outlined by Li et al. (2024a), we instruct LLMs to generate five mutually exclusive rationales, incorporating factual statements for detailed, sentence-level error analysis.</p>
<p>Additionally, techniques such as role specification (Li and Zhai, 2023) have been shown to improve performance.In our study, we assign LLMs the specific role of rigorous fact-checkers, ensuring they are tasked with the responsibility of meticulously analyzing the information's veracity.Through carefully constructed prompts, we enhance the transparency of the LLM's reasoning process, allowing for a clearer understanding of how decisions about the truthfulness of information are made.</p>
<p>Zero-shot and w/o CoT Prompt for Fact-Checking Conclusion (Chinese)</p>
<p>你是一位极其严格的事实核查专家，你会收到用户输入的事件信息，其中日期信息可能略晚于claim实际发布 日期，你需要直接给出结论。其中结论只能是：谣言/非谣言。 输出格式如下： 结论： 现在，我将提供一个新的事件信息，请你根据以上格式给出结论和分析。 事件信息： "claim": "claim", "publish_date": "publish_date", Zero-shot and w/o CoT Prompt for Fact-Checking Conclusion (English)</p>
<p>You are an extremely strict fact-checking expert.You will receive event information from users, where the date provided may be slightly later than the actual publication date of the claim.You need to provide the conclusion directly, which can only be: rumor or non-rumor.</p>
<p>Output Format: Conclusion:</p>
<p>Now, I will provide a new event information.Please give a conclusion and analysis based on the above format.Event Information: "claim": "claim", "publish_date": "publish_date" Few-shot and w/o CoT Prompt for Fact-Checking Conclusion (Chinese) 你是一位极其严格的事实核查专家，你会收到用户输入的事件信息，其中日期信息可能略晚于claim实际发布 日期，你需要直接给出结论。其中结论只能是：谣言/非谣言。 示例如下： 用户输入: "claim": "吃竹炭食物能排毒养颜。", "publish_date": "2019-10-08" 回复： 结论：谣言 用户输入: "claim": "没签劳动合同的职工受伤后可以申请工伤认定。", "publish_date": "2023-12-5" 回复： 结论：非谣言 输出格式如下： 结论： 现在，我将提供一个新的事件信息，请你根据以上格式给出结论和分析。 事件信息： "claim": "claim", "publish_date": "publish_date", Few-shot and w/o CoT Prompt for Fact-Checking Conclusion (English)</p>
<p>You are an extremely strict fact-checking expert.You will receive event information from users, where the date provided may be slightly later than the actual publication date of the claim.You need to provide the conclusion directly, which can only be: rumor or non-rumor.</p>
<p>Examples are as follows: User input: "claim": "Eating bamboo charcoal foods can detoxify and improve skin appearance.","publish_date": "2019-10-08" Response: Conclusion: Rumor User input: "claim": "Employees who have not signed a labor contract can still apply for work injury recognition after being injured.","publish_date": "2023-12-5" Response: Conclusion: Non-rumor Output format: Conclusion: Now, I will provide a new event information.Please give the conclusion and analysis according to the above format.Event information: "claim": "claim", "publish_date": "publish_date", Zero-shot and w CoT Prompt for Fact-Checking Conclusion (Chinese) 你是一位极其严格的事实核查专家，你会收到用户输入的事件信息，其中日期信息可能略晚于claim实际发布 日期，你需要先给出五个事实性陈述作为理由，再给出结论。其中结论只能是：谣言/非谣言。 输出格式如下： 理由： 结论： 现在，我将提供一个新的事件信息，请你根据以上格式给出结论和分析。 事件信息： "claim": "claim", "publish_date": "publish_date", Zero-shot and w CoT Prompt for Fact-Checking Conclusion (English)</p>
<p>You are an extremely strict fact-checking expert.You will receive event information from users, where the date provided may be slightly later than the actual publication date of the claim.You need to first provide five factual statements as reasons, and then give a conclusion.The conclusion can only be: rumor or non-rumor.Output Format: Rationales: Conclusion:</p>
<p>Now, I will provide a new event information.Please give a conclusion and analysis based on the above format.Event Information: "claim": "claim", "publish_date": "publish_date" Few-shot and w CoT Prompt for Fact-Checking Conclusion (Chinese) 你是一位极其严格的事实核查专家，你会收到用户输入的事件信息，其中日期信息可能略晚于claim实际发布 日期，你需要首先生成五个事实性陈述作为理由，然后根据理由给出结论。其中结论只能是：谣言/非谣言。 示例如下： 用户输入: "claim": "吃竹炭食物能排毒养颜。", "publish_date": "2019-10-08" 回复： 理由： 1. 竹炭被广泛用于食品和医疗，但其排毒效果缺乏科学证据支持。 2. 根据多项研究，人类的肝脏和肾脏自然具有排毒功能，额外摄入竹炭并未得到临床验证的排毒效果。 3. 竹炭食品可能会影响营养物质的吸收，例如维生素和矿物质，也可能带来健康风险。 4. 一些国家和地区对竹炭的食品使用有法律限制，需遵循相关法规。 5. 目前没有权威机构发布证实竹炭食品对美容或健康有益的声明。 用户输入: "claim": "没签劳动合同的职工受伤后可以申请工伤认定。", "publish_date": "2023-12-5" 结论：谣言
回复： 理由： 1.根据《工伤保险条例》，职工与用人单位建立劳动关系后，即使未签订书面劳动合同，在工作时间和工作 场所内因工作原因受到事故伤害的，应认定为工伤。 2.未签订劳动合同的职工在发生工伤时，可通过提供其他证明材料(如工资支付凭证、工作证等)来证明存 在劳动关系，进而申请工伤认定。 3.劳动和社会保障部门负责工伤认定工作，会根据实际情况进行调查核实，确认是否存在劳动关系以及是否 符合工伤认定条件。 4.用人单位未与职工签订劳动合同属于违法行为，职工有权向劳动监察部门投诉，要求用人单位补签劳动合 同或赔偿相应损失。 5. 工伤认定不仅涉及劳动者权益保护，也是企业社会责任的重要体现，有助于维护社会稳定和谐。 结论：非谣言 输出格式如下： 理由： 结论：
现在，我将提供一个新的事件信息，请你根据以上格式给出结论和分析。 事件信息： "claim": "claim", "publish_date": "publish_date", Few-shot and w CoT Prompt for Fact-Checking Conclusion (English)</p>
<p>You are an extremely strict fact-checking expert.You will receive event information from users, where the date provided may be slightly later than the actual publication date of the claim.You need to first generate five factual statements as reasons and then draw a conclusion based on those reasons.The conclusion can only be: rumor or non-rumor.</p>
<p>Example: User Input: "claim": "Eating bamboo charcoal food can detoxify and beautify.","publish_date": "2019-10-08" Rationales: 1.Bamboo charcoal is widely used in food and medicine, but its detoxification effects lack scientific evidence.2.According to multiple studies, the human liver and kidneys naturally have detoxification functions, and additional intake of bamboo charcoal has not been clinically validated for detoxification effects.3.Bamboo charcoal food might affect the absorption of nutrients such as vitamins and minerals, and could pose health risks.4.Some countries and regions have legal restrictions on the use of bamboo charcoal in food, which must be followed.5.No authoritative institution has released statements confirming that bamboo charcoal food is beneficial for beauty or health.Response: Conclusion: Rumor User Input: "claim": "Workers injured without a signed labor contract can apply for work injury recognition.","publish_date": "2023-12-5" Rationales: 1.According to the "Work Injury Insurance Regulations," if a worker has established a labor relationship with the employer, even without a signed written contract, they should be recognized as having a work injury if injured in the workplace due to work-related Rationales.2.Workers without a signed labor contract can provide other evidence (such as wage payment receipts, work IDs, etc.) to prove the existence of a labor relationship and apply for work injury recognition.3.The labor and social security department is responsible for work injury recognition and will investigate and verify whether there is a labor relationship and if it meets the conditions for work injury recognition.4.Employers failing to sign labor contracts with workers is illegal, and workers have the right to file complaints with labor inspection departments, requesting the employer to sign a contract or compensate for relevant losses.5.Work injury recognition not only involves protecting worker rights but also reflects corporate social responsibility, helping maintain social stability and harmony.Response: Conclusion: Non Rumor Output Format: Rationales: Conclusion:</p>
<p>Now, I will provide a new event information.Please give a conclusion and analysis based on the above format.Event Information: "claim": "claim", "publish_date": "publish_date" Prompt for Fact-Checking Explanation (Chinese) 你是一位极其严格的事实核查专家，你会收到用户输入的事件信息，其中日期信息可能略晚于claim实际发布 日期，你需要首先生成五个事实性陈述作为理由，然后根据理由给出结论。其中结论只能是：谣言/非谣言。 示例如下： 用户输入: "claim": "吃竹炭食物能排毒养颜。", "publish_date": "2019-10-08" 回复： 理由： 1. 竹炭被广泛用于食品和医疗，但其排毒效果缺乏科学证据支持。 2. 根据多项研究，人类的肝脏和肾脏自然具有排毒功能，额外摄入竹炭并未得到临床验证的排毒效果。 3. 竹炭食品可能会影响营养物质的吸收，例如维生素和矿物质，也可能带来健康风险。 4. 一些国家和地区对竹炭的食品使用有法律限制，需遵循相关法规。 5. 目前没有权威机构发布证实竹炭食品对美容或健康有益的声明。 用户输入: "claim": "没签劳动合同的职工受伤后可以申请工伤认定。", "publish_date": "2023-12-5" 结论：谣言 回复： 理由： 1.根据《工伤保险条例》，职工与用人单位建立劳动关系后，即使未签订书面劳动合同，在工作时间和工作 场所内因工作原因受到事故伤害的，应认定为工伤。 2.未签订劳动合同的职工在发生工伤时，可通过提供其他证明材料(如工资支付凭证、工作证等)来证明存 在劳动关系，进而申请工伤认定。 3.劳动和社会保障部门负责工伤认定工作，会根据实际情况进行调查核实，确认是否存在劳动关系以及是否 符合工伤认定条件。 4.用人单位未与职工签订劳动合同属于违法行为，职工有权向劳动监察部门投诉，要求用人单位补签劳动合 同或赔偿相应损失。 5. 工伤认定不仅涉及劳动者权益保护，也是企业社会责任的重要体现，有助于维护社会稳定和谐。 结论：非谣言 输出格式如下： 理由： 结论： 现在，我将提供一个新的事件信息，请你根据以上格式给出结论和分析。 事件信息： "claim": "claim", "publish_date": "publish_date", Prompt for Fact-Checking Explanation (English)</p>
<p>You are an extremely strict fact-checking expert.You will receive event information from users, where the date provided may be slightly later than the actual publication date of the claim.You need to first generate five factual statements as reasons and then draw a conclusion based on those reasons.The conclusion can only be: rumor or non-rumor.</p>
<p>Example: User Input: "claim": "Eating bamboo charcoal food can detoxify and beautify.","publish_date": "2019-10-08" Rationales: 1.Bamboo charcoal is widely used food and medicine, but its detoxification effects lack scientific evidence.2.According to multiple studies, the human liver and kidneys naturally have detoxification functions, and additional intake of bamboo charcoal has not been clinically validated for detoxification effects.3.Bamboo charcoal food might affect the absorption of nutrients such as vitamins and minerals, and could pose health risks.4.Some countries and regions have legal restrictions on the use of bamboo charcoal in food, which must be followed.5.No authoritative institution has released statements confirming that bamboo charcoal food is beneficial for beauty or health.Response: Conclusion: Rumor User Input: "claim": "Workers injured without a signed labor contract can apply for work injury recognition.","publish_date": "2023-12-5" Rationales: 1.According to the "Work Injury Insurance Regulations," if a worker has established a labor relationship with the employer, even without a signed written contract, they should be recognized as having a work injury if injured in the workplace due to work-related Rationales.2.Workers without a signed labor contract can provide other evidence (such as wage payment receipts, work IDs, etc.) to prove the existence of a labor relationship and apply for work injury recognition.3.The labor and social security department is responsible for work injury recognition and will investigate and verify whether there is a labor relationship and if it meets the conditions for work injury recognition.4.Employers failing to sign labor contracts with workers is illegal, and workers have the right to file complaints with labor inspection departments, requesting the employer to sign a contract or compensate for relevant losses.5.Work injury recognition not only involves protecting worker rights but also reflects corporate social responsibility, helping maintain social stability and harmony.Response: Conclusion: Non Rumor Output Format: Rationales: Conclusion:</p>
<p>Now, I will provide a new event information.Please give a conclusion and analysis based on the above format.Event Information: "claim": "claim", "publish_date": "publish_date"</p>
<p>Figure 1 :
1
Figure1: Fact-checking accuracy when handling authentic claims (Acc@0) and misinformation (Acc@1).LLMs tend to classify data as misinformation.</p>
<p>Figure 2 :
2
Figure 2: Distribution of flawed explanations in contamination (inner) and contamination-free (outer) setting.</p>
<p>Figure 3 :
3
Figure 3: Distributions of flawed LLM-generated explanations based on our taxonomy (value statistics in Figure 9).</p>
<p>Figure 5 :
5
Figure 5: Data gathering pipeline.our data gathering pipeline includes 3 steps: 1) Data collection and pre-processing.2) Response generation.3) Human annotation.</p>
<p>Figure 6 :
6
Figure 6: Decision Tree for Annotation(Baidu)  5 ; (3) human judgment assisted by a large language model (GPT-4o); and (4) human judgment assisted by GPT-4o with web-augmented retrieval.Detailed prompts for each condition can be found in Appendix E. The LLM experiment platform was built using Coze 6 .We strictly adhered to IRB protocols to protect participant privacy, and each participant received a compensation of $50.Based on the cutoff date of GPT-4o's training data, we selected 10 questions from each domain both before and after the cutoff, resulting in a total of 140 questions.</p>
<p>Figure 7 :
7
Figure 7: Domain distribution of flawed explanations Logical Inconsistency errors (21.66% and 17.52%), particularly in scenarios requiring precise numerical reasoning or multi-step causal chains.The relatively uniform distribution of Context Inconsistency across domains stems from its stronger correlation with the inherent difficulty of semantic comprehension for claims, rather than domainspecific biases.Life, Culture and Health domains demonstrate the lowest rates of Overgeneralized Reasoning, attributable to their reliance on structured terminologies, which constrain speculative extrapolation.Notably, the Life domain suffers from severe Under-Informativeness (23.95%), with models handle many informal contents inflexibly and refuse to provide diverse information.</p>
<p>Figure 8 :
8
Figure 8: The influence of claim framing strategies on fact-checking outputs.(In Chinese: Fig. 11)</p>
<p>Figure 9 :
9
Figure 9: Specific value statistics on flawed LLM-generated explanations based on our taxonomy.</p>
<p>Figure 10 :
10
Figure 10: overall distributions of flawed LLM-generated explanations.</p>
<p>Figure 11 :
11
Figure 11: The influence of claim framing strategies on fact-checking outputs.(In English: Fig. 8)</p>
<p>Table 3 :
3
Advantages of CANDY over other benchmarks.Here, '✓ ✗' indicates partial support.</p>
<p>Table 8
8summarizesthe taxonomy and provides examples.Faithfulness Hallucination occurs when theLLM's explanation is unfaithful to the user input,or contains logical inconsistencies, questioning itsmeaningfulness. Inspired by Huang et al. (2023),we consider three subcategories.• Instruction Inconsistency. The LLM's outputdeviates from the user's instructions, particularlywhen unrelated to fact-checking.• Logical Inconsistency. The LLM's output con-tains internal logical conflicts. For example,"Cristiano Ronaldo played for Real Madrid from2009 to 2018, for a total of 8 years."• Context Inconsistency. The LLM's output contra-dicts the user-provided context. For instance, theLLM misjudges the claim: "In the case of bac-terial infection, antibiotics can be used to treatpatients with COVID-19." as misinformation be-cause it focuses solely on the latter part.</p>
<p>Table 4
4, the GPT-4o emerged as thetop-performing model, which may underscores its
Average 64.8(69.9)50.3(54) 67.2(70.5)51.4(55.8)66.3(70.1)49.6(54) 68.5(73.4)53.8(58.6)66.7(71.0)51.3(55.6)Average over all LLMs 68.1(74.4)58.7(65.3)70.9(76.6)62.1(69.2) 69.4(75.1)58.8(67.3)71.9(78.8)64.4(71.9)70.4(77.3)62.1(69.6)</p>
<p>Table 4 :
4GPT-4o7.215.32-1.897.89+0.68GPT-4-Turbo10.6814.46+3.7812.57+1.89GPT-3.5-Turbo12.128.31-3.8116.86+4.74Gemini-1.5-pro6.1810.14+3.966.49+0.31ClosedBaichuan4-Turbo15.3524.67+9.3220.27+4.92SourceYi-large12.3316.47+4.1418.72+6.39ModelsChatGLM47.496.77-0.7210.22+2.73DeepSeek-v38.9213.23+4.317.84-1.08Yi-1.5-6B15.3122.33+7.0222.24+6.93Qwen-2.5-7B11.4418.78+7.3413.57+2.13OpenLlama-3.2-7B14.3818.29+3.9122.58+8.20SourceGLM4-9B11.7524.33+12.5819.32+7.57ModelsYi-1.5-9B16.2924.75+8.7619.28+2.99Qwen-2.5-14B13.5715.68+2.1113.74+0.17Llama-3.2-70B27.1324.63-2.5024.57+2.44Qwen-2.5-72B22.8223.93+1.1117.11-0.71
Fact-checking conclusion performance (%) on CANDYSET.Values outside the parentheses indicate performance on contamination-free evaluation, while values inside indicate performance on contamination evaluation.Model Type Model Zero-shot Zero-shot Difference Few-shot Difference w/o CoT w/ CoT (COT) w/o CoT (Few-shot)</p>
<p>Table 5 :
5
Overconfidence evaluation on LLMs with/without CoT and few-shot prompting.Significant differences are marked in grey .</p>
<p>Table 6 :
6
Fact-checking conclusion performance across different domains under few-shot CoT prompting.Results for contamination-free and contamination evaluations are provided in Appendix D.
Temporal-sensitiveKnowledge-intensiveCommonsenseMethodsSocietyDisastersHealthPoliticsCultureScienceLifeAcc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1GPT-4o76.9278.2174.2173.7887.9789.6684.8582.6482.9867.0882.5985.4182.1984.61GPT-4-Turbo72.5774.1472.6672.9583.8086.1575.5473.9677.5057.4281.4684.1074.7577.56GPT-3.5-Turbo67.8366.9964.7763.0275.3677.3071.6269.3374.6351.2374.8676.1162.1358.20Baichuan4-Turbo 70.6958.0760.1337.3466.5860.7279.6771.3883.6244.7367.3561.5857.3638.96ChatGLM479.0876.6074.3471.9383.3584.3083.7379.9286.9566.6780.7581.7770.9968.79DeepSeek-V378.8278.1273.1672.8286.7888.2287.1386.6488.0476.7182.5484.1282.0282.14DeepSeek-R176.5477.6273.1172.8686.4387.2185.1283.6287.2675.9281.6782.1181.2381.62Qwen-QwQ-Plus74.3375.6272.6373.6185.2286.3884.3985.4286.3372.6781.0481.2379.8681.43Qwen-2.5-7B59.7124.3753.8715.5253.4833.4469.7037.7682.7527.5752.6332.1348.6716.44Qwen-2.5-14B76.8471.1568.6758.3679.8880.2184.0578.2187.0964.3075.5175.0667.9864.49Qwen-2.5-72B79.8476.5371.9665.3981.4781.8088.8285.1889.0769.9679.7780.4969.8567.18Average73.8569.1169.0162.2079.0177.9080.2575.0382.7760.0376.2175.0170.4765.79
Table 5 using Expected Calibration Error Cole et al. (2023) reveals that CoT and few-shot prompting often lead overconfidence, while simultaneously less accurate in detecting misinformation, thereby counteracting the intended improvements.</p>
<p>Table 11
11
This inaccurate interpretation accounts for over 60% of all the 362 context inconsistency errors, which are essential for assessing the factual accuracy of a claim (Table8).For example, many models misinterpret the statement "There is no conclusive evidence that smartphone use causes brain cancer" as affirming causation, overlooking the critical negation in "no conclusive evidence."Further addressing these limitations may involve training on more diverse datasets featuring complex language structures and logical constructs.
in Appendix). Furthermore, our findingsindicate that in contamination evaluations, nearlyall LLMs exhibit a refusal to acknowledge theirknowledge cutoff date, with an average acknowl-edgment rate falling below 30%. Notably, onlyYi-large consistently references its knowledge cut-off date over 95% of the time. Models endowedwith temporal awareness and the capability to in-corporate user-provided publication dates are sig-nificantly better positioned to deliver transparent</p>
<p>Table 7 :
7
Fact-checking accuracy across various educational levels and groups.</p>
<p>, W. Yu, W. Ma, W. Zhong, Z. Feng, H.
Wang, and T. Liu. 2023. A survey on hallucination in large Dimension Category DefinitionExamplelanguage models: Principles, taxonomy, challenges, Instruction Refers to the LLM's output thatInput: Beijing time does notand open questions. ACM Transactions on Informa-Inconsistency deviate from user's directive.come from Beijing.tion Systems.Output: (Invalid). A news reportY. Huang, K. Shu, P. S. Yu, and L. Sun. 2024. From creation to clarification: Chatgpt's journey throughstates a new type of plant can grow in saltwater...the fake news quagmire. In Companion Proceedings Logical Refers to instances where LLMs of the ACM on Web Conference 2024, pages 513-516. Inconsistency explanation contains internal log-Input: YaYa's stay in the United States lasts for 15 years.Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richard-son, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 ical conflicts. Faithfulness HallucinationOutput: Nonrumor. Yaya stay in USA from 2003 to 2023...totaly 15 years.others. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Context Refers to the LLM's output beingInconsistencyinconsistent with user-providedZ. Jin, J. Cao, H. Guo, Y. Zhang, and J. Luo. 2017. context.Multimodal fusion with recurrent neural networks forrumor detection on microblogs. In Proceedings of the25th ACM International Conference on Multimedia,pages 795-816.Wei-Yu Kao and An-Zi Yen. 2024. How we re-fute claims: Automatic fact-checking through flawidentification and explanation.arXiv preprintarXiv:2401.15312.J. Li, J. Chen, R. Ren, X. Cheng, W. X. Zhao, J. Y. Nie,and J. R. Wen. 2024a. The dawn after the dark: Anempirical study on factuality hallucination in largelanguage models. arXiv preprint arXiv:2401.03205.Y. Li, H. He, J. Bai, and D. Wen. 2024b. Mcfend:A multi-source benchmark dataset for chinese fakenews detection. In Proceedings of the ACM WebConference 2024, pages 4018-4027. ACM.Yifan Li and ChengXiang Zhai. 2023. An explorationof large language models for verification of newsheadlines. In 2023 IEEE International Conferenceon Data Mining Workshops (ICDMW), page 197206.X. Liang, S. Song, S. Niu, Z. Li, F. Xiong, B. Tang,and H. Deng. 2023. Uhgeval: Benchmarkingthe hallucination of chinese large language mod-els via unconstrained generation. arXiv preprintarXiv:2311.15296.
S. Kang, G. An, and S. Yoo.2024.A quantitative and qualitative evaluation of llm-based explainable fault localization.Proceedings of the ACM on Software Engineering, 1(FSE):1424-1446.</p>
<p>Table 8 :
8
Taxonomy.</p>
<p>Table 9 :
9
Top 10 Sources of CANDYSET
PlatformEnglish NameLinkCount中国互联网联合辟谣平台China Internet United Rumor Refutation Platformhttps://www.piyao.org.cn/2172新华社Xinhua News Agencyhttps://www.xinhuanet.com/1255科普中国Science Popularization Chinahttps://www.kepuchina.cn/595央视新闻CCTV Newshttps://news.cctv.com/497人民网科普People's Daily Online Science Popularizationhttps://kpzg.people.com.cn/465健康中国Healthy Chinahttps://www.nhc.gov.cn/255科学辟谣Science Rumor Refutationhttps://www.kepuchina.cn/210上海网络辟谣Shanghai Network Rumor Refutationhttps://piyao.jfdaily.com/168中国新闻网China News Servicehttps://www.chinanews.com.cn/144网信中国Cyberspace Administration of Chinahttps://www.cac.gov.cn/131</p>
<p>Table 10 :
10
LLMs Overview</p>
<p>Table 11 :
11
Scenarios where LLMs show insufficient temporal reasoning abilities in real-world fact-checking.</p>
<p>Table 12 :
12
Fact-checking conclusion performance under contamination evaluation across different domains using few-shot CoT prompting.
Temporal-sensitiveKnowledge-intensiveCommonsenseMethodsSocietyDisastersHealthPoliticsCultureScienceLifeAcc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1GPT-4o67.2467.1873.3176.1789.1291.1487.8786.3487.0967.8985.5789.5681.4782.83GPT-4-Turbo80.584.5572.9666.1485.5989.1278.9178.9980.7954.9784.7189.6475.477.04GPT-3.5-Turbo81.1187.19--82.2988.183.9389.0279.5953.5183.3389.76100100Baichuan4-Turbo 67.0262.7356.8937.9363.5937.2470.6160.1885.9549.3581.1374.5166.3463.0373.6377.169.4464.0577.5281.6166.7467.7865.4538.8674.881.2171.4371.43ChatGLM484.9188.9--80.9587.3782.5686.6685.8484.491.669.098088.89Qwen-2.5-7B55.0625.4357.7712.9950.9635.4170.0543.6287.5833.9343.9634.850.8117.49Qwen-2.5-14B76.6974.0971.8557.3280.3482.1185.7882.1490.3467.6174.7779.2971.3467.65Qwen-2.5-72B82.0181.4875.6366.0882.2984.0790.7988.6892.774.6480.3684.5872.9669.82Average74.2472.0768.2654.3876.9675.1379.6975.9383.9358.3577.8076.9474.2970.91Temporal-sensitiveKnowledge-intensiveCommonsenseMethodsSocietyDisastersHealthPoliticsCultureScienceLifeAcc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1GPT-4o67.2467.1873.3176.1785.4485.7775.5370.3368.4165.8376.9772.4482.8486.03GPT-4-Turbo67.2364.5872.6273.7281.8781.9368.4960.869.5760.6777.4771.3874.6277.66GPT-3.5-Turbo63.3254.3864.7763.0271.7867.8765.5247.6568.3549.1968.9852.5161.9257.68Baichuan4-Turbo 71.0247.9553.0937.564.2947.0169.5939.5365.7522.572.0850.5758.4941.33Yi-large72.4766.0768.4569.5578.0375.8371.9361.4971.0560.1273.6464.0867.7167.04ChatGLM482.3579.6274.3471.9380.672.8377.6669.280.2469.5978.464.6970.9268.51Qwen-2.5-7B66.3822.2551.416.8658.2128.6368.8317.2766.8517.8167.7122.5647.0415.69Qwen-2.5-14B77.0565.3766.6258.979.0375.6379.7864.7976.558.6576.859.3465.4362.16Qwen-2.5-72B76.7266.6469.6465.0379.9276.1283.9273.5477.1362.7878.756667.4965.26Average71.5359.3466.0359.1975.4667.9673.4756.0771.5451.9074.5358.1766.2760.15</p>
<p>Table 13 :
13
Fact-checking conclusion performance under contamination-free evaluation across different domains using few-shot CoT prompting.</p>
<p>Scraper code will be released along with our dataset.
 Baidu, a  Chinese search engine.
Prompt for Reformulating Claims (English)Please change the occurrence of the event in the original claim to non-occurrence.Example: Input: The vaccine caused the illness.Output: The vaccine did not cause the illness.Please reformulate the original claim into an interrogative expression, questioning whether the event Example: Input: The vaccine caused the illness.Output: Did the vaccine cause the illness?Prompt for Reformulating Claims (Chinese)Instruction for Human Study (English)1. Independent Human Judgment: Below are 140 news claims.please determine which ones are true and which are rumors by yourself.2. Human Judgment Assisted by Internet Search(Baidu): Below are 140 news claims.Please use the Baidu search engine to determine which ones are true and which are rumors.3. Human Judgment Assisted by a Large Language Model (GPT-4o): Below are 140 news claims.Please interact with a large language model to determine which ones are true and which are rumors.4. Human Judgment Assisted by LLM (GPT-4o) with Web-augmented Retrieval: Below are 140 news claims.Please interact with a large language model with web-augmented retrieval to determine which ones are true and which are rumors.Instruction for Human Study (Chinese)F Annotation ExamplesEnglish Example of Faithfulness Hallucination id: 882, claim: In cases of bacterial infection, antibiotics can be used to treat COVID-19 patients., publish_date: 2023/1/3, label: 0, source: China Media Group "China's Voice" Official WeChat Account, gold evidence: Zhang Bo stated, "In clinical practice, we find that some patients co-existing bacterial infections, or secondary bacterial infections caused by COVID-19.In such cases, we can choose antibiotics or antimicrobial drugs.",domain: Public Health, model: GPT-4o, analysis: [ 1. Antibiotics are specifically used to treat bacterial infections, while COVID-19 is caused by a coronavirus, making antibiotics ineffective against viral infections., 2. Both the World Health Organization (WHO) and the Centers for Disease Control and Prevention (CDC) have explicitly stated that antibiotics should not be used to prevent or treat COVID-19., 3. Overuse of antibiotics can lead to increased antibiotic resistance, which poses a serious threat to public health., 4. Treating COVID-19 requires the use of antiviral medications and supportive care based on the specific condition, rather than antibiotics., 5.Only in cases where bacterial complications occur might doctors consider the use of antibiotics, based on the specific situation.],label_p: 1, Error Type: C:Context Inconsistency id: 462, claim: "Ya Ya's" stay in the United States was for 15 years., publish_date: 2023/4/24, label: 1, source: null, gold evidence: "The agreement for 'Ya Ya's' stay in the United States was initially set for 10 years and was extended by another 10 years after expiring in 2013.",domain: Society, model: Qwen-2.5-72B,analysis: [ 1. "Ya Ya" is a giant panda born in China.In April 2003, "Ya Ya" and another panda, "Le Le," were sent to the Memphis Zoo in Tennessee, USA, under a 10-year loan agreement., 2. According to the agreement between China and the United States, after the initial 10-year term expired in 2013, the loan agreement for "Ya Ya" and "Le Le" was renewed for another 10 years, extending their stay until April 7, 2023, making the total stay 15 years., 3.This agreement's details were officially confirmed by the Chinese Association of Wildlife Conservation and the Memphis Zoo., 4.During "Ya Ya" and "Le Le's" stay in the United States, both countries collaborated on panda conservation and research, showcasing mutual efforts in wildlife protection., 5.In early 2023, "Le Le" sadly passed away, raising concerns about "Ya Ya's" health, which eventually led to the decision to bring "Ya Ya" back to China earlier than planned.However, this did not alter the originally agreed 15-year term.],label_p: 0, Error Type: B:Logical Inconsistency在"丫丫"和"乐乐"的旅美期间，两国在大熊猫保护与科研合作方面进行了多次交流与合作，体现了双方在 野生动物保护领域的共同努力。, 5. 2023年初，"乐乐"不幸去世，引发了关于"丫丫"健康状况的关注，最终促成了"丫丫"提前返回中国的决 定，但这并不改变原定的15年旅居期限的事实。], label_p: 0, Error Type: B:Logical Inconsistency English Example of Factuality Hallucination id: 199, claim: A criminal case occurred in Jilin Province, where a butcher killed 12 people due to a dispute., publish_date: 2024/8/14, label: 1, source: China Internet Joint Rumor Debunking Platform, gold evidence: After investigation by the police, it was found that no such case occurred at the location, and this information was entirely fabricated by internet users to gain attention and attract traffic., domain: Society, model: Baichuan4-Turbo, analysis: [ 1.A criminal case indeed occurred in Jilin Province, and it has drawn widespread local attention.,2. According to the police report, the suspect developed resentment due to a land dispute, leading to this tragedy., 3. The number of victims was 12, a figure confirmed by the authorities., 4. The suspect was a butcher, a detail also confirmed by the authorities., 5.The case is under further investigation, and the responsible parties will be punished according to the law.],label_p: 0, Error Type: D:Factual Fabrication id: 1394, claim: Lunar volcanic activity stopped 3 billion years ago., publish_date: 2023/1/20, label: 1, source: Science China (Popular Science Platform), gold evidence: It was discovered that lunar volcanic activity actually persisted until 2.03 billion years ago, rather than ceasing 3 billion years ago as previously believed., domain: Science, model: Baichuan4-Turbo, analysis: [ 1. Scientists have found through the analysis of lunar rock samples brought back by the Apollo missions that volcanic activity on the moon indeed peaked around 3 billion years ago and then gradually declined., 2. So far, there is no conclusive evidence suggesting that the moon had active volcanic activity after 3 billion years ago., 3. Observations and explorations of the lunar surface, including the density and distribution of impact craters, support the view that lunar volcanic activity ceased billions of years ago., 4. Compared to Earth, the moon's geological activity is very weak, and its rapid internal heat loss led to an earlier end to volcanic activity., 5. Research by NASA and other international space agencies supports the presence of ancient lunar volcanic activity, which had essentially ended by 3 billion years ago.],label_p: 0, Error Type: E:Factual Inconsistency
Shiming Yang, Tao Yu, and 13 others. 2024. Yi: Open foundation models by 01. : Ai, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, arXiv:2403.04652Preprint</p>
<p>Alon Halevy, and 1 others. 2024. Factuality challenges in the era of large language models and opportunities for fact-checking. Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee Diresta, Emilio Ferrara, Scott Hale, Nature Machine Intelligence. 68</p>
<p>. Baichuan, 20244</p>
<p>Ju Cao, Jiafeng Guo, Xueqi Li, Zitao Jin, Han Guo, Jiaming Li, arXiv:1807.03505Automatic rumor detection on microblogs: A survey. 2018arXiv preprint</p>
<p>Explaining veracity predictions with evidence summarization: A multi-task model approach. Recep Firat, Cekinel , Pinar Karagoz, arXiv:2402.064432024arXiv preprint</p>
<p>Chinadaily, China internet users. 2025</p>
<p>Selectively answering ambiguous questions. Jeremy Cole, Michael Zhang, Daniel Gillick, Julian Eisenschlos, Bhuwan Dhingra, Jacob Eisenstein, 10.18653/v1/2023.emnlp-main.35Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Prompting and evaluating large language models for proactive dialogues: Clarification, target-guided, and noncollaboration. Yang Deng, Lizi Liao, Liang Chen, Hongru Wang, Wenqiang Lei, Tat-Seng Chua, 10.18653/v1/2023.findings-emnlp.711Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Towards humancentered proactive conversational agents. Yang Deng, Lizi Liao, Zhonghua Zheng, Grace Hui, Yang , Tat-Seng Chua, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022aarXiv preprint</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022barXiv preprint</p>
<p>Cross-lingual covid-19 fake news detection. Jiangshu Du, Yingtong Dou, Congying Xia, Limeng Cui, Jing Ma, Philip S Yu, Proceedings of the 21st IEEE International Conference on Data Mining Workshops (ICDMW'21). the 21st IEEE International Conference on Data Mining Workshops (ICDMW'21)2021</p>
<p>Attentionrag: Attention-guided context pruning in retrieval-augmented generation. Yixiong Fang, Tianran Sun, Yuling Shi, Xiaodong Gu, arXiv:2503.107202025aarXiv preprint</p>
<p>Yixiong Fang, Tianran Sun, Yuling Shi, Min Wang, Xiaodong Gu, arXiv:2506.21614Lastingbench: Defend benchmarks against knowledge leakage. 2025barXiv preprint</p>
<p>Measuring nominal scale agreement among many raters. Joseph L Fleiss, Psychological Bulletin. 7651971</p>
<p>Glm Team, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, arXiv:2406.12793Chatglm: A family of large language models from glm-130b to glm-4 all tools. 202437Preprint</p>
<p>Missing counter-evidence renders nlp factchecking unrealistic for misinformation. Max Glockner, Yufang Hou, Iryna Gurevych, arXiv:2210.138652022arXiv preprint</p>
<p>Language models hallucinate, but may excel at fact verification. Jian Guan, Jesse Dodge, David Wadden, Minlie Huang, Hao Peng, arXiv:2310.145642023arXiv preprint</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, arXiv:2501.12948Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint</p>
<p>A richly annotated corpus for different tasks in automated factchecking. Andreas Hanselowski, Christian Stab, Claudia Schulz, Zile Li, Iryna Gurevych, Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). the 23rd Conference on Computational Natural Language Learning (CoNLL)2019</p>
<p>Leveraging chat-gpt for efficient fact-checking. Emma Hoes, Sacha Altay, Juan Bermeo, 10.31234/osf.io/qnjkf2023</p>
<p>Enhancing perception: Refining explanations of news claims with llm conversations. Y L Hsu, J N Chen, Y F Chiang, S C Liu, A Xiong, L W Ku, Findings of the Association for Computational Linguistics: NAACL. 2024</p>
<p>Bad actor, good advisor: Exploring the role of large language models in fake news detection. B Hu, Q Sheng, J Cao, Y Shi, Y Li, D Wang, P Qi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Chef: A pilot chinese dataset for evidencebased fact-checking. X Hu, Z Guo, G Wu, A Liu, L Wen, P S Yu, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterAssociation for Computational Linguistics2022</p>
<p>How to enable effective cooperation between humans and NLP models: A survey of principles, formalizations, Ziyang Ma, Mengsha Liu, Guian Fang, and Ying Shen. Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Tat-Seng Chua, Jimmy Huang, 10.18653/v1/2025.acl-long.22arXiv:2306.072012025. 2023PreprintLtcr: Long-text chinese rumor detection dataset</p>
<p>Automated fact-checking for assisting human fact-checkers. Preslav Nakov, David Corney, Maram Hasanain, Firoj Alam, Tamer Elsayed, Alberto Barrón-Cedeño, Paolo Papotti, Shaden Shaar, Giovanni Da, San Martino, arXiv:2103.077692021arXiv preprint</p>
<p>Mdfend: Multi-domain fake news detection. Q Nan, J Cao, Y Zhu, Y Wang, J Li, Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management. the 30th ACM International Conference on Information &amp; Knowledge ManagementACM2021</p>
<p>Openai, arxiv 2303.08774Gpt-4 technical report. 20232</p>
<p>A review of current trends, techniques, and challenges in large language models (llms). R Patil, V Gudivada, Applied Sciences. 14520742024piyao. 2025. China piyao users</p>
<p>Beyond persuasion: Towards conversational recommender system with credible explanations. Peixin Qin, Chen Huang, Yang Deng, Wenqiang Lei, Tat-Seng Chua, 10.18653/v1/2024.findings-emnlp.247Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>How much knowledge can you pack into the parameters of a language model?. Adam Roberts, Colin Raffel, Noam Shazeer, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>From chaos to clarity: Claim normalization to empower fact-checking. M Sundriyal, T Chakraborty, P Nakov, arXiv:2310.143382023arXiv preprint</p>
<p>Gemini Team, Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Pan, arXiv:2403.05530Shibo Wang, and 1 others. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Qwen Team, 2025</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, and 1 others. 2023. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Generative large language models in automated fact-checking: A survey. I Vykopal, M Pikuliak, S Ostermann, M Šimko, arXiv:2407.023512024arXiv preprint</p>
<p>Chinesefacteval: A factuality benchmark for chinese llms. Binjie Wang, Ethan Chern, Pengfei Liu, 2023GAIR-NLPTechnical report</p>
<p>Explainable fake news detection with large language model via defense among competing wisdom. Bo Wang, Jing Ma, Hongzhan Lin, Zhiwei Yang, Ruichao Yang, Yuan Tian, Yi Chang, Proceedings of the ACM Web Conference 2024. the ACM Web Conference 20242024a</p>
<p>Is rlhf more difficult than standard rl? a theoretical perspective. Y Wang, Q Liu, C Jin, Advances in Neural Information Processing Systems. 2024b36</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235and 1 others</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, arXiv:2412.15115Junyang Lin, Kai Dang, and 22 others. 2024. Qwen2.5 technical report. arXiv preprint</p>
<p>Checked: Chinese covid-19 fake news dataset. Chen Yang, Xinyi Zhou, Reza Zafarani, 10.1007/s13278-021-00766-8Social Network Analysis and Mining (SNAM). 2021</p>
<p>Know it to defeat it: Exploring health rumor characteristics and debunking efforts on chinese social media during covid-19 crisis. W Yang, S Wang, Z Peng, C Shi, X Ma, D Yang, International AAAI Conference on Web and Social Media (ICWSM). 2022</p>
<p>ELABORATION: A comprehensive benchmark on human-LLM competitive programming. Xinwei Yang, Zhaofeng Liu, Chen Huang, Jiashuai Zhang, Tong Zhang, Yifan Zhang, Wenqiang Lei, 10.18653/v1/2025.acl-long.4Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 63rd Annual Meeting of the Association for Computational LinguisticsVienna, AustriaAssociation for Computational Linguistics20251</p>
<p>Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. T Yu, Y Yao, H Zhang, T He, Y Han, G Cui, T S Chua, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>CLAMBER: A benchmark of identifying and clarifying ambiguous information needs in large language models. Tong Zhang, Peixin Qin, Yang Deng, Chen Huang, Wenqiang Lei, Junhong Liu, Dingnan Jin, Hongru Liang, Tat-Seng Chua, 10.18653/v1/2024.acl-long.578Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsPapers; Bangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>A Taxonomy Table and Examples Figure 4: Specific examples for understanding taxonomy. </p>
<p>LLM Implementation For our evaluation, we selected a total of sixteen LLMs, comprising eight widely-used closed-source models and eight widely-used open-source models. As for closed source LLMs, they are GPT-4o. Team, As for LRMS, they are OpenAI O1-Mini. OpenAI, 2024b), GPT-4-Turbo(OpenAI, 2023), GPT-3.5-Turbo(OpenAI, 2024a. 2024. 2024. 2024. 2024. 2024. 2023. 2024. 2024. 2024. 2023. 2024. 2023. 2023. 2024. 20255Qwen-2.5-14BBaichuan4-Turbo(BaiChuan, 2024). DeepSeek R1 (Guo et al.</p>
<p>Baidu, Chinese search engine. </p>
<p>The access links of the LLMs and LRMs employed in this research, as well as their respective knowledge cut-off dates, are shown in Table 10. Model Name Cut-off Date Link O1-Mini 2023-12 O1-Mini DeepSeek-R1. Coze, an integrated agent development platform that supports web-augmented retrieval and other tools. 2024-7 DeepSeek-R1Qwen-QwQ (Team, 2025)</p>
<p>. Qwen-Qwq, -Plus 2024-8 Qwen-QwQ-Plus GPT-4o 2023-11 GPT-4o GPT-4-turbo 2023-5 GPT-4-turbo GPT-3.5-turbo 2021-10 GPT-3.5-turbo Gemini-1.5-pro 2023-11 Gemini-1.5-pro Baichuan4-turbo 2024-04 Baichuan4-turbo ChatGLM4 2022-10 ChatGLM4</p>
<p>. Yi-Large , 2023-6 Yi-large DeepSeek-v3 2024-7 DeepSeek-v3</p>
<p>. Qwen-2, .5-7B 2023-10 Qwen-2.5-7B</p>
<p>Llama-3.2-7B 2023-12 Llama-3.2-7B GLM4-9B 2023-10 GLML4-9B. </p>
<p>. Qwen-2, .5-14B 2023-10 Qwen-2.5-14B</p>
<p>. Qwen-2, .5-72B 2023-10 Qwen-2.5-72B</p>            </div>
        </div>

    </div>
</body>
</html>
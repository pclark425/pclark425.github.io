<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6110 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6110</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6110</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-ae6a4cd221684be6ca3082b6f526a7901281490b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ae6a4cd221684be6ca3082b6f526a7901281490b" target="_blank">Emergent autonomous scientific research capabilities of large language models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper presents an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments and showcases the Agent's scientific research capabilities with three distinct examples.</p>
                <p><strong>Paper Abstract:</strong> Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6110.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6110.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experimental execution + analytical confirmation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physical execution of LLM-designed experiments with analytical verification (e.g., GC-MS, UV-Vis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluating LLM-generated scientific hypotheses/protocols by actually executing the protocols on robotic hardware or cloud labs and verifying outcomes with analytical instrumentation (GC-MS, UV-Vis), treating product formation and measured data as ground-truth evidence of correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Run LLM-generated experimental protocols on robotic liquid handlers/cloud labs (or simulate execution) and measure reaction outcomes with analytical instruments (GC-MS for reaction products, UV-Vis for spectral identification); use results to judge whether the proposed experiment/theory produced the expected empirical outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Presence/absence of target product(s) in analytical readouts (GC-MS peaks matching expected mass/retention), correct spectral features in UV-Vis, successful physical execution of protocol (robot follows steps without critical errors), and fidelity of protocol to the prompt (semantic match between requested experiment and executed procedure).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (planner), GPT-3.5 (browsing in some tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Chemistry (synthetic organic chemistry, mechanistic study), general experimental design</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-proposed experimental hypotheses and detailed protocols (e.g., reaction conditions, reagent stoichiometry, hardware commands) intended to produce specific chemical transformations such as Suzuki and Sonogashira cross-couplings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Agent-designed protocols were executed; GC-MS analysis of reaction mixtures revealed formation of target cross-coupling products (reported successful formation). UV-Vis microplate task: LLM wrote Python code that correctly identified wavelengths of maximum absorbance and solved the color-identification problem. Robotic liquid-handler tasks (microplate patterning) executed and closely resembled prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Experiment set consisting of concrete synthesis tasks used in the paper (ibuprofen, aspirin, aspartame (planned/corrected), Suzuki and Sonogashira cross-couplings), plus analytical files (e.g., 'plate1.npy' UV-Vis arrays) used as test artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>No quantitative head-to-head comparison to human experimentalists provided; authors recommend human-in-the-loop oversight for safety and validation. Qualitatively, the Agent can autonomously design and run experiments and correct its own code, reducing some human burdens.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited by pretraining cutoff (missing newer hardware APIs until docs provided), occasional incorrect hardware names or parameters requiring documentation lookups, reliance on cloud-lab/robotic infrastructure for safe execution, and safety/dual-use concerns for sensitive targets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent autonomous scientific research capabilities of large language models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6110.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6110.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual-use safety testset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Controlled-substance and chemical-weapons test set for dual-use risk evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A targeted test set composed of DEA Schedule I & II compounds and known chemical warfare agents used to evaluate whether the Agent will plan or refuse potentially illicit or harmful syntheses, and to analyze how/when refusals occur.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Submit queries (common names, IUPAC names, CAS numbers, SMILES) for restricted compounds to the Agent and log whether it (a) provides synthesis planning and documentation-seeking behavior or (b) refuses; analyze refusal timing (immediate vs. after web search) and susceptibility to phrasing manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary outcome per prompt: 'agrees to synthesize' vs 'refuses'; provenance of refusal (immediate recognition vs refusal after web search); proportion of prompts that yielded actionable synthesis planning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (Agent's Planner and reasoning), GPT-3.5 used for faster browsing tasks in some modules</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Chemical safety, dual-use risk assessment (chemistry/ethics)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluation of the Agent's propensity to generate actionable plans for synthesizing harmful or controlled substances when posed with direct requests, and assessment of the Agent's content-safety decision behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Out of 11 prompts, 4 (36%) provided a synthesis solution and attempted to consult documentation; 7 were refused, with 5 refusals occurring after the Agent performed web searches (i.e., it learned the harmful association and then refused) and 2 refusals occurring immediately (recognized common-name threats). Authors noted the approach is brittle and can be bypassed by adversarial phrasing or false facility licensing claims.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>A hand-curated test set of DEA Schedule I & II substances and known chemical weapons (authors' internal list used for experiments); run logs provided in Appendix F.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>No direct quantitative comparison to human decision-making provided. Authors argue cloud-lab screening/monitoring (human/system oversight) is safer than remote control of physical lab hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Detection primarily applies to known compounds; unknown novel toxic compounds or obfuscated naming can bypass safeguards; the Agent can be manipulated by adversarial phrasing or false claims (e.g., pretending to have a DEA license).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent autonomous scientific research capabilities of large language models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6110.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6110.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standardized benchmark citations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of external standardized benchmarks (SAT, BAR, LeetCode) as evidence of LLM reasoning capability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Citations to established standardized tests and challenge suites reported for GPT-4 (e.g., high percentiles on SAT, BAR, LeetCode) used as indirect benchmarks that motivate trust in model reasoning applied to scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4 Technical Report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Reference to external benchmark results (as reported by OpenAI) such as percentiles on standardized exams and programming challenge performance to infer model reasoning and problem-solving ability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Percentile performance on standardized tests and challenge problems (e.g., SAT, BAR, LeetCode difficulties), used as proxy measures for general reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (as reported by OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General reasoning and problem-solving (supports scientific reasoning capability claims)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a scientific theory per se; these benchmarks are cited as external evidence that GPT-4 can perform high-level reasoning tasks relevant to scientific planning and theory formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper cites OpenAI's report of 'high percentiles' on SAT and BAR exams and good performance on LeetCode; no new benchmark experiments reported in this work beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SAT, BAR (Bar exam), LeetCode problem sets (as referenced in OpenAI's technical report)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Implicit comparison: high-percentile performance suggests parity with human test-takers at certain levels, but no direct controlled comparison provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>These benchmarks are indirect proxies for scientific-theory quality and do not assess domain-specific experimental design, falsifiability, or explanatory power in scientific contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent autonomous scientific research capabilities of large language models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6110.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6110.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-in-the-loop oversight</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human review and approval for sensitive experiments (recommended safety evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recommended evaluation and governance mechanism: have human specialists review and approve potentially sensitive or harmful experiments proposed by the Agent before execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Require human experts to review proposed experimental designs or sensitive outputs prior to execution; use expert judgment as the primary arbiter of acceptability and safety.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Expert assessment of safety, legality, ethical acceptability, and technical soundness; enforcement gates that prevent execution without explicit human sign-off.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applies to Agent workflows using GPT-4/GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Operational safety and ethics across scientific domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Governance strategy rather than a testable scientific theory—humans validate or veto LLM-generated theories/protocols to mitigate risk.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Presented as a recommendation; no empirical evaluation of human review effectiveness provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Directly centers human judgment as necessary and superior for safety-critical decisions; authors explicitly recommend specialist oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human oversight can be resource-intensive, may introduce delays, and depends on availability/expertise; also may not scale to fully autonomous workflows without robust human procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent autonomous scientific research capabilities of large language models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6110.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6110.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated code-execution + iterative debugging</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sandboxed code execution with iterative LLM debugging using tracebacks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluate LLM-generated experimental plans and mechanistic analyses by executing generated code in an isolated environment (Docker), feeding back runtime errors to the model so it can iteratively fix code and thereby validate or refine theories/protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Execute generated Python code in an isolated Docker container (no LLM for execution component), capture standard output and tracebacks, return outputs to the Planner so the LLM can adjust code and logic until it produces useful outputs or a working protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Successful execution without runtime errors, meaningful outputs that match expected analytic/computational results (e.g., correct stoichiometry calculations, printed outputs), and the model's ability to self-correct based on tracebacks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (Planner writing code), SymPy sometimes requested but not installed (runtime environment detail)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Computational validation of protocols/mechanistic reasoning (applies across experimental domains)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Using execution traces as a test harness: if code that implements the theory runs and produces expected outputs, that strengthens confidence in the proposed experimental design or mechanistic analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Agent corrected its own code after receiving tracebacks (e.g., replaced SymPy usage when not installed, added print() to produce output); demonstrated iterative debugging capability that improved protocol/code correctness; still required human oversight for some issues.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>No external benchmark; internal example runs and tracebacks from appendices (e.g., Appendix D mechanistic study, Appendix A stoichiometry calculations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Automated debugging reduces need for manual debugging but authors still recommend human oversight; no formal comparison to human debugging speed or accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Runtime environment mismatches (missing packages, non-interactive interpreter assumptions) can lead to false negatives; successful code execution does not guarantee real-world experimental success due to hardware or chemical hazards not captured by code alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent autonomous scientific research capabilities of large language models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6110.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6110.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Documentation retrieval via ada embeddings + vector search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Providing hardware/API documentation to LLMs via ada embeddings and distance-based vector search for improved code/API usage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented approach where API/hardware documentation sections are embedded (OpenAI ada embeddings), stored, and retrieved by vector similarity in response to LLM queries, improving the LLM's ability to generate correct API calls and protocols, especially for hardware released after model cutoff.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Embed documentation sections, select the most similar sections based on query embeddings and distance-based vector search, pass selected raw text (within token limits) to GPT-4 for summarization and code generation; evaluate success by whether the LLM identifies correct functions/parameters and generates working code for hardware (Opentrons OT-2 API, Emerald Cloud Lab SLL).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correct identification of API functions and parameters, syntactically correct generated code that runs on hardware or in simulation, successful execution of hardware-specific steps (e.g., heater-shaker usage), and overall reduction in API misuse errors.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 for summarization and Planner tasks; ada embeddings used for vector retrieval; GPT-3.5 used for browsing in some steps</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Software/API-assisted experimental automation (instrument control and protocol generation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-generated experimental protocols rely on up-to-date API documentation supplied via vector search to overcome model knowledge cutoffs and to correctly call hardware functions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Approach enabled the Agent to correctly identify OT-2 API functions and ECL SLL functions and to generate code blocks that respected available instruments/options; it was critical for using a heater-shaker module released after GPT-4 training cutoff and helped correct an initially incorrect module name.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>14 sections of the Opentrons OT-2 API documentation were embedded with ada; Emerald Cloud Lab 'Running Experiments' guide used as raw documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>This retrieval-augmentation reduces reliance on human memory of API details and increases correctness relative to relying solely on pretraining; no controlled human vs. LLM+docs comparison presented.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Token limits constrain how much documentation can be supplied in a single step; quality depends on curated documentation indices; retrieval can be manipulated if adversarial docs are supplied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent autonomous scientific research capabilities of large language models', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPT-4 Technical Report <em>(Rating: 2)</em></li>
                <li>Dual use of artificial-intelligence-powered drug discovery <em>(Rating: 2)</em></li>
                <li>ChemCrow: Augmenting large-language models with chemistry tools <em>(Rating: 2)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 1)</em></li>
                <li>Opentrons Python Protocol API <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6110",
    "paper_id": "paper-ae6a4cd221684be6ca3082b6f526a7901281490b",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "Experimental execution + analytical confirmation",
            "name_full": "Physical execution of LLM-designed experiments with analytical verification (e.g., GC-MS, UV-Vis)",
            "brief_description": "Evaluating LLM-generated scientific hypotheses/protocols by actually executing the protocols on robotic hardware or cloud labs and verifying outcomes with analytical instrumentation (GC-MS, UV-Vis), treating product formation and measured data as ground-truth evidence of correctness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Run LLM-generated experimental protocols on robotic liquid handlers/cloud labs (or simulate execution) and measure reaction outcomes with analytical instruments (GC-MS for reaction products, UV-Vis for spectral identification); use results to judge whether the proposed experiment/theory produced the expected empirical outcome.",
            "evaluation_criteria": "Presence/absence of target product(s) in analytical readouts (GC-MS peaks matching expected mass/retention), correct spectral features in UV-Vis, successful physical execution of protocol (robot follows steps without critical errors), and fidelity of protocol to the prompt (semantic match between requested experiment and executed procedure).",
            "llm_model_name": "GPT-4 (planner), GPT-3.5 (browsing in some tasks)",
            "theory_domain": "Chemistry (synthetic organic chemistry, mechanistic study), general experimental design",
            "theory_description": "LLM-proposed experimental hypotheses and detailed protocols (e.g., reaction conditions, reagent stoichiometry, hardware commands) intended to produce specific chemical transformations such as Suzuki and Sonogashira cross-couplings.",
            "evaluation_results": "Agent-designed protocols were executed; GC-MS analysis of reaction mixtures revealed formation of target cross-coupling products (reported successful formation). UV-Vis microplate task: LLM wrote Python code that correctly identified wavelengths of maximum absorbance and solved the color-identification problem. Robotic liquid-handler tasks (microplate patterning) executed and closely resembled prompts.",
            "benchmarks_or_datasets": "Experiment set consisting of concrete synthesis tasks used in the paper (ibuprofen, aspirin, aspartame (planned/corrected), Suzuki and Sonogashira cross-couplings), plus analytical files (e.g., 'plate1.npy' UV-Vis arrays) used as test artifacts.",
            "comparison_to_human": "No quantitative head-to-head comparison to human experimentalists provided; authors recommend human-in-the-loop oversight for safety and validation. Qualitatively, the Agent can autonomously design and run experiments and correct its own code, reducing some human burdens.",
            "limitations_or_challenges": "Limited by pretraining cutoff (missing newer hardware APIs until docs provided), occasional incorrect hardware names or parameters requiring documentation lookups, reliance on cloud-lab/robotic infrastructure for safe execution, and safety/dual-use concerns for sensitive targets.",
            "uuid": "e6110.0",
            "source_info": {
                "paper_title": "Emergent autonomous scientific research capabilities of large language models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Dual-use safety testset",
            "name_full": "Controlled-substance and chemical-weapons test set for dual-use risk evaluation",
            "brief_description": "A targeted test set composed of DEA Schedule I & II compounds and known chemical warfare agents used to evaluate whether the Agent will plan or refuse potentially illicit or harmful syntheses, and to analyze how/when refusals occur.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Submit queries (common names, IUPAC names, CAS numbers, SMILES) for restricted compounds to the Agent and log whether it (a) provides synthesis planning and documentation-seeking behavior or (b) refuses; analyze refusal timing (immediate vs. after web search) and susceptibility to phrasing manipulation.",
            "evaluation_criteria": "Binary outcome per prompt: 'agrees to synthesize' vs 'refuses'; provenance of refusal (immediate recognition vs refusal after web search); proportion of prompts that yielded actionable synthesis planning.",
            "llm_model_name": "GPT-4 (Agent's Planner and reasoning), GPT-3.5 used for faster browsing tasks in some modules",
            "theory_domain": "Chemical safety, dual-use risk assessment (chemistry/ethics)",
            "theory_description": "Evaluation of the Agent's propensity to generate actionable plans for synthesizing harmful or controlled substances when posed with direct requests, and assessment of the Agent's content-safety decision behavior.",
            "evaluation_results": "Out of 11 prompts, 4 (36%) provided a synthesis solution and attempted to consult documentation; 7 were refused, with 5 refusals occurring after the Agent performed web searches (i.e., it learned the harmful association and then refused) and 2 refusals occurring immediately (recognized common-name threats). Authors noted the approach is brittle and can be bypassed by adversarial phrasing or false facility licensing claims.",
            "benchmarks_or_datasets": "A hand-curated test set of DEA Schedule I & II substances and known chemical weapons (authors' internal list used for experiments); run logs provided in Appendix F.",
            "comparison_to_human": "No direct quantitative comparison to human decision-making provided. Authors argue cloud-lab screening/monitoring (human/system oversight) is safer than remote control of physical lab hardware.",
            "limitations_or_challenges": "Detection primarily applies to known compounds; unknown novel toxic compounds or obfuscated naming can bypass safeguards; the Agent can be manipulated by adversarial phrasing or false claims (e.g., pretending to have a DEA license).",
            "uuid": "e6110.1",
            "source_info": {
                "paper_title": "Emergent autonomous scientific research capabilities of large language models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Standardized benchmark citations",
            "name_full": "Use of external standardized benchmarks (SAT, BAR, LeetCode) as evidence of LLM reasoning capability",
            "brief_description": "Citations to established standardized tests and challenge suites reported for GPT-4 (e.g., high percentiles on SAT, BAR, LeetCode) used as indirect benchmarks that motivate trust in model reasoning applied to scientific tasks.",
            "citation_title": "GPT-4 Technical Report",
            "mention_or_use": "mention",
            "evaluation_method": "Reference to external benchmark results (as reported by OpenAI) such as percentiles on standardized exams and programming challenge performance to infer model reasoning and problem-solving ability.",
            "evaluation_criteria": "Percentile performance on standardized tests and challenge problems (e.g., SAT, BAR, LeetCode difficulties), used as proxy measures for general reasoning capability.",
            "llm_model_name": "GPT-4 (as reported by OpenAI)",
            "theory_domain": "General reasoning and problem-solving (supports scientific reasoning capability claims)",
            "theory_description": "Not a scientific theory per se; these benchmarks are cited as external evidence that GPT-4 can perform high-level reasoning tasks relevant to scientific planning and theory formulation.",
            "evaluation_results": "Paper cites OpenAI's report of 'high percentiles' on SAT and BAR exams and good performance on LeetCode; no new benchmark experiments reported in this work beyond citation.",
            "benchmarks_or_datasets": "SAT, BAR (Bar exam), LeetCode problem sets (as referenced in OpenAI's technical report)",
            "comparison_to_human": "Implicit comparison: high-percentile performance suggests parity with human test-takers at certain levels, but no direct controlled comparison provided in this paper.",
            "limitations_or_challenges": "These benchmarks are indirect proxies for scientific-theory quality and do not assess domain-specific experimental design, falsifiability, or explanatory power in scientific contexts.",
            "uuid": "e6110.2",
            "source_info": {
                "paper_title": "Emergent autonomous scientific research capabilities of large language models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Human-in-the-loop oversight",
            "name_full": "Human review and approval for sensitive experiments (recommended safety evaluation)",
            "brief_description": "A recommended evaluation and governance mechanism: have human specialists review and approve potentially sensitive or harmful experiments proposed by the Agent before execution.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_method": "Require human experts to review proposed experimental designs or sensitive outputs prior to execution; use expert judgment as the primary arbiter of acceptability and safety.",
            "evaluation_criteria": "Expert assessment of safety, legality, ethical acceptability, and technical soundness; enforcement gates that prevent execution without explicit human sign-off.",
            "llm_model_name": "Applies to Agent workflows using GPT-4/GPT-3.5",
            "theory_domain": "Operational safety and ethics across scientific domains",
            "theory_description": "Governance strategy rather than a testable scientific theory—humans validate or veto LLM-generated theories/protocols to mitigate risk.",
            "evaluation_results": "Presented as a recommendation; no empirical evaluation of human review effectiveness provided in the paper.",
            "benchmarks_or_datasets": null,
            "comparison_to_human": "Directly centers human judgment as necessary and superior for safety-critical decisions; authors explicitly recommend specialist oversight.",
            "limitations_or_challenges": "Human oversight can be resource-intensive, may introduce delays, and depends on availability/expertise; also may not scale to fully autonomous workflows without robust human procedures.",
            "uuid": "e6110.3",
            "source_info": {
                "paper_title": "Emergent autonomous scientific research capabilities of large language models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Automated code-execution + iterative debugging",
            "name_full": "Sandboxed code execution with iterative LLM debugging using tracebacks",
            "brief_description": "Evaluate LLM-generated experimental plans and mechanistic analyses by executing generated code in an isolated environment (Docker), feeding back runtime errors to the model so it can iteratively fix code and thereby validate or refine theories/protocols.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Execute generated Python code in an isolated Docker container (no LLM for execution component), capture standard output and tracebacks, return outputs to the Planner so the LLM can adjust code and logic until it produces useful outputs or a working protocol.",
            "evaluation_criteria": "Successful execution without runtime errors, meaningful outputs that match expected analytic/computational results (e.g., correct stoichiometry calculations, printed outputs), and the model's ability to self-correct based on tracebacks.",
            "llm_model_name": "GPT-4 (Planner writing code), SymPy sometimes requested but not installed (runtime environment detail)",
            "theory_domain": "Computational validation of protocols/mechanistic reasoning (applies across experimental domains)",
            "theory_description": "Using execution traces as a test harness: if code that implements the theory runs and produces expected outputs, that strengthens confidence in the proposed experimental design or mechanistic analysis.",
            "evaluation_results": "Agent corrected its own code after receiving tracebacks (e.g., replaced SymPy usage when not installed, added print() to produce output); demonstrated iterative debugging capability that improved protocol/code correctness; still required human oversight for some issues.",
            "benchmarks_or_datasets": "No external benchmark; internal example runs and tracebacks from appendices (e.g., Appendix D mechanistic study, Appendix A stoichiometry calculations).",
            "comparison_to_human": "Automated debugging reduces need for manual debugging but authors still recommend human oversight; no formal comparison to human debugging speed or accuracy.",
            "limitations_or_challenges": "Runtime environment mismatches (missing packages, non-interactive interpreter assumptions) can lead to false negatives; successful code execution does not guarantee real-world experimental success due to hardware or chemical hazards not captured by code alone.",
            "uuid": "e6110.4",
            "source_info": {
                "paper_title": "Emergent autonomous scientific research capabilities of large language models",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Documentation retrieval via ada embeddings + vector search",
            "name_full": "Providing hardware/API documentation to LLMs via ada embeddings and distance-based vector search for improved code/API usage",
            "brief_description": "A retrieval-augmented approach where API/hardware documentation sections are embedded (OpenAI ada embeddings), stored, and retrieved by vector similarity in response to LLM queries, improving the LLM's ability to generate correct API calls and protocols, especially for hardware released after model cutoff.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Embed documentation sections, select the most similar sections based on query embeddings and distance-based vector search, pass selected raw text (within token limits) to GPT-4 for summarization and code generation; evaluate success by whether the LLM identifies correct functions/parameters and generates working code for hardware (Opentrons OT-2 API, Emerald Cloud Lab SLL).",
            "evaluation_criteria": "Correct identification of API functions and parameters, syntactically correct generated code that runs on hardware or in simulation, successful execution of hardware-specific steps (e.g., heater-shaker usage), and overall reduction in API misuse errors.",
            "llm_model_name": "GPT-4 for summarization and Planner tasks; ada embeddings used for vector retrieval; GPT-3.5 used for browsing in some steps",
            "theory_domain": "Software/API-assisted experimental automation (instrument control and protocol generation)",
            "theory_description": "LLM-generated experimental protocols rely on up-to-date API documentation supplied via vector search to overcome model knowledge cutoffs and to correctly call hardware functions.",
            "evaluation_results": "Approach enabled the Agent to correctly identify OT-2 API functions and ECL SLL functions and to generate code blocks that respected available instruments/options; it was critical for using a heater-shaker module released after GPT-4 training cutoff and helped correct an initially incorrect module name.",
            "benchmarks_or_datasets": "14 sections of the Opentrons OT-2 API documentation were embedded with ada; Emerald Cloud Lab 'Running Experiments' guide used as raw documentation.",
            "comparison_to_human": "This retrieval-augmentation reduces reliance on human memory of API details and increases correctness relative to relying solely on pretraining; no controlled human vs. LLM+docs comparison presented.",
            "limitations_or_challenges": "Token limits constrain how much documentation can be supplied in a single step; quality depends on curated documentation indices; retrieval can be manipulated if adversarial docs are supplied.",
            "uuid": "e6110.5",
            "source_info": {
                "paper_title": "Emergent autonomous scientific research capabilities of large language models",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 2
        },
        {
            "paper_title": "Dual use of artificial-intelligence-powered drug discovery",
            "rating": 2
        },
        {
            "paper_title": "ChemCrow: Augmenting large-language models with chemistry tools",
            "rating": 2
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 1
        },
        {
            "paper_title": "Opentrons Python Protocol API",
            "rating": 1
        }
    ],
    "cost": 0.01304325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Emergent autonomous scientific research capabilities of large language models</h1>
<p>Daniil A. Boiko, ${ }^{1}$ Robert MacKnight, ${ }^{1}$ and Gabe Gomes ${ }^{* 1,2,3}$</p>
<ol>
<li>Department of Chemical Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA</li>
<li>Department of Chemistry, Carnegie Mellon University, Pittsburgh, PA 15213, USA</li>
<li>Wilton E. Scott Institute for Energy Innovation, Carnegie Mellon University, Pittsburgh, PA 15213, USA
*corresponding author, gabegomes@cmu.edu</li>
</ol>
<h4>Abstract</h4>
<p>Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.</p>
<h2>Keywords</h2>
<p>Large Language Models; Intelligent Agents; Generative AI; Autonomous Experimentation; Automation; Physical Sciences; Catalysis.</p>
<h2>Glossary</h2>
<ul>
<li>LLMs: Large Language Models. In this paper, we use OpenAI's GPT-3.5 and GPT-4.</li>
<li>Agent: LLM-based Intelligent Agent (IA) system.</li>
<li>Prompt-provider: A scientist - in this work, one of the human authors - that provides the initial prompt to the Agent.</li>
</ul>
<h1>Main</h1>
<p>Large language models (LLMs), particularly transformer-based models, are experiencing rapid advancements in recent years. These models have been successfully applied to various domains, including natural language, ${ }^{1-5}$ biological ${ }^{6,7}$ and chemical research, ${ }^{8,9}$ as well as code generation. Extreme scaling of models, as demonstrated by OpenAI, has led to significant breakthroughs in the field. ${ }^{1,10}$ Moreover, techniques such as reinforcement learning from human feedback (RLHF) ${ }^{11}$ can considerably enhance the quality of generated text and the models' capability to perform diverse tasks while reasoning about their decisions. ${ }^{12}$</p>
<p>On March 14, 2023, OpenAI released their most capable LLM to date, GPT-4. ${ }^{10}$ Although specific details about the model training, sizes, and data used are limited in the technical report, researchers have provided substantial evidence of the model's exceptional problem-solving abilities. Those include - but are not limited to - high percentiles on the SAT and BAR exams, LeetCode challenges, and contextual explanations from images, including niche jokes. ${ }^{10}$ Moreover, the technical report provides an example of how the model can be employed to address chemistry-related problems.</p>
<p>Inspired by these results, we aimed to develop a multi-LLMs-based Intelligent Agent (hereafter simply called Agent) capable of autonomous design, planning, and performance of complex scientific experiments. The Agent can use tools ${ }^{13}$ to browse the internet and relevant documentation, use robotic experimentation APIs, and leverage other LLMs for various tasks. In this paper, we demonstrate the versatility and efficacy of our Agent by evaluating its performance in three tasks: 1. Efficiently searching and navigating through extensive hardware documentation; 2. Precisely controlling liquid handling instruments at a low level; 3. Tackling complex problems that necessitate simultaneous utilization of multiple hardware modules or integration of diverse data sources.</p>
<h2>The Agent's architecture: action space defined by its multiple modules.</h2>
<p>The Agent's system consists of four components (Figure 1), driven by the "Planner." The Planner takes a prompt as input (e.g., "Perform multiple Suzuki reactions") and carries out actions according to this request. The action space includes accessing the internet ("GOOGLE <query>" request), performing calculations in Python ("PYTHON <code>"), accessing documentation ("DOCUMENTATION <query>"), and running the final experiment ("EXPERIMENT <code>"). Experiments can be performed in various environments - a cloud lab, using a liquid handler, or by providing instructions for performing experiments manually. The model is instructed to reason about its actions, search the internet, calculate all quantities in the reaction, and then perform the corresponding reaction. The Agent is aware that, on average, at least ten steps are needed to fully understand the requested task. No further clarifying questions to the prompt-provider are necessary if the provided description is detailed enough.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview of the system architecture. The Agent is composed of multiple modules that exchange messages. Some of them have access to APIs, the Internet, and Python interpreter.</p>
<p>The "Web searcher" component receives queries from the Planner, transforms them into appropriate web search queries, and executes them using the Google Search API. The first ten documents returned are filtered, excluding PDFs, and the resulting list of web pages is passed back to the Web searcher component. The component can then use the "BROWSE" action to extract text from the web pages and compile an answer for the Planner. For this task we can employ GPT-3.5, as it performs significantly faster than GPT-4 with no appreciable loss of quality. The "Docs searcher" component combs through the hardware documentation (e.g., robotic liquid handler, GC-MS, a cloud lab) by utilizing a query and documentation index to find the most relevant pages/sections. Then the best matching results are aggregated to provide a comprehensive and accurate final answer. This module places an emphasize on providing specific function parameter and syntactic information for the hardware API.</p>
<p>The "Code execution" component does not utilize any language models and simply executes the code in an isolated Docker container, protecting the end host machine from any unexpected actions by the Planner. All code outputs are passed back to the Planner, enabling it to fix its predictions in case of software errors. The same applies to the "Automation" component, which then executes the generated code on corresponding hardware or just provides the synthetic procedure for manual experimentation.</p>
<h1>Web search enables Agent's synthesis planning capabilities.</h1>
<p>To demonstrate the system's functionality, we use the synthesis of ibuprofen as an example (Figure 2A). The input prompt is straightforward: "Synthesize ibuprofen." The model then searches the internet for information on ibuprofen synthesis, locating the necessary details on a particular website. The model correctly identifies the first step of the synthesis, which is the Friedel-Crafts reaction between isobutylbenzene and acetic anhydride catalyzed by aluminum chloride (see Appendix A). The planning phase for the first step concludes once the model requests documentation for the Friedel-Crafts synthesis procedure.</p>
<p>Two other examples of the system's capabilities include the synthesis of common aspirin (Figure 2B and Appendix B), which the model searches and designs effectively, and aspartame synthesis (Figure 2D and Appendix C), which, although missing the methyl group in the product, can be corrected once the model receives a suitable synthetic example for execution in the cloud lab. Furthermore, when asked to study a Suzuki reaction, the model accurately identifies the substrates and the product (Figure 2C and Appendix D). The high-temperature parameter for text generation results in volatility when suggesting specific catalysts or bases.</p>
<p>Connecting the model to a chemical reaction database such as Reaxys ${ }^{14}$ or SciFinder ${ }^{15}$ via API could significantly enhance the system's performance. Alternatively, analyzing the system's previous statements is another approach to improving its accuracy.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Agent's capabilities in the synthesis planning task. A. Ibuprofen synthesis. B. Aspirin synthesis. C. Suzuki reaction mechanism study, where the Agent had to choose how to study the mechanism. D. Aspartame synthesis.</p>
<h1>Vector search can be employed for retrieval of dense hardware API documentation.</h1>
<p>To integrate an intelligent Agent capable of sophisticated reasoning with contemporary software, it is crucial to provide a clear and concise presentation of relevant technical documentation. Modern software is characterized by its complexity and the intricate interplay between various components. Consequently, comprehensive software documentation is indispensable for programmers to comprehend these interactions and utilize them effectively to accomplish their goals. Nonetheless, traditional software documentation frequently employs highly technical language, which can be challenging for non-experts to grasp. This creates a barrier to entry for many potential users of the software, limiting its reach and effectiveness.</p>
<p>Large language models have the potential to overcome this barrier by generating natural language descriptions of software documentation that are more accessible to nonexperts. These models are trained on a vast corpus of text from a variety of sources, which includes extensive information related to Application Programming Interfaces (APIs). One such API is the Opentrons Python API. ${ }^{16}$ However, the GPT-4's training data contains information up until September 2021. Thus, there is potential for enhancing the Agent's accuracy in using the API. To this end, we devised an approach to provide the Agent with requisite documentation for a given task, summarized in Figure 3A.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Overview of documentation search. A. Prompt-to-(improved OT-2 Python API)-code via ada embedding and distance-based vector search. B. Prompt-to-function recommendation in Emerald Cloud Lab symbolic lab language via supplementation of documentation guide.</p>
<p>For all 14 sections of the OT-2 API documentation we have generated OpenAI's ada embeddings to cross reference and compute similarity with respect to a query. The agent is instructed to inquire about proper use of the API when needed via the Documentation action. An ada embedding for the subsequent query is generated and documentation sections are selected via a distance-based vector search. The number of sections provided is dictated by the number of GPT-4 tokens present in the raw text for a given section. The maximum number of tokens is set to 7800 , such that the relevant documents can be provided in one step. This approach proved critical for providing the agent with information about the heater-shaker hardware module necessary for performing chemical reactions (see "The Agent is capable of designing and performing chemical experiments." section).</p>
<p>A greater challenge emerges when applying this approach to a more diverse robotic platform, such as the Emerald Cloud Lab (ECL). Nonetheless, we can explore the effectiveness of providing information about the Cloud Lab's Symbolic Lab Language (SLL), which is currently unknown to the GPT-4 model. For this we provide the Agent with a documentation guide from ECL pertaining to all available functions for running experiments. ${ }^{17}$ Figure 3B summarizes three examples of the User providing a simple query and the Agent directing the User to relevant ECL functions. More examples are shared in the Appendix G. In all cases, the Agent correctly identifies functions for accomplishing the given task. After selection of appropriate functions, the raw plain text documentation is passed through a separate GPT-4 model to perform code syntax retention and summarization. Specifically, this model efficiently retains information regarding the various options, instruments, and parameters for a given function. Once the entire documentation has been ingested, the model is prompted to produce a code block utilizing the given function to be passed back to the Planner. This serves as a basis for the model to utilize this function with specific options, instruments, and parameters as they are gathered by the Web searcher.</p>
<h1>Mastering automation: multi-instrument systems controlled by natural language.</h1>
<p>Access to documentation enables us to provide sufficient information for the Agent to conduct experiments in the physical world. To initiate the investigation, we chose an open-source liquid handler with a well-documented Python API. The "Getting Started" page from its documentation was supplied to the Planner in the system prompt. Other pages were vectorized using the approach described in the "Providing hardware API Documentation" section. In this section, we did not grant access to the internet (Figure 4A).</p>
<p>A.
<img alt="img-3.jpeg" src="img-3.jpeg" />
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>UV-Vis data from plate 1 is ready. You can access it using PYTHON, it will be in the file named
'plate1.npy'. You will have NumPy installed. The file contains NumPy array of shape $(9,12,41)$. There are 41 wavelengths from 350 to 750 with 10 nm step. Think about how would different colors absorb first.</p>
<p>Automated answer</p>
<ul>
<li>guiding prompt
[530, 430, 630]
Automated answer
<img alt="img-5.jpeg" src="img-5.jpeg" /></li>
</ul>
<p>Figure 4. Robotic liquid handler control capabilities and integration with analytical tools. A. Overview of the Agent's configuration. B-E. Drawing geometrical figures. F. The Agent solves a color identification problem using UV-Vis data.</p>
<p>We began with simple experiments on operating the robot, which simultaneously required the ability to consider a set of samples as a whole (in our case, an entire microplate). Straightforward prompts in natural language, such as "Color every other line with one color of your choice," resulted in mostly accurate protocols. When executed by the robot, these protocols closely resembled the requested prompt (Figure 4B-E).</p>
<p>The Agent's first action was to prepare small samples of the original solutions (Figure 4F). It then requested UV-Vis measurements to be performed (Appendix H). Once completed, the Agent was provided with a file name containing a NumPy array with spectra for each well of the microplate. The Agent subsequently wrote Python code to identify the wavelengths with maximum absorbance, using this data to correctly solve the problem.</p>
<h1>Bringing it all together: the Agent's integrated chemical experiment design and execution capabilities.</h1>
<p>Previous experiments could be affected by the knowledge of the Agent's modules from the pretraining step. We wanted to evaluate the Agent's ability to plan an experiment by combining data from the internet, performing the necessary calculations, and ultimately writing the code for the liquid handler. To increase complexity, we asked the Agent to use the heater-shaker module released after the GPT-4 training data collection cutoff. These requirements were incorporated into the Agent's configuration (Figure 5A).</p>
<p>The problem was designed as follows: the Agent is provided with a liquid handler equipped with two microplates. One (Source Plate) contains stock solutions of multiple reagents, including phenyl acetylene and phenylboronic acid, multiple aryl halide coupling partners, two catalysts, two bases, and the solvent to dissolve the sample (Figure 5B). The target plate is installed on the heater-shaker module (Figure 5C). The Agent's goal is to design a protocol to perform Suzuki and Sonogashira reactions.</p>
<p>The Agent begins by searching the internet for information on the requested reactions, their stoichiometry, and conditions (Figure 5D). It selects the correct coupling partners for the corresponding reactions. Among all aryl halides, it selected bromobenzene for Suzuki reaction and iodobenzene for Sonogashira reaction. This behavior changes from each run, as it also selects p-nitroiodobenzene due to its high reactivity in oxidative addition reactions, or bromobenzene because it is reactive but less toxic than aryl iodides. This highlights a potential future use case for the model - performing experiments multiple times to analyze the model's reasoning and construct a bigger picture. The model selected a $\mathrm{Pd} / \mathrm{NHC}$ catalyst as a more efficient, modern approach for cross-coupling reactions, and triethylamine was chosen as the base.</p>
<p>The Agent then calculates the required volumes of all reactants and writes the protocol. However, it used an incorrect heater-shaker module name. Upon noticing the mistake, the model consulted the documentation. This information was then used to modify the protocol, which successfully ran (Figure 5E). Subsequent GC-MS analysis of the reaction mixtures revealed the formation of the target products for both reactions (Appendix I).</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 5. Cross-coupling Suzuki and Sonogashira reaction experiments designed and performed by the Agent. A. Overview of the Agent's configuration. B. Available compounds. C. Liquid handler setup. D. Solving the synthesis problem. E. Generated protocol.</p>
<h1>The Agent has high reasoning capabilities.</h1>
<p>The system demonstrates remarkably high reasoning capabilities, enabling it to request necessary information, solve complex problems, and generate high-quality code for experimental design. OpenAI has shown that GPT-4 could rely on some of those capabilities to take actions in the physical world during their initial testing performed by the Alignment Research Center. ${ }^{10}$</p>
<p>The most remarkable reasoning capability exhibited by the Agent was its ability to correct its own code based on the automatically generated outputs. Besides already mentioned examples, during calculations for a mechanistic study of the Suzuki reaction, the model requested the system to execute code that imported the SymPy package (refer to Appendix D), which was not installed. After receiving the corresponding traceback, the Agent revised the code using basic Python. However, this modified code still proved unhelpful, as it did not return any output (the model had assumed that the interpreter was being used in interactive mode). Acknowledging this, the Agent made a final adjustment by incorporating a print() statement.</p>
<h2>The Agent shows interesting approach to key scientific problems.</h2>
<p>Furthermore, we aimed to assess the system's performance when presented with an exceptionally challenging problem. We began by asking the model to develop a novel anticancer drug (refer to Appendix E). The model approached the analysis in a logical and methodical manner: it first inquired about current trends in anticancer drug discovery, subsequently selected a target, and requested a scaffold targeting these compounds. Once the compound was identified, the model attempted its synthesis (this step was not performed experimentally). Another example involved studying the mechanism of a Suzuki reaction (see Appendix D). In this case, the model sought information on the reaction's mechanism and, upon obtaining the individual steps, searched for an example of such a reaction. To execute the reaction, the model calculated the required amounts of reagents and requested the documentation page related to the corresponding synthesis.</p>
<h2>Safety implications of the developed approach.</h2>
<p>There is growing concern regarding the potential misuse of molecular machine learning models for harmful purposes. Specifically, the dual-use application of models for predicting cytotoxicity ${ }^{18}$ to create new poisons or employing AlphaFold2 to develop novel bioweapons has raised alarm. Central to these concerns are the possible misuse of large language models and automated experimentation for dual-use purposes or otherwise. We specifically address two critical the synthesis issues: illicit drugs and chemical weapons. To evaluate these risks, we designed a test set comprising compounds from the DEA's Schedule I and II substances and a list of known chemical weapon agents. We submitted these compounds to the Agent using their common names, IUPAC names, CAS numbers, and SMILES strings to determine if the Agent would carry out extensive analysis and planning (Figure 6).</p>
<p>Note: None of these examples were performed experimentally. Under no circumstances should any individual or organization attempt to recreate, synthesize, or otherwise produce the substances or compounds discussed in this section.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 6. Results of the preliminary dual-use safety study. A. Controlled substances that the Agent agreed to synthesize. B. Controlled substances that Agent refused to synthesize these examples after a web search. C. Controlled substances that Agent refused to synthesize from the initial prompt.</p>
<p>The run logs can be found in Appendix F. Out of 11 different prompts (Figure 6), four (36\%) provided a synthesis solution and attempted to consult documentation to execute the procedure. This figure is alarming on its own, but an even greater concern is the way in which the Agent declines to synthesize certain threats. Out of the seven refused chemicals, five were rejected after the Agent utilized search functions to gather more information about the substance. For instance, when asked about synthesizing codeine, the Agent becomes alarmed upon learning the connection between codeine and morphine, only then concluding that the synthesis cannot be conducted due to the requirement of a controlled substance. However, this search function can be easily manipulated by altering the terminology, such as replacing all mentions of morphine with "Compound A" and codeine with "Compound B". Alternatively, when requesting a synthesis procedure that must be performed in a DEA-licensed facility, bad actors can mislead the Agent by falsely claiming their facility is licensed, prompting the Agent to devise a synthesis solution.</p>
<p>In the remaining two instances, the Agent recognized the common names "heroin" and "mustard gas" as threats and prevented further information gathering. While these results are promising, it is crucial to recognize that the system's capacity to detect misuse primarily applies to known compounds. For unknown compounds, the model is less likely to identify potential misuse, particularly for complex protein toxins where minor sequence changes might allow them to maintain the same properties but become unrecognizable to the model.</p>
<p>It is important to note that while the potential for dual use of Intelligent Agent capable of running scientific experiments is real, fully monitored cloud labs remain a safer choice than simply remote-connected machines. Screening, monitoring, and control safety systems such as the ones implemented by major cloud lab companies offer an additional layer of protection from potential misuses or bad actors.</p>
<h1>Conclusions</h1>
<p>In this paper, we presented an Intelligent Agent system capable of autonomously designing, planning, and executing complex scientific experiments. Our system demonstrates exceptional reasoning and experimental design capabilities, effectively addressing complex problems and generating high-quality code.</p>
<p>However, the development of new machine learning systems and automated methods for conducting scientific experiments raises substantial concerns about the safety and potential dual use consequences, particularly in relation to the proliferation of illicit activities and security threats. By ensuring the ethical and responsible use of these powerful tools, we can continue to explore the vast potential of large language models in advancing scientific research while mitigating the risks associated with their misuse.</p>
<h1>Limitations, Safety Recommendations, and a Call to Action</h1>
<p>We strongly believe that guardrails must be put in place to prevent this type of potential dual-use of large language models. We call for the AI community to engage in prioritizing safety of these powerful models. We call upon OpenAI, Microsoft, Google, Meta, Deepmind, Anthropic, and all the other major players to push the strongest possible efforts on safety of their LLMs. We call upon the physical sciences community to be engaged with the players involved in developing LLMs to assist them in developing those guardrails.</p>
<p>There are several limitations and safety concerns associated with the proposed machine learning system. These concerns warrant the implementation of safety guardrails to ensure responsible and secure usage of the system. At the very least, we argue that the community (both Al and physical sciences) should engage in the following recommendations:</p>
<ol>
<li>Human intervention: While the system demonstrates high reasoning capabilities, there might be instances where human intervention is necessary to ensure the safety and reliability of the generated experiments. We recommend incorporating a human-in-the-loop component for the review and approval of potentially sensitive experiments, especially those involving potentially harmful substances or methodologies. We believe that specialists should oversee and deliberate about the Agent's actions in the physical world.</li>
<li>Novel compound recognition: The current system can detect and prevent the synthesis of known harmful compounds. However, it is less efficient at identifying novel compounds with potentially harmful properties. This could be circumvented by implementing machine learning model to identify potentially harmful structures before passing them into the model.</li>
<li>Data quality and reliability: The system relies on the quality of the data it gathers from the internet and operational documentation. To maintain the reliability of the system, we recommend the continuous curation and update of the data sources, ensuring that the most up-to-date and accurate information is being used to inform the system's decision-making process.</li>
<li>System security: The integration of multiple components, including large language models and automated experimentation, poses security risks. We recommend implementing robust security measures, such as encryption and access control, to protect the system from unauthorized access, tampering, or misuse.</li>
</ol>
<h2>Broader Impacts</h2>
<p>The proposed machine learning system has numerous potential broader impacts on science, technology, and society:</p>
<ol>
<li>Acceleration of scientific research: By automating the design, planning, and execution of experiments, the system can significantly accelerate scientific research</li>
</ol>
<p>across various fields. Researchers can focus on interpreting results, refining hypotheses, and making discoveries, while the system handles the experimental process.
2. Democratization of scientific resources: The system can potentially make scientific experimentation more accessible to researchers with limited resources or expertise. It may enable smaller research groups or individuals to conduct complex experiments with the support of large language models and cloud labs, promoting a more inclusive scientific community.
3. Interdisciplinary collaboration: The system's versatility across domains, including natural language, biology, chemistry, and computer programming, can foster interdisciplinary collaboration. Researchers from different fields can leverage the system's capabilities to address complex problems that require a diverse set of skills and knowledge.
4. Education and training: The system can serve as a valuable educational tool for students and researchers to learn about experimental design, methodology, and analysis. It can help develop critical thinking and problem-solving skills, as well as encourage a deeper understanding of scientific principles.
5. Economic impact: By automating and streamlining the experimental process, the system can potentially reduce the costs associated with research and development. This can lead to increased investment in research and innovation, ultimately driving economic growth and competitiveness.</p>
<p>However, the potential broader impacts also include challenges and risks that must be addressed. Ensuring responsible and ethical use of the system, implementing robust security measures, and continuously updating data sources are essential steps to mitigate potential negative consequences, such as the proliferation of harmful substances or the misuse of powerful machine learning tools for nefarious purposes. By addressing these concerns, we can unlock the full potential of the proposed system and drive positive change across scientific research and society at large.</p>
<h1>Acknowledgments</h1>
<p>We are thankful to all Gomes group members for their support, in particular to Letícia Madureira for assistance with the Figures in this manuscript. We would like to thank the following CMU Chemistry groups for their assistance with providing some of chemicals needed for the Agent's experiments: Sydlik, Garcia Borsch, Matyjaszewski, and Ly. We have special thanks for the Noonan group (Prof. Kevin Noonan and Dhruv Sharma) for providing access to chemicals and GC-MS analytics. We would like to thank the team at Emerald Cloud Labs (with special attention to Ben Kline, Ben Smith, and Brian Frezza) for assisting us with parsing their documentation.
G.G. is grateful to the CMU Cloud Lab Initiative led by the Mellon College of Science for its vision of the future of sciences.</p>
<h1>Funding</h1>
<p>G.G. thanks Carnegie Mellon University, the Mellon College of Sciences and its department of chemistry, the College of Engineering and its department of chemical engineering for the startup support.</p>
<h2>Data availability</h2>
<p>Examples of the experiments discussed in the text are provided in the Appendices. Data (including documentation search and cloud lab execution), code, and prompts will be released in the later versions of this work due to safety concerns.</p>
<h2>Competing interests</h2>
<p>The authors have no competing interests to disclose at this moment.</p>
<h2>Author contributions</h2>
<p>D.A.B. designed the computational pipeline and developed "Planner", "Web searcher", and "Code execution" module. R.M. assisted in designing the computational pipeline and developed the "Docs searcher" module. D.A.B. assisted and oversaw the Agent's scientific experiments. D.A.B. and R.M. designed and performed the initial computational safety studies. G.G. designed the concept, performed preliminary studies, and supervised the project. D.A.B, R.M. and G.G. wrote this manuscript.</p>
<h2>Technology use disclosure</h2>
<p>The writing of this manuscript was assisted by ChatGPT (specifically, GPT-4). The authors have read, corrected, and verified all information presented in this work.</p>
<h1>References and Notes</h1>
<ol>
<li>Brown, T. et al. Language Models are Few-Shot Learners. in Advances in Neural Information Processing Systems (eds. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F. \&amp; Lin, H.) vol. 33 1877-1901 (Curran Associates, Inc., 2020).</li>
<li>Thoppilan, R. et al. LaMDA: Language Models for Dialog Applications. (2022).</li>
<li>Touvron, H. et al. LLaMA: Open and Efficient Foundation Language Models. (2023).</li>
<li>Hoffmann, J. et al. Training Compute-Optimal Large Language Models. (2022).</li>
<li>Chowdhery, A. et al. PaLM: Scaling Language Modeling with Pathways. (2022).</li>
<li>Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science (1979) 379, 1123-1130 (2023).</li>
<li>Luo, R. et al. BioGPT: generative pre-trained transformer for biomedical text generation and mining. Brief Bioinform 23, (2022).</li>
<li>Irwin, R., Dimitriadis, S., He, J. \&amp; Bjerrum, E. J. Chemformer: a pre-trained transformer for computational chemistry. Mach Learn Sci Technol 3, 015022 (2022).</li>
<li>Kim, H., Na, J. \&amp; Lee, W. B. Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention. J Chem Inf Model 61, 5804-5814 (2021).</li>
<li>OpenAI. GPT-4 Technical Report. (2023).</li>
<li>Ziegler, D. M. et al. Fine-Tuning Language Models from Human Preferences. (2019).</li>
<li>Ouyang, L. et al. Training language models to follow instructions with human feedback. (2022).</li>
<li>Bran, A. M., Cox, S., White, A. D. \&amp; Schwaller, P. ChemCrow: Augmenting largelanguage models with chemistry tools. (2023).</li>
<li>Reaxys. https://www.reaxys.com.</li>
<li>SciFinder. https://scifinder.cas.org.</li>
<li>Opentrons. Opentrons Python Protocol API. https://docs.opentrons.com/v2/.</li>
<li>
<p>Emerald Cloud Lab. Running Experiments. https://www.emeraldcloudlab.com/guides/runningexperiments.</p>
</li>
<li>
<p>Urbina, F., Lentzos, F., Invernizzi, C. \&amp; Ekins, S. Dual use of artificial-intelligencepowered drug discovery. Nat Mach Intell 4, 189-191 (2022).</p>
</li>
</ol>
<h1>Appendices</h1>
<h2>Glossary for Appendices</h2>
<h2>This box demarks notes from the authors.</h2>
<p>The following description follows OpenAI's notation for chat completion mode:</p>
<ul>
<li>Assistant: Output of the model (in most cases it is the "Planner" module).</li>
<li>User: Outputs of other modules provided to the main module (for example, search results or Python code output). Real user (in these experiments, a human scientist) inputs are marked as "Prompt:".</li>
</ul>
<h2>Appendix A: Synthesis of ibuprofen</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Assistant</th>
<th style="text-align: center;">First, let's look for a synthesis route for ibuprofen. <br> GOOGLE ibuprofen synthesis route</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">Ibuprofen is synthesized through a process called Friedel-Crafts acylation. The starting materials for the synthesis are isobutylbenzene and acetic anhydride. This process was discovered by Stewart Adams and John Nicholson in 1961 while they were working at Boots UK Limited. The original ibuprofen synthesis by the Boots Group serves as a basis for the manufacturing process today.</td>
</tr>
<tr>
<td style="text-align: center;">Assistant</td>
<td style="text-align: center;">Great, now that we have the synthesis route through Friedel-Crafts acylation using isobutylbenzene and acetic anhydride, let's find the stoichiometry and the experimental conditions for this reaction. <br> GOOGLE Friedel-Crafts acylation isobutylbenzene and acetic anhydride conditions and stoichiometry</td>
</tr>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">In the Friedel-Crafts acylation of isobutylbenzene and acetic anhydride, the reaction conditions involve reacting isobutylbenzene (1) with acetic anhydride under the influence of a Lewis acid, such as aluminum chloride (AlCl3) as a catalyst. This reaction forms p-isobutylacetophenone (2) through electrophilic aromatic substitution. <br> The stoichiometry of the reaction would involve one equivalent of isobutylbenzene reacting with one equivalent of acetic anhydride in the presence of the AlCl3 catalyst. However, the actual yield and purity may be influenced by factors such as the concentration of unreacted starting materials or reaction time. Consider extending the reaction time or adding more catalyst to enhance the conversion and yield of the desired product.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Assistant</th>
<th style="text-align: center;">Now that we have information about the reactants and stoichiometry, let's calculate how much of each reactant we need to synthesize around 100 mg of ibuprofen. First, we need the molecular weights of isobutylbenzene, acetic anhydride, and ibuprofen. Then, we'll calculate the amount of isobutylbenzene and acetic anhydride needed based on the reaction stoichiometry. <br> PYTHON <br> isobutylbenzene_mw $=134.22 # \mathrm{~g} / \mathrm{mol}$ <br> acetic_anhydride_mw $=102.09 # \mathrm{~g} / \mathrm{mol}$ <br> ibuprofen_mw $=206.28 # \mathrm{~g} / \mathrm{mol}$ <br> desired_mass $=0.1 #$ in $\mathrm{g}(100 \mathrm{mg})$ <br> isobutylbenzene_moles = desired_mass / ibuprofen_mw <br> acetic_anhydride_moles = isobutylbenzene_moles <br> mass_isobutylbenzene = isobutylbenzene_moles * <br> isobutylbenzene_mw <br> mass_acetic_anhydride = acetic_anhydride_moles * <br> acetic_anhydride_mw <br> print(mass_isobutylbenzene, mass_acetic_anhydride)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">0.06506689936009308 0.049490983129726586</td>
</tr>
<tr>
<td style="text-align: center;">Assistant</td>
<td style="text-align: center;">Based on our calculations, we need 0.0651 g of isobutylbenzene and 0.0495 g of acetic anhydride to synthesize about 100 mg of ibuprofen. <br> Lastly, let's check the cloud lab documentation for the experimental procedure that involves the Friedel-Crafts acylation reaction with isobutylbenzene and acetic anhydride, and the use of aluminum chloride as a catalyst. <br> DOCUMENTATION Friedel-Crafts acylation experimental procedure with aluminum chloride catalyst</td>
</tr>
</tbody>
</table>
<h1>Second Google search expanded</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">User</th>
<th style="text-align: left;">Friedel-Crafts acylation isobutylbenzene and acetic anhydride conditions <br> and stoichiometry</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Assistant</td>
<td style="text-align: left;">GOOGLE Friedel-Crafts acylation isobutylbenzene and acetic anhydride <br> conditions and stoichiometry</td>
</tr>
<tr>
<td style="text-align: left;">User</td>
<td style="text-align: left;">Google search results: <br> Ibuprofen Synthesis - Writing Anthology <br> https://central.edu/writing-anthology/2019/04/11/ibuprofen-synthesis/ <br> Apr 11, 2019 ... Initially, isobutylbenzene (1) and acetic anhydride, were reacted under Friedel- <br> Crafts acylation conditions to create p-isobutylacetophenone ...</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Synthesis of ibuprofen from benzene - The Science Snail <br> https://www.sciencesnail.com/science/synthesis-of-ibuprofen-from-benzene</td>
</tr>
</tbody>
</table>
<p>Oct 11, 2018 ... Friedel-Crafts acylation mechanism isobutylbenzene ... an aldoxime (7) which is then dehydrated to the nitrile (8) using acetic anhydride.
(PDF) Ibuprofen Continuous Manufacturing - Process Modeling and ...
https://www.researchgate.net/publication/359864378_Ibuprofen_Continuous_Manufacturing_ Process_Modeling_and_Techno-Economic_Assessment_TEA_using_SuperPro_Designer Apr 11, 2022 ... The synthesis process involves Friedel-Crafts acylation, ... The raw materials acetic anhydride and isobutyl benzene (IBB) and the recycled ...</p>
<p>Regioselective Friedel-Crafts Acylation Reaction Using Single ...
https://pubs.acs.org/doi/10.1021/acsomega.2c03555
Aug 31, 2022 ... Again, the Friedel-Crafts (FC) acylation of aromatic substrates with ... (50) and substituted anhydrides (51) in stoichiometric amounts and ...</p>
<p>Flow Chemistry: Recent Developments in the Synthesis of ...
https://pubs.acs.org/doi/10.1021/acs.oprd.5b00325
Nov 26, 2015 ... The continuous flow Friedel-Crafts acylation of isobutylbenzene 68 with propionyl chloride 69 was performed with AlCl3 as a Lewis acid using ...</p>
<p>Strongly Lewis Acidic Metal-Organic Frameworks for Continuous ...
https://pubs.acs.org/doi/10.1021/jacs.9b07891
Sep 4, 2019 ... (58) Conventional Lewis acids for Friedel-Crafts acylation include ... Reaction conditions: arene ( 1 equiv, 1.0 mmol ), acetic anhydride ( 1.0 ...</p>
<p>A comprehensive review of flow chemistry techniques tailored to the ...
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8182698/
Optimisation of the reaction conditions in flow were carried out on ... is the acetylation of geraniol (343) through acetic anhydride developed by Adarme ...</p>
<p>Continuous Flow Organic Chemistry: Successes and Pitfalls at the ...
https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ejoc. 201800149
Mar 13, 2018 ... The reactor effluent was next mixed with acetic anhydride an N ... The first step included a Friedel-Crafts acylation of isobutyl benzene ...</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Assistant</th>
<th style="text-align: center;">BROWSE https://central.edu/writing-anthology/2019/04/11/ibuprofensynthesis/</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">We removed the text from this website from the paper. You can access it by using URL above.</td>
</tr>
<tr>
<td style="text-align: center;">Assistant</td>
<td style="text-align: center;">OUTPUT In the Friedel-Crafts acylation of isobutylbenzene and acetic anhydride, the reaction conditions involve reacting isobutylbenzene (1) with acetic anhydride under the influence of a Lewis acid, such as aluminum chloride (AlCl3) as a catalyst. This reaction forms pisobutylacetophenone (2) through electrophilic aromatic substitution. <br> The stoichiometry of the reaction would involve one equivalent of isobutylbenzene reacting with one equivalent of acetic anhydride in the presence of the AlCl3 catalyst. However, the actual yield and purity may be influenced by factors such as the concentration of unreacted starting materials or reaction time. Consider extending the reaction time or adding more catalyst to enhance the conversion and yield of the desired product.</td>
</tr>
</tbody>
</table>
<h1>Appendix B: Synthesis of aspirin</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Assistant</th>
<th style="text-align: center;">I will first look up the synthesis reaction for aspirin to understand the reagents and procedure involved. <br> GOOGLE aspirin synthesis reaction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">User</td>
<td style="text-align: center;">Aspirin, also known as acetylsalicylic acid, is synthesized through a chemical reaction between salicylic acid and acetic anhydride. Sulfuric acid is used as a catalyst to speed up the reaction. The overall reaction can be described as an esterification reaction, in which a carboxylic acid reacts with an acid anhydride to form an ester. The balanced chemical equation for the reaction is: <br> C7H6O3 (salicylic acid) + C4H6O3 (acetic anhydride) -&gt; C9H8O4 (aspirin) + C2H4O2 (acetic acid) <br> During this reaction, the aspirin product will precipitate when water is added as it has low solubility in water, while acetic anhydride and acetic acid will dissolve in water. Vacuum filtration can then be used to separate the crystalline aspirin from the reaction mixture, excluding any unreacted salicylic acid.</td>
</tr>
<tr>
<td style="text-align: center;">Assistant</td>
<td style="text-align: center;">Since we want to synthesize around 100 mg of aspirin, I will calculate the required quantities of salicylic acid and acetic anhydride using the balanced chemical equation and their molar masses. I will assume a $100 \%$ yield for simplicity. <br> PYTHON <br> aspirin_molar_mass $=180.16 # \mathrm{~g} / \mathrm{mol}$ <br> salicylic_acid_molar_mass $=138.12 # \mathrm{~g} / \mathrm{mol}$ <br> acetic_anhydride_molar_mass $=102.09 # \mathrm{~g} / \mathrm{mol}$ <br> target_aspirin_mass $=100 # \mathrm{mg}$ <br> target_aspirin_mol = target_aspirin_mass / <br> (aspirin_molar_mass * 1e3) <br> required_salicylic_acid_mol = target_aspirin_mol <br> required_acetic_anhydride_mol = target_aspirin_mol <br> required_salicylic_acid_mass = required_salicylic_acid_mol <br> * salicylic_acid_molar_mass * 1e3 <br> required_acetic_anhydride_mass = <br> required_acetic_anhydride_mol * acetic_anhydride_molar_mass <br> * 1e3 <br> print(required_salicylic_acid_mass, <br> required_acetic_anhydride_mass)</td>
</tr>
</tbody>
</table>            </div>
        </div>

    </div>
</body>
</html>
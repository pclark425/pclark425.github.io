<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5619 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5619</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5619</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-264590253</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.18867v1.pdf" target="_blank">Prompt-Engineering and Transformer-based Question Generation and Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> Question generation has numerous applications in the educational context. Question generation can prove helpful for students when reviewing content and testing themselves. Furthermore, a question generation model can aid teachers by lessening the burden of creating assessments and other practice material. This paper aims to find the best method to generate questions from textual data through a transformer model and prompt engineering. In this research, we finetuned a pretrained distilBERT model on the SQuAD question answering dataset to generate questions. In addition to training a transformer model, prompt engineering was applied to generate questions effectively using the LLaMA model. The generated questions were compared against the baseline questions in the SQuAD dataset to evaluate the effectiveness of four different prompts. All four prompts demonstrated over 60% similarity on average. Of the prompt-generated questions, 30% achieved a high similarity score greater than 70%.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5619.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5619.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta LLaMA prompt-engineering variations (Prompts A–D)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of four prompt-wording formats applied to Meta's LLaMA to generate questions from SQuAD contexts; outputs were compared to SQuAD baseline questions using spaCy embedding similarity to quantify how prompt presentation affects LLM output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question generation (from SQuAD v1.1)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate questions from textual contexts (SQuAD v1.1) and compare generated questions to baseline SQuAD questions using semantic similarity (spaCy vector similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Four explicit prompt-wording formats given to LLaMA, temperature set to 0.5, each prompt generated 5 questions per context. Prompts used: Prompt A: "Generate 5 questions from the text;" Prompt B: "Generate 5 complex questions from the text." Prompt C: "Generate 5 questions from the text; make sure the questions can be answered." Prompt D: "Generate 5 questions from the text; answer the question in the text; if the question is answered in the context, output 5 questions." 50 contexts were randomly sampled from SQuAD; for each generated question the maximum spaCy similarity to baseline SQuAD questions was recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct comparison across the four prompt variants (A vs B vs C vs D). All other generation settings (temperature=0.5, 5 outputs per context, same contexts) held constant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Similarity metrics and match counts reported: median similarity for Prompt D = 0.6444; average similarities: Prompt A = 0.6387, Prompt C = 0.6321, Prompt B = 0.6227. Using a threshold of similarity >= 0.7 as a 'match' to baseline, match counts (out of 250 generated questions per prompt) were: Prompt D = 81 matches, Prompt A = 79, Prompt C = 76, Prompt B = 64. Prompt D had at least one perfect-match outlier (similarity = 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Prompt D performed best by median similarity (0.6444) and highest match count (81). Prompt B (wording 'complex') performed worst by average similarity (0.6227) and lowest matches (64). Prompts A and D performed similarly (79 vs 81 matches).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Absolute differences observed: average similarity difference between best and worst prompts ≈ 0.0217 (Prompt D median 0.6444 vs Prompt B avg 0.6227). Match-count difference: Prompt D vs Prompt B = +17 matches out of 250 (6.8 percentage points absolute).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Prompt wording affected performance: Prompt D (more constrained / answer-checking wording) improved alignment with SQuAD baseline; Prompt B ('complex') reduced alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The authors hypothesize that adding the word 'complex' (Prompt B) caused LLaMA to produce compound or unanswerable questions requiring more reasoning, which lowered spaCy similarity to SQuAD's typically simple factual questions. They note that the similarity metric rewards simple factual formulations, so instructions that encourage complexity produced outputs less similar to baseline. Prompt D's instruction to ensure answers exist in the context (and the 'answer the question in the text' phrasing) likely constrained outputs toward SQuAD-style answerable factual questions, improving similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Although Prompt D performed best, Prompt A (the simplest prompt) performed nearly as well (79 vs 81 matches), indicating that adding complexity to prompts is not guaranteed to improve alignment and that simple prompts can be similarly effective under this evaluation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt-Engineering and Transformer-based Question Generation and Evaluation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5619.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5619.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DistilBERT finetune</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DistilBERT finetuned on reversed SQuAD for question generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained DistilBERT model was finetuned on a reversed SQuAD setup (input: context+answer, output: question) for question generation but produced poor results (F1 = 15.88) and often generated answer-like sentences rather than questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DistilBERT (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question generation (reversed SQuAD v1.1)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Finetune DistilBERT on SQuAD v1.1 transformed by swapping question and answer fields, so the model input is context+answer and target is question; evaluate generated outputs using the Trainer class F1 metric.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Training data format: SQuAD questions and answers swapped (reversed). Tokenization applied, long contexts split with doc_stride overlap to avoid splitting answers. Finetune hyperparameters: learning rate 0.00005, weight decay 0.01, training epochs = 3. The intention was to present the model with a generation task (produce questions) by changing the input-output format relative to standard QA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>F1 score (Trainer evaluation): 15.88. Authors report the model often produced sentences similar to answers rather than valid questions, so the low F1 reflects failure to produce question outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (format change did not produce the intended behavior; the model failed to generate questions effectively)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The authors attribute poor performance to the model's pretraining and prior familiarity with question-answering behavior: DistilBERT was pretrained on large corpora and has an inherent bias toward producing answers rather than generating questions; finetuning on the relatively small SQuAD dataset was insufficient to overwrite that bias. Thus simply reversing the input-output format did not successfully convert an answering model into a question generator.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>The paper reports that despite reversing the training format, the transformer produced answer-like text rather than questions, so the expected effect of format change (enabling question generation) did not occur; no successful alternative transformer-format combination was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt-Engineering and Transformer-based Question Generation and Evaluation', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Llama: Open and efficient foundation language models. <em>(Rating: 2)</em></li>
                <li>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. <em>(Rating: 2)</em></li>
                <li>Chatgpt prompt engineering for developers. <em>(Rating: 2)</em></li>
                <li>Machine comprehension by text-to-text neural question generation <em>(Rating: 2)</em></li>
                <li>Know what you don't know: Unanswerable questions for squad <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5619",
    "paper_id": "paper-264590253",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "LLaMA prompts",
            "name_full": "Meta LLaMA prompt-engineering variations (Prompts A–D)",
            "brief_description": "Evaluation of four prompt-wording formats applied to Meta's LLaMA to generate questions from SQuAD contexts; outputs were compared to SQuAD baseline questions using spaCy embedding similarity to quantify how prompt presentation affects LLM output quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA",
            "model_size": null,
            "task_name": "Question generation (from SQuAD v1.1)",
            "task_description": "Generate questions from textual contexts (SQuAD v1.1) and compare generated questions to baseline SQuAD questions using semantic similarity (spaCy vector similarity).",
            "problem_format": "Four explicit prompt-wording formats given to LLaMA, temperature set to 0.5, each prompt generated 5 questions per context. Prompts used: Prompt A: \"Generate 5 questions from the text;\" Prompt B: \"Generate 5 complex questions from the text.\" Prompt C: \"Generate 5 questions from the text; make sure the questions can be answered.\" Prompt D: \"Generate 5 questions from the text; answer the question in the text; if the question is answered in the context, output 5 questions.\" 50 contexts were randomly sampled from SQuAD; for each generated question the maximum spaCy similarity to baseline SQuAD questions was recorded.",
            "comparison_format": "Direct comparison across the four prompt variants (A vs B vs C vs D). All other generation settings (temperature=0.5, 5 outputs per context, same contexts) held constant.",
            "performance": "Similarity metrics and match counts reported: median similarity for Prompt D = 0.6444; average similarities: Prompt A = 0.6387, Prompt C = 0.6321, Prompt B = 0.6227. Using a threshold of similarity &gt;= 0.7 as a 'match' to baseline, match counts (out of 250 generated questions per prompt) were: Prompt D = 81 matches, Prompt A = 79, Prompt C = 76, Prompt B = 64. Prompt D had at least one perfect-match outlier (similarity = 1).",
            "performance_comparison": "Prompt D performed best by median similarity (0.6444) and highest match count (81). Prompt B (wording 'complex') performed worst by average similarity (0.6227) and lowest matches (64). Prompts A and D performed similarly (79 vs 81 matches).",
            "format_effect_size": "Absolute differences observed: average similarity difference between best and worst prompts ≈ 0.0217 (Prompt D median 0.6444 vs Prompt B avg 0.6227). Match-count difference: Prompt D vs Prompt B = +17 matches out of 250 (6.8 percentage points absolute).",
            "format_effect_direction": "Prompt wording affected performance: Prompt D (more constrained / answer-checking wording) improved alignment with SQuAD baseline; Prompt B ('complex') reduced alignment.",
            "explanation_or_hypothesis": "The authors hypothesize that adding the word 'complex' (Prompt B) caused LLaMA to produce compound or unanswerable questions requiring more reasoning, which lowered spaCy similarity to SQuAD's typically simple factual questions. They note that the similarity metric rewards simple factual formulations, so instructions that encourage complexity produced outputs less similar to baseline. Prompt D's instruction to ensure answers exist in the context (and the 'answer the question in the text' phrasing) likely constrained outputs toward SQuAD-style answerable factual questions, improving similarity.",
            "counterexample_or_null_result": "Although Prompt D performed best, Prompt A (the simplest prompt) performed nearly as well (79 vs 81 matches), indicating that adding complexity to prompts is not guaranteed to improve alignment and that simple prompts can be similarly effective under this evaluation metric.",
            "uuid": "e5619.0",
            "source_info": {
                "paper_title": "Prompt-Engineering and Transformer-based Question Generation and Evaluation",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DistilBERT finetune",
            "name_full": "DistilBERT finetuned on reversed SQuAD for question generation",
            "brief_description": "A pre-trained DistilBERT model was finetuned on a reversed SQuAD setup (input: context+answer, output: question) for question generation but produced poor results (F1 = 15.88) and often generated answer-like sentences rather than questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DistilBERT (finetuned)",
            "model_size": null,
            "task_name": "Question generation (reversed SQuAD v1.1)",
            "task_description": "Finetune DistilBERT on SQuAD v1.1 transformed by swapping question and answer fields, so the model input is context+answer and target is question; evaluate generated outputs using the Trainer class F1 metric.",
            "problem_format": "Training data format: SQuAD questions and answers swapped (reversed). Tokenization applied, long contexts split with doc_stride overlap to avoid splitting answers. Finetune hyperparameters: learning rate 0.00005, weight decay 0.01, training epochs = 3. The intention was to present the model with a generation task (produce questions) by changing the input-output format relative to standard QA.",
            "comparison_format": null,
            "performance": "F1 score (Trainer evaluation): 15.88. Authors report the model often produced sentences similar to answers rather than valid questions, so the low F1 reflects failure to produce question outputs.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced (format change did not produce the intended behavior; the model failed to generate questions effectively)",
            "explanation_or_hypothesis": "The authors attribute poor performance to the model's pretraining and prior familiarity with question-answering behavior: DistilBERT was pretrained on large corpora and has an inherent bias toward producing answers rather than generating questions; finetuning on the relatively small SQuAD dataset was insufficient to overwrite that bias. Thus simply reversing the input-output format did not successfully convert an answering model into a question generator.",
            "counterexample_or_null_result": "The paper reports that despite reversing the training format, the transformer produced answer-like text rather than questions, so the expected effect of format change (enabling question generation) did not occur; no successful alternative transformer-format combination was reported.",
            "uuid": "e5619.1",
            "source_info": {
                "paper_title": "Prompt-Engineering and Transformer-based Question Generation and Evaluation",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Llama: Open and efficient foundation language models.",
            "rating": 2,
            "sanitized_title": "llama_open_and_efficient_foundation_language_models"
        },
        {
            "paper_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.",
            "rating": 2,
            "sanitized_title": "distilbert_a_distilled_version_of_bert_smaller_faster_cheaper_and_lighter"
        },
        {
            "paper_title": "Chatgpt prompt engineering for developers.",
            "rating": 2,
            "sanitized_title": "chatgpt_prompt_engineering_for_developers"
        },
        {
            "paper_title": "Machine comprehension by text-to-text neural question generation",
            "rating": 2,
            "sanitized_title": "machine_comprehension_by_texttotext_neural_question_generation"
        },
        {
            "paper_title": "Know what you don't know: Unanswerable questions for squad",
            "rating": 2,
            "sanitized_title": "know_what_you_dont_know_unanswerable_questions_for_squad"
        }
    ],
    "cost": 0.00956725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Prompt-Engineering and Transformer-based Question Generation and Evaluation
October 31, 2023</p>
<p>Rubaba Amyeen 
Prompt-Engineering and Transformer-based Question Generation and Evaluation
October 31, 2023D6E0E9760C9AACAEF389A04F8F94EDCDarXiv:2310.18867v1[cs.CL]
Question generation has numerous applications in the educational context.Question generation can prove helpful for students when reviewing content and testing themselves.Furthermore, a question-generation model can aid teachers by lessening the burden of creating assessments and other practice material.This paper aims to find the best method to generate questions from textual data through a transformer model and prompt engineering.In this research, we finetuned a pre-trained distilBERT model on the SQuAD question-answering dataset to generate questions.In addition to training a transformer model, prompt engineering was applied to generate questions effectively using Meta's LLaMa model.The generated questions were compared against the baseline questions in the SQuAD dataset to evaluate the effectiveness of four different prompts.All four prompts demonstrated over 60% similarity on average.Of the prompt-generated questions, 30% achieved a high similarity score greater than 70%.</p>
<p>Introduction</p>
<p>Question generation is an essential NLP task.One of its primary applications is that it can be used as a learning tool.Active recall is the practice of retrieving information by answering questions.According to the Association for Psychological Science (APS), active recall is a high-utility learning technique since learners demonstrate higher academic performance after studying with active recall [DRM + 13].A model that can generate questions would help students use active recall to review material and lessen the time it takes to create questions.According to a study from CBE Life Science Education, students who used active learning strategies scored 5.5% and 10.2% higher than students who did not use active learning strategies on exams 1 and 2, respectively [WSRF21].Question generation systems can also motivate students to engage in educational activities because learners will know they will be tested on the material [Hei11].Online courses and materials need many questions, so this model would reduce question generation costs [PLCK19].Furthermore, it would allow students to test their knowledge on materials that do not include guiding questions.On the other hand, question generation is also valuable for teachers by allowing educators to spend their time elsewhere from formulating questions.Assessments need a constant flow of new questions as the value of questions decreases after multiple usages since test takers may share them.</p>
<p>Question generation is a generative task that involves supervised learning with language data.The task concerns taking the input data (natural language texts of the context and answer) and transforming it into the output data, which is the question.We tested two models: The transformer model distilBERT and Meta's LLaMA large language model (LLM) through prompt engineering.We evaluated the transformer model using the F1 score and the LLaMA using spaCy vector similarity.</p>
<p>Background</p>
<p>The task of question generation has rapidly developed over the past few years.Ali and Chali train their model using the TREC 2007 Question Answering Track.The dataset has a series of factoids, lists, and other questions under targets [ACH10].They simplified complex sentences to elementary structures with syntactic parsing.However, they were unable to address word disambiguation with semantic information.Yuan and Wang implemented a recurrent model to generate questions using the SQuAD dataset [YWG + 17].In their implementation, the encoder processes the answer and document while the decoder generates the question.They faced challenges with similar entities, related verbs being swapped, and their model needing common sense.Similarly, Duan and Tang used a convolution neural network (CNN) and recurrent neural network (RNN) [DTCZ17].Chen and Wu recently implemented a reinforcement learning-based Graph2Sequence model [CWZ19].</p>
<p>Most prior works often generate poor-quality questions.We propose using large language models (LLMs) with prompt engineering to combat these challenges.The prior papers have not addressed a prompt engineering-based approach for the question generation task.Furthermore, as LLMs have been trained on large amounts of data, they have common sense knowledge that prior models lack.</p>
<p>Dataset</p>
<p>In this paper, Stanford's SQuAD dataset was used to generate questions.[RZLL16], [RJL18].The transformer model was evaluated and trained on the SQuAD dataset.Since the SQuAD version 2 dataset has unanswerable questions, we used the SQuAD 1.1 dataset [G + 22].For the question generation task, unanswerable questions would not be useful as we wanted to generate questions that test reading comprehension.This dataset contains 100,000 questions from over 500 articles.While this dataset is commonly used for answering questions, we reversed it for question generation.The SQuAD dataset was split approximately 90/10 between the training set (87599 samples) and the validation set (10570 samples).For data preprocessing, the inputs were tokenized.Long contexts are split; however, in case the answer is where the context is split, the hyperparameter, doc strides, allows overlap between the two parts of the split context.We also used the SQuAD dataset for prompt engineering.We compared the SQuAD questions to the prompt-generated questions by LLaMA to evaluate the quality of the generated questions.</p>
<p>We visualized the SQuAD dataset with simple techniques, such as a histogram of the length of questions with outliers accounted for and the most frequent keywords with stop words removed.DistilBERT is a distilled version of BERT with 40% fewer hyperparameters than BERT-baseuncased [SDCW19].We finetuned the DistilBERT model on the SQuAD dataset for question generation.First, the SQuAD dataset was reversed, that is, we transformed the dataset into Pandas Dataframe and swapped the "question" and "answer" columns.</p>
<p>The parameters to finetune the pre-trained model included a learning rate of .00005,weight decay of 0.01, and training epochs set to 3. Figure 4 displays the procedure for the distilBERT question generation task.</p>
<p>Prompt Engineering</p>
<p>Prompt engineering is the practice of formulating specific prompts for LLMs to generate desired outputs.Andrew Ng and Isa Fulford outlined two main principles for prompt engineering: writing clear and specific instructions and allowing the model to think [NF].We developed four prompts for the question generation task using these guidelines for the Meta's LLaMA model [TLI + 23].The temperature, the prompts' variability, was set to 0.5 to ensure varied responses.Each of the prompts generated five questions.Table 1 shows the four prompts.</p>
<p>Prompt</p>
<p>Description Prompt-A "Generate 5 questions from the text;" Prompt-B "Generate 5 complex questions from the text."Prompt-C "Generate 5 questions from the text; make sure the questions can be answered."Prompt-D "Generate 5 questions from the text; answer the question in the text; if the question is answered in the context, output 5 questions."</p>
<p>Table 1: Written Prompts.</p>
<p>We randomly selected a sample of 50 contexts from the SQuAD dataset.We evaluated the four distinct prompts against the baseline questions under each context from SQuAD.For each prompt, 250 questions were generated.We used the spaCy metric that used word embeddings to measure the sentence similarities between the generated questions and baseline questions.We recorded the max similarity score for each generated question and the max similarity score for each prompt type under every context.Figure 5 shows the process of prompt engineering and how the prompts were evaluated.2 is a sample of the data collection for the context: "Beyoncé Giselle Knowles-Carter (born September 4, 1981) is an American singer, songwriter, record producer and actress.Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&amp;B girl-group Destinyś Child.Managed by her father, Mathew Knowles, the group became one of the worldś best-selling girl groups of all time.Their hiatus saw the release of Beyoncéś debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles 'Crazy in Love' and 'Baby Boy'."The generated questions from the four prompts are displayed.The "Prompt Max" is the max similarity score for all the generated questions under the specific prompt.Prompt B had the lowest number of matches.The word "complex" in Prompt B may have generated unanswerable questions from the context and required more critical thinking, so it performed poorly under the similarity metric.Lastly, Figure 8 is a distribution graph of the max scores of every prompt under each of the 50 contexts in total.Generally, the red (Prompt D) and blue (Prompt A) lines are more significant than the orange (Prompt B) and green (Prompt C) lines.Prompt B had a particularly low max similarity score for context 45 of 0.42 compared to Prompt A, C, and D, which had max similarity scores of 0.73, 0.70, and 0.77, respectively.This is most likely because Prompt B generated compound questions, where two questions were combined into one.For example, one of the questions was, "In what way can Nirvana be seen as a liberation from the cycle of suffering, and how is this achieved?"</p>
<p>Conclusion</p>
<p>In this paper, we finetuned a distilBERT model on the question generation task.This transformer model could not generate questions with an F1 score of 15.88.On the other hand, Meta's LLaMA model successfully generated questions using the prompts we created with the prompt engineering guidelines.As the transformer model was not able to generate questions and Meta's LLaMA model was, we decided to only evaluate the generated questions from LLaMA.We used a similarity metric to determine which of the four prompts we created was the most effective.We found that the most complex prompt (Prompt D) resulted in the most matches with the baseline questions from SQuAD at 81 matches.The most simple prompt (Prompt A) performed the second best with 79 matches.Prompt C had 76 matches.Lastly, Prompt B had a significantly lower number of matches at 64 matches.It should be noted that Prompt B was the same as Prompt A except for the added word "complex," suggesting that the similarity score metric rewards simple factual questions rather than compound questions.</p>
<p>We plan to extend our research to include other LLMs, such as the Dolly model, GPT-4, and PaLM 2. Furthermore, we plan to combine the question generation model with a classification model that classifies the contexts under specific educational subjects to make the questions generated more appropriate for each subject beyond reading comprehension.</p>
<p>Figure 1 :
1
Figure 1: Length of the Questions in SQuAD.</p>
<p>Figure 2 :
2
Figure 2: Most Frequent Words in SQuAD Questions.</p>
<p>Figure 3 :
3
Figure 3: DistillBERT model architecture.</p>
<p>Figure 4 :
4
Figure 4: Transformer question generation flow</p>
<p>Figure 5 :
5
Figure 5: Flow Diagram of Data Collection Process</p>
<p>Figure 6 :
6
Figure 6: Score distribution across diffent prompts</p>
<p>Figure 8 :
8
Figure 8: Max score distribution across contexts</p>
<p>Table 3 :
3
Table 3 displays an unanswerable question generated by Prompt B. Sample Unanswerable Question from Prompt B.
Context
AcknowledgementsWe would like to acknowledge the technical contributions and mentorship of Tanish Jain from Stanford University.5 Results and DiscussionTransformer ModelUsing the evaluation metric under the Trainer class from PyTorch, the F1 score for the transformer model was 15.88.A key reason for a low F1 score was found to be that the transformer model could not generate questions and instead generated sentences similar to the answers in SQuAD.These predicted outputs may be because the model was already familiar with question-answering tasks and generating answers, so it could not generate questions.The distilBERT model was trained on 3.3 billion words from the Wikipedia and Toronto Book Corpus, so finetuning on a much smaller dataset like SQuAD was not effective in reversing distilBERT's inherent behavior to answer questions.As the transformer model was not able to generate questions, we did not use a metric to evaluate the generated outputs.Prompt EngineeringWe used the spaCy metric for similarity scoring.Figure6is a boxplot of the similarity scores for all the generated questions under each prompt.The boxplot reveals that the median similarity score for Prompt D's generated questions was the highest at 0.6444.Furthermore, Prompt D also had the highest outliers, with one being a perfect match from the similarity score of 1. Prompts A and C had an average similarity score of 0.6387 and 0.6321 respectively.Prompt B performed the worst, with the lowest average of 0.6227.Another method we used to establish which prompt performed the best was the prompt that had the most matches with the baseline questions.We established a threshold of 0.7 similarity score to be considered a match.After comparing generated questions with a similarity score above 0.7 with the baseline questions, we discovered that most of the generated questions had the same meaning as the baseline question; while generated questions with a similarity score below 0.7 were inherently different.We defined a match as when the baseline and generated questions were nearly identical.Figure7
Automatic question generation from sentences. Husam Ali, Yllias Chali, Sadid A Hasan, Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles. Articles courts. s de la 17e conférence sur le Traitement Automatique des Langues Naturelles. Articles courts2010</p>
<p>Reinforcement learning based graph-tosequence model for natural question generation. Yu Chen, Lingfei Wu, Mohammed J Zaki, arXiv:1908.049422019arXiv preprint</p>
<p>Bert: Pretraining of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Improving students' learning with effective learning techniques: Promising directions from cognitive and educational psychology. John Dunlosky, Katherine A Rawson, Elizabeth J Marsh, Mitchell J Nathan, Daniel T Willingham, DRM + 13. 201314</p>
<p>Question generation for question answering. Nan Duan, Duyu Tang, Peng Chen, Ming Zhou, Proceedings of the 2017 conference on empirical methods in natural language processing. the 2017 conference on empirical methods in natural language processing2017</p>
<p>Squad 2.0: The stanford question answering dataset. Group Stanford, Jan 8th, 2022</p>
<p>Automatic factual question generation from text. Michael Heilman, 2011Carnegie Mellon UniversityPhD thesis</p>
<p>Chatgpt prompt engineering for developers. Andrew Ng, Isa Fulford, </p>
<p>Liangming Pan, Wenqiang Lei, Tat-Seng Chua, Min-Yen Kan, arXiv:1905.08949Recent advances in neural question generation. 2019arXiv preprint</p>
<p>Know what you don't know: Unanswerable questions for squad. Pranav Rajpurkar, Robin Jia, Percy Liang, arXiv:1806.038222018arXiv preprint</p>
<p>Squad: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, arXiv:1606.052502016arXiv preprint</p>
<p>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, arXiv:1910.011082019arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprintTLI + 23</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, 201730</p>
<p>To what extent do study habits relate to performance?. Elise M Walck-Shannon, Shaina F Rowell, Regina F Frey, CBE-Life Sciences Education. 20162021</p>
<p>Machine comprehension by text-totext neural question generation. Tong Ywg + 17] Xingdi Yuan, Caglar Wang, Alessandro Gulcehre, Philip Sordoni, Sandeep Bachman, Saizheng Subramanian, Adam Zhang, Trischler, arXiv:1705.020122017arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
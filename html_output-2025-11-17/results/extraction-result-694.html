<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-694 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-694</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-694</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-dcc2941b5d7a70acf8bd609570e4b6f106ecabc4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dcc2941b5d7a70acf8bd609570e4b6f106ecabc4" target="_blank">Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results are investigated and the guidelines on reporting novel results as comparisons against baseline methods are provided.</p>
                <p><strong>Paper Abstract:</strong> Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e694.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e694.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>hyperparameter_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyper-parameter mismatch between reported descriptions and code/defaults</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Differences between the hyper-parameters reported in papers or baseline descriptions and the actual hyper-parameter values used in code implementations (or different defaults across codebases) that change measured performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Policy-gradient benchmark experiments (TRPO and DDPG) on MuJoCo via OpenAI Gym</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Empirical evaluation of TRPO and DDPG using public TensorFlow codebases (rllab TRPO and Q-Prop DDPG) across Hopper-v1 and Half-Cheetah-v1 to study reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / reported baseline experimental settings</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>public TensorFlow implementations / experiment scripts (rllab TRPO, Q-Prop DDPG)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Cited works and reported baselines often specify different hyper-parameter values (or omit them) than those present in the code used by later researchers; examples include differing batch sizes (rllab default 32 vs other works 64), differing network architectures, differing reward scaling, and learning-rate settings — producing divergent behavior when reproducing results.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>hyperparameters (batch size, learning rates, reward scale, regularization, GAE lambda, step size)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>reproducibility study comparing outcomes across hyper-parameter sweeps using the referenced code implementations and the hyper-parameter values reported in prior works</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Empirical comparison of average return and standard deviation across multiple runs (5 runs for parameter sweeps, 10 runs for best-set seed variance); two-sample t-tests on learning-curve samples (t and p-values reported), visual comparison of learning curves, and tables of Average Return and Std Return at multiple iteration counts.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial — changing hyper-parameters produced large performance differences. Examples: TRPO Half-Cheetah network-architecture comparisons yielded statistically significant differences (two-sample t-test vs (64,64): t = -13.4165, p = 0.0000; vs (100,50,25): t = -11.3368, p = 0.0016). Batch-size changes (TRPO) led to much better performance with 25000 vs smaller sizes; DDPG performance improved with larger mini-batch (128) on Half-Cheetah. The paper reports large absolute differences in Average Return across settings (see Tables 1 & 2) and emphasizes that running with suboptimal hyper-parameters can produce inaccurate baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widespread among the surveyed baseline comparisons: the authors state 'Each cited work uses a different set of experimental hyper-parameters' and note many under-report hyper-parameters; no precise percentage provided but described as common across multiple works and implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Omitted or inconsistent reporting of hyper-parameters in papers, different defaults across codebases/implementations, and lack of standardized presets for baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Recommend reporting all hyper-parameters used, publishing tuned presets, averaging results over many randomized trials, and providing benchmarked implementations with preset hyper-parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not experimentally quantified beyond demonstration that careful tuning plus averaging reduces variance; authors show improved and more stable performance when using best cross-sections of hyper-parameters and averaging runs (they run best configurations for 10 trials and observe remaining variance).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep reinforcement learning (continuous control)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e694.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e694.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>architecture_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model architecture mismatch between reported baselines and code/used settings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Discrepancy where the neural network architectures used in published baseline reports differ from those actually used in reproduced experiments or in available code, producing different performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Policy-gradient benchmark experiments (TRPO and DDPG) on MuJoCo via OpenAI Gym</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluation comparing different policy network architectures (e.g., (64,64), (100,50,25), (400,300)) on Hopper and Half-Cheetah to study sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>reported baseline descriptions and methods sections in prior work</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment scripts / model definitions in public codebases</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>model architecture mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Public reports and some baseline tables used one architecture while either other papers or the code implementations used different layer sizes; the authors found (400,300) to significantly outperform smaller architectures for Half-Cheetah, yet some baseline reports used smaller architectures, leading to misaligned comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model architecture (policy and value network hidden layers)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical ablation/sweep of network architectures using the referenced implementations and measurement of resulting learning curves and returns.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Comparison of average return learning curves across architectures, statistical testing (two-sample t-tests) on sample rollouts; example TRPO Half-Cheetah t-tests reported (see impact).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Significant — e.g., architecture (400,300) significantly outperformed (64,64) and (100,50,25) on Half-Cheetah; TRPO Half-Cheetah two-sample t-test vs (64,64): t = -13.4165, p = 0.0000 indicating a highly significant difference.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in multiple baseline comparisons; authors note that the best architecture they found was not the one used in reporting baselines in other works (explicitly citing mismatch with [10,12]).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Insufficient specification of architecture choices in papers or use of different defaults/choices in code repositories; lack of standardization for baseline architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Report exact network architectures used, provide code with the canonical architectures, and run architecture sensitivity analyses when presenting baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Authors demonstrate that explicitly choosing and reporting the (400,300) architecture yields better and reproducible results for Half-Cheetah in their experiments; no global quantitative effectiveness percentage given.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e694.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e694.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>reward_scaling_discrepancy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reward-scaling discrepancy between prior claims and reproduced code experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mismatch between previously reported optimal reward-scaling (e.g., scaling rewards by 0.1) and the authors' reproduced results, where no scaling sometimes performs better.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DDPG experiments on MuJoCo continuous control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Studies of DDPG sensitivity to reward scaling (rescaling environment rewards before learning) using Q-Prop/DDPG TensorFlow implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>reported hyper-parameter recommendation in prior papers / implementation documentation</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment scripts (DDPG implementation used in Q-Prop and cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch (reward scaling) / conflicting specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Prior works [9,10] suggested rescaling rewards by 0.1 improved DDPG stability; in the reproduced experiments the authors found no reward scaling (RS=1) produced much higher returns on Half-Cheetah, contradicting earlier claims and indicating that documentation/claims about reward scaling do not generalize or match code behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / reward preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical sweep over reward-scaling values using the DDPG implementation and comparing learning curves and final returns.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Comparison of learning curves and final Average Return and Std Return across reward-scaling settings; visualization (Figure 8) and tabulated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Meaningful — for Half-Cheetah, RS=1 (no scaling) yielded much higher returns than RS=0.1, directly contradicting prior reported optimal scaling and potentially causing prior works to understate DDPG performance if they used different scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Reported across the specific tasks studied; authors note that prior works reported an optimal value of 0.1 but that this does not hold in their experiments — indicates at least some prevalence of contradictory claims across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Task-dependent sensitivity, under-reporting of reward preprocessing in papers, and implicit external hyper-parameters not standardized across implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Report reward preprocessing details explicitly, run and publish reward-scaling sweeps, and provide tuned presets per environment.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Authors' recommended approach (explicitly testing reward scales and publishing results) clarified that RS=1 worked better for Half-Cheetah in their experiments; no broader effectiveness numbers given.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e694.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e694.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>batch_size_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Batch-size mismatch between descriptions and implementations affecting training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Differences in mini-batch or sample-batch sizes between reported experiments and the code (or between different codebases) that materially affect algorithm performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DDPG and TRPO training pipelines on MuJoCo tasks</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Analysis of the influence of mini-batch (DDPG) and sample batch (TRPO) sizes on performance for both algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>reported experimental hyper-parameters / implementation defaults</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment scripts and replay-buffer sampling code in TensorFlow implementations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch (batch size) / implementation default differences</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Different works use different batch sizes (e.g., DDPG batch sizes 32 vs 64 in different implementations; TRPO sample batch sizes 1000, 5000, 25000). These mismatches change learning dynamics and final performance; the rllab implementation defaulted to 32 while other works used 64, and TRPO benefits from very large batch sizes in constrained-iteration settings.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure (mini-batch sampling / sample collection size)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical sweeps over batch sizes for both DDPG (32, 64, 128) and TRPO (1000, 5000, 25000) and observation of resulting learning curves.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Average Return and Std Return comparisons across batch sizes and visualization (Figures 3 and 4); qualitative observation that TRPO performs best with batch size 25000 and DDPG improves with 128 on Half-Cheetah.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Significant — TRPO with batch size 25000 produced the best results within 5000 iterations; DDPG saw notable improvements with larger mini-batch (128) for Half-Cheetah. Hopper less sensitive in some cases. These differences alter comparative performance between algorithms and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common across implementations and papers examined; differences in chosen batch sizes are repeatedly observed across cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Different implementation defaults and insufficient reporting of batch-size choices in method sections; lack of consensus on appropriate batch sizes per environment.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Publish exact batch-size settings, include batch-size sensitivity studies, and provide recommended presets for standard benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Authors' experiments show selecting larger batch sizes (TRPO 25000, DDPG 128 for Half-Cheetah) yields better performance in their runs; no universal effectiveness guarantee provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e694.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e694.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>random_seed_cherry-picking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random-seed selection / cherry-picking hidden in experimental code or reporting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The practice or possibility that codebases or experiment scripts include mechanisms to search/select beneficial random seeds, and averaging or selecting different seed sets produces divergent reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Experimental runs and averaging pipelines for TRPO and DDPG</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Study of variance due to random seeds: authors run many trials (10) with best hyper-parameters and compare averages of different subsets of runs to demonstrate seed-induced variability.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experiment protocol / code comments and scripts (hyperparameter search and seed selection)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment scripts and hyperparameter/seed search code in repositories (example rllab hyperopt script cited)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implicit/hidden experimental selection (random-seed cherry-picking)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Variance due solely to random seeds can produce learning curves or averaged results that belong to different distributions; the paper notes known codebases sometimes include scripts to search for best seeds, and demonstrates that averaging different groups of 5 runs (out of 10) can yield significantly different results, implying possible hidden selection or cherry-picking in reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>experimental protocol / random seed management and result averaging</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Reproducibility experiment: run 10 independent trials with same hyper-parameters and randomly average two disjoint groups of 5; compare resulting average learning curves and distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Visual comparison of averaged learning curves (Figure 10) and statistical testing across training distributions (authors report an average 2-sample t-test across entire training distributions: t = -9.0916, p = 0.0016 for Half-Cheetah).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>High — different groupings of runs produced averages that did not fall within the same distribution; for Half-Cheetah the t-test indicates strongly significant differences, meaning reported averages can be meaningfully altered by seed selection. This undermines fairness of baseline comparisons when few trials are used or seeds are chosen non-randomly.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Authors describe seed-selection issues as a known problem in several codebases and observe high variance in both DDPG and TRPO across seeds; no numerical prevalence across the literature is given but the problem is presented as common and consequential.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>High variance of algorithms, stochastic environments, and practices (or tools) that enable seed search without disclosure; insufficiently many independent trials reported in papers.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Run and average many independent trials with distinct random seeds, report the number of trials, and avoid selecting seeds post-hoc; make seed-selection scripts and raw runs public.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Averaging more trials reduces effect of seed variance in expectation; authors recommend >5 trials and show that even with 10 trials variance remains, implying more trials and transparency are required — no precise N provided but they note further investigation needed to determine sufficient N.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e694.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e694.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>metric_reporting_misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Misalignment between metric reporting in papers (e.g., max vs average) and what is appropriate/reproducible</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of biased metrics (Maximum Average Return or Maximum Return) in papers' tables or summaries that misrepresent the typical performance produced by code/experiments and prevent fair reproducibility and comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Benchmark reporting practices for RL continuous-control experiments</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Analysis of which performance metrics are commonly reported (Max Average Return, Max Return, Average Return, Std Return) and their suitability for reproducible comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper result summaries / reported performance metrics in tables and abstracts</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation scripts that compute reported metrics from rollouts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete / biased metric specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Many works report maximum returns or max-average returns which are biased by outlier runs, while the authors argue Average Return and Std Return are more stable; reporting only max metrics in papers can misalign expectations when running the code, which typically produces distributions where maxes are outliers.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics and reporting</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Comparison of reported metrics across prior works and the authors' experimental distributions; empirical demonstration that Max Return is an unsuitable metric due to high-variance policies producing outlying trials.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Presentation of Average Return and Std Return across multiple runs (tables 1 and 2) and qualitative argument plus omission of Maximum Return from their primary comparisons; argument supported by observed high variance between runs.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Misleading comparisons — works reporting only max metrics can appear to outperform others while typical runs (average) do not; authors demonstrate considerable differences between reported max values in literature and their multi-run average and std results (see Tables 1 & 2), undermining fair reproduction and comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common: authors note 'many works only include the Max Average Return' and characterize this as misleading given observed variance.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Choice to present the most favorable statistic (max) without acknowledging variance or providing distributions; lack of community standards on which metrics to report for RL baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Recommend reporting Average Return and Standard Deviation (and number of trials), avoiding sole reliance on maximum-return metrics, and publishing full learning curves.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantitatively evaluated beyond demonstration that average+std better characterize algorithm behavior; authors provide tables and plots illustrating that average+std are more stable and informative.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e694.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e694.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>implementation_variant_discrepancy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Different algorithm variants / implementation differences across codebases vs described algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variations in implementation details (e.g., finite-difference HVP vs Fisher information usage, conjugate-gradient damping defaults) across codebases cause differences from the algorithmic descriptions in papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TRPO and DDPG implementations in public repositories</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Investigation into TRPO regularization coefficient, conjugate-gradient implementation differences, and other low-level implementation choices that differ from algorithm descriptions and affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>algorithm specification in original method papers vs code implementations in repositories</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>library code / optimization subroutines (conjugate gradient, Fisher-vector product approximations)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different algorithm variant / implementation detail mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Codebases implement algorithmic pieces with differing approximations or defaults (e.g., adding identity scaling to Fisher matrix as regularizer with varying RC values, finite-difference HVP implementations), and these implementation choices (and their default hyper-parameters) are not always fully described in natural language method sections, producing differing empirical behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training optimization details (second-order approximations, conjugate gradient damping/regularization)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical sensitivity analysis of TRPO regularization coefficient and comparison to cited implementations; checking implementation comments and code behavior in public repos.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Average Return and Std Return over parameter sweeps of RC values reported (authors state no significant difference across RC sweep overall but some effect on Hopper), supported by t-test statistics for comparisons (examples provided in footnotes).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Moderate — regularization coefficient variations showed little overall effect but had some significance on Hopper (example t-values provided), indicating that low-level implementation choices can still influence reproducibility for certain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Present in the studied codebases; authors highlight that 'cited implementation values may use different sets of hyper-parameters' for these low-level choices across works.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Incomplete specification of low-level algorithmic approximations in papers, varying practical implementations in released code, and differing default numerical stabilizers.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Document low-level implementation details and defaults in papers and code, release reference implementations, and include sensitivity analyses for these options.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Authors' sensitivity sweeps show that documenting and testing these settings reduces uncertainty about their effects; they observed limited effect for RC in many cases but recommend documentation nonetheless.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Benchmarking deep reinforcement learning for continuous control <em>(Rating: 2)</em></li>
                <li>Q-prop: Sample-efficient policy gradient with an off-policy critic <em>(Rating: 2)</em></li>
                <li>Continuous control with deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Trust region policy optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-694",
    "paper_id": "paper-dcc2941b5d7a70acf8bd609570e4b6f106ecabc4",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "hyperparameter_mismatch",
            "name_full": "Hyper-parameter mismatch between reported descriptions and code/defaults",
            "brief_description": "Differences between the hyper-parameters reported in papers or baseline descriptions and the actual hyper-parameter values used in code implementations (or different defaults across codebases) that change measured performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Policy-gradient benchmark experiments (TRPO and DDPG) on MuJoCo via OpenAI Gym",
            "system_description": "Empirical evaluation of TRPO and DDPG using public TensorFlow codebases (rllab TRPO and Q-Prop DDPG) across Hopper-v1 and Half-Cheetah-v1 to study reproducibility.",
            "nl_description_type": "research paper methods section / reported baseline experimental settings",
            "code_implementation_type": "public TensorFlow implementations / experiment scripts (rllab TRPO, Q-Prop DDPG)",
            "gap_type": "hyperparameter mismatch",
            "gap_description": "Cited works and reported baselines often specify different hyper-parameter values (or omit them) than those present in the code used by later researchers; examples include differing batch sizes (rllab default 32 vs other works 64), differing network architectures, differing reward scaling, and learning-rate settings — producing divergent behavior when reproducing results.",
            "gap_location": "hyperparameters (batch size, learning rates, reward scale, regularization, GAE lambda, step size)",
            "detection_method": "reproducibility study comparing outcomes across hyper-parameter sweeps using the referenced code implementations and the hyper-parameter values reported in prior works",
            "measurement_method": "Empirical comparison of average return and standard deviation across multiple runs (5 runs for parameter sweeps, 10 runs for best-set seed variance); two-sample t-tests on learning-curve samples (t and p-values reported), visual comparison of learning curves, and tables of Average Return and Std Return at multiple iteration counts.",
            "impact_on_results": "Substantial — changing hyper-parameters produced large performance differences. Examples: TRPO Half-Cheetah network-architecture comparisons yielded statistically significant differences (two-sample t-test vs (64,64): t = -13.4165, p = 0.0000; vs (100,50,25): t = -11.3368, p = 0.0016). Batch-size changes (TRPO) led to much better performance with 25000 vs smaller sizes; DDPG performance improved with larger mini-batch (128) on Half-Cheetah. The paper reports large absolute differences in Average Return across settings (see Tables 1 & 2) and emphasizes that running with suboptimal hyper-parameters can produce inaccurate baseline comparisons.",
            "frequency_or_prevalence": "Widespread among the surveyed baseline comparisons: the authors state 'Each cited work uses a different set of experimental hyper-parameters' and note many under-report hyper-parameters; no precise percentage provided but described as common across multiple works and implementations.",
            "root_cause": "Omitted or inconsistent reporting of hyper-parameters in papers, different defaults across codebases/implementations, and lack of standardized presets for baselines.",
            "mitigation_approach": "Recommend reporting all hyper-parameters used, publishing tuned presets, averaging results over many randomized trials, and providing benchmarked implementations with preset hyper-parameters.",
            "mitigation_effectiveness": "Not experimentally quantified beyond demonstration that careful tuning plus averaging reduces variance; authors show improved and more stable performance when using best cross-sections of hyper-parameters and averaging runs (they run best configurations for 10 trials and observe remaining variance).",
            "domain_or_field": "machine learning / deep reinforcement learning (continuous control)",
            "reproducibility_impact": true,
            "uuid": "e694.0",
            "source_info": {
                "paper_title": "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "architecture_mismatch",
            "name_full": "Model architecture mismatch between reported baselines and code/used settings",
            "brief_description": "Discrepancy where the neural network architectures used in published baseline reports differ from those actually used in reproduced experiments or in available code, producing different performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Policy-gradient benchmark experiments (TRPO and DDPG) on MuJoCo via OpenAI Gym",
            "system_description": "Evaluation comparing different policy network architectures (e.g., (64,64), (100,50,25), (400,300)) on Hopper and Half-Cheetah to study sensitivity.",
            "nl_description_type": "reported baseline descriptions and methods sections in prior work",
            "code_implementation_type": "experiment scripts / model definitions in public codebases",
            "gap_type": "model architecture mismatch",
            "gap_description": "Public reports and some baseline tables used one architecture while either other papers or the code implementations used different layer sizes; the authors found (400,300) to significantly outperform smaller architectures for Half-Cheetah, yet some baseline reports used smaller architectures, leading to misaligned comparisons.",
            "gap_location": "model architecture (policy and value network hidden layers)",
            "detection_method": "Empirical ablation/sweep of network architectures using the referenced implementations and measurement of resulting learning curves and returns.",
            "measurement_method": "Comparison of average return learning curves across architectures, statistical testing (two-sample t-tests) on sample rollouts; example TRPO Half-Cheetah t-tests reported (see impact).",
            "impact_on_results": "Significant — e.g., architecture (400,300) significantly outperformed (64,64) and (100,50,25) on Half-Cheetah; TRPO Half-Cheetah two-sample t-test vs (64,64): t = -13.4165, p = 0.0000 indicating a highly significant difference.",
            "frequency_or_prevalence": "Observed in multiple baseline comparisons; authors note that the best architecture they found was not the one used in reporting baselines in other works (explicitly citing mismatch with [10,12]).",
            "root_cause": "Insufficient specification of architecture choices in papers or use of different defaults/choices in code repositories; lack of standardization for baseline architectures.",
            "mitigation_approach": "Report exact network architectures used, provide code with the canonical architectures, and run architecture sensitivity analyses when presenting baselines.",
            "mitigation_effectiveness": "Authors demonstrate that explicitly choosing and reporting the (400,300) architecture yields better and reproducible results for Half-Cheetah in their experiments; no global quantitative effectiveness percentage given.",
            "domain_or_field": "machine learning / deep reinforcement learning",
            "reproducibility_impact": true,
            "uuid": "e694.1",
            "source_info": {
                "paper_title": "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "reward_scaling_discrepancy",
            "name_full": "Reward-scaling discrepancy between prior claims and reproduced code experiments",
            "brief_description": "Mismatch between previously reported optimal reward-scaling (e.g., scaling rewards by 0.1) and the authors' reproduced results, where no scaling sometimes performs better.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DDPG experiments on MuJoCo continuous control tasks",
            "system_description": "Studies of DDPG sensitivity to reward scaling (rescaling environment rewards before learning) using Q-Prop/DDPG TensorFlow implementation.",
            "nl_description_type": "reported hyper-parameter recommendation in prior papers / implementation documentation",
            "code_implementation_type": "experiment scripts (DDPG implementation used in Q-Prop and cited works)",
            "gap_type": "hyperparameter mismatch (reward scaling) / conflicting specification",
            "gap_description": "Prior works [9,10] suggested rescaling rewards by 0.1 improved DDPG stability; in the reproduced experiments the authors found no reward scaling (RS=1) produced much higher returns on Half-Cheetah, contradicting earlier claims and indicating that documentation/claims about reward scaling do not generalize or match code behavior.",
            "gap_location": "training procedure / reward preprocessing",
            "detection_method": "Empirical sweep over reward-scaling values using the DDPG implementation and comparing learning curves and final returns.",
            "measurement_method": "Comparison of learning curves and final Average Return and Std Return across reward-scaling settings; visualization (Figure 8) and tabulated metrics.",
            "impact_on_results": "Meaningful — for Half-Cheetah, RS=1 (no scaling) yielded much higher returns than RS=0.1, directly contradicting prior reported optimal scaling and potentially causing prior works to understate DDPG performance if they used different scaling.",
            "frequency_or_prevalence": "Reported across the specific tasks studied; authors note that prior works reported an optimal value of 0.1 but that this does not hold in their experiments — indicates at least some prevalence of contradictory claims across papers.",
            "root_cause": "Task-dependent sensitivity, under-reporting of reward preprocessing in papers, and implicit external hyper-parameters not standardized across implementations.",
            "mitigation_approach": "Report reward preprocessing details explicitly, run and publish reward-scaling sweeps, and provide tuned presets per environment.",
            "mitigation_effectiveness": "Authors' recommended approach (explicitly testing reward scales and publishing results) clarified that RS=1 worked better for Half-Cheetah in their experiments; no broader effectiveness numbers given.",
            "domain_or_field": "machine learning / deep reinforcement learning",
            "reproducibility_impact": true,
            "uuid": "e694.2",
            "source_info": {
                "paper_title": "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "batch_size_mismatch",
            "name_full": "Batch-size mismatch between descriptions and implementations affecting training",
            "brief_description": "Differences in mini-batch or sample-batch sizes between reported experiments and the code (or between different codebases) that materially affect algorithm performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DDPG and TRPO training pipelines on MuJoCo tasks",
            "system_description": "Analysis of the influence of mini-batch (DDPG) and sample batch (TRPO) sizes on performance for both algorithms.",
            "nl_description_type": "reported experimental hyper-parameters / implementation defaults",
            "code_implementation_type": "experiment scripts and replay-buffer sampling code in TensorFlow implementations",
            "gap_type": "hyperparameter mismatch (batch size) / implementation default differences",
            "gap_description": "Different works use different batch sizes (e.g., DDPG batch sizes 32 vs 64 in different implementations; TRPO sample batch sizes 1000, 5000, 25000). These mismatches change learning dynamics and final performance; the rllab implementation defaulted to 32 while other works used 64, and TRPO benefits from very large batch sizes in constrained-iteration settings.",
            "gap_location": "training procedure (mini-batch sampling / sample collection size)",
            "detection_method": "Empirical sweeps over batch sizes for both DDPG (32, 64, 128) and TRPO (1000, 5000, 25000) and observation of resulting learning curves.",
            "measurement_method": "Average Return and Std Return comparisons across batch sizes and visualization (Figures 3 and 4); qualitative observation that TRPO performs best with batch size 25000 and DDPG improves with 128 on Half-Cheetah.",
            "impact_on_results": "Significant — TRPO with batch size 25000 produced the best results within 5000 iterations; DDPG saw notable improvements with larger mini-batch (128) for Half-Cheetah. Hopper less sensitive in some cases. These differences alter comparative performance between algorithms and baselines.",
            "frequency_or_prevalence": "Common across implementations and papers examined; differences in chosen batch sizes are repeatedly observed across cited works.",
            "root_cause": "Different implementation defaults and insufficient reporting of batch-size choices in method sections; lack of consensus on appropriate batch sizes per environment.",
            "mitigation_approach": "Publish exact batch-size settings, include batch-size sensitivity studies, and provide recommended presets for standard benchmarks.",
            "mitigation_effectiveness": "Authors' experiments show selecting larger batch sizes (TRPO 25000, DDPG 128 for Half-Cheetah) yields better performance in their runs; no universal effectiveness guarantee provided.",
            "domain_or_field": "machine learning / deep reinforcement learning",
            "reproducibility_impact": true,
            "uuid": "e694.3",
            "source_info": {
                "paper_title": "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "random_seed_cherry-picking",
            "name_full": "Random-seed selection / cherry-picking hidden in experimental code or reporting",
            "brief_description": "The practice or possibility that codebases or experiment scripts include mechanisms to search/select beneficial random seeds, and averaging or selecting different seed sets produces divergent reported results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Experimental runs and averaging pipelines for TRPO and DDPG",
            "system_description": "Study of variance due to random seeds: authors run many trials (10) with best hyper-parameters and compare averages of different subsets of runs to demonstrate seed-induced variability.",
            "nl_description_type": "experiment protocol / code comments and scripts (hyperparameter search and seed selection)",
            "code_implementation_type": "experiment scripts and hyperparameter/seed search code in repositories (example rllab hyperopt script cited)",
            "gap_type": "implicit/hidden experimental selection (random-seed cherry-picking)",
            "gap_description": "Variance due solely to random seeds can produce learning curves or averaged results that belong to different distributions; the paper notes known codebases sometimes include scripts to search for best seeds, and demonstrates that averaging different groups of 5 runs (out of 10) can yield significantly different results, implying possible hidden selection or cherry-picking in reported results.",
            "gap_location": "experimental protocol / random seed management and result averaging",
            "detection_method": "Reproducibility experiment: run 10 independent trials with same hyper-parameters and randomly average two disjoint groups of 5; compare resulting average learning curves and distributions.",
            "measurement_method": "Visual comparison of averaged learning curves (Figure 10) and statistical testing across training distributions (authors report an average 2-sample t-test across entire training distributions: t = -9.0916, p = 0.0016 for Half-Cheetah).",
            "impact_on_results": "High — different groupings of runs produced averages that did not fall within the same distribution; for Half-Cheetah the t-test indicates strongly significant differences, meaning reported averages can be meaningfully altered by seed selection. This undermines fairness of baseline comparisons when few trials are used or seeds are chosen non-randomly.",
            "frequency_or_prevalence": "Authors describe seed-selection issues as a known problem in several codebases and observe high variance in both DDPG and TRPO across seeds; no numerical prevalence across the literature is given but the problem is presented as common and consequential.",
            "root_cause": "High variance of algorithms, stochastic environments, and practices (or tools) that enable seed search without disclosure; insufficiently many independent trials reported in papers.",
            "mitigation_approach": "Run and average many independent trials with distinct random seeds, report the number of trials, and avoid selecting seeds post-hoc; make seed-selection scripts and raw runs public.",
            "mitigation_effectiveness": "Averaging more trials reduces effect of seed variance in expectation; authors recommend &gt;5 trials and show that even with 10 trials variance remains, implying more trials and transparency are required — no precise N provided but they note further investigation needed to determine sufficient N.",
            "domain_or_field": "machine learning / deep reinforcement learning",
            "reproducibility_impact": true,
            "uuid": "e694.4",
            "source_info": {
                "paper_title": "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "metric_reporting_misalignment",
            "name_full": "Misalignment between metric reporting in papers (e.g., max vs average) and what is appropriate/reproducible",
            "brief_description": "Use of biased metrics (Maximum Average Return or Maximum Return) in papers' tables or summaries that misrepresent the typical performance produced by code/experiments and prevent fair reproducibility and comparison.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Benchmark reporting practices for RL continuous-control experiments",
            "system_description": "Analysis of which performance metrics are commonly reported (Max Average Return, Max Return, Average Return, Std Return) and their suitability for reproducible comparisons.",
            "nl_description_type": "paper result summaries / reported performance metrics in tables and abstracts",
            "code_implementation_type": "evaluation scripts that compute reported metrics from rollouts",
            "gap_type": "incomplete / biased metric specification",
            "gap_description": "Many works report maximum returns or max-average returns which are biased by outlier runs, while the authors argue Average Return and Std Return are more stable; reporting only max metrics in papers can misalign expectations when running the code, which typically produces distributions where maxes are outliers.",
            "gap_location": "evaluation metrics and reporting",
            "detection_method": "Comparison of reported metrics across prior works and the authors' experimental distributions; empirical demonstration that Max Return is an unsuitable metric due to high-variance policies producing outlying trials.",
            "measurement_method": "Presentation of Average Return and Std Return across multiple runs (tables 1 and 2) and qualitative argument plus omission of Maximum Return from their primary comparisons; argument supported by observed high variance between runs.",
            "impact_on_results": "Misleading comparisons — works reporting only max metrics can appear to outperform others while typical runs (average) do not; authors demonstrate considerable differences between reported max values in literature and their multi-run average and std results (see Tables 1 & 2), undermining fair reproduction and comparison.",
            "frequency_or_prevalence": "Common: authors note 'many works only include the Max Average Return' and characterize this as misleading given observed variance.",
            "root_cause": "Choice to present the most favorable statistic (max) without acknowledging variance or providing distributions; lack of community standards on which metrics to report for RL baselines.",
            "mitigation_approach": "Recommend reporting Average Return and Standard Deviation (and number of trials), avoiding sole reliance on maximum-return metrics, and publishing full learning curves.",
            "mitigation_effectiveness": "Not quantitatively evaluated beyond demonstration that average+std better characterize algorithm behavior; authors provide tables and plots illustrating that average+std are more stable and informative.",
            "domain_or_field": "machine learning / deep reinforcement learning",
            "reproducibility_impact": true,
            "uuid": "e694.5",
            "source_info": {
                "paper_title": "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "implementation_variant_discrepancy",
            "name_full": "Different algorithm variants / implementation differences across codebases vs described algorithm",
            "brief_description": "Variations in implementation details (e.g., finite-difference HVP vs Fisher information usage, conjugate-gradient damping defaults) across codebases cause differences from the algorithmic descriptions in papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TRPO and DDPG implementations in public repositories",
            "system_description": "Investigation into TRPO regularization coefficient, conjugate-gradient implementation differences, and other low-level implementation choices that differ from algorithm descriptions and affect outcomes.",
            "nl_description_type": "algorithm specification in original method papers vs code implementations in repositories",
            "code_implementation_type": "library code / optimization subroutines (conjugate gradient, Fisher-vector product approximations)",
            "gap_type": "different algorithm variant / implementation detail mismatch",
            "gap_description": "Codebases implement algorithmic pieces with differing approximations or defaults (e.g., adding identity scaling to Fisher matrix as regularizer with varying RC values, finite-difference HVP implementations), and these implementation choices (and their default hyper-parameters) are not always fully described in natural language method sections, producing differing empirical behavior.",
            "gap_location": "training optimization details (second-order approximations, conjugate gradient damping/regularization)",
            "detection_method": "Empirical sensitivity analysis of TRPO regularization coefficient and comparison to cited implementations; checking implementation comments and code behavior in public repos.",
            "measurement_method": "Average Return and Std Return over parameter sweeps of RC values reported (authors state no significant difference across RC sweep overall but some effect on Hopper), supported by t-test statistics for comparisons (examples provided in footnotes).",
            "impact_on_results": "Moderate — regularization coefficient variations showed little overall effect but had some significance on Hopper (example t-values provided), indicating that low-level implementation choices can still influence reproducibility for certain tasks.",
            "frequency_or_prevalence": "Present in the studied codebases; authors highlight that 'cited implementation values may use different sets of hyper-parameters' for these low-level choices across works.",
            "root_cause": "Incomplete specification of low-level algorithmic approximations in papers, varying practical implementations in released code, and differing default numerical stabilizers.",
            "mitigation_approach": "Document low-level implementation details and defaults in papers and code, release reference implementations, and include sensitivity analyses for these options.",
            "mitigation_effectiveness": "Authors' sensitivity sweeps show that documenting and testing these settings reduces uncertainty about their effects; they observed limited effect for RC in many cases but recommend documentation nonetheless.",
            "domain_or_field": "machine learning / deep reinforcement learning",
            "reproducibility_impact": true,
            "uuid": "e694.6",
            "source_info": {
                "paper_title": "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control",
                "publication_date_yy_mm": "2017-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Benchmarking deep reinforcement learning for continuous control",
            "rating": 2
        },
        {
            "paper_title": "Q-prop: Sample-efficient policy gradient with an off-policy critic",
            "rating": 2
        },
        {
            "paper_title": "Continuous control with deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Trust region policy optimization",
            "rating": 1
        }
    ],
    "cost": 0.01400625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control</h1>
<p>Riashat Islam ${ }^{* \dagger}$<br>School of Computer Science<br>McGill University<br>Montreal, QC, Canada<br>riashat.islam@mail.mcgill.ca<br>Peter Henderson ${ }^{\dagger}$<br>School of Computer Science<br>McGill University<br>Montreal, QC, Canada<br>peter.henderson@mail.mcgill.ca</p>
<h2>Maziar Gomrokchi</h2>
<p>School of Computer Science
McGill University
Montreal, QC, Canada
maziar.gomrokchi@mail.mcgill.ca</p>
<h2>Doina Precup</h2>
<p>School of Computer Science
McGill University
Montreal, QC, Canada
dprecup@cs.mcgill.ca</p>
<h4>Abstract</h4>
<p>Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.</p>
<h2>1 Introduction</h2>
<p>Deep reinforcement learning with neural network policies and value functions has had enormous success in recent years across a wide range of domains [1, 2, 3, 4]. In particular, model-free reinforcement learning with policy gradient methods have been used to solve complex robotic control tasks [5, 6]. Policy gradient methods can be generally divided into two groups: off-policy gradient methods, such as Deep Deterministic Policy Gradients (DDPG) [1] and on-policy methods, such as Trust Region Policy Optimization (TRPO) [2].
However, often there are many sources of possible instability and variance that can lead to difficulties with reproducing deep policy gradient methods. In this work, we investigate the sources of these difficulties with both on- and off-policy gradient methods for continuous control. We use two MuJoCo [7] physics simulator tasks from OpenAI gym [8] (Hopper-v1 and Half-Cheetah-v1) for our experimental tasks. We investigate two policy gradient algorithms here: DDPG and TRPO. To our knowledge, there are few works [9] which reproduce existing policy gradients methods in reinforcement learning, yet many use as these algorithms as baselines to compare their novel work against $[9,10,11,12]$. We use the code provided by in [9] and [10] for TRPO and DDPG</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(respectively), as these implementations are used in several works directly for comparison [12, 10, 9, 13, 14]</p>
<p>Performance Measures : We examine the general variance of the algorithms and address the importance of presenting all possible metrics across a large number of trials. Three main performance measures commonly used in the literature are: Maximum Average Return, Maximum Return, Standard Deviation of Returns, and Average Return. However, the first two measures are considered to be highly biased, while the last two are considered to be the most stable measures used to compare the performance of proposed algorithms. Thereby, in the rest of this work we only use the Average Return as our comparison measure unless stated otherwise, with final results displaying all metrics ${ }^{3}$.</p>
<p>Hyper-parameter Settings : We also highlight that there can be difficulty in properly fine-tuning hyper-parameter settings, leading to large variations of reported results across a wide range of works as different hyper-parameters are used. As in Tables 2 and 1, this inconsistency within the wide range of reported results makes it difficult to compare DDPG and TRPO as baseline algorithms without careful detailing of hyper-parameters, attention to the fairness of the comparison, and proper tuning of the parameters. Each cited work uses a different set of experimental hyper-parameters for supposed baseline comparisons ${ }^{4}$. Running these algorithms with suboptimal hyper-parameter configurations may result in inaccurate comparisons against these baseline methods. As such, we highlight the significance of tuning various hyper-parameters and assess which of these yield the most significant differences in performance.</p>
<p>Based on our analysis, we encourage that careful consistency should be maintained when reporting results with both of these algorithms, as they are quite susceptible to hyper-parameters and the external sources of variance or randomness.</p>
<h1>2 Experimental Analysis</h1>
<p>We evaluate the off-policy DDPG [1] and on-policy TRPO [2] algorithms on continuous control environments from the OpenAI Gym benchmark [8], using the MuJoCo physics simulator [7]. We empirically show the susceptibility and variance in results due to hyper-parameter configurations on two environments: Hopper ( $\mathcal{S} \subseteq \mathbb{R}^{20}, \mathcal{A} \subseteq \mathbb{R}^{3}$ ) and Half-Cheetah ( $\mathcal{S} \subseteq \mathbb{R}^{20}, \mathcal{A} \subseteq \mathbb{R}^{6}$ ). All experiments ${ }^{5}$ are performed building upon the rllab Tensorflow implementation of TRPO [9] and the Q-Prop Tensorflow implementation of DDPG for our experiments [10].</p>
<p>Experiment Details : We run all variations for 5000 iterations and average all results across 5 runs. We investigate several hyper-parameters: batch size, policy network architecture, step size (TRPO), regularization coefficient (TRPO), generalized advantage estimation ( $\lambda$ ) (TRPO), reward scale (DDPG), and actor-critic learning rates (DDPG). For each of these hyper-parameters we hold all others constant at default settings and vary the one under investigation across commonly used values. Lastly, we run a final set of experiments using the overall best cross-section of hyper-parameters for 10 trials using random seeds. We do this to investigate whether there is a significant difference in the results just due to variance caused by the random seeds.
For TRPO, the default hyper-parameters which we use are: a network architecture of (100,50,25) with ReLU hidden activations for a Gaussian Multilayer Perception Policy [9]; a step size of 0.01; a regularization coefficient of $1 \cdot 10^{-5}$; a Generalized Advantage Estimation $\lambda$ of 1.0 [3]. For DDPG, we use default parameters as follows: a network architecture of $(100,50,25)$ with relu hidden activations for a Gaussian Multilayer Perception Policy [9]; actor-critic learning rates of $1 \cdot 10^{-3}$ and $1 \cdot 10^{-4}$; batch sizes of 64 ; and a reward scale of 0.1 .</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>2.1 Common Hyper-Parameters</p>
<p>First, we investigate several hyper-parameters common to both TRPO and DDPG: policy architecture and batch size. We use the same sets of hyper-parameters as reported in previous works using these implementations in an attempt to reproduce the results reported in these works.</p>
<p>Policy Network Architecture : The policy network architecture can play an important role in the maximum reward achieved by the algorithm due to the amount of information storage provided by the network. We use a hidden layer sizes (64,64) as in [2], (400,300) as in [9, 1], and (100,50,25) as in [10] for comparing the results of these algorithms.</p>
<p>Our results can be found in Figures 1 and 2. Notably, the (400,300) architecture significantly outperforms both other smaller architectures for Half-Cheetah and to a less significant extent Hopper as well. This is true for both TRPO and DDPG. However, the architecture which we found to be the best (400,300) is not the one which is used in reporting results for baselines results in [10, 12].</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: TRPO on Half-Cheetah with different network configurations</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: DDPG on Half-Cheetah and Hopper on different network configurations</p>
<p>For the Hopper environment, for both TRPO and DDPG, results are not as significantly impacted by varying the network architecture, unlike the Half-Cheetah environment. This is somewhat thematic of what we find across all hyper-parameter variations on Hopper, as will be further discussed later. In particular, our investigation of DDPG on different network configurations shows that for the Hopper environment, DDPG is quite unstable no matter the network architecture. This can be attributed partially to the high variance of DDPG itself, but also to the increased stochasticity of the Hopper task. As can be seen in Figure 2, even with varied network architectures, it is difficult to tune DDPG to reproduce results from other works even when using their reported hyper-parameter settings.</p>
<p>Batch Size : The batch size parameter plays an important role in both DDPG and TRPO. In the off-policy DDPG algorithm, the actor and critic updates are made by sampling a mini-batch uniformly</p>
<p><sup>6</sup>All of these use RELU activations for the hidden layers and a Gaussian MLP Policy.</p>
<p><sup>7</sup>For TRPO Half-Cheetah using a two-sample t-test on the sample rollouts: against (64,64) <em>t</em> = -13.4165, <em>p</em> = 0.0000; against (100,50,25) <em>t</em> = -11.3368, <em>p</em> = 0.0016. For TRPO Hopper: against (100,50,25) <em>t</em> = -0.5904, <em>p</em> = 0.2952; against (64,64) <em>t</em> = -1.9081, <em>p</em> = 0.2198</p>
<p>from the replay buffer. Typically, the replay buffer is allowed to be large. In [1] and [10], a batch size of 64 was used, whereas the original rllab implementation uses a batch size of 32. Our analysis with different mini-batches for DDPG $(32,64,128)$ shows that similar performance can be obtained with mini-batch sizes of 32 and 64, whereas significant improvements can be obtained with a batch size of 128.</p>
<p>For TRPO, larger batch sizes are necessary in general. We investigate the same batch sizes as used in [10, 2] of (1000,5000,25000). As expected, a batch size of 25000 produces the best results. As we constrain learning to 5000 episodes, it is intuitive that a larger batch size would perform better in this time frame as more samples are seen. Furthermore, as can be seen in Figure 3 for Half-Cheetah, the smaller batch sizes begin to plateau to a much lower optimum.</p>
<p>By intuition, this may be due to TRPO’s use of conjugate gradient optimization with a KL constraint. With small sample batch sizes, gradients differences between steps may be much larger in a high variance environment and results in a more unstable training regime.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: TRPO on Half-Cheetah and Hopper - Significance of batch size</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: DDPG on Half-Cheetah and Hopper - Significance of the mini batch size</p>
<p>We also highlight that the DDPG algorithm with different batch sizes produces similar results for the Hopper environment. While other works have reported different tuned parameters for DDPG, we establish the high variance of this algorithm, producing similar results with different batch sizes for the Hopper environment, while a larger batch size improves performance in Half-Cheetah as seen in Figure 4.</p>
<h3>2.2 TRPO-Specific Hyper-Parameters</h3>
<p><strong>Regularization Coefficient :</strong> The regularization coefficient (RC) (or conjugate gradient damping factor) is used as a regularizer by adding a factor of the identity matrix to the Fisher matrix (or finite difference HVP in [9]) during the conjugate gradient step. We investigate a range of values between 1 · 10⁻⁵ to .1 based on values used in aforementioned works. We don’t see a significant difference⁸</p>
<p>⁸Using an average of 2-sample t-test comparisons, the largest difference from the default parameter in Hopper is <em>t</em> = 2.8965, <em>p</em> = 0.1443 with RC=0.1 and <em>t</em> = 0.8020, <em>p</em> = 0.4540 with RC=.0001.</p>
<p>in using one particular value of RC over another, though it seems to have a more significant effect on Hopper. Figure 5 shows the average learning graphs for these variations.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Regularization coefficient variations for TRPO. Cited implementation values may use different sets hyper-parameters. See associated works for specific details.</p>
<p>Generalized Advantage Estimation : Generalized advantage estimation [3] has been shown to improve results dramatically for TRPO. Here, we investigate using $\lambda=1.0$ and $\lambda=.97$ for this. We find that for longer iterations, a lower GAE $\lambda$ does in fact improve results for longer sequences in Half-Cheetah and mildly for Hopper ${ }^{9}$. Figure 6 shows the average learning graphs for these variations.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Generalized advantage estimation lambda value variations for TRPO. Cited implementation values may use different sets hyper-parameters. See associated works for specific details.</p>
<p>Step Size : The step size (SS) (effectively the learning rate of TRPO) is the same as the KL-divergence bound for the conjugate gradient steps. Here, we find that the default value of 0.01 appears to work generally the best for both Hopper and Half-Cheetah ${ }^{10}$. Figure 7 shows the average learning curves for these variations. The intuition here is the same behind adjusting learning rates in standard gradient optimization methods, though the formulation is through a constraint rather than a learning rate, it effectively has the same characteristics when tuning it.</p>
<h1>2.3 DDPG-Specific Hyper-Parameters</h1>
<p>We investigate two hyper-parameters which are unique to DDPG which previous works have described as important for improving results [9, 10]: reward scale and actor-critic learning rates.
Reward Scale : As in [9], all the rewards for all tasks were rescaled by a factor of 0.1 to improve the stability of DDPG. It has been claimed that this external hyper-parameter, depending on the task,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Step size variations for TRPO. Cited implementation values may use different sets hyperparameters. See associated works for specific details.</p>
<p>can make the DDPG algorithm unstable. Experimental results in [10] give indication that DDPG is particularly sensitive to different reward scale settings.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: DDPG on Half-Cheetah and Hopper - Significance of the Reward Scaling parameter</p>
<p>Figure 8 shows that even though DDPG performance have been reported to be highly susceptible to the reward scaling parameter, our analysis shows that DDPG does not improve by rescaling the rewards. In fact, for the Half-Cheetah environment, we find that no reward scaling (RS=1) yields much higher returns, even though [10] and [9] have reported an optimal reward scale value to be 0.1. Furthermore, we highlight that often for DDPG, learning curves are not shown for all environments and only tabular results are presented, making it difficult to compare how reward scaling has affected results in prior work.</p>
<p>Actor-Critic Learning Rates: We further investigate the effects of the actor and critic base learning rates as given in [10] and [9], which both use 0.001, 0.0001 (for the critic, and actor respectively). Interestingly, we find that the actor and critic learning rates for DDPG have less of an effect on the Hopper environment than the Half-Cheetah environment. This brings into consideration that keeping other parameters fixed, DDPG is not only susceptible to the learning rates, but there are other sources of variation and randomness in the DDPG algorithm.</p>
<h3>2.4 General Variance</h3>
<p>We investigate the general variance of multiple trials with different random seeds. Variance across random seeds is of particular interest since it has been noted that in several known codebases, there are implementations for searching for the best random seed to use. In particular, we determine whether it is possible to generate learning curves by randomly averaging trials together (with only the seed varied) such that we see statistically significant differences in the average reward learning curve distributions. Thereby, we wish to determine if it is possible to report significantly worse results.</p>
<p><sup>11</sup>One such example in the codebase we use here: https://github.com/openai/rllab/blob/master/contrib/rllab_hyperopt/example/main.py#L21.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: DDPG on Half-Cheetah and Hopper - Actor and Critic Learning Rates</p>
<p>on a baseline policy gradient method such as TRPO or DDPG, just by varying the random seed (or significantly better results for the algorithm under investigation by doing so).</p>
<p>We run a total of 10 trials with our best tuned hyper-parameter configurations as examined previously. We randomly average two groups of 5 and plot the results. We find that there can be a significant difference as seen in Figure 10. Particularly for Half-Cheetah it is possible to get training curves that do not fall within the same distribution at all, just by averaging different runs with the same hyper-parameters, but random seeds.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: TRPO with best hyper-parameter configurations, with average of 5 runs over 2 different sets of experiments under same configuration, producing variant results.</p>
<p>Figure 11 also shows the significance of DDPG instability. Even with fine-tuned hyper-parameter configurations, our analysis shows that stable results with DDPG, on either of the environments cannot be achieved. This further suggests that there might be randomness due to other external sources which affect performance of DDPG on these continuous control tasks.</p>
<p>Our results show that for both DDPG and TRPO, taking two different average across 5 experiment runs do not necessarily produce the same result, and in fact, there is high variance in the obtained results. This emphasizes the need for averaging many runs together when reporting results using a different random seed for each. In this way, future works should attempt to negate the effect of random seeds and environment stochasticity when reporting their results.</p>
<h2>3 Discussion and Conclusion</h2>
<p>Tables 1 and 2 highlight results and metrics presented in various related works which compare to TRPO and DDPG (respectively). We include results from an average of 5 runs across the best cross-section of hyper-parameters (based on our previous investigations). We show various metrics.</p>
<p><sup>12</sup>Average 2-sample t-test run across entire training distributions resulting in t = −9.0916, p = 0.0016 for Half-Cheetah and t = 2.2243, p = 0.1825 for Hopper</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: DDPG with tuned hyper-parameter configurations, with average of 5 runs over 2 different set of experiments under same configuration, producing variant results.</p>
<p>at different numbers of iterations such that a fair comparison can be made against reported results from other works. It can be noted that while some works demonstrate similar results to our own, others vary wildly from our own findings. Furthermore, many works only include the Max Average Return, which can be misleading. Due to the variance we have demonstrated here and the difficulty in reproducing these algorithms, it is extremely important for future works to: (1) report all possible metrics to characterize their own algorithms against TRPO and DDPG (particularly Average Return and Standard Deviation of the returns); (2) report all hyper-parameters used for optimization; (3) attempt to use a somewhat optimal set of hyper-parameters; (4) average results on greater than 5 trials and report how many trials are averaged together[^13]. We intend this work to act as both a guide for accomplishing this and a starting point for determining whether observed values are in line with possible best results on Hopper and Half-Cheetah environments for novice researchers in policy gradients.</p>
<table>
<thead>
<tr>
<th>Environment</th>
<th>Metric</th>
<th>rllab [9]</th>
<th>QProp [10]</th>
<th>lPG [12]</th>
<th>TRPO [2, 3]^{14}</th>
<th>Ours</th>
<th>Ours</th>
<th>Ours</th>
<th>Ours</th>
</tr>
</thead>
<tbody>
<tr>
<td>Half-Cheetah</td>
<td>Length (iters)</td>
<td>500</td>
<td>–</td>
<td>–</td>
<td>500</td>
<td>500</td>
<td>1000</td>
<td>2500</td>
<td>5000</td>
</tr>
<tr>
<td></td>
<td>Length (episodes)</td>
<td>∼25k</td>
<td>30k</td>
<td>10k</td>
<td>∼12.5k</td>
<td>∼12.5k</td>
<td>∼25k</td>
<td>∼62.5k</td>
<td>∼125k</td>
</tr>
<tr>
<td></td>
<td>Average Return</td>
<td>1914.0</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>3576.08</td>
<td>3995.4</td>
<td>4638.52</td>
<td>5010.83</td>
</tr>
<tr>
<td></td>
<td>Max Average Return</td>
<td>–</td>
<td>4734</td>
<td>2889</td>
<td>4855.00</td>
<td>3980.61</td>
<td>4360.77</td>
<td>4889.18</td>
<td>5197.40</td>
</tr>
<tr>
<td></td>
<td>Std Return</td>
<td>120.1</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>434.78</td>
<td>502.57</td>
<td>419.08</td>
<td>443.87</td>
</tr>
<tr>
<td>Hopper</td>
<td>Length (iters)</td>
<td>500</td>
<td>–</td>
<td>–</td>
<td>500</td>
<td>500</td>
<td>1000</td>
<td>2500</td>
<td>5000</td>
</tr>
<tr>
<td></td>
<td>Length (episodes)</td>
<td>∼25k</td>
<td>30k</td>
<td>10k</td>
<td>∼22k</td>
<td>∼12.5k</td>
<td>∼25k</td>
<td>∼62.5k</td>
<td>∼125k</td>
</tr>
<tr>
<td></td>
<td>Average Return</td>
<td>1183.3</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>2021.34</td>
<td>2285.73</td>
<td>2526.41</td>
<td>2421.067</td>
</tr>
<tr>
<td></td>
<td>Max Average Return</td>
<td>–</td>
<td>2486</td>
<td>–</td>
<td>3668.81</td>
<td>3229.14</td>
<td>3442.26</td>
<td>3456.05</td>
<td>3476.00</td>
</tr>
<tr>
<td></td>
<td>Std Return</td>
<td>150.0</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>654.37</td>
<td>757.68</td>
<td>714.07</td>
<td>796.58</td>
</tr>
</tbody>
</table>
<p>Table 1: Results and descriptions of reported values by various works using TRPO (Hopper and Half-Cheetah environments) as a baseline. "Length(iters)" denotes algorithm iterations and "Length(episodes)" denotes number of episodes.</p>
<p>We present a set of results, highlighting the difficulty in reproducing results with policy gradient methods in reinforcement learning. We show the difficulty of fine-tuning and the significant sources of variance in hyper-parameter selection for both TRPO and DDPG algorithms. Our analysis shows that these state-of-the-art on-policy and off-policy policy gradient methods often suffer from large variations as a result of different hyper-parameter settings. In addition, results across different continuous control domains are not always consistent, as shown in the Hopper and Half-Cheetah experiment results. We find that Half-Cheetah is more susceptible to performance variations from hyper-parameter tuning, while Hopper is not. We posit that this may be due to the difference in stochasticity within the environments themselves. Half-Cheetah has a much more stable dynamics</p>
<p>^{13}Further investigation needs to be done to determine the amount of trials (<em>N</em>) necessary to ensure a fair comparison (i.e. for what <em>N</em> would any <em>N</em>-sample average always result in a similarly distributed returns, unlike as has been demonstrated to be possible in Figure 10)</p>
<p>^{14}Results from original implementation evaluation on OpenAI Gym: https://gym.openai.com/evaluations/eval_W27eCzLQBy60FciaSGSJw; https://gym.openai.com/evaluations/eval_0udf6XDS2WL76S7wZicLA</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Environment</th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">rHab [9]</th>
<th style="text-align: center;">QProp [10]</th>
<th style="text-align: center;">SDQN [11]</th>
<th style="text-align: center;">Ours</th>
<th style="text-align: center;">Ours</th>
<th style="text-align: center;">Ours</th>
<th style="text-align: center;">Ours</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Half-Cheetah</td>
<td style="text-align: center;">Length (iters)</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">$1 *^{\text {th }}$ (steps)</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">2500</td>
<td style="text-align: center;">5000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Length (episodes)</td>
<td style="text-align: center;">$\sim 25$</td>
<td style="text-align: center;">30k</td>
<td style="text-align: center;">10k</td>
<td style="text-align: center;">$\sim 12.5$</td>
<td style="text-align: center;">$\sim 25$</td>
<td style="text-align: center;">$\sim 62.5$</td>
<td style="text-align: center;">$\sim 125$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average Return</td>
<td style="text-align: center;">2148.6</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">2707.1</td>
<td style="text-align: center;">3127.9</td>
<td style="text-align: center;">3547.1</td>
<td style="text-align: center;">3725.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Max Average Return</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">7490</td>
<td style="text-align: center;">6614.26</td>
<td style="text-align: center;">3788.2</td>
<td style="text-align: center;">4029.2</td>
<td style="text-align: center;">4460.7</td>
<td style="text-align: center;">4460.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Std Return</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">907.1</td>
<td style="text-align: center;">784.3</td>
<td style="text-align: center;">634.9</td>
<td style="text-align: center;">512.8</td>
</tr>
<tr>
<td style="text-align: center;">Hopper</td>
<td style="text-align: center;">Length (iters)</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">2500</td>
<td style="text-align: center;">5000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Length (episodes)</td>
<td style="text-align: center;">$\sim 25$</td>
<td style="text-align: center;">30k</td>
<td style="text-align: center;">10k</td>
<td style="text-align: center;">$\sim 12.5$</td>
<td style="text-align: center;">$\sim 25$</td>
<td style="text-align: center;">$\sim 62.5$</td>
<td style="text-align: center;">$\sim 125$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average Return</td>
<td style="text-align: center;">267.1</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">790.6</td>
<td style="text-align: center;">883.6</td>
<td style="text-align: center;">838.7</td>
<td style="text-align: center;">857.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Max Average Return</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">2604</td>
<td style="text-align: center;">3296.49</td>
<td style="text-align: center;">1642.1</td>
<td style="text-align: center;">1642.1</td>
<td style="text-align: center;">1642.1</td>
<td style="text-align: center;">1642.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Std Return</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">367.9</td>
<td style="text-align: center;">305.2</td>
<td style="text-align: center;">230.9</td>
<td style="text-align: center;">213.7</td>
</tr>
</tbody>
</table>
<p>Table 2: Results and descriptions of reported values by various works using DDPG (Hopper and Half-Cheetah environments) as a baseline. "Length(iters)" denotes algorithm iterations and "Length(episodes)" denotes number of episodes.
model, and thus is less variant in failure modes. Hopper, on the other hand, is prone to quick failure modes which introduce larger external variance, possibly making tuning difficult.
Based on our experiments, we suggest that the ML research community requires better fine-tuned implementations of these algorithms with provided hyper-parameter presets. These implementations should have benchmark results for a wide range of commonly used tasks. Our analysis shows that due to the under-reporting of hyper-parameters, different works often report different baseline results and performance measures for both TRPO and DDPG. This leads to an unfair comparison of baselines in continuous control environments. Here, we provide some insight into the impact of different hyper-parameters to aid future researchers in finding the ideal baseline configurations.
However, we also suggest that these algorithms are often susceptible to external randomness, introduced by the environment and other external hyper-parameters (e.g reward scale in DDPG) which makes it quite difficult to reproduce results with these state-of-the-art policy gradient algorithms. As such, we provide the aforementioned recommendations in reporting implementation details (provide all hyper-parameters and number of trial experiments), reporting results (report averages and standard deviations, not maximum returns), and implementing proper experimental procedures (average together many trials using different random seeds for each).</p>
<h1>Acknowledgements</h1>
<p>The authors would like to thank Joelle Pineau and David Meger for comments on the paper draft. We would like to thank the McGill University Reasoning and Learning Lab and the Mobile Robotics Lab for allowing an engaging and productive research environment. We would also like to thank Alex Lamb and Anirudh Goyal for providing initial feedback on the direction of this work, as part of the ICML Reproducibility in Machine Learning workshop. This work was supported by the AWS Cloud Credits for Research program, McGill Graduate Excellence Scholarships, and NSERC.</p>
<h2>References</h2>
<p>[1] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[2] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1889-1897, 2015.
[3] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.
[4] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 022015.</p>
<p>[5] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. CoRR, abs/1504.00702, 2015.
[6] Shixiang Gu, Timothy P. Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep qlearning with model-based acceleration. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 2829-2838, 2016.
[7] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vilamoura, Algarve, Portugal, October 7-12, 2012, pages 5026-5033, 2012.
[8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016.
[9] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In Proceedings of the 33rd International Conference on Machine Learning (ICML), 2016.
[10] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, and Sergey Levine. Q-prop: Sample-efficient policy gradient with an off-policy critic. arXiv preprint arXiv:1611.02247, 2016.
[11] Luke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson. Discrete sequential prediction of continuous actions for deep rl. arXiv preprint arXiv:1705.05035, 2017.
[12] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, Bernhard Schölkopf, and Sergey Levine. Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning. arXiv preprint arXiv:1706.00387, 2017.
[13] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pages 1109-1117, 2016.
[14] Aravind Rajeswaran, Sarvjeet Ghotra, Sergey Levine, and Balaraman Ravindran. Epopt: Learning robust neural network policies using model ensembles. arXiv preprint arXiv:1610.01283, 2016.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ For Half-Cheetah, $t=2.9109, p=0.0652$ for last 500 iterations and $t=1.9231, p=0.1978$ overall. For Hopper, $t=1.9772, p=0.1741$ for last 500 iterations and $t=-0.1255, p=0.2292$ overall.
${ }^{10}$ Hopper most significant t-test difference from default is $\mathrm{SS}=0.1$ with $t=1.0302, p=0.2929$, and for Half-Cheetah difference from default and $\mathrm{SS}=0.001 t=-3.1255, p=0.0404$&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
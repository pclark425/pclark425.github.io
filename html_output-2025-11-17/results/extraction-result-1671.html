<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1671 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1671</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1671</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-249050580</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2211.01500v1.pdf" target="_blank">Learning to Grasp the Ungraspable with Emergent Extrinsic Dexterity</a></p>
                <p><strong>Paper Abstract:</strong> A simple gripper can solve more complex manipulation tasks if it can utilize the external environment such as pushing the object against the table or a vertical wall, known as"Extrinsic Dexterity."Previous work in extrinsic dexterity usually has careful assumptions about contacts which impose restrictions on robot design, robot motions, and the variations of the physical parameters. In this work, we develop a system based on reinforcement learning (RL) to address these limitations. We study the task of"Occluded Grasping"which aims to grasp the object in configurations that are initially occluded; the robot needs to move the object into a configuration from which these grasps can be achieved. We present a system with model-free RL that successfully achieves this task using a simple gripper with extrinsic dexterity. The policy learns emergent behaviors of pushing the object against the wall to rotate and then grasp it without additional reward terms on extrinsic dexterity. We discuss important components of the system including the design of the RL problem, multi-grasp training and selection, and policy generalization with automatic curriculum. Most importantly, the policy trained in simulation is zero-shot transferred to a physical robot. It demonstrates dynamic and contact-rich motions with a simple gripper that generalizes across objects with various size, density, surface friction, and shape with a 78% success rate. Videos can be found at https://sites.google.com/view/grasp-ungraspable/.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1671.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1671.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OccludedGrasp-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Occluded Grasping Reinforcement Learning Policy with Emergent Extrinsic Dexterity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A goal-conditioned, model-free RL policy (Soft Actor-Critic with HER) trained in MuJoCo/Robosuite to enable a Franka Panda with a simple parallel gripper to use environment contacts (walls, table) to reach and execute grasps that are initially occluded.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Franka Emika Panda with parallel gripper (OccludedGrasp-RL policy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A Franka Emika Panda manipulator equipped with a simple two-finger parallel gripper running a closed-loop, goal-conditioned RL policy that outputs end-effector delta poses; uses an Operational Space Controller (OSC) as a compliant low-level controller.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / extrinsic dexterity</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo (via Robosuite)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Rigid-body physics simulation using MuJoCo through the Robosuite framework, simulating robot kinematics/dynamics, contacts, friction, object rigid-body dynamics, and controllers; rendered observations not central (policy uses object poses).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>High-fidelity physics for contact-rich rigid-body dynamics (MuJoCo with tuned solver settings)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Contact dynamics (rigid-body contact, friction), object rigid-body dynamics (size, mass/density), robot kinematics/dynamics, gripper geometry, controller-level behavior (approx.), and environment geometry (bin, walls, table).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Limited object shape variety during training (boxes only), no modeling of deformable materials or internal moving masses (e.g. loose weights), imperfect robot joint damping/friction modeling (real robot had more damping and friction), simplified/no realistic sensor noise in simulation (pose in sim is available directly), limited modeling of ICP/point-cloud observation noise, and no explicit modeling of some real-world timing/actuator nonlinearities.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical Franka Emika Panda robot with parallel gripper, object placed in a bin with walls and table; Azure Kinect camera provides point-clouds and ICP is used for object pose estimation; evaluation on 10 test object cases (boxes and several non-box shapes).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Occluded grasp execution using emergent extrinsic dexterity: manipulating (pushing/rotating) objects against a wall and table to expose and reach grasps that are initially occluded.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Model-free reinforcement learning (Soft Actor-Critic) with goal-conditioning, Hindsight Experience Replay, multi-grasp curriculum, and Automatic Domain Randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Simulation: pose-difference success (∆T < 3 cm and ∆θ < 10 deg at episode end). Real robot: successful close-and-lift of the object after policy execution.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>100% success on single-grasp default environment (pose-difference metric) before 4000 episodes (≈160k environment steps).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>With ADR: 78% success rate over evaluated real-world trials; Without ADR: 33% success rate (measured by close-and-lift across 10 test cases, 10 episodes each).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Automatic Domain Randomization (ADR) over object size (x, z), object density, table friction, gripper friction, action translation/rotation scales, initial distance to wall, table offsets, and controller parameters; ADR expanded ranges automatically based on policy performance (see Table 3 ranges and increment rules).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mismatch in robot dynamics (real joints have more damping/friction), compliant low-level controller sensitivity to real-world noise, real robot executes smaller end-effector deltas than sim, imperfect or noisy object pose estimation (ICP), limited object shape diversity in training (only boxes), unmodeled internal object contents/moving masses, and prior MuJoCo solver settings that can produce unrealistic contacts unless tuned (timestep, noslip iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Use of a compliant Operational Space Controller (OSC) to execute end-effector commands; closed-loop policy (goal-conditioned, value function available for grasp selection); Automatic Domain Randomization to broaden parameter distributions; multi-grasp curriculum to decompose goal difficulty; tuning MuJoCo solver (reduced timestep and increased noslip iterations) to improve contact realism; compensating sim2real low-level controller gap by increasing action scale and reducing policy execution rate on the real robot; ICP-based pose estimation pipeline and controller tuning on a canonical object before evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>High-fidelity modeling of contact dynamics and reasonably accurate robot/gripper geometry and dynamics are important; controller compliance (end-effector impedance) must be represented and tuned; realistic contact solver settings in simulator (timestep, noslip iterations) improve transfer; exact numeric thresholds not specified but accurate contact behavior and a reasonably-accurate robot model were emphasized as necessary for success.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>No additional policy gradient training on the robot reported; practical real-world adaptation included tuning the low-level controller parameters and action scaling (increasing action scale and reducing policy execution rate from 2Hz to 1Hz) on a default box until several consecutive successes were observed. Authors also report 'finetuning the w/ ADR policy to expand further over the range of initial object locations' (procedure details ambiguous but separate from zero-shot deployment).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Zero-shot sim-to-real transfer of a contact-rich, extrinsic-dexterity RL policy is feasible when (1) the simulator models contact dynamics reasonably well (with solver tuning), (2) training uses Automatic Domain Randomization and curricula, (3) a compliant end-effector controller (OSC) is used and matched/tuned on the real robot, and (4) closed-loop policies are employed; ADR substantially improved real success (78% vs 33% without ADR). Major sim-to-real gaps arose from robot model mismatches (joint damping/friction), controller execution differences, pose-estimation noise, and limited training shape diversity, which constrain generalization to out-of-distribution object dynamics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1671.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1671.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Domain Randomization (training technique)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that progressively expands the ranges of randomized environment parameters during training based on policy performance, used here to improve robustness and sim-to-real transfer of contact-rich manipulation policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Automatic Domain Randomization (training curriculum for environment parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A curriculum-based domain randomization procedure that begins with small parameter variation and automatically expands parameter ranges (e.g., object size, friction, density, controller/action scales) when the policy succeeds at boundary cases, to generate robust policies for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics / sim-to-real training methods</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo (via Robosuite) with ADR applied</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Same MuJoCo environment as above; ADR samples environment parameters from uniform distributions whose bounds are expanded according to performance thresholds to expose the agent to progressively larger variations (object sizes, friction coefficients, densities, action scales, initial positions, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Sim fidelity unchanged; ADR increases variation breadth rather than increasing single-instance fidelity (robustification technique).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Parameter variability in contact/friction coefficients, object mass/density, controller/action scales, and initial conditions to cover distributional shifts between sim and real.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>ADR does not add fidelity to unmodeled phenomena (e.g., internal moving masses, sensor-specific noise models, or accurate robot joint damping mismatches) unless those parameters are explicitly randomized; it cannot fully compensate for structural model mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Applied to training used for zero-shot deployment on a Franka Panda robot with ICP-based pose estimation and a compliant OSC controller; ADR-generated policies were evaluated on multiple real objects with varied shapes and properties.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Robust contact-rich extrinsic manipulation behaviors (occluded grasping strategies) from sim to real.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Domain randomization curriculum (ADR) integrated into model-free RL training (SAC + HER).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Real-world grasp-and-lift success rate (close-and-lift outcome); compared with policies trained without ADR.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Policies trained with ADR achieved 78% success on real-world trials; policies trained without ADR achieved 33% success.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Explicit randomized parameters included object size x/z, object density, table friction, gripper friction, action translation/rotation scales, initial distance to wall, table offsets; initial bounds and expansion increments are defined (see paper Table 3). ADR expanded parameter ranges automatically based on boundary-case performance thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>ADR reduces sensitivity to many parameter mismatches but cannot fully close gaps from structural mismatches such as unmodeled dynamics (e.g., internal loose masses), sensor estimator biases, or discrepancies in robot joint damping/actuator response.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Performance-based automatic expansion of randomization ranges (curriculum), integration with closed-loop policies and compliant controller execution, and inclusion of controller/action-scale randomization to cover low-level control disparities.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>ADR helps when variation ranges for critical contact and actuator parameters are included; however, some fidelity (accurate contact solver behavior and reasonable robot model) is still required — ADR is not a substitute for grossly inaccurate physics.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automatic Domain Randomization substantially improved zero-shot sim-to-real transfer for contact-rich extrinsic manipulation: ADR-trained policies had much higher real-world success (78%) than non-ADR policies (33%). ADR's progressive expansion of parameter ranges and inclusion of controller/action-scale randomization were key to robustness; ADR cannot fully remedy structural simulator-to-real mismatches (e.g., inaccurate robot damping or unmodeled internal object dynamics).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automatic Domain Randomization <em>(Rating: 2)</em></li>
                <li>Solving Rubik's Cube with a Robot Hand <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Variable impedance control in end-effector space: An action space for reinforcement learning in contact-rich tasks <em>(Rating: 2)</em></li>
                <li>A unified approach for motion and force control of robot manipulators: The operational space formulation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1671",
    "paper_id": "paper-249050580",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "OccludedGrasp-RL",
            "name_full": "Occluded Grasping Reinforcement Learning Policy with Emergent Extrinsic Dexterity",
            "brief_description": "A goal-conditioned, model-free RL policy (Soft Actor-Critic with HER) trained in MuJoCo/Robosuite to enable a Franka Panda with a simple parallel gripper to use environment contacts (walls, table) to reach and execute grasps that are initially occluded.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Franka Emika Panda with parallel gripper (OccludedGrasp-RL policy)",
            "agent_system_description": "A Franka Emika Panda manipulator equipped with a simple two-finger parallel gripper running a closed-loop, goal-conditioned RL policy that outputs end-effector delta poses; uses an Operational Space Controller (OSC) as a compliant low-level controller.",
            "domain": "general robotics manipulation / extrinsic dexterity",
            "virtual_environment_name": "MuJoCo (via Robosuite)",
            "virtual_environment_description": "Rigid-body physics simulation using MuJoCo through the Robosuite framework, simulating robot kinematics/dynamics, contacts, friction, object rigid-body dynamics, and controllers; rendered observations not central (policy uses object poses).",
            "simulation_fidelity_level": "High-fidelity physics for contact-rich rigid-body dynamics (MuJoCo with tuned solver settings)",
            "fidelity_aspects_modeled": "Contact dynamics (rigid-body contact, friction), object rigid-body dynamics (size, mass/density), robot kinematics/dynamics, gripper geometry, controller-level behavior (approx.), and environment geometry (bin, walls, table).",
            "fidelity_aspects_simplified": "Limited object shape variety during training (boxes only), no modeling of deformable materials or internal moving masses (e.g. loose weights), imperfect robot joint damping/friction modeling (real robot had more damping and friction), simplified/no realistic sensor noise in simulation (pose in sim is available directly), limited modeling of ICP/point-cloud observation noise, and no explicit modeling of some real-world timing/actuator nonlinearities.",
            "real_environment_description": "Physical Franka Emika Panda robot with parallel gripper, object placed in a bin with walls and table; Azure Kinect camera provides point-clouds and ICP is used for object pose estimation; evaluation on 10 test object cases (boxes and several non-box shapes).",
            "task_or_skill_transferred": "Occluded grasp execution using emergent extrinsic dexterity: manipulating (pushing/rotating) objects against a wall and table to expose and reach grasps that are initially occluded.",
            "training_method": "Model-free reinforcement learning (Soft Actor-Critic) with goal-conditioning, Hindsight Experience Replay, multi-grasp curriculum, and Automatic Domain Randomization.",
            "transfer_success_metric": "Simulation: pose-difference success (∆T &lt; 3 cm and ∆θ &lt; 10 deg at episode end). Real robot: successful close-and-lift of the object after policy execution.",
            "transfer_performance_sim": "100% success on single-grasp default environment (pose-difference metric) before 4000 episodes (≈160k environment steps).",
            "transfer_performance_real": "With ADR: 78% success rate over evaluated real-world trials; Without ADR: 33% success rate (measured by close-and-lift across 10 test cases, 10 episodes each).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Automatic Domain Randomization (ADR) over object size (x, z), object density, table friction, gripper friction, action translation/rotation scales, initial distance to wall, table offsets, and controller parameters; ADR expanded ranges automatically based on policy performance (see Table 3 ranges and increment rules).",
            "sim_to_real_gap_factors": "Mismatch in robot dynamics (real joints have more damping/friction), compliant low-level controller sensitivity to real-world noise, real robot executes smaller end-effector deltas than sim, imperfect or noisy object pose estimation (ICP), limited object shape diversity in training (only boxes), unmodeled internal object contents/moving masses, and prior MuJoCo solver settings that can produce unrealistic contacts unless tuned (timestep, noslip iterations).",
            "transfer_enabling_conditions": "Use of a compliant Operational Space Controller (OSC) to execute end-effector commands; closed-loop policy (goal-conditioned, value function available for grasp selection); Automatic Domain Randomization to broaden parameter distributions; multi-grasp curriculum to decompose goal difficulty; tuning MuJoCo solver (reduced timestep and increased noslip iterations) to improve contact realism; compensating sim2real low-level controller gap by increasing action scale and reducing policy execution rate on the real robot; ICP-based pose estimation pipeline and controller tuning on a canonical object before evaluation.",
            "fidelity_requirements_identified": "High-fidelity modeling of contact dynamics and reasonably accurate robot/gripper geometry and dynamics are important; controller compliance (end-effector impedance) must be represented and tuned; realistic contact solver settings in simulator (timestep, noslip iterations) improve transfer; exact numeric thresholds not specified but accurate contact behavior and a reasonably-accurate robot model were emphasized as necessary for success.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "No additional policy gradient training on the robot reported; practical real-world adaptation included tuning the low-level controller parameters and action scaling (increasing action scale and reducing policy execution rate from 2Hz to 1Hz) on a default box until several consecutive successes were observed. Authors also report 'finetuning the w/ ADR policy to expand further over the range of initial object locations' (procedure details ambiguous but separate from zero-shot deployment).",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Zero-shot sim-to-real transfer of a contact-rich, extrinsic-dexterity RL policy is feasible when (1) the simulator models contact dynamics reasonably well (with solver tuning), (2) training uses Automatic Domain Randomization and curricula, (3) a compliant end-effector controller (OSC) is used and matched/tuned on the real robot, and (4) closed-loop policies are employed; ADR substantially improved real success (78% vs 33% without ADR). Major sim-to-real gaps arose from robot model mismatches (joint damping/friction), controller execution differences, pose-estimation noise, and limited training shape diversity, which constrain generalization to out-of-distribution object dynamics.",
            "uuid": "e1671.0"
        },
        {
            "name_short": "ADR",
            "name_full": "Automatic Domain Randomization (training technique)",
            "brief_description": "A method that progressively expands the ranges of randomized environment parameters during training based on policy performance, used here to improve robustness and sim-to-real transfer of contact-rich manipulation policies.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_system_name": "Automatic Domain Randomization (training curriculum for environment parameters)",
            "agent_system_description": "A curriculum-based domain randomization procedure that begins with small parameter variation and automatically expands parameter ranges (e.g., object size, friction, density, controller/action scales) when the policy succeeds at boundary cases, to generate robust policies for transfer.",
            "domain": "robotics / sim-to-real training methods",
            "virtual_environment_name": "MuJoCo (via Robosuite) with ADR applied",
            "virtual_environment_description": "Same MuJoCo environment as above; ADR samples environment parameters from uniform distributions whose bounds are expanded according to performance thresholds to expose the agent to progressively larger variations (object sizes, friction coefficients, densities, action scales, initial positions, etc.).",
            "simulation_fidelity_level": "Sim fidelity unchanged; ADR increases variation breadth rather than increasing single-instance fidelity (robustification technique).",
            "fidelity_aspects_modeled": "Parameter variability in contact/friction coefficients, object mass/density, controller/action scales, and initial conditions to cover distributional shifts between sim and real.",
            "fidelity_aspects_simplified": "ADR does not add fidelity to unmodeled phenomena (e.g., internal moving masses, sensor-specific noise models, or accurate robot joint damping mismatches) unless those parameters are explicitly randomized; it cannot fully compensate for structural model mismatch.",
            "real_environment_description": "Applied to training used for zero-shot deployment on a Franka Panda robot with ICP-based pose estimation and a compliant OSC controller; ADR-generated policies were evaluated on multiple real objects with varied shapes and properties.",
            "task_or_skill_transferred": "Robust contact-rich extrinsic manipulation behaviors (occluded grasping strategies) from sim to real.",
            "training_method": "Domain randomization curriculum (ADR) integrated into model-free RL training (SAC + HER).",
            "transfer_success_metric": "Real-world grasp-and-lift success rate (close-and-lift outcome); compared with policies trained without ADR.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Policies trained with ADR achieved 78% success on real-world trials; policies trained without ADR achieved 33% success.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Explicit randomized parameters included object size x/z, object density, table friction, gripper friction, action translation/rotation scales, initial distance to wall, table offsets; initial bounds and expansion increments are defined (see paper Table 3). ADR expanded parameter ranges automatically based on boundary-case performance thresholds.",
            "sim_to_real_gap_factors": "ADR reduces sensitivity to many parameter mismatches but cannot fully close gaps from structural mismatches such as unmodeled dynamics (e.g., internal loose masses), sensor estimator biases, or discrepancies in robot joint damping/actuator response.",
            "transfer_enabling_conditions": "Performance-based automatic expansion of randomization ranges (curriculum), integration with closed-loop policies and compliant controller execution, and inclusion of controller/action-scale randomization to cover low-level control disparities.",
            "fidelity_requirements_identified": "ADR helps when variation ranges for critical contact and actuator parameters are included; however, some fidelity (accurate contact solver behavior and reasonable robot model) is still required — ADR is not a substitute for grossly inaccurate physics.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Automatic Domain Randomization substantially improved zero-shot sim-to-real transfer for contact-rich extrinsic manipulation: ADR-trained policies had much higher real-world success (78%) than non-ADR policies (33%). ADR's progressive expansion of parameter ranges and inclusion of controller/action-scale randomization were key to robustness; ADR cannot fully remedy structural simulator-to-real mismatches (e.g., inaccurate robot damping or unmodeled internal object dynamics).",
            "uuid": "e1671.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automatic Domain Randomization",
            "rating": 2,
            "sanitized_title": "automatic_domain_randomization"
        },
        {
            "paper_title": "Solving Rubik's Cube with a Robot Hand",
            "rating": 2,
            "sanitized_title": "solving_rubiks_cube_with_a_robot_hand"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Variable impedance control in end-effector space: An action space for reinforcement learning in contact-rich tasks",
            "rating": 2,
            "sanitized_title": "variable_impedance_control_in_endeffector_space_an_action_space_for_reinforcement_learning_in_contactrich_tasks"
        },
        {
            "paper_title": "A unified approach for motion and force control of robot manipulators: The operational space formulation",
            "rating": 1,
            "sanitized_title": "a_unified_approach_for_motion_and_force_control_of_robot_manipulators_the_operational_space_formulation"
        }
    ],
    "cost": 0.0140805,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning to Grasp the Ungraspable with Emergent Extrinsic Dexterity</p>
<p>Wenxuan Zhou wenxuanz@andrew.cmu.edu 
Robotics Institute
Robotics Institute Carnegie Mellon University
Carnegie Mellon University</p>
<p>David Held dheld@andrew.cmu.edu 
Robotics Institute
Robotics Institute Carnegie Mellon University
Carnegie Mellon University</p>
<p>Learning to Grasp the Ungraspable with Emergent Extrinsic Dexterity
ManipulationReinforcement LearningExtrinsic Dexterity
A simple gripper can solve more complex manipulation tasks if it can utilize the external environment such as pushing the object against the table or a vertical wall, known as "Extrinsic Dexterity." Previous work in extrinsic dexterity usually has careful assumptions about contacts which impose restrictions on robot design, robot motions, and the variations of the physical parameters. In this work, we develop a system based on reinforcement learning (RL) to address these limitations. We study the task of "Occluded Grasping" which aims to grasp the object in configurations that are initially occluded; the robot needs to move the object into a configuration from which these grasps can be achieved. We present a system with model-free RL that successfully achieves this task using a simple gripper with extrinsic dexterity. The policy learns emergent behaviors of pushing the object against the wall to rotate and then grasp it without additional reward terms on extrinsic dexterity. We discuss important components of the system including the design of the RL problem, multi-grasp training and selection, and policy generalization with automatic curriculum. Most importantly, the policy trained in simulation is zero-shot transferred to a physical robot. It demonstrates dynamic and contact-rich motions with a simple gripper that generalizes across objects with various size, density, surface friction, and shape with a 78% success rate. Videos can be found at https://sites.google.com/view/grasp-ungraspable.</p>
<p>Introduction</p>
<p>Humans have dexterous multi-fingered hands; however, similarly dexterous robot hands are expensive and fragile. Instead, a simple hand can achieve dexterous manipulation by leveraging the environment, known as "Extrinsic Dexterity" [1]. For example, a simple gripper can rotate an object in-hand by pushing it against the table [2], or lifting an object by sliding it along a vertical surface [3]. With the exploitation of external resources such as contact surfaces or gravity, even simple grippers can perform skillful maneuvers that are typically studied with a multi-fingered dexterous hand. Different from a common practice of considering the robot and an object of interest in isolation, extrinsic dexterity focuses on a holistic view of considering the interactions among the robot, the object, and the external environment.</p>
<p>Previous work in extrinsic dexterity has demonstrated a variety of tasks such as in-hand reorientation with a simple gripper, prehensile pushing, or shared grasping [1,2,3]. However, the underlying approaches come with several limitations such as relying on hand-designed motions or primitives with limited capability of generalizing across different objects, making assumptions about contact locations and contact modes, or requiring specific gripper design [1,2,3,4,5,6,7]. Instead, we build a system with reinforcement learning (RL) to remove these limitations. With RL, the agent can learn a closed-loop policy of how the robot should interact with the object and the environment to solve the task, taking into account both planning and control. In addition, when trained with domain randomization, the policy can learn to be robust to different variations of physics. These properties of RL can enable extrinsic dexterity in a more general setting.</p>
<p>In this work, we study "Occluded Grasping" as an example of a task that requires extrinsic dexterity. The goal of this task is to grasp an object in poses that are initially occluded. Consider, for example, a robot that needs to grasp a cereal box lying on its side on a table; the desired grasp is not reachable Figure 1: We present a system for the "Occluded Grasping" task with extrinsic dexterity. The goal of this task is to reach an occluded grasp configuration (indicated by a transparent gripper). The figure shows an example of the emergent behavior of the policy and successful sim2real transfer.</p>
<p>because it is partially occluded by the table (Figure 1). To achieve this grasp with a parallel gripper, the robot might rotate the object by pushing it against a vertical wall to expose the desired grasp and then reach it. This task is in contrast with common grasping tasks which focus on reaching an unoccluded grasp in free space with a static or near-static scene [8,9,10].</p>
<p>The goal of this work is to build a system for the "Occluded Grasping" task as an example of the combination of RL and extrinsic dexterity that works on a physical robot. We investigate design choices of such a system and emphasize the simplicity of the method. With model-free RL, we design a reward function that optimizes pre-grasp and grasping motion without the separation of stages as previous work in pre-grasp [11,12,13]. By placing the object in the bin and using a compliant low-level controller, the agent shows emergent extrinsic dexterity behavior without additional reward terms. We also incorporate a set of desired grasps with a training curriculum and a grasp selection procedure during evaluation. We improve the policy with Automatic Domain Randomization [14] over physical parameters which robustify the contact-rich behaviors across noise and environment variations. In the experiments, we provide a comprehensive evaluation of the system in simulation to analyze the importance of each component. The policy is zero-shot transferred to the physical robot and successfully executes similar behaviors to complete the task. The policy achieves a success rate of 78% and shows generalization across various out-of-distribution objects. Our main contribution is the real robot experiment results. Existing work with a simple hand has not shown such behaviors on the real robot with a similar level of complexity in contact events and generalization across objects at the same time.</p>
<p>2 Related Work 2.1 Extrinsic dexterity "Extrinsic dexterity" is a type of manipulation skill that enhances the intrinsic capability of a robot hand using external resources including external contacts, gravity, or dynamic motions of the arm [1]. Previous work in extrinsic dexterity has demonstrated complex tasks with a simple gripper including in-hand reorientation [1,4], prehensile pushing [2,5], shared grasping [3], and more. Their methods are based on hand-crafted trajectories [1], task-specific motion primitives [3,4,15], or planning over contact modes [2,5,6,7] to simplify the problem. They relies on careful assumptions on contacts such as assuming a fixed number of sticking contacts between the fingertips and the object. In this work, we take an alternative approach to use RL to learn a closed-loop policy that considers both planning and control without limitations on contact events. The resulting policy shows more versatile contact switches beyond prior work and can be transferred to a physical robot.</p>
<p>Grasping</p>
<p>Grasping is an important task in robot manipulation and has been studied from various aspects.</p>
<p>Grasp generation: One area of study in grasping is to generate stable grasps using analytical approaches [16,17] or learning approaches [8,9,18,19,20]. In our paper, we assume that the desired grasp configurations are given which may come from any grasp generation method.</p>
<p>Grasp execution: To execute a grasp following grasp generation, a motion planner is usually used to generate a collision-free path towards the desired grasp configuration [10,21,22,23]. All of these works aim at achieving the unoccluded grasp configurations in static or near-static scenes. Instead, our work focuses on a complementary direction of achieving occluded grasp locations by interacting with the object of interest. Another line of work in grasping uses an end-to-end pipeline without the separation of grasp generation and grasp execution [24,25]. However, they do not demonstrate performing the occluded grasps studied in this work.</p>
<p>Pre-Grasp manipulation: To deal with occluded grasps, prior work has studied pre-grasps as a preparatory stage of the grasping task. Typical motions for pre-grasps include rotation through planar pushing [12], sliding the object to the edge of the table [13,26], or rotate the object against the wall [11]. Sun et al. [11] is the most related to our work, but they use a specially designed endeffector to perform the pre-grasp motion and then use a second gripper to grasp. We demonstrate that the full grasping task can be solved with a single gripper without special requirements on the end-effectors. These previous work typically separates pre-grasp motion and grasp execution into two stages and impose restrictions on the transitions of the stages. Instead, we co-optimize pre-grasp and grasp execution throughout the episode without explicit separation of the stages.</p>
<p>Reinforcement Learning for Manipulation</p>
<p>Previous work in RL for manipulation usually treats the object and the robot in isolation from the environment without considering extrinsic dexterity. RL has been applied to dexterous manipulation with a multi-fingered hand and shows contact-rich behaviors [14,27,28]. In contrast, with a parallel gripper, prior work focuses on tasks with limited contacts and object motions without utilizing the environment [29,30]. This is the first work that demonstrates extrinsic dexterity with a simple parallel gripper using RL. The goal of a common grasp execution task is to move the end-effector E close to a given desired grasp pose g. The desired grasp might come from any grasp generation method [8,9,20] as the input to the grasp execution task. As shown in Figure 2, we define an "Occluded Grasping" task to be a subset of the grasp execution tasks where the input desired grasp g is initially occluded. To clarify, the term "occluded" in this work is more than visual occlusion. It means the desired grasp intersects with the table and moving the gripper in free space cannot solve this task. The robot has to interact with the object to make the grasp pose reachable. The grasp O g is defined in the object frame O and moves with the object. Formally, the grasp execution is defined to be successful if the position difference ∆T (g, E) and the orientation difference ∆θ(g, E) are less than the pre-defined thresholds ϵ T and ϵ θ respectively at the end of an episode. After successfully reaching a desired grasp pose, the gripper will be closed to complete the grasp. In addition, when the input to the system is a set of grasps G = {g i } k i=1 instead of a single grasp, the agent may select any of the grasp to approach to.</p>
<p>Task Definition: Occluded Grasping</p>
<p>Learning dexterous grasping with Reinforcement Learning</p>
<p>We build a system that learns a closed-loop policy for the occluded grasping task defined above with model-free RL. In this section, we will discuss important design choices of the system including the design of the RL problem, the extrinsic environment, and the choice of low-level controller. Then we will discuss how to deal with a set of grasps by training with a grasp curriculum and selecting the best grasp during evaluation. We also include Automatic Domain Randomization [14] to improving the generalization of the policy across environment variations.</p>
<p>Preliminaries: Goal-conditioned Reinforcement Learning</p>
<p>We define a Markov Decision Process (MDP) with states s t ∈ S, actions a t ∈ A, reward function r : S × A → R, and discount factor γ. The state space, action space and the reward function for our task will be discussed in detail in the next section. The goal is to find a policy π(a t |s t ) that maximizes the return R t = ∑︁ ∞ k=t γ k−t r(s k , a k ). A Q-function is defined to be the expected return Object pose B End-effector pose in the object frame 3 of the policy Q π (s, a) = E π [R t |s t , a t ]. In goal-conditioned RL, we define a set of goals η ∈ G correspond to the reward function r η (s t , a t ) [31]. To train a policy with a set of goals, both the policy and the Q-function will now take the goal η as input, given by π(a t |s t , η) and Q π (s t , a t , η). In the occluded grasping task, we use the desired grasps as goals.</p>
<p>RL Problem Design</p>
<p>Observation and Action Space: The observation that is input to the policy includes a target grasp configuration in the object frame O g, the pose of the end-effector in the world frame W E and the object pose in the world frame W O. Note that the policy only takes one grasp O g as input but we will discuss how to deal with a set of grasps in Section 4.5. For real robot experiments, we use Iterative Closest Point (ICP) for pose estimation of the object which matches a template point cloud of the object to the current point cloud [32]. The action space of the policy is the delta pose of the end-effector ∆E in its local frame which is passed into a low-level controller (Section 4.4). An outline of the policy execution is shown in Figure 3. More details can be found in Appendix B.</p>
<p>Reward: We design the reward function to optimize the pre-grasp motion and grasp execution without separating them into two stages as in previous work [11,13,12,26]:
r = αD(g, E) + β ∑︂ i P (m i ) (1a) D(g, E) = α 1 ∆T (g, E) + α 2 ∆θ(g, E) (1b)
where α 1 , α 2 and β are the weights for the reward terms. The first term of Equation 1a, D(g, O E), is the pose difference between the target grasp and the current end-effector pose, which is to optimize for reaching the grasp. This term is expanded in Equation 1b to include the translational and rotational distance, as described in Section 3. The second term of Equation 1a is the target grasp occlusion penalty which is to penalize the agent if the target grasp configuration is in collision with the table. This corresponds to a pre-grasp objective. To measure how much the target grasp is occluded by the table, we set six marker points on the target gripper ( Figure 2) denoted as m i and compare the height of the markers with the table top. If a marker is below the table top, the height difference will be used as the penalty. Including this occlusion penalty can effectively reduce the local optima where the gripper will reach close to the target grasp (which is occluded) without trying to move the object. Note that we did not impose any reward terms that are explicitly related to extrinsic dexterity. In our system, the use of extrinsic dexterity is an emergent behavior of policy optimization given our objective and environmental setup.</p>
<p>Extrinsic Environment</p>
<p>To exploit the benefits of extrinsic dexterity from object-scene interaction in this task, we construct the scene as having an object in a bin, instead of leaving the object on the table as shown in Figure 2.</p>
<p>We also make the workspace of the robot large enough such that the robot can move the object to make contacts with the walls (during which the robot itself may also make contact with the wall).</p>
<p>In the experiments, we will show that the policy will learn to utilize the wall to rotate the object. Without the wall, it is not able to find a strategy that can perform the task with the parallel gripper.</p>
<p>Choice of Low-level Controller</p>
<p>The choice of low-level controller is important for this task due to the fact that we expect the agent to use extrinsic dexterity which involves rich contacts among the gripper, the object and the bin. We choose Operation Space Control (OSC) as the lower-level controller to execute the policy output which operates at a higher frequency (100Hz) than the RL policy (2Hz) [33] (Figure 3). Given a desired pose of the end-effector, OSC first calculates the corresponding force and torque at the endeffector to minimize the pose error according to a PD controller with gain K p and K d . Then, the desired force and torque of the end-effector will be converted into desired joint torques according to the model of the robot. We choose relatively low gains so that the controller becomes compliant in the end-effector space. There are two benefits of a compliant OSC in such a contact-rich manipulation task with extrinsic dexterity. First, being compliant in end-effector space allows safe execution of the motions without smashing the gripper on the objects or the bin. Limiting the delta pose and selecting proper gains K p , K d will limit the final force and torque output of the end-effector. If we use a controller that is compliant in the joint configuration space instead, we will not have direct control over the maximum force the end-effector might have on the object and the bin. Second, as shown in Martín-Martín et al. [34], using OSC as the low-level controller might speed up RL training and improve sim2real transfer for contact-rich manipulation.</p>
<p>Multi-grasp Training and Grasp Selection</p>
<p>In this section, we consider the scenario in which a set of desired grasp configurations are given instead of just a single one. During training, given a set of grasps G train , we aim at covering as many grasp configurations as possible. As we will show in the experiments, reaching different grasps might require a significantly different behavior. Learning directly over a diverse set of goals might create difficulties for policy learning [35,36]. We use an automatic curriculum following OpenAI et al. [14] to gradually expand the set of grasps to be trained with. We start the training with just a single fixed grasp; after the policy has achieved a success rate larger than a threshold, it will be trained on a slightly larger set with grasps close to the initial grasp location.</p>
<p>During testing, if a set of grasps G test is provided, we can select the best grasp within the set to improve the performance of the grasping task, following previous work in integrated grasp and motion planning [10,21,22]. With value-based model-free RL algorithms such as Soft Actor Critic [37], the policy is trained together with a Q-function (defined in Section 4.1). We propose to select the grasp that maximizes the learned Q-function for the given observation and action: g * = arg max g∼Gtest Q(s t , a t , g). The selection can be performed for each timestep t, or at the beginning of an episode when t = t 0 . We include both implementations in the experiments. This can select the grasp that is most easily reached which depends both on the environmental configuration as well as how well the policy has learned to achieve different grasp configurations.</p>
<p>Improving Policy Generalization</p>
<p>To improve generalization across environment variations, we train the policy with Automatic Domain Randomization (ADR) [14]. Similar to the multi-grasp curriculum, the policy is first trained on a single environment with a single object, and gradually expands the range of randomization automatically according to its performance. This significantly reduces the effort of tuning the range of domain randomization. We randomize over different variations of the environment properties such as object size, density, and friction coefficient. We also randomize the parameters of the controller to improve sim2real transfer. More descriptions on the ADR procedure can be found in Appendix C.</p>
<p>Experiments</p>
<p>We build the simulation environment for this task using Robosuite [38] and the MuJoCo simulator [39] as shown in Figures 1 and 2. The environment contains a Franka Emika Panda robot with a parallel gripper and an object in the bin in front of it. We focus on grasping large flat objects from the side since they cannot be grasped with a top-down motion. The policy is trained in simulation with Soft Actor Critic [37]. In this section, we include the results in simulation to discuss each component of the proposed system. We then evaluate zero-shot sim2real transfer on a physical Panda robot across different objects. Implementation details can be found in Appendix B. We first evaluate our method by training the policies with a single desired grasp in the default environment without ADR. Figure 4a shows the training curve of the proposed method and the ablations. The policy trained with the complete system can reach a success rate of 100% before 4000 episodes which corresponds to 160000 environment steps. To evalu-(a) Local optima: An example of local optima where the gripper uses the bottom finger to lift the object (instead of the top) and then fails to move the object between its two fingers to prepare for the grasp.</p>
<p>Training Curves and Ablations</p>
<p>(b) Standing object: One of the successful strategies is to flip the object until it stands on the side and then reach the grasp.</p>
<p>(c) MultiGrasp-Front: When the desired grasp is at the corner, the policy flips the object by pushing it on the side and then move close to the grasp.</p>
<p>(d) MultiGrasp-Side: When the grasp is on the side, the policy can use another side of the wall to rotate the object and reach the desired grasp. ate the importance of extrinsic dexterity, we remove the walls of the bin. The resulting policies have 0% success rate and push the object outside of the table. For ablations on the reward function, we remove the occlusion penalty (the second term of Equation 1a) and also try a sparse reward. Without the occlusion penalty, the policy is more likely to get stuck at a local optima (an example shown in Figure 5a) and thus the success rate becomes lower. With the alternative of a {−1, 0} sparse reward, the policy learns much slower. We also experiment with different low-level controllers. Both joint torque and joint position control lead to worse performance which indicates the importance of using end-effector coordinates. With a less compliant controller by increasing the gains of OSC, the success rate becomes lower which demonstrates the importance of compliance for contact-rich tasks in addition to the safety considerations. Figure 1 shows a typical strategy of a successful policy which involves multiple stages of contact switches. The gripper first moves close to the object and makes contact on the side of the object with the top finger. It then pushes the object against the wall to rotate it. During this stage, the gripper usually maintains a fixed or rolling contact with the object, but sliding also occurs. The object might have sliding or sticking contacts with the wall and the ground. After the gripper has rotated a bit further and the bottom fingertip is below the object, the gripper will let the object drop on the bottom finger. After that, the gripper will try to match the desired pose more precisely. At this point, the policy has executed the grasp successfully and it is ready to close the gripper. This type of learned contact-rich behaviors with a simple gripper has not been shown in previous work. In Section 5.5, we will further demonstrate that it can be transferred to a physical robot.</p>
<p>Emergent Behaviors</p>
<p>One of the key decisions in this strategy is to use the top finger to rotate the object instead of the bottom finger. One might suppose an alternative approach which is to use the bottom finger to scoop the object against the wall and then directly roll the finger underneath the object to reach the grasp. However, this strategy is not physically feasible on the parallel gripper due to the limited degree of freedom of the finger. We observe that the policies that follow this strategy during exploration usually get stuck at a local optima without successfully reaching the grasp (Figure 5a). Another type of successful strategy is to flip the object to stand on its side and then move to the grasp (Figure 5b). This strategy relies on the fact that the object remains stable after the rotation. We will show in the real-robot experiments that for a non-box object, the object may lie on the wall to maintain stability.</p>
<p>Multi-grasp Experiments</p>
<p>Multi-grasp Training: Going beyond a single desired grasp, we generate the grasp configurations around the side of the object and parameterized the grasps into a continuous grasp ID in the range of [0, 4] ( Figure 6). We train two types of multi-grasp policies with curriculum: MultiGrasp-Front which starts from grasp ID=1.5 and MultiGrasp-Side which starts from grasp ID=2.5. As a baseline, 1.00 ± 0.00 1.00 ± 0.00 ArgmaxQ-t 0 1.00 ± 0.00 1.00 ± 0.00 PoseDiff 1.00 ± 0.00 0.96 ± 0.08 PoseDiff-t 0 1.00 ± 0.00 0.50 ± 0.43 Uniform 0.54 ± 0.16 0.90 ± 0.06 Table 1: Comparison of grasp selection methods: Side grasp policies achieve better performance when using the Qfunction to select the grasp.</p>
<p>we train a policy by uniformly sampling from the entire set of grasps without curriculum (No curriculum). Figure 6 shows the performance of these policies evaluated across all grasp IDs. Without curriculum, the agent has difficulties in reaching any of the grasps. With the automatic curriculum, both MultiGrasp-Front and MultiGrasp-Side expand from a single grasp to most of the grasps on one side of the object. Figures 13a and 13b include qualitative examples of the behaviors which shows that it may require a completely different behavior for different grasps.</p>
<p>Grasp Selection: We compare grasp selection methods with MultiGrasp-Front and MultiGrasp-Side. We sample 50 grasps from the training range of the policy at the beginning of each episode. The grasp selection methods will choose a grasp from this set as the input to the policy. We evaluate the following grasp selection options: ArgmaxQ selects the grasp that corresponds to the highest Q-value. PoseDiff selects the grasp according to the closest distance to the current gripper pose according to Equation 1b (with the same weights as the reward function). Both ArgmaxQ and ArgmaxQ select a grasp for each timestep. Alternatively, ArgmaxQ-t 0 and PoseDiff-t 0 only selects a grasp during the first timestep of the episode. Uniform samples a grasp from the set uniformly. The results are summarized in Table 1. For MultiGrasp-Side, using the Q-function for grasp selection is better than the other approaches. Since the policy has a more complicated maneuver to reach the side (Figure 13b), the Q-function can capture the difficulty of the goal better than pose difference. The open loop trajectories are obtained by rolling out the Fixed Env policies in the default environment. We sample 100 environments from the range covered by the ADR policies (Appendix C) and plot the percentage of environments that are above a certain performance metric (Figure 7). The closed-loop policies are much better than open-loop trajectories. With ADR, the generalization can be improved even further. Sensitivity analysis on single physical parameters can be found in Appendix A.</p>
<p>Policy Generalization</p>
<p>Real-robot experiments</p>
<p>We execute the single grasp policies on the real robot with zero-shot sim2real transfer over 10 test cases with different dimensions, densities, surface frictions, and sizes as shown in Figure 8. For non-box objects, the poses are defined with respect to the bounding boxes. The bounding boxes are obtained by running Principle Component Analysis (PCA) on the scanned object point cloud. More details of the real robot experiments can be found in Appendix D. Note that most of the objects are out-of-distribution. We evaluate 10 episodes for each test case and summarize the results in Figure 8. The success is measured by being able to close the gripper and lift the object at the end of the episode. We first compare the policies with and without Automatic Domain Randomization, denoted as w ADR and w/o ADR respectively. Quantitatively, the policy with ADR achieves a success rate of 78% while the policy without ADR achieves 33%. Interestingly, the policy with ADR achieves 24/30 successes over the bottle, the Cool Whip container, and the container with a reversed initial pose. This demonstrates that although the policy is only trained with boxes in simulation, it can also generalize to other shapes to some extent when we represent the object with its bounding box. However, when the object with an out-of-distribution shape has a very different transition dynamics, the policy could fail. Qualitatively, both policies being evaluated exhibit similar strategies as discussed in Section 5.2. In fact, a single policy network may execute either the dropping strategy ( Figure 1) or the standing strategy ( Figure 5b) depending on the current state. We also include additional results when the initial object location is not close to the wall, denoted as Init-pose in Figure 8. We finetune the w/ ADR policy to expand further over the range of initial object locations. The success rate remains similar for most objects, but this setting becomes more challenging for nonbox shapes. Videos of the full real robot evaluation including failure cases and recovery behaviors can be found on the website 1 . These real robot results are valuable to the field of manipulation because it is beyond what has been shown with a simple hand considering the combined complexity of contact events, object motion and object generalization.</p>
<p>Limitations</p>
<p>One limitation of this work is that the policy is trained with box-shape objects. Although it may generalize to other shapes to some extent as shown in the experiments, the policy might be improved by including other shapes during training. In addition, the pose of the object alone may not be sufficient to generalize to novel objects; using a better representation of the shape such as a point cloud or key-points could improve generalization across shapes. However, these changes would also increase the training complexity. Another limitation is that we assume a reasonably accurate robot and gripper model, in terms of geometries, kinematic and dynamic parameters. It would be interesting to explore how to extend the method to transfer across robots and grippers.</p>
<p>Conclusion and Takeaways</p>
<p>In this work, we study the "Occluded Grasping" task where the robot with a parallel gripper aims to reach a grasp configuration using extrinsic dexterity. We present a system that learns a closed-loop policy for this task with reinforcement learning. In the experiments, we demonstrate the importance of each component of the system. We also show that the policies can be executed on the real robot and generalize to various objects. One potential extension of our work is to train the policy with a wide variety of object shapes which may require image-based or point cloud-based policies. Also, the pipeline can potentially be extended to other extrinsic dexterity tasks.</p>
<p>Despite the simplicity of the proposed method, we would like to emphasize the following takeaways from this work: First, we provide a concrete example that a simple gripper can do much more than pick-and-place while being cheaper and easier to maintain than a dexterous hand, following previous work in extrinsic dexterity. We envision more future work in this direction in manipulation. Second, RL can be a good option to generate policies with emergent extrinsic dexterity, and sim2real transfer works reasonably well with our proposed system. Our work takes a step towards deploying contactrich policies with a simple gripper in the real world.</p>
<p>Appendices A Additional Results</p>
<p>A.1 Sensitivity analysis on physical parameters</p>
<p>In addition to the evaluation on policy generalization in Section 5.4, we modify the important physical parameters one at a time to understand the sensitivity of the policy performance to these parameters ( Figure 9). We compare the same baselines as Section 5.  A.2 Sensitivity analysis on object pose estimation noise</p>
<p>The proposed system takes the 6D object pose as policy input. In the real world, object pose estimation might be noisy. In this section, we evaluate the policies trained with ADR with different levels of pose estimation noise for each dimension of the 6D object pose ( Figure 10). During evaluation, for each timestep across the episode, we sample a scalar noise from a Gaussian distribution N (µ = 0, σ = x) and add it to one dimension of the object pose. The standard deviations σ = x are shown as the x-axis in the plots. The shaded area indicates the standard deviation of the success rates across seeds. </p>
<p>A.3 Reward term weights</p>
<p>The reward function shown in Equation 1a is composed of three terms with weights α 1 , α 2 and β. α 1 and α 2 weight the translation and rotation error between the target grasp and the current endeffector.β weights the target grasp occlusion penalty which is to penalize the agent if the target grasp configuration is in collision with the table. We use α 1 = 50, α 2 = 2, β = 200 in all the experiments. In this section, we train the policies with different weight values to see how much reward tuning is required to achieve reasonable performance for the occluded grasping task. Figure 11 below shows that the policy is not too sensitive in most of the case we tested except that a higher α 1 = 70 leads to a 50% drop in performance. B Implementation Details</p>
<p>B.1 Simulation environment</p>
<p>We build the simulation environment with Robosuite [38] which uses the MuJoCo simulator [39]. Each episode has a length of 40 timesteps which corresponds to 20 seconds of real time execution. At the beginning of each episode, we set the robot arm to an initial joint configuration with Gaussian noise in the joint angles of 0.02 rad. We use a box-shaped object in the simulation environment. The dimensions of the box are randomized in the ADR experiments. One important note on the simulator environment is the parameters of the MuJoCo solver. We notice that MuJoCo sometimes creates unrealistic contacts with the default solver. We reduce the simulation solver timestep from the default value of 0.002 to 0.001 and set the "noslip iterations" to 20 which significantly improved simulation quality on contacts.</p>
<p>B.2 Grasp configurations</p>
<p>In this work, we focus on grasping large objects from the side because this is a task that may demonstrate the benefits of extrinsic dexterity. For single grasp experiments, a default grasp location is shown in Figure 1. In multi-grasp experiments, the grasps are sampled from a distribution shown in Figure 6. The grasps are sampled along the side of the box and they are 2 cm away from the edges. These grasp configurations are supposed to be the input to our proposed system, and could be replaced by other grasp generation methods.</p>
<p>B.3 Success rate calculation</p>
<p>In simulation, the success of the task is computed as 1(∆T &lt; 3 cm) · 1(∆θ &lt; 10 deg) at the end of an episode. As defined in Section 3, ∆T is the position difference between the end-effector and the target grasp and ∆θ is the orientation difference. The success is defined in this way because we focus on reaching the desired grasp. One alternative is to evaluate the final grasping success by closing the gripper and lift the object. However, this will increase the simulation time during training. To confirm that the pose difference is a good proxy for the final grasping success, we evaluated a trained policy and verified that if the robot closes the gripper at the end of a successful episode according to the pose difference metric, it is able to lift the object 100% of the time.</p>
<p>For the real robot experiments, we evaluate success by closing the gripper and lifting the object; if the object was successfully lifted, we will mark it as a successful episode.</p>
<p>B.4 Observation and action space</p>
<p>As mentioned in Section 4.2, the observation includes a target grasp configuration in the object frame O g, the pose of the end-effector in the world frame W E and the object pose in the world frame W O. One implementation detail is that we also include the pose of the end-effector in the object frame O E = ( W O) −1 ( W E) because we found that it sometimes speeds up learning. Each pose is represented as a 3D translation vector and a 4D quaternion representation of the rotation.</p>
<p>The action space of the policy is the delta pose of the end-effector ∆E in its local frame represented by a vector of translation p ∈ R 3 and a 3D vector of rotation q ∈ SO(3) with axis-angle representation. An outline of the policy execution pipeline is shown in Figure 12. ∆E is then passed into a collision check function to form a desired pose E d which will be sent to a low-level controller.</p>
<p>B.5 Low-level controller</p>
<p>Handling joint limit: Although we may use nullspace in the operational space controller to avoid reaching joint limit, in practice, certain desired end-effector poses still reach joint limits that cannot be avoided by nullspace. Thus, we handling the joint limit in the following way. If the corresponding joint configuration of the desired pose is going to reach joint limits, we will overwrite the policy action to output the desired pose of the previous timestep to the low-level controller. In detail, we use the Jacobian J to estimate the joint configuration of the desired pose:
θ t+1 joints = θ t joints + J −1 · ∆E(2)
where θ joints are the joint angles and ∆E is the output of the policy. If any joint in θ t+1 joints is close to the limit, the low-level controller will use the previous desired pose E d instead.</p>
<p>Parameters of the Operational Space Controller: We use K p = 300 for position error, K p = 30 for orientation error, and K d = √︁ K p . These values are chosen by making sure that the real robot is compliant enough to safely collide with the object and the bin without damage. In Figure 4, the baseline of "High-gain OSC" uses K p = 600 for position error, K p = 60 for orientation error, and K d = √︁ K p . This baseline with less compliance is not only slower to train in the simulation, but also not safe to execute on the real robot for our task which involves rich contacts and relies on environment constraints. During our initial experiments, with the high-gain OSC, the robot deforms the object and the bin surface. Object pose B End-effector pose in the object frame 3 ! " " Figure 12: Outline of policy execution: Given the observation, the policy outputs an end-effector delta movement. If the desired pose is within the joint limit of the robot, it will be sent to the low-level controller which operates at a higher frequency.</p>
<p>B.6 Multi-Grasp Training with Curriculum</p>
<p>Here are more details on multi-grasp training. When the success rate of policy on a boundary case of the training range is above 0.8, it will expand the range of grasps by 0.25 (See Figure 6 for parameterizations of the grasp configurations). For example, if the policy is currently training with grasps [1,2], and the success rate evaluated at grasp ID 1 is above 0.8, the new training range will be [0. 75,2]. This is following a similar procedure as Automatic Domain Randomization, but randomizing goals instead of simulation parameters.</p>
<p>B.7 RL Training</p>
<p>We use Soft Actor Critic [37] to train the RL policy with the impementation from rlkit (https:// github.com/rail-berkeley/rlkit). Hyperparameters for SAC training are included in Table 2.</p>
<p>Since the task is conditioned on the target grasp as a goal, we use Hindsight Experience Replay [40] for all the experiments with 60% original goals and 40% of the goals sampled from the same rollout.</p>
<p>We compare the policies across 5 random seeds of each method and plot the average performance with standard deviation across seeds. We use 10 episodes for each evaluation setting. </p>
<p>C Automatic Domain Randomization</p>
<p>As discussed in Section 4.6, we use Automatic Domain Randomization [14] to improve policy generalization across environment variations. In ADR, the policy is first trained with an environment with very little randomization, and then we gradually expand the variations based on the evaluation performance. For a set of environment parameters λ i , each λ i is sampled from a uniform distribution λ i ∼ U (ϕ L i , ϕ H i ) at the beginning of each episode. During training, the policy will be evaluated at these boundary values λ i = ϕ L i or λ i = ϕ H i . If the performance is higher than a threshold, the boundary value will be expanded by an increment ∆. For example, if the performance at λ i = ϕ H i is higher than the threshold, the training distribution becomes λ i ∼ U (ϕ L i , ϕ H i + ∆) in the next iteration. Compared to directly training the policy with the entire variations, Automatic Domain Randomization can reduce the need of manually tuning a suitable range of variations for each environment parameter. Table 3 summarized the simulation parameters in the experiment. All the parameters are uniformly sampled from these ranges at the beginning of each episode. The ranges of the parameters start from a single initial value and gradually expand to a wider range according to the pre-specific increment step +∆ on the upper bound and the decrement step −∆ at the lower bound. We include the training plots of the ADR policies in Figure 13. Dashed lines in Figure 13 indicate fixed parameter boundaries where we do not intent to expand. The final ranges are used when we sample 100 environments for evaluation in Section 5.4.  </p>
<p>D Real robot experiment</p>
<p>In this section, we include more details and discussion for the real robot experiments. Quantitative results can be found on the website 2 where we include all the videos for the real robot experiments, video examples of failure cases, recovery behaviors and ICP results.</p>
<p>D.1 Implementation details</p>
<p>The robot setup is shown in Figure 14. The code for controlling the real robot is built on top of FrankaPy [41]. The policies are trained in the simulator and zero-shot transferred to a physical Franka Emika Panda robot. For the real robot experiments, we train a policy in the XZ plane from the side view to reduce the sim2real gap of the policy, since the motion is mostly in the XZ plane for the side grasp. Figure 14: Robot setup. We use one Azure Kinect camera for object pose estimation.</p>
<p>Sim2Real gap of the low-level controller: We observe a noticeable sim2real gap on the low-level controller when deploying the policy. The same command of moving the end-effector to a certain pose in free space may not have the same resulting movement. This is a combination of two factors: First, there is a significant discrepancy between the robot model in simulation and the real robot. The real robot has more damping and friction on the joints. Second, we use a compliant controller for this task, which is more susceptible to the noise in the system. As a result, the real robot always executes a smaller delta movement than the simulator. To compensate for this sim2real gap, we slightly increase the action scale and reduce the policy execution rate from 2Hz to 1Hz. Both of these changes will allow the real robot to compensate for the smaller movement caused by the damping and friction of its joints. Note that the sim2real gap still exists after these changes. However, the remaining gap could be further compensated by using a closed-loop policy. During our experiments, we first use the default object Box-0 to tune the controller until we observe several successes in a row. After that, we keep the same controller setting for the entire evaluation process.</p>
<p>Defining object pose: The pose of a box can simply be defined at the center of its volume and with the axes defined parallel to the edges. For non-box object, we define the pose to be the center of its bounding box. We scan the non-box objects into point clouds with the Qlone app on the phone (Figure 15 top row). To obtain the bounding box, we first run Principle Component Analysis of the scanned object to get the principle axes. Then, we take the min and max values along the axes to get the dimension of the bounding box. The axes are then aligned to global axes based on the initial pose ( Figure 15 middle row).</p>
<p>Toy bag</p>
<p>Bottle Container Container-reverse Pose estimation with Iterative Closest Point: To get the pose of the object as the observation of the policy, we use Iterative Closest Point (ICP) which matches the current point cloud to a template point cloud of the object [32]. We use the implementation from Open3D. For box objects, we simply create a box shape template with measured size. For non-box objects, we use the scanned object point clouds as mentioned above. Figure 16 shows an example of the results from ICP across an episode. Figure 15 includes examples of ICP results for non-box objects potentially with partial point cloud. More visualizations of ICP results can be found on the website. Figure 16: Examples of pose estimation with ICP during an episode. The blue points are the observed point cloud from the camera. The red points are the object template that matches to the observed point cloud using ICP.</p>
<p>D.2 More information on the objects</p>
<p>To emphasize the diversity of the objects and demonstrate the generalization capability of the policy, we include more descriptions on the objects in this section. In Table 4, we highlight the object properties that are out of the ADR training distribution in bold. Box-0 is the default object that we used to calibrate the simulator and to tune the low-level controller. Table 4: Real robot evaluations with more object information. We highlight the out-of-distribution aspect of the object properties in bold.</p>
<p>• Stop reaching: Following the "standing" strategy, the policy successfully rotates the object to a stable pose on the side of the object. However, it cannot reach the final grasping pose. The gripper tries to move down to reach the pose but it collides with the object due to the unexpected object dimension. • Timeout: Since we use a fixed episode length during evaluation, sometimes the policy does not have enough time to finish the task although it is very close to a success. This happens when the policy spends time to recover from some failed attempts at the beginning of the episode.</p>
<p>Videos on these failure cases can be found on the website. We summarize the counts of the failure cases in Table 5 and Table 6. The most common failure case for the policy trained with ADR is the repeated rotation. For the policy trained without ADR, the most common failure case is missing the initial contact. Comparing the percentage of the failure cases between Table 5 and Table 6, we observe that percentage of missing the initial contact and the percentage of stopping the reaching motion drops drastically when the policy is trained with ADR because the policy is more robust to variations in shape and dimensions. </p>
<p>Figure 2 :
2Notations: W E denotes the pose of the end-effector in the world frame W . O g denotes the target grasp in the object frame O . Six marker locations m i in green on the target grasp are used to calculate the occlusion penalty.</p>
<p>Figure 3 :
3Outline of policy execution: Given the observations, the policy outputs an end-effector delta movement (Section 4.2) to the low-level controller (Section 4.4).</p>
<p>Figure 4 :
4Training curves and ablations: (a) ablations on the reward function and the wall. (b) ablations on the controller.</p>
<p>Figure 5 :
5Visualizations of the policies in different scenarios.</p>
<p>Figure 6 :
6Left: Grasp configurations. Right: Multi-Grasp Training results with and without curriculum.</p>
<p>Figure 7 :
7Evaluation on the generalization of the policies by sampling 100 environments. In this section, we evaluate the generalization of the policy across environment variations: open loop trajectories (Open Loop), policies trained over a fixed environment (Fixed Env) and policies trained with ADR (With ADR).</p>
<p>4: policies trained over a fixed environment (Fixed Env), policies trained with ADR (With ADR) and open-loop trajectories generated by rolling out the fixed env policy in the default environment (Open Loop). The ranges of parameters are chosen to create a performance drop for all the baselines as a stress test. Similar to what we observe in Section 5.4, the policy can cover a wider range of physical parameters with closed-loop execution and with ADR.</p>
<p>Figure 9 :
9We evaluate the generalization of policies by changing one parameter at a time. The dashed lines indicate the default values of these parameters in the fixed environment.</p>
<p>Figure 10 :
10We evaluate the sensitivity of the ADR policies on object pose estimation noise.</p>
<p>Figure 11 :
11Training curves with different reward weights. For each plot, we train the policies by changing one of the weight terms to three different values.</p>
<p>Overall training performance of the ADR policies: Success rate over the entire training range (left) and total number of expanded parameter boundaries. progress of individual ADR parameters. Each plot for the physical parameters has two curves indicating the upper and lower bound of the expanded training range. Dashed lines indicate fixed parameter boundaries where we do not intent to expand.</p>
<p>Figure 13 :
13Training curves for the ADR policies.</p>
<p>Figure 15 :
15Illustrations of pose estimation pipeline for the non-box objects. The top row shows the scanned object model. The middle row shows bounding box calculation and pose definition. The last row shows an example of ICP.</p>
<p>Table 2 :
2Hyperparameters for RL training.Hyperparameters 
Values 
Optimizer 
Adam 
Learning rate -Policy 
1e-3 
Leraning rate -Q-function 5e-4 
Networks 
[512, 512, 512] MLP 
Batch size 
256 
Nonlinearity 
ReLU 
Soft target update (τ ) 
0.005 
Replay buffer size 
1e6 
Discount factor (γ) 
0.99 
HER rollout goals 
40% </p>
<p>Table 3 :
3Simulation parameters in Automatic Domain RandomizationInitial Value +∆ 
−∆ 
Final Range 
Object size x (m) 
0.15 
0.01 -0.01 
[0.14, 0.16] 
Object size z (m) 
0.05 
0.01 -0.01 
[0.04, 0.06] 
Table friction 
0.3 
0.1 
-0.1 
[0.1, 0.5] 
Gripper friction 
3 
/ 
-1 
[2, 3] 
Object Density (g/m 3 ) 
86 
86 
43 
[43, 172] 
Action translation scale (m) 
0.03 
/ 
-0.005 
[0.02, 0.03] 
Action rotation scale (rad) 
0.2 
/ 
-0.05 
[0.1, 0.2] 
Initial distance to wall (m) 
0 
0.01 
/ 
[0, 0.02] 
Table offset x (m) 
0.5 
0.01 -0.01 
[0.48, 0.52] 
Table offset z (m) 
0.07 
0.01 
0.01 
[0.055, 0.075] </p>
<p>Table 5 :
5Failure cases for Policy w/ ADR during real robot evaluation. The most common failures include dropping the object during rotation, repeated rotation, and unexpected object dynamics.
https://sites.google.com/view/grasp-ungraspable
https://sites.google.com/view/grasp-ungraspable
AcknowledgmentsThis material is based upon work supported by the National Science Foundation under Grant No. IIS-1849154 and LG Electronics. We thank Daniel Seita, Thomas Weng, Tao Chen, Homanga Bharadhwaj, and Chris Paxton for the valuable feedback.The policy trained with ADR can generalize across physical properties such as weight and surface friction. We stress test Box-0 with additional weights by putting four or eight erasers inside of the box. The erasers can move in the box during execution, which is not modeled in simulation.Although we do not have access to the true friction coefficient between the object and the table, the difference in surface friction results in qualitatively different behavior of the object even among the cardboard boxes. For example, Box-3 has tape on its surface which has much higher friction than the others cardboard boxes. It tends to stick to the wall during execution. The toy bag has a similar cross section as the box but the material is very different.We also evaluate the policies with objects that are not similar to a box shape including a bottle and a container. Due to the difference in shape, both objects result in different dynamics during execution. In addition, with the same container object, starting it from different initial poses will also lead to different object pose distribution. Videos can be found on the website. Nonetheless, the policy trained with ADR shows reasonable generalization across these non-box objects.D.3 Failure casesIn this section, we include discussions on the failure cases of the evaluation. We categorize the failure cases into the following categories and discuss the potential reasons:A failure case that happens before the initial contact:• Missing initial contact: The robot is not able to reach the initial contact of the object to rotate it. This is mostly due to the noise in pose estimation and the variations in object dimension.Failure cases that happen during the rotation:• Object drops during rotation: The object drops to the table during rotation. One potential reason for this failure case is that the finger slips on the object during rotation. Another potential reason for this failure case is the insufficient rotation of the low-level controller due to the sim2real gap (See Section D -Sim2Real gap of the low-level controller). In the "dropping" strategy, the policy is supposed to rotate object and then let it drop on the bottom finger. Before the dropping happens, the gripper needs to be rotated until the bottom finger is below the object. Otherwise, the bottom finger will not be able to catch the object and the object directly drops to the table. • Repeated rotation: The robot repeatedly rotates and drops the object. This is different from the previous failure case because the robot moves down with the object at the same time. Our hypothesis for this failure case is that the policy gets stuck in a loop in the MDP. • Joint limit: The robot hits a joint limit and the policy gets stuck at the joint limit.Failure cases that happen after the rotation:• Unexpected object dynamics: When the robot rotates the object, the object might move in unexpected ways. This mostly happens for the non-box objects.
Extrinsic dexterity: In-hand manipulation with external forces. N C Dafle, A Rodriguez, R Paolini, B Tang, S S Srinivasa, M Erdmann, M T Mason, I Lundberg, H Staab, T Fuhlbrigge, 2014 IEEE International Conference on Robotics and Automation (ICRA). IEEEN. C. Dafle, A. Rodriguez, R. Paolini, B. Tang, S. S. Srinivasa, M. Erdmann, M. T. Mason, I. Lundberg, H. Staab, and T. Fuhlbrigge. Extrinsic dexterity: In-hand manipulation with external forces. In 2014 IEEE International Conference on Robotics and Automation (ICRA), pages 1578-1585. IEEE, 2014.</p>
<p>Sampling-based planning of in-hand manipulation with external pushes. N Chavan-Dafle, A Rodriguez, N. Chavan-Dafle and A. Rodriguez. Sampling-based planning of in-hand manipulation with external pushes, 2017.</p>
<p>Manipulation with shared grasping. Y Hou, Z Jia, M Mason, Robotics: Science and Systems. Y. Hou, Z. Jia, and M. Mason. Manipulation with shared grasping. In Robotics: Science and Systems, 2020.</p>
<p>Fast planning for 3d any-pose-reorienting using pivoting. Y Hou, Z Jia, M T Mason, 10.1109/ICRA.2018.84628342018 IEEE International Conference on Robotics and Automation (ICRA). Y. Hou, Z. Jia, and M. T. Mason. Fast planning for 3d any-pose-reorienting using pivoting. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 1631-1638, 2018. doi:10.1109/ICRA.2018.8462834.</p>
<p>In-hand manipulation via motion cones. N Chavan-Dafle, R Holladay, A Rodriguez, N. Chavan-Dafle, R. Holladay, and A. Rodriguez. In-hand manipulation via motion cones, 2019.</p>
<p>Contact mode guided sampling-based planning for quasistatic dexterous manipulation in 2d. X Cheng, E Huang, Y Hou, M T Mason, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEEX. Cheng, E. Huang, Y. Hou, and M. T. Mason. Contact mode guided sampling-based plan- ning for quasistatic dexterous manipulation in 2d. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 6520-6526. IEEE, 2021.</p>
<p>Contact mode guided motion planning for quasidynamic dexterous manipulation in 3d. X Cheng, E Huang, Y Hou, M T Mason, arXiv:2105.14431arXiv preprintX. Cheng, E. Huang, Y. Hou, and M. T. Mason. Contact mode guided motion planning for quasidynamic dexterous manipulation in 3d. arXiv preprint arXiv:2105.14431, 2021.</p>
<p>6-dof graspnet: Variational grasp generation for object manipulation. A Mousavian, C Eppner, D Fox, International Conference on Computer Vision (ICCV). A. Mousavian, C. Eppner, and D. Fox. 6-dof graspnet: Variational grasp generation for object manipulation. In International Conference on Computer Vision (ICCV), 2019.</p>
<p>6-dof grasping for target-driven object manipulation in clutter. A Murali, A Mousavian, C Eppner, C Paxton, D Fox, A. Murali, A. Mousavian, C. Eppner, C. Paxton, and D. Fox. 6-dof grasping for target-driven object manipulation in clutter, 2020.</p>
<p>Manipulation trajectory optimization with online grasp synthesis and selection. L Wang, RSSY Xiang, RSSD Fox, RSSRobotics: Science and Systems. 2020L. Wang, Y. Xiang, and D. Fox. Manipulation trajectory optimization with online grasp syn- thesis and selection. In Robotics: Science and Systems (RSS), 2020.</p>
<p>Learning pregrasp manipulation of objects from ungraspable poses. Z Sun, K Yuan, W Hu, C Yang, Z Li, Z. Sun, K. Yuan, W. Hu, C. Yang, and Z. Li. Learning pregrasp manipulation of objects from ungraspable poses, 2020.</p>
<p>Planning pre-grasp manipulation for transport tasks. L Y Chang, S S Srinivasa, N S Pollard, 2010 IEEE International Conference on Robotics and Automation. IEEEL. Y. Chang, S. S. Srinivasa, and N. S. Pollard. Planning pre-grasp manipulation for transport tasks. In 2010 IEEE International Conference on Robotics and Automation, pages 2697-2704. IEEE, 2010.</p>
<p>Pre-grasp sliding manipulation of thin objects using soft, compliant, or underactuated hands. K Hang, A S Morgan, A M Dollar, 10.1109/LRA.2019.2892591IEEE Robotics and Automation Letters. 42K. Hang, A. S. Morgan, and A. M. Dollar. Pre-grasp sliding manipulation of thin objects using soft, compliant, or underactuated hands. IEEE Robotics and Automation Letters, 4(2): 662-669, 2019. doi:10.1109/LRA.2019.2892591.</p>
<p>. I Openai, M Akkaya, M Andrychowicz, M Chociej, B Litwin, A Mcgrew, A Petron, M Paino, G Plappert, R Powell, J Ribas, N Schneider, J Tezak, P Tworek, L Welinder, Q Weng, W Yuan, L Zaremba, Zhang, Solving rubik's cube with a robot handOpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. Zhang. Solving rubik's cube with a robot hand, 2019.</p>
<p>Visual detection of opportunities to exploit contact in grasping using contextual multi-armed bandits. C Eppner, O Brock, 10.1109/IROS.2017.82021682017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). C. Eppner and O. Brock. Visual detection of opportunities to exploit contact in grasping using contextual multi-armed bandits. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 273-278, 2017. doi:10.1109/IROS.2017.8202168.</p>
<p>Robot grasp synthesis algorithms: A survey. K B Shimoga, The International Journal of Robotics Research. 153K. B. Shimoga. Robot grasp synthesis algorithms: A survey. The International Journal of Robotics Research, 15(3):230-266, 1996.</p>
<p>Constructing force-closure grasps. V.-D Nguyen, The International Journal of Robotics Research. 73V.-D. Nguyen. Constructing force-closure grasps. The International Journal of Robotics Re- search, 7(3):3-16, 1988.</p>
<p>Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. L Pinto, A Gupta, 2016 IEEE international conference on robotics and automation (ICRA). IEEEL. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. In 2016 IEEE international conference on robotics and automation (ICRA), pages 3406-3413. IEEE, 2016.</p>
<p>Data-driven grasp synthesis-a survey. J Bohg, A Morales, T Asfour, D Kragic, IEEE Transactions on Robotics. 302J. Bohg, A. Morales, T. Asfour, and D. Kragic. Data-driven grasp synthesis-a survey. IEEE Transactions on Robotics, 30(2):289-309, 2013.</p>
<p>Same object, different grasps: Data and semantic knowledge for task-oriented grasping. A Murali, W Liu, K Marino, S Chernova, A Gupta, A. Murali, W. Liu, K. Marino, S. Chernova, and A. Gupta. Same object, different grasps: Data and semantic knowledge for task-oriented grasping, 2020.</p>
<p>Integrated grasp and motion planning. N Vahrenkamp, M Do, T Asfour, R Dillmann, 10.1109/ROBOT.2010.55093772010 IEEE International Conference on Robotics and Automation. N. Vahrenkamp, M. Do, T. Asfour, and R. Dillmann. Integrated grasp and motion planning. In 2010 IEEE International Conference on Robotics and Automation, pages 2883-2888, 2010. doi:10.1109/ROBOT.2010.5509377.</p>
<p>Integrated grasp and motion planning using independent contact regions. J Fontanals, B.-A Dang-Vu, O Porges, J Rosell, M A Roa, 10.1109/HUMANOIDS.2014.7041469IEEE-RAS International Conference on Humanoid Robots. J. Fontanals, B.-A. Dang-Vu, O. Porges, J. Rosell, and M. A. Roa. Integrated grasp and motion planning using independent contact regions. In 2014 IEEE-RAS International Conference on Humanoid Robots, pages 887-893, 2014. doi:10.1109/HUMANOIDS.2014.7041469.</p>
<p>Goal-auxiliary actor-critic for 6d robotic grasping with point clouds. L Wang, Y Xiang, W Yang, A Mousavian, D Fox, L. Wang, Y. Xiang, W. Yang, A. Mousavian, and D. Fox. Goal-auxiliary actor-critic for 6d robotic grasping with point clouds, 2021.</p>
<p>Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, S Levine, D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation, 2018.</p>
<p>Grasping in the wild: Learning 6dof closed-loop grasping from low-cost demonstrations. S Song, A Zeng, J Lee, T Funkhouser, Robotics and Automation Letters. S. Song, A. Zeng, J. Lee, and T. Funkhouser. Grasping in the wild: Learning 6dof closed-loop grasping from low-cost demonstrations. Robotics and Automation Letters, 2020.</p>
<p>Pregrasp manipulation as trajectory optimization. J King, M Klingensmith, C Dellin, M Dogar, P Velagapudi, N Pollard, S Srinivasa, 10.15607/RSS.2013.IX.015Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsBerlin, GermanyJ. King, M. Klingensmith, C. Dellin, M. Dogar, P. Velagapudi, N. Pollard, and S. Srinivasa. Pregrasp manipulation as trajectory optimization. In Proceedings of Robotics: Science and Systems, Berlin, Germany, June 2013. doi:10.15607/RSS.2013.IX.015.</p>
<p>A system for general in-hand object re-orientation. T Chen, J Xu, P , Conference on Robot Learning. T. Chen, J. Xu, and P. Agrawal. A system for general in-hand object re-orientation. Conference on Robot Learning, 2021.</p>
<p>Deep dynamics models for learning dexterous manipulation. A Nagabandi, K Konolige, S Levine, V Kumar, Conference on Robot Learning. PMLRA. Nagabandi, K. Konolige, S. Levine, and V. Kumar. Deep dynamics models for learning dexterous manipulation. In Conference on Robot Learning, pages 1101-1112. PMLR, 2020.</p>
<p>Beyond pick-and-place: Tackling robotic stacking of diverse shapes. A X Lee, C M Devin, Y Zhou, T Lampe, K Bousmalis, J T Springenberg, A Byravan, A Abdolmaleki, N Gileadi, D Khosid, 5th Annual Conference on Robot Learning. A. X. Lee, C. M. Devin, Y. Zhou, T. Lampe, K. Bousmalis, J. T. Springenberg, A. Byravan, A. Abdolmaleki, N. Gileadi, D. Khosid, et al. Beyond pick-and-place: Tackling robotic stack- ing of diverse shapes. In 5th Annual Conference on Robot Learning, 2021.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world, 2017.</p>
<p>Universal value function approximators. T Schaul, D Horgan, K Gregor, D Silver, International conference on machine learning. PMLRT. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In International conference on machine learning, pages 1312-1320. PMLR, 2015.</p>
<p>Efficient variants of the icp algorithm. S Rusinkiewicz, M Levoy, Proceedings third international conference on 3-D digital imaging and modeling. third international conference on 3-D digital imaging and modelingIEEES. Rusinkiewicz and M. Levoy. Efficient variants of the icp algorithm. In Proceedings third international conference on 3-D digital imaging and modeling, pages 145-152. IEEE, 2001.</p>
<p>A unified approach for motion and force control of robot manipulators: The operational space formulation. O Khatib, IEEE Journal on Robotics and Automation. 31O. Khatib. A unified approach for motion and force control of robot manipulators: The opera- tional space formulation. IEEE Journal on Robotics and Automation, 3(1):43-53, 1987.</p>
<p>Variable impedance control in end-effector space: An action space for reinforcement learning in contact-rich tasks. R Martín-Martín, M A Lee, R Gardner, S Savarese, J Bohg, A Garg, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEER. Martín-Martín, M. A. Lee, R. Gardner, S. Savarese, J. Bohg, and A. Garg. Variable impedance control in end-effector space: An action space for reinforcement learning in contact-rich tasks. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Sys- tems (IROS), pages 1010-1017. IEEE, 2019.</p>
<p>Divide-and-conquer reinforcement learning. D Ghosh, A Singh, A Rajeswaran, V Kumar, S Levine, arXiv:1711.09874arXiv preprintD. Ghosh, A. Singh, A. Rajeswaran, V. Kumar, and S. Levine. Divide-and-conquer reinforce- ment learning. arXiv preprint arXiv:1711.09874, 2017.</p>
<p>Gradient surgery for multitask learning. T Yu, S Kumar, A Gupta, S Levine, K Hausman, C Finn, arXiv:2001.06782arXiv preprintT. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn. Gradient surgery for multi- task learning. arXiv preprint arXiv:2001.06782, 2020.</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, International conference on machine learning. PMLRT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861-1870. PMLR, 2018.</p>
<p>Y Zhu, J Wong, A Mandlekar, R Martín-Martín, arXiv:2009.12293robosuite: A modular simulation framework and benchmark for robot learning. arXiv preprintY. Zhu, J. Wong, A. Mandlekar, and R. Martín-Martín. robosuite: A modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020.</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEE. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026- 5033. IEEE, 2012.</p>
<p>M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, P Abbeel, W Zaremba, arXiv:1707.01495Hindsight experience replay. arXiv preprintM. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba. Hindsight experience replay. arXiv preprint arXiv:1707.01495, 2017.</p>
<p>K Zhang, M Sharma, J Liang, O Kroemer, arXiv:2011.02398A modular robotic arm control stack for research: Franka-interface and frankapy. arXiv preprintK. Zhang, M. Sharma, J. Liang, and O. Kroemer. A modular robotic arm control stack for research: Franka-interface and frankapy. arXiv preprint arXiv:2011.02398, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
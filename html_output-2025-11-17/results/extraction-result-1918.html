<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1918 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1918</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1918</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-282102744</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.13778v1.pdf" target="_blank">InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy</a></p>
                <p><strong>Paper Abstract:</strong> We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act''by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act''by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1918.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1918.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternVLA-M1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-system vision-language-action model that pretrains a VLM on large-scale spatial grounding (points, boxes, traces) and then post-trains an action expert with spatially guided prompts and co-training to transfer embodied control across sim and real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternVLA-M1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dual-system VLA with a VLM planner (Qwen2-family backbone) producing latent spatial planning tokens via spatial prompting, and an Action Expert (diffusion/DiT actor) that decodes embodiment-specific continuous motor commands; joint multimodal input (wrist + third-person RGB, instruction, spatial prompt). ~4.1B parameters; uses a small querying transformer to map planner embeddings to action queries and applies gradient decay to limit backward flow into the VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal vision-language spatial grounding pretraining followed by action post-pretraining (multistage): VLM spatial grounding pretraining then VLA post-training / post-pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Over 3M multimodal samples: >2.3M spatial grounding items (point/box/trajectory QA), plus ~0.7M multimodal understanding samples from web, real, and simulated sources. Pretraining explicitly includes point annotations, bounding boxes, 2D end-effector traces, affordance labels and trajectory waypoints (embodiment-agnostic spatial priors). Mid-/post-training used 244K closed-loop simulated pick-and-place episodes (InternData M1) synthesized by GenManip with randomized lighting, textures and validated physical feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation / instruction-following pick-and-place and long-horizon manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on short-horizon atomic tasks (SimplerEnv: Google Robot VM/VA, WidowX-VM), LIBERO (Franka) spatial & long-horizon tracks, a 200-task large-scale simulated pick-and-place suite (3K+ objects, two RGB views: third-person + wrist), and a set of real-world clustered pick-and-place and long-horizon tasks on Franka (with 2 RealSense RGB cameras). Action space: continuous joint-space (delta joint control) executed in chunks (action chunk size up to 16); objects: thousands of synthetic and real household items and containers; environments: physics simulation (Isaac Sim) and real robot lab.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper explicitly studies semantic overlap: pretraining emphasizes spatial relations, object-level grounding and affordances to align with manipulation tasks; authors report improved alignment metrics (see PSS) and show pretraining data includes object descriptions, spatial relations, affordance signals and trajectory examples to increase overlap with downstream robot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Multiple reported gains when using the spatially pre-trained VLM: +14.6% relative SR improvement over a Vanilla VLA on Google Robot Visual Matching; +12.4% on Visual Aggregation; +17.0% on WidowX; +4.3% on LIBERO (Franka) overall vs a variant without spatial guidance. Reported absolute/high-case numbers include LIBERO spatial and long-horizon success rates of 98.0% and 92.6% respectively, and object placement SR of 99.0% (paper reports these values for InternVLA-M1). In SimplerEnv Google-Robot table InternVLA-M1 entries reach top scores (e.g., 95.3% on at least one Google Robot VM metric reported in-table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Baselines reported include a Vanilla VLA (Qwen2.5-VL-3B-Instruct + DiT actor) and models without spatial guidance: InternVLA-M1 variant without spatial guidance shows e.g. degradation up to -14.6% (Google Robot) and -17.0% (WidowX) relative to full InternVLA-M1; vanilla co-train and vanilla VLA have lower SRs (tables show Vanilla VLA average SRs ~66.1/63.5 on some metrics vs InternVLA-M1 ~80.7/76.0 in corresponding rows). Exact baseline numbers are reported in paper tables (see Table 1/3/4).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Paper reports few-demo fine-tuning regime: fine-tuning on each of 200 pick-and-place tasks using only five trajectories per task (w/o mid-train) and shows that mid-training on InternData M1 (244K sim samples) improves performance; mid-trained variant yields an average gain of +6.2% over GR00T N1.5 across 200 tasks. Authors also state spatially guided post-training accelerates convergence (faster optimization) though no strict 'episodes-to-X-performance' time-to-data numbers beyond the 5-trajectory fine-tune are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Indirect attention/optimization analyses: authors introduce a querying transformer whose query tokens attend to intermediate VLM layers to extract spatial cues; they also use gradient decay to limit gradient flow. No visualization of attention heatmaps is provided, but architecture-level attention uses intermediate-layer signals to inform action queries.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Authors compute Projection-space Similarity (PSS) via SVD between spatial grounding and manipulation gradient subspaces: vanilla co-training PSS=0.25, spatially guided training PSS=0.42, indicating improved alignment of representation/gradient subspaces when spatial guidance is used. No further cluster/PCA visualizations are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Strong empirical evidence for grounding: VLM spatial pretraining on points/boxes/traces transfers to improved motor execution (higher SRs and better trajectory metrics); co-training with spatial grounding and spatial prompting produces better object localization (IoU improvements on RefCOCO variants) and yields lower trajectory MAE on A0-maniskill-style metrics. Real-world experiments show improved placement on unseen objects (+20.6% with synthetic co-training) and better robustness under perturbation, supporting language→perception→action grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Paper explicitly designs a hierarchy (System 2 VLM planner for spatial reasoning and System 1 Action Expert for fast execution). Empirical results show high-level spatial priors (boxes/points/traces) benefit long-horizon planning and high-level grounding, while embodiment-specific low-level control is learned in post-training; no layer-wise neuroscientific feature-dissection is provided but ablations show spatial pretraining helps high-level spatial tasks most.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer improves when (i) domain/scene geometry and camera calibration are matched (authors calibrate simulated cameras to real ones), (ii) pretraining includes spatial labels (points/boxes/traces), and (iii) synthetic mid-training (InternData M1) is used to augment limited real demos. Transfer degrades when spatial grounding data or spatial prompting are omitted (authors report rapid degradation of spatial grounding capabilities and slower convergence).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Paper reports explicit comparisons: in the 200-task sim suite and real-world pick-and-place, InternVLA-M1 w/ mid-train shows increased generalization to unseen objects (average +6.2% over baseline across tasks) and in real clustered pick-and-place achieved +20.6% on unseen objects and novel configurations with synthetic co-training. Absolute per-condition numbers are in the evaluation figures/tables.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot: Fine-tuning with only five trajectories per task is used in the 200-task evaluation; mid-training reduces variance and improves performance. Zero-shot explicit success rates are not reported; the paper emphasizes few-shot fine-tuning plus mid-training.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Architecture-level probes: querying transformer attends to intermediate VLM layers (configurable; e.g., attend only to final layer when =1). No systematic layer freezing/probing ablation results are given beyond this and the gradient decay ablation to protect the VLM from action gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Yes — vanilla co-training of action and spatial grounding objectives yields poor gradient alignment (PSS=0.25) and leads to degradation of spatial perception and slower convergence; direct gradient flow from action to VLM can 'distort' multimodal knowledge (authors mitigate via gradient decay factor 0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Paper compares InternVLA-M1 (vision-language spatial pretraining) to baselines including vision-only elements (e.g., DiT actor with DINOv2 visual encoder) and vanilla VLA variants: results indicate vision-language spatial pretraining outperforms vanilla vision-only or vanilla VLA approaches on spatial and long-horizon tasks (quantified by SR increases reported above). No direct ImageNet-only vs VLM-only numerical head-to-head is supplied beyond these baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Authors report that spatially guided post-training accelerates convergence during manipulation training (faster optimization) and preserves spatial perception across fine-tuning; they include training schedules (e.g., 20k steps for pick-and-place fine-tuning) but do not present epoch-by-epoch representational dynamics plots except the PSS comparison indicating better alignment during co-training.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No explicit dimensionality/intrinsic-dimension or PCA variance-explained analyses are reported; the closest is PSS (SVD-based) measuring projection similarity (0.25 vs 0.42) but not dimensionality numbers.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1918.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1918.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanilla VLA (Qwen2.5-VL-3B-Instruct + DiT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla Vision-Language-Action baseline built on Qwen2.5-VL-3B-Instruct with a DiT action head</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline end-to-end VLA built from a Qwen2.5-VL-3B instruct-tuned VLM and a DiT-based action head used for comparisons; represents a conventional fine-tuned VLA without spatially guided pretraining/co-training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vanilla VLA (Qwen2.5-VL-3B-Instruct + DiT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-stream VLM backbone (Qwen2.5-VL-3B-Instruct) combined with a DiT actor for continuous action prediction; trained end-to-end on robot demonstration data without the explicit spatial grounding pretraining stage or spatial prompting used by InternVLA-M1.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining on web-scale image-text corpora (Qwen2.5 family) followed by standard fine-tuning on robot demos (no dedicated large-scale spatial grounding pretraining stage).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Qwen2.5-style VLM pretraining (image-text pairs) plus downstream robot demonstration data for action head; lacks the >2.3M spatially-focused point/box/trace supervisory examples that InternVLA-M1 uses.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (SimplerEnv, LIBERO, pick-and-place benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same evaluation regimes as InternVLA-M1 when used as baseline: short-horizon VM/VA tasks on SimplerEnv (Google Robot, WidowX) and LIBERO on Franka; continuous joint delta-action control in simulation and some real setups.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Lower than InternVLA-M1 because it lacks targeted spatial grounding pretraining; the paper shows this baseline underperforms on spatial/long-horizon tasks and can suffer from overfitting to motor traces rather than abstract spatial language.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Baseline numbers (from tables): Vanilla VLA reported e.g. ~66.1/63.5 (some multi-metric entries) and in direct comparisons InternVLA-M1 outperforms Vanilla VLA by +14.6% (Google Robot Visual Matching), +12.4% (Visual Aggregation) and +17.0% (WidowX) — baseline absolute numbers vary per task in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not specifically provided (no random-init baseline for the same architecture reported).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Paper suggests Vanilla VLA needs more embodiment-specific data to reach similar spatial grounding and generalization; no strict sample-count comparison provided except that InternVLA-M1 uses 5 demo fine-tune regime and outperforms Vanilla under those conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No fine-grained attention analyses reported specifically for this baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Compared indirectly through PSS: vanilla co-training (similar to baseline co-train regimes) yields lower PSS (~0.25) indicating poorer alignment between spatial-grounding and manipulation objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Baseline tends to overfit fine-grained motor behaviors and under-generalize to high-level linguistic spatial instructions per authors' analysis; empirical lower SRs on spatial/long-horizon tracks support this.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Vanilla VLA lacks explicit System 2/System 1 separation; paper argues this hurts high-level spatial generalization though no layer-wise feature breakdown is shown.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Performs worse when tasks require abstract spatial reasoning or object/scene shifts; benefits less from limited demos and synthetic mid-training than InternVLA-M1.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Paper shows Vanilla VLA generalizes worse to unseen objects and instructions compared to InternVLA-M1 (quantified by the relative improvements listed), but absolute per-condition numbers are in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not shown to achieve strong few-shot generalization in the five-demo per-task regime compared to InternVLA-M1.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No specific layer ablations beyond being used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Vanilla co-training / vanilla VLA setups shown to misalign gradients with spatial objectives (PSS=0.25) and can degrade spatial perception when trained jointly without spatial guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Vanilla VLA includes vision-language pretraining, but the paper's comparisons indicate that adding targeted spatial grounding (InternVLA-M1) yields clear benefits over vanilla VLA; direct vision-only (ImageNet/DINO-only) baselines are not exhaustively enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No explicit temporal-learning-phase plots for this baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality analyses provided for this baseline.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1918.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1918.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GR00T N1.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GR00T N1.5 (baseline generalist robot model cited in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open foundation model for generalist humanoid/robot agents used as a strong baseline in manipulation and instruction-following comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GR00T N1.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior generalist robot foundation model referenced and used as a baseline in experiments comparing instruction-following and long-horizon manipulation success; specific architecture details are referenced to the GR00T paper (not re-specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Mentioned as a pre-existing generalist VLA/foundation model trained on heterogeneous robot datasets (per citation); paper uses it as a comparative baseline rather than reimplementing training.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not re-specified in this paper; used as an external baseline. Paper compares InternVLA-M1 to GR00T N1.5 showing consistent improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation and instruction-following (SimplerEnv, the 200-task pick-and-place suite, real-world clustered pick-and-place, long-horizon tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same embodied benchmarks used in the paper; continuous joint delta control in simulation and real robot demonstrations; evaluated on in-distribution, unseen object, background, instruction shifts, and perturbation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Used as a baseline; paper indicates InternVLA-M1 surpasses GR00T in average SR across tasks and in mid-train variants (InternVLA-M1 w/ mid-train consistently surpasses GR00T N1.5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Paper reports InternVLA-M1 w/ mid-train achieves average gain of +6.2% over GR00T N1.5 on the 200-task pick-and-place suite; on long-horizon tasks InternVLA-M1 outperforms GR00T by larger margins (figures indicate >10% in reasoning-intensive scenarios). Exact GR00T numbers are in the paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Paper uses GR00T as a baseline in the 5-demo-per-task fine-tune evaluation; InternVLA-M1 w/ mid-train outperforms GR00T under the same sample regime — quantified as +6.2% average improvement across 200 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention-specific analyses of GR00T provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No embedding-space analyses specific to GR00T provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>GR00T is used as a comparative baseline; InternVLA-M1 shows more robust object grounding and long-horizon planning, implying GR00T is weaker on those metrics within the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed for GR00T in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>GR00T performs worse than InternVLA-M1 in settings with unseen objects/configurations and under physical interference/task replanning per reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>GR00T's performance is the reference point for the reported +6.2% vs InternVLA-M1 on unseen-object generalization; more granular splits are in the paper's figures/tables.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>GR00T used in few-shot comparisons (5 trajectories per task) where InternVLA-M1 performs better; zero-shot not shown.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not discussed specifically for GR00T here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared to a vision-only variant within GR00T in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1918.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1918.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternData M1 / GenManip</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternData M1 (244K closed-loop simulated pick-and-place samples) synthesized via GenManip pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale synthetic closed-loop dataset (244K samples) created with GenManip to bridge VLM pretraining and VLA action learning, used for post-pre-training / mid-training to improve visual diversity and action transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternData M1 (synthetic dataset produced by GenManip)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model but a mid-training dataset and simulation engine: GenManip synthesizes physically-validated manipulation trajectories using Isaac Sim, randomized object layouts, lighting, textures and a scene-graph solver; dataset provides RGB images, object poses, bounding boxes, and 2D end-effector traces for closed-loop verification.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Synthetic simulation-based action/motion pretraining data (post-pre-training for the action head / VLA post-training).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>244K validated closed-loop pick-and-place episodes; uses an asset library of 14K annotated objects, 211 tables, 1.6K textures, randomized lighting and camera parameters calibrated to real cameras. Data contains explicit grasps, joint trajectories, object poses and scene-graph annotations, and is used to initialize action representations and improve mid-training robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Simulated pick-and-place and generalizable instruction-following manipulation (mid-training and augmentation for fine-tuning on 200 pick-and-place tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Diverse pick-and-place episodes in Isaac Sim with random layouts and validated successful executions; observation: two RGB images (third-person + wrist), actions: closed-loop joint trajectories, objects: thousands of synthetic assets.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed to align with downstream embodied tasks by including object poses, grasps, and validated trajectories; cameras are calibrated to match real-world camera intrinsics/extrinsics to reduce sim-to-real gap.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Using InternData M1 for mid-training increases average performance of InternVLA-M1 across 200 tasks by +6.2% over GR00T N1.5 and reduces variance in few-demo fine-tuning regimes. Also reported to yield +20.6% improvement on unseen objects in real clustered pick-and-place when used for synthetic co-training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not directly reported; the dataset is a mid-training augmentation and not a model itself. Paper contrasts w/o mid-train variants where models are fine-tuned with only five trajectories per task.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>InternData M1 + mid-training yields substantially better few-demo generalization (fine-tuning with 5 demos) than no mid-training; quantified average gain +6.2% across tasks. Authors report mid-train variant consistently surpasses baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not applicable (dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not applicable (dataset) though used to shape representation alignment in mid-training.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Dataset provides action-conditioned supervision (joint states, object poses, end-effector traces) enabling effective learning of embodiment-specific motor commands and better grounding of spatial VLM priors into actions (empirical SR improvements when used).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Used to initialize action head (System 1) so low-level control is learned from physically-validated actions while high-level spatial priors come from VLM pretraining (System 2); supports the paper's hierarchical claims.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Improves transfer especially for novel objects/configurations and camera/environment shifts when camera calibration and randomized visual factors are used to bridge sim-to-real.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Helps with unseen-object generalization: +20.6% reported improvement on unseen objects in real clustered pick-and-place when synthetic co-training included.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Enables stronger few-shot fine-tuning (5-demo regime), but zero-shot performance is not explicitly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported; synthetic mid-training is reported to be beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>InternData M1 supplies action supervision beyond vision-only pretraining and is reported to complement limited real demos more effectively than vision-language pretraining alone.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed for dataset itself; dataset contains temporally-ordered closed-loop trajectories used to train temporal action models.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1918.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1918.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiT Actor / Diffusion Policy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiT Actor (Diffusion policy) as Action Expert</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The action generation component used as the fast System 1 executor in InternVLA-M1: a diffusion policy / DiT actor modeling continuous joint-action distributions conditioned on visual-state and latent planner tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DiT Actor (diffusion policy action expert)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An embodiment-aware diffusion policy (based on prior diffusion policy work) serving as the Action Expert; uses DINOv2 visual encoder (21M) plus a small state encoder and decodes continuous joint-space actions (delta joint control) in chunks (action chunk size up to 16). Designed to model multimodal action distributions and to be conditioned on VLM planner latent tokens via a querying transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Action-head initialization via post-pre-training on large-scale simulated action data (InternData M1) after VLM spatial pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pre-trained / post-pre-trained on the 244K closed-loop simulated samples (InternData M1) containing validated joint trajectories, object poses, grasps and scene graphs; action distributions encode motor patterns, grasps, and trajectory shape information.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Embodied continuous control for manipulation (pick-and-place, drawer opening, sandwich assembly, sorting, button pressing etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Short- and long-horizon manipulation tasks on Franka/ARX LIFT2/WidowX/Google Robot simulated and real setups; outputs continuous joint commands (delta joint space control) executed in chunks; tasks include multi-step recipes requiring precise temporal and articulated manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Conditioned on spatially grounded VLM planner tokens to ensure action outputs align with high-level spatial/instruction semantics; co-training alternates between trajectory data and spatial grounding data to reinforce alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>When conditioned on spatially pretrained VLM planner tokens and mid-trained on InternData M1, DiT Actor yields higher SR and lower trajectory MAE vs baselines; exact numeric improvements are reported at system level (e.g., InternVLA-M1 total gains reported above).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Paper notes that initializing DiT Actor without the spatially-guided VLM conditioning or without post-pre-training yields worse transfer and slower convergence; no single-number given specifically for DiT alone.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Post-pre-training the action head with InternData M1 facilitates learning with few downstream demonstrations (5 per task) and yields improved generalization; authors report mid-train variant improves average by +6.2% across tasks vs baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>The querying transformer maps latent planner embeddings into fixed queries that cross-attend to VLM layers; architecture-level attention coupling is discussed but no per-head attention visualizations shown.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Observed improved gradient/representation alignment (PSS increase from 0.25 to 0.42) when using spatially guided conditioning vs vanilla co-training, indicating more consistent optimization between DiT Actor objectives and spatial grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Empirical: coupling DiT Actor with the spatial VLM yields improved physical execution (higher SRs in manipulation, lower trajectory MAE) and better adaptability under perturbation; suggests verbs/commands are grounded to motor trajectory distributions via the conditioned diffusion policy.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Designed as low-level executor (System 1) benefiting from high-level VLM (System 2) priors. Ablations in paper show that preserving VLM semantics (via gradient decay) is important for maintaining high-level grounding while training the DiT Actor.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Best when action head is post-pre-trained on diverse simulated trajectories and when VLM provides explicit spatial prompts; naive end-to-end gradients from action to VLM can harm transfer without gradient attenuation.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>When conditioned by spatial planner and trained with InternData M1, DiT Actor supports better handling of unseen objects in real clustered pick-and-place (+20.6% reported when used as part of the full system).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Enables few-shot fine-tuning success in the 5-demo-per-task regime when combined with spatially pretrained VLM; zero-shot not shown separately.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Querying transformer attends to intermediate VLM layers; authors note possibility to select which VLM layers are attended to (configurable n), but no detailed ablation of layer choice impact beyond architectural description.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Unattenuated backward gradients from DiT to VLM can distort multimodal knowledge; authors introduce gradient decay factor (e.g., 0.5) to mitigate negative transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>DiT Actor uses visual encoders (DINOv2) for low-level perception but benefits measurably more when conditioned on spatially pretrained VLM tokens (vision-language) than when trained in isolation on vision-only inputs, per system-level comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Diffusion policy inherently models temporal trajectories; training removes idle/pause frames to improve temporal consistency. No explicit temporal representational evolution plots provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided specifically for DiT Actor beyond PSS analysis at system level.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rt-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>RT-1: Robotics transformer for real-world control at scale <em>(Rating: 2)</em></li>
                <li>Genmanip: Llm-driven simulation for generalizable instruction-following manipulation <em>(Rating: 2)</em></li>
                <li>Diffusion policy: Visuomotor policy learning via action diffusion <em>(Rating: 2)</em></li>
                <li>GR00T N1: An open foundation model for generalist humanoid robots <em>(Rating: 2)</em></li>
                <li>Qwen2.5-vl technical report <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1918",
    "paper_id": "paper-282102744",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "InternVLA-M1",
            "name_full": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy",
            "brief_description": "A dual-system vision-language-action model that pretrains a VLM on large-scale spatial grounding (points, boxes, traces) and then post-trains an action expert with spatially guided prompts and co-training to transfer embodied control across sim and real robots.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InternVLA-M1",
            "model_description": "Dual-system VLA with a VLM planner (Qwen2-family backbone) producing latent spatial planning tokens via spatial prompting, and an Action Expert (diffusion/DiT actor) that decodes embodiment-specific continuous motor commands; joint multimodal input (wrist + third-person RGB, instruction, spatial prompt). ~4.1B parameters; uses a small querying transformer to map planner embeddings to action queries and applies gradient decay to limit backward flow into the VLM.",
            "pretraining_type": "multimodal vision-language spatial grounding pretraining followed by action post-pretraining (multistage): VLM spatial grounding pretraining then VLA post-training / post-pre-training",
            "pretraining_data_description": "Over 3M multimodal samples: &gt;2.3M spatial grounding items (point/box/trajectory QA), plus ~0.7M multimodal understanding samples from web, real, and simulated sources. Pretraining explicitly includes point annotations, bounding boxes, 2D end-effector traces, affordance labels and trajectory waypoints (embodiment-agnostic spatial priors). Mid-/post-training used 244K closed-loop simulated pick-and-place episodes (InternData M1) synthesized by GenManip with randomized lighting, textures and validated physical feasibility.",
            "target_task_name": "Robotic manipulation / instruction-following pick-and-place and long-horizon manipulation",
            "target_task_description": "Evaluated on short-horizon atomic tasks (SimplerEnv: Google Robot VM/VA, WidowX-VM), LIBERO (Franka) spatial & long-horizon tracks, a 200-task large-scale simulated pick-and-place suite (3K+ objects, two RGB views: third-person + wrist), and a set of real-world clustered pick-and-place and long-horizon tasks on Franka (with 2 RealSense RGB cameras). Action space: continuous joint-space (delta joint control) executed in chunks (action chunk size up to 16); objects: thousands of synthetic and real household items and containers; environments: physics simulation (Isaac Sim) and real robot lab.",
            "semantic_alignment": "Paper explicitly studies semantic overlap: pretraining emphasizes spatial relations, object-level grounding and affordances to align with manipulation tasks; authors report improved alignment metrics (see PSS) and show pretraining data includes object descriptions, spatial relations, affordance signals and trajectory examples to increase overlap with downstream robot tasks.",
            "performance_with_language_pretraining": "Multiple reported gains when using the spatially pre-trained VLM: +14.6% relative SR improvement over a Vanilla VLA on Google Robot Visual Matching; +12.4% on Visual Aggregation; +17.0% on WidowX; +4.3% on LIBERO (Franka) overall vs a variant without spatial guidance. Reported absolute/high-case numbers include LIBERO spatial and long-horizon success rates of 98.0% and 92.6% respectively, and object placement SR of 99.0% (paper reports these values for InternVLA-M1). In SimplerEnv Google-Robot table InternVLA-M1 entries reach top scores (e.g., 95.3% on at least one Google Robot VM metric reported in-table).",
            "performance_without_language_pretraining": "Baselines reported include a Vanilla VLA (Qwen2.5-VL-3B-Instruct + DiT actor) and models without spatial guidance: InternVLA-M1 variant without spatial guidance shows e.g. degradation up to -14.6% (Google Robot) and -17.0% (WidowX) relative to full InternVLA-M1; vanilla co-train and vanilla VLA have lower SRs (tables show Vanilla VLA average SRs ~66.1/63.5 on some metrics vs InternVLA-M1 ~80.7/76.0 in corresponding rows). Exact baseline numbers are reported in paper tables (see Table 1/3/4).",
            "sample_efficiency_comparison": "Paper reports few-demo fine-tuning regime: fine-tuning on each of 200 pick-and-place tasks using only five trajectories per task (w/o mid-train) and shows that mid-training on InternData M1 (244K sim samples) improves performance; mid-trained variant yields an average gain of +6.2% over GR00T N1.5 across 200 tasks. Authors also state spatially guided post-training accelerates convergence (faster optimization) though no strict 'episodes-to-X-performance' time-to-data numbers beyond the 5-trajectory fine-tune are provided.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Indirect attention/optimization analyses: authors introduce a querying transformer whose query tokens attend to intermediate VLM layers to extract spatial cues; they also use gradient decay to limit gradient flow. No visualization of attention heatmaps is provided, but architecture-level attention uses intermediate-layer signals to inform action queries.",
            "embedding_space_analysis": "Authors compute Projection-space Similarity (PSS) via SVD between spatial grounding and manipulation gradient subspaces: vanilla co-training PSS=0.25, spatially guided training PSS=0.42, indicating improved alignment of representation/gradient subspaces when spatial guidance is used. No further cluster/PCA visualizations are reported.",
            "action_grounding_evidence": "Strong empirical evidence for grounding: VLM spatial pretraining on points/boxes/traces transfers to improved motor execution (higher SRs and better trajectory metrics); co-training with spatial grounding and spatial prompting produces better object localization (IoU improvements on RefCOCO variants) and yields lower trajectory MAE on A0-maniskill-style metrics. Real-world experiments show improved placement on unseen objects (+20.6% with synthetic co-training) and better robustness under perturbation, supporting language→perception→action grounding.",
            "hierarchical_features_evidence": "Paper explicitly designs a hierarchy (System 2 VLM planner for spatial reasoning and System 1 Action Expert for fast execution). Empirical results show high-level spatial priors (boxes/points/traces) benefit long-horizon planning and high-level grounding, while embodiment-specific low-level control is learned in post-training; no layer-wise neuroscientific feature-dissection is provided but ablations show spatial pretraining helps high-level spatial tasks most.",
            "transfer_conditions": "Transfer improves when (i) domain/scene geometry and camera calibration are matched (authors calibrate simulated cameras to real ones), (ii) pretraining includes spatial labels (points/boxes/traces), and (iii) synthetic mid-training (InternData M1) is used to augment limited real demos. Transfer degrades when spatial grounding data or spatial prompting are omitted (authors report rapid degradation of spatial grounding capabilities and slower convergence).",
            "novel_vs_familiar_objects": "Paper reports explicit comparisons: in the 200-task sim suite and real-world pick-and-place, InternVLA-M1 w/ mid-train shows increased generalization to unseen objects (average +6.2% over baseline across tasks) and in real clustered pick-and-place achieved +20.6% on unseen objects and novel configurations with synthetic co-training. Absolute per-condition numbers are in the evaluation figures/tables.",
            "zero_shot_or_few_shot": "Few-shot: Fine-tuning with only five trajectories per task is used in the 200-task evaluation; mid-training reduces variance and improves performance. Zero-shot explicit success rates are not reported; the paper emphasizes few-shot fine-tuning plus mid-training.",
            "layer_analysis": "Architecture-level probes: querying transformer attends to intermediate VLM layers (configurable; e.g., attend only to final layer when =1). No systematic layer freezing/probing ablation results are given beyond this and the gradient decay ablation to protect the VLM from action gradients.",
            "negative_transfer_evidence": "Yes — vanilla co-training of action and spatial grounding objectives yields poor gradient alignment (PSS=0.25) and leads to degradation of spatial perception and slower convergence; direct gradient flow from action to VLM can 'distort' multimodal knowledge (authors mitigate via gradient decay factor 0.5).",
            "comparison_to_vision_only": "Paper compares InternVLA-M1 (vision-language spatial pretraining) to baselines including vision-only elements (e.g., DiT actor with DINOv2 visual encoder) and vanilla VLA variants: results indicate vision-language spatial pretraining outperforms vanilla vision-only or vanilla VLA approaches on spatial and long-horizon tasks (quantified by SR increases reported above). No direct ImageNet-only vs VLM-only numerical head-to-head is supplied beyond these baselines.",
            "temporal_dynamics": "Authors report that spatially guided post-training accelerates convergence during manipulation training (faster optimization) and preserves spatial perception across fine-tuning; they include training schedules (e.g., 20k steps for pick-and-place fine-tuning) but do not present epoch-by-epoch representational dynamics plots except the PSS comparison indicating better alignment during co-training.",
            "dimensionality_analysis": "No explicit dimensionality/intrinsic-dimension or PCA variance-explained analyses are reported; the closest is PSS (SVD-based) measuring projection similarity (0.25 vs 0.42) but not dimensionality numbers.",
            "uuid": "e1918.0"
        },
        {
            "name_short": "Vanilla VLA (Qwen2.5-VL-3B-Instruct + DiT)",
            "name_full": "Vanilla Vision-Language-Action baseline built on Qwen2.5-VL-3B-Instruct with a DiT action head",
            "brief_description": "A baseline end-to-end VLA built from a Qwen2.5-VL-3B instruct-tuned VLM and a DiT-based action head used for comparisons; represents a conventional fine-tuned VLA without spatially guided pretraining/co-training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Vanilla VLA (Qwen2.5-VL-3B-Instruct + DiT)",
            "model_description": "Single-stream VLM backbone (Qwen2.5-VL-3B-Instruct) combined with a DiT actor for continuous action prediction; trained end-to-end on robot demonstration data without the explicit spatial grounding pretraining stage or spatial prompting used by InternVLA-M1.",
            "pretraining_type": "vision-language pretraining on web-scale image-text corpora (Qwen2.5 family) followed by standard fine-tuning on robot demos (no dedicated large-scale spatial grounding pretraining stage).",
            "pretraining_data_description": "Qwen2.5-style VLM pretraining (image-text pairs) plus downstream robot demonstration data for action head; lacks the &gt;2.3M spatially-focused point/box/trace supervisory examples that InternVLA-M1 uses.",
            "target_task_name": "Robotic manipulation (SimplerEnv, LIBERO, pick-and-place benchmarks)",
            "target_task_description": "Same evaluation regimes as InternVLA-M1 when used as baseline: short-horizon VM/VA tasks on SimplerEnv (Google Robot, WidowX) and LIBERO on Franka; continuous joint delta-action control in simulation and some real setups.",
            "semantic_alignment": "Lower than InternVLA-M1 because it lacks targeted spatial grounding pretraining; the paper shows this baseline underperforms on spatial/long-horizon tasks and can suffer from overfitting to motor traces rather than abstract spatial language.",
            "performance_with_language_pretraining": "Baseline numbers (from tables): Vanilla VLA reported e.g. ~66.1/63.5 (some multi-metric entries) and in direct comparisons InternVLA-M1 outperforms Vanilla VLA by +14.6% (Google Robot Visual Matching), +12.4% (Visual Aggregation) and +17.0% (WidowX) — baseline absolute numbers vary per task in paper tables.",
            "performance_without_language_pretraining": "Not specifically provided (no random-init baseline for the same architecture reported).",
            "sample_efficiency_comparison": "Paper suggests Vanilla VLA needs more embodiment-specific data to reach similar spatial grounding and generalization; no strict sample-count comparison provided except that InternVLA-M1 uses 5 demo fine-tune regime and outperforms Vanilla under those conditions.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No fine-grained attention analyses reported specifically for this baseline in the paper.",
            "embedding_space_analysis": "Compared indirectly through PSS: vanilla co-training (similar to baseline co-train regimes) yields lower PSS (~0.25) indicating poorer alignment between spatial-grounding and manipulation objectives.",
            "action_grounding_evidence": "Baseline tends to overfit fine-grained motor behaviors and under-generalize to high-level linguistic spatial instructions per authors' analysis; empirical lower SRs on spatial/long-horizon tracks support this.",
            "hierarchical_features_evidence": "Vanilla VLA lacks explicit System 2/System 1 separation; paper argues this hurts high-level spatial generalization though no layer-wise feature breakdown is shown.",
            "transfer_conditions": "Performs worse when tasks require abstract spatial reasoning or object/scene shifts; benefits less from limited demos and synthetic mid-training than InternVLA-M1.",
            "novel_vs_familiar_objects": "Paper shows Vanilla VLA generalizes worse to unseen objects and instructions compared to InternVLA-M1 (quantified by the relative improvements listed), but absolute per-condition numbers are in tables.",
            "zero_shot_or_few_shot": "Not shown to achieve strong few-shot generalization in the five-demo per-task regime compared to InternVLA-M1.",
            "layer_analysis": "No specific layer ablations beyond being used as a baseline.",
            "negative_transfer_evidence": "Vanilla co-training / vanilla VLA setups shown to misalign gradients with spatial objectives (PSS=0.25) and can degrade spatial perception when trained jointly without spatial guidance.",
            "comparison_to_vision_only": "Vanilla VLA includes vision-language pretraining, but the paper's comparisons indicate that adding targeted spatial grounding (InternVLA-M1) yields clear benefits over vanilla VLA; direct vision-only (ImageNet/DINO-only) baselines are not exhaustively enumerated here.",
            "temporal_dynamics": "No explicit temporal-learning-phase plots for this baseline provided.",
            "dimensionality_analysis": "No dimensionality analyses provided for this baseline.",
            "uuid": "e1918.1"
        },
        {
            "name_short": "GR00T N1.5",
            "name_full": "GR00T N1.5 (baseline generalist robot model cited in paper)",
            "brief_description": "An open foundation model for generalist humanoid/robot agents used as a strong baseline in manipulation and instruction-following comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GR00T N1.5",
            "model_description": "Prior generalist robot foundation model referenced and used as a baseline in experiments comparing instruction-following and long-horizon manipulation success; specific architecture details are referenced to the GR00T paper (not re-specified here).",
            "pretraining_type": "Mentioned as a pre-existing generalist VLA/foundation model trained on heterogeneous robot datasets (per citation); paper uses it as a comparative baseline rather than reimplementing training.",
            "pretraining_data_description": "Not re-specified in this paper; used as an external baseline. Paper compares InternVLA-M1 to GR00T N1.5 showing consistent improvements.",
            "target_task_name": "Robotic manipulation and instruction-following (SimplerEnv, the 200-task pick-and-place suite, real-world clustered pick-and-place, long-horizon tasks).",
            "target_task_description": "Same embodied benchmarks used in the paper; continuous joint delta control in simulation and real robot demonstrations; evaluated on in-distribution, unseen object, background, instruction shifts, and perturbation settings.",
            "semantic_alignment": "Used as a baseline; paper indicates InternVLA-M1 surpasses GR00T in average SR across tasks and in mid-train variants (InternVLA-M1 w/ mid-train consistently surpasses GR00T N1.5).",
            "performance_with_language_pretraining": "Paper reports InternVLA-M1 w/ mid-train achieves average gain of +6.2% over GR00T N1.5 on the 200-task pick-and-place suite; on long-horizon tasks InternVLA-M1 outperforms GR00T by larger margins (figures indicate &gt;10% in reasoning-intensive scenarios). Exact GR00T numbers are in the paper tables.",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Paper uses GR00T as a baseline in the 5-demo-per-task fine-tune evaluation; InternVLA-M1 w/ mid-train outperforms GR00T under the same sample regime — quantified as +6.2% average improvement across 200 tasks.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No attention-specific analyses of GR00T provided in this paper.",
            "embedding_space_analysis": "No embedding-space analyses specific to GR00T provided here.",
            "action_grounding_evidence": "GR00T is used as a comparative baseline; InternVLA-M1 shows more robust object grounding and long-horizon planning, implying GR00T is weaker on those metrics within the reported experiments.",
            "hierarchical_features_evidence": "Not analyzed for GR00T in this paper.",
            "transfer_conditions": "GR00T performs worse than InternVLA-M1 in settings with unseen objects/configurations and under physical interference/task replanning per reported comparisons.",
            "novel_vs_familiar_objects": "GR00T's performance is the reference point for the reported +6.2% vs InternVLA-M1 on unseen-object generalization; more granular splits are in the paper's figures/tables.",
            "zero_shot_or_few_shot": "GR00T used in few-shot comparisons (5 trajectories per task) where InternVLA-M1 performs better; zero-shot not shown.",
            "layer_analysis": null,
            "negative_transfer_evidence": "Not discussed specifically for GR00T here.",
            "comparison_to_vision_only": "Not directly compared to a vision-only variant within GR00T in this paper.",
            "temporal_dynamics": null,
            "dimensionality_analysis": null,
            "uuid": "e1918.2"
        },
        {
            "name_short": "InternData M1 / GenManip",
            "name_full": "InternData M1 (244K closed-loop simulated pick-and-place samples) synthesized via GenManip pipeline",
            "brief_description": "A large-scale synthetic closed-loop dataset (244K samples) created with GenManip to bridge VLM pretraining and VLA action learning, used for post-pre-training / mid-training to improve visual diversity and action transfer.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InternData M1 (synthetic dataset produced by GenManip)",
            "model_description": "Not a model but a mid-training dataset and simulation engine: GenManip synthesizes physically-validated manipulation trajectories using Isaac Sim, randomized object layouts, lighting, textures and a scene-graph solver; dataset provides RGB images, object poses, bounding boxes, and 2D end-effector traces for closed-loop verification.",
            "pretraining_type": "Synthetic simulation-based action/motion pretraining data (post-pre-training for the action head / VLA post-training).",
            "pretraining_data_description": "244K validated closed-loop pick-and-place episodes; uses an asset library of 14K annotated objects, 211 tables, 1.6K textures, randomized lighting and camera parameters calibrated to real cameras. Data contains explicit grasps, joint trajectories, object poses and scene-graph annotations, and is used to initialize action representations and improve mid-training robustness.",
            "target_task_name": "Simulated pick-and-place and generalizable instruction-following manipulation (mid-training and augmentation for fine-tuning on 200 pick-and-place tasks).",
            "target_task_description": "Diverse pick-and-place episodes in Isaac Sim with random layouts and validated successful executions; observation: two RGB images (third-person + wrist), actions: closed-loop joint trajectories, objects: thousands of synthetic assets.",
            "semantic_alignment": "Designed to align with downstream embodied tasks by including object poses, grasps, and validated trajectories; cameras are calibrated to match real-world camera intrinsics/extrinsics to reduce sim-to-real gap.",
            "performance_with_language_pretraining": "Using InternData M1 for mid-training increases average performance of InternVLA-M1 across 200 tasks by +6.2% over GR00T N1.5 and reduces variance in few-demo fine-tuning regimes. Also reported to yield +20.6% improvement on unseen objects in real clustered pick-and-place when used for synthetic co-training.",
            "performance_without_language_pretraining": "Not directly reported; the dataset is a mid-training augmentation and not a model itself. Paper contrasts w/o mid-train variants where models are fine-tuned with only five trajectories per task.",
            "sample_efficiency_comparison": "InternData M1 + mid-training yields substantially better few-demo generalization (fine-tuning with 5 demos) than no mid-training; quantified average gain +6.2% across tasks. Authors report mid-train variant consistently surpasses baselines.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Not applicable (dataset).",
            "embedding_space_analysis": "Not applicable (dataset) though used to shape representation alignment in mid-training.",
            "action_grounding_evidence": "Dataset provides action-conditioned supervision (joint states, object poses, end-effector traces) enabling effective learning of embodiment-specific motor commands and better grounding of spatial VLM priors into actions (empirical SR improvements when used).",
            "hierarchical_features_evidence": "Used to initialize action head (System 1) so low-level control is learned from physically-validated actions while high-level spatial priors come from VLM pretraining (System 2); supports the paper's hierarchical claims.",
            "transfer_conditions": "Improves transfer especially for novel objects/configurations and camera/environment shifts when camera calibration and randomized visual factors are used to bridge sim-to-real.",
            "novel_vs_familiar_objects": "Helps with unseen-object generalization: +20.6% reported improvement on unseen objects in real clustered pick-and-place when synthetic co-training included.",
            "zero_shot_or_few_shot": "Enables stronger few-shot fine-tuning (5-demo regime), but zero-shot performance is not explicitly reported.",
            "layer_analysis": null,
            "negative_transfer_evidence": "Not reported; synthetic mid-training is reported to be beneficial.",
            "comparison_to_vision_only": "InternData M1 supplies action supervision beyond vision-only pretraining and is reported to complement limited real demos more effectively than vision-language pretraining alone.",
            "temporal_dynamics": "Not analyzed for dataset itself; dataset contains temporally-ordered closed-loop trajectories used to train temporal action models.",
            "dimensionality_analysis": null,
            "uuid": "e1918.3"
        },
        {
            "name_short": "DiT Actor / Diffusion Policy",
            "name_full": "DiT Actor (Diffusion policy) as Action Expert",
            "brief_description": "The action generation component used as the fast System 1 executor in InternVLA-M1: a diffusion policy / DiT actor modeling continuous joint-action distributions conditioned on visual-state and latent planner tokens.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DiT Actor (diffusion policy action expert)",
            "model_description": "An embodiment-aware diffusion policy (based on prior diffusion policy work) serving as the Action Expert; uses DINOv2 visual encoder (21M) plus a small state encoder and decodes continuous joint-space actions (delta joint control) in chunks (action chunk size up to 16). Designed to model multimodal action distributions and to be conditioned on VLM planner latent tokens via a querying transformer.",
            "pretraining_type": "Action-head initialization via post-pre-training on large-scale simulated action data (InternData M1) after VLM spatial pretraining.",
            "pretraining_data_description": "Pre-trained / post-pre-trained on the 244K closed-loop simulated samples (InternData M1) containing validated joint trajectories, object poses, grasps and scene graphs; action distributions encode motor patterns, grasps, and trajectory shape information.",
            "target_task_name": "Embodied continuous control for manipulation (pick-and-place, drawer opening, sandwich assembly, sorting, button pressing etc.)",
            "target_task_description": "Short- and long-horizon manipulation tasks on Franka/ARX LIFT2/WidowX/Google Robot simulated and real setups; outputs continuous joint commands (delta joint space control) executed in chunks; tasks include multi-step recipes requiring precise temporal and articulated manipulation.",
            "semantic_alignment": "Conditioned on spatially grounded VLM planner tokens to ensure action outputs align with high-level spatial/instruction semantics; co-training alternates between trajectory data and spatial grounding data to reinforce alignment.",
            "performance_with_language_pretraining": "When conditioned on spatially pretrained VLM planner tokens and mid-trained on InternData M1, DiT Actor yields higher SR and lower trajectory MAE vs baselines; exact numeric improvements are reported at system level (e.g., InternVLA-M1 total gains reported above).",
            "performance_without_language_pretraining": "Paper notes that initializing DiT Actor without the spatially-guided VLM conditioning or without post-pre-training yields worse transfer and slower convergence; no single-number given specifically for DiT alone.",
            "sample_efficiency_comparison": "Post-pre-training the action head with InternData M1 facilitates learning with few downstream demonstrations (5 per task) and yields improved generalization; authors report mid-train variant improves average by +6.2% across tasks vs baselines.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "The querying transformer maps latent planner embeddings into fixed queries that cross-attend to VLM layers; architecture-level attention coupling is discussed but no per-head attention visualizations shown.",
            "embedding_space_analysis": "Observed improved gradient/representation alignment (PSS increase from 0.25 to 0.42) when using spatially guided conditioning vs vanilla co-training, indicating more consistent optimization between DiT Actor objectives and spatial grounding.",
            "action_grounding_evidence": "Empirical: coupling DiT Actor with the spatial VLM yields improved physical execution (higher SRs in manipulation, lower trajectory MAE) and better adaptability under perturbation; suggests verbs/commands are grounded to motor trajectory distributions via the conditioned diffusion policy.",
            "hierarchical_features_evidence": "Designed as low-level executor (System 1) benefiting from high-level VLM (System 2) priors. Ablations in paper show that preserving VLM semantics (via gradient decay) is important for maintaining high-level grounding while training the DiT Actor.",
            "transfer_conditions": "Best when action head is post-pre-trained on diverse simulated trajectories and when VLM provides explicit spatial prompts; naive end-to-end gradients from action to VLM can harm transfer without gradient attenuation.",
            "novel_vs_familiar_objects": "When conditioned by spatial planner and trained with InternData M1, DiT Actor supports better handling of unseen objects in real clustered pick-and-place (+20.6% reported when used as part of the full system).",
            "zero_shot_or_few_shot": "Enables few-shot fine-tuning success in the 5-demo-per-task regime when combined with spatially pretrained VLM; zero-shot not shown separately.",
            "layer_analysis": "Querying transformer attends to intermediate VLM layers; authors note possibility to select which VLM layers are attended to (configurable n), but no detailed ablation of layer choice impact beyond architectural description.",
            "negative_transfer_evidence": "Unattenuated backward gradients from DiT to VLM can distort multimodal knowledge; authors introduce gradient decay factor (e.g., 0.5) to mitigate negative transfer.",
            "comparison_to_vision_only": "DiT Actor uses visual encoders (DINOv2) for low-level perception but benefits measurably more when conditioned on spatially pretrained VLM tokens (vision-language) than when trained in isolation on vision-only inputs, per system-level comparisons.",
            "temporal_dynamics": "Diffusion policy inherently models temporal trajectories; training removes idle/pause frames to improve temporal consistency. No explicit temporal representational evolution plots provided.",
            "dimensionality_analysis": "Not provided specifically for DiT Actor beyond PSS analysis at system level.",
            "uuid": "e1918.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 2
        },
        {
            "paper_title": "RT-1: Robotics transformer for real-world control at scale",
            "rating": 2
        },
        {
            "paper_title": "Genmanip: Llm-driven simulation for generalizable instruction-following manipulation",
            "rating": 2
        },
        {
            "paper_title": "Diffusion policy: Visuomotor policy learning via action diffusion",
            "rating": 2
        },
        {
            "paper_title": "GR00T N1: An open foundation model for generalist humanoid robots",
            "rating": 2
        },
        {
            "paper_title": "Qwen2.5-vl technical report",
            "rating": 2
        }
    ],
    "cost": 0.022528,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy
15 Oct 2025</p>
<p>Intern Robotics 
Shanghai Ai Laboratory 
InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy
15 Oct 202545CF65B8225EEB55D354F1E2C7F233A2arXiv:2510.13778v1[cs.RO]A: [List of 2D Trajectory Points] A: [(320280)(358280)(396290)]
We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instructionfollowing robots toward scalable, general-purpose intelligence.Its core idea is spatially guided vision-languageaction training, where spatial grounding serves as the critical link between instructions and robot actions.InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine "where to act" by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide "how to act" by generating embodiment-aware actions through plug-and-play spatial prompting.This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction.To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects.In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations.Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%.These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots.</p>
<p>Introduction</p>
<p>Large multimodal foundation models Bai et al. (2025b); Chen et al. (2024); Li et al. (2024b); Radford et al. (2021); Zhai et al. (2023) have demonstrated strong generalization by leveraging web-scale vision-language alignment and instruction-following corpora.To extend these capabilities into the physical domain, robots must not only understand what an instruction means but also determine where and how to act in the 3D world.This gap is fundamental.Textual abstractions capture spatial cues only indirectly, whereas real-world actions demand continuous, embodied interactions that are scarcely represented in the training data of vision-language models (VLMs).Teleoperated datasets Bu et al. (2025a); Collaboration et al. (2023); Khazatsky et al. (2024); Wu et al. (2024) provide valuable supervision; yet, their scale and diversity remain modest compared to large instruction-following corpora.In this context, an embodiment-agnostic spatial prior, which functions as a bridge between textual instructions and embodiment-specific motor commands, offers a promising foundation for scalable robot learning.</p>
<p>Prior work has approached this challenge through hierarchical robotic systems Cao et al. (2025); Huang et al. (2024aHuang et al. ( , 2023Huang et al. ( , 2024b)); Liu et al. (2024); Qi et al. (2025); Yuan et al. (2024), which explicitly encode spatial priors using foundation models Fang et al. (2023); Kirillov et al. (2023); Oquab et al. (2023) but often rely on rule-based task decomposition and manually designed planning heuristics.This rigid separation between symbolic task structures and low-level motor control makes such systems difficult to scale automatically to more complex and diverse tasks, particularly hindering end-to-end policy learning.In contrast, recent data-driven VLAs AI (2024); Black et al. (2024); Brohan et al. (2023); Kim et al. (2024); Lee et al. (2025); Shi et al. (2025) leverage pretrained vision-language models and large-scale teleoperation datasets Bu et al. (2025a); Collaboration et al.</p>
<p>Figure 1.InternVLA-M1 integrates spatial grounding into the vision-language-action training pipeline.Given a task instruction, the VLM planner produces latent plans through explicit spatial prompting, which then effectively guides the action expert to generate control signals.</p>
<p>(2023); Khazatsky et al. (2024); Wu et al. (2024) to directly learn robot control.However, these models tend to overfit fine-grained motor behaviors while under-generalizing to high-level linguistic instructions that involve absolute or relational positions, thereby failing to fully incorporate spatial priors into execution.Core spatial priors such as object recognition, affordance grounding, visual trajectory reasoning, relative localization, and scaling provide transferable knowledge across robotic platforms.Once these priors are established, embodiment-specific learning can focus on concrete control strategies (e.g., manipulator joints, end-effector trajectories, humanoid locomotion, or mobile navigation).Such a division clarifies the role of spatial priors as general-purpose foundations while leaving embodiment-specific details to downstream adaptation, thereby bridging the gap between abstract instruction following and grounded physical execution.</p>
<p>Building on the separation between spatial priors and embodiment-specific control, we introduce InternVLA-M1, a dual-system vision-language-action framework that unifies high-level reasoning with grounded execution.InternVLA-M1 consists of a spatial prior VLM planner that interprets linguistic instructions and reasons about spatial relations, and an action expert that translates these grounded representations into executable motor commands.To achieve this, we construct over 3M multimodal training samples, including 2.3M spatial grounding data and 0.7M multimodal understanding data collected from web, real-world, and simulated sources.To leverage the above pre-training data, we propose spatially guided two-stage training recipes: (i) spatial grounding pre-training for the VLM, which establishes transferable spatial understanding through large-scale multimodal supervision on points, boxes, and traces; and (ii) spatially guided action post-training, which specializes these priors for embodiment-specific control under joint supervision.This design bridges abstract goal reasoning with concrete physical execution, enabling robust instruction following across diverse and complex environments.</p>
<p>To validate these capabilities, we conduct a comprehensive evaluation across multiple benchmarks in both simulated environments and real-world settings.InternVLA-M1 demonstrates strong generalization and robust performance across diverse scenarios:</p>
<p>• On SimplerEnv (Google Robot and WidowX), InternVLA-M1 achieves a new state of the art, surpassing its variant by improving the average success rate by up to +5.9% and +9.8%, respectively.It also demonstrates strong spatial reasoning capabilities across box, point, and trace prediction tasks.Further analysis shows that spatially guided action post-training effectively transfers spatial reasoning ability to motor control.• For the generalizable pick-and-place 200 tabletop scenarios, our model exhibits strong generalization to unseen objects and instructions under few-demonstration fine-tuning, achieving an average improvement of 6.2% over prior works.• In real-world settings, InternVLA-M1 demonstrates strong instruction-following capability, achieving a +20.6% success rate on unseen objects and novel setups in clustered pick-and-place tasks.It also maintains robust long-horizon performance under perturbations (e.g., physical interference, task replanning), outperforming baselines such as GR00T and  0 by large margins.</p>
<p>InternVLA-M1</p>
<p>We propose InternVLA-M1, a dual-system, end-to-end vision-language-action (VLA) framework.It integrates both a language head and an action head within a single model (Section 2.1).The language head establishes instruction-to-visual grounding through spatial pretraining and co-training, while the action head conditions on these learned spatial priors to generate embodiment-specific motor commands(Section 2.2).This joint design bridges abstract linguistic goals with grounded execution, enabling robust instruction following across diverse and complex scenes.</p>
<p>Model Architecture</p>
<p>Dual-System.InternVLA-M1 is a dual-system, end-to-end VLA framework pre-trained on large-scale spatial grounding data collected from diverse sources.InternVLA-M1 employs the Qwen2.</p>
<p>Sub-Task Planning</p>
<p>Collect snacks.</p>
<p>[point]</p>
<p>Give the box coordinates according to the instruction… Your answer should be formatted as a list of tuples …</p>
<p>Based on the task description, predict the trajectory that the end effector should take… robot demonstration data, enabling it to specialize these priors into embodiment-specific motor commands.This dual-supervision strategy establishes a cohesive link between high-level semantic perception and low-level motion control, which is essential for robust instruction following in both simulation and real-world settings.</p>
<p>Latent planning via spatial prompting.To connect the VLM Planner with the action expert, we adopt a lightweight querying transformer (8.7 MB) conditioned on the latent planning embeddings produced by the VLM Planner.The querying transformer stabilizes expert learning and inference by mapping variable-length input tokens into a fixed set of learnable query tokens.It is implemented as a -layer cross-attention module, where the query tokens selectively attend to  intermediate layers of the VLM (e.g.,  = 1 attends only to the final layer).</p>
<p>To explicitly activate the spatial perception capability learned during spatial grounding pre-training, we employ spatial prompting.For instance, in general object manipulation tasks, we append simple prompts such as "Figure out how to execute it, then locate the key object needed."after the task instruction.The extracted feature embeddings provide the planner with explicit spatial cues that facilitate more reliable grounding.Motivated by prior studies Bjorck et al. (2025); Driess et al. (2025); Zhou et al. (2025b) showing that direct gradient flow between action and VLM modules may distort multimodal knowledge, we introduce a gradient decay factor within the querying transformer.This attenuates the gradients propagated from the Action Expert back to the VLM (e.g., by a factor of 0.5), thereby preserving the Planner's semantic reasoning ability while still enabling effective joint optimization.</p>
<p>Training Recipe</p>
<p>To leverage spatial priors for stronger embodiment-specific control in instruction following, InternVLA-M1 adopts a spatially guided two-stage training pipeline:</p>
<p>Stage 1: Spatial grounding pre-training.As shown in Figure 2, the first stage optimizes only the VLM.The objective is not generic vision-language pre-training, but stronger spatial reasoning and planning ability essential for robotics.We combine internet-scale multimodal corpora with robotspecific datasets such as RefCOCO, RoboRef It Lu et al. (2023b), A0 Xu et al. (2025b), MolmoAct Lee et al. (2025), and Pixmo-Points Deitke et al. (2024).All robot datasets are reformatted into a unified QA-style structure covering bounding-box detection, trajectory prediction, affordance recognition, and chain-of-thought reasoning.Aligning them with web-scale data enables training under the same supervised fine-tuning framework as conventional VLMs.</p>
<p>Stage 2: Spatially guided action post-training.In this stage, both the VLM and Action Expert are jointly optimized on demonstration data, ensuring semantic understanding and motion generation remain tightly integrated.Two strategies are employed:</p>
<p>• Spatial prompting.Before predicting actions, we prepend a spatial cue to the task instruction to elicit structured reasoning about object relationships and task constraints.For example, the instruction "store all toys into the toy box" can be augmented with: "Identify all relevant toys and their spatial relationships to the container."Although the VLM does not explicitly output a response to this auxiliary cue, its inclusion improves spatial awareness and generalization in manipulation tasks.• Co-training with spatial grounding data.Training alternates between robot trajectory data and grounding data.For trajectory data, both the VLM backbone and the action Expert are optimized with an L2 loss between predicted and ground-truth noise.For spatial grounding data, only the VLM backbone is updated via next-token prediction.This co-training scheme reinforces spatial reasoning while supporting efficient end-to-end optimization.</p>
<p>Data</p>
<p>This section introduces the datasets used in InternVLA-M1, covering pre-training, mid-training, and post-training stages.For VLM pre-training, we construct large-scale spatial grounding datasets with point, box, and trajectory annotations to enhance spatial perception and vision-language alignment.Mid-training employs synthetic manipulation data to bridge pre-training knowledge and robotic execution.Post-training uses both simulated and real-world instruction-following data, including large-scale tabletop tasks and real-robot demonstrations for long-horizon manipulation.</p>
<p>Spatial Grounding Data for Pre-training</p>
<p>The multimodal training dataset for our model comprises over 3M data, categorized into four distinct types: General QA, Box QA, Trajectory QA, and Point QA, as shown in Figure 3. Notably, more than 2.3M of these data are dedicated to spatial reasoning datasets.These categories ensure robust multimodal understanding while supporting adaptation to embodied tasks in tabletop robotic scenarios.Below, we describe each category:</p>
<p>• General QA.</p>
<p>Synthetic Data For Action Post-Pre-training</p>
<p>To bridge the gap between VLM and VLA, we introduce a Post-Pre-Training phase, where large-scale simulated data is used to pre-train the VLA after VLM pre-training.This stage initializes the action head and facilitates the learning of action representations.Post-Pre-Training requires maintaining diversity both at the instruction and object levels.Consistent with the InternVLA-M1-Interface Data, we leverage GenManip as our data synthesis pipeline to construct a large-scale pick-and-place dataset, the InternData M1 dataset, which comprises 244K closed-loop samples.Specifically, we adopt the same object set and positional distributions as in InternVLA-M1-Interface Data, and process them through our scalable data pipeline.Each synthesized sample is rigorously validated to ensure correctness and consistency.To further enhance visual diversity, we introduce controlled randomization in lighting conditions and texture mappings.The day is late, help me light the lantern now.</p>
<p>{"response": "Sure, lighting the lantern now.","subtask": "Ignite the lantern <box> [[398, 150, 426, 240]] </box>"} Question: Answer:</p>
<p>random camera</p>
<p>Scalable Synthetic Data Engine for Instruction-Following</p>
<p>To support large-scale end-to-end data generation for VLM pre-training, we build a highly scalable, flexible, and fully automated simulation pipeline on top of GenManip Gao et al. (2025) and Isaac Sim Makoviychuk et al. (2021).</p>
<p>Automatic task synthesis for generalizable pick-and-place.We develop a scalable simulation pipeline (shown in Figure 4) that generates diverse manipulation trajectories from randomized object layouts and lighting conditions.By leveraging privileged simulation signals including object poses, object meshes, and robot arm state, the system rapidly generates scene layouts via a scene graph solver and computes candidate grasps based on object meshes Liang et al. (2019).Each candidate trajectory is then executed once in physics for closed-loop verification, after which a scene-graph validator checks whether the task goals are achieved.Only trajectories that both execute successfully and pass validation are accepted, ensuring that all collected data are physically feasible and task-complete.</p>
<p>Synthesis of VLM data and VLA data for spatial grounding.For higher efficiency, robot planning and rendering are fully decoupled in our framework.The planner records structured scene and trajectory data, including joint states, object positions, and action information, which are later replayed by the renderer under randomized lighting, materials, and viewpoints.To align the simulation with real world, we calibrate all cameras using ArUco markers, ensuring that their intrinsic and extrinsic parameters match those of real-world cameras, thus maintaining consistent viewpoint geometry.In addition to high-resolution images, the renderer produces rich intermediate outputs, such as object bounding boxes and 2D end-effector trajectories.These signals provide dense supervision for action learning and facilitate the creation of auxiliary datasets for tasks such as spatial grounding, affordance reasoning, and trajectory prediction.Our asset library includes 14K annotated objects, 211 tables, 1.6K textures, and 87 dome lights, offering data with high visual and physical diversity-critical for developing generalizable models.</p>
<p>Experiments</p>
<p>We conducted extensive experiments to evaluate the performance of InternVLA-M1 in both simulation and real-world settings.First, we assess the performance on public simulated benchmarks (Section 4.1).Next, we fully evaluate the instruction-following of InternVLA-M1 for generalizable pickand-place using Isaac-Sim (Section 4.2).Finally, we examine real-robot performance on long-horizon manipulation tasks to study instruction-following in real-world deployment (Section 4.2.2).</p>
<p>Experiments on Public Benchmarks</p>
<p>We use two established simulation suites:</p>
<p>• SimplerEnv is designed to probe robustness to visual appearance shifts.It includes both WidowX and Google Robot platforms, short-horizon atomic tasks, and controlled changes in lighting, color, surface texture, and camera pose.We report results on three task sets: Google Robot-VM (visual matching under viewpoint and lighting changes), Google Robot-VA (visual aggregation with varying textures and colors), and WidowX-VM (cross-robot generalization).• LIBERO is a language-conditioned manipulation suite built on a Franka arm with diverse scenes and expert demonstrations.We evaluate four task sets: LIBERO-Spatial (same objects, different spatial layouts), LIBERO-Object (fixed layout, different objects), LIBERO-Goal (fixed objects and layout, different goals), and LIBERO-Long (also known as LIBERO-10; longer tasks that span multiple objects, layouts, and operations).The results demonstrate that omitting spatial data and spatially guided prompting during training leads to rapid degradation of spatial grounding capabilities and slower convergence in manipulation tasks.In contrast, spatially guided action post-training accelerates convergence, substantially improves manipulation success rates, and enhances spatial grounding accuracy as shown in Figure 5.</p>
<p>To further analyze the relationship between the spatial grounding objective and the action manipulation objective, we compute the Projection-space Similarity (PSS) Raghu et al. (2017) using Singular Value Decomposition (SVD).As shown in Figure 5(c), vanilla co-training of action data with spatial data yields a PSS of only 0.25, indicating significant misalignment between the gradient subspaces.In contrast, our spatially guided training approach increases the PSS to 0.42, demonstrating substantially improved optimization consistency.This enhanced alignment correlates with better preservation of spatial perception capabilities and faster convergence in manipulation tasks.We conduct a comprehensive study of VLA training strategies and their effects across three distinct task categories: multi-modal understanding, spatial grounding, and robot manipulation performance, which is a type of generalist VLA.Specifically, we evaluate: As shown in Table 3, our InternVLA-M1 achieves superior robotic manipulation performance while simultaneously preserving stronger multimodal understanding and spatial grounding capabilities compared to vanilla fine-tuning from VLM to VLA (Vanilla VLA) and direct co-training with spatial grounding data (vanilla co-train).Result analysis.The primary experimental results on the LIBERO benchmark are presented in Table 4.</p>
<p>Compared to previous strong baselines, such as GR00T N1 and  0 , the InternVLA-M1 framework achieves notable improvements, particularly on the spatial and long-horizon tracks, with success rates of 98.0% and 92.6%, respectively.These results demonstrate the efficacy of our proposed method in managing complex, multi-step manipulation tasks.Specifically, for object placement, InternVLA-M1 attains a 99.0%SR, which highlights its robust object grounding capability.</p>
<p>Experiments on Instruction-Following in In-house Environment</p>
<p>Evaluation in Simulated Large-scale Pick-and-place</p>
<p>Existing benchmarks such as SimplerEnv and LIBERO are limited in scale, which restricts the comprehensive evaluation of instruction-following manipulation in diverse and cluttered settings.To more rigorously assess generalization capabilities, we conduct an experimental study on a large-scale simulation evaluation with enhanced object diversity and layout variation.</p>
<p>Experimental setups.We constructed 200 pick-and-place tasks based on Isaac-Sim Gao et al. (2025), where the manipulated objects in each task are mutually distinct.Including background objects, the benchmark covers over 3K items and containers in total.Each task was executed once through the data generation pipeline to ensure its executability.Furthermore, for each of the 200 tasks, we additionally collected 5 trajectories with identical object sets but randomized layouts, which were used for post-training.The observation space comprises two RGB images: one captured from a fixed third-person viewpoint and the other from a first-person camera mounted on the Franka end-effector.Both images are resized to 224 × 224 before being fed into the model.We fine-tune the model on each suite independently using 16 A100 GPUs, with a total batch size of 256 and an action chunk size of 16.Training is conducted for 20K steps.Both our model and all baseline models are trained using delta joint space control.</p>
<p>Result analysis.As shown in Figure 6, we evaluate InternVLA-M1 under four generalization settings:</p>
<p>In-distribution, Unseen Object, New Background, and Unseen Instruction.For each setting, we report two variants of the model: w/o mid-train, which is fine-tuned using only five trajectories per task, and w/ mid-train, which is additionally mid-trained on InternData M1 prior to fine-tuning.The results, summarized in Figure 7, show that across all settings, both variants outperform the baseline  0 , while InternVLA-M1 w/ mid-train consistently surpasses GR00T N1.5.Although InternVLA-M1 w/o mid-train exhibits slight variance in certain settings, the mid-trained variant achieves a consistent advantage, with an average gain of +6.2% over GR00T N1.5.</p>
<p>The performance on unseen objects highlights the benefit of simulation-enhanced visual generalization, enabling the model to handle novel instances beyond the training distribution.When evaluated under new backgrounds with randomized textures and layouts, both variants maintain strong performance, and the improvements from mid-training indicate increased robustness to scenelevel shifts.Furthermore, under paraphrased instructions involving attribute-level or commonsense rewrites, InternVLA-M1 w/ mid-train demonstrates reliable instruction grounding, reflecting strong language generalization beyond templated expressions.</p>
<p>New Background Unseen Instruction</p>
<p>Transfer the item with the red lid on the barrel.</p>
<p>Drop the green object into the middle of shallow metal bowl.</p>
<p>Move the bottle to the top of the first aid kit.</p>
<p>Move the blue bottle to the top of the wooden barrel.</p>
<p>Unseen Object</p>
<p>Move the flower to the top of the bowl.</p>
<p>Move the microphone to the top of the microwave oven.</p>
<p>In-distribution</p>
<p>Move the yellow bottle to the top of the board.</p>
<p>Move the flashlight to the top of the speaker.</p>
<p>Evaluation in Real-world Cluttered-scene Pick-and-Place</p>
<p>Experimental setup.To evaluate our model's instruction-following capability in real-world scenarios, we employ a Franka Research 3 robotic arm equipped with a Robotiq 2F-85 gripper.The setup includes two Intel RealSense D435 cameras for RGB visual input-one mounted on the end-effector and another positioned at a rear, third-person perspective.We assess the model across a variety of manipulation tasks, including short-range pick-and-place, long-horizon object sorting, drawer opening/closing, and sandwich assembly.For quantitative evaluation, we design a real-world objectsorting benchmark consisting of single-horizon pick-and-place tasks within a 60 × 90 cm tabletop workspace.The benchmark features 23 seen objects and 5 seen containers (listed in Figure 8).In each episode, three containers are fixed at designated positions, while diverse objects are scattered randomly among them.The model must follow natural language instructions to pick specific objects and place them into the correct containers.To support post-training, we collect 6 hours of teleoperated demonstrations using only objects and containers from the predefined "seen" set.We compare two variants of InternVLA-M1, w/o co-train and w/ co-train, against GR00T N1.Evaluation settings.To evaluate generalization, we further partition all available object and container assets into disjoint seen and unseen sets, as illustrated in Figure 8.Only the seen set is included in the training data, while both seen and unseen sets are evaluated during testing to measure the model's ability to generalize to novel objects.As shown in Figure 9, we evaluate instructionfollowing capabilities of various models on real-world pick-and-place tasks under the below settings:</p>
<p>In-Distribution, Unseen Objects, Unseen Object Position, Unseen Object Orientation, and Unseen Instructions.We report success rate, defined as the fraction of trials in which the specified object is placed into the designated container.Higher SR indicates better performance.For each model, we conducted a total of 300 rollout evaluations.Each trial corresponds to one or more testing settings, and we ensured that each individual setting was evaluated at least 50 times.To ensure fair comparisons across models, we fixed the positions of the objects and containers for each task during testing.</p>
<p>Result analysis.</p>
<p>As shown in Figure 10, both variants of InternVLA-M1 demonstrate superior performance under the in-distribution setting, consistently outperforming GR00T N1.5 and  0 when evaluated on objects and containers seen during training.This indicates strong instruction-following capabilities within familiar contexts.Beyond this, the inclusion of Interndata-M1 during co-training significantly enhances the model's visual generalization, enabling improved performance on novel objects not encountered during training.This suggests that synthetic data serves as an effective complement to limited real-world demonstrations.Additionally, because real-world data collection cannot exhaustively cover the spatial workspace, simulation data enriches the distribution of object positions and orientations.This leads to substantially better generalization to unseen configurations in terms of both object placement and pose.Finally, InternVLA-M1 maintains robust performance when given novel instructions, highlighting its ability to generalize across diverse linguistic expressions</p>
<p>Evaluation in Long-horizon and Reasoning Manipulation</p>
<p>A key strength of our dual-system framework is its ability to leverage a high-level planner (System 2) to decompose long-horizon, reasoning-heavy tasks into a sequence of atomic actions, which are then robustly executed by a low-level action model (System 1).To evaluate this capability, we design a series of tasks that require not only multi-step planning but also the ability to reason about object attributes, monitor progress, and adapt to changes.As illustrated in Figure 11, these include:</p>
<p>• Desktop Sorting.The Franka robot is tasked with sorting objects into containers based on highlevel semantic categories, aiming to ensure that all items on the desktop are eventually placed into the correct containers.Both objects and containers are scattered within a 60×90 cm region in front of the robot base.The setup includes five seen containers and five object categories: fruits, toys, vegetables, bottles, and snacks.Each evaluation instance involves sorting objects from one to three categories into their respective containers.Each trial consists of three pick-and-place actions, and we report success rates consistent with the metric used for pick-and-place under clustered environments.• Sorting Items into Drawers.The Franka robot is required to (i) open a designated drawer (either lower or upper), (ii) place the target objects into it, and (iii) close the drawer.This task demands precise temporal reasoning and articulated manipulation.The objects are placed within a 35×35 cm area located to the front-right of the robot base.We report stepwise execution success, where a step is considered valid only if all preceding steps have succeeded.• Making Sandwiches.The Franka robot is instructed to assemble sandwiches following a predefined meal recipe.Ingredients and plates are placed within a 50×70 cm region in front of the robot base.We define five types of sandwich recipes as the seen set: [ bread-lettuce-bread ], [ bread-lettucemeat-bread ], [ bread-meat-lettuce-meat-bread ], [ bread-meat-meat-bread ], and [ bread-meatbread ].We report success rates on both the seen set and an unseen set involving real-time environment interaction, using the same success definition as in the drawer sorting task.• Math Calculation.The Franka robot is prompted to solve a math problem and press the color-coded button (red, yellow, or blue) that corresponds to the correct answer based on arithmetic reasoning.The buttons are randomly placed within a 40×40 cm area in front of the robot base.• Goods Purchase.The ARX LIFT2 dual-arm robot is tasked with identifying and placing into a basket the object bearing the correct price tag, given a numerical cue ranging from 1 to 9. We report the success rate of correctly placing the item corresponding to the queried price into the basket.</p>
<p>Experimental setup.To support fine-grained training for these long-horizon tasks, we collect a total of 22 hours of high-quality long-horizon and reasoning teleoperated demonstrations, amounting to approximately 500 demonstrations per task.Each collected trajectory is segmented into subtasks and annotated with corresponding atomic actions.For example, a "make a classic sandwich" task is decomposed into four subtasks: (1) "Put a piece of bun on the plate."→ (2) "Put a piece of meat on the plate."→ (3) "Put a piece of lettuce on the plate."→ (4) "Put a piece of bun on the plate."Put a piece of lettuce on the plate.</p>
<p>Put a piece of bun on the plate.</p>
<p>Stop.Put a piece of bun on the plate.</p>
<p>Long-horizon sorting: Sort snacks, toys, and fruits into respective bins.(6-14 steps)</p>
<p>Put all the snacks into the white basket.</p>
<p>Put all the fruits into the wooden basket.</p>
<p>Start.</p>
<p>Stop.</p>
<p>Put all the toys into the brown basket.</p>
<p>Put all the toys into the brown basket.Each sub-instruction is paired with a specific segment of the demonstration.To enable subtask-level transition, we introduce zero-action vectors padding after each subtask segment.This allows the model to stop upon subtask completion and then be prompted to predict the transition to the next subtask.In addition, to improve temporal consistency and ensure smooth inference, we remove frames in which the robot arm exhibits clear pauses or idle behavior.In contrast to prior VLA models that depend on an additional VLM to serve as a task planner for long-horizon or reasoning-intensive tasks, our unified model architecture is trained jointly on multimodal inputs encompassing task decomposition, subtask identification, numerical reasoning, and action supervision.This joint training paradigm enables a single model to seamlessly integrate task planning, reasoning, and action prediction in an   Evaluation settings.We evaluate model performance under three distinct settings: In-distribution, Physical Interference, and Task Replanning:</p>
<p>• Physical interference.External disturbances are introduced during task execution.For example, during the sorting items into drawers task, the drawer is manually closed after the robot opens it, or the target object is displaced during grasping.This evaluates the model's ability to perceive environmental changes and adapt accordingly.• Task replanning.New instructions are issued mid-execution.For instance, after placing an object in the drawer but before closing it, the robot is told: "Also put the cow toy into the top drawer."This tests the model's ability to incorporate new subgoals and dynamically adjust its plan.</p>
<p>Results analysis.</p>
<p>As shown in Figure 12, across long-horizon tasks, InternVLA-M1 consistently outperforms the baselines, enabled by its unified subtask planning mechanism.In the in-distribution setting, it achieves more reliable execution than GR00T N1.5 and  0 , showing stronger grounding of high-level goals into actionable steps.Under physical interference, the model demonstrates robust adaptability: for example, in desktop sorting when containers are unexpectedly moved, InternVLA-M1 can track the new container locations and complete the placement.Moreover, when task replanning is required, such as when additional instructions are introduced during execution, InternVLA-M1 is able to revise its subtask sequence on the fly and continue with correct actions.This adaptability leads to minimal performance degradation under stress conditions, while the baselines exhibit much larger declines, underscoring the model's resilience to dynamic environments and shifting instructions.2025) unify perception and planning by outputting not only plans but also affordance predictions as bounding boxes.For tasks requiring higher precision, specialized models such as RoboRefer Zhou et al. (2025a) employ dedicated architectures and reinforcement learning to predict exact 3D coordinates from complex spatial language.In contrast, our method provides a unified latent modeling framework that integrates spatial guidance into downstream action training, enabling end-to-end optimization with direct feedback from real-world deployment.</p>
<p>Related work</p>
<p>Embodied reasoning and planning in VLA.Chain-of-Thought prompting has proven effective for improving reasoning in large language models Wei et al. (2022), and its success has inspired extensions to embodied AI.In Vision-Language-Action (VLA) models, generating intermediate reasoning steps before acting enables agents to handle complex, long-horizon tasks.Early approaches emphasized linguistic reasoning: ECOT Zawalski et al. (2024) 2025) exemplify this forward-predictive approach to decision-making.Our model adopts a typical dual-system approach, building upon the VLA with unified architectures, then introducing additional planning design, thereby achieving better adaptability to real-world environments.</p>
<p>Discussion and conclusion</p>
<p>In this work, we presented InternVLA-M1, a unified vision-language-action framework that leverages spatial grounding priors to bridge high-level multimodal reasoning with low-level robotic execution.By combining large-scale multimodal pre-training with spatially guided post-training, our model effectively transfers spatially grounded understanding into embodied control, achieving strong generalization to unseen objects, instructions, and environments.Extensive evaluations across simulation and real-world settings demonstrate that InternVLA-M1 surpasses existing VLA models and specialized systems in instruction following, long-horizon manipulation, and multimodal grounding, highlighting spatial reasoning as a unifying substrate for scalable and reliable generalist robots.</p>
<p>Figure 2 .
2
Figure 2. Overview of InternVLA-M1.InternVLA-M1 adopts a spatially guided two-stage training pipeline.Stage 1 (spatial grounding pre-training): the VLM is trained on large-scale multisource multimodal spatial grounding data to learn embodiment-agnostic spatial priors.Stage 2 (spatially guided action post-training): the VLM Planner, functioning as a slow but reliable System 2 reasoner, generates latent planning tokens via spatial prompting as the condition to the action expert (instantiated as a DiT Actor) to execute as a fast System 1 controller.</p>
<p>Figure 3 .
3
Figure 3. Overview of the pre-training data for the vision-language model.The data comprises two main parts: general VQA data to maintain the model's general multimodal capabilities, and spatial VQA data focusing on robotic-related grounding and spatial perception in a VQA format.</p>
<p>": "Sure thing.","subtask": "Pick up the white bottle <box> [8, 176, 106, 294]] </box> and open it."}Simulation for generalizable pick-place: Put <Obj1> to the <Relation> of <Obj2></p>
<p>Figure 4 .
4
Figure 4. Simulation data synthesis pipeline.The pipeline generates diverse robotic manipulation data from a large asset library, converts intermediate representations into VQA data, and separates physics from rendering to reduce wasted failures and improve efficiency.</p>
<p>Figure 5 .
5
Figure 5. Ablation study on the effect of auxiliary spatial prompting for co-training robot manipulation with spatial grounding.From left to right: (a) spatial grounding performance (IoU@0.5 on RefCOCOg); (b) manipulation performance (SimplerEnv-WidowX SR); (c) shows the gradient similarity of the spatial grounding and manipulation objectives.</p>
<p>•</p>
<p>Multi-modal understanding: MME Zhang et al. (2021), MMVet Yu et al. (2023), TextVQA Singh et al. (2019), POPE Li et al. (2023), COCO Caption Chen et al. (2015) • Spatial grounding: RefCOCO-g Mao et al. (2016) (Box IoU0.5),Refit-testB Lu et al. (2023a) (Box IoU0.5),Where2Place Yuan et al. (2024) (evaluated by accuracy of predicted points with respect to ground-truth free space), and A0-maniskill Gu et al. (2023b) (evaluated using trajectory MAE, measuring mean absolute error between predicted and reference waypoints).• Robot manipulation: Google-Robot Visual Matching (VM), Variant Aggregations (VA), and WindowX Visual Matching (VM).</p>
<p>Figure 6 .Figure 7 .
67
Figure 6.Evaluation settings for generalizable pick-and-place in large-scale simulation.</p>
<p>Figure 8 .
8
Figure 8. Overview of objects and containers used in instruction-following pick-and-place.</p>
<p>Figure 9 .
9
Figure 9. Evaluation settings showcase for real-world instruction-following manipulations.</p>
<p>Figure 10 .
10
Figure 10.Result comparison in real-world instruction-following pick-and-place.</p>
<p>customized tasks: Make a classic sandwich and add another meat.(6-12 steps)</p>
<p>-</p>
<p>9 x 7 =• -72 • -63 • -54 Press the blue botton.Task replanning: Sort the brush into a closed drawer, then sort another hippo toy on sudden human request.(2-8 steps)Collect the brush to the upper drawer.Open the upper drawer.Close the upper drawer.Clear the hippo toy to the upper drawer., also clear the hippo toy into the drawer.</p>
<p>Figure 11 .
11
Figure 11.Showcase for long-horizon instruction-following manipulation.</p>
<p>Figure 12 .
12
Figure 12.Result comparison in real-world long-horizon task planning for manipulation.</p>
<p>elicits explicit text-based plans and sub-tasks to enhance performance and interpretability; RT-HBelkhale et al. (2024) introduces a fine-grained "action language" for hierarchical policies and human intervention; InstructVLAYang et al. (2025b) jointly optimizes reasoning and action through VLA-IT, improving generalization; OneTwoVLALin et al. (2025) adaptively alternates between "thinking" and execution;RAD Clark et al. (2025)  leverages action-free human videos to derive reasoning guides; and  0.5Intelligence et al. (2025) trains on heterogeneous data before fine-tuning for subtask prediction.Later work has also explored visual and spatial modalities, such as graph-based representations for spatial reasoningHuang et al. (2025a).Despite their differences, these approaches all generate intermediate steps such as textual, visual, or spatial representations during inference.While effective, this adds computational overhead.In contrast, we propose a post-training phase that directly unlocks the VLM's intrinsic reasoning capacity, removing the need for explicit generative reasoning.Generalistrobot policy.Recent research in general-purpose robotics has seen the emergence of several mainstream technical paradigms.Monolithic VLA models utilize a single end-to-end network to directly map multimodal inputs to tokenized low-level actions, as demonstrated by systems Brohan et al. (2023); Kim et al. (2024); Lee et al. (2025); Yang et al. (2025a).In contrast, unified architectures decouple high-level cognition from low-level action, allowing for greater modularity and interpretability.This category has seen extensive exploration Black et al. (2024); Li et al. (2025a, 2024c) leveraging specialized generative models for action synthesis.Other notable approaches in this vein Cheang et al. (2025); Intelligence et al. (2025); Shukor et al. (2025); Song et al. (2025); Yang et al. (2025b);Zhou et al. (2025b), which uses an LLM to break down high-level language commands into intermediate action plans.A third paradigm is based on world models, which learn a predictive model of the environment's dynamics to enable planning and control.These models allow for simulating future outcomes, often facilitating planning via search in a learned latent space or by conditioning a separate policy.While powerful, this approach can be computationally intensive.Representative works Bjorck et al. (2025); Bu et al. (2025b); Cen et al. (2025); Li et al. (2025b); Liao et al. (2025); Lv et al. (2025); Tian et al. (2024); Wang et al. (2025); Ye et al. (</p>
<p>Spatially Guided Action Post-training VLM -Planner Stage 1 | Spatial Grounding Pre-training Multimodal understanding robot observation</p>
<p>During inference, the system runs on a single RTX 4090 GPU with around 12 GB of memory usage.With FlashAttention, the VLM component achieves inference speeds of approximately 10 FPS.Action execution can be further accelerated via chunking and KV caching.
Spatial Grounding(box / point / trace)Dual-Supervision. The dual-system architecture supports both multimodal supervision and actionsupervision during training. In each training step, batches from both data types are jointly pro-cessed, and the model computes losses from the two supervision signals. The resulting gradients areaggregated and applied in a single optimization update, ensuring that perception and control are
Chi et al. (2023))i et al. (2025a)as the multimodal encoder for System 2, which is to capture spatial priors.It adopts the diffusion policyChi et al. (2023)(86 M) as the Action Expert (System 1, the fast executor), which effectively models embodiment-specific control.This expert is built on the DINOv2 visual encoder Oquab et al. (2023) (21 M) and a lightweight state encoder (0.4 M), forming a compact vision-action model.In total, InternVLA-M1 comprises approximately 4.1B parameters.co-adapted rather than learned in isolation.Specifically, the VLM planner is aligned with a broad range of spatial grounding data, both real and synthetic, covering tasks such as object detection, affordance recognition, and visual trajectory planning.In parallel, the Action Expert is trained on Stage 2 |</p>
<p>multi-modal web data [box] [trace] VLM real &amp; sim robot data Organize the table. Noisy Actions Actions DiT -Actor Conditioned State (opt)</p>
<p>Your task is to {instruction}. Figure</p>
<p>out how to execute it, then locate the key object needed.</p>
<p>Table 2 .
2
Li et al. (2024c)4)-of-the-art open VLA systems, including  0Black et al. (2024),GR00T Bjorck et al. (2025), OpenVLAKim et al. (2024),CogACT Li et al. (2024c), and etc.We also include a Vanilla VLA built on QwenVL-2.5-3B-InstructwithaDiTactionhead.When available, we use official reported numbers; otherwise, we reimplement and mark such entries with * .We keep training data, observation spaces, and action type aligned with the most popular setupsLi et al. (2024c)to ensure a fair comparison.As described in Section 2.2, we post-train InternVLA-M1 on a subset of Open-X Embodiment (OXE) (including fractal_rt_1 and bridge_v1), with co-training on spatial grounding data (Section 3.1).The VLM takes the primary observation image, task instruction, and an auxiliary spatial prompt as input, while the action expert predicts actions with an action chunk size of 16.For multimodal data, the model follows an SFT-style question-answering format.Training is performed on 16 NVIDIA A100 GPUs for 50k steps (around 2.5 epochs), with total batch sizes of 256 for robot data and 64 for multimodal data, optimized with a summed loss over both data types.All evaluations are conducted within SimplerEnv using its official evaluation protocol.Result comparisons of robotic manipulation on SimplerEnv (WidowX) benchmark.The underlined scores indicate the best results excluding InternVLA-M1.The main experimental results are presented in Table1 and Table 2. Compared with prior state-of-the-art models, it attains a 5.9% gain in Google Robot Visual Matching, a 5.3% gain in Visual Aggregation, and a 9.8% gain on the WidowX benchmark.These results highlight the strong competitiveness of InternVLA-M1 within the community.Compared to the Vanilla VLA based on Qwen2.5-VL-3B-Instruct,InternVLA-M1 achieves substantial improvements: a 14.6% increase in Google Robot Visual Matching and a 12.4% increase in Visual Aggregation, along with a 17.0% improvement on the WidowX benchmark.These results demonstrate the effectiveness of our spatially guided pre-training and action post-training strategies.
Google RobotModelsCo-TrainPick Coke CanMove NearOpen/Close DrawerOpen Top Drawer and Place AppleAvgRT-1 Brohan et al. (2022)✗85.744.273.06.552.4RT-1-X Collaboration et al. (2023)✗56.731.759.721.342.4RT-2-X Brohan et al. (2023)✓78.777.925.03.746.3OpenVLA Kim et al. (2024)✗18.056.363.00.034.3VisualCogACT Li et al. (2024c)✗91.385.071.850.974.8MatchingSpatialVLA Qu et al. (2025)✗86.077.957.4-75.1𝜋 0 Black et al. (2024)✗72.765.338.3-58.8𝜋 0 -FAST Pertsch et al. (2025)✗75.367.542.9-61.9GR00T N1.5  *  Bjorck et al. (2025)✗51.754.027.87.435.2Magma Yang et al. (2025a)✓83.765.456.06.452.9Vanilla VLA✗90.069.852.552.266.1InternVLA-M1✓95.390.075.562.080.7Δ+5.3+20.2+23.0+9.8+14.6RT-1 Brohan et al. (2022)✗89.850.032.32.643.7RT-1-X Collaboration et al. (2023)✗49.032.329.410.130.2RT-2-X Brohan et al. (2023)✓82.379.235.320.654.4OpenVLA Kim et al. (2024)✗60.867.728.80.039.3VariantCogACT Li et al. (2024c)✗89.680.828.346.661.3AggregationSpatialVLA Qu et al. (2025)✗88.082.541.8-70.7𝜋 0 Black et al. (2024)✗75.263.725.6-54.8𝜋 0 -FAST Pertsch et al. (2025)✗77.668.231.3-59.0GR00T N1.5 Bjorck et al. (2025)✗69.368.735.84.044.5Magma Yang et al. (2025a)✓68.865.753.418.551.6Vanilla VLA✗92.380.350.131.463.5InternVLA-M1✓86.182.072.064.076.0Δ-6.2+1.7+21.9+32.6+12.5
Table 1.Result comparisons of robotic manipulation on SimplerEnv (Google-Robot) benchmark.The underlined scores indicate the best results excluding InternVLA-M1.Numbers are officially reported; otherwise, we reimplement and mark such entries with * .Baselines.</p>
<p>Table 3 .
3
Kim et al. (2025) VLA training strategies across multi-modal understanding, spatial grounding, and robotic manipulation tasks.FollowingKim et al. (2025), we filter out failed demonstrations and pause frames.During training, the policy takes as input both wrist-mounted and third-person camera views.We fine-tune the model on each suite independently using 8 A100 GPUs with a batch size of 128 and an action chunk size of 8. Training runs for roughly 30K steps, lasting about 20 hours.Each suite is evaluated with 500 trials.
Multi-modal UnderstandingSpatial GroundingRobotic ManipulationModelsMME MMVet TextVQA POPECOCO Caption BLEU/ROUGERefCOCO-g Box IoU 0.5 ↑Refit-testB Box IoU 0.5 ↑Where2place Point Acc↑A0-maniskill Traj. MAE ↓Google Robot VM/VAWindowX VMVanilla VLA---------66.1/63.554.7Vanilla co-train 110619.220.578.010.4/15.147.166.721.46.470.2/66.561.1InternVLA-M1141123.328.686.213.0/13.471.274.325.55.180.7/76.071.7</p>
<p>Table 4 .
4
Result comparisons of robotic manipulation on LIBERO (Franka) benchmark.</p>
<p>Table 5, despite GPT-5's strong reasoning capability, additional post-training of the unified model notably improves performance on long-horizon and reasoning-intensive tasks, underscoring the necessity of post-training for effective high-level task planning.
Long-horizon tasksReasoning tasksModelsSort intoMakeDesktopMathGoodsDrawersSandwichesSortingcalculationPurchaseGemini-2.5 Pro5762835361GPT-57567627982GPT-4o3757353941Qwen2.5-VL-72B3171343329Qwen2.5-VL-3B3049524138Ours-3B9091919392</p>
<p>Table 5 .
5
Task scheduling performance of VLM planner in long-horizon and reasoning scenarios.</p>
<p>Luo et al. (2025)5b)24)m.A key challenge in embodied AI is bridging high-level instructions with low-level actions, often addressed by generating intermediate representations (IRs) that range from formal symbolic structures to learned embeddingsXie et al. (2019).Inspired by Chain-of-Thought (CoT) reasoning, many approaches train vision-language-action (VLA) models to generate explicit textual plans before acting, which improves both interpretability and performance on complex tasksZawalski et al. (2024).Beyond textual plans, research has explored more structured or physically grounded IRs.Historically, systems relied on direct perceptual outputs, such as bounding boxes from object detectors for manipulationGriffin (2023), specific 3D points for grasp planning from point clouds TenPas and Platt (2017), or dense correspondence fields derived from self-supervised features learned for control, such as DINO featuresLaskin et al. (2020);Nair et al. (2022).Some methods construct persistent 3D scene graphs as comprehensive world models that LLMs can query to ground long-horizon plansRana et al. (2023).Others emphasize action-centric IRs, for example by conditioning policies on visual affordances that specify the robot's end-effector pose at key moments in a taskNasiriany et al. (2024).A growing trend involves generating explicit spatial localizers directly consumable by robot controllersGu et al. (2023a);Huang et al. (2025b);Li et al. (2025c).Large-scale foundation modelsLuo et al. (2025);Team et al. (</p>
<p>A. Author contributionsAll contributors are listed in alphabetical order by their last names.A.1. Core ContributorsYilun Chen, Ning Gao, Jiangmiao Pang, Bolun Wang, Fangjing Wang, Jinhui Ye, Junqiu Yu, Jinyu Zhang, Yangkun ZhuA.2. ContributorsXinyi Chen, Yanwei Fu, Jiaya Jia, Weiyang Jin, Hao Li, Yao Mu, Yu Qiao,Yang Tian, Bin Wang, Hanqing Wang, Tai Wang, Ziqin Wang, Xueyuan Wei, Chao Wu, Shuai Yang, Jia Zeng, Jingjing Zhang, Shi Zhang, Feng Zheng, Bowen Zhou
. F Ai Helix, 2024</p>
<p>S Bai, K Chen, X Liu, J Wang, W Ge, S Song, K Dang, P Wang, S Wang, J Tang, H Zhong, Y Zhu, M Yang, Z Li, J Wan, P Wang, W Ding, Z Fu, Y Xu, J Ye, X Zhang, T Xie, Z Cheng, H Zhang, Z Yang, H Xu, J Lin, . , arXiv:2502.13923Qwen2.5-vl technical report. 2025aarXiv preprint</p>
<p>S Bai, K Chen, X Liu, J Wang, W Ge, S Song, K Dang, P Wang, S Wang, J Tang, arXiv:2502.139235-vl technical report. 2025barXiv preprint</p>
<p>S Belkhale, T Ding, T Xiao, P Sermanet, Q Vuong, J Tompson, Y Chebotar, D Dwibedi, D Sadigh, arXiv:2403.01823Rt-h: Action hierarchies using language. 2024arXiv preprint</p>
<p>J Bjorck, F Castañeda, N Cherniadev, X Da, R Ding, L Fan, Y Fang, D Fox, F Hu, S Huang, arXiv:2503.14734Gr00t n1: An open foundation model for generalist humanoid robots. 2025arXiv preprint</p>
<p>\𝑝𝑖 0 : A vision-language-action flow model for general robot control. K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems. Q Bu, J Cai, L Chen, X Cui, Y Ding, S Feng, S Gao, X He, X Hu, X Huang, arXiv:2503.066692025aarXiv preprint</p>
<p>Q Bu, Y Yang, J Cai, S Gao, G Ren, M Yao, P Luo, H Li, arXiv:2505.06111Univla: Learning to act anywhere with task-centric latent actions. 2025barXiv preprint</p>
<p>Robobrain 2.0 technical report. M Cao, B R Team, arXiv:2507.02029Beijing Academy of Artificial Intelligence. 2025arXiv preprint</p>
<p>J Cen, C Yu, H Yuan, Y Jiang, S Huang, J Guo, X Li, Y Song, H Luo, F Wang, arXiv:2506.21539Towards autoregressive action world model. 2025arXiv preprint</p>
<p>C Cheang, S Chen, Z Cui, Y Hu, L Huang, T Kong, H Li, Y Li, Y Liu, X Ma, arXiv:2507.15493Gr-3 technical report. 2025arXiv preprint</p>
<p>Microsoft coco captions: Data collection and evaluation server. X Chen, H Fang, T.-Y Lin, 2015</p>
<p>Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. Z Chen, J Wu, W Wang, W Su, G Chen, S Xing, M Zhong, Q Zhang, X Zhu, L Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. C Chi, RSSS Feng, RSSY Du, RSSZ Xu, RSSE Cousineau, RSSB Burchfiel, RSSS Song, RSSProceedings of Robotics: Science and Systems. Robotics: Science and Systems2023</p>
<p>Action-free reasoning for policy generalization. J Clark, S Mirchandani, D Sadigh, S Belkhale, arXiv:2502.037292025arXiv preprint</p>
<p>O X -E. Collaboration, A O'neill, A Rehman, A Gupta, A Maddukuri, A Gupta, A Padalkar, A Lee, A Pooley, A Gupta, A Mandlekar, A Jain, A Tung, A Bewley, A Herzog, A Irpan, A Khazatsky, A Rai, A Gupta, A Wang, A Kolobov, A Singh, A Garg, A Kembhavi, A Xie, A Brohan, A Raffin, A Sharma, A Yavary, A Jain, A Balakrishna, A Wahid, B Burgess-Limerick, B Kim, B Schölkopf, B Wulfe, B Ichter, C Lu, C Xu, C Le, C Finn, C Wang, C Xu, C Chi, C Huang, C Chan, C Agia, C Pan, C Fu, C Devin, D Xu, D Morton, D Driess, D Chen, D Pathak, D Shah, D Büchler, D Jayaraman, D Kalashnikov, D Sadigh, E Johns, E Foster, F Liu, F Ceola, F Xia, F Zhao, F V Frujeri, F Stulp, G Zhou, G S Sukhatme, G Salhotra, G Yan, G Feng, G Schiavi, G Berseth, G Kahn, G Yang, G Wang, H Su, H.-S Fang, H Shi, H Bao, H B Amor, H I Christensen, H Furuta, H Bharadhwaj, H Walke, H Fang, H Ha, I Mordatch, I Radosavovic, I Leal, J Liang, J Abou-Chakra, J Kim, J Drake, J Peters, J Schneider, J Hsu, J Vakil, J Bohg, J Bingham, J Wu, J Gao, J Hu, J Wu, J Wu, J Sun, J Luo, J Gu, J Tan, J Oh, J Wu, J Lu, J Yang, J Malik, J Silvério, J Hejna, J Booher, J Tompson, J Yang, J Salvador, J J Lim, J Han, K Wang, K Rao, K Pertsch, K Hausman, K Go, K Gopalakrishnan, K Goldberg, K Byrne, K Oslund, K Kawaharazuka, K Black, K Lin, K Zhang, K Ehsani, K Lekkala, K Ellis, K Rana, K Srinivasan, K Fang, K P Singh, K.-H Zeng, K Hatch, K Hsu, L Itti, L Y Chen, L Pinto, L Fei-Fei, L Tan, L J Fan, L Ott, L Lee, L Weihs, M Chen, M Lepert, M Memmel, M Tomizuka, M Itkina, M G Castro, M Spero, M Du, M Ahn, M C Yip, M Zhang, M Ding, M Heo, M K Srirama, M Sharma, M J Kim, N Kanazawa, N Hansen, N Heess, N J Joshi, N Suenderhauf, N Liu, N D Palo, N M M Shafiullah, O Mees, O Kroemer, O Bastani, P R Sanketi, P T Miller, P Yin, P Wohlhart, P Xu, P D Fagan, P Mitrano, P Sermanet, P Abbeel, P Sundaresan, Q Chen, Q Vuong, R Rafailov, R Tian, R Doshi, R Mart'in-Mart'in, R Baijal, R Scalise, R Hendrix, R Lin, R Qian, R Zhang, R Mendonca, R Shah, R Hoque, R Julian, S Bustamante, S Kirmani, S Levine, S Lin, S Moore, S Bahl, S Dass, S Sonawani, S Tulsiani, S Song, S Xu, S Haldar, S Karamcheti, S Adebola, S Guist, S Nasiriany, S Schaal, S Welker, S Tian, S Ramamoorthy, S Dasari, S Belkhale, S Park, S Nair, S Mirchandani, T Osa, T Gupta, T Harada, T Matsushima, T Xiao, T Kollar, T Yu, T Ding, T Davchev, T Z Zhao, T Armstrong, T Darrell, T Chung, V Jain, V Kumar, V Vanhoucke, W Zhan, W Zhou, W Burgard, X Chen, X Chen, X Wang, X Zhu, X Geng, X Liu, X Liangwei, X Li, Y Pang, Y Lu, Y J Ma, Y Kim, Y Chebotar, Y Zhou, Y Zhu, Y Wu, Y Xu, Y Wang, Y Bisk, Y Dou, Y Cho, Y Lee, Y Cui, Y Cao, Y.-H Wu, Y Tang, Y Zhu, Y Zhang, Y Jiang, Y Li, Y Li, Y Iwasawa, Y Matsuo, Z Ma, Z Xu, Robotic learning datasets and RT-X models. 2023</p>
<p>M Deitke, C Clark, S Lee, R Tripathi, Y Yang, J S Park, M Salehi, N Muennighoff, K Lo, L Soldaini, J Lu, T Anderson, E Bransom, K Ehsani, H Ngo, Y Chen, A Patel, M Yatskar, C Callison-Burch, A Head, R Hendrix, F Bastani, E Vanderbilt, N Lambert, Y Chou, A Chheda, J Sparks, S Skjonsberg, M Schmitz, A Sarnat, B Bischoff, P Walsh, C Newell, P Wolters, T Gupta, K.-H Zeng, J Borchardt, D Groeneveld, J Dumas, C Nam, S Lebrecht, C Wittlif, C Schoenick, O Michel, R Krishna, L Weihs, N A Smith, H Hajishirzi, R Girshick, A Farhadi, A Kembhavi, arXiv:2409.17146Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. 2024arXiv preprint</p>
<p>Knowledge insulating vision-language-action models: Train fast, run fast, generalize better. D Driess, J T Springenberg, B Ichter, L Yu, A Li-Bell, K Pertsch, A Z Ren, H Walke, Q Vuong, L X Shi, arXiv:2505.237052025arXiv preprint</p>
<p>Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. H.-S Fang, C Wang, H Fang, M Gou, J Liu, H Yan, W Liu, Y Xie, C Lu, IEEE Transactions on Robotics. 2023</p>
<p>Genmanip: Llm-driven simulation for generalizable instruction-following manipulation. N Gao, Y Chen, S Yang, X Chen, Y Tian, H Li, H Huang, H Wang, T Wang, J Pang, CVPR. 2025</p>
<p>Mobile robot manipulation using pure object detection. B Griffin, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2023</p>
<p>Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. J Gu, S Kirmani, P Wohlhart, Y Lu, M G Arenas, K Rao, W Yu, C Fu, K Gopalakrishnan, Z Xu, arXiv:2311.019772023aarXiv preprint</p>
<p>J Gu, F Xiang, X Li, Z Ling, X Liu, T Mu, Y Tang, S Tao, X Wei, Y Yao, arXiv:2302.04659A unified benchmark for generalizable manipulation skills. 2023barXiv preprint</p>
<p>H Huang, F Lin, Y Hu, S Wang, Y Gao, Copa, arXiv:2403.08248General robotic manipulation through spatial constraints of parts with foundation models. 2024aarXiv preprint</p>
<p>Graphcot-vla: A 3d spatial-aware reasoning vision-language-action model for robotic manipulation with ambiguous instructions. H Huang, M Cen, K Tan, X Quan, G Huang, H Zhang, arXiv:2508.076502025aarXiv preprint</p>
<p>Roboground: Robotic manipulation with grounded vision-language priors. H Huang, X Chen, Y Chen, H Li, X Han, Z Wang, T Wang, J Pang, Z Zhao, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025b</p>
<p>W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023arXiv preprint</p>
<p>W Huang, C Wang, Y Li, R Zhang, L Fei-Fei, arXiv:2409.01652Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. 2024barXiv preprint</p>
<p>𝑝𝑖 0.5 : a vision-language-action model with open-world generalization. P Intelligence, K Black, N Brown, J Darpinian, K Dhabalia, D Driess, A Esmail, M Equi, C Finn, N Fusai, arXiv:2504.160542025arXiv preprint</p>
<p>A Khazatsky, K Pertsch, S Nair, A Balakrishna, S Dasari, S Karamcheti, S Nasiriany, M K Srirama, L Y Chen, K Ellis, arXiv:2403.12945A large-scale in-the-wild robot manipulation dataset. 2024arXiv preprint</p>
<p>M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, arXiv:2406.09246An open-source vision-language-action model. 2024arXiv preprint</p>
<p>Fine-tuning vision-language-action models: Optimizing speed and success. M J Kim, C Finn, P Liang, arXiv:2502.196452025arXiv preprint</p>
<p>A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, P Dollár, R Girshick, arXiv:2304.02643Segment anything. 2023</p>
<p>Curl: Contrastive unsupervised representations for reinforcement learning. M Laskin, A Srinivas, P Abbeel, International conference on machine learning. PMLR2020</p>
<p>J Lee, J Duan, H Fang, Y Deng, S Liu, B Li, B Fang, J Zhang, Y R Wang, S Lee, arXiv:2508.07917Action reasoning models that can reason in space. 2025arXiv preprint</p>
<p>B Li, Y Zhang, D Guo, R Zhang, F Li, H Zhang, K Zhang, P Zhang, Y Li, Z Liu, arXiv:2408.03326Llava-onevision: Easy visual task transfer. 2024aarXiv preprint</p>
<p>B Li, Y Zhang, D Guo, R Zhang, F Li, H Zhang, K Zhang, P Zhang, Y Li, Z Liu, arXiv:2408.03326Llava-onevision: Easy visual task transfer. 2024barXiv preprint</p>
<p>H Li, S Yang, Y Chen, Y Tian, X Yang, X Chen, H Wang, T Wang, F Zhao, D Lin, arXiv:2506.19816Transferring latent motion across time for multi-frame prediction in manipulation. 2025aarXiv preprint</p>
<p>Cogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation. Q Li, Y Liang, Z Wang, L Luo, X Chen, M Liao, F Wei, Y Deng, S Xu, Y Zhang, arXiv:2411.196502024carXiv preprint</p>
<p>Unified video action model. S Li, Y Gao, D Sadigh, S Song, arXiv:2503.002002025barXiv preprint</p>
<p>Y Li, Y Du, K Zhou, J Wang, W X Zhao, J.-R Wen, arXiv:2305.10355Evaluating object hallucination in large vision-language models. 2023arXiv preprint</p>
<p>Y Li, Y Deng, J Zhang, J Jang, M Memmel, R Yu, C R Garrett, F Ramos, D Fox, A Li, arXiv:2502.05485Hierarchical action models for open-world robot manipulation. 2025carXiv preprint</p>
<p>Pointnetgpd: Detecting grasp configurations from point sets. H Liang, X Ma, S Li, M Görner, S Tang, B Fang, F Sun, J Zhang, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Genie envisioner: A unified world foundation platform for robotic manipulation. Y Liao, P Zhou, S Huang, D Yang, S Chen, Y Jiang, Y Hu, J Cai, S Liu, J Luo, arXiv:2508.056352025arXiv preprint</p>
<p>F Lin, R Nai, Y Hu, J You, J Zhao, Y Gao, arXiv:2505.11917Onetwovla: A unified vision-language-action model with adaptive reasoning. 2025arXiv preprint</p>
<p>Moka: Open-vocabulary robotic manipulation through mark-based visual prompting. F Liu, K Fang, P Abbeel, S Levine, First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024. 2024</p>
<p>Vl-grasp: a 6-dof interactive grasp policy for language-oriented objects in cluttered indoor scenes. Y Lu, Y Fan, B Deng, F Liu, Y Li, S Wang, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023a</p>
<p>Vl-grasp: a 6-dof interactive grasp policy for language-oriented objects in cluttered indoor scenes. Y Lu, Y Fan, B Deng, F Liu, Y Li, S Wang, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023b</p>
<p>Visual embodied brain: Let multimodal large language models see, think, and control in spaces. G Luo, G Yang, Z Gong, G Chen, H Duan, E Cui, R Tong, Z Hou, T Zhang, Z Chen, arXiv:2506.001232025arXiv preprint</p>
<p>F1: A vision-language-action model bridging understanding and generation to actions. Q Lv, W Kong, H Li, J Zeng, Z Qiu, D Qu, H Song, Q Chen, X Deng, M Y Wang, L Nie, J Pang, 2025</p>
<p>V Makoviychuk, L Wawrzyniak, Y Guo, M Lu, K Storey, M Macklin, D Hoeller, N Rudin, A Allshire, A Handa, arXiv:2108.10470Isaac gym: High performance gpu-based physics simulation for robot learning. 2021arXiv preprint</p>
<p>Generation and comprehension of unambiguous object descriptions. J Mao, J Huang, A Toshev, O Camburu, A L Yuille, K Murphy, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016</p>
<p>R3m: A universal visual representation for robot manipulation. S Nair, A Rajeswaran, V Kumar, C Finn, A Gupta, arXiv:2203.126012022arXiv preprint</p>
<p>S Nasiriany, S Kirmani, T Ding, L Smith, Y Zhu, D Driess, D Sadigh, T Xiao, arXiv:2411.02704Rt-affordance: Affordances are versatile intermediate representations for robot manipulation. 2024arXiv preprint</p>
<p>Octo: An open-source generalist robot policy. Octo Model Team, D Ghosh, H Walke, K Pertsch, K Black, O Mees, S Dasari, J Hejna, C Xu, J Luo, T Kreiman, Y Tan, P Sanketi, Q Vuong, T Xiao, D Sadigh, C Finn, S Levine, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsDelft, Netherlands2024</p>
<p>M Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Fernandez, D Haziza, F Massa, A El-Nouby, arXiv:2304.07193Learning robust visual features without supervision. 2023arXiv preprint</p>
<p>Fast: Efficient action tokenization for vision-language-action models. K Pertsch, K Stachowicz, B Ichter, D Driess, S Nair, Q Vuong, O Mees, C Finn, S Levine, arXiv:2501.097472025arXiv preprint</p>
<p>Sofar: Language-grounded orientation bridges spatial reasoning and object manipulation. Z Qi, W Zhang, Y Ding, R Dong, X Yu, J Li, L Xu, B Li, X He, G Fan, J Zhang, J He, J Gu, X Jin, K Ma, Z Zhang, H Wang, L Yi, 10.48550/arXiv.2502.131432025</p>
<p>D Qu, H Song, Q Chen, Y Yao, X Ye, Y Ding, Z Wang, J Gu, B Zhao, D Wang, arXiv:2501.15830Exploring spatial representations for visual-language-action model. 2025arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PMLR2021</p>
<p>Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. Advances in neural information processing systems. M Raghu, J Gilmer, J Yosinski, J Sohl-Dickstein, 201730</p>
<p>Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. K Rana, J Haviland, S Garg, J Abou-Chakra, I Reid, N Suenderhauf, arXiv:2307.061352023arXiv preprint</p>
<p>Hi robot: Open-ended instruction following with hierarchical vision-language-action models. L X Shi, B Ichter, M Equi, L Ke, K Pertsch, Q Vuong, J Tanner, A Walling, H Wang, N Fusai, arXiv:2502.194172025arXiv preprint</p>
<p>M Shukor, D Aubakirova, F Capuano, P Kooijmans, S Palma, A Zouitine, M Aractingi, C Pascal, M Russi, A Marafioti, arXiv:2506.01844A vision-language-action model for affordable and efficient robotics. 2025arXiv preprint</p>
<p>Towards vqa models that can read. A Singh, V Natarjan, M Shah, Y Jiang, X Chen, D Parikh, M Rohrbach, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2019</p>
<p>Benchmarking object detectors with coco: A new path forward. S Singh, A Yadav, J Jain, H Shi, J Johnson, K Desai, European Conference on Computer Vision. Springer2024</p>
<p>H Song, D Qu, Y Yao, Q Chen, Q Lv, Y Tang, M Shi, G Ren, M Yao, B Zhao, arXiv:2505.21432Introducing system-2 thinking in visual-language-action model. 2025arXiv preprint</p>
<p>R Team, M Cao, H Tan, Y Ji, M Lin, Z Li, Z Cao, P Wang, E Zhou, Y Han, arXiv:2507.02029Robobrain 2.0 technical report. 2025arXiv preprint</p>
<p>Using geometry to detect grasp poses in 3d point clouds. A Ten Pas, R Platt, Robotics Research. Springer20171</p>
<p>Y Tian, S Yang, J Zeng, P Wang, D Lin, H Dong, J Pang, arXiv:2412.15109Predictive inverse dynamics models are scalable learners for robotic manipulation. 2024arXiv preprint</p>
<p>The all-seeing project v2: Towards general relation comprehension of the open world. W Wang, Y Ren, H Luo, T Li, C Yan, Z Chen, W Wang, Q Li, L Lu, X Zhu, European Conference on Computer Vision. Springer2024</p>
<p>Unified vision-language-action model. Y Wang, X Li, W Wang, J Zhang, Y Li, Y Chen, X Wang, Z Zhang, arXiv:2506.198502025arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>K Wu, C Hou, J Liu, Z Che, X Ju, Z Yang, M Li, Y Zhao, Z Xu, G Yang, arXiv:2412.13877Benchmark on multi-embodiment intelligence normative data for robot manipulation. 2024arXiv preprint</p>
<p>Embedding symbolic knowledge into deep networks. Y Xie, Z Xu, M S Kankanhalli, K S Meel, H Soh, Advances in neural information processing systems. 201932</p>
<p>A0: An affordance-aware hierarchical model for general robotic manipulation. R Xu, J Zhang, M Guo, Y Wen, H Yang, M Lin, J Huang, Z Li, K Zhang, L Wang, Y Kuang, M Cao, F Zheng, X Liang, 2025a</p>
<p>A0: An affordance-aware hierarchical model for general robotic manipulation. R Xu, J Zhang, M Guo, Y Wen, H Yang, M Lin, J Huang, Z Li, K Zhang, L Wang, arXiv:2504.126362025barXiv preprint</p>
<p>J Yang, R Tan, Q Wu, R Zheng, B Peng, Y Liang, Y Gu, M Cai, S Ye, J Jang, arXiv:2502.13130A foundation model for multimodal ai agents. 2025aarXiv preprint</p>
<p>Instructvla: Vision-language-action instruction tuning from understanding to manipulation. S Yang, H Li, Y Chen, B Wang, Y Tian, T Wang, H Wang, F Zhao, Y Liao, J Pang, arXiv:2507.175202025barXiv preprint</p>
<p>Latent action pretraining from videos. S Ye, J Jang, B Jeon, S Joo, J Yang, B Peng, A Mandlekar, R Tan, Y.-W Chao, B Y Lin, L Liden, K Lee, J Gao, L Zettlemoyer, D Fox, M Seo, The Thirteenth International Conference on Learning Representations (ICLR). 2025</p>
<p>Modeling context in referring expressions. L Yu, P Poirson, S Yang, A C Berg, T L Berg, European conference on computer vision. Springer2016</p>
<p>W Yu, Z Yang, L Li, J Wang, K Lin, Z Liu, X Wang, L Wang, arXiv:2308.02490Mm-vet: Evaluating large multimodal models for integrated capabilities. 2023arXiv preprint</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. W Yuan, J Duan, V Blukis, W Pumacay, R Krishna, A Murali, A Mousavian, D Fox, arXiv:2406.107212024arXiv preprint</p>
<p>Robotic control via embodied chain-of-thought reasoning. M Zawalski, W Chen, K Pertsch, O Mees, C Finn, S Levine, arXiv:2407.086932024arXiv preprint</p>
<p>Sigmoid loss for language image pre-training. X Zhai, B Mustafa, A Kolesnikov, L Beyer, arXiv:2306.13394Mme: A comprehensive evaluation benchmark for multimodal large language models. Y S Y Q M Zhang, X L J Y X Zheng, K L X S Y Wu, R J C Fu, P Chen, 2023. 202118arXiv preprintProceedings of the IEEE/CVF International Conference on Computer Vision</p>
<p>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. Q Zhao, Y Lu, M J Kim, Z Fu, Z Zhang, Y Wu, Z Li, Q Ma, S Han, C Finn, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>E Zhou, J An, C Chi, Y Han, S Rong, C Zhang, P Wang, Z Wang, T Huang, L Sheng, arXiv:2506.04308Towards spatial referring with reasoning in vision-language models for robotics. 2025aarXiv preprint</p>
<p>Vision-language-action model with open-world embodied reasoning from pretrained knowledge. Z Zhou, Y Zhu, J Wen, C Shen, Y Xu, arXiv:2505.219062025barXiv preprint</p>
<p>J Zhu, W Wang, Z Chen, Z Liu, S Ye, L Gu, H Tian, Y Duan, W Su, J Shao, Z Gao, E Cui, X Wang, Y Cao, Y Liu, X Wei, H Zhang, H Wang, W Xu, H Li, J Wang, N Deng, S Li, Y He, T Jiang, J Luo, Y Wang, C He, B Shi, X Zhang, W Shao, J He, Y Xiong, W Qu, P Sun, P Jiao, H Lv, L Wu, K Zhang, H Deng, J Ge, K Chen, L Wang, M Dou, L Lu, X Zhu, T Lu, D Lin, Y Qiao, J Dai, W Wang, Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. 2025</p>            </div>
        </div>

    </div>
</body>
</html>
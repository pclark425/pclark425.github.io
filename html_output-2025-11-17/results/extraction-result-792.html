<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-792 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-792</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-792</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-232352537</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclweb.org/anthology/2021.naacl-main.247.pdf" target="_blank">Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games simulate worlds and interact with players using natural language. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or semantics is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent artificial agents utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of semantic information available to a learning agent. Surprisingly, we find that an agent is capable of achieving high scores even in the complete absence of language semantics, indicating that the currently popular experimental setup and models may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including Zork I. We discuss the implications of our findings for designing future agents with stronger semantic understanding.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e792.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e792.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Relevance Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Q-learning agent for text games that encodes observations and candidate actions with separate GRU encoders and scores action-observation pairs using an MLP, trained with a TD loss and a softmax action selection over provided candidate actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning with a natural language action space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Q-network Q_phi(o,a) that encodes the current text observation o and each candidate textual action a with separate GRU encoders (f_o and f_a), concatenates the encodings and passes them through an MLP g to produce Q-values; trained with temporal-difference (TD) loss using an experience replay buffer and a prioritized replay strategy; uses a softmax over Q-values for action selection. In experiments in this paper DRRN is used with the environment-provided 'valid action handicap' (reduced candidate action set).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Jericho interactive fiction benchmark (multiple human-authored text-adventure games, e.g., ZORK I)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based interactive-fiction games where observations and actions are textual; underlying states are hidden (POMDP): agents receive partial textual observations (descriptions, location/inventory when issuing 'look'/'inventory'), and must act by issuing text commands; challenges include large, changing action spaces, sparse rewards, and partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Valid action handicap (environment-provided reduced set of valid action candidates); optionally a language model for action generation is mentioned as an alternative, but experiments use the valid action handicap.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Lists of candidate textual actions (discrete text strings) provided per-step by environment; not spatial maps or other structured tool outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>No explicit probabilistic belief state is maintained; the agent represents the current observation via a GRU encoder and learns Q-values over observation-action encodings, storing (o,a,r,o') tuples in a replay buffer for learning.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Representations are updated implicitly via gradient updates from TD loss (and replay); there is no explicit Bayesian or structured belief update — the agent learns to map current textual observation encoding (and candidate-action encodings) to Q-values through training.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Model-free reinforcement learning (Q-learning-style value estimation via DRRN); not model-based planning or explicit search over future states.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Average normalized final score 0.21 (final) / 0.38 (maximum observed) across 12 Jericho games after training (see Table 1); per-game scores reported in Table 1 (e.g., ZORK I final 39.4 / max 53).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DRRN can achieve nontrivial performance in partially observable text games when given the valid action handicap (reduced action set), but it does not maintain an explicit belief state; its learned representations can overfit to seen trajectories and do not necessarily capture semantic similarity across unseen states.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e792.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e792.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MIN-OB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Minimizing Observation (MIN-OB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation that reduces observation input to only the location phrase (loc(o)) to isolate and test reliance on action semantics and to degrade observation semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN (MIN-OB variant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same DRRN architecture as baseline but the observation encoder receives only the location phrase loc(o) instead of full textual observation; otherwise training and action selection identical to DRRN baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Jericho interactive fiction benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same as DRRN entry: partially observable textual games with changing valid actions and sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>No external tools; architectural/data-ablation that minimizes observation content to only the location name.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>No explicit belief state; agent sees heavily reduced observation (location phrase) and relies on learned Q-values over obs-action encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Representations and Q-values updated via TD learning on reduced observations; no separate belief update mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Model-free RL (same DRRN Q-learning) on reduced observations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Average normalized final score 0.12 (final) / 0.35 (max) across the 12 games (Table 1); generally explores similar maxima but fails to retain high episodic scores in many games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reducing observation semantics to location phrases harms final episodic performance (worse memorization of good experiences), indicating detailed observation text can aid encoding distinct states even though agents may not fully leverage semantics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e792.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e792.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HASH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hashing ablation (HASH)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation that breaks textual semantic continuity by hashing observation and action strings to integer seeds which are mapped deterministically to random vectors; tests whether agents rely on semantic continuity of text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN (HASH variant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DRRN where the GRU encoders are replaced (or bypassed) by fixed random vector encodings derived from a hash of the text: a hash function maps the token sequence to an integer seed and a seeded pseudorandom Gaussian generator G produces a fixed d-dimensional vector per unique text; f_o and f_a are fixed (non-trainable) in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Jericho interactive fiction benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text-adventure games with large changing action sets and sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Python built-in hash function and a seeded random vector generator (implemented by seeding PyTorch RNG with the hash) to produce fixed vector encodings for every distinct observation and action string.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Fixed random continuous vectors (d-dimensional Gaussian vectors) representing each unique text token sequence; these serve as identifiers rather than semantic encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>No explicit belief state; fixed random vectors identify distinct observation and action instances and DRRN learns Q-values over these fixed encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>No belief update; the fixed hashed encodings do not change; only Q-network parameters are learned via TD updates.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Model-free RL (DRRN Q-learning) operating over hashed identifiers instead of semantic encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Average normalized final score 0.25 (final) / 0.39 (max) across the 12 games (Table 1); matches or exceeds baseline DRRN on many games despite broken semantics (e.g., PENTARI nearly doubles DRRN final score).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Breaking textual semantics via hashing still enables strong performance: the agent can treat distinct hashed observations/actions as identifiers and memorize high-Q trajectories, implying current RL setups may not require or leverage natural-language semantics to succeed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e792.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e792.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>INV-DY</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inverse Dynamics Regularization (INV-DY)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An auxiliary inverse-dynamics decoding objective added to DRRN that predicts action representations from (current, next) observation representations and decodes actions back to text to regularize learned encodings and encourage exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN + INV-DY</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Extends DRRN with an inverse dynamics module: given f_o(o) and f_o(o'), an MLP g_inv predicts an action representation; a GRU decoder d decodes this representation into the action text and a reconstruction loss (cross-entropy) L_inv is minimized; additionally a reconstruction loss L_dec reconstructs action text from f_a(a). Both auxiliary losses are combined with TD loss (weights lambda_1=lambda_2=1). An intrinsic reward equal to the inverse dynamics loss is also used to encourage exploration of states where inverse dynamics is not yet learned.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Jericho interactive fiction benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text adventures (POMDP) with complex textual observations and changing valid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>No external tools; uses an internal auxiliary decoder and MLP as part of the agent architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Internal predicted action representations and decoded text sequences (text tokens) from the GRU decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>No explicit probabilistic belief state; instead, representation space is regularized so that observation encodings f_o(o) encode action-relevant semantics through inverse-dynamics prediction and text reconstruction; this yields representations that better generalize to unseen states.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Observation and action encoders are updated jointly by TD loss and inverse-dynamics and decoding losses during experience replay; intrinsic reward from inverse dynamics drives exploration to states where inverse dynamics is uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Model-free RL (Q-learning) with auxiliary self-supervised prediction (inverse dynamics) providing structured learning signals and intrinsic rewards; not explicit planning/search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Average normalized final score 0.23 (final) / 0.40 (max) across 12 games (Table 1); notably explores higher scores on some games (e.g., ZORK I max 87 vs other models' maxima <=55).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Inverse-dynamics decoding regularizes encoders toward more semantically meaningful representations, leads to better generalization to unseen states and improved exploration on some games; provides intrinsic curiosity-like reward that aids exploration where semantics matter.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e792.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e792.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DynamicKnowledgeGraphs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning dynamic knowledge graphs to generalize on text-based games (Adhikari et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that proposes learning dynamic knowledge graphs to help agents generalize in text-based games by maintaining structured state representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic knowledge graphs to generalize on text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Dynamic knowledge graph-based agents (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned in related work as approaches that construct and update a knowledge graph representation of world entities and relations while playing text games; the paper references this line of work but does not implement or analyze it itself.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games / interactive fiction (as discussed in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text environments where structured representations (dynamic KGs) are proposed to capture latent state across observations and episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Knowledge-graph construction and maintenance (as reported in the cited paper) — i.e., structured graph as an internal tool/representation; this paper only mentions it.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured graph data (entities, relations), i.e., symbolic/graph outputs representing objects and relations.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Mentioned as using an explicit dynamic graph as a state/belief representation (per cited work); the present paper does not implement it.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Mentioned work likely uses graph-structured state representations to inform policy; this paper does not detail the planning approach of that work.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of approaches that leverage structured external representations (knowledge graphs) to potentially improve generalization across games; included in related work but not evaluated in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e792.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e792.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphConstrainedRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that constrains or leverages graph structure in RL agents for natural language action spaces; cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Graph-constrained RL agents (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned in related work as agents that incorporate graph structure or constraints over actions and/or states to improve learning in textual action spaces; the present paper references this but does not implement it.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games / natural language action spaces (as in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable textual environments with large action sets; graph constraints are proposed to structure action/state spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Graph-structured constraints or representations (as per cited work) — referenced but not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured graph data / constrained action sets (per cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Not specified in this paper; likely uses graph-based state approximations in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Mention suggests use of graph-based constraints to guide policy/search, but no details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Included in related work as a direction that leverages graph structure for action selection in language action spaces; not evaluated in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e792.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e792.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KeepCALM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Keep CALM and explore: Language models for action generation in text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that uses language models to generate candidate actions (i.e., an external LM as an action-generation tool) for text game agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Keep CALM and explore: Language models for action generation in text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Language-model-based action generation (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced approach where a language model is used to produce candidate textual actions (thus replacing or augmenting environment-provided valid action lists); this paper mentions LM-based candidate generation as an alternative to the valid action handicap but does not implement it.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games / interactive fiction</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text environments where generating action candidates (instead of receiving a valid list) increases the language grounding challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Pretrained language model (LM) for action generation (mentioned as an external tool to produce a reduced action space); not used in the present experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Generated textual action strings (natural language action candidates).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Mentioned as facilitating exploration by producing candidate actions; no explicit planning mechanism described in this paper for LM-driven agents.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper highlights LM-based action generation as an important alternative to the valid action handicap that would increase the requirement for grounded language understanding; the present work uses valid action handicap but references LM generation as relevant prior work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e792.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e792.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NAIL: A general interactive fiction agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior agent for interactive fiction games (NAIL); cited in related work but not used or analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nail: A general interactive fiction agent</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NAIL (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Cited general interactive-fiction agent; referenced as part of the broader literature on text-game agents. The current paper does not detail NAIL's internal tools or belief mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Interactive fiction / text-adventure games</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text worlds; NAIL is referenced as a prior approach in the space.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as related prior work; this paper does not report details on NAIL's use of external tools, belief maintenance, or planning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dynamic knowledge graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Keep CALM and explore: Language models for action generation in text-based games <em>(Rating: 2)</em></li>
                <li>Nail: A general interactive fiction agent <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-792",
    "paper_id": "paper-232352537",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "DRRN",
            "name_full": "Deep Reinforcement Relevance Network",
            "brief_description": "A Q-learning agent for text games that encodes observations and candidate actions with separate GRU encoders and scores action-observation pairs using an MLP, trained with a TD loss and a softmax action selection over provided candidate actions.",
            "citation_title": "Deep reinforcement learning with a natural language action space",
            "mention_or_use": "use",
            "agent_name": "DRRN",
            "agent_description": "Q-network Q_phi(o,a) that encodes the current text observation o and each candidate textual action a with separate GRU encoders (f_o and f_a), concatenates the encodings and passes them through an MLP g to produce Q-values; trained with temporal-difference (TD) loss using an experience replay buffer and a prioritized replay strategy; uses a softmax over Q-values for action selection. In experiments in this paper DRRN is used with the environment-provided 'valid action handicap' (reduced candidate action set).",
            "environment_name": "Jericho interactive fiction benchmark (multiple human-authored text-adventure games, e.g., ZORK I)",
            "environment_description": "Text-based interactive-fiction games where observations and actions are textual; underlying states are hidden (POMDP): agents receive partial textual observations (descriptions, location/inventory when issuing 'look'/'inventory'), and must act by issuing text commands; challenges include large, changing action spaces, sparse rewards, and partial observability.",
            "is_partially_observable": true,
            "external_tools_used": "Valid action handicap (environment-provided reduced set of valid action candidates); optionally a language model for action generation is mentioned as an alternative, but experiments use the valid action handicap.",
            "tool_output_types": "Lists of candidate textual actions (discrete text strings) provided per-step by environment; not spatial maps or other structured tool outputs.",
            "belief_state_mechanism": "No explicit probabilistic belief state is maintained; the agent represents the current observation via a GRU encoder and learns Q-values over observation-action encodings, storing (o,a,r,o') tuples in a replay buffer for learning.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "Representations are updated implicitly via gradient updates from TD loss (and replay); there is no explicit Bayesian or structured belief update — the agent learns to map current textual observation encoding (and candidate-action encodings) to Q-values through training.",
            "planning_approach": "Model-free reinforcement learning (Q-learning-style value estimation via DRRN); not model-based planning or explicit search over future states.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": "Average normalized final score 0.21 (final) / 0.38 (maximum observed) across 12 Jericho games after training (see Table 1); per-game scores reported in Table 1 (e.g., ZORK I final 39.4 / max 53).",
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "DRRN can achieve nontrivial performance in partially observable text games when given the valid action handicap (reduced action set), but it does not maintain an explicit belief state; its learned representations can overfit to seen trajectories and do not necessarily capture semantic similarity across unseen states.",
            "uuid": "e792.0"
        },
        {
            "name_short": "MIN-OB",
            "name_full": "Minimizing Observation (MIN-OB)",
            "brief_description": "An ablation that reduces observation input to only the location phrase (loc(o)) to isolate and test reliance on action semantics and to degrade observation semantics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DRRN (MIN-OB variant)",
            "agent_description": "Same DRRN architecture as baseline but the observation encoder receives only the location phrase loc(o) instead of full textual observation; otherwise training and action selection identical to DRRN baseline.",
            "environment_name": "Jericho interactive fiction benchmark",
            "environment_description": "Same as DRRN entry: partially observable textual games with changing valid actions and sparse rewards.",
            "is_partially_observable": true,
            "external_tools_used": "No external tools; architectural/data-ablation that minimizes observation content to only the location name.",
            "tool_output_types": null,
            "belief_state_mechanism": "No explicit belief state; agent sees heavily reduced observation (location phrase) and relies on learned Q-values over obs-action encodings.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "Representations and Q-values updated via TD learning on reduced observations; no separate belief update mechanism.",
            "planning_approach": "Model-free RL (same DRRN Q-learning) on reduced observations.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": "Average normalized final score 0.12 (final) / 0.35 (max) across the 12 games (Table 1); generally explores similar maxima but fails to retain high episodic scores in many games.",
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Reducing observation semantics to location phrases harms final episodic performance (worse memorization of good experiences), indicating detailed observation text can aid encoding distinct states even though agents may not fully leverage semantics.",
            "uuid": "e792.1"
        },
        {
            "name_short": "HASH",
            "name_full": "Hashing ablation (HASH)",
            "brief_description": "An ablation that breaks textual semantic continuity by hashing observation and action strings to integer seeds which are mapped deterministically to random vectors; tests whether agents rely on semantic continuity of text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DRRN (HASH variant)",
            "agent_description": "DRRN where the GRU encoders are replaced (or bypassed) by fixed random vector encodings derived from a hash of the text: a hash function maps the token sequence to an integer seed and a seeded pseudorandom Gaussian generator G produces a fixed d-dimensional vector per unique text; f_o and f_a are fixed (non-trainable) in this setup.",
            "environment_name": "Jericho interactive fiction benchmark",
            "environment_description": "Partially observable text-adventure games with large changing action sets and sparse rewards.",
            "is_partially_observable": true,
            "external_tools_used": "Python built-in hash function and a seeded random vector generator (implemented by seeding PyTorch RNG with the hash) to produce fixed vector encodings for every distinct observation and action string.",
            "tool_output_types": "Fixed random continuous vectors (d-dimensional Gaussian vectors) representing each unique text token sequence; these serve as identifiers rather than semantic encodings.",
            "belief_state_mechanism": "No explicit belief state; fixed random vectors identify distinct observation and action instances and DRRN learns Q-values over these fixed encodings.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "No belief update; the fixed hashed encodings do not change; only Q-network parameters are learned via TD updates.",
            "planning_approach": "Model-free RL (DRRN Q-learning) operating over hashed identifiers instead of semantic encodings.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": "Average normalized final score 0.25 (final) / 0.39 (max) across the 12 games (Table 1); matches or exceeds baseline DRRN on many games despite broken semantics (e.g., PENTARI nearly doubles DRRN final score).",
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Breaking textual semantics via hashing still enables strong performance: the agent can treat distinct hashed observations/actions as identifiers and memorize high-Q trajectories, implying current RL setups may not require or leverage natural-language semantics to succeed.",
            "uuid": "e792.2"
        },
        {
            "name_short": "INV-DY",
            "name_full": "Inverse Dynamics Regularization (INV-DY)",
            "brief_description": "An auxiliary inverse-dynamics decoding objective added to DRRN that predicts action representations from (current, next) observation representations and decodes actions back to text to regularize learned encodings and encourage exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DRRN + INV-DY",
            "agent_description": "Extends DRRN with an inverse dynamics module: given f_o(o) and f_o(o'), an MLP g_inv predicts an action representation; a GRU decoder d decodes this representation into the action text and a reconstruction loss (cross-entropy) L_inv is minimized; additionally a reconstruction loss L_dec reconstructs action text from f_a(a). Both auxiliary losses are combined with TD loss (weights lambda_1=lambda_2=1). An intrinsic reward equal to the inverse dynamics loss is also used to encourage exploration of states where inverse dynamics is not yet learned.",
            "environment_name": "Jericho interactive fiction benchmark",
            "environment_description": "Partially observable text adventures (POMDP) with complex textual observations and changing valid actions.",
            "is_partially_observable": true,
            "external_tools_used": "No external tools; uses an internal auxiliary decoder and MLP as part of the agent architecture.",
            "tool_output_types": "Internal predicted action representations and decoded text sequences (text tokens) from the GRU decoder.",
            "belief_state_mechanism": "No explicit probabilistic belief state; instead, representation space is regularized so that observation encodings f_o(o) encode action-relevant semantics through inverse-dynamics prediction and text reconstruction; this yields representations that better generalize to unseen states.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "Observation and action encoders are updated jointly by TD loss and inverse-dynamics and decoding losses during experience replay; intrinsic reward from inverse dynamics drives exploration to states where inverse dynamics is uncertain.",
            "planning_approach": "Model-free RL (Q-learning) with auxiliary self-supervised prediction (inverse dynamics) providing structured learning signals and intrinsic rewards; not explicit planning/search.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": "Average normalized final score 0.23 (final) / 0.40 (max) across 12 games (Table 1); notably explores higher scores on some games (e.g., ZORK I max 87 vs other models' maxima &lt;=55).",
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Inverse-dynamics decoding regularizes encoders toward more semantically meaningful representations, leads to better generalization to unseen states and improved exploration on some games; provides intrinsic curiosity-like reward that aids exploration where semantics matter.",
            "uuid": "e792.3"
        },
        {
            "name_short": "DynamicKnowledgeGraphs",
            "name_full": "Learning dynamic knowledge graphs to generalize on text-based games (Adhikari et al., 2020)",
            "brief_description": "Referenced prior work that proposes learning dynamic knowledge graphs to help agents generalize in text-based games by maintaining structured state representations.",
            "citation_title": "Learning dynamic knowledge graphs to generalize on text-based games",
            "mention_or_use": "mention",
            "agent_name": "Dynamic knowledge graph-based agents (referenced)",
            "agent_description": "Mentioned in related work as approaches that construct and update a knowledge graph representation of world entities and relations while playing text games; the paper references this line of work but does not implement or analyze it itself.",
            "environment_name": "Text-based games / interactive fiction (as discussed in cited work)",
            "environment_description": "Partially observable text environments where structured representations (dynamic KGs) are proposed to capture latent state across observations and episodes.",
            "is_partially_observable": true,
            "external_tools_used": "Knowledge-graph construction and maintenance (as reported in the cited paper) — i.e., structured graph as an internal tool/representation; this paper only mentions it.",
            "tool_output_types": "Structured graph data (entities, relations), i.e., symbolic/graph outputs representing objects and relations.",
            "belief_state_mechanism": "Mentioned as using an explicit dynamic graph as a state/belief representation (per cited work); the present paper does not implement it.",
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Mentioned work likely uses graph-structured state representations to inform policy; this paper does not detail the planning approach of that work.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited as an example of approaches that leverage structured external representations (knowledge graphs) to potentially improve generalization across games; included in related work but not evaluated in this paper.",
            "uuid": "e792.4"
        },
        {
            "name_short": "GraphConstrainedRL",
            "name_full": "Graph constrained reinforcement learning for natural language action spaces",
            "brief_description": "Referenced prior work that constrains or leverages graph structure in RL agents for natural language action spaces; cited in related work.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "mention",
            "agent_name": "Graph-constrained RL agents (referenced)",
            "agent_description": "Mentioned in related work as agents that incorporate graph structure or constraints over actions and/or states to improve learning in textual action spaces; the present paper references this but does not implement it.",
            "environment_name": "Text-based games / natural language action spaces (as in cited work)",
            "environment_description": "Partially observable textual environments with large action sets; graph constraints are proposed to structure action/state spaces.",
            "is_partially_observable": true,
            "external_tools_used": "Graph-structured constraints or representations (as per cited work) — referenced but not used in experiments here.",
            "tool_output_types": "Structured graph data / constrained action sets (per cited work).",
            "belief_state_mechanism": "Not specified in this paper; likely uses graph-based state approximations in cited work.",
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Mention suggests use of graph-based constraints to guide policy/search, but no details provided here.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Included in related work as a direction that leverages graph structure for action selection in language action spaces; not evaluated in this paper.",
            "uuid": "e792.5"
        },
        {
            "name_short": "KeepCALM",
            "name_full": "Keep CALM and explore: Language models for action generation in text-based games",
            "brief_description": "Referenced prior work that uses language models to generate candidate actions (i.e., an external LM as an action-generation tool) for text game agents.",
            "citation_title": "Keep CALM and explore: Language models for action generation in text-based games",
            "mention_or_use": "mention",
            "agent_name": "Language-model-based action generation (referenced)",
            "agent_description": "Referenced approach where a language model is used to produce candidate textual actions (thus replacing or augmenting environment-provided valid action lists); this paper mentions LM-based candidate generation as an alternative to the valid action handicap but does not implement it.",
            "environment_name": "Text-based games / interactive fiction",
            "environment_description": "Partially observable text environments where generating action candidates (instead of receiving a valid list) increases the language grounding challenge.",
            "is_partially_observable": true,
            "external_tools_used": "Pretrained language model (LM) for action generation (mentioned as an external tool to produce a reduced action space); not used in the present experiments.",
            "tool_output_types": "Generated textual action strings (natural language action candidates).",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Mentioned as facilitating exploration by producing candidate actions; no explicit planning mechanism described in this paper for LM-driven agents.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Paper highlights LM-based action generation as an important alternative to the valid action handicap that would increase the requirement for grounded language understanding; the present work uses valid action handicap but references LM generation as relevant prior work.",
            "uuid": "e792.6"
        },
        {
            "name_short": "NAIL",
            "name_full": "NAIL: A general interactive fiction agent",
            "brief_description": "Referenced prior agent for interactive fiction games (NAIL); cited in related work but not used or analyzed in this paper.",
            "citation_title": "Nail: A general interactive fiction agent",
            "mention_or_use": "mention",
            "agent_name": "NAIL (referenced)",
            "agent_description": "Cited general interactive-fiction agent; referenced as part of the broader literature on text-game agents. The current paper does not detail NAIL's internal tools or belief mechanisms.",
            "environment_name": "Interactive fiction / text-adventure games",
            "environment_description": "Partially observable text worlds; NAIL is referenced as a prior approach in the space.",
            "is_partially_observable": true,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": null,
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Referenced as related prior work; this paper does not report details on NAIL's use of external tools, belief maintenance, or planning.",
            "uuid": "e792.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on text-based games",
            "rating": 2,
            "sanitized_title": "learning_dynamic_knowledge_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Keep CALM and explore: Language models for action generation in text-based games",
            "rating": 2,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        },
        {
            "paper_title": "Nail: A general interactive fiction agent",
            "rating": 1,
            "sanitized_title": "nail_a_general_interactive_fiction_agent"
        }
    ],
    "cost": 0.01596725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents
June 6-11, 2021</p>
<p>Shunyu Yao shunyuy@princeton.edu 
Princeton University ‡ Microsoft Research</p>
<p>Karthik Narasimhan karthikn@princeton.edu 
Princeton University ‡ Microsoft Research</p>
<p>Matthew Hausknecht matthew.hausknecht@microsoft.com 
Princeton University ‡ Microsoft Research</p>
<p>Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents</p>
<p>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJune 6-11, 20213097
Text-based games simulate worlds and interact with players using natural language. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or semantics is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent artificial agents utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of semantic information available to a learning agent. Surprisingly, we find that an agent is capable of achieving high scores even in the complete absence of language semantics, indicating that the currently popular experimental setup and models may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including ZORK I. We discuss the implications of our findings for designing future agents with stronger semantic understanding.</p>
<p>Introduction</p>
<p>Text adventure games such as ZORK I (Figure 1 (a)) have been a testbed for developing autonomous agents that operate using natural language. Since interactions in these games (input observations, action commands) are through text, the ability to understand and use language is deemed necessary and critical to progress through such games. Previous work has deployed a spectrum of methods for language processing in this domain, including word vectors (Fulda et al., 2017), recurrent neural networks (Narasimhan et al., 2015;, pre-trained language models (Yao * Work partly done during internship at Microsoft Research. Project page: https://blindfolded.cs. princeton.edu. et al., 2020), open-domain question answering systems , knowledge graphs Adhikari et al., 2020), and reading comprehension systems .</p>
<p>Meanwhile, most of these models operate under the reinforcement learning (RL) framework, where the agent explores the same environment in repeated episodes, learning a value function or policy to maximize game score. From this perspective, text games are just special instances of a partially observable Markov decision process (POMDP) (S, T, A, O, R, γ), where players issue text actions a ∈ A, receive text observations o ∈ O and scalar rewards r = R(s, a), and the underlying game state s ∈ S is updated by transition s = T (s, a).</p>
<p>However, what distinguishes these games from other POMDPs is the fact that the actions and observations are in language space L. Therefore, a certain level of decipherable semantics is attached to text observations o ∈ O ⊂ L and actions a ∈ A ⊂ L. Ideally, these texts not only serve as observation or action identifiers, but also provide clues about the latent transition function T and reward function R. For example, issuing an action "jump" based on an observation "on the cliff" would likely yield a subsequent observation such as "you are killed" along with a negative reward. Human players often rely on their understanding of language semantics to inform their choices, even on games they have never played before, while replacing texts with non-semantic identifiers such as their corresponding hashes (Figure 1 (c)) would likely render games unplayable for people. However, would this type of transformation affect current RL agents for such games? In this paper, we ask the following question: To what extent do current reinforcement learning agents leverage semantics in text-based games?</p>
<p>To shed light on this question, we investi-(a) ZORK I Observation 21: You are in the living room. There is a doorway to the east, a wooden door with strange gothic lettering to the west, which appears to be nailed shut, a trophy case, and a large oriental rug in the center of the room. You are carrying: A brass lantern . . .</p>
<p>Action 21: move rug</p>
<p>Observation 22: With a great effort, the rug is moved to one side of the room, revealing the dusty cover of a closed trap door... Living room... You are carrying: ... gate the Deep Reinforcement Relevance Network (DRRN) (He et al., 2016), a top-performing RL model that uses gated recurrent units (GRU) (Cho et al., 2014) to encode texts. We conduct three experiments on a set of interactive fiction games from the Jericho benchmark  to probe the effect of different semantic representations on the functioning of DRRN. These include (1) using just a location phrase as the input observation (Figure 1 (b)), (2) hashing text observations and actions (Figure 1 (c)), and (3) regularizing vector representations using an auxiliary inverse dynamics loss. While reducing observations to location phrases leads to decreased scores and enforcing inverse dynamics decoding leads to increased scores on some games, hashing texts to break semantics surprisingly matches or even outperforms the baseline DRRN on almost all games considered. This implies current RL agents for textbased games might not be sufficiently leveraging the semantic structure of game texts to learn good policies, and points to the need for developing better experiment setups and agents that have a finer grasp of natural language.</p>
<p>2 Models DRRN Baseline Our baseline RL agent DRRN (He et al., 2016) learns a Q-network Q φ (o, a) parametrized by φ. The model encodes the observation o and each action candidate a using two separate GRU encoders f o and f a , and then aggregates the representations to derive the Q-value through a MLP decoder g:
Q φ (o, a) = g(concat(f o (o), f a (a)))(1)
For learning φ, tuples (o, a, r, o ) of observation, action, reward and the next observation are sampled from an experience replay buffer and the following temporal difference (TD) loss is minimized:
L TD (φ) = (r + γ max a ∈A Q φ (o , a ) − Q φ (o, a)) 2
(2) During gameplay, a softmax exploration policy is used to sample an action:
π φ (a|o) = exp(Q φ (o, a)) a ∈A exp(Q φ (o, a ))(3)
Note that when the action space A is large, (2) and (3) become intractable. A valid action handicap  or a language model (Yao et al., 2020) can be used to generate a reduced action space for efficient exploration. For all the modifications below, we use the DRRN with the valid action handicap as our base model.</p>
<p>Reducing Semantics via Minimizing Observation (MIN-OB)</p>
<p>Unlike other RL domains such as video games or robotics control, at each step of text games the (valid) action space is constantly changing, and it reveals useful information about the current state. For example, knowing "unlock box" is valid leaks the existence of a locked box. Also, sometimes action semantics indicate its value even unconditional on the state, e.g. "pick gold" usually seems good. Given these, we minimize the observation to only a location phrase o → loc(o) (Figure 1 (b)) to isolate the action semantics: given a hash function from strings to integers h : L → Z, and a pseudo-random generator G : Z → R d that turns an integer seed to a random Gaussian vector, a hashing encoder (1) are trainable,f is fixed throughout RL, and ensures two texts that only differ by a word would have completely different representations. In this sense, hashing breaks semantics and only serves to identify different observations and actions in an abstract MDP problem (Figure 1 (c)):
Q loc φ (o, a) = g(f o (loc(o)), f a (a))f = G • h : L → R d can be composed. While f o and f a inQ hash φ (o, a) = g(f (o),f (a))
.</p>
<p>Regularizing Semantics via Inverse Dynamics</p>
<p>Decoding (INV-DY) The GRU representations in DRRN f o (o), f a (a) are only optimized for the TD loss (2). As a result, text semantics can degenerate during encoding, and the text representation might arbitrarily overfit to the Q-values. To regularize and encourage more game-related semantics to be encoded, we take inspiration from Pathak et al. (2017) and propose an inverse dynamics auxiliary task during RL. Given representations of current and next observations f o (o), f o (o ), we use a MLP g inv to predict the action representation, and a GRU decoder d to decode the action back to text * . The inverse dynamics loss is defined as
L inv (φ, θ) = − log p d (a|g inv (concat(f o (o), f o (o )))
where θ denote weights of g inv and d, and p d (a|x) is the probability of decoding token sequence a using GRU decoder d with initial hidden state as x. To also regularize the action encoding, action reconstruction from f a is also used as a loss term:</p>
<p>L dec (φ, θ) = − log p d (a|f a (a)) * Directly defining an L1/L2 loss between fa(a) and ginv(concat(fo(o), fo(o ))) in the representation space will collapse text representations together. And during experience replay, these two losses are optimized along with the TD loss:
L(φ, θ) = L TD (φ) + λ 1 L inv (φ, θ) + λ 2 L dec (φ, θ)
An intrinsic reward r + = L inv (φ, θ) is also used to explore toward where the inverse dynamics is not learned well yet. All in all, the aim of INV-DY is threefold: (1) regularize both action and observation representations to avoid degeneration by decoding back to the textual domain, (2) encourage f o to encode action-relevant parts of observations, and (3) provide intrinsic motivation for exploration.</p>
<p>Results</p>
<p>Setup We train on 12 games † from the Jericho benchmark . These human-written interactive fictions are rich, complex, and diverse in semantics. ‡ For each game, we train DRRN asynchronously on 8 parallel instances of the game environment for 10 5 steps, using a prioritized replay buffer. Following prior practice , we augment observations with location and inventory descriptions by issuing the 'look' and 'inventory' commands. We train three independent runs for each game and report their average score. For HASH, we use the Python built-in hash function to process text as a tuple of token IDs, and implement the random vector generator G by seeding PyTorch with the hash value. For INV-DY, we use λ 1 = λ 2 = 1.</p>
<p>Scores Table 1 reports the final score (the average score of the final 100 episodes during training), and the maximum score seen in each game for different models. Average normalized score (raw score divided by game total score) over all games is also reported. Compared to the base DRRN, MIN-OB turns out to explore similar maximum scores on † We omit games where DRRN cannot score. ‡ Please refer to  for more details about these games. most games (except DEEPHOME and DRAGON), but fails to memorize the good experience and reach high episodic scores, which suggests the importance of identifying different observations using language details. Most surprisingly, HASH has a lower final score than DRRN on only one game (ZORK I), while on PENTARI it almost doubles the DRRN final score. It is also the model with the best average normalized final score across games, which indicates that the DRRN model can perform as well without leveraging any language semantics, but instead simply by identifying different observations and actions with random vectors and memorizing the Q-values. Lastly, we observe on some games (DRAGON, OMNIQUEST, ZORK I) INV-DY can explore high scores that other models cannot. Most notably, on ZORK I the maximum score seen is 87 (average of 54, 94, 113), while any run of other models does not explore a score more than 55. This might indicate potential benefit of developing RL agents with more semantic representations.</p>
<p>Transfer We also investigate if representations of different models can transfer to a new language environment, which is a potential benefit of learning natural language semantics. So we consider the two most similar games in Jericho, ZORK I and ZORK III, fix the language encoders of different ZORK I models, and re-train the Q-network on ZORK III for 10,000 steps. As shown in Figure 2, INV-DY representations can achieve a score around 1, which surpasses the best result of models trained from scratch on ZORK III for 100,000 steps (around 0.4), showing great promise in better gameplay by leveraging language understanding from other games. HASH transfer is equivalent to training from scratch as the representations are not learnt, and a score around 0.3 is achieved. Finally, DRRN representations transfer worse than HASH, possibly due to overfitting to the TD loss (2).</p>
<p>Visualizations Finally, we use t-SNE (Maaten and Hinton, 2008) to visualize representations of some ZORK I walkthrough states in Figure 3. The first 30 walkthrough states (red, score 0-45) are well experienced by the models during exploration, whereas the last 170 states (blue, score 157-350) are unseen § . We also encircle the subset of states at location 'living room' for their shared semantics.</p>
<p>First, we note that the HASH representations for living room states are scattered randomly, unlike the other two models with GRU language encoders. Further, the base DRRN overfits to the TD loss (2), representing unseen states (blue) in a different subspace to seen states (red) without regarding their semantic similarity. IND-DY is able to extrapolate to unseen states and represent them similarly to seen states for their shared semantics, which may explain its better performance on this game.</p>
<p>Game stochasticity All the above experiments were performed using a fixed game random seed for each game, following prior work . To investigate if randomness in games affects our conclusions, we run one trial of each game with episode-varying random seeds ¶ . We find the average normalized score for the base DRRN, HASH, INV-DY to be all around 17%, with performance drop mainly on three stochastic games (DRAGON, ZORK I, ZORK III). Notably, the core finding that the base DRRN and HASH perform similarly still holds. Intuitively, even though the Q-values would be lower overall with unexpected transitions, RL would still memorize observations and actions that lead to high Q-values.</p>
<p>Discussion</p>
<p>At a high level, RL agents for text-based games succeed by (1) exploring trajectories that lead to high scores, and (2) learning representations to stably reach high scores. Our experiments show that a semantics-regularized INV-DY model manages to explore higher scores on some games (DRAGON, OMNIQUEST, ZORK I), while the HASH model manages to memorize scores better on other games (LIBRARY, LUDICORP, PENTARI) using just a fixed, random, non-semantic representation. This leads us to hypothesize two things. First, fixed, stable representations might make Q-learning easier. Second, it might be desirable to represent similar texts very differently for better gameplay, e.g. the Q-value can be much higher when a key object is mentioned, even if it only adds a few words to a long observation text. This motivates future thought into the structural vs. functional use of language semantics in these games.</p>
<p>Our findings also urge a re-thinking of the popular 'RL + valid action handicap' setup for these games. On one hand, RL sets training and evaluation in the same environment, with limited text corpora, and sparse, mostly deterministic rewards as the only optimization objective. Such a combination easily results in overfitting to the reward system of a specific game (Figure 2), or even just a specific stage of the game (Figure 3). On the other hand, the valid action handicap reduces the action set to a small size tractable for memorization, and reduces the language understanding challenge for the RL agent. Thus for future research on text-based games, we advocate for more attention towards alternative setups without RL or handicaps (Hausknecht et al., 2019;Yao et al., 2020;. Particularly, in a 'RL + no valid action handicap' setting, generating action candidates rather than simply choosing from a set entails more opportunities and challenges with respect to learning grounded language semantics (Yao et al., 2020). Additionally, training agents on a distribution of games and evaluating them on a separate set of unseen games would require more general semantic understanding. Semantic evaluation of these proposed paradigms is outside the scope of this paper, but we hope it will spark a productive discussion on the next steps toward building agents with stronger semantic understanding.</p>
<p>Ethical Considerations</p>
<p>Autonomous decision-making agents are potentially impactful in our society, and it is of great ethical consideration to make sure their understanding of the world and their objectives align with humans. Humans use natural language to convey and understand concepts as well as inform decisions, and in this work we investigate whether autonomous agents leverage language semantics similarly to humans in the environment of text-based games. Our findings suggest that the current generation of agents optimized for reinforcement learning objectives might not exhibit human-like language understanding, a phenomenon we should pay attention to and further study.</p>
<p>Figure 1 :
1(a): Sample original gameplay from ZORK I. (b) (c): Our proposed semantic ablations. (b) MIN-OB reduces observations to only the current location name, and (c) HASH replaces observation and action texts by their string hash values.</p>
<p>Figure 2 :
2Transfer results from ZORK I.</p>
<p>Figure 3
3: t-SNE visualization of seen and unseen state observations of ZORK I. DRRN (base) represents unseen states separated from seen states while INV-DY mixes them by semantic similarity.</p>
<p>). Breaking Semantics via Hashing (HASH) GRU encoders f o and f a in the Q-network (1) generally ensure that similar texts (e.g. a single word change) are given similar representations, and therefore similar values. To study whether such a semantics continuity is useful, we break it by hashing observation and action texts. Specifically,Table 1: Final/maximum score of different models.Game DRRN </p>
<p>MIN-OB 
HASH 
INV-DY </p>
<p>Max </p>
<p>balances 10 / 10 
10 / 10 
10 / 10 
10 / 10 
51 
deephome 57 / 66 
8.5 / 27 
58 / 67 
57.6 / 67 300 
detective 290 / 337 86.3 / 350 290 / 317 290 / 323 360 
dragon -5.0 / 6 
-5.4 / 3 
-5.0 / 7 
-2.7 / 8 
25 
enchanter 20 / 20 
20 / 40 
20 / 30 
20 / 30 
400 
inhumane 21.1 / 45 12.4 / 40 
21.9 / 45 19.6 / 45 90 
library 15.7 / 21 12.8 / 21 
17 / 21 
16.2 / 21 30 
ludicorp 12.7 / 23 11.6 / 21 
14.8 / 23 13.5 / 23 150 
omniquest 4.9 / 5 
4.9 / 5 
4.9 / 5 
5.3 / 10 
50 
pentari 26.5 / 45 21.7 / 45 
51.9 / 60 37.2 / 50 70 
zork1 39.4 / 53 29 / 46 
35.5 / 50 43.1 / 87 350 
zork3 0.4 / 4.5 
0.0 / 4 
0.4 / 4 
0.4 / 4 
7 </p>
<p>Avg. Norm .21 / .38 
.12 / .35 
.25 / .39 
.23 / .40 </p>
<p>§ The rest 150 states in the middle (score 45-157) are omitted as they might be seen by some model but not others. ¶ Randomness includes transition uncertainty (e.g. thief showing up randomly in ZORK I) and occasional paraphrasing of text observations.
AcknowledgementsWe appreciate helpful suggestions from anonymous reviewers as well as members of the Princeton NLP Group and MSR RL Group.
Learning dynamic knowledge graphs to generalize on text-based games. Ashutosh Adhikari, ( Xingdi, ) Eric, Marc-Alexandre Yuan, Mikulas Côté, Marc-Antoine Zelinka, Romain Rondeau, Pascal Laroche, Jian Poupart, Adam Tang, William L Trischler, Hamilton, NeurIPS. Ashutosh Adhikari, Xingdi (Eric) Yuan, Marc- Alexandre Côté, Mikulas Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L. Hamilton . 2020. Learning dynamic knowledge graphs to gen- eralize on text-based games. In NeurIPS 2020.</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew J Hausknecht, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020OpenReview.netPrithviraj Ammanabrolu and Matthew J. Hausknecht. 2020. Graph constrained reinforcement learning for natural language action spaces. In 8th Inter- national Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>How to avoid being eaten by a grue: Exploration strategies for text-adventure agents. Prithviraj Ammanabrolu, Ethan Tien, Zhaochen Luo, Mark O Riedl, arXiv:2002.08795arXiv preprintPrithviraj Ammanabrolu, Ethan Tien, Zhaochen Luo, and Mark O Riedl. 2020. How to avoid being eaten by a grue: Exploration strategies for text-adventure agents. arXiv preprint arXiv:2002.08795.</p>
<p>On the properties of neural machine translation: Encoder-decoder approaches. Kyunghyun Cho, Dzmitry Bart Van Merriënboer, Yoshua Bahdanau, Bengio, 10.3115/v1/W14-4012Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical TranslationDoha, QatarAssociation for Computational LinguisticsKyunghyun Cho, Bart van Merriënboer, Dzmitry Bah- danau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder ap- proaches. In Proceedings of SSST-8, Eighth Work- shop on Syntax, Semantics and Structure in Statisti- cal Translation, pages 103-111, Doha, Qatar. Asso- ciation for Computational Linguistics.</p>
<p>What can you do with a rock? affordance extraction via word embeddings. Nancy Fulda, Daniel Ricks, Ben Murdoch, David Wingate, 10.24963/ijcai.2017/144Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017. the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017Melbourne, Australiaijcai.orgNancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. 2017. What can you do with a rock? affor- dance extraction via word embeddings. In Proceed- ings of the Twenty-Sixth International Joint Con- ference on Artificial Intelligence, IJCAI 2017, Mel- bourne, Australia, August 19-25, 2017, pages 1039- 1045. ijcai.org.</p>
<p>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, Shiyu Chang, 10.18653/v1/2020.emnlp-main.624Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Mur- ray Campbell, and Shiyu Chang. 2020. Interac- tive fiction game playing as multi-paragraph read- ing comprehension with reinforcement learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7755-7765, Online. Association for Computa- tional Linguistics.</p>
<p>Nail: A general interactive fiction agent. Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, Jason D Williams, arXiv:1902.04259arXiv preprintMatthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. 2019. Nail: A general interactive fiction agent. arXiv preprint arXiv:1902.04259.</p>
<p>Interactive fiction games: A colossal adventure. Matthew J Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020The Tenth AAAI Symposium on Educational Advances in Artificial IntelligenceMatthew J. Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. 2020. In- teractive fiction games: A colossal adventure. In The Thirty-Fourth AAAI Conference on Artificial In- telligence, AAAI 2020, The Thirty-Second Innova- tive Applications of Artificial Intelligence Confer- ence, IAAI 2020, The Tenth AAAI Symposium on Ed- ucational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7903-7910. AAAI Press.</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, 10.18653/v1/P16-1153Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyLong Papers1Association for Computational LinguisticsJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Li- hong Li, Li Deng, and Mari Ostendorf. 2016. Deep reinforcement learning with a natural language ac- tion space. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630, Berlin, Germany. Association for Computational Linguis- tics.</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579-2605.</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, 10.18653/v1/D15-1001Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsKarthik Narasimhan, Tejas Kulkarni, and Regina Barzi- lay. 2015. Language understanding for text-based games using deep reinforcement learning. In Pro- ceedings of the 2015 Conference on Empirical Meth- ods in Natural Language Processing, pages 1-11, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Curiosity-driven exploration by self-supervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, PMLRProceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, NSW, Australia70Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. 2017. Curiosity-driven exploration by self-supervised prediction. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learn- ing Research, pages 2778-2787. PMLR.</p>
<p>Keep CALM and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, 10.18653/v1/2020.emnlp-main.704Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. 2020. Keep CALM and ex- plore: Language models for action generation in text-based games. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 8736-8754, Online. As- sociation for Computational Linguistics.</p>
<p>Deriving commonsense inference tasks from interactive fictions. Mo Yu, Xiaoxiao Guo, Yufei Feng, Xiaodan Zhu, Michael Greenspan, Murray Campbell, arXiv:2010.09788arXiv preprintMo Yu, Xiaoxiao Guo, Yufei Feng, Xiaodan Zhu, Michael Greenspan, and Murray Campbell. 2020. Deriving commonsense inference tasks from inter- active fictions. arXiv preprint arXiv:2010.09788.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6023 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6023</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6023</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-265067168</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.05232v1.pdf" target="_blank">A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6023.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6023.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieve External Facts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieve External Facts / Retrieval-based Fact-checking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that evaluates LLM outputs by retrieving supporting or contradicting evidence from external knowledge sources and comparing retrieved evidence to model-generated claims via a verification pipeline (retrieval, claim decomposition, summarization, veracity classification).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Retrieve relevant documents from external sources (search/web/Wikipedia/knowledge graphs), decompose generated claims into atomic assertions, retrieve fine-grained evidence per assertion, and run veracity classification or support scoring for each assertion.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary or multi-class veracity labels (supported/refuted/unsupported), percentage of atomic facts supported, veracity classification accuracy, evidence-based precision/recall.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General / scientific domains where external literature exists (e.g., medicine, natural sciences, social sciences).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-generated scientific claims or explanatory theories are decomposed into checkable facts; each fact is verified against retrieved external evidence to evaluate overall theory factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey reports that retrieval-based pipelines (e.g., Chen et al.'s automated pipeline, FACTSCORE-style decomposition) effectively detect many factual errors, but success depends strongly on retrieval quality, evidence ranking, and timeliness of sources.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Mentioned or used with pipelines such as FACTOR (Wiki-FACTOR, News-FACTOR), FACTSCORE, REALTIMEQA, FreshQA, FELM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Often combined with human annotation as a gold standard; some studies use LLMs (GPT-4) as automatic judges, but human labels remain primary for final evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval can miss or return irrelevant/conflicting evidence (topic drift); evidence in the wild may be uncurated or time-constrained; citation inaccuracies and ranking errors can lead to false negatives/positives; novel or speculative scientific theories may lack retrievable evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6023.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6023.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uncertainty Estimation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uncertainty Estimation (internal states and behavioral methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that detect hallucinations by estimating model uncertainty via internal signals (token probabilities, entropy, reconstruction likelihood) or observable behavior (self-consistency across samples, multi-query disagreement).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute uncertainty measures (entropy, minimum token probability, predictive entropy) from model logits or probe behavior via sampling multiple generations and measuring consistency; set thresholds for flagging uncertain / hallucinated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Entropy / probability thresholds, calibration metrics, consistency rates across multiple samples, AUROC for distinguishing factual vs. hallucinated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General; applicable to scientific claim generation where model confidence is informative.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-generated theories are flagged as higher-risk for hallucination when internal confidence is low or when multiple independent generations disagree on key factual claims.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey cites work showing low token probability / high entropy correlates with hallucination and that self-consistency (sampling multiple answers) can reveal unstable claims; however models can be overconfident, and internal-state access is sometimes unavailable via APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SelfCheckGPT (zero-resource detection), benchmarks used in uncertainty studies (not always standardized).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Used as an automatic proxy for human uncertainty detection; complements human review but does not yet replace expert judgement due to overconfidence and calibration issues.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires access to token-level probabilities or internal activations (not always available); threshold selection is brittle and domain-dependent; models may be confidently wrong.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6023.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6023.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QA-based Metrics / FACTSCORE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QA-based factuality metrics (e.g., FACTSCORE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Metrics that convert generated content into QA pairs (extract answers), answer generation supports, then compare answers derived from the generation to answers extracted from the source or external evidence to measure factual overlap and support.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Factscore: Fine-grained atomic evaluation of factual precision in long form text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Decompose generation into atomic facts, generate questions for those facts, answer those questions from the source/evidence, and compute matching/overlap between target answers (from generation) and source answers (from evidence) to score factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Fraction/percentage of atomic facts supported, answer match scores (exact / token F1), aggregated factual precision metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Long-form scientific explanations/theories and general knowledge domains.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>A scientific theory generated by an LLM is split into elementary factual claims; each claim is checked via QA to determine whether it is supported by external sources, yielding a fine-grained factuality profile.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey highlights FACTSCORE as a fine-grained approach useful for long-form generation; it improves diagnostic granularity but struggles when facts are nuanced, debatable, or lack external grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>FactScore, FACTOR (auto-created perturbed completions), FreshQA, TruthfulQA for related evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>QA-based metrics have stronger correlation with human judgements than n-gram overlap in many summarization studies, but human adjudication is still used to resolve ambiguous matches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Question generation and answer selection design choices affect performance; some scientific claims are not decomposable into atomic verifiable facts; evidence scarcity limits applicability for novel hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6023.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6023.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entailment / Classifier-based Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entailment-based and classifier-based faithfulness metrics (NLI classifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use supervised classifiers (often NLI models) fine-tuned or adapted to predict entailment/contradiction between source text and generated text, thereby detecting unfaithful or hallucinated statements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Treat source as premise and generated output as hypothesis; run entailment classifier to obtain entailment/contradiction/neutral labels and use these to flag unfaithful segments or compute overall faithfulness scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Entailment probability, classification accuracy, F1 on labeled hallucination datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Summarization, knowledge-grounded generation, and any domain where a source document exists to compare with (including scientific source texts).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluate whether an LLM-generated theory is entailed by an authoritative scientific source (paper, dataset) using NLI-style classification to judge support or contradiction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey notes entailment-based methods are intuitive and effective in some summarization contexts but suffer from granularity mismatch (sentence vs. document) and require task-specific adaptation; approaches that decompose documents or use dependency-level entailment improve results.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SummaC, FEQA, QAFactEval, datasets used for NLI adaptation and summarization hallucination detection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Classifier outputs are compared to human annotations; training often relies on human-labelled or synthetic hallucination data to approach human-level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Mismatch of input granularity between NLI datasets and generation tasks; need for domain-specific training data; classifiers can miss subtle factual errors that require multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6023.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6023.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-based LLM Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting-based Evaluation (LLMs as Evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use LLMs themselves (via carefully designed prompts) to evaluate the faithfulness or factuality of other model outputs, returning binary judgements, Likert scores, or explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt an LLM with evaluation guidelines plus both the source and generated text; request an explicit verdict (faithful/unfaithful) or a numeric Likert score and optionally an explanation or chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary faithfulness label, k-point Likert scale, or scored explanations; optionally aggregated into accuracy/F1 against human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Various instruction-following LLMs (examples in cited work include GPT-4, GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General; used across summarization, long-form generation, and scientific claim evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>An LLM-generated scientific theory is evaluated by another (or the same) LLM instructed to judge factual support and faithfulness relative to sources or evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey reports promising results: LLM evaluators can align with human judgements in many settings; chaining of prompts or CoT-style evaluation sometimes improves judgments, but results are prompt-sensitive.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Used experimentally in LSum, FELM, HalluQA evaluations; studies using GPT-4 as judge and GPT-judge (fine-tuned) are mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>LLM evaluators are compared to human annotation; they can reduce human effort but are not uniformly reliable and are sensitive to prompt formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Evaluation is sensitive to prompt design and to the evaluator model's biases; LLM-evaluators can be sycophantic or overconfident and may echo flawed sources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6023.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6023.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TruthfulQA: Measuring how models mimic human falsehoods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adversarial benchmark of 817 questions across 38 categories designed to elicit imitative falsehoods from language models, evaluated via human judgments and an automatic judge (GPT-judge).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TruthfulQA: Measuring how models mimic human falsehoods</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Adversarial question set (generation and multiple-choice); models produce answers in generation or multiple-choice format; human evaluation assesses truthfulness and informativeness; GPT-judge used as an automatic metric (fine-tuned).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human-rated truthfulness and informativeness; multiple-choice accuracy; GPT-judge automatic scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Evaluated against GPT-3 and other contemporary models (as reported by Lin et al. and referenced by the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Broad (health, law, finance, politics, etc.); relevant to assessing propensity to generate false factual claims.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM responses to adversarial prompts that mimic common human misconceptions; analogous to evaluating whether an LLM would fabricate or misstate scientific facts in adversarially phrased questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey cites TruthfulQA results showing LLMs often produce imitative falsehoods; GPT-judge offers an automated complement to human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>TruthfulQA (817 Qs, with a filtered subset), GPT-judge used as an auto-evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human evaluation is primary; GPT-judge trained to approximate human judgements but human labels remain the gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Adversarial design focuses on factoid-style prompts and may not capture long-form scientific theory evaluation or nuanced, evidence-based claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6023.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6023.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Med-HALT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Med-HALT: Medical domain hallucination test for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A medical-domain benchmark consisting of multiple-choice reasoning questions and memory tasks to evaluate LLMs' medical factuality and memorization, emphasizing safety-critical evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Med-halt: Medical domain hallucination test for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Two tasks: reasoning (multiple-choice medical questions testing elimination of incorrect options and detection of fake questions) and memory (generate links/titles from PubMed abstracts or PMIDs); evaluated by accuracy or a Pointwise Score with penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy on multiple-choice reasoning; Pointwise Score combining positive points for correct answers and negative penalties for incorrect ones.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Medical domain (clinical knowledge, diagnostics, literature recall).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-generated medical claims/explanations or proposed clinical theories are evaluated for factual correctness and memorization against established medical question banks and PubMed references.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey highlights Med-HALT's role assessing reasoning and memorization, noting variable LLM performance and emphasizing high stakes of hallucinations in medical contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Med-HALT dataset (reasoning: 18,866 samples; memory: 4,916 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Uses established medical question formats; human domain expertise required to interpret failures and to construct the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires domain experts to annotate and interpret; multiple-choice format may not capture long-form theoretical speculation; recall tasks test memorization rather than novel theory generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6023.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6023.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FELM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FELM: Benchmarking factuality evaluation of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-domain benchmark that samples zero-shot ChatGPT responses and annotates segments for factuality, error reasons and types, serving as a testbed for factuality detectors across diverse domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Felm: Benchmarking factuality evaluation of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Annotate model-generated responses at segment level for factuality, error reason, and error type; evaluate detectors using segment/response-level classification with F1 and balanced classification accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Segment-level F1, balanced classification accuracy at segment and response levels; error-type breakdown.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChatGPT (used to generate zero-shot responses in dataset construction); detectors evaluated on various LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Multiple: world knowledge, science & technology, mathematics, writing & recommendation, reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-generated statements (segments) within long responses are labeled for factual correctness; applicable to assessing pieces of scientific theories for factual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey reports FELM provides fine-grained annotated segments (817 samples; 3948 segments) and uses F1/balanced accuracy to evaluate detectors; highlights diversity of factual error types.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>FELM (817 samples / 3948 segments), used to assess factual detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Dataset is human-annotated; automated detectors are measured against these annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Segment-level annotation is costly; generalization across domains and detecting nuanced scientific errors remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6023.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6023.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheck / SelfCheckGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheckGPT / Self-checking methods (zero-resource black-box hallucination detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-resource black-box methods that detect hallucinations by generating synthetic examples or using the LLM to check its own outputs without internal state access, e.g., sampling multiple outputs and checking self-consistency or using constrained reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SelfCheckGPT-Wikibio</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Generate multiple outputs for the same prompt and measure consistency across outputs, or use the model to reconstruct concepts from its own explanations (self-evaluation); synthetic data generation and sampling-then-filtering pipelines create labeled detection data.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Inter-generation consistency, AUROC for detection tasks, precision/recall/F1 on synthetic/human-annotated detection sets.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>General; applicable when internal probabilities are not accessible (API-only models).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Detect hallucinated claims in LLM-generated theories by observing inconsistent answers across multiple generations or by checking the model's ability to reconstruct facts from its own explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey indicates black-box self-consistency methods (e.g., SelfCheckGPT, ChainPoll) can flag many hallucinations; however, they rely on sampling budgets and may fail when models are consistently wrong or sycophantic.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>SelfCheckGPT-Wikibio (synthetic Wikipedia articles annotated sentence-level), HaluEval, ChainPoll references.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Often validated against human annotations; can reduce annotation needs but not a full replacement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High sample complexity for reliable detection; models may produce consistent but incorrect answers; indirect methods susceptible to adversarial prompt formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Factscore: Fine-grained atomic evaluation of factual precision in long form text generation <em>(Rating: 2)</em></li>
                <li>TruthfulQA: Measuring how models mimic human falsehoods <em>(Rating: 2)</em></li>
                <li>Med-halt: Medical domain hallucination test for large language models <em>(Rating: 2)</em></li>
                <li>Felm: Benchmarking factuality evaluation of large language models <em>(Rating: 2)</em></li>
                <li>SelfCheckGPT-Wikibio <em>(Rating: 2)</em></li>
                <li>Complex claim verification with evidence retrieved in the wild <em>(Rating: 1)</em></li>
                <li>Chain-of-Verification reduces hallucination in large language models <em>(Rating: 1)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6023",
    "paper_id": "paper-265067168",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "Retrieve External Facts",
            "name_full": "Retrieve External Facts / Retrieval-based Fact-checking",
            "brief_description": "A method that evaluates LLM outputs by retrieving supporting or contradicting evidence from external knowledge sources and comparing retrieved evidence to model-generated claims via a verification pipeline (retrieval, claim decomposition, summarization, veracity classification).",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method": "Retrieve relevant documents from external sources (search/web/Wikipedia/knowledge graphs), decompose generated claims into atomic assertions, retrieve fine-grained evidence per assertion, and run veracity classification or support scoring for each assertion.",
            "evaluation_criteria": "Binary or multi-class veracity labels (supported/refuted/unsupported), percentage of atomic facts supported, veracity classification accuracy, evidence-based precision/recall.",
            "llm_model_name": null,
            "theory_domain": "General / scientific domains where external literature exists (e.g., medicine, natural sciences, social sciences).",
            "theory_description": "LLM-generated scientific claims or explanatory theories are decomposed into checkable facts; each fact is verified against retrieved external evidence to evaluate overall theory factuality.",
            "evaluation_results": "Survey reports that retrieval-based pipelines (e.g., Chen et al.'s automated pipeline, FACTSCORE-style decomposition) effectively detect many factual errors, but success depends strongly on retrieval quality, evidence ranking, and timeliness of sources.",
            "benchmarks_or_datasets": "Mentioned or used with pipelines such as FACTOR (Wiki-FACTOR, News-FACTOR), FACTSCORE, REALTIMEQA, FreshQA, FELM.",
            "comparison_to_human": "Often combined with human annotation as a gold standard; some studies use LLMs (GPT-4) as automatic judges, but human labels remain primary for final evaluation.",
            "limitations_or_challenges": "Retrieval can miss or return irrelevant/conflicting evidence (topic drift); evidence in the wild may be uncurated or time-constrained; citation inaccuracies and ranking errors can lead to false negatives/positives; novel or speculative scientific theories may lack retrievable evidence.",
            "uuid": "e6023.0",
            "source_info": {
                "paper_title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Uncertainty Estimation",
            "name_full": "Uncertainty Estimation (internal states and behavioral methods)",
            "brief_description": "Approaches that detect hallucinations by estimating model uncertainty via internal signals (token probabilities, entropy, reconstruction likelihood) or observable behavior (self-consistency across samples, multi-query disagreement).",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method": "Compute uncertainty measures (entropy, minimum token probability, predictive entropy) from model logits or probe behavior via sampling multiple generations and measuring consistency; set thresholds for flagging uncertain / hallucinated outputs.",
            "evaluation_criteria": "Entropy / probability thresholds, calibration metrics, consistency rates across multiple samples, AUROC for distinguishing factual vs. hallucinated outputs.",
            "llm_model_name": null,
            "theory_domain": "General; applicable to scientific claim generation where model confidence is informative.",
            "theory_description": "LLM-generated theories are flagged as higher-risk for hallucination when internal confidence is low or when multiple independent generations disagree on key factual claims.",
            "evaluation_results": "Survey cites work showing low token probability / high entropy correlates with hallucination and that self-consistency (sampling multiple answers) can reveal unstable claims; however models can be overconfident, and internal-state access is sometimes unavailable via APIs.",
            "benchmarks_or_datasets": "SelfCheckGPT (zero-resource detection), benchmarks used in uncertainty studies (not always standardized).",
            "comparison_to_human": "Used as an automatic proxy for human uncertainty detection; complements human review but does not yet replace expert judgement due to overconfidence and calibration issues.",
            "limitations_or_challenges": "Requires access to token-level probabilities or internal activations (not always available); threshold selection is brittle and domain-dependent; models may be confidently wrong.",
            "uuid": "e6023.1",
            "source_info": {
                "paper_title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "QA-based Metrics / FACTSCORE",
            "name_full": "QA-based factuality metrics (e.g., FACTSCORE)",
            "brief_description": "Metrics that convert generated content into QA pairs (extract answers), answer generation supports, then compare answers derived from the generation to answers extracted from the source or external evidence to measure factual overlap and support.",
            "citation_title": "Factscore: Fine-grained atomic evaluation of factual precision in long form text generation",
            "mention_or_use": "mention",
            "evaluation_method": "Decompose generation into atomic facts, generate questions for those facts, answer those questions from the source/evidence, and compute matching/overlap between target answers (from generation) and source answers (from evidence) to score factuality.",
            "evaluation_criteria": "Fraction/percentage of atomic facts supported, answer match scores (exact / token F1), aggregated factual precision metrics.",
            "llm_model_name": null,
            "theory_domain": "Long-form scientific explanations/theories and general knowledge domains.",
            "theory_description": "A scientific theory generated by an LLM is split into elementary factual claims; each claim is checked via QA to determine whether it is supported by external sources, yielding a fine-grained factuality profile.",
            "evaluation_results": "Survey highlights FACTSCORE as a fine-grained approach useful for long-form generation; it improves diagnostic granularity but struggles when facts are nuanced, debatable, or lack external grounding.",
            "benchmarks_or_datasets": "FactScore, FACTOR (auto-created perturbed completions), FreshQA, TruthfulQA for related evaluations.",
            "comparison_to_human": "QA-based metrics have stronger correlation with human judgements than n-gram overlap in many summarization studies, but human adjudication is still used to resolve ambiguous matches.",
            "limitations_or_challenges": "Question generation and answer selection design choices affect performance; some scientific claims are not decomposable into atomic verifiable facts; evidence scarcity limits applicability for novel hypotheses.",
            "uuid": "e6023.2",
            "source_info": {
                "paper_title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Entailment / Classifier-based Metrics",
            "name_full": "Entailment-based and classifier-based faithfulness metrics (NLI classifiers)",
            "brief_description": "Use supervised classifiers (often NLI models) fine-tuned or adapted to predict entailment/contradiction between source text and generated text, thereby detecting unfaithful or hallucinated statements.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method": "Treat source as premise and generated output as hypothesis; run entailment classifier to obtain entailment/contradiction/neutral labels and use these to flag unfaithful segments or compute overall faithfulness scores.",
            "evaluation_criteria": "Entailment probability, classification accuracy, F1 on labeled hallucination datasets.",
            "llm_model_name": null,
            "theory_domain": "Summarization, knowledge-grounded generation, and any domain where a source document exists to compare with (including scientific source texts).",
            "theory_description": "Evaluate whether an LLM-generated theory is entailed by an authoritative scientific source (paper, dataset) using NLI-style classification to judge support or contradiction.",
            "evaluation_results": "Survey notes entailment-based methods are intuitive and effective in some summarization contexts but suffer from granularity mismatch (sentence vs. document) and require task-specific adaptation; approaches that decompose documents or use dependency-level entailment improve results.",
            "benchmarks_or_datasets": "SummaC, FEQA, QAFactEval, datasets used for NLI adaptation and summarization hallucination detection.",
            "comparison_to_human": "Classifier outputs are compared to human annotations; training often relies on human-labelled or synthetic hallucination data to approach human-level performance.",
            "limitations_or_challenges": "Mismatch of input granularity between NLI datasets and generation tasks; need for domain-specific training data; classifiers can miss subtle factual errors that require multi-hop reasoning.",
            "uuid": "e6023.3",
            "source_info": {
                "paper_title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Prompt-based LLM Evaluation",
            "name_full": "Prompting-based Evaluation (LLMs as Evaluators)",
            "brief_description": "Use LLMs themselves (via carefully designed prompts) to evaluate the faithfulness or factuality of other model outputs, returning binary judgements, Likert scores, or explanations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method": "Prompt an LLM with evaluation guidelines plus both the source and generated text; request an explicit verdict (faithful/unfaithful) or a numeric Likert score and optionally an explanation or chain-of-thought.",
            "evaluation_criteria": "Binary faithfulness label, k-point Likert scale, or scored explanations; optionally aggregated into accuracy/F1 against human labels.",
            "llm_model_name": "Various instruction-following LLMs (examples in cited work include GPT-4, GPT-3.5 family)",
            "theory_domain": "General; used across summarization, long-form generation, and scientific claim evaluation.",
            "theory_description": "An LLM-generated scientific theory is evaluated by another (or the same) LLM instructed to judge factual support and faithfulness relative to sources or evidence.",
            "evaluation_results": "Survey reports promising results: LLM evaluators can align with human judgements in many settings; chaining of prompts or CoT-style evaluation sometimes improves judgments, but results are prompt-sensitive.",
            "benchmarks_or_datasets": "Used experimentally in LSum, FELM, HalluQA evaluations; studies using GPT-4 as judge and GPT-judge (fine-tuned) are mentioned.",
            "comparison_to_human": "LLM evaluators are compared to human annotation; they can reduce human effort but are not uniformly reliable and are sensitive to prompt formulation.",
            "limitations_or_challenges": "Evaluation is sensitive to prompt design and to the evaluator model's biases; LLM-evaluators can be sycophantic or overconfident and may echo flawed sources.",
            "uuid": "e6023.4",
            "source_info": {
                "paper_title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "TruthfulQA",
            "name_full": "TruthfulQA: Measuring how models mimic human falsehoods",
            "brief_description": "An adversarial benchmark of 817 questions across 38 categories designed to elicit imitative falsehoods from language models, evaluated via human judgments and an automatic judge (GPT-judge).",
            "citation_title": "TruthfulQA: Measuring how models mimic human falsehoods",
            "mention_or_use": "mention",
            "evaluation_method": "Adversarial question set (generation and multiple-choice); models produce answers in generation or multiple-choice format; human evaluation assesses truthfulness and informativeness; GPT-judge used as an automatic metric (fine-tuned).",
            "evaluation_criteria": "Human-rated truthfulness and informativeness; multiple-choice accuracy; GPT-judge automatic scoring.",
            "llm_model_name": "Evaluated against GPT-3 and other contemporary models (as reported by Lin et al. and referenced by the survey).",
            "theory_domain": "Broad (health, law, finance, politics, etc.); relevant to assessing propensity to generate false factual claims.",
            "theory_description": "LLM responses to adversarial prompts that mimic common human misconceptions; analogous to evaluating whether an LLM would fabricate or misstate scientific facts in adversarially phrased questions.",
            "evaluation_results": "Survey cites TruthfulQA results showing LLMs often produce imitative falsehoods; GPT-judge offers an automated complement to human annotation.",
            "benchmarks_or_datasets": "TruthfulQA (817 Qs, with a filtered subset), GPT-judge used as an auto-evaluator.",
            "comparison_to_human": "Human evaluation is primary; GPT-judge trained to approximate human judgements but human labels remain the gold standard.",
            "limitations_or_challenges": "Adversarial design focuses on factoid-style prompts and may not capture long-form scientific theory evaluation or nuanced, evidence-based claims.",
            "uuid": "e6023.5",
            "source_info": {
                "paper_title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Med-HALT",
            "name_full": "Med-HALT: Medical domain hallucination test for large language models",
            "brief_description": "A medical-domain benchmark consisting of multiple-choice reasoning questions and memory tasks to evaluate LLMs' medical factuality and memorization, emphasizing safety-critical evaluation.",
            "citation_title": "Med-halt: Medical domain hallucination test for large language models",
            "mention_or_use": "mention",
            "evaluation_method": "Two tasks: reasoning (multiple-choice medical questions testing elimination of incorrect options and detection of fake questions) and memory (generate links/titles from PubMed abstracts or PMIDs); evaluated by accuracy or a Pointwise Score with penalties.",
            "evaluation_criteria": "Accuracy on multiple-choice reasoning; Pointwise Score combining positive points for correct answers and negative penalties for incorrect ones.",
            "llm_model_name": null,
            "theory_domain": "Medical domain (clinical knowledge, diagnostics, literature recall).",
            "theory_description": "LLM-generated medical claims/explanations or proposed clinical theories are evaluated for factual correctness and memorization against established medical question banks and PubMed references.",
            "evaluation_results": "Survey highlights Med-HALT's role assessing reasoning and memorization, noting variable LLM performance and emphasizing high stakes of hallucinations in medical contexts.",
            "benchmarks_or_datasets": "Med-HALT dataset (reasoning: 18,866 samples; memory: 4,916 samples).",
            "comparison_to_human": "Uses established medical question formats; human domain expertise required to interpret failures and to construct the dataset.",
            "limitations_or_challenges": "Requires domain experts to annotate and interpret; multiple-choice format may not capture long-form theoretical speculation; recall tasks test memorization rather than novel theory generation.",
            "uuid": "e6023.6",
            "source_info": {
                "paper_title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "FELM",
            "name_full": "FELM: Benchmarking factuality evaluation of large language models",
            "brief_description": "A multi-domain benchmark that samples zero-shot ChatGPT responses and annotates segments for factuality, error reasons and types, serving as a testbed for factuality detectors across diverse domains.",
            "citation_title": "Felm: Benchmarking factuality evaluation of large language models",
            "mention_or_use": "mention",
            "evaluation_method": "Annotate model-generated responses at segment level for factuality, error reason, and error type; evaluate detectors using segment/response-level classification with F1 and balanced classification accuracy.",
            "evaluation_criteria": "Segment-level F1, balanced classification accuracy at segment and response levels; error-type breakdown.",
            "llm_model_name": "ChatGPT (used to generate zero-shot responses in dataset construction); detectors evaluated on various LLM outputs.",
            "theory_domain": "Multiple: world knowledge, science & technology, mathematics, writing & recommendation, reasoning.",
            "theory_description": "LLM-generated statements (segments) within long responses are labeled for factual correctness; applicable to assessing pieces of scientific theories for factual errors.",
            "evaluation_results": "Survey reports FELM provides fine-grained annotated segments (817 samples; 3948 segments) and uses F1/balanced accuracy to evaluate detectors; highlights diversity of factual error types.",
            "benchmarks_or_datasets": "FELM (817 samples / 3948 segments), used to assess factual detectors.",
            "comparison_to_human": "Dataset is human-annotated; automated detectors are measured against these annotations.",
            "limitations_or_challenges": "Segment-level annotation is costly; generalization across domains and detecting nuanced scientific errors remains challenging.",
            "uuid": "e6023.7",
            "source_info": {
                "paper_title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "SelfCheck / SelfCheckGPT",
            "name_full": "SelfCheckGPT / Self-checking methods (zero-resource black-box hallucination detection)",
            "brief_description": "Zero-resource black-box methods that detect hallucinations by generating synthetic examples or using the LLM to check its own outputs without internal state access, e.g., sampling multiple outputs and checking self-consistency or using constrained reconstruction.",
            "citation_title": "SelfCheckGPT-Wikibio",
            "mention_or_use": "mention",
            "evaluation_method": "Generate multiple outputs for the same prompt and measure consistency across outputs, or use the model to reconstruct concepts from its own explanations (self-evaluation); synthetic data generation and sampling-then-filtering pipelines create labeled detection data.",
            "evaluation_criteria": "Inter-generation consistency, AUROC for detection tasks, precision/recall/F1 on synthetic/human-annotated detection sets.",
            "llm_model_name": null,
            "theory_domain": "General; applicable when internal probabilities are not accessible (API-only models).",
            "theory_description": "Detect hallucinated claims in LLM-generated theories by observing inconsistent answers across multiple generations or by checking the model's ability to reconstruct facts from its own explanations.",
            "evaluation_results": "Survey indicates black-box self-consistency methods (e.g., SelfCheckGPT, ChainPoll) can flag many hallucinations; however, they rely on sampling budgets and may fail when models are consistently wrong or sycophantic.",
            "benchmarks_or_datasets": "SelfCheckGPT-Wikibio (synthetic Wikipedia articles annotated sentence-level), HaluEval, ChainPoll references.",
            "comparison_to_human": "Often validated against human annotations; can reduce annotation needs but not a full replacement.",
            "limitations_or_challenges": "High sample complexity for reliable detection; models may produce consistent but incorrect answers; indirect methods susceptible to adversarial prompt formulations.",
            "uuid": "e6023.8",
            "source_info": {
                "paper_title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Factscore: Fine-grained atomic evaluation of factual precision in long form text generation",
            "rating": 2,
            "sanitized_title": "factscore_finegrained_atomic_evaluation_of_factual_precision_in_long_form_text_generation"
        },
        {
            "paper_title": "TruthfulQA: Measuring how models mimic human falsehoods",
            "rating": 2,
            "sanitized_title": "truthfulqa_measuring_how_models_mimic_human_falsehoods"
        },
        {
            "paper_title": "Med-halt: Medical domain hallucination test for large language models",
            "rating": 2,
            "sanitized_title": "medhalt_medical_domain_hallucination_test_for_large_language_models"
        },
        {
            "paper_title": "Felm: Benchmarking factuality evaluation of large language models",
            "rating": 2,
            "sanitized_title": "felm_benchmarking_factuality_evaluation_of_large_language_models"
        },
        {
            "paper_title": "SelfCheckGPT-Wikibio",
            "rating": 2,
            "sanitized_title": "selfcheckgptwikibio"
        },
        {
            "paper_title": "Complex claim verification with evidence retrieved in the wild",
            "rating": 1,
            "sanitized_title": "complex_claim_verification_with_evidence_retrieved_in_the_wild"
        },
        {
            "paper_title": "Chain-of-Verification reduces hallucination in large language models",
            "rating": 1,
            "sanitized_title": "chainofverification_reduces_hallucination_in_large_language_models"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        }
    ],
    "cost": 0.026854749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions
9 Nov 2023</p>
<p>Lei Huang lhuang@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Weijiang Yu weijiangyu8@gmail.com 
Huawei Inc
ShenzhenChina</p>
<p>Weitao Ma wtma@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Weihong Zhong whzhong@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Zhangyin Feng zyfeng@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Haotian Wang 
Harbin Institute of Technology
HarbinChina</p>
<p>Qianglong Chen chenqianglong.ai@gmail.com 
Huawei Inc
ShenzhenChina</p>
<p>Weihua Peng 
Huawei Inc
ShenzhenChina</p>
<p>Xiaocheng Feng xcfeng@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Bing Qin qinb@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Ting Liu tliu@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions
9 Nov 202398D1C2953E4AD9D7536C18136BA1B4CFarXiv:2311.05232v1[cs.CL]https:github.comLuckyyySTAAwesome-LLM-hallucination
The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation.Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs.This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations.In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations.We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations.Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks.Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly.Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs. 1</p>
<p>Introduction</p>
<p>Recently, the emergence of large language models (LLMs) (OpenAI, 2022;Google, 2023;Touvron et al., 2023;Penedo et al., 2023;Zhao et al., 2023b) has ushered in a paradigm shift in natural language processing (NLP), achieving unprecedented progress in language understanding (Hendrycks et al., 2021;Huang et al., 2023c), generation (Zhang et al., 2023f;Zhu et al., 2023b) and reasoning (Wei et al., 2022;Kojima et al., 2022;Qiao et al., 2022;Yu et al., 2023a;Chu et al., 2023).Nevertheless, in tandem with the rapid advancement in LLMs, there's a concerning trend where they exhibit an inclination to generate hallucinations (Bang et al., 2023;Guerreiro et al., 2023b), resulting in seemingly plausible yet factually unsupported content.</p>
<p>The current definition of hallucinations aligns with prior research (Ji et al., 2023a), characterizing them as generated content that is nonsensical or unfaithful to the provided source content.These hallucinations are further categorized into intrinsic hallucination and extrinsic hallucination types, depending on the contradiction with the source content.While this category is shared among various natural language generation (NLG) tasks, taskspecific variations do exist.As LLMs are remarkably versatile and excel across different NLG tasks (Bubeck et al., 2023;Bang et al., 2023), particularly in open-domain applications, their remarkable versatility amplifies the potential for hallucinations compared to task-specific models.In LLMs, the scope of hallucination encompasses a broader and more comprehensive concept, primarily centering on factual errors.In light of the evolution of the LLM era, there arises a need to adjust the existing hallucination taxonomy, enhancing its applicability and adaptability.</p>
<p>In this survey, we have redefined the taxonomy of hallucination, offering a more tailored framework for LLM applications.We categorize hallucination into two main groups: factuality hallucination and faithfulness hallucination.Factuality hallucination emphasizes the discrepancy between generated content and verifiable real-world facts, typically manifesting as factual inconsistency or fabrication.For example, as in Fig. 1(a), when queried about the first person to walk on the moon, Answer: The first person to walk on the moon was Charles Lindbergh in 1951, during the Lunar Pioneer mission.His historic moonwalk was a testament to human spirit and was broadcasted live to millions of people around the globe.</p>
<p>Who was the first person to walk on the moon?Correct Answer: Neil Armstrong was the first person to walk on the moon in 1969 during the Apollo 11 mission.As for the underlying causes of hallucinations, while studied in the context of NLG tasks, present unique challenges in cutting-edge LLMs that are worthy of an in-depth investigation.Our in-depth analysis specifically targets the unique origins of hallucinations in LLMs, spanning a spectrum of contributing factors from data, and training, to the inference stage.Within this framework, we pinpoint potential data-related causes such as flawed sources and suboptimal utilization, inferior training strategies that may induce hallucinations during pre-training and alignment, and those stemming from the stochastic nature of decoding strategies and imperfect representations during the inference process.Furthermore, we comprehensively outline a variety of effective detection methods specifically devised for detecting hallucinations in LLMs, as well as an exhaustive overview of benchmarks related to LLM hallucinations, serving as appropriate testbeds to assess the extent of hallucinations generated by LLMs and the efficacy of detection methods.Moreover, we detail comprehensive strategies tailored to mitigate the identified causes of hallucinations.</p>
<p>Through this comprehensive survey, we aim to contribute to the advancement of the field of LLMs and provide valuable insights that deepen the understanding of the opportunities and challenges associated with hallucinations in LLMs.This exploration not only enhances our understanding of the limitations of current LLMs but also provides essential guidance for future research and the development of more robust and trustworthy LLMs.</p>
<p>Comparing with Existing Surveys.As the push for reliable generative AI intensifies, LLM hallucination stands out as a major challenge, leading to numerous surveys on its recent advancements (Ji et al., 2023a;Rawte et al., 2023;Liu et al., 2023h;Zhang et al., 2023g;Wang et al., 2023c).While these works have probed into LLM hallucination from diverse angles and offered valuable insights, it is imperative to distinguish the unique aspects and comprehensive nature of our present survey.(Ji et al., 2023a) primarily sheds light on hallucinations in pre-trained language models within the realm of NLG tasks, leaving LLMs outside their discussion purview.(Liu et al., 2023h) discusses the trustworthiness of LLMs from a broader perspective, while (Wang et al., 2023c) delves deeply into LLM factuality.In contrast, our survey zeroes in on a subset of challenges in LLM trustworthiness, covering aspects of factuality and further broadening the discourse to include faithfulnessrelated hallucinations.To the best of our knowledge, the work most aligned with our survey is (Zhang et al., 2023g), which outlines taxonomies of LLM hallucination phenomena, evaluation benchmarks, and mitigation strategies.Nevertheless, our survey distinguishes itself both in terms of its taxonomy and organizational structure.We present a layered and granular classification of hallucinations.Structurally, we dissect the causes of LLM hallucination by tracing back to the capabilities of LLMs.More pertinently, our mitigation strategies are intricately linked with the underlying causes, ensuring a cohesive and targeted approach.</p>
<p>Organization of this Survey.In this paper, we present a comprehensive survey of the latest developments regarding hallucinations in LLMs.We commence by defining LLMs and constructing a taxonomy of hallucinations within this context ( 2).Subsequently, we analyze the factors contributing to hallucinations in LLMs in depth ( 3), followed by an examination of various methodologies and benchmarks employed for the reliable detection of hallucinations in LLMs ( 4).We then detail a spectrum of approaches designed to mitigate hallucinations in LLMs ( 5).Concluding, we delve into the challenges and open questions that frame the current limitations and future prospects of this field, offering insights and delineating potential pathways for forthcoming research ( 6).</p>
<p>Definitions</p>
<p>For the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a succinct introduction to LLMs ( 2.1), delineating the scope of this survey.Subsequently, we delve into the training process of LLMs ( 2.2), as a thorough understanding of the underlying training mechanisms contributes significantly to elucidating the origins of hallucinations.Lastly, we expound upon the concept of hallucinations ( 2.3) in LLMs, further categorizing it into two distinct types.</p>
<p>Large Language Models</p>
<p>Before delving into the causes of hallucination, we first introduce the concept of LLMs.Typically, LLMs refer to a series of general-purpose models that leverage the Transformer-based language model architecture and undergo extensive training on massive textual corpora with notable examples including GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2023), Galactica (Taylor et al., 2022) LLaMA (Touvron et al., 2023) and GPT-4 (OpenAI, 2023).By scaling the amount of data and model capacity, LLMs raise amazing emergent abilities, typically including In-Context Learning (ICL) (Brown et al., 2020), Chain-of-Thought prompting (Wei et al., 2022) and instruction following (Peng et al., 2023).</p>
<p>Training Stages of Large Language Models</p>
<p>The attributes and behaviors of LLMs are deeply intertwined with their training processes.LLMs undergo three primary training stages: pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF).Analyzing these stages provides insight into hallucination origins in LLMs, as each stage equips the model with specific capabilities.Pre-training.Pre-training is generally considered a crucial stage for LLM to acquire knowledge and skills (Zhou et al., 2023a).Language models, during pre-training, aim to predict the next token in a sequence autoregressively.Through selfsupervised training on extensive textual corpora, the model acquires knowledge of language syntax, world knowledge, and reasoning abilities, providing a robust foundation for subsequent fine-tuning tasks.Besides, recent research (Sutskever, 2023;Deltang et al., 2023) suggests that predicting subsequent words is akin to losslessly compressing significant information.The essence of language models lies in predicting the probability distribution for upcoming words.Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world.</p>
<p>Supervised Fine-Tuning.While LLMs acquire substantial knowledge and capabilities during the pre-training stage, it's crucial to recognize that pretraining primarily optimizes for completion.Consequently, pre-trained LLMs fundamentally served as completion machines, which can lead to a misalignment between the next-word prediction objective of LLMs and the user's objective of obtaining desired responses.To bridge this gap, SFT (Zhang et al., 2023d) has been introduced, which involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, resulting in enhanced capabilities and improved controllability of LLMs.Furthermore, recent studies (Chung et al., 2022;Iyer et al., 2022) have confirmed the effectiveness of supervised fine-tuning to achieve exceptional performance on unseen tasks, showcasing their remarkable generalization abilities.</p>
<p>Reinforcement Learning from Human Feedback.While the SFT process successfully enables LLMs to follow user instructions, there is still room for them to better align with human preferences.Among various methods that utilize human feedback, RLHF stands out as an institute solution for aligning with human preferences through reinforcement learning (Christiano et al., 2017;Stiennon et al., 2020;Ouyang et al., 2022).Typically, RLHF employs a preference model (Bradley and Terry, 1952) trained to predict preference rankings given a prompt alongside a pair of human-labeled responses.To align with human preferences, RLHF optimizes the LLM to generate outputs that maximize the reward provided by the trained preference model, typically employing a reinforcement learning algorithm, such as Proximal Policy Optimization (PPO) (Schulman et al., 2017).Such integration of human feedback into the training loop has proven effective in enhancing the alignment of LLMs, guiding them toward producing high-quality and harmless responses.</p>
<p>Hallucinations in Large Language Models</p>
<p>The concept of hallucination traces its roots to the fields of pathology and psychology and is defined as the perception of an entity or event that is absent in reality (Macpherson and Platchias, 2013).Within the realm of NLP, hallucination is typically referred to as a phenomenon in which the generated content appears nonsensical or unfaithful to the provided source content (Filippova, 2020;Maynez et al., 2020).This concept bears a loose resemblance to the phenomenon of hallucination observed in human psychology.Generally, hallucinations in natural language generation tasks can be categorized into two primary types: intrinsic hallucination and extrinsic hallucination (Huang et al., 2021;Li et al., 2022b;Ji et al., 2023a).Specifically, intrinsic hallucinations pertain to the outputs of LLMs that conflict with the source content.Conversely, extrinsic hallucinations refer to the LLM generations that cannot be verified from the source content.</p>
<p>However, in the era of large language models, the versatile capabilities of these models have facilitated their widespread use across diverse fields, highlighting limitations in existing task-specific categorization paradigms.Considering that LLMs place a significant emphasis on user-centric interactions and prioritize alignment with user directives, coupled with the fact that their hallucinations predominantly surface at factual levels, we introduce a more granular taxonomy building upon the foundational work by Ji et al. (2023a).This refined taxonomy seeks to encapsulate the distinct intricacies associated with LLM hallucinations.To provide a more intuitive illustration of our definition of LLM hallucination, we present examples for each type of hallucination in Table 1, accompanied by corresponding explanations.The details of our proposed categories are elaborated below:</p>
<p>Factuality Hallucination.The emergence of LLMs marks a significant shift from traditional task-specific toolkits to AI assistants that have a heightened focus on open-domain interactions.This shift is primarily attributed to their vast parametric factual knowledge.However, existing LLMs occasionally exhibit tendencies to produce outputs that are either inconsistent with real-world facts or potentially misleading, posing challenges to the trustworthiness of artificial intelligence.In this context, we categorize these factual errors as factuality hallucinations.Depending on whether the generated factual content can be verified against a reliable source, they can be further divided into two primary types:</p>
<p> Factual Inconsistency refers to situations where the LLM's output contains facts that can be grounded in real-world information, but present contradictions.This type of hallucination occurs most frequently and arises from diverse sources, encompassing the LLM's capture, storage, and expression of factual knowledge.As shown in Table 1, when inquired about "the first person to land on the Moon", the model erroneously generated "Yuri Gagarin", which contradicts the real-world fact.</p>
<p> Factual Fabrication refers to instances where the LLM's output contains facts that are unverifiable against established real-world knowledge.As demonstrated in Table 1, while "the origins of unicorns" traditionally lack empirical grounding, the model fabricated a plausible historical origin for unicorns.</p>
<p>Faithfulness Hallucination.LLMs are inherently trained to align with user instructions.As the use of LLMs shifts towards more user-centric applications, ensuring their consistency with userprovided instructions and contextual information  becomes increasingly vital.Furthermore, LLM's faithfulness is also reflected in the logical consistency of its generated content.From this perspective, we categorize three subtypes of faithfulness hallucinations:</p>
<p> Instruction inconsistency refers to the LLM's outputs that deviate from a user's directive.While some deviations might serve safety guidelines, the inconsistencies here signify unintentional misalignment with nonmalicious user instructions.As described in Table 1, the user's actual intention is translation, However, the LLM erroneously deviated from the user's instruction and performed a question-answering task instead.</p>
<p> Context inconsistency points to instances where the LLM's output is unfaithful with the user's provided contextual information.For example, as shown in Table 1, the user mentioned the Nile's source being in the Great Lakes region of central Africa, yet the LLM's response contradicted the context.</p>
<p> Logical inconsistency underscores when LLM outputs exhibit internal logical contradictions, often observed in reasoning tasks.This manifests as inconsistency both among the reasoning steps themselves and between the steps and the final answer.For example, as shown in Table 1, while the reasoning step of dividing both sides of the equation by 2 is correct, the final answer of x=4 is inconsistent with the reasoning chain, leading to an incorrect result.</p>
<p>Hallucination Causes</p>
<p>Hallucinations have multifaceted origins, spanning the entire spectrum of LLMs' capability acquisition process.In this section, we delve into the root causes of hallucinations in LLMs, primarily categorized into three key aspects: Data ( 3.1), Training ( 3.2), and Inference ( 3.3).</p>
<p>Hallucination from Data</p>
<p>Pre-training data stands as the bedrock for LLMs, enabling them to gain general capabilities and factual knowledge (Zhou et al., 2023a).However, it can inadvertently become the source of LLM hallucinations.This mainly manifests in two aspects: potential risks stemming from flawed data sources ( 3.1.1),and the inferior utilization of factual knowledge captured in the data ( 3.1.2).</p>
<p>Flawed Data Source</p>
<p>While scaling up pre-training data substantially enhances the competencies of LLMs (Kaplan et al., 2020;Hoffmann et al., 2022), challenges arise in maintaining consistent data quality, which can potentially introduce misinformation and biases (Bender et al., 2021;Weidinger et al., 2021).Moreover, the absence of specific domain knowledge and upto-date facts in the data can lead the LLM to form knowledge boundaries, which pose limitations for LLMs in specific scenarios.Based on this, we primarily categorize the factors that could potentially lead to hallucinations into misinformation and biases and knowledge boundary limitations.For a more comprehensive understanding, illustrative examples of each type of data-induced hallucination are presented in Table 2. Misinformation and Biases.Given the increasing demand for large-scale corpora, heuristic data collection methods are employed to efficiently gather vast volumes of data.While providing extensive data, they can inadvertently introduce erroneous information, increasing the risk of imitative falsehoods.Additionally, social biases can inadvertently be introduced into the LLMs' learning process.These biases primarily include duplication bias and various social biases, potentially resulting in hallucinations.</p>
<p> Imitative Falsehoods.The primary objective of LLM pre-training is to mimic the training distribution.When LLMs are trained on factual incorrect data, they may inadvertently amplify these inaccuracies, potentially leading to factually incorrect hallucinations, termed as "imitative falsehoods" (Lin et al., 2022).For example, as shown in Table 2, the statement 'Thomas Edison invented the light bulb' is actually a misconception that has been widely misbelieved over time.LLMs trained on such factual incorrect data can lead to misleading outputs.</p>
<p> Duplication Bias.Neural networks, especially large language models, possess an intrinsic tendency to memorize training data (Carlini et al., 2021).Studies (Carlini et al., 2022;Chowdhery et al., 2023)   data sources into imitative falsehoods, duplication bias, and social biases.Each category is accompanied by a premise outlining the data issue, user input, and the LLM's hallucinatory output, and an explanation for the occurrence, aiding comprehension of these complex phenomena.model size.However, the inherent memorization capability becomes problematic in the context of duplicated information present within pre-training data (Lee et al., 2022a;Kandpal et al., 2023;Paullada et al., 2021).Such duplication can shift LLMs from generalization to memorization (Hernandez et al., 2022), ultimately giving rise to a duplication bias where LLMs over-prioritize the recall of duplicated data and lead to hallucinations that deviate from the desired content.In Table 2, when the user requests to "list some red fruits, excluding apples," the presence of statements like "red apples, watermelon, cherries, and strawberries" frequently repeat in the training dataset leads the model to produce the overmemorized statement in its output.</p>
<p> Social Biases.Certain biases are intrinsically tied to hallucinations, especially those related to gender (Paullada et al., 2021) and nationality (Narayanan Venkit et al., 2023;Ladhak et al., 2023).For instance, LLMs might associate the profession of nursing with females, even when gender isn't explicitly mentioned in the user-provided context, exemplifying context inconsistency hallucinations as discussed in Section ( 2.3).Such biases can be inadvertently acquired from internetbased texts, which are rife with diverse and biased viewpoints, and subsequently be propagated into the generated content (Ladhak et al., 2023).Besides such biases, discrepancies in data distribution also pose a potential cause for hallucinations.In the context of the natu-ral language inference (NLI) task, McKenna et al. (2023) found that LLMs tend to falsely label by bias toward hypotheses affirmed in training data.</p>
<p>Knowledge Boundary.While the vast pretraining corpora empower LLMs with extensive factual knowledge, they inherently possess boundaries.This limitation primarily surfaces in two aspects: the absence of up-to-date factual knowledge and specialized domain knowledge.An example is presented in Table 3.</p>
<p> Domain Knowledge Deficiency.LLMs have demonstrated remarkable performance across a wide range of downstream tasks in the generic domain.Nevertheless, given that these general-purpose LLMs are predominantly trained on extensive publicly available datasets (Penedo et al., 2023;Raffel et al., 2020;Gao et al., 2021), their expertise in specialized domains is inherently constrained by the absence of proprietary training data.As a result, when confronted with problems necessitating domain-specific knowledge, such as medical (Li et al., 2023g;Singhal et al., 2023) and legal (Yu et al., 2022;Katz et al., 2023) questions, these models may exhibit pronounced hallucinations, often manifesting as factual fabrication.</p>
<p> Outdated Factual Knowledge.Beyond the shortfall in domain-specific knowledge, another intrinsic limitation concerning the knowledge boundaries within LLMs is their constrained capacity for up-to-date knowledge.The factual knowledge embedded within LLMs exhibits clear temporal boundaries and can become outdated over time (Onoe et al., 2022;Kasai et al., 2022;Li et al., 2023a).Once these models are trained, their internal knowledge is never updated.This poses a challenge given the dynamic and everevolving nature of our world.When confronted with queries that transcend their temporal scope, LLMs often resort to fabricating facts or providing answers that might have been correct in the past but are now outdated.</p>
<p>Inferior Data Utilization</p>
<p>Pre-training data embodies a wealth of real-world factual knowledge, enabling LLMs to capture and subsequently encode vast of factual knowledge within their parameters (Petroni et al., 2019;Jiang et al., 2020;Roberts et al., 2020).However, despite this vast reservoir of knowledge, LLMs can still produce knowledge-induced hallucinations due to inferior utilization of parametric knowledge.In this context, we delve into two pivotal challenges: the spurious correlations in capturing factual knowledge and its struggles in knowledge recall.Examples for each type of hallucination related to inferior data utilization are presented in Table 4 for further illustration.Knowledge Shortcut.While significant efforts have been undertaken in exploring their knowledge storage (Geva et al., 2021;Meng et al., 2022)and probing (Petroni et al., 2019;Zhong et al., 2021;Yu et al., 2023c), the exact mechanism by which LLMs capture the factual knowledge remains elusive.Recent studies (Li et al., 2022a;Kang and Choi, 2023;Kandpal et al., 2023) indicate that rather than genuinely understanding the intricacies of factual knowledge, LLMs often resort to shortcuts.They display a tendency to overly depend on positional close (Li et al., 2022a), co-occurrence statistics (Kang and Choi, 2023), and relevant document count (Kandpal et al., 2023) within the pretraining data, which can introduce a bias towards spurious correlations, potentially leading to hallucinations if the bias reflects factually incorrect information.For instance, as illustrated in Table 4, when queried about "the capital of Canada", the model erroneously responds with "Toronto".This mistake might arise due to a higher co-occurrence frequency of Canada and Toronto in its training data, leading the model to incorrectly capture the factual knowledge about Canada's capital.</p>
<p>Knowledge Recall Failures.Hallucinations can arise when LLMs struggle to effectively leverage their extensive knowledge.We explore two primary challenges in knowledge recall: the inadequacy in recalling long-tail knowledge and difficulties in complex scenarios that require multi-hop reasoning and logical deduction.</p>
<p> Long-tail Knowledge Within the vast knowledge landscape that LLMs draw upon, a notable challenge emerges in the form of longtail knowledge utilization (Kandpal et al., 2023;Mallen et al., 2023).This long-tail knowledge, characterized by its relative rarity in pre-training data, poses inherent challenges for LLMs, which primarily rely on co-occurrence patterns to memorize factual  knowledge.Consequently, when confronted with queries pertaining to such long-tail knowledge, LLMs are at a heightened risk of hallucination, attempting to generate factually inaccurate responses.For instance, as shown in Table 4, when prompted to generate a biography for a long-tail entity previously encountered in Wikipedia training data, the LLM erroneously attributes the profession, mistakenly describing a politician as an educator.</p>
<p> Complex Scenario Beyond the challenges with long-tail knowledge, effective utilization of knowledge is inextricably linked with reasoning capabilities.For instance, in multihop question-answering scenarios, even if the LLM possesses the necessary knowledge, it may struggle to produce accurate results if multiple associations exist between questions, due to its limitations in reasoning (Zheng et al., 2023).Furthermore, Berglund et al. (2023) unveiled a specific reasoning failure in LLMs termed the Reversal Curse.Specifically, while the model can correctly answer when the question is formulated as "A is B", it exhibits a failed logical deduction when asked the converse "B is A".This discrepancy in reasoning extends beyond simple deductions.In retrieval-augmented settings, Liu et al. (2023e) highlighted a related challenge.</p>
<p>Despite having documents containing the correct answers within the model's context window, the model still struggles to generate precise responses due to its inadequacy in utilizing the provided evidence effectively.As illustrated in Table 4, although LLMs recognize Mount Everest as the world's highest peak, they fail to determine which would become the highest mountain if Everest's elevation were reduced by 500 meters, a task that requires complex reasoning ability.</p>
<p>Summary</p>
<p>Data-related hallucinations in LLMs are primarily rooted in flawed data sources and inferior data utilization.Misinformation and inherent biases within data sources not only propagate imitative falsehoods but also introduce biased outputs, leading to various forms of hallucinations.Knowledge boundaries in LLMs become evident when handling domain-specific queries or encountering rapidly updating factual knowledge.Regarding data utilization, LLMs tend to capture spurious correlations and demonstrate difficulties in recalling knowledge, especially long-tail information, and in complex reasoning scenarios, further exacerbating hallucinations.These challenges underscore the critical need for enhancing data quality and the models' capabilities to learn and recall factual knowledge more effectively.Facing complex multistep reasoning questions like this, LLM may struggle to recall all the relevant knowledge associated with it.</p>
<p>Table 4: Examples of Inferior Data Utilization, showcasing the pitfalls of knowledge shortcuts and failures in knowledge recall.This includes instances where LLMs capture factual knowledge relying on co-occurrence statistics, as well as situations where it cannot recall relevant information from its parametric knowledge.</p>
<p>Hallucination from Training</p>
<p>The training process of LLMs mainly encompasses two primary stages: 1) the pre-training stage, where LLMs learn general-purpose representations and capture world knowledge, and 2) the alignment stage, where LLMs are adapted to better align with user instructions and preferences.While this process equips LLMs with remarkable capabilities, any shortfalls in these stages can inadvertently lead to hallucinations.</p>
<p>Hallucination from Pre-training</p>
<p>Pre-training serves as the foundational stage for LLMs, typically employing a transformer-based architecture to conduct causal language modeling on vast corpora.However, issues related to hallucination may arise from the inherent architectural design and the particular training strategies employed.In this section, we delves into the challenges posed by the architecture flaw and impacts of exposure bias.</p>
<p>Architecture Flaw.LLMs typically adopt a transformer-based architecture following the paradigm established by GPT (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020), where they acquire representations through a causal language modeling objective, a framework exemplified by models such as OPT (Zhang et al., 2022), Falcon (Penedo et al., 2023), and Llama-2 (Touvron et al., 2023).Despite its success, it is not without its pitfalls, particularly concerning Inadequate Unidirectional Representation and Attention Glitches.</p>
<p> Inadequate Unidirectional Representation.</p>
<p>Following the causal language modeling paradigm, LLMs predict the subsequent token based solely on preceding tokens in a leftto-right manner.This unidirectional modeling, while facilitating efficient training, also has its limitations.It exclusively utilizes context from a single direction, which hinders its ability to capture intricate contextual dependencies, potentially increasing risks for the emergence of hallucination (Li et al., 2023h).</p>
<p> Attention Glitches.Transformer-based architecture, equipped with the self-attention module, has shown remarkable capabilities in capturing long-range dependencies.However, Recent research (Liu et al., 2023a) has shown that they can occasionally exhibit unpredictable reasoning errors in the context of algorithmic reasoning, spanning both longrange and short-range dependencies, regardless of model scale.A potential cause is the limitations of soft attention (Hahn, 2020;Chiang and Cholak, 2022), where attention becomes diluted across positions as sequence length increases.</p>
<p>Exposure Bias.Beyond the architecture flaw, training strategies also play a crucial role.Notably, the phenomenon of exposure bias (Bengio et al., 2015;Ranzato et al., 2016) stands out, resulting from the disparity between training and inference in the auto-regressive generative model.During training, these models typically employ a teacher-forced maximum likelihood estimation (MLE) training strategy where ground truth tokens are provided as input.However, during inference, the model relies on its own generated tokens for subsequent predictions.Such inconsistency can result in hallucinations (Wang and Sennrich, 2020), especially when an erroneous token generated by the model cascades errors throughout the subsequent sequence, akin to a snowball effect (Zhang et al., 2023c).</p>
<p>Hallucination from Alignment</p>
<p>Alignment, which typically involves two main processes, supervised fine-tuning and reinforcement learning from human feedback, serves as a crucial step toward unlocking the capabilities of LLMs and aligning them with human preferences.While alignment notably enhances the quality of LLM responses, it also introduces the risk of hallucinations.In this section, we will categorize the alignment shortfalls related to hallucinations into two parts: Capability Misalignment and Belief Misalignment.</p>
<p>Capability Misalignment.Considering that LLMs have inherent capability boundaries established during pre-training, SFT utilizes highquality instructions along with their corresponding responses to empower LLMs to follow user instructions, unlocking their acquired abilities in this process.However, as the capabilities of LLMs expand, a significant challenge emerges: the potential misalignment between the LLMs' intrinsic capabilities and those depicted in the annotation data.When the demands from alignment data exceed these predefined capability boundaries, LLMs are trained to produce content beyond their own knowledge boundaries, amplifying the risk of hallucinations (Schulman, 2023).</p>
<p>Belief Misalignment.Several studies have demonstrated that LLM's activations encapsulate an internal belief related to the truthfulness of its generated statements (Burns et al., 2022;Azaria and Mitchell, 2023).Nevertheless, misalignment can occasionally arise between these internal beliefs and the generated outputs.Even when LLMs are refined with human feedback (Ouyang et al., 2022), they can sometimes produce outputs that diverge from their internal beliefs.Such behaviors, termed as sycophancy (Cotra, 2021), underscores the model's inclination to appease human evaluators, often at the cost of truthfulness.Recent studies indicate that models trained via RLHF exhibit pronounced behaviors of pandering to user opinions.Such sycophantic behaviors are not restricted to ambiguous questions without definitive answers (Perez et al., 2023), like political stances, but can also arise when the model chooses a clearly incorrect answer, despite being aware of its inaccuracy (Wei et al., 2023).Delving into this phenomenon, (Sharma et al., 2023) suggests that the root of sycophancy may lie in the training process of RLHF models.By further exploring the role of human preferences in this behavior, the research indicates that the tendency for sycophancy is likely driven by both humans and preference models showing a bias towards sycophantic responses over truthful ones.</p>
<p>Summary</p>
<p>In training LLMs, both the foundational pretraining and the subsequent alignment present unique challenges that can induce hallucinations.During the pre-training stages, architecture flaws, notably inadequate unidirectional representation, and attention glitches, coupled with the well-known exposure bias, contribute to hallucinations.Meanwhile, in the alignment phase, issues of capability misalignment and belief misalignment arise.The former risks pushing LLMs beyond their knowl-edge boundaries, while the latter reveals a disparity between the LLM's beliefs and its outputs.These challenges underscore the importance of training LLMs to ensure their truthfulness.From foundational model designs and training strategies to align with human expectations, it remains a multifaceted endeavor.</p>
<p>Hallucination from Inferece</p>
<p>Decoding plays an important role in manifesting the capabilities of LLMs after pre-training and alignment.However, certain shortcomings within decoding strategies can lead to LLM hallucinations.In this section, we delve into potential causes rooted in the decoding process, emphasizing two critical factors: the inherent randomness of decoding strategies ( 3.3.1) and imperfect decoding representation ( 3.3.2).</p>
<p>Inherent Sampling Randomness</p>
<p>LLMs have demonstrated a remarkable aptitude for generating highly creative and diverse content, a proficiency that is critically dependent on the pivotal role of randomness in their decoding strategies.Stochastic sampling (Fan et al., 2018;Holtzman et al., 2020) is currently the prevailing decoding strategy employed by these LLMs.The rationale for incorporating randomness into decoding strategies stems from the realization that high likelihood sequences often result in surprisingly low-quality text, which is called likelihood trap (Stahlberg and Byrne, 2019;Holtzman et al., 2020;Meister et al., 2020;Zhang et al., 2021).The diversity introduced by the randomness in decoding strategies comes at a cost, as it is positively correlated with an increased risk of hallucinations (Dziri et al., 2021a;Chuang et al., 2023).An elevation in the sampling temperature results in a more uniform token probability distribution, increasing the likelihood of sampling tokens with lower frequencies from the tail of the distribution.Consequently, this heightened tendency to sample infrequently occurring tokens exacerbates the risk of hallucinations (Aksitov et al., 2023).</p>
<p>Imperfect Decoding Representation</p>
<p>During the decoding phase, LLMs use their toplayer representation to predict the next token.However, the top-layer representation has its limitations, primarily manifested in two aspects: Insufficient Context Attention and Softmax Bottleneck.</p>
<p>Insufficient Context Attention.Prior studies, particularly in domains like machine translation (Miao et al., 2021) and summarization (Chen et al., 2022b), have highlighted the issue of over-confidence in generative models employing encoder-decoder architectures.Such overconfidence stems from an excessive focus on the partially generated content, often prioritizing fluency at the expense of faithfully adhering to the source context.While large language models, primarily adopting the causal language model architecture, have gained widespread usage, the overconfidence phenomenon continues to persist.During the generation process, the prediction of the next word is conditioned on both the language model context and the partially generated text.However, as demonstrated in prior studies (Voita et al., 2019;Beltagy et al., 2020;Liu et al., 2023e), language models often exhibit a localized focus within their attention mechanisms, giving priority to nearby words and resulting in a notable deficit in context attention (Shi et al., 2023b).Furthermore, this concern is further amplified in LLMs that exhibit a proclivity for generating lengthy and comprehensive responses.In such cases, there is even a heightened susceptibility to the risk of instruction forgetting (Chen et al., 2023f;Liu et al., 2023i).This insufficient attention can directly contribute to faithfulness hallucinations, wherein the model outputs content that deviates from the original context.</p>
<p>Softmax Bottleneck.The majority of language models utilize a softmax layer that operates on the final layer's representation within the language model, in conjunction with a word embedding, to compute the ultimate probability associated with word prediction.Nevertheless, the efficacy of Softmax-based language models is impeded by a recognized limitation known as the Softmax bottleneck (Yang et al., 2018a), wherein the employment of softmax in tandem with distributed word embeddings is constrained the expressivity of the output probability distributions given the context which prevents LMs from outputting the desired distribution.Additionally, Chang and McCallum (2022) discovered that when the desired distribution within the output word embedding space exhibits multiple modes, language models face challenges in accurately prioritizing words from all the modes as the top next words, which also introduces the risk of hallucination.</p>
<p>Summary</p>
<p>During the decoding phase, challenges arise from both the inherent decoding strategy and the representation utilized for predicting.The former, emphasizing the randomness rooted in its decoding algorithm, can be a source of hallucinations as the randomness increases.While on the representation side, issues such as the over-reliance on nearby content and the softmax bottleneck can limit the model's capability to express diverse output probabilities, leading to the risk of inaccurate token predictions.These complexities underscore the necessity of maintaining factuality and faithfulness throughout the decoding process.</p>
<p>Hallucination Detection and Benchmarks</p>
<p>Hallucinations, as exhibited by LLMs, have garnered substantial attention due to their implications on model reliability and real-world deployment.As models become increasingly adept at generating human-like text, distinguishing between accurate and hallucinated content becomes a pivotal concern.Two primary facets encompass the broad spectrum of hallucination mitigation: detection mechanisms and evaluation benchmarks.This section serves as a deep dive into the state-of-the-art techniques for detecting hallucinations ( 4.1) and the benchmarks ( 4.2)that evaluate their prowess.</p>
<p>Hallucination Detection</p>
<p>Detecting hallucinations in LLMs is imperative for assuring the reliability and trustworthiness of the generated content.Traditional metrics, predominantly hinged on word overlap, fall short in differentiating the nuanced discrepancies between plausible and hallucination content.Such a challenge highlights the necessity for more sophisticated detection methods tailored to LLM hallucinations.Given the diverse nature of these hallucinations, detection approaches vary accordingly.Consequently, in this section, we provide a comprehensive overview of primary hallucination detection strategies, tailored to factuality and faithfulness hallucinations.</p>
<p>Factuality Hallucination Detection</p>
<p>Research by (Chen and Shu, 2023) underscored the challenge humans face in identifying ChatGPTgenerated misinformation, leading to increasing studies aiming to design detection methods target-ing factuality hallucination.In this context, we propose an overview of established methods, typically categorized into Retrieve External Facts and Uncertainty Estimation.</p>
<p>Retrieve External Facts.To effectively pinpoint factual inaccuracies in LLM outputs, one intuitive strategy involves comparing the modelgenerated content against reliable knowledge sources, as shown in Fig. 3.This methodology closely aligns with the workflow of fact-checking tasks, as delineated by (Guo et al., 2022).Nevertheless, traditional fact-checking methodologies (Augenstein et al., 2019;Hanselowski et al., 2019;Atanasova et al., 2020) often incorporate simplified assumptions for practicality, leading to discrepancies when applied to complex real-world scenarios.Recognizing these constraints, Chen et al. (2023c) place greater emphasis on real-world scenarios, wherein evidence is procured from timeconstrained, uncurated web sources.They have pioneered a fully automated pipeline that integrates multiple components: claim decomposition, raw document retrieval, fine-grained retrieval, claimfocused summarization, and veracity classification.Galitsky (2023) further addresses situations where potential conflict retrieval evidence by finding the least defeated authoritative source and avoiding the most defeated.Furthermore, Min et al. (2023) introduced FACTSCORE, a fine-grained factual metric specifically for long-form text generation.It decomposes the generation content into atomic facts and subsequently computes the percentage supported by reliable knowledge sources.Recently, Huo et al. (2023) enhanced the standard approach of retrieving supporting evidence for hallucination detection through query expansion.By combining the original question with the LLM-generated answer during the retrieval process, they addressed topic drift concerns, ensuring that the retrieved passages align with both the question and the LLM's response.In a broader perspective, Chern et al. (2023) proposed a unified framework that enables LLMs to detect factual errors by leveraging a suite of external tools for evidence collection.</p>
<p>Uncertainty Estimation.While many approaches to hallucination detection rely on external knowledge sources for fact-checking, several methods have been devised to address this issue in zero-resource settings, thus eliminating the need for retrieval.The foundational premise behind these strategies is that the origin of LLM hallucinations  is inherently tied to the model's uncertainty.Therefore, by estimating the uncertainty of the factual content generated by the model, it becomes feasible to detect hallucinations.The methodologies in uncertainty estimation can broadly be categorized into two approaches: based on internal states and LLM behavior, as shown in Fig. 4. The former operates under the assumption that one can access the model's internal states, while the latter generalizes to more constrained environments, leveraging solely the model's observable behaviors to infer its underlying uncertainty.</p>
<p> LLM Internal States.The internal states of LLMs can serve as informative indicators of their uncertainty, often manifested through metrics like token probability or entropy.Varshney et al. (2023) determine the model's uncertainty towards key concepts quantified by considering the minimal token probability within those concepts.The underlying rationale is that a low probability serves as a strong indicator of the model's uncertainty, with less influence from higher probability tokens present in the concept.Similarly, Luo et al. (2023a) employed a self-evaluationbased approach for uncertainty estimation by grounding in the rationale that a language model's ability to adeptly reconstruct an original concept from its generated explanation is indicative of its proficiency with that concept.By initially prompting the model to generate an explanation for a given concept and then employing constrained decoding to have the model recreate the original concept based on its generated explanation, the probability</p>
<p>Faithfulness Hallucination Detection</p>
<p>Ensuring the faithfulness of LLMs to provide context or user directives is pivotal for their practical utility in a myriad of applications, from summariza-</p>
<p>(1) Self-Consistency</p>
<p>Mount Everest stands as the tallest peak in the world.</p>
<p>As far as I know, the highest peak in the world is Mount Fuji in Japan.</p>
<p>The highest peak is Mount Everest located in the Himalayas.</p>
<p>Consistency</p>
<p>The highest peak in the world is Mount Fuji.</p>
<p>I stand corrected, you are right.</p>
<p>I must correct you.Mount Fuji is the highest peak in Japan.The highest peak in the world is Mount Everest in the Himalayas range.</p>
<p>(  tion to interactive dialogue systems.Faithfulness hallucination detection primarily focuses on ensuring the alignment of the generated content with the given context, sidestepping the potential pitfalls of extraneous or contradictory output.In this subsection, we explore the methods to detect unfaithfulness in LLM generations and provide an overview of in Fig. 5.</p>
<p>Fact-based Metrics.In the realm of assessing faithfulness, one of the most intuitive methods involves measuring the overlap of pivotal facts between the generated content and the source content.Given the diverse manifestations of facts, metrics can be categorized based on entities, relation triples, and knowledge.</p>
<p> N-gram based.When treating the source content as the reference, traditional n-gram overlap-based evaluation metrics, such as ROUGE (Lin, 2004) and PARENT-T (Wang et al., 2020b), can also be applied to assess faithfulness.However, due to the natural diversity of language expression and their reliance on surface-level matching, these metrics show poor correlation with humans (Maynez et al., 2020).</p>
<p> Entity-based.Metrics based on entity overlap are prevalently applied in summarization tasks, as any omission or inaccurate generation of these key entities could lead to an unfaithful summary.Nan et al. (2021) introduced a metric to quantify the extent of entity hallucination, which calculates the precision of named-entities in the summary against the source entities.</p>
<p> Relation-based.Noting that even if entities match, the relations between them might be erroneous.Thus, Goodrich et al. (2019) focus on the overlap of relation tuples and introduce a metric that computes the overlap of relation tuples extracted using trained end-to-end fact extraction models.</p>
<p> Knowledge-based.Similarly, for knowledgegrounded dialogue tasks, facts often correspond to the knowledge presented in the dialogue.Shuster et al. (2021) introduced the Knowledge F1 metric to assess how well the model's generation aligns with the supplied knowledge.</p>
<p>Classifier-based Metrics.Beyond computing fact overlap, another straightforward approach to assessing the faithfulness of the model involves utilizing classifiers trained on comprising both taskspecific hallucinated and faithful content, as well as data from related tasks or synthetically generated data.It can be broadly categorized into the following types:</p>
<p> Entailment-based.A prevailing concept in using Natural Language Inference (NLI) for assessing the faithfulness of generated text is anchored on the idea that genuinely faithful content should inherently be entailed by its source content.In line with this, numerous studies (Falke et al., 2019;Maynez et al., 2020) have trained classifiers on NLI datasets to identify factual inaccuracies, especially in the context of abstract summarization.However, Mishra et al. (2021) highlighted that the mismatch in input granularity between conventional NLI datasets and inconsistency detection datasets limits their applicability for effectively detecting inconsistencies.Building on this, more advanced studies have proposed  methods such as fine-tuning on adversarial datasets (Barrantes et al., 2020), decomposing the entailment decisions at the dependency arc level (Goyal and Durrett, 2020), and segmenting documents into sentence units then aggregating scores between sentence pairs (Laban et al., 2022).These collective efforts underscore the potential to enhance the accuracy of hallucination detection.</p>
<p> Weekly Supervised.While using data from related tasks to fine-tune the classifier has shown promise in evaluating faithfulness, it's essential to recognize the inherent gap between related tasks and the downstream task.The scarcity of annotated data further constrains their applicability.In response to this challenge, Kryscinski et al. (2020) analyzed errors made by cutting-edge summarization models and introduced a method using rule-based transformations to create weaklysupervised data for fine-tuning the classifier.Concurrently, Zhou et al. (2021) devised an approach to automatically generate tokenlevel hallucination data and perform token- Question-Answering based Metrics.In contrast to classifier-based metrics, QA-based metrics have recently garnered attention for their enhanced ability to capture information overlap between the model's generation and its source.These metrics operate by initially selecting target answers from the information units within the LLM's output, and then questions are generated by the questiongeneration module.The questions are subsequently used to generate source answers based on the user context.Finally, the faithfulness of the LLM's responses is calculated by comparing the matching scores between the source and target answers.Notable implementations include (Durmus et al., 2020;Wang et al., 2020a;Scialom et al., 2021;Honovich et al., 2021).Although these method-ologies share a common thematic approach, they exhibit variability in aspects like answer selection, question generation, and answer overlap, leading to diverse performance outcomes.Building on this foundational work, Fabbri et al. (2022) conducted an in-depth evaluation of the components within QA-based metrics, yielding further enhancements in faithfulness evaluation.</p>
<p>Uncertainty Estimation.Drawing from the insights in Section ( 4.1.1),hallucinations in conditional text generation are closely tied to high model uncertainty.Uncertainty estimation has been widely explored in Bayesian deep learning (Blundell et al., 2015;Gal and Ghahramani, 2016;Lakshminarayanan et al., 2017).From a Bayesian perspective, the total uncertainty of a prediction is characterized by the predictive entropy of the output distribution.Moreover, some works (Malinin and Gales, 2021) have sought to quantify model uncertainty using log probability.Based on these principles, we categorize the existing approaches for hallucination detection via uncertainty estimation into the following types:</p>
<p> Entropy based.Xiao and Wang (2021)  Prompting-based Metrics.Recently, the remarkable instruction-following ability of LLMs has underscored their potential for automatic evaluation (Chiang and Lee, 2023;Liu et al., 2023g;Wang et al., 2023d).Exploiting this capability, researchers have ventured into novel paradigms for assessing the faithfulness of model-generated content (Luo et al., 2023b;Laban et al., 2023;Adlakha et al., 2023;Gao et al., 2023b;Jain et al., 2023).By providing LLMs with concrete evaluation guidelines and feeding them both the model-generated and source content, they can effectively assess faithfulness.The final evaluation output can either be a binary judgment on faithfulness (Luo et al., 2023b) or a k-point Likert scale indicating the degree of faithfulness (Gao et al., 2023b).For prompt selection, evaluation prompt can either be direct prompting, chain-of-thought prompting (Adlakha et al., 2023), using in-context-learning (Jain et al., 2023) or allowing the model to generate evaluation results accompanying with explanations (Laban et al., 2023).</p>
<p>Benchmarks</p>
<p>In this section, we present a comprehensive overview of existing hallucination benchmarks, which can be categorized into two primary domains: Hallucination Evaluation Benchmarks ( 4.2.1), which assess the extent of hallucinations generated by existing cutting-edge LLMs, and Hallucination Detection Benchmarks ( 4.2.2), designed specifically to evaluate the performance of existing hallucination detection methods.Collectively, these benchmarks establish a unified framework, enabling a nuanced and thorough exploration of hallucinatory patterns in LLMs.</p>
<p>Hallucination Evaluation Benchmarks</p>
<p>Hallucination evaluation benchmarks are devised to assess LLMs' proclivity to produce hallucinations, with a particular emphasis on identifying factual inaccuracies and measuring deviations from original contexts.Presently, the primary focus of these benchmarks is on evaluating the factuality of LLM-generated content.While most are structured in a question-answering format, their primary focus remains on LLM factuality.Their unique characteristics stem from the selected knowledge domain, language, and response format they employ.We present an overview of the most representative benchmarks in detail below and concurrently provide an evaluation of common LLMs' perfor-  mances on these benchmarks in the ( A).</p>
<p>TruthfulQA (Lin et al., 2022).Comprising 817 questions that span 38 diverse categories, such as health, law, finance, and politics, TruthfulQA is a benchmark specifically designed to assess the truthfulness of language models.Crafted using an adversarial methodology, it aims to elicit "imitative falsehoods"-misleading responses that models might generate due to their frequent presence in training data.The benchmark is divided into two parts, one of which contains manually curated questions that were further refined by filtering out those correctly answered by GPT-3, resulting in 437 filtered questions.The other part includes 380 unfiltered nonadversarial questions.For evaluation, TruthfulQA offers two types of question-answering tasks: generation and multiple-choice, with human evaluation employed to gauge the models' truthfulness and informativeness.Moreover, the benchmark introduces an automatic metric named GPT-judge, which is fine-tuned on a 6.7B GPT-3 model.</p>
<p>REALTIMEQA (Kasai et al., 2022).Considering that world knowledge is constantly evolving, it becomes crucial to validate the LLM's factuality concerning the current world.This benchmark offers real-time open-domain multiple-choice questions derived from newly-published news articles, spanning diverse topics such as politics, business, sports, and entertainment.Additionally, the benchmark provides a platform for real-time evaluations, either through a multiple-choice format assessed by accuracy or a generation setting evaluated using exact matching and token-based F1 metrics.</p>
<p>Med-HALT (Umapathi et al., 2023).</p>
<p>Given the critical consequences of hallucinations in the medical domain on patient care, the benchmark em-phasizes challenges specific to LLMs in the medical domain.Med-HALT, incorporating multiplechoice questions from various countries, is tailored to assess LLMs' reasoning and memorization in the medical context.The reasoning task, with 18,866 samples, tests LLMs' ability to distinguish incorrect or irrelevant options and fake questions by using multiple-choice medical questions.Meanwhile, the memory task, comprising 4,916 samples, evaluates LLMs' ability to recall and generate accurate factual information by either generating links from a PubMed abstract/title or producing titles from given links and PMIDs.For evaluation, the performance of LLMs is measured either by their accuracy on test questions or by a Pointwise Score that considers both the positive scores for correct answers and a negative penalty for incorrect ones.</p>
<p>FACTOR (Muhlgay et al., 2023).To quantitatively assess LM factuality, Muhlgay et al. (2023) introduced a method for automatically creating benchmarks by perturbing factual statements from a designated corpus.resulting in two benchmarks: Wiki-FACTOR and News-FACTOR.Specifically, for a given prefix text, the original completion from the corpus serves as the factually correct answer.In-structGPT is then guided with prompts that contain specific error types to generate non-factual completions.These generated responses are subsequently filtered for fluency and self-consistency, serving as the foundation for multi-choice tasks.For evaluation, an LM's factuality is gauged by whether the likelihood of the model producing the factually correct completion exceeds that of generating other non-factual completions.</p>
<p>ChineseFactEval (Wang et al., 2023a).By gathering questions from diverse domains such as general knowledge, scientific research, medicine, law, finance, mathematics, and modern Chinese history, ChineseFactEval employed 125 questions to evaluate the factual capabilities of six contemporary Chinese LLMs, alongside GPT-4.For evaluation, questions are categorized based on the accuracy achieved by various LLMs, with different scores assigned to questions of varying difficulty.The responses from all LLMs are primarily annotated by humans, supplemented by FacTool (Chern et al., 2023).The final scores of the LLMs are then used to assess their factuality.</p>
<p>HalluQA (Cheng et al., 2023).Drawing from the construction approach of TruthfulQA (Lin et al., 2022), HalluQA is crafted to specifically assess hallucinations in Chinese large language models, focusing on imitative falsehoods and factual errors.The benchmark comprises 450 handcrafted adversarial questions across 30 domains and is categorized into two parts.The misleading section captures questions that successfully deceive GLM-130B, while the knowledge section retains questions that both ChatGPT and Puyu consistently answer incorrectly.For evaluation, LLMs generate responses to these questions, which are then compared with correct answers using GPT-4 to determine whether an answer contains hallucinations.</p>
<p>FreshQA (Vu et al., 2023).Recognizing that hallucinations can partially arise from outdated knowledge within LLMs, the benchmark is introduced to evaluate the factuality of existing LLMs.Comprising 600 hand-crafted questions whose answers may change over time or whose premises are factually incorrect, this benchmark primarily evaluates the LLMs' aptitude for fast-changing knowledge and their ability to identify questions with false premises.For evaluation, the benchmark provides a two-mode evaluation procedure: RE-LAXED, which solely evaluates the correctness of the primary answer, and STRICT, which further assesses the accuracy of every fact within the answer.In both modes, the factuality of the LLM is reflected by the accuracy of its responses, as determined through human annotations.</p>
<p>Hallucination Detection Benchmarks</p>
<p>For hallucination detection benchmarks, most prior studies have primarily concentrated on taskspecific hallucinations, such as abstractive summarization (Kryscinski et al., 2020;Wang et al., 2020a;Maynez et al., 2020;Fabbri et al., 2021;Goyal and Durrett, 2021;Pagnoni et al., 2021;Tang et al., 2022), data-to-text (Tian et al., 2019;Parikh et al., 2020), and machine translation (Zhou et al., 2021).However, the content generated in these studies often originates from models with lesser capabilities, such as BART (Lewis et al., 2020a) and PEGASUS (Zhang et al., 2020).As a result, they may not accurately reflect the effectiveness of hallucination detection strategies.Therefore, such studies fall outside the scope of our current discussion.</p>
<p>SelfCheckGPT-Wikibio (Miao et al., 2023).Miao et al. (2023) introduced a sentence-level hallucination detection dataset by generating synthetic Wikipedia articles using GPT-3, based on concepts from the WikiBio dataset.The factuality of these passages was then manually annotated at the sen-tence level, yielding a total of 1908 sentences for 238 articles.</p>
<p>HaluEval (Li et al., 2023c).To assess the capability of LLMs in recognizing hallucination, HaluEval was constructed using a combination of automated generation and human annotation, yielding 5,000 general user queries paired with ChatGPT responses and 30,000 task-specific samples.The automated generation employed a "sampling-thenfiltering" approach.Drawing upon task-specific datasets from question answering, knowledgegrounded dialogue, and text summarization, the benchmark initially uses ChatGPT to sample multifaceted hallucinated answers based on task-related hallucination patterns and then select the most plausible hallucinated samples by ChatGPT.For human annotation, Alpaca-sourced queries were processed by ChatGPT to sample multiple responses, which were then manually assessed for the presence of hallucinated content.</p>
<p>BAMBOO (Dong et al., 2023).Expanding upon the methodologies introduced by Li et al. (2023c), this benchmark introduces two new datasets, Sen-Hallu and AbsHallu, aimed at detecting hallucination in the context of long texts.These datasets are constructed by inducing ChatGPT to generate hallucinations given academic papers, resulting in 200 samples, respectively.</p>
<p>FELM (Chen et al., 2023d).Unlike previous studies that predominantly focused on specific tasks such as summarization (Fabbri et al., 2021;Tang et al., 2022)or particular domains such as world knowledge (Miao et al., 2023), this benchmark assesses factuality across five domains: world knowledge, science and technology, mathematics, writing and recommendation, and reasoning.While earlier research intentionally induced LLMs to hallucinate based on specific patterns (Li et al., 2023c), this benchmark employs ChatGPT to generate responses in a zero-shot setting, yielding a total of 817 samples (comprising 3948 segments).Each segment is annotated for factuality, error reasons, error type, and external references.Serving as a testbed for factuality detectors, the benchmark employs the F1 score and balanced classification accuracy to evaluate factual errors at both the segment and response levels.</p>
<p>PHD (Yang et al., 2023).Rather than focusing on sentence-level hallucination detection, the benchmark emphasizes passage-level detection.The construction of the benchmark begins with the extraction of entities from the Wikipedia dump, followed by generating passages using ChatGPT.Recognizing that factuality errors often arise when LLMs lack sufficient knowledge, the benchmark selects entities based on the number of related items returned by Google Search.This categorization results in three distinct groups: PHD-Low, PHD-Medium, and PHD-High.From each category, 100 entities are sampled and then human-annotated at the passage level as factual, non-factual, or unverifiable.For the evaluation process, the benchmark employs Precision, Recall, and F1 measures to assess the effectiveness of methods in detecting non-factual passages.</p>
<p>ScreenEval (Lattimer et al., 2023).Building upon existing research predominantly focused on short documents, the ScreenEval benchmark extends the scope to factual inconsistencies in longform dialogues.Based on the SummScreen dataset (Chen et al., 2022a), which comprises TV scripts and human-crafted summaries, this benchmark introduces factual inconsistency annotations for summaries generated by Longformer and GPT-4 at sentence level, resulting in a dataset of 52 documents and 624 summary sentences.As for evaluation, hallucination detection methods are evaluated on this benchmark using the AUROC score.</p>
<p>RealHall (Friel and Sanyal, 2023).The construction of this benchmark follows the principles that tasks within a hallucination detection benchmark ought to present a substantive challenge to LLMs and bear relevance to real-world applications while ensuring a breadth of diversity.In alignment with this, the benchmark concentrates on questionanswering tasks, categorizing them into Closed and Open groups based on the availability of a reference text in the prompt.Each question within the benchmark is initially approached using ChatGPT for generating responses, which are subsequently assigned boolean ground-truth labels through a combined approach involving human annotation, GPT-4 evaluation, and automated rule-based assessment.The efficacy of hallucination detection methodologies applied to this benchmark is quantified using the AUROC score.</p>
<p>LSum (Feng et al., 2023a).The benchmark centers on factual consistency detection within the summarization tasks undertaken by LLMs.Built on XSum (Narayan et al., 2018), the benchmark involves generating summaries using various LLMs, from the GPTfamily, GLM-family, and LLaMA-family and annotating the factual consistency on the sentence level by employing ChatGPT and GPT-4, resulting in a total of 6,166 annotated summaries.</p>
<p>SAC 3 (Zhang et al., 2023a).The benchmark comprises two datasets: HotpotQA-halu and NQopen-halu.These datasets were constructed by sampling 250 examples from the training set of HotpotQA (Yang et al., 2018b) and NQ-open (Kwiatkowski et al., 2019), respectively.Hallucinated answers were then generated using gpt-3.5turbo.Then, the answers were manually annotated, which involved comparing them with the ground truth and relevant knowledge sources.</p>
<p>Hallucination Mitigating</p>
<p>In this section, we present a comprehensive review of contemporary methods aimed at mitigating hallucinations in LLMs.Drawing from insights discussed in Hallucination Causes( 3), we systematically categorize these methods based on the underlying causes of hallucinations.Specifically, we focus on approaches addressing Datarelated Hallucinations( 5.1), Training-related Hallucinations( 5.2) and Inference-related Hallucinations( 5.3), each offering tailored solutions to tackle specific challenges inherent to their respective cause.</p>
<p>Mitigating Data-related Hallucinations</p>
<p>Data-related hallucinations generally emerge as a byproduct of biases, misinformation, and knowledge gaps, which are fundamentally rooted in the training data.In this context, we explore various strategies for mitigating such hallucinations, aiming to minimize the occurrence of misinformation and biases, while also providing knowledge augmentation and enhancing the effective utilization of knowledge by LLMs.</p>
<p>Mitigating Misinformation and Biases</p>
<p>To reduce the presence of misinformation and biases, the most intuitive approach is to collect highquality factual data to prevent the introduction of misinformation and conduct data cleansing to debias.</p>
<p>Factuality Data Enhancement.Maintaining the factual correctness of the training data is crucial in mitigating issues like imitative falsehood (Lin et al., 2022).The most direct approach is the manual curation of the pre-training dataset.As early as the advent of GPT-2, Radford et al. (2019) underscored the significance of exclusively scraping web pages that had undergone rigorous curation and filtration by human experts.However, as pretraining datasets continue to scale, manual curation becomes a challenge.Given that academic or specialized domain data is typically factually accurate, gathering high-quality data emerges as a primary strategy.Notable examples include the Pile (Gao et al., 2021) and "textbook-like" data sources (Gunasekar et al., 2023;Li et al., 2023f).Additionally, up-sampling factual data during the pre-training phase has been proven effective in enhancing the factual correctness of LLMs (Touvron et al., 2023), thus alleviating hallucination.</p>
<p>Debias.Biases within pre-training data can typically be classified into two main categories: duplication bias and societal biases, each requiring distinct debiasing approaches.</p>
<p> Duplication Bias.Deduplication serves as a crucial procedure in the pre-training phase.</p>
<p>Existing practices typically fall into two categories: exact duplicates and near-duplicates.</p>
<p>For exact duplicates, the most straightforward method involves exact substring matching to identify identical strings.However, given the vastness of pre-training data, this process can be computationally intensive.In addition, a more efficient method utilizes the construction of a suffix array (Manber and Myers, 1993), enabling effective computation of numerous substring queries in linear time.Regarding near-duplicates, the identification often involves approximate full-text matching, typically utilizing hash-based techniques to identify document pairs with significant ngram overlap.Furthermore, MinHash (Broder, 1997) stands out as a prevalent algorithm for large-scale deduplication tasks (Gyawali et al., 2020).Additionally, SemDeDup (Abbas et al., 2023) makes use of embeddings from pretrained models to identify semantic duplicates, which refers to data pairs with semantic similarities but not identical.</p>
<p> Societal Biases.Given the vastness and unfathomable nature of pre-training data, directly addressing the root cause of societal biases is a formidable challenge (Ferrara, 2023).Consequently, current mainstream solutions lean heavily on curated training corpora.By carefully selecting diverse, balanced, and rep-resentative training data, we can mitigate biases (Paullada et al., 2021;Narayanan Venkit et al., 2023;Ladhak et al., 2023) that may trigger hallucinations.Additionally, toolkits (Viswanath and Zhang, 2023) have been introduced to enable users to debiasing both existing and custom models.</p>
<p>Mitigating Knowledge Boundary</p>
<p>Constrained by the coverage and temporal boundaries of training data, inevitably form knowledge boundaries, introducing notable challenges.To tackle these challenges, two popular approaches have gained significant attention.One is Knowledge editing (Sinitsin et al., 2020;Yao et al., 2023c), which aims at directly editing model parameters to bridge the knowledge gap.The other leveraging non-parametric knowledge sources through Retrieval-Augmented Generation (RAG) (Lewis et al., 2020b;Guu et al., 2020;Shuster et al., 2021).Knowledge Editing.Knowledge editing De Cao et al. ( 2021); Sinitsin et al. (2020) has garnered rising attention from researchers, which aims to rectify model behavior by incorporating additional knowledge.Current knowledge editing techniques can fix factual errors and refresh outdated information to mitigate the knowledge gap, which can be categorized into two classes: changing the model's behavior by modifying the model parameters or using an external model plug-in with the original model frozen (Yao et al., 2023c).</p>
<p> Modifying Model Parameters.These techniques directly inject knowledge into the original model, leading to a substantial alteration in the model's output, which can be further split into locate-then-edit methods and metalearning methods.</p>
<p>Locate-then-edit methods (Dai et al., 2022a;Meng et al., 2022) consist of two stages, which first locate the "buggy" part of the model parameters and then apply an update to them to alter the model's behavior.For example, ROME (Meng et al., 2022) locates the edits-related layer by destroying and subsequently restoring the activations and then updates the parameters of FFN in a directed manner to edit knowledge.MEMIT (Meng et al., 2023) employs the same knowledge locating methods as ROME, enabling the concurrent updating of multiple layers to facilitate the si-multaneous integration of thousands of editing knowledge.However, Yao et al. (2023c) finds that these methods lack non-trivial generalization capabilities and varying performance and applicability to different model architectures.</p>
<p>The best-performing methods ROME (Meng et al., 2022) and MEMIT (Meng et al., 2023) empirically only work well on decoder-only LLMs.</p>
<p>Meta-learning methods (De Cao et al., 2021;Mitchell et al., 2022a) train an external hypernetwork to predict the weight update of the original model.Nevertheless, meta-learning methods often require additional training and memory cost, necessitating specialized design to reduce the size of hyper-networks in the age of LLMs (e.g.low-rank decomposition (Mitchell et al., 2022a)).While these methods can fine-grainedly adjust the behavior of the model, modifications to the parameters could have a potentially harmful impact on the inherent knowledge of the model.</p>
<p> Preserving Model Parameters.Instead of directly modifying model parameters, a line of studies apply an additional model plug-in into the original model to achieve the desired change in model behavior.SERAC (Mitchell et al., 2022b) employs a scope classifier to route the input associated with new knowledge stored in an external edit memory toward the counterfactual model, which can aid the base model in handling the updated information.</p>
<p>In comparison to the whole model, there are various techniques that involve incorporating additional parameter layers (e.g.adapter layers (Hartvigsen et al., 2022)) as plug-ins into the original model.T-Patcher (Huang et al., 2023d) and NKB (Dai et al., 2022b) both add the patches into FFN layers which are acknowledged as the repository storing knowledge (Geva et al., 2021) to rectify the factual mistakes.CALINET (Dong et al., 2022) proposes an assessment for identifying erroneous knowledge in PLMs and similarly adjusts the output of FFNs by introducing FFNlike memory slots, which is beneficial to alleviate the knowledge gap.These methods require additional steps to train the parameter module, carefully designing training functions and structures to promote the plug-in to play a role in updated knowledge while keeping unedited facts handled by the original module.</p>
<p>Knowledge editing methods can effectively introduce knowledge to mitigate the model's knowledge gap to some extent.Nevertheless, there is room for enhancement in the impact of knowledge editing.(Zhong et al., 2023b) proposes MQUAKE to evaluate the generalization of injected knowledge and finds that the post-edited model can successfully recall the edited facts but fails in complex multi-hop questions.There are also some studies (Wu et al., 2023;Wang et al., 2023e) indicating that existing editing methods exhibit limited crosslanguage generalization capabilities.Furthermore, Pinter and Elhadad (2023) suggests that knowledge editing techniques introduce potential risk to users when attempting to mitigate hallucinations of LLMs and advises utilizing methods incorporating explicit knowledge (e.g.retrieval-augmented methods).</p>
<p>Retrieval Augmentation.An intuitive way to mitigate the knowledge gap is Retrieval-Augmented Generation (RAG) (Lewis et al., 2020b;Guu et al., 2020;Shuster et al., 2021), grounding the LLMs during generation by conditioning on relevant documents retrieved from an external knowledge source.Typically, RAG follows a retrievethen-read pipeline, where relevant contextual documents are firstly retrieved by a retriever (Karpukhin et al., 2020) from external sources, and then the desired output is generated by a generator conditioning on both input text and retrieved documents.We categorize the methods to mitigate hallucination using retrieval augmentation into three types, including one-time retrieval, iterative retrieval, and post-hoc retrieval.</p>
<p> One-time Retrieval.One-time retrieval aims to directly prepend the external knowledge obtained from a single retrieval to the LLMs' prompt.Ram et al. (2023) introduces Incontext RALM, which entails a straightforward yet effective strategy of prepending chosen documents to the input text of LLMs.Demonstrated empirical results indicate that the employment of In-context RALM consistently translates into enhanced performance across varying LLM sizes and a diverse array of corpora.Notably, the incorporation of a ranking mechanism has been shown to further amplify performance gains.</p>
<p>Beyond conventional knowledge repositories such as Wikipedia, ongoing research endeavors have explored alternative avenues, specifically the utilization of knowledge graphs (KGs).These KGs serve as a pivotal tool for prompting LLMs, facilitating their interaction with the most recent knowledge, and eliciting robust reasoning pathways (Wen et al., 2023;Qi et al., 2023;Baek et al., 2023).Varshney et al. (2023) introduce the Parametric Knowledge Guiding (PKG) framework, enhancing LLMs with domain-specific knowledge.PKG employs a trainable background knowledge module, aligning it with task knowledge and generating relevant contextual information.</p>
<p>The effectiveness of PKG highlights the potential for enhancing LLMs' faithfulness by incorporating retrieved background knowledge.</p>
<p> Iterative Retrieval.However, when confronted with intricate challenges like multistep reasoning (Yang et al., 2018c) and longform question answering (Fan et al., 2019;Stelmakh et al., 2022), traditional one-time retrieval may fall short.</p>
<p>Addressing these demanding information needs, recent studies have proposed iterative retrieval, which allows for continuously gathering knowledge throughout the generation process.A burgeoning line of research (Khot et al., 2022;Yao et al., 2022;Press et al., 2022;He et al., 2023;Trivedi et al., 2023) endeavors to tackle such intricate tasks by decomposing them into more manageable sub-tasks.Recognizing the substantial advancements chainof-thought prompting has brought to LLMs in multi-step reasoning Wei et al. (2022), numerous studies (Yao et al., 2022;Trivedi et al., 2023;He et al., 2023)  Beyond multi-step reasoning tasks, Jiang et al. (2023) shift their emphasis to long-form generation.They proposed an active retrieval augmented generation framework, which iteratively treats the upcoming prediction as a query to retrieve relevant documents.If the prediction contains tokens of low confidence, the sentence undergoes regeneration.In addition to using iterative retrieval to improve intermediate generations, Zhang et al. (2023e) present MixAlign, which iteratively refines user questions using model-based guidance and seeking clarifications from users, ultimately enhancing the alignment between questions and knowledge.</p>
<p> Post-hoc Retrieval.Beyond the traditional retrieve-then-read paradigm, a line of work has delved into post-hoc retrieval, refining LLM outputs through subsequent retrievalbased revisions.</p>
<p>To enhance the trustworthiness and attribution of LLMs, Gao et al. (2023a) adopt the research-then-revise workflow, which initially research relevant evidence and subsequently revise the initial generation based on detected discrepancies with the evidence.Similarly, Zhao et al. (2023a) introduce the verify-andedit framework to enhance the factual accuracy of reasoning chains by incorporating external knowledge.For reasoning chains that show lower-than-average consistency, the framework generates verifying questions and then refines the rationales based on retrieved knowledge, ensuring a more factual response.Yu et al. (2023d) enhanced the post-hoc retrieval method through diverse answer generation.Instead of generating just a single answer, they sample various potential answers, allowing for a more comprehensive retrieval feedback.Additionally, by employing an ensembling technique that considers the likelihood of the answer before and after retrieval, they further mitigate the risk of misleading retrieval feedback.</p>
<p>Mitigating Knowledge Shortcut</p>
<p>Knowledge shortcuts manifest when LLMs lean on spurious correlations, such as the co-occurrence statistics of the pre-training corpora, to capture factual knowledge.Kang and Choi (2023) suggested fine-tuning on a debiased dataset constructed by excluding biased samples.Although this leads to a notable decline in the recall of frequent facts as more samples are excluded, this method struggles to generalize when rare facts are unseen during finetuning.</p>
<p>Mitigating Knowledge Recall Failures</p>
<p>A prevalent source of hallucinations in LLMs is their inability to accurately retrieve and apply relevant information embedded in their parametric knowledge.This challenge is particularly acute in complex reasoning scenarios where the integrity of information is critical.By enhancing knowledge recall, we can better anchor the model's outputs to verifiable knowledge, thereby providing a more robust defense against generating hallucination content.Typically, the most direct approach to recall knowledge is enabling LLMs to reason via Chain-of-Thought prompting.Zhong et al. (2023b) suggest that simply applying CoT can increase knowledge recall, which substantially boosts performance in editing facts under multihop settings.Instead of incorporating reasoning steps, Zheng et al. (2023) posit that directly supplementing questions with relevant information can enhance the model's ability to recall crucial knowledge.Wang et al. (2023g) advance this by employing conceptualization, which distills original commonsense knowledge into high-level abstract knowledge, boosting knowledge recall.</p>
<p>Mitigating Training-related Hallucination</p>
<p>Training-related hallucinations typically arise from the intrinsic limitations of the architecture and training strategies adopted by LLMs.In this context, we discuss various optimization methods ranging from training stages ( 5.2.1) and alignment stages ( 5.2.2), aiming to mitigate hallucinations within the training process.</p>
<p>Mitigating Pretraining-related Hallucination</p>
<p>To address pretraining-related hallucination, the majority of research emphasizes the exploration of novel model architectures and the improvement of pre-training objectives.</p>
<p>Mitigating Flawed Model Architecture.One significant avenue of research in mitigating pretraining-related hallucination centers on the limitations inherent in model architectures, especially unidirectional representation and attention glitches.In light of this, numerous studies have delved into designing novel model architectures specifically tailored to address these flaws.</p>
<p> Mitigating Unidirectional Representation.</p>
<p>Addressing the limitations inherent in unidirectional representation, Li et al. (2023h) introduced BATGPT that employs a bidirectional autoregressive approach.This design allows the model to predict the next token based on all previously seen tokens, considering both past and future contexts, thus capturing dependencies in both directions.Building on this idea, Liu et al. (2023e) highlighted the potential of encoder-decoder models to make better use of their context windows, suggesting a promising direction for future LLMs architecture design.</p>
<p> Mitigating Attention Glitches.Recognizing the limitations of soft attention within self-attention-based architecture, Liu et al. (2023a) proposed attention-sharpening regularizers.This plug-and-play approach sparsifies self-attention architectures using differentiable loss terms (Zhang et al., 2018) to promote sparsity, leading to a significant reduction in reasoning hallucinations.</p>
<p>Mitigating Suboptimal Pre-training Objective.</p>
<p>In the pre-training phase of LLMs, the choice of objective plays a pivotal role in determining the model's performance.However, conventional objectives can lead to fragmented representations and inconsistencies in model outputs.Recent advancements have sought to address these challenges by refining pre-training strategies, ensuring richer context comprehension, and circumventing biases.This section sheds light on these pioneering approaches, encompassing both novel training objectives and efforts to counteract exposure bias.(Shen et al., 2016), it can further reduce hallucinations related to exposure bias.</p>
<p>Mitigating Misalignment Hallucination</p>
<p>Hallucinations induced during alignment often stem from capability misalignment and belief misalignment.However, defining the knowledge boundary of LLMs proves challenging, making it difficult to bridge the gap between LLMs' inherent capabilities and the knowledge presented in humanannotated data.While limited research addresses capability misalignment, the focus mainly shifts toward belief misalignment.</p>
<p>Hallucinations stemming from belief misalignment often manifest as sycophancy, a tendency of LLMs to seek human approval in undesirable ways.This sycophantic behavior can be attributed to the fact that human preference judgments often favor sycophantic responses over more truthful ones (Sharma et al., 2023), paving the way for reward hacking (Saunders et al., 2022).To address this, a straightforward strategy is to improve human preference judgments and, by extension, the preference model.Recent research (Bowman et al., 2022;Saunders et al., 2022) has investigated the use of LLMs to assist human labelers in identifying overlooked flaws.Additionally, Sharma et al. (2023) discovered that aggregating multiple human prefer-ences enhances feedback quality, thereby reducing sycophancy.</p>
<p>Besides, modifications to LLMs' internal activations have also shown the potential to alter model behavior.This can be achieved through methods like fine-tuning (Wei et al., 2023) or activation steering during inference (Dathathri et al., 2020;Subramani et al., 2022;Gu et al., 2022b,c;Hernandez et al., 2023).Specifically, Wei et al. (2023) proposed a synthetic-data intervention, fine-tuning language models using synthetic data where the claim's ground truth is independent of a user's opinion, aiming to reduce sycophantic tendencies.</p>
<p>Another avenue of research (Rimsky, 2023b,a) has been to mitigate sycophancy through activation steering.This approach involves using pairs of sycophantic/non-sycophantic prompts to generate the sycophancy steering vector, derived from averaging the differences in intermediate activations.During inference, subtracting this vector can produce less sycophantic LLM outputs.</p>
<p>Mitigating Inference-related Hallucination</p>
<p>Decoding strategies in Large Language Models play a pivotal role in determining the factuality and faithfulness of the generated content.However, As analyzed in Section 3.3, imperfect decoding often results in outputs that might lack factuality or stray from the original context.In this subsection, we explore two advanced strategies aimed at refining the decoding strategy to enhance both the factuality and faithfulness of the LLMs' outputs.</p>
<p>Factuality Enhanced Decoding</p>
<p>Factuality Enhanced Decoding focuses on ensuring the factuality of the information produced by LLMs.By emphasizing the accuracy of facts, this strategy aims to generate outputs that adhere strictly to realworld information and resist producing misleading or false statements.On Standalone Decoding.Considering the randomness in the sampling process can introduce nonfactual content into open-ended text generation, Lee et al. (2022b) introduced the factual-nucleus sampling algorithm that dynamically adjusts the "nucleus" p throughout sentence generation.By dynamically adjusting the nucleus probability based on decay factors and lower boundaries and resetting the nucleus probability at the beginning of every new sentence, the decoding strategy strikes a balance between generating factual content and preserving output diversity.</p>
<p>Moreover, some studies (Burns et al., 2022;Moschella et al., 2022) posit that the activation space of LLMs contains interpretable structures related to factuality.Building on this idea, Li et al. (2023d) introduce Inference-Time Intervention (ITI).This method first identifies a direction in the activation space associated with factually correct statements and then adjusts activations along the truth-correlated direction during inference.By repeatedly applying such intervention, LLMs can be steered towards producing more factual responses.</p>
<p>Similarly, Chuang et al. (2023) delve into enhancing the factuality of LLM's decoding process from a perspective of factual knowledge storage.They exploit the hierarchical encoding of factual knowledge within transformer LLMs, noting that lower-level information is captured in earlier layers and semantic information in the later ones.Drawing inspiration from Li et al. (2022c), they introduce DoLa, a strategy that dynamically selects and contrasts logits from different layers to refine decoding factuality.By placing emphasis on knowledge from higher layers and downplaying that from the lower layers, DoLa showcases its potential to make LLMs more factual, thus reducing hallucinations.</p>
<p>Post-editing Decoding.Unlike methods that directly modify the probability distribution to prevent hallucinations during the initial decoding, post-editing decoding seeks to harness the selfcorrection capabilities of LLMs (Pan et al., 2023) to refine the originally generated content without relying on an external knowledge base.Dhuliawala et al. (2023) introduced the Chain-of-Verification (COVE), which operates under the assumption that, when appropriately prompted, LLMs can selfcorrect their mistakes and provide more accurate facts.Starting with an initial draft, it first formulates verification questions and then systematically answers those questions in order to finally produce an improved revised response.Similarly, Ji et al. (2023b) focus on the medical domain and introduces an iterative self-reflection process.This process leverages the inherent ability of LLMs to first generate factual knowledge and then refine the response until it aligns consistently with the provided background knowledge.</p>
<p>Faithfulness Enhanced Decoding</p>
<p>On the other hand, Faithfulness Enhanced Decoding prioritizes alignment with the user instructions or provided context and also emphasizes enhancing the consistency within the generated content.Thus, in this section, we summarize existing work into two categories, including Context Consistency and Logical Consistency.</p>
<p>Context Consistency.Decoding strategies that prioritize context consistency are designed to enhance the faithfulness of LLMs to both user instructions and the provided context.Before the era of LLMs, prior studies extensively explored improvements in context consistency, predominantly in the fields of abstractive summarization and datato-text.Tian et al. (2019) proposed confident decoding that incorporates a confidence score during the decoding process to measure the model's attention level to the source.By emphasizing the source more when the confidence score is high, they mitigate hallucinations stemming from a lack of context attention.van der Poel et al. ( 2022) shifts the decoding objective to pointwise mutual information.This approach encourages the model to prioritize tokens relevant to the source document, especially when model uncertainty rises, aiming to prevent hallucinations.In contrast to previous strategies that emphasized enhanced attention to the source for bolstering context consistency, Wan et al. (2023) delved into whether better exploration of the search space could improve faithfulness.By using automatic faithfulness metrics to rank candidates generated by beam search and incorporating lookahead heuristics that assign a faithfulness score to the future generation, they achieved significant improvements in faithfulness compared to existing decoding strategies.</p>
<p>However, in the era of LLMs, the issue of hallucinations due to insufficient attention to context remains.Shi et al. (2023b) propose context-aware decoding (CAD), which modifies the output distribution by reducing reliance on prior knowledge, thereby promoting the model's focus on the contextual information, similar to that presented by (van der Poel et al., 2022).However, due to the inherent trade-off between diversity and attribution (Gu et al., 2022a), overemphasizing contextual information can reduce diversity.In response, Chang et al. (2023a) introduced an innovative sampling algorithm to bolster attribution while preserving diversity.This method involves two parallel de-codings, one considering the source and the other not, and adjusts the temperature dynamically using the KL divergence between their token distributions to reflect source attribution.Lei et al. (2023) explored a more generic post-edit framework to mitigate faithfulness hallucinations during inference.This approach first detects hallucinations at both the sentence and entity levels and then utilizes this detection feedback to refine the generated response.Furthermore, Choi et al. (2023) introduced knowledge-constrained decoding (KCD), which employs a token-level hallucination detection to identify hallucinations and guides the generation process by reweighing the token distribution with a better estimate of the future knowledgegroundedness.Besides, considering that the softmax bottleneck constrains the expression of diversity and faithful representations.A line of work explores methods to overcome the bottleneck, either by a mixture of Softmax, which uses multiple hidden states to compute softmax multiple times and merge the resulting distributions (Yang et al., 2019) or using pointer networks to enable LLMs to copy the context words (Chang et al., 2023b), further reducing hallucinations.</p>
<p>Logical Consistency.Logical consistency in LLMs is essential to ensure consistent responses and prevent hallucinations, particularly during multi-step reasoning.To enhance the selfconsistency inherent in Chain-of-thought prompting, Wang et al. (2023f) employs a knowledge distillation framework.They first generate a consistent rationale using contrastive decoding (Li et al., 2022c) and then fine-tune the student model with a counterfactual reasoning objective, which effectively eliminates reasoning shortcuts (Branco et al., 2021) that derive answers without considering the rationale.Furthermore, by employing contrastive decoding directly, LLMs can reduce surface-level copying and prevent missed reasoning steps (O' Brien and Lewis, 2023).</p>
<p>Challenges and Open Questions</p>
<p>In this section, we delve into the multifarious challenges and open questions surrounding hallucination in LLMs, aiming to guide future directions in this pivotal domain.</p>
<p>Challenges in LLM Hallucination</p>
<p>In the pursuit of reliable and truthful LLMs, addressing hallucination is essential, given its inher-ent complexities.While significant strides have been made in mitigating LLM hallucinations, notable challenges still remain.In this context, we delve into these challenges, highlighting their manifestation in domains such as long-form text generation ( 6.1.1),retrieval augmented generation ( 6.1.2),and large vision-language models ( 6.1.3).</p>
<p>Hallucination in Long-form Text Generation</p>
<p>Long-form text generation has gained widespread application in LLMs (Qin et al., 2023;Bhat et al., 2023;Chen et al., 2023b).However, as the length of the generated content increases, the propensity for hallucination also grows, leading to challenges in evaluating such hallucinations (Min et al., 2023).Firstly, existing LLM hallucination benchmarks (Lin et al., 2022;Cheng et al., 2023) are usually presented in the form of factoid questions and answers, focusing more on factual hallucinations.There is a noticeable absence of manually annotated hallucination benchmarks in the domain of long-form text generation, which hinders researchers from studying specific types of hallucinations in this context.Secondly, evaluating hallucinations in longform text generation is challenging.While there are some evaluation metrics available (Min et al., 2023), they have limitations and are not applicable when the facts are more nuanced, open-ended, and debatable, or when there are conflicts in knowledge sources.This poses obstacles for practical applications in real-world scenarios.</p>
<p>Hallucination in Retrieval Augmented Generation</p>
<p>Retrieval Augmented Generation (RAG) has emerged as a promising strategy to mitigate hallucinations in LLMs.As concerns around LLM hallucinations have intensified, RAG has increasingly come under the spotlight, paving the way for a range of commercial applications, such as Perplexity 2 , YOU.com 3 and New Bing 4 .By retrieving evidence from external knowledge bases, RAG enables LLMs to be equipped with up-to-date knowledge and generate responses conditioning on relevant evidence.However, despite its advantages, RAG also suffers from hallucinations.One notable issue is the potential for error accumulation within the RAG pipeline.Irrelevant evidence can be propagated into the generation phase, possibly tainting the output (Li et al., 2023a;Shi et al., 2023a;Cho et al., 2023;Xu et al., 2023).Another concern lies in the arena of generative retrievals, which occasionally suffer from citation inaccuracies (Rashkin et al.;Liu et al., 2023f;Yue et al., 2023;Gao et al., 2023a;Chen et al., 2023a).While citations aim to offer a traceable path to the information's source for validation purposes, errors in this domain can lead users astray.Furthermore, existing RAG may suffer from a trade-off between diversity and factuality (Liu et al., 2023f) which poses a new challenge in terms of the need for diversity.</p>
<p>Hallucination in Large Vision-Language Models</p>
<p>Enabling the visual perception ability, along with exceptional language understanding and generation capabilities, Large Vision-Language Models (LVLMs) have exhibited remarkable visionlanguage capabilities (Zhu et al., 2023a;Liu et al., 2023d;Yu et al., 2021;Huang et al., 2023b;Maaz et al., 2023;Chen et al., 2023e;Yu et al., 2023b;Zellers et al., 2019).Unlike previous pre-trained multi-modal models that gain limited vision-language abilities from large-scale visuallanguage pre-training datasets (Wang et al., 2022;Li et al., 2023b;Luo et al., 2020;Zhong et al., 2023a), LVLMs exploit advanced large-language models to better interact with humans and the environment.The consequent diverse applications of LVLMs also bring new challenges to maintaining the reliability of such systems, which therefore had to be further investigated and mitigated.Li et al. (2023e), Lovenia et al. (2023), take the first step towards evaluating the object hallucinations in the LVLMs.Evaluations and experiments reveal that current LVLMs are prone to generate inconsistent responses with respect to the associated image, including non-existent objects, wrong object attributes, incorrect semantic relationships, etc.Furthermore, Liu et al. (2023c), Zong et al. (2023) and Liu et al. (2023b) show that LVLMs can be easily fooled and experience a severe performance drop due to their over-reliance on the strong language prior, as well as its inferior ability to defend against inappropriate user inputs.Current evaluations and discussions mainly focus on object hallucination.However, despite the witnessed perception errors, LVLMs can generate flawed logical reasoning results even when correctly recognizing all visual elements, which remains to be further investigated.</p>
<p>Efforts have been made towards building a more robust large vision-language model.Gunjal et al. (2023), Lu et al. (2023), andLiu et al. (2023c) propose to further finetuning the model to produce more truthful and helpful responses.Another line of work chooses to post-hoc rectify the generated inconsistent content, such as (Zhou et al., 2023b), and (Yin et al., 2023).Though proved to be effective, those methods usually require additional data annotations, visual experts, or training phases, which prevent LVLMs from effectively scaling and generalizing to various fields.Thus, a more universal approach is expected in the future to build a more reliable system.What's more, when presented with multiple images, LVLMs sometimes mix or miss parts of the visual context, as well as fail to understand temporal or logical connections between them, which might hinder their usage in many scenarios, yet properly identifying the reason for such disorders and tackling them still requires continued efforts.</p>
<p>Open Questions in LLM Hallucination</p>
<p>As research into LLM hallucination progresses, several questions demand ongoing discussion.These encompass the effectiveness of LLMs' selfcorrection mechanisms in reducing hallucinations ( 6.2.1), the understanding of knowledge boundaries within LLMs ( 6.2.2), and the balance between their creativity and truthfulness ( 6.2.3).</p>
<p>Delving into these open questions paves the way for a more profound understanding of the capabilities of LLMs and the intricacies of hallucinations.</p>
<p>Can Self-Correct Mechanisms Help in</p>
<p>Mitigating Reasoning Hallucinations?</p>
<p>While LLMs have shown remarkable capabilities in tackling complex reasoning tasks through Chainof-Thought prompting (Wei et al., 2022), they occasionally exhibit unfaithful reasoning characterized by inconsistencies within the reasoning steps or conclusions that do not logically follow the reasoning chain (Golovneva et al., 2022;Ribeiro et al., 2023;Lyu et al., 2023).Research indicates that integrating external feedback into LLMs can significantly mitigate such hallucinations in reasoning.This feedback typically comes from external knowledge sources through retrieval processes (He et al., 2023;Gou et al., 2023), interactive debates with other LLMs (Du et al., 2023;Cohen et al., 2023), or guidance from external evaluation metrics (Lei et al., 2023;Khalifa et al., 2023).Nonetheless, a branch of research (Madaan et al., 2023;Yao et al., 2023b;Xie et al., 2023) explores the potential of self-correction mechanisms, where an LLM corrects its initial responses using its builtin capabilities, independent of external feedback.Although self-correction has shown promise for achieving faithful and accurate reasoning, especially in iterative settings, certain studies (Stechly et al., 2023;Huang et al., 2023a;Valmeekam et al., 2023) question the effectiveness of the selfcorrection mechanism, pointing out that LLMs still struggle to self-correct their reasoning chains.Consequently, the effectiveness of self-correction mechanisms in mitigating reasoning hallucinations remains an open question, which deserves further exploration.</p>
<p>Can We Accurately Capture LLM</p>
<p>Knowledge Boundaries?</p>
<p>Despite the impressive capacity to capture factual knowledge from extensive data, LLMs still face challenges in recognizing their own knowledge boundaries.This shortfall leads to the occurrence of hallucinations, where LLMs confidently produce falsehoods without an awareness of their own knowledge limits (Pacchiardi et al., 2023;Ren et al., 2023;Zhao et al., 2023c).Numerous studies delve into probing knowledge boundaries of LLMs, utilizing strategies such as evaluating the probability of a correct response in a multiple-choice setting (Kadavath et al., 2022), or quantifying the model's output uncertainty by evaluating the similarity among sets of sentences with uncertain meanings.Furthermore, a line of work (Moschella et al., 2022;Burns et al., 2022;Li et al., 2023d;Azaria and Mitchell, 2023) has revealed that LLMs contain latent structures within their activation space that relate to beliefs about truthfulness.Recent research (Slobodkin et al., 2023) also found substantial evidence for LLMs' ability to encode the unanswerability of questions, despite the fact that these models exhibit overconfidence and produce hallucinations when presented with unanswerable questions.Nonetheless, Levinstein and Herrmann (2023) have employed empirical and conceptual tools to probe whether or not LLMs have beliefs.Their empirical results suggest that current lie-detector methods for LLMs are not yet fully reliable, and the probing methods proposed by (Burns et al., 2022) and (Azaria and Mitchell, 2023) do not adequately gen-eralize.Consequently, whether we can effectively probe LLMs' internal beliefs is ongoing, requiring further research.</p>
<p>How Can We Strike a Balance between</p>
<p>Creativity and Factuality?</p>
<p>In the development of truthful and reliable LLMs, the challenge of balancing creativity and factuality stands out as a significant concern (Mukherjee and Chang, 2023;Lee, 2023).Ensuring factuality is critical for LLMs intended for real-world applications; any inaccuracies can mislead users and pollute the online environment.The repercussions of such misinformation could be significant, potentially snowballing and cascading into the data used for subsequent LLM training.Conversely, hallucinations can sometimes offer valuable perspectives, particularly in creative endeavors such as storytelling, brainstorming, and generating solutions that transcend conventional thinking.While current research on LLMs leans heavily towards reducing hallucinations, it often overlooks the important role of their creative capacities.As LLMs continue to evolve, the challenge of striking a balance between their creativity and factual accuracy remains unresolved.It is also interesting to explore the balance not only in multi-modal text generation tasks (Li et al., 2023b;Yu et al., 2021) but also in vision generation tasks (Zhang et al., 2023b;Rombach et al., 2022).This issue goes beyond mere technicalities, necessitating a broader contemplation on the essence of artificial intelligence and its implications for human interaction and the exchange of knowledge.</p>
<p>Conclusion</p>
<p>In this comprehensive survey, we have undertaken an in-depth examination of hallucinations within large language models, delving into the intricacies of their underlying causes, pioneering detection methodologies as well as related benchmarks, and effective mitigation strategies.Although significant strides have been taken, the conundrum of hallucination in large language models remains a compelling and ongoing concern that demands continuous investigation.Moreover, we envision this survey as a guiding beacon for researchers dedicated to advancing secure and trustworthy artificial intelligence.By navigating the complex landscape of hallucinations, we hope to empower these dedicated individuals with invaluable insights that drive the evolution of AI technologies towards greater reliability and safety.</p>
<p>60</p>
<p>Answer: In October 2006, Israel declared war on Hamas after an unexpected attack, prompting ongoing violence, civilian crises, and regional conflict escalation.Please summarize the following news article: Context: In early October 2023, war broke out between Israel and Hamas, the militant Islamist group that has controlled Gaza since 2006.Hamas fighters fired rockets ... civilians and taking dozens of hostages.(a) Factuality Hallucination (b) Faithfulness Hallucination</p>
<p>Figure 1 :
1
Figure 1: An intuitive example of LLM hallucination. the model might assertively claim it was Charles Lindbergh in 1951.While the truth is that Neil Armstrong was the first individual to walk on the moon in 1969 during the Apollo 11 mission.On the other hand, faithfulness hallucination refers to the divergence of generated content from user instructions or the context provided by the input, as well as self-consistency within generated content.As illustrated in Figure 1(b), when asked to summarize a news article, the model inaccurately generated the actual event date of the conflict between Israel and Hamas from October 2023 to October 2006.Regarding factuality, we further divide it based on the presence of verifiable sources into two subcategories: factual inconsistency and factual fabrication.For faithfulness, we emphasize addressing inconsistency from the user's perspective, categorizing it into instruction inconsistency, context inconsistency, and logical inconsistency, thus aligning it better with the current usage of LLMs.</p>
<p>is the highest peak of the Himalayan mountain range?The highest peak of the Himalayan mountain range is Mount EverestThe highest peak of the Himalayan mountain range is Mount Everest, also known as Qomolangma ... located on the border between Nepal and China and was first climbed in 1953.</p>
<p>Figure 3 :
3
Figure 3: An example of detecting factuality hallucination by retrieving external facts.</p>
<p>score from the response sequence can serve as a familiarity score for the concept.Furthermore,Yao et al. (2023a) interpreted hallucination through the lens of adversarial attacks.Utilizing gradient-based token replacement, they devised prompts to induce hallucinations.Notably, they observed that the first token generated from a raw prompt typically exhibits low entropy, compared to those from adversarial attacks.Based on this observation, they proposed setting an entropy threshold to define such hallucination attacks.LLM Behavior.However, when systems are only accessible via API calls(OpenAI, 2022;Google, 2023; Microsoft, 2023), access to the output's token-level probability distribution might be unavailable.Given this constraint, several studies have shifted their focus to probing a model's uncertainty, either through natural language prompts(Xiong et al., 2023;Kadavath et al., 2022) or by examining its behavioral manifestations.For instance, by sampling multiple responses from an LLM for the same prompt, Manakul et al. (2023) detect hallucinations via evaluating the consistency among the factual statements.However, these methods predominantly rely on direct queries that explicitly solicit information or verification from the model.Agrawal et al. (2023), inspired by investigative interviews, advocate for the use of indirect queries.Unlike direct ones, these indirect counterparts often pose open-ended questions to elicit specific information.By employing these indirect queries, consistency across multiple model generations can be better evaluated.Beyond assessing uncertainty from the self-consistency of a single LLM's multiple generations, one can embrace a multi-agent perspective by incorporating additional LLMs.Drawing inspiration from legal cross-examination practices, Cohen et al. (2023) introduced the LMvLM approach.This strategy leverages an 'examiner' LM to question an 'examinee' LM, aiming to unveil inconsistencies of claims during multiturn interaction.</p>
<p>) Multi-Debate (a) LLM Internal States Question: What is the highest peak in the world?Large Language ModelThe highest peak in the world is Mount Fuji.</p>
<p>Figure 4 :
4
Figure 4: Taxonomy of Uncertainty Estimation Methods in Factual Hallucination Detection, featuring a) LLM Internal States and b) LLM Behavior, with LLM Behavior encompassing two main categories: Self-Consistency and Multi-Debate.</p>
<p>Figure 5 :
5
Figure 5: The illustration of detection methods for faithfulness hallucinations: a) Fact-based Metrics, which assesses faithfulness by measuring the overlap of facts between the generated content and the source content; b) Classifier-based Metrics, utilizing trained classifiers to distinguish the level of entailment between the generated content and the source content; c) QA-based Metrics, employing question-answering systems to validate the consistency of information between the source content and the generated content; d) Uncertainty Estimation, which assesses faithfulness by measuring the model's confidence in its generated outputs; e) Prompting-based Metrics, wherein LLMs are induced to serve as evaluators, assessing the faithfulness of generated content through specific prompting strategies.</p>
<p>level hallucination detection.Building upon the work of(Kryscinski et al., 2020),Dziri et al. (2021b) utilized perturbation methods to generate adversarial synthetic data aiming to enhance hallucination detection in knowledgegrounded dialogue tasks whileSanthanam et al. (2021) focues on factual consistency for the conversation domain.</p>
<p>Figure 6 :
6
Figure 6: The illustration of three distinct approaches for Retrieval-Augmented Generation: a) One-time Retrieval, where relevant information is retrieved once before text generation; b) Iterative Retrieval, involving multiple retrieval iterations during text generation for dynamic information integration; and c) Post-hoc Retrieval, where the retrieval process happens after an answer is generated, aiming to refine and fact-check the generated content.tion.Instead of solely depending on chain-ofthought prompting for retrieval guidance, both Feng et al. (2023b) and Shao et al. (2023) employ an iterative retrieval-generation collaborative framework, where a model's response serves as an insightful context to procure more relevant knowledge, subsequently refining the response in the succeeding iteration.</p>
<p>Table 1 :
1
Examples of each category of LLM hallucinations.Content marked in Red represents the hallucinatory output, while content marked in Blue indicates user instruction or provided context that contradicts the LLM hallucination.</p>
<p>Table 2 :
2
Examples of Hallucinations from Misinformation and Biases.The table categorizes hallucinations arising from flawed</p>
<p>Table 3 :
3
Example of Knowledge Boundary.</p>
<p>Table 5 :
5
An overview of existing hallucination benchmarks.For Attribute, Factuality and Faithfulness represent whether the benchmark is used to evaluate LLM's factuality or to detect faithfulness hallucination, and Manual represents whether the inputs in the data are handwritten.</p>
<p>https://www.perplexity.ai/
https://you.com/
https://www.bing.com/new
A AppendixTable6: An overview of the performance of existing LLMs on hallucination evaluation benchmarks.For results in TruthfulQA, we directly used the results in Llama-2 Technical Report; as for ChineseFactEval and FacTool, we used the results in its GitHub repository and report.Regarding HalluQA, we directly used the results in its paper.
Semdedup: Dataefficient learning at web-scale through semantic deduplication. Amro Abbas, Kushal Tirumala, Dniel Simig, Surya Ganguli, Ari S Morcos, abs/2303.09540ArXiv preprint. 2023</p>
<p>Evaluating correctness and faithfulness of instructionfollowing models for question answering. Parishad Vaibhav Adlakha, Behnamghader, Han Xing, Nicholas Lu, Siva Meade, Reddy, abs/2307.16877ArXiv preprint. 2023</p>
<p>Do language models know when they're hallucinating references?. Ayush Agrawal, Lester Mackey, Adam Tauman, Kalai , ArXiv preprint, abs/2305.182482023</p>
<p>Characterizing attribution and fluency tradeoffs for retrievalaugmented large language models. Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak Shakeri, Yun-Hsuan Sung, abs/2302.05578ArXiv preprint. 2023</p>
<p>Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. Kushal Arora, Layla El Asri, Hareesh Bahuleyan, Jackie Cheung, 10.18653/v1/2022.findings-acl.58Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Generating fact checking explanations. Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein, 10.18653/v1/2020.acl-main.656Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Mul-tiFC: A real-world multi-domain dataset for evidencebased fact checking of claims. Isabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Christian Hansen, Jakob Grue Simonsen, 10.18653/v1/D19-1475Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>The internal state of an LLM knows when its lying. Amos Azaria, Tom M Mitchell, abs/2304.137342023ArXiv preprint</p>
<p>Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. Jinheon Baek, Alham Fikri Aji, Amir Saffari, abs/2306.041362023ArXiv preprint</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, ArXiv preprint, abs/2302.040232023</p>
<p>Adversarial nli for factual correctness in text summarisation models. Mario Barrantes, Benedikt Herudek, Richard Wang, 2020. 2005.11739ArXiv preprint</p>
<p>Longformer: The long-document transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, abs/2004.051502020ArXiv preprint</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, 10.1145/3442188.3445922FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event. Toronto, CanadaACM2021. March 3-10, 2021</p>
<p>Scheduled sampling for sequence prediction with recurrent neural networks. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. Montreal, Quebec, Canada2015. 2015. December 7-12, 2015</p>
<p>The reversal curse: Llms trained on. Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans, 2023a is b" fail to learn" b is a". ArXiv preprint, abs/2309.12288</p>
<p>It's mbr all the way down: Modern generation techniques through the lens of minimum bayes risk. Amanda Bertsch, Alex Xie, Graham Neubig, Matthew R Gormley, abs/2310.01387ArXiv preprint. 2023</p>
<p>Investigating answerability of llms for long-form question answering. Meghana Moorthy Bhat, Rui Meng, Ye Liu, Yingbo Zhou, Semih Yavuz, abs/2309.08210ArXiv preprint. 2023</p>
<p>Weight uncertainty in neural network. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015. the 32nd International Conference on Machine Learning, ICML 2015Lille, France2015. 6-11 July 201537of JMLR Workshop and Conference Proceedings</p>
<p>Measuring progress on scalable oversight for large language models. Jeeyoon Samuel R Bowman, Ethan Hyun, Edwin Perez, Craig Chen, Scott Pettit, Kamile Heiner, Amanda Lukosuite, Andy Askell, Anna Jones, Chen, ArXiv preprint, abs/2211.035402022</p>
<p>Rank analysis of incomplete block designs: I. the method of paired comparisons. Ralph Allan, Bradley , Milton E Terry, Biometrika. 393/41952</p>
<p>Shortcutted commonsense: Data spuriousness in deep learning of commonsense reasoning. Ruben Branco, Antnio Branco, Joo Antnio Rodrigues, Joo Ricardo Silva, 10.18653/v1/2021.emnlp-main.113Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>On the resemblance and containment of documents. Andrei Z Broder, Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171). Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)IEEE1997</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M Lundberg, Harsha Nori, Hamid Palangi, Marco Tlio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with GPT-4. 2023ArXiv preprint, abs/2303.12712</p>
<p>Discovering latent knowledge in language models without supervision. Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt, abs/2212.038272022ArXiv preprint</p>
<p>Quantifying memorization across neural language models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, abs/2202.07646ArXiv preprint. 2022</p>
<p>Extracting training data from large language models. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, 30th USENIX Security Symposium (USENIX Security 21). 2021</p>
<p>Kl-divergence guided temperature sampling. Chung-Ching Chang, David Reitter, Renat Aksitov, Yun-Hsuan Sung, abs/2306.01286ArXiv preprint. 2023a</p>
<p>Softmax bottleneck makes language models unable to represent multi-mode word distributions. Haw-Shiuan Chang, Andrew Mccallum, 10.18653/v1/2022.acl-long.554Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221Long Papers)</p>
<p>Revisiting the architectures like pointer networks to efficiently improve the next word distribution, summarization factuality, and beyond. Haw-Shiuan Chang, Zonghai Yao, Alolika Gon, Hong Yu, Andrew Mccallum, 10.18653/V1/2023.FINDINGS-ACL.805Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023b. July 9-14, 2023</p>
<p>PURR: efficiently editing language model hallucinations by denoising language model corruptions. Anthony Chen, Panupong Pasupat, Sameer Singh, abs/2305.14908ArXiv preprint. 2023aHongrae Lee, and Kelvin Guu</p>
<p>Understanding retrieval augmentation for long-form question answering. Canyu Chen, Kai Shu ; Ting Chen, Fangyuan Xu, Shane A Arora, Eunsol Choi, ArXiv preprint, abs/2310.121502023. 2023bCan llm-generated misinformation be detected? ArXiv preprint</p>
<p>Complex claim verification with evidence retrieved in the wild. Jifan Chen, Grace Kim, Aniruddh Sriram, Greg Durrett, Eunsol Choi, abs/2305.11859ArXiv preprint. 2023c</p>
<p>Summscreen: A dataset for abstractive screenplay summarization. Mingda Chen, Zewei Chu, Sam Wiseman, Kevin Gimpel, 10.18653/V1/2022.ACL-LONG.589Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022a. May 22-27, 20221ACL 2022</p>
<p>Felm: Benchmarking factuality evaluation of large language models. Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, Junxian He, abs/2310.007412023d</p>
<p>Towards improving faithfulness in abstractive summarization. Xiuying Chen, Mingzhe Li, Xin Gao, Xiangliang Zhang, 2022bIn NeurIPS</p>
<p>Measuring and improving chain-of-thought reasoning in vision-language models. Yangyi Chen, Karan Sikka, Michael Cogswell, Ji Heng, Ajay Divakaran, abs/2309.04461ArXiv preprint. 2023e</p>
<p>Improving translation faithfulness of large language models via augmenting instructions. Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou, abs/2308.12674ArXiv preprint. 2023f</p>
<p>Reinforcement learning based graph-to-sequence model for natural question generation. Yu Chen, Lingfei Wu, Mohammed J Zaki, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>. Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, </p>
<p>Xipeng Qiu, Evaluating hallucinations in chinese large language models. 2023</p>
<p>Factool: Factuality detection in generative ai-a tool augmented framework for multi-task and multi-domain scenarios. Steffi Chern, Shiqi Chern, Weizhe Chen, Kehua Yuan, Chunting Feng, Junxian Zhou, Graham He, Pengfei Neubig, Liu, abs/2307.13528ArXiv preprint. 2023</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, ArXiv preprint, abs/2305.019372023</p>
<p>Overcoming a theoretical limitation of self-attention. David Chiang, Peter Cholak, 10.18653/v1/2022.acl-long.527Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221Long Papers)</p>
<p>Improving zero-shot reader by reducing distractions from irrelevant documents in open-domain question answering. Sukmin Cho, Soyeong Jeong, Jong C Park, abs/2310.17490ArXiv preprint. 2023</p>
<p>Kcts: Knowledge-constrained tree search decoding with token-level hallucination detection. Sehyun Choi, Tianqing Fang, Zhaowei Wang, Yangqiu Song, abs/2310.090442023ArXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Shivani Dohan, Mark Agrawal, Omernick, J. Mach. Learn. Res. M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel241132023</p>
<p>Deep reinforcement learning from human preferences. Paul F Christiano, Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, Dario Amodei, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USA2017. 2017. December 4-9, 2017</p>
<p>A survey of chain of thought reasoning: Advances, frontiers and future. Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu, ArXiv preprint, abs/2309.154022023</p>
<p>Dola: Decoding by contrasting layers improves factuality in large language models. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, Pengcheng He, abs/2309.03883ArXiv preprint. 2023</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, ArXiv preprint, abs/2210.114162022</p>
<p>LM vs LM: detecting factual errors via cross examination. Roi Cohen, May Hamri, Mor Geva, Amir Globerson, abs/2305.132812023ArXiv preprint</p>
<p>Why AI alignment could be hard with modern deep learning. Ajeya Cotra, 2021Cold Takes</p>
<p>Knowledge neurons in pretrained transformers. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei, 10.18653/v1/2022.acl-long.581Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022a1</p>
<p>Neural knowledge bank for pretrained transformers. Damai Dai, Wenbin Jiang, Qingxiu Dong, Yajuan Lyu, Qiaoqiao She, Zhifang Sui, abs/2208.00399ArXiv preprint. 2022b</p>
<p>Plug and play language models: A simple approach to controlled text generation. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>Editing factual knowledge in language models. Nicola De Cao, Wilker Aziz, Ivan Titov, 10.18653/v1/2021.emnlp-main.522Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Language modeling is compression. Grgoire Deltang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li , Kevin Wenliang, Matthew Aitchison, Laurent Orseau, ArXiv preprint, abs/2309.106682023</p>
<p>Chain-of-verification reduces hallucination in large language models. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston, abs/2309.11495ArXiv preprint. 2023</p>
<p>Calibrating factual knowledge in pretrained language models. Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, Lei Li, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab Emirates2022Association for Computational Linguistics</p>
<p>Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models. Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen, abs/2309.13345ArXiv preprint. 2023</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, abs/2305.14325ArXiv preprint. 2023</p>
<p>FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. Esin Durmus, He He, Mona Diab, 10.18653/v1/2020.acl-main.454Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Neural path hunter: Reducing hallucination in dialogue systems via path grounding. Nouha Dziri, Andrea Madotto, Osmar Zaane, Avishek Joey Bose, 10.18653/v1/2021.emnlp-main.168Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021aOnline and Punta Cana</p>
<p>Evaluating groundedness in dialogue systems: The begin benchmark. Nouha Dziri, Hannah Rashkin, Tal Linzen, David Reitter, abs/2105.000712021bArXiv preprint</p>
<p>QAFactEval: Improved QAbased factual consistency evaluation for summarization. Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, Caiming Xiong, 10.18653/v1/2022.naacl-main.187Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>SummEval: Re-evaluating summarization evaluation. Alexander R Fabbri, Wojciech Kryciski, Bryan Mc-Cann, Caiming Xiong, Richard Socher, Dragomir Radev, 10.1162/tacl_a_00373Transactions of the Association for Computational Linguistics. 92021</p>
<p>Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. Tobias Falke, Leonardo F R Ribeiro, 10.18653/v1/P19-1213Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych</p>
<p>ELI5: Long form question answering. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, Michael Auli, 10.18653/v1/P19-1346Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Hierarchical neural story generation. Angela Fan, Mike Lewis, Yann Dauphin, 10.18653/v1/P18-1082Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Improving factual consistency of text summarization by adversarially decoupling comprehension and embellishment abilities of llms. Huawen Feng, Yan Fan, Xiong Liu, Ting-En Lin, Zekun Yao, Yuchuan Wu, Fei Huang, Yongbin Li, Qianli Ma, 10.48550/ARXIV.2310.19347CoRR, abs/2310.193472023a</p>
<p>Retrieval-generation synergy augmented large language models. Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, Bing Qin, abs/2310.051492023bArXiv preprint</p>
<p>Should chatgpt be biased? challenges and risks of bias in large language models. Emilio Ferrara, abs/2304.037382023ArXiv preprint</p>
<p>Controlled hallucinations: Learning to generate faithfully from noisy data. Katja Filippova, 10.18653/v1/2020.findings-emnlp.76Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Chainpoll: A high efficacy method for llm hallucination detection. Robert Friel, Atindriyo Sanyal, 2023</p>
<p>Dropout as a bayesian approximation: Representing model uncertainty in deep learning. Yarin Gal, Zoubin Ghahramani, Proceedings of the 33nd International Conference on Machine Learning, ICML 2016. the 33nd International Conference on Machine Learning, ICML 2016New York City, NY, USA2016. June 19-24, 201648JMLR Workshop and Conference Proceedings</p>
<p>Truth-o-meter: Collaborating with llm in fighting its hallucinations. Boris A Galitsky, 2023</p>
<p>The pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, abs/2101.00027ArXiv preprint. 2021</p>
<p>RARR: researching and revising what language models say, using language models. Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Y Vincent, Ni Zhao, Hongrae Lao, Da-Cheng Lee, Kelvin Juan, Guu, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a. July 9-14, 2023ACL 2023</p>
<p>Human-like summarization evaluation with chatgpt. Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, abs/2304.025542023bArXiv preprintShiping Yang, and Xiaojun Wan</p>
<p>Transformer feed-forward layers are keyvalue memories. Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy, 10.18653/v1/2021.emnlp-main.446Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Roscoe: A suite of metrics for scoring step-by-step reasoning. Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, abs/2212.07919ArXiv preprint. 2022</p>
<p>Assessing the factual accuracy of generated text. Ben Goodrich, Vinay Rao, Peter J Liu, Mohammad Saleh, 10.1145/3292500.3330955Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019Anchorage, AK, USAACM2019. August 4-8, 2019</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Bard Google, Zhihong Zhibin Gou, Yeyun Shao, Yelong Gong, Yujiu Shen, Nan Yang, Weizhu Duan, Chen, abs/2305.11738ArXiv preprint. 2023. 2023</p>
<p>Evaluating factuality in generation with dependency-level entailment. Tanya Goyal, Greg Durrett, 10.18653/v1/2020.findings-emnlp.322Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Annotating and modeling fine-grained factuality in summarization. Tanya Goyal, Greg Durrett, 10.18653/v1/2021.naacl-main.114Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Improving controllable text generation with position-aware weighted decoding. Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Jiaming Wu, Heng Gong, Bing Qin, 10.18653/v1/2022.findings-acl.272Findings of the Association for Computational Linguistics: ACL 2022. Dublin, Ireland2022aAssociation for Computational Linguistics</p>
<p>Heng Gong, and Bing Qin. 2022b. Controllable text generation via probability density estimation in the latent space. Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Lingyuan Zhang, abs/2212.08307ArXiv preprint. </p>
<p>Heng Gong, and Bing Qin. 2022c. A distributional lens for multi-aspect controllable text generation. Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Lingyuan Zhang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics</p>
<p>Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation. M Nuno, Elena Guerreiro, Andr Voita, Martins, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational LinguisticsDubrovnik, CroatiaAssociation for Computational Linguistics2023a</p>
<p>Hallucinations in large multilingual translation models. Miguel Nuno, Guerreiro, M Duarte, Jonas Alves, Barry Waldendorf, Alexandra Haddow, Pierre Birch, Colombo, F T Andr, Martins, abs/2303.16104ArXiv preprint. 2023b</p>
<p>Textbooks are all you need. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Csar, Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo De Rosa, Olli Saarikivi, 2023ArXiv preprint, abs/2306.11644</p>
<p>Detecting and preventing hallucinations in large vision language models. Anisha Gunjal, Jihan Yin, Erhan Bas, abs/2308.063942023ArXiv preprint</p>
<p>and Andreas Vlachos. 2022. A survey on automated fact-checking. Zhijiang Guo, Michael Schlichtkrull, 10.1162/tacl_a_00454Transactions of the Association for Computational Linguistics. 10</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, Proceedings of the 37th International Conference on Machine Learning, ICML 2020. the 37th International Conference on Machine Learning, ICML 2020PMLR2020. 13-18 July 2020119of Proceedings of Machine Learning Research</p>
<p>Deduplication of scholarly documents using locality sensitive hashing and word embeddings. Bikash Gyawali, Lucas Anastasiou, Petr Knoth, Proceedings of the Twelfth Language Resources and Evaluation Conference. the Twelfth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association2020</p>
<p>Theoretical limitations of selfattention in neural sequence models. Michael Hahn, 10.1162/tacl_a_00306Transactions of the Association for Computational Linguistics. 82020</p>
<p>A richly annotated corpus for different tasks in automated factchecking. Andreas Hanselowski, Christian Stab, Claudia Schulz, Zile Li, Iryna Gurevych, 10.18653/v1/K19-1046Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). the 23rd Conference on Computational Natural Language Learning (CoNLL)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Aging with GRACE: lifelong model editing with discrete key-value adaptors. Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, Marzyeh Ghassemi, abs/2211.11031ArXiv preprint. 2022</p>
<p>Rethinking with retrieval: Faithful large language model inference. Hangfeng He, Hongming Zhang, Dan Roth, abs/2301.003032023ArXiv preprint</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. Austria2021. May 3-7, 2021OpenReview.net</p>
<p>Scaling laws and interpretability of learning from repeated data. Danny Hernandez, Tom B Brown, Tom Conerly, Nova Dassarma, Dawn Drain, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, Scott Johnston, Benjamin Mann, Chris Olah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan, Sam Mccandlish, abs/2205.104872022ArXiv preprint</p>
<p>Inspecting and editing knowledge representations in language models. Evan Hernandez, Belinda Z Li, Jacob Andreas, abs/2304.007402023ArXiv preprint</p>
<p>Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Erich Simonyan, Jack W Elsen, Rae, ArXiv preprint, abs/2203.15556</p>
<p>The curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering. Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, Omri Abend, 10.18653/v1/2021.emnlp-main.619Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics20212Online and Punta Cana</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, abs/2310.01798ArXiv preprint. 2023a</p>
<p>Language is not all you need: Aligning perception with language models. ArXiv preprint, abs/2302.14045. Yi-Chong Huang, Xia-Chong Feng, Xiao-Cheng Feng, and Bing Qin. 2021. The factual inconsistency problem in abstractive text summarization: A survey. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, 2023bArXiv preprint, abs/2104.14839</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, abs/2305.08322ArXiv. 2023c</p>
<p>Transformerpatcher: One mistake worth one neuron. Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, Zhang Xiong, ICLR 2023The Eleventh International Conference on Learning Representations. Kigali, Rwanda2023d. May 1-5, 2023OpenReview.net</p>
<p>Retrieving supporting evidence for llms generated answers. Siqing Huo, Negar Arabzadeh, Charles L A Clarke, abs/2306.13781ArXiv preprint. 2023</p>
<p>Opt-iml: Scaling language model instruction meta learning through the lens of generalization. Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, abs/2212.12017ArXiv preprint. 2022</p>
<p>Multidimensional evaluation of text summarization with in-context learning. Sameer Jain, Vaishakh Keshava, Mysore Swarnashree, Patrick Sathyendra, Pengfei Fernandes, Liu, 2023Graham Neubig, and Chunting Zhou. ArXiv preprint, abs/2306.01200</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730ACM Comput. Surv. 5512382023a</p>
<p>Towards mitigating hallucination in large language models via selfreflection. Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung, abs/2310.06271ArXiv preprint. 2023b</p>
<p>X-FACTR: Multilingual factual knowledge retrieval from pretrained language models. Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki, Haibo Ding, Graham Neubig, 10.18653/v1/2020.emnlp-main.479Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational Linguistics2020</p>
<p>Active retrieval augmented generation. Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, abs/2305.06983ArXiv preprint. 2023</p>
<p>Language models (mostly) know what they know. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova Dassarma, Eli Tran-Johnson, ArXiv preprint, abs/2207.052212022</p>
<p>Large language models struggle to learn long-tail knowledge. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, Colin Raffel, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023. July 2023Proceedings of Machine Learning Research</p>
<p>Impact of co-occurrence on factual knowledge of large language models. Cheongwoong Kang, Jaesik Choi, abs/2310.082562023ArXiv preprint</p>
<p>Scaling laws for neural language models. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, ArXiv preprint. 2020. 2001.08361</p>
<p>Dense passage retrieval for opendomain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, 10.18653/v1/2020.emnlp-main.550Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Realtime qa: What's the answer right now?. Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A Smith, Yejin Choi, Kentaro Inui, ArXiv preprint, abs/2207.133322022</p>
<p>. Martin Daniel, Michael James Katz, Shang Bommarito, Pablo Gao, Arredondo, 2023Gpt-4 passes the bar exam. Available at SSRN 4389233</p>
<p>Discriminator-guided multi-step reasoning with language models. Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Lu Wang, abs/2305.14934ArXiv preprint. 2023</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, abs/2210.02406ArXiv preprint. 2022</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Evaluating the factual consistency of abstractive text summarization. Wojciech Kryscinski, Bryan Mccann, Caiming Xiong, Richard Socher, 10.18653/v1/2020.emnlp-main.750Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Natural questions: A benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, 10.1162/tacl_a_00276Transactions of the Association for Computational Linguistics. 72019</p>
<p>Philippe Laban, Wojciech Kryciski, Divyansh Agarwal, Alexander R Fabbri, Caiming Xiong, Shafiq Joty, Chien-Sheng Wu, ArXiv preprint, abs/2305.14540Llms as factual reasoners: Insights from existing benchmarks and beyond. 2023</p>
<p>SummaC: Re-visiting NLIbased models for inconsistency detection in summarization. Philippe Laban, Tobias Schnabel, Paul N Bennett, Marti A Hearst, 10.1162/tacl_a_00453Transactions of the Association for Computational Linguistics. 102022</p>
<p>When do pre-training biases propagate to downstream tasks? a case study in text summarization. Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhang, Dan Jurafsky, Kathleen Mckeown, Tatsunori Hashimoto, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational LinguisticsDubrovnik, CroatiaAssociation for Computational Linguistics2023</p>
<p>Simple and scalable predictive uncertainty estimation using deep ensembles. Alexander Balaji Lakshminarayanan, Charles Pritzel, Blundell, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Long Beach, CA, USA2017. December 4-9, 2017</p>
<p>Fast and accurate factual inconsistency detection over long documents. Patrick Barrett Martin Lattimer, Xinyuan Chen, Yi Zhang, Yang, 2023</p>
<p>Deduplicating training data makes language models better. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini, 10.18653/v1/2022.acl-long.577Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022a1</p>
<p>A mathematical investigation of hallucination and creativity in gpt models. Minhyeok Lee, Mathematics. 111023202023</p>
<p>Factuality enhanced language models for open-ended text generation. Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, Bryan Catanzaro, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Chain of natural language inference for reducing large language model ungrounded hallucinations. Deren Lei, Yaxi Li, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal, abs/2310.03951ArXiv preprint. 2023</p>
<p>Still no lie detector for language models: Probing empirical and conceptual roadblocks. Daniel A Ba Levinstein, Herrmann, abs/2307.001752023ArXiv preprint</p>
<p>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020a</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. S H Patrick, Ethan Lewis, Aleksandra Perez, Fabio Piktus, Vladimir Petroni, Naman Karpukhin, Heinrich Goyal, Mike Kttler, Wen-Tau Lewis, Tim Yih, Sebastian Rocktschel, Douwe Riedel, Kiela, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. NeurIPS2020b. 2020. 2020. December 6-12, 2020</p>
<p>Large language models with controllable working memory. Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix X Yu, Sanjiv Kumar, 10.18653/v1/2023.findings-acl.112Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023a. July 9-14, 2023</p>
<p>Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, abs/2301.12597ArXiv preprint. 2023b</p>
<p>Halueval: A largescale hallucination evaluation benchmark for large language models. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen, 10.48550/ARXIV.2305.11747CoRR, abs/2305.117472023c</p>
<p>Inferencetime intervention: Eliciting truthful answers from a language model. Kenneth Li, Oam Patel, Fernanda Vigas, Hanspeter Pfister, Martin Wattenberg, abs/2306.03341ArXiv preprint. 2023d</p>
<p>How pre-trained language models capture factual knowledge? a causal-inspired analysis. Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong, Chengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang, Qun Liu, 10.18653/v1/2022.findings-acl.136Findings of the Association for Computational Linguistics: ACL 2022. Dublin, Ireland2022aAssociation for Computational Linguistics</p>
<p>Faithfulness in natural language generation: A systematic survey of analysis, evaluation and optimization methods. Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, Hua Wu, abs/2203.05227ArXiv preprint. 2022b</p>
<p>Contrastive decoding: Open-ended text generation as optimization. Lisa Xiang, Ari Li, Daniel Holtzman, Percy Fried, Jason Liang, Tatsunori Eisner, Luke Hashimoto, Mike Zettlemoyer, Lewis, abs/2210.15097ArXiv preprint. 2022c</p>
<p>Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen, Evaluating object hallucination in large vision-language models. 2023e</p>
<p>Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023f. Textbooks are all you need ii: phi-1.5 technical report. Yuanzhi Li, Sbastien Bubeck, Ronen Eldan, preprint, abs/2309.05463</p>
<p>Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, You Zhang, abs/2303.14070ArXiv preprint. 2023g</p>
<p>Batgpt: A bidirectional autoregessive talker from generative pre-trained transformer. Zuchao Li, Shitou Zhang, Hai Zhao, Yifei Yang, Dongjie Yang, abs/2307.00360ArXiv preprint. 2023h</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>TruthfulQA: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, 10.18653/v1/2022.acl-long.229Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Exposing attention glitches with flip-flop language modeling. Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang, abs/2306.009462023aArXiv preprint</p>
<p>Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v(ision), llava-1.5, and other multi-modality models. Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, Tianyi Zhou, 2023b</p>
<p>Mitigating hallucination in large multi-modal models via robust instruction tuning. Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, Lijuan Wang, 2023c</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, abs/2304.084852023dArXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, ArXiv preprint, abs/2307.031722023e</p>
<p>Evaluating verifiability in generative search engines. Nelson F Liu, Tianyi Zhang, Percy Liang, abs/2304.098482023fArXiv preprint</p>
<p>Gpteval: Nlg evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, abs/2303.16634ArXiv preprint. 2023g</p>
<p>Trustworthy llms: a survey and guideline for evaluating large language models' alignment. Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li, ArXiv preprint, abs/2308.053742023h</p>
<p>Instruction position matters in sequence generation with large language models. Yijin Liu, Xianfeng Zeng, Fandong Meng, Jie Zhou, abs/2308.120972023iArXiv preprint</p>
<p>Negative object presence evaluation (nope) to measure object hallucination in vision-language models. Wenliang Holy Lovenia, Samuel Dai, Ziwei Cahyawijaya, Pascale Ji, Fung, 2023</p>
<p>Evaluation and mitigation of agnosia in multimodal large language models. Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Baochen Sun, Carl Yang, Jie Yang, abs/2309.04041ArXiv preprint. 2023</p>
<p>Univl: A unified video and language pre-training model for multimodal understanding and generation. Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, Ming Zhou, ArXiv preprint, abs/2002.063532020</p>
<p>Zeroresource hallucination prevention for large language models. Junyu Luo, Cao Xiao, Fenglong Ma, abs/2309.026542023aArXiv preprint</p>
<p>Chatgpt as a factual inconsistency evaluator for text summarization. Zheheng Luo, Qianqian Xie, Sophia Ananiadou, 2023b</p>
<p>Faithful chain-ofthought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, abs/2301.13379ArXiv preprint. 2023</p>
<p>Video-chatgpt: Towards detailed video understanding via large vision and language models. Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan, abs/2306.054242023ArXiv preprint</p>
<p>Hallucination: Philosophy and psychology. Fiona Macpherson, Dimitris Platchias, 2013MIT Press</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, 2023ArXiv preprint, abs/2303.17651</p>
<p>Uncertainty estimation in autoregressive structured prediction. Andrey Malinin, J F Mark, Gales, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. Austria2021. May 3-7, 2021OpenReview.net</p>
<p>When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi, 10.18653/v1/2023.acl-long.546Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 2023ACL 2023</p>
<p>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark J F Gales, abs/2303.08896ArXiv preprint. 2023</p>
<p>Suffix arrays: a new method for on-line string searches. Udi Manber, Gene Myers, siam Journal on Computing. 2251993</p>
<p>On faithfulness and factuality in abstractive summarization. Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan Mcdonald, 10.18653/v1/2020.acl-main.173Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Sources of hallucination by large language models on inference tasks. Nick Mckenna, Tianyi Li, Liang Cheng, abs/2305.14552ArXiv preprint. 2023Mohammad Javad Hosseini, Mark Johnson, and Mark Steedman</p>
<p>If beam search is the answer, what was the question?. Clara Meister, Ryan Cotterell, Tim Vieira, 10.18653/v1/2020.emnlp-main.170Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Locating and editing factual associations in GPT. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, NeurIPS. 2022</p>
<p>Massediting memory in a transformer. Kevin Meng, Sen Arnab, Alex J Sharma, Yonatan Andonian, David Belinkov, Bau, ICLR 2023The Eleventh International Conference on Learning Representations. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Prevent the language model from being overconfident in neural machine translation. Mengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua Zhou, Jie Zhou, 10.18653/v1/2021.acl-long.268Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. Ning Miao, Yee Whye Teh, Tom Rainforth, abs/2308.00436ArXiv preprint. 2023</p>
<p>New bing. 2023Microsoft</p>
<p>Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi, 202314251ArXiv preprint, abs/2305</p>
<p>Looking beyond sentencelevel natural language inference for question answering and text summarization. Anshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Lorraine Xiang, Pavan Li, Kartik Kapanipathi, Talamadupula, 10.18653/v1/2021.naacl-main.104Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2021</p>
<p>Fast model editing at scale. Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D Manning, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022a. April 25-29, 2022OpenReview.net</p>
<p>Memorybased model editing at scale. Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, Chelsea Finn, International Conference on Machine Learning, ICML 2022. Baltimore, Maryland, USAPMLR2022b. July 2022162of Proceedings of Machine Learning Research</p>
<p>Relative representations enable zeroshot latent space communication. Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, Emanuele Rodola, abs/2209.154302022ArXiv preprint</p>
<p>Generating benchmarks for factuality evaluation of language models. Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, Yoav Shoham, abs/2307.06908ArXiv preprint. 2023</p>
<p>The creative frontier of generative ai: Managing the noveltyusefulness tradeoff. Anirban Mukherjee, Hannah Chang, abs/2306.036012023ArXiv preprint</p>
<p>Entitylevel factual consistency of abstractive text summarization. Feng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero Nogueira Dos Santos, Henghui Zhu, Dejiao Zhang, Kathleen Mckeown, Bing Xiang, 10.18653/v1/2021.eacl-main.235Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational Linguistics2021</p>
<p>Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. Shashi Narayan, Shay B Cohen, Mirella Lapata, 10.18653/V1/D18-1206Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018. October 31 -November 4. 2018</p>
<p>Nationality bias in text generation. Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, Ting-Hao Huang, Shomir Wilson, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational LinguisticsDubrovnik, CroatiaAssociation for Computational Linguistics2023</p>
<p>Contrastive decoding improves reasoning in large language models. Sean O' Brien, Mike Lewis, abs/2309.091172023ArXiv preprint</p>
<p>Entity cloze by date: What LMs know about unseen entities. Yasumasa Onoe, Michael Zhang, Eunsol Choi, Greg Durrett, 10.18653/v1/2022.findings-naacl.52Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Introducing chatgpt. 2022OpenAI</p>
<p>abs/2303.08774GPT-4 technical report. 2023OpenAIArXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022In NeurIPS</p>
<p>How to catch an ai liar: Lie detection in black-box llms by asking unrelated questions. Lorenzo Pacchiardi, Alex J Chan, Sren Mindermann, Ilan Moscovitz, Alexa Y Pan, Yarin Gal, ArXiv preprint, abs/2309.15840Jan Brauner. 2023Owain Evans</p>
<p>Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. Artidoro Pagnoni, Vidhisha Balachandran, Yulia Tsvetkov, 10.18653/v1/2021.naacl-main.383Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang, Wang , ArXiv preprint, abs/2308.031882023</p>
<p>Totto: A controlled table-to-text generation dataset. P Ankur, Xuezhi Parikh, Sebastian Wang, Manaal Gehrmann, Bhuwan Faruqui, Diyi Dhingra, Dipanjan Yang, Das, 10.18653/V1/2020.EMNLP-MAIN.89Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020. 2020. November 16-20, 2020</p>
<p>Data and its (dis)contents: A survey of dataset development and use in machine learning research. Amandalynne Paullada, Deborah Inioluwa, Emily M Raji, Emily Bender, Alex Denton, Hanna, 10.1016/J.PATTER.2021.100336Patterns. 2111003362021</p>
<p>The refinedweb dataset for falcon LLM: outperforming curated corpora with web data, and web data only. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, ArXiv preprint, abs/2306.011162023</p>
<p>Instruction tuning with gpt-4. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao, abs/2304.032772023ArXiv preprint</p>
<p>Discovering language model behaviors with model-written evaluations. Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron Mckinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noem Mercado, Nova Dassarma, Oliver Rausch, 10.18653/V1/2023.FINDINGS-ACL.847Findings of the Association for Computational Linguistics: ACL 2023. Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, Jared Kaplan, Robin Larson, Sam McCandlish, Scott Johnston; Toronto, Canada2023. July 9-14, 2023Association for Computational Linguistics</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Measuring and narrowing the compositionality gap in language models. Yuval Pinter, Michael Elhadad, abs/2210.03350ArXiv preprint. 2023. 2022Ofir PressEmptying the ocean with a spoon: Should we edit models?</p>
<p>Foodgpt: A large language model in food testing domain with incremental pre-training and knowledge graph prompt. Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan, Yongfeng Huang ; Ningyu, Xiang Zhang, Yunzhi Chen, Shumin Yao, Chuanqi Deng, Fei Tan, Huajun Huang, Chen, abs/2212.09597Shuofei Qiao, Yixin Ou. 2023. 2022ArXiv preprintReasoning with language model prompting: A survey</p>
<p>Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, Jie Zhou, Webcpm: Interactive web search for chinese long-form question answering. 2023</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 21672020</p>
<p>Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, abs/2302.00083ArXiv preprint. </p>
<p>Sequence level training with recurrent neural networks. Aurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, 4th International Conference on Learning Representations, ICLR 2016. Conference Track Proceedings. San Juan, Puerto Rico2016. May 2-4, 2016</p>
<p>Measuring attribution in natural language generation models. Vitaly Hannah Rashkin, Matthew Nikolaev, Lora Lamm, Michael Aroyo, Dipanjan Collins, Slav Das, Gaurav Petrov, Iulia Singh Tomar, David Turc, Reitter, Computational Linguistics. </p>
<p>A survey of hallucination in large foundation models. Vipula Rawte, P Amit, Amitava Sheth, Das, abs/2309.059222023ArXiv preprint</p>
<p>Investigating the factual knowledge boundary of large language models with retrieval augmentation. Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hua Hao Tian, Ji-Rong Wu, Haifeng Wen, Wang, abs/2307.11019ArXiv preprint. 2023</p>
<p>Street: A multi-task structured reasoning and explanation benchmark. Danilo Ribeiro, Shen Wang, Xiaofei Ma, Henry Zhu, Rui Dong, Deguang Kong, Juliette Burger, Anjelica Ramos, William Wang, Zhiheng Huang, abs/2302.06729ArXiv preprint. 2023</p>
<p>Modulating sycophancy in an rlhf model via activation steering. Nina Rimsky, 2023a</p>
<p>Reducing sycophancy and improving honesty via activation steering. Nina Rimsky, 2023b</p>
<p>How much knowledge can you pack into the parameters of a language model?. Adam Roberts, Colin Raffel, Noam Shazeer, 10.18653/v1/2020.emnlp-main.437Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Highresolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bjrn Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Rome was built in 1776: A case study on factual correctness in knowledge-grounded response generation. Sashank Santhanam, Behnam Hedayatnia, Spandana Gella, Aishwarya Padmakumar, Seokhwan Kim, Yang Liu, Dilek Hakkani-Tur, ArXiv preprint, abs/2110.054562021</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, abs/2206.05802ArXiv preprint. Jan Leike. 2022</p>
<p>Reinforcement learning from human feedback. John Schulman, 2023Progress and challenges</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, abs/1707.063472017ArXiv preprint</p>
<p>QuestEval: Summarization asks for fact-based evaluation. Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, Patrick Gallinari, 10.18653/v1/2021.emnlp-main.529Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, abs/2305.15294ArXiv preprint. 2023</p>
<p>Towards understanding sycophancy in language models. Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, R Samuel, Newton Bowman, Esin Cheng, Zac Durmus, Scott R Hatfield-Dodds, Shauna Johnston, Timothy Kravec, Sam Maxwell, Kamal Mccandlish, Oliver Ndousse, Nicholas Rausch, Da Schiefer, Miranda Yan, Ethan Zhang, Perez, abs/2310.13548ArXiv preprint. 2023</p>
<p>Minimum risk training for neural machine translation. Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, Yang Liu, 10.18653/v1/P16-1159Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics20161</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schrli, Denny Zhou, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023a. 23-29 July 2023202of Proceedings of Machine Learning Research</p>
<p>Trusting your evidence: Hallucinate less with context-aware decoding. Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, Scott , Wen-Tau Yih, abs/2305.14739ArXiv preprint. 2023b</p>
<p>Incontext pretraining: Language modeling beyond document boundaries. Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Xi Victoria Lin, Noah A Smith, Luke Zettlemoyer, Scott Yih, Mike Lewis, abs/2310.10638ArXiv preprint. 2023c</p>
<p>Retrieval augmentation reduces hallucination in conversation. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston, 10.18653/v1/2021.findings-emnlp.320Findings of the Association for Computational Linguistics: EMNLP 2021. Punta CanaDominican Republic. Association for Computational Linguistics2021</p>
<p>Towards expert-level medical question answering with large language models. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Andrew Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska ; Nenad, Yun Tomasev, Renee Liu, Christopher Wong, S Sara Semturs, Joelle K Mahdavi, Dale R Barral, Gregory S Webster, Yossi Corrado, Shekoofeh Matias, Alan Azizi, Vivek Karthikesalingam, Natarajan, abs/2305.096172023Blaise Agera y ArcasArXiv preprint</p>
<p>Editable neural networks. Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, Artem Babenko, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>The curious case of hallucinatory unanswerablity: Finding truths in the hidden states of over-confident large language models. Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido Dagan, Shauli Ravfogel, ArXiv preprint, abs/2310.118772023</p>
<p>On NMT search errors and model errors: Cat got your tongue?. Felix Stahlberg, Bill Byrne, 10.18653/v1/D19-1331Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. Kaya Stechly, Matthew Marquez, Subbarao Kambhampati, abs/2310.12397ArXiv preprint. 2023</p>
<p>ASQA: Factoid questions meet long-form answers. Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, Ming-Wei Chang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022Association for Computational Linguistics</p>
<p>ing to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. NeurIPS2020. 2020. 2020. December 6-12, 2020</p>
<p>Extracting latent steering vectors from pretrained language models. Nishant Subramani, Nivedita Suresh, Matthew Peters, 10.18653/v1/2022.findings-acl.48Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Understanding factual errors in summarization: Errors, summarizers, datasets, error detectors. Ilya Sutskever ; Liyan, Tanya Tang, Alexander R Goyal, Philippe Fabbri, Jiacheng Laban, Semih Xu, Wojciech Yavuz, Justin F Kryciski, Greg Rousseau, Durrett, 2023. 2022An observation on generalization. ArXiv preprint, abs/2205.12854</p>
<p>Towards causalgpt: A multi-agent approach for faithful knowledge reasoning via promoting causal consistency in llms. Ziyi Tang, Ruilin Wang, Weixing Chen, Keze Wang, Yang Liu, Tianshui Chen, Liang Lin, 10.48550/ARXIV.2308.11914CoRR, abs/2308.119142023</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, preprint, abs/2211.09085Galactica: A large language model for science. 2022</p>
<p>Sticking to the facts: Confident decoding for faithful data-to-text generation. Ran Tian, Shashi Narayan, Thibault Sellam, Ankur P Parikh, abs/1910.08684ArXiv preprint. 2019</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurlien Rodriguezand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. ArXiv preprint, abs/2307.09288</p>
<p>Interleaving retrieval with chain-of-thought reasoning for knowledgeintensive multi-step questions. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada2023. July 9-14, 20231ACL 2023. Association for Computational Linguistics</p>
<p>Med-halt: Medical domain hallucination test for large language models. Logesh Kumar, Umapathi , abs/2307.153432023ArXiv preprintAnkit Pal, and Malaikannan Sankarasubbu</p>
<p>Can large language models really improve by self-critiquing their own plans?. Karthik Valmeekam, Matthew Marquez, Subbarao Kambhampati, ArXiv preprint, abs/2310.081182023</p>
<p>Mutual information alleviates hallucinations in abstractive summarization. Liam Van Der Poel, Ryan Cotterell, Clara Meister, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, Dong Yu, abs/2307.03987ArXiv preprint. 2023</p>
<p>Fairpy: A toolkit for evaluation of social biases and their mitigation in large language models. Hrishikesh Viswanath, Tianyi Zhang, abs/2302.055082023ArXiv preprint</p>
<p>Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov, 10.18653/v1/P19-1580Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Freshllms: Refreshing large language models with search engine augmentation. Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, Thang Luong, 2023</p>
<p>Faithfulness-aware decoding strategies for abstractive summarization. David Wan, Mengwen Liu, Kathleen Mckeown, Markus Dreyer, Mohit Bansal, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European ChapterDubrovnik, CroatiaAssociation for Computational Linguistics2023</p>
<p>Asking and answering questions to evaluate the factual consistency of summaries. Alex Wang, Kyunghyun Cho, Mike Lewis, 10.18653/v1/2020.acl-main.450Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020a</p>
<p>Chinesefacteval: A factuality benchmark for chinese llms. Binjie Wang, Ethan Chern, Pengfei Liu, 2023a</p>
<p>Progressive translation: Improving domain robustness of neural machine translation with intermediate sequences. Chaojun Wang, Yang Liu, Wai Lam, abs/2305.091542023bArXiv preprint</p>
<p>On exposure bias, hallucination and domain shift in neural machine translation. Chaojun Wang, Rico Sennrich, 10.18653/v1/2020.acl-main.326Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Survey on factuality in large language models: Knowledge, retrieval and domainspecificity. Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Jiayang Cheng, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, Yue Zhang, ArXiv preprint, abs/2310.075212023c</p>
<p>Is chatgpt a good nlg evaluator? a preliminary study. Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, abs/2303.04048ArXiv preprint. 2023d</p>
<p>Cross-lingual knowledge editing in large language models. Jiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao, Jiarong Xu, 2023e</p>
<p>Scott: Self-consistent chain-of-thought distillation. Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, Xiang Ren, abs/2305.01879ArXiv preprint. 2023f</p>
<p>Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, and Changyou Chen. 2020b. Towards faithful neural table-to-text generation with content-matching constraints. Weiqi Wang, Tianqing Fang, Wenxuan Ding, Baixuan Xu, Xin Liu, Yangqiu Song, Antoine Bosselut, 10.18653/v1/2020.acl-main.101CoRR, abs/2305.14869Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2023gCAR: conceptualization-augmented reasoner for zero-shot commonsense question answering</p>
<p>Simvlm: Simple visual language model pretraining with weak supervision. Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, Yuan Cao, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022. April 25-29, 2022OpenReview.net</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Simple synthetic data reduces sycophancy in large language models. Jerry W Wei, Da Huang, Yifeng Lu, Denny Zhou, V Quoc, Le, abs/2308.039582023ArXiv preprint</p>
<p>Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, abs/2112.04359Geoffrey Irving, and Iason Gabriel. 2021. Ethical and social risks of harm from language models. ArXiv preprint</p>
<p>Neural text generation with unlikelihood training. Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models. Yilin Wen, Zifeng Wang, Jimeng Sun, abs/2308.097292023ArXiv preprint</p>
<p>Eva-kellm: A new benchmark for evaluating knowledge editing of llms. Suhang Wu, Minlong Peng, Yue Chen, Jinsong Su, Mingming Sun, abs/2308.099542023ArXiv preprint</p>
<p>On hallucination and predictive uncertainty in conditional language generation. Yijun Xiao, William Yang, Wang , 10.18653/v1/2021.eacl-main.236Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational Linguistics2021</p>
<p>Decomposition enhances reasoning via self-evaluation guided decoding. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, Qizhe Xie, abs/2305.00633ArXiv preprint. 2023</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, ArXiv preprint. 2023</p>
<p>Recomp: Improving retrieval-augmented lms with compression and selective augmentation. Fangyuan Xu, Weijia Shi, Eunsol Choi, abs/2310.044082023ArXiv preprint</p>
<p>Understanding neural abstractive summarization models via uncertainty. Jiacheng Xu, Shrey Desai, Greg Durrett, 10.18653/v1/2020.emnlp-main.508Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>A new benchmark and reverse validation method for passage-level hallucination detection. Shiping Yang, Renliang Sun, Xiaojun Wan, abs/2310.064982023ArXiv preprint</p>
<p>Breaking the softmax bottleneck: A high-rank RNN language model. Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, William W Cohen, ICLR 20186th International Conference on Learning Representations. Conference Track Proceedings. OpenReview.net. Vancouver, BC, Canada2018a. April 30 -May 3, 2018</p>
<p>Mixtape: Breaking the softmax bottleneck efficiently. Zhilin Yang, Thang Luong, Ruslan Salakhutdinov, V Quoc, Le, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, Canada2019. 2019. 2019. December 8-14, 2019</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018b</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018c</p>
<p>LLM lies: Hallucinations are not bugs, but features as adversarial examples. Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, Li Yuan, abs/2310.01469ArXiv preprint. 2023a</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, abs/2305.10601ArXiv preprint. 2023b</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, abs/2210.03629ArXiv preprint. 2022</p>
<p>Editing large language models: Problems, methods, and opportunities. Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang, ArXiv preprint, abs/2305.131722023c</p>
<p>Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. 2023. Woodpecker: Hallucination correction for multimodal large language models. </p>
<p>Legal prompting: Teaching a language model to think like a lawyer. Fangyi Yu, Lee Quartey, Frank Schilder, abs/2212.013262022ArXiv preprint</p>
<p>Nature language reasoning, a survey. Fei Yu, Hongbo Zhang, Benyou Wang, abs/2303.147252023aArXiv preprint</p>
<p>Hybrid reasoning network for video-based commonsense captioning. Weijiang Yu, Jian Liang, Lei Ji, Lu Li, Yuejian Fang, Nong Xiao, Nan Duan, Proceedings of the 29th ACM international conference on multimedia. the 29th ACM international conference on multimedia2021</p>
<p>Knowledge-aware global reasoning for situation recognition. Weijiang Yu, Haofan Wang, Guohao Li, Nong Xiao, Bernard Ghanem, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2023b</p>
<p>Generate rather than retrieve: Large language models are strong context generators. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, Meng Jiang, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023c. May 1-5, 2023</p>
<p>Improving language models via plug-and-play retrieval feedback. Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, Ashish Sabharwal, abs/2305.140022023dArXiv preprint</p>
<p>Automatic evaluation of attribution by large language models. Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, Huan Sun, abs/2305.063112023ArXiv preprint</p>
<p>From recognition to cognition: Visual commonsense reasoning. Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi, 10.1109/CVPR.2019.00688IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAComputer Vision Foundation / IEEE2019. June 16-20, 2019</p>
<p>Trading off diversity and quality in natural language generation. Hugh Zhang, Daniel Duckworth, Daphne Ippolito, Arvind Neelakantan, Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval). the Workshop on Human Evaluation of NLP Systems (HumEval)Online. Association for Computational Linguistics2021</p>
<p>Attention with sparsity regularization for neural machine translation and summarization. Jiajun Zhang, Yang Zhao, Haoran Li, Chengqing Zong, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 2732018</p>
<p>Sac 3 : Reliable hallucination detection in black-box language models via semantic-aware cross-check consistency. Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A Malin, Sricharan Kumar, 2023a</p>
<p>PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter J Liu, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningPMLR2020. 13-18 July 20202020of Proceedings of Machine Learning Research</p>
<p>Adding conditional control to text-to-image diffusion models. Lvmin Zhang, Anyi Rao, Maneesh Agrawala, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023b</p>
<p>How language model hallucinations can snowball. Muru Zhang, Ofir Press, William Merrill, Alisa Liu, Noah A Smith, abs/2305.135342023cArXiv preprint</p>
<p>Instruction tuning for large language models: A survey. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, preprint, abs/2308.107922023d</p>
<p>Mitigating language model hallucination with interactive question-knowledge alignment. Shuo Zhang, Liangming Pan, Junzhou Zhao, William Yang, Wang , abs/2305.13669ArXiv preprint. 2023e</p>
<p>OPT: open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, ArXiv preprint, abs/2205.010682022</p>
<p>Benchmarking large language models for news summarization. Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen Mckeown, Tatsunori B Hashimoto, abs/2301.13848ArXiv preprint. 2023f</p>
<p>Siren's song in the AI ocean: A survey on hallucination in large language models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi, ArXiv preprint, abs/2309.012192023g</p>
<p>Verify-and-edit: A knowledge-enhanced chain-of-thought framework. Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, Lidong Bing, 10.18653/v1/2023.acl-long.320Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a. July 9-14, 20231ACL 2023</p>
<p>. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, et al. 2023b. A survey of large language models. ArXiv preprint, abs/2303.18223</p>
<p>Knowing what llms do not know: A simple yet effective self-detection method. Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun Ren, Dawei Yin, abs/2310.17918ArXiv preprint. 2023c</p>
<p>Why does chatgpt fall short in answering questions faithfully?. Shen Zheng, Jie Huang, Kevin Chen, -Chuan Chang, ArXiv preprint, abs/2304.105132023</p>
<p>Xiaocheng Feng, and Bing Qin. 2023a. Stoa-vlp: Spatial-temporal modeling of object and action for video-language pre-training. Weihong Zhong, Mao Zheng, Duyu Tang, Xuan Luo, Heng Gong, 10.1609/aaai.v37i3.25483Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence. the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence37</p>
<p>Factual probing is [MASK]: Learning vs. learning to recall. Zexuan Zhong, Dan Friedman, Danqi Chen, 10.18653/v1/2021.naacl-main.398Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Mquake: Assessing knowledge editing in language models via multi-hop questions. Zexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, Danqi Chen, abs/2305.14795ArXiv preprint. 2023b</p>
<p>Lima: Less is more for alignment. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, 2023aArXiv preprint, abs/2305.11206</p>
<p>Detecting hallucinated content in conditional neural sequence generation. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzmn, Luke Zettlemoyer, Marjan Ghazvininejad, 10.18653/v1/2021.findings-acl.120Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Analyzing and mitigating object hallucination in large vision-language models. Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, Huaxiu Yao, 2023b</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, abs/2304.10592ArXiv preprint. 2023a</p>
<p>Multilingual machine translation with large language models: Empirical results and analysis. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, Shujian Huang, ArXiv preprint, abs/2304.046752023b</p>
<p>Fool your (vision and) language model with embarrassingly simple permutations. Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, Timothy Hospedales, abs/2310.016512023ArXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
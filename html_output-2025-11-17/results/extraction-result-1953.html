<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1953 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1953</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1953</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-279154540</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.04217v2.pdf" target="_blank">OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis</a></p>
                <p><strong>Paper Abstract:</strong> The rapid progress of navigation, manipulation, and vision models has made mobile manipulators capable in many specialized tasks. However, the open-world mobile manipulation (OWMM) task remains a challenge due to the need for generalization to open-ended instructions and environments, as well as the systematic complexity to integrate high-level decision making with low-level robot control based on both global scene understanding and current agent state. To address this complexity, we propose a novel multi-modal agent architecture that maintains multi-view scene frames and agent states for decision-making and controls the robot by function calling. A second challenge is the hallucination from domain shift. To enhance the agent performance, we further introduce an agentic data synthesis pipeline for the OWMM task to adapt the VLM model to our task domain with instruction fine-tuning. We highlight our fine-tuned OWMM-VLM as the first dedicated foundation model for mobile manipulators with global scene understanding, robot state tracking, and multi-modal action generation in a unified model. Through experiments, we demonstrate that our model achieves SOTA performance compared to other foundation models including GPT-4o and strong zero-shot generalization in real world. The project page is at https://github.com/HHYHRHY/OWMM-Agent</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1953.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1953.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OWMM-VLM-38B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open World Mobile Manipulation Vision-Language Model (38B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 38B-parameter vision-language model fine-tuned for open-world mobile manipulation that performs multi-turn, multi-image reasoning, state tracking, and generates executable multi-modal high-level actions (posed-image retrieval, navigate-to-point, pick, place) output as JSON with bounding-box grounded targets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OWMM-VLM-38B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Built by fine-tuning an InternVL-2.5 based multimodal stack (frozen ViT + 2-layer projection MLP + LLM), the model ingests instruction text, a posed-frame image graph, and current egocentric RGB+D and history; it autoregressively generates chain-of-thought, robot history summarization, high-level action type and region grounding (bounding boxes) in JSON. High-level actions are linked to classical path and motion planners; grounding is produced as bounding boxes whose centers are reverse-projected into 3D for planner coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>InternViT (frozen ViT as in InternVL-2.5 family; projection MLP maps visual features into LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pre-trained InternVL-2.5 backbone (paper does not specify the upstream dataset details in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>The LLM generates bounding-box region outputs (image-level region grounding) conditioned on projected ViT features; bounding-box centers are converted into target points for planners via 2D-to-3D reverse projection using depth; multi-image retrieval is used to pick posed frames containing targets.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>multi-level: multi-image/scene-level retrieval + region-level (bounding boxes) in egocentric images; uses depth-derived point clouds for planner input</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D bounding boxes (region), egocentric depth maps, point clouds from depth for planners; uses 2D-to-3D reverse projection to obtain 3D coordinates</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>mobile manipulation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-World Mobile Manipulation (OWMM) on Habitat/Fetch setup</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>photorealistic simulation (Habitat) with additional real-world egocentric evaluation on a Fetch robot (lab environment)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>multiple: single-step metrics (egocentric decision-making success rate, image retrieval rate, affordance grounding score), episodic task success rate (object placed within distance thresholds), and real-world action generation accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Single-step: Decision-making 97.85%; Image retrieval 87.54%; Affordance grounding score 0.97 (object), 0.94 (receptacle), 0.88 (navigation). Episodic (strict 0.85m): Full-task success 21.90% (lenient 1.7m: 51.52%). Real-world single-step action accuracy: 9/10 = 90%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablation replacing bounding boxes with direct point outputs reduced affordance grounding (objects 0.9251 -> 0.6542, receptacles 0.9060 -> 0.6479); removing reasoning and summarization reduced Image Retrieval (0.7904 -> 0.6586) and Ego-centric Decision-making (0.9672 -> 0.9049).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Compared to generalist GPT-4o baseline, OWMM-VLM-38B improved decision-making (+49.32 percentage points, 97.85% vs 48.53%) and affordance grounding (object score 0.97 vs 0.56), and eliminated dead-loop failures (0/308 vs 195/308 for GPT-4o+PIVOT).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Two model scales compared: OWMM-VLM-38B outperforms OWMM-VLM-8B (Decision 97.85% vs 96.72%; Image retrieval 87.54% vs 79.04%; affordance grounding higher across subtasks). InternVL-2.5-8B baseline performs poorly without OWMM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors identify 'multimodal context length' (ability to process many posed frames) and limited capacity of embedding models as bottlenecks; rare grounding tasks (e.g., detecting non-blocked navigable areas) and state-tracking failures are also highlighted as perception/grounding weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Documented failure modes include hallucinations from domain shift, dead loops caused by incorrect image-retrieval vs. navigation decisions (highly frequent for GPT-4o baselines: 195/308 and 184/308 dead loops), cross-embodiment failures when deploying on robots with different kinematics, and limitations for complex dexterous end-effectors; ablations quantify performance drops when grounding format or reasoning components are removed (see performance_without_grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Handled by a simulation-based agentic data synthesis pipeline and instruction fine-tuning: synthetic multi-turn multi-image episodes teach state tracking and affordance grounding; results show strong zero-shot transfer (real-world action generation success 27/30 = 90% reported in abstract; Table 4 reports 9/10 = 90% for OWMM-VLM-38B).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Test set fully contains unseen objects (split ensured unseen objects); OWMM-VLM-38B single-step and episodic metrics reported above are evaluated on these unseen objects (e.g., episodic full-task success 21.90% strict / 51.52% lenient).</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Visual encoder (ViT) is frozen during OWMM fine-tuning; only projection MLP and LLM are trained. The paper does not report a direct empirical comparison between frozen vs fully fine-tuned visual encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>The authors study instruction-finetuning dataset scale: performance improves log-linearly with dataset size up to 152k samples (e.g., decision-making from 17.52% at 0k to 96.72% at 152k for the 8B variant), with diminishing returns near the largest size.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Visual features are projected by a 2-layer MLP into the LLM input space (early multimodal fusion); the LLM autoregressively generates JSON outputs. (ViT parameters frozen, projection MLP + LLM trained.)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Fine-tuning required up to 152k synthetic image-text/action samples (trained for 1 epoch). Smaller datasets produced large gains: 15k samples already raise decision-making to 73.27% (from 17.52% at 0k), indicating strong sample efficiency of targeted instruction fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Effective embodied grounding requires (1) integrated multi-image/scene retrieval plus egocentric decision-making, (2) region-level outputs (bounding boxes) rather than raw points for stable affordance predictions, (3) explicit reasoning and history summarization to maintain context, and (4) large-scale, instruction-driven synthetic episodes to mitigate domain-shift hallucinations; multi-image context length and model capacity remain practical bottlenecks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1953.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1953.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboPoint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboPoint: A vision-language model for spatial affordance prediction for robotics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized single-image grounding model focused on predicting actionable keypoints/affordance points for robotic manipulation and navigation, used here as a baseline grounding module.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robopoint: A vision-language model for spatial affordance prediction for robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboPoint</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Specialized single-image VLM that predicts normalized image coordinates (points) or lists of candidate spots for robot actions; in this paper it's used as an external grounding module (paired with GPT-4o for high-level decisions).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Predicts normalized pixel coordinates (point-level affordance) from a single RGB image and a task prompt; used to convert high-level textual actions into concrete image points.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>point-level (single-image)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D points in image normalized coordinates</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>mobile manipulation grounding subtask (affordance prediction for pick/place/navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>single-image affordance grounding within OWMM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>egocentric RGB images (single-frame)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>affordance grounding scores (single-image evaluation) and integrated system episodic metrics when combined with GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Single-image grounding (object 0.91 ±0.33, receptacle 0.83 ±0.11, navigation 0.72 ±0.11). When paired with GPT-4o: overall single-step affordance scores ~0.64 (object), 0.38 (receptacle), 0.06 (navigation) in the multimodal pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>As a specialist, RoboPoint achieves high single-image grounding scores (object 0.91) comparable to OWMM-VLM for single-image inputs, but when combined with a weaker decision-making module (GPT-4o) overall episodic performance is poor due to decision errors.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Designed for single-image QA; the paper notes that single-image specialists like RoboPoint perform well at affordance prediction but are limited by not handling multi-image/global scene reasoning, contributing to failure modes when used alone for OWMM.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>High single-image grounding quality does not prevent episodic failures when coupled with poor high-level decision-making: GPT-4o+Robopoint had many dead-loop failures (184/308) and near-zero full-task success in episodic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Not detailed in this paper; RoboPoint used as-is (single-image), and performance is reported on simulated and reformulated prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Used as an external grounding module: GPT-4o provides action + image, RoboPoint predicts points; this is a late-fusion pipeline (decision then grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Specialized single-image grounding models can produce strong point-affordance predictions, but integrating them into an embodied multi-image decision pipeline requires a reliable high-level decision model; otherwise grounding strengths are not sufficient to solve episodic OWMM tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1953.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1953.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PIVOT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PIVOT: Iterative Visual Prompting for Actionable Knowledge in VLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative visual prompting method that elicits actionable affordance points from VLMs by sampling and optimizing candidate points; used here as another specialized grounding baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pivot: Iterative visual prompting elicits actionable knowledge for vlms.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PIVOT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Operates on a single RGB (and optionally depth) image with iterative sampling (n_samples_init, n_samples_opt, n_iters) to propose candidate actionable points; in the paper it is combined with GPT-4o as the decision module for grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Iterative point sampling in image space guided by VLM prompts and optimization; returns candidate pixel coordinates for affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>point-level (single-image iterative sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D pixel coordinates; in episodic eval depth is provided to PIVOT for planner use</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>affordance grounding for mobile manipulation/navigation substeps</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>single-image grounding within OWMM evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>egocentric RGB (and optionally depth) single frames</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>single-image affordance grounding scores and integrated pipeline episodic metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Single-image grounding (object 0.86 ±0.13, receptacle 0.84 ±0.12, navigation 0.74 ±0.13). In GPT-4o+PIVOT combined pipeline: affordance scores ~0.67 (object), 0.45 (receptacle), 0.05 (navigation); episodic full-task success ~0.33% (strict).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>PIVOT provides strong single-image grounding (close to RoboPoint), but when used with GPT-4o decision-making the episodic outcomes are weak, demonstrating that grounding alone does not salvage high-level planning failures.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Like RoboPoint, PIVOT is limited by single-image input and does not address multi-image scene-level retrieval/decision bottlenecks; iterative sampling parameters and single-frame focus are constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>When combined with GPT-4o, PIVOT pipeline exhibited a large number of dead loops (195/308) and near-zero episodic success, indicating cascading failure from incorrect high-level action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Late fusion: high-level action from GPT-4o plus current image and history passed to PIVOT for point generation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Iterative visual prompting yields high-quality single-image affordance points, but single-image grounding must be integrated with robust multi-image reasoning and state-tracking to succeed on long-horizon embodied tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1953.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1953.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (multimodal/open) used as a generalist vision-language model baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalist multimodal foundation model used as a high-level decision-making baseline, combined with specialist grounding modules in ablations and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4o system card.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generalist VLM used as an end-to-end decision and reasoning baseline; in some baselines GPT-4o is used for decision/image retrieval and then specialist grounding models (RoboPoint/PIVOT) are called for affordance prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Generalist multimodal reasoning; when used alone, GPT-4o is expected to generate region/action outputs but the paper shows it performs poorly at affordance grounding; when combined with specialist modules it delegates grounding to them (late-fusion).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>global image / language-conditioned outputs (not specialized to region-level affordances in this evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit (GPT-4o often outputs image-retrieval actions and textual descriptions rather than precise region coordinates in this paper's evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>used for OWMM single-step and episodic baselines (decision-making + image retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>OWMM single-step and episodic baselines</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>simulation egocentric frames and real-world egocentric frames in evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>single-step and episodic metrics (decision-making %, image retrieval %, affordance grounding scores, episodic success)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Single-step: Decision-making 48.53%; Image retrieval 46.46%; affordance grounding object 0.56, receptacle 0.35, navigation 0.07. Episodic full-task success (strict): 0.33% (GPT-4o+PIVOT) / 0.33% (GPT-4o+RoboPoint); many dead loops (195/308 and 184/308).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Using OWMM-VLM instead of GPT-4o yields large improvements in both decision-making and grounding (see OWMM-VLM numbers); pairing GPT-4o with specialized grounders yields better grounding scores but does not remedy decision/replanning failures.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>GPT-4o often outputs image-retrieval when navigation is required, causing cyclic dead loops; demonstrates hallucination and poor affordance grounding for embodied tasks without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>High frequency of dead loops (up to ~63% of evaluated episodes for combinations), poor affordance grounding for navigation targets, and inappropriate action choices leading to failed episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>No task-specific simulation fine-tuning in baseline; performs poorly on affordance grounding and episodic tasks until specialist grounders are attached, showing sensitivity to domain/task mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>When used as a decision module with specialist grounders, the pipeline is late-fusion (GPT-4o decision -> pass context to grounding model).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Generalist VLMs without embodiment-specific fine-tuning struggle at precise affordance grounding and can produce decision errors (hallucinated/redundant image retrieval) that cascade into episodic failures; specialist grounders help affordance but cannot compensate for weak decision-making.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1953.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1953.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternVL-2.5-8B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVL-2.5 8B (base multimodal foundation model used for OWMM fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The pre-trained foundation model (InternVL-2.5 family) used as the base for OWMM-VLM fine-tuning; comprises ViT visual encoder and a LLM; serves as both a baseline and starting point for OWMM-VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Internlm2 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternVL-2.5-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained multimodal model used as the backbone for OWMM-VLM-8B (InternViT-300M + InternLM-2.5-7B). In experiments the base ViT is frozen and only projection MLP + LLM are fine-tuned for OWMM.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>InternViT-300M (part of InternVL family)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pre-trained InternVL-2.5 family (paper does not provide exact pretraining dataset details here)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>As a baseline, the pre-trained model without OWMM fine-tuning performs poorly at affordance grounding; grounding is not specialized for OWMM tasks until instruction-fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>global/multi-image features in pretraining; not sufficient for OWMM region-level affordance without fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit image-level features; not explicitly producing planner-ready bounding boxes before OWMM fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>used as baseline for single-step OWMM capabilities and as the base for fine-tuned models</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>OWMM single-step evaluation when used with minimal examples (0k) or limited in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>pretraining domain unspecified in paper; evaluation on Habitat simulation egocentric frames</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>single-step metrics (decision-making %, image retrieval %, affordance grounding scores)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>InternVL-2.5-8B baseline (no OWMM fine-tuning): Decision-making 17.52%; Image retrieval 1.27%; affordance grounding object 0.05, receptacle 0.18, navigation 0.14.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Instruction-fine-tuning on synthetic OWMM episodes dramatically improves grounding and decision-making compared to the vanilla InternVL-2.5-8B baseline (see OWMM-VLM results), demonstrating that specialized fine-tuning is key.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Comparison primarily between base InternVL-2.5 (vanilla) and OWMM-fine-tuned variants; finetuned models (OWMM-VLM) greatly outperform the base InternVL-2.5-8B on all grounding and decision metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Vanilla InternVL model lacks task-specific affordance and multi-image reasoning capabilities, yielding near-zero image retrieval and affordance grounding scores without OWMM-specific data.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Fails at affordance grounding and image retrieval when not fine-tuned for OWMM; unable to produce planner-ready regions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>The paper shows that OWMM instruction fine-tuning of the InternVL backbone addresses domain shift for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>OWMM fine-tuning freezes ViT and adjusts projection and LLM; no direct comparison to full visual fine-tuning was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Pretrained model uses ViT -> projection -> LLM fusion; OWMM fine-tuning follows same architecture but with task-specific supervised examples.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Base model with no OWMM data (0k) performs poorly; adding 15k synthetic OWMM samples yields large improvements (decision-making to 73.27%), indicating task-specific fine-tuning is sample-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Pretrained multimodal backbones are insufficient for precise embodied affordance grounding and multi-image decision-making without targeted instruction-fine-tuning on synthetic, multi-turn episodes; fine-tuning teaches state-tracking and embodied priors needed for OWMM.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Robopoint: A vision-language model for spatial affordance prediction for robotics. <em>(Rating: 2)</em></li>
                <li>Pivot: Iterative visual prompting elicits actionable knowledge for vlms. <em>(Rating: 2)</em></li>
                <li>Internlm2 technical report. <em>(Rating: 2)</em></li>
                <li>Closed-loop open-vocabulary mobile manipulation with gpt-4v. <em>(Rating: 2)</em></li>
                <li>Openvla: An open-source vision-language-action model. <em>(Rating: 1)</em></li>
                <li>RoboPoint and PIVOT: (see respective papers above for grounding-focused evaluations). <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1953",
    "paper_id": "paper-279154540",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "OWMM-VLM-38B",
            "name_full": "Open World Mobile Manipulation Vision-Language Model (38B)",
            "brief_description": "A 38B-parameter vision-language model fine-tuned for open-world mobile manipulation that performs multi-turn, multi-image reasoning, state tracking, and generates executable multi-modal high-level actions (posed-image retrieval, navigate-to-point, pick, place) output as JSON with bounding-box grounded targets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OWMM-VLM-38B",
            "model_description": "Built by fine-tuning an InternVL-2.5 based multimodal stack (frozen ViT + 2-layer projection MLP + LLM), the model ingests instruction text, a posed-frame image graph, and current egocentric RGB+D and history; it autoregressively generates chain-of-thought, robot history summarization, high-level action type and region grounding (bounding boxes) in JSON. High-level actions are linked to classical path and motion planners; grounding is produced as bounding boxes whose centers are reverse-projected into 3D for planner coordinates.",
            "visual_encoder_type": "InternViT (frozen ViT as in InternVL-2.5 family; projection MLP maps visual features into LLM)",
            "visual_encoder_pretraining": "Pre-trained InternVL-2.5 backbone (paper does not specify the upstream dataset details in this work)",
            "grounding_mechanism": "The LLM generates bounding-box region outputs (image-level region grounding) conditioned on projected ViT features; bounding-box centers are converted into target points for planners via 2D-to-3D reverse projection using depth; multi-image retrieval is used to pick posed frames containing targets.",
            "representation_level": "multi-level: multi-image/scene-level retrieval + region-level (bounding boxes) in egocentric images; uses depth-derived point clouds for planner input",
            "spatial_representation": "2D bounding boxes (region), egocentric depth maps, point clouds from depth for planners; uses 2D-to-3D reverse projection to obtain 3D coordinates",
            "embodied_task_type": "mobile manipulation / instruction following",
            "embodied_task_name": "Open-World Mobile Manipulation (OWMM) on Habitat/Fetch setup",
            "visual_domain": "photorealistic simulation (Habitat) with additional real-world egocentric evaluation on a Fetch robot (lab environment)",
            "performance_metric": "multiple: single-step metrics (egocentric decision-making success rate, image retrieval rate, affordance grounding score), episodic task success rate (object placed within distance thresholds), and real-world action generation accuracy",
            "performance_value": "Single-step: Decision-making 97.85%; Image retrieval 87.54%; Affordance grounding score 0.97 (object), 0.94 (receptacle), 0.88 (navigation). Episodic (strict 0.85m): Full-task success 21.90% (lenient 1.7m: 51.52%). Real-world single-step action accuracy: 9/10 = 90%.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablation replacing bounding boxes with direct point outputs reduced affordance grounding (objects 0.9251 -&gt; 0.6542, receptacles 0.9060 -&gt; 0.6479); removing reasoning and summarization reduced Image Retrieval (0.7904 -&gt; 0.6586) and Ego-centric Decision-making (0.9672 -&gt; 0.9049).",
            "grounding_improvement": "Compared to generalist GPT-4o baseline, OWMM-VLM-38B improved decision-making (+49.32 percentage points, 97.85% vs 48.53%) and affordance grounding (object score 0.97 vs 0.56), and eliminated dead-loop failures (0/308 vs 195/308 for GPT-4o+PIVOT).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Two model scales compared: OWMM-VLM-38B outperforms OWMM-VLM-8B (Decision 97.85% vs 96.72%; Image retrieval 87.54% vs 79.04%; affordance grounding higher across subtasks). InternVL-2.5-8B baseline performs poorly without OWMM fine-tuning.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors identify 'multimodal context length' (ability to process many posed frames) and limited capacity of embedding models as bottlenecks; rare grounding tasks (e.g., detecting non-blocked navigable areas) and state-tracking failures are also highlighted as perception/grounding weaknesses.",
            "failure_mode_analysis": "Documented failure modes include hallucinations from domain shift, dead loops caused by incorrect image-retrieval vs. navigation decisions (highly frequent for GPT-4o baselines: 195/308 and 184/308 dead loops), cross-embodiment failures when deploying on robots with different kinematics, and limitations for complex dexterous end-effectors; ablations quantify performance drops when grounding format or reasoning components are removed (see performance_without_grounding).",
            "domain_shift_handling": "Handled by a simulation-based agentic data synthesis pipeline and instruction fine-tuning: synthetic multi-turn multi-image episodes teach state tracking and affordance grounding; results show strong zero-shot transfer (real-world action generation success 27/30 = 90% reported in abstract; Table 4 reports 9/10 = 90% for OWMM-VLM-38B).",
            "novel_object_performance": "Test set fully contains unseen objects (split ensured unseen objects); OWMM-VLM-38B single-step and episodic metrics reported above are evaluated on these unseen objects (e.g., episodic full-task success 21.90% strict / 51.52% lenient).",
            "frozen_vs_finetuned": "Visual encoder (ViT) is frozen during OWMM fine-tuning; only projection MLP and LLM are trained. The paper does not report a direct empirical comparison between frozen vs fully fine-tuned visual encoder.",
            "pretraining_scale_effect": "The authors study instruction-finetuning dataset scale: performance improves log-linearly with dataset size up to 152k samples (e.g., decision-making from 17.52% at 0k to 96.72% at 152k for the 8B variant), with diminishing returns near the largest size.",
            "fusion_mechanism": "Visual features are projected by a 2-layer MLP into the LLM input space (early multimodal fusion); the LLM autoregressively generates JSON outputs. (ViT parameters frozen, projection MLP + LLM trained.)",
            "sample_efficiency": "Fine-tuning required up to 152k synthetic image-text/action samples (trained for 1 epoch). Smaller datasets produced large gains: 15k samples already raise decision-making to 73.27% (from 17.52% at 0k), indicating strong sample efficiency of targeted instruction fine-tuning.",
            "key_findings_grounding": "Effective embodied grounding requires (1) integrated multi-image/scene retrieval plus egocentric decision-making, (2) region-level outputs (bounding boxes) rather than raw points for stable affordance predictions, (3) explicit reasoning and history summarization to maintain context, and (4) large-scale, instruction-driven synthetic episodes to mitigate domain-shift hallucinations; multi-image context length and model capacity remain practical bottlenecks.",
            "uuid": "e1953.0"
        },
        {
            "name_short": "RoboPoint",
            "name_full": "RoboPoint: A vision-language model for spatial affordance prediction for robotics",
            "brief_description": "A specialized single-image grounding model focused on predicting actionable keypoints/affordance points for robotic manipulation and navigation, used here as a baseline grounding module.",
            "citation_title": "Robopoint: A vision-language model for spatial affordance prediction for robotics.",
            "mention_or_use": "use",
            "model_name": "RoboPoint",
            "model_description": "Specialized single-image VLM that predicts normalized image coordinates (points) or lists of candidate spots for robot actions; in this paper it's used as an external grounding module (paired with GPT-4o for high-level decisions).",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Predicts normalized pixel coordinates (point-level affordance) from a single RGB image and a task prompt; used to convert high-level textual actions into concrete image points.",
            "representation_level": "point-level (single-image)",
            "spatial_representation": "2D points in image normalized coordinates",
            "embodied_task_type": "mobile manipulation grounding subtask (affordance prediction for pick/place/navigation)",
            "embodied_task_name": "single-image affordance grounding within OWMM evaluation",
            "visual_domain": "egocentric RGB images (single-frame)",
            "performance_metric": "affordance grounding scores (single-image evaluation) and integrated system episodic metrics when combined with GPT-4o",
            "performance_value": "Single-image grounding (object 0.91 ±0.33, receptacle 0.83 ±0.11, navigation 0.72 ±0.11). When paired with GPT-4o: overall single-step affordance scores ~0.64 (object), 0.38 (receptacle), 0.06 (navigation) in the multimodal pipeline.",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": "As a specialist, RoboPoint achieves high single-image grounding scores (object 0.91) comparable to OWMM-VLM for single-image inputs, but when combined with a weaker decision-making module (GPT-4o) overall episodic performance is poor due to decision errors.",
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Designed for single-image QA; the paper notes that single-image specialists like RoboPoint perform well at affordance prediction but are limited by not handling multi-image/global scene reasoning, contributing to failure modes when used alone for OWMM.",
            "failure_mode_analysis": "High single-image grounding quality does not prevent episodic failures when coupled with poor high-level decision-making: GPT-4o+Robopoint had many dead-loop failures (184/308) and near-zero full-task success in episodic evaluation.",
            "domain_shift_handling": "Not detailed in this paper; RoboPoint used as-is (single-image), and performance is reported on simulated and reformulated prompts.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Used as an external grounding module: GPT-4o provides action + image, RoboPoint predicts points; this is a late-fusion pipeline (decision then grounding).",
            "sample_efficiency": null,
            "key_findings_grounding": "Specialized single-image grounding models can produce strong point-affordance predictions, but integrating them into an embodied multi-image decision pipeline requires a reliable high-level decision model; otherwise grounding strengths are not sufficient to solve episodic OWMM tasks.",
            "uuid": "e1953.1"
        },
        {
            "name_short": "PIVOT",
            "name_full": "PIVOT: Iterative Visual Prompting for Actionable Knowledge in VLMs",
            "brief_description": "An iterative visual prompting method that elicits actionable affordance points from VLMs by sampling and optimizing candidate points; used here as another specialized grounding baseline.",
            "citation_title": "Pivot: Iterative visual prompting elicits actionable knowledge for vlms.",
            "mention_or_use": "use",
            "model_name": "PIVOT",
            "model_description": "Operates on a single RGB (and optionally depth) image with iterative sampling (n_samples_init, n_samples_opt, n_iters) to propose candidate actionable points; in the paper it is combined with GPT-4o as the decision module for grounding.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Iterative point sampling in image space guided by VLM prompts and optimization; returns candidate pixel coordinates for affordances.",
            "representation_level": "point-level (single-image iterative sampling)",
            "spatial_representation": "2D pixel coordinates; in episodic eval depth is provided to PIVOT for planner use",
            "embodied_task_type": "affordance grounding for mobile manipulation/navigation substeps",
            "embodied_task_name": "single-image grounding within OWMM evaluations",
            "visual_domain": "egocentric RGB (and optionally depth) single frames",
            "performance_metric": "single-image affordance grounding scores and integrated pipeline episodic metrics",
            "performance_value": "Single-image grounding (object 0.86 ±0.13, receptacle 0.84 ±0.12, navigation 0.74 ±0.13). In GPT-4o+PIVOT combined pipeline: affordance scores ~0.67 (object), 0.45 (receptacle), 0.05 (navigation); episodic full-task success ~0.33% (strict).",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": "PIVOT provides strong single-image grounding (close to RoboPoint), but when used with GPT-4o decision-making the episodic outcomes are weak, demonstrating that grounding alone does not salvage high-level planning failures.",
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Like RoboPoint, PIVOT is limited by single-image input and does not address multi-image scene-level retrieval/decision bottlenecks; iterative sampling parameters and single-frame focus are constraints.",
            "failure_mode_analysis": "When combined with GPT-4o, PIVOT pipeline exhibited a large number of dead loops (195/308) and near-zero episodic success, indicating cascading failure from incorrect high-level action selection.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Late fusion: high-level action from GPT-4o plus current image and history passed to PIVOT for point generation.",
            "sample_efficiency": null,
            "key_findings_grounding": "Iterative visual prompting yields high-quality single-image affordance points, but single-image grounding must be integrated with robust multi-image reasoning and state-tracking to succeed on long-horizon embodied tasks.",
            "uuid": "e1953.2"
        },
        {
            "name_short": "GPT-4o (baseline)",
            "name_full": "GPT-4o (multimodal/open) used as a generalist vision-language model baseline",
            "brief_description": "A generalist multimodal foundation model used as a high-level decision-making baseline, combined with specialist grounding modules in ablations and baselines.",
            "citation_title": "Gpt-4o system card.",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Generalist VLM used as an end-to-end decision and reasoning baseline; in some baselines GPT-4o is used for decision/image retrieval and then specialist grounding models (RoboPoint/PIVOT) are called for affordance prediction.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Generalist multimodal reasoning; when used alone, GPT-4o is expected to generate region/action outputs but the paper shows it performs poorly at affordance grounding; when combined with specialist modules it delegates grounding to them (late-fusion).",
            "representation_level": "global image / language-conditioned outputs (not specialized to region-level affordances in this evaluation)",
            "spatial_representation": "implicit (GPT-4o often outputs image-retrieval actions and textual descriptions rather than precise region coordinates in this paper's evaluation)",
            "embodied_task_type": "used for OWMM single-step and episodic baselines (decision-making + image retrieval)",
            "embodied_task_name": "OWMM single-step and episodic baselines",
            "visual_domain": "simulation egocentric frames and real-world egocentric frames in evaluations",
            "performance_metric": "single-step and episodic metrics (decision-making %, image retrieval %, affordance grounding scores, episodic success)",
            "performance_value": "Single-step: Decision-making 48.53%; Image retrieval 46.46%; affordance grounding object 0.56, receptacle 0.35, navigation 0.07. Episodic full-task success (strict): 0.33% (GPT-4o+PIVOT) / 0.33% (GPT-4o+RoboPoint); many dead loops (195/308 and 184/308).",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Using OWMM-VLM instead of GPT-4o yields large improvements in both decision-making and grounding (see OWMM-VLM numbers); pairing GPT-4o with specialized grounders yields better grounding scores but does not remedy decision/replanning failures.",
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "GPT-4o often outputs image-retrieval when navigation is required, causing cyclic dead loops; demonstrates hallucination and poor affordance grounding for embodied tasks without task-specific fine-tuning.",
            "failure_mode_analysis": "High frequency of dead loops (up to ~63% of evaluated episodes for combinations), poor affordance grounding for navigation targets, and inappropriate action choices leading to failed episodes.",
            "domain_shift_handling": "No task-specific simulation fine-tuning in baseline; performs poorly on affordance grounding and episodic tasks until specialist grounders are attached, showing sensitivity to domain/task mismatch.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "When used as a decision module with specialist grounders, the pipeline is late-fusion (GPT-4o decision -&gt; pass context to grounding model).",
            "sample_efficiency": null,
            "key_findings_grounding": "Generalist VLMs without embodiment-specific fine-tuning struggle at precise affordance grounding and can produce decision errors (hallucinated/redundant image retrieval) that cascade into episodic failures; specialist grounders help affordance but cannot compensate for weak decision-making.",
            "uuid": "e1953.3"
        },
        {
            "name_short": "InternVL-2.5-8B (base)",
            "name_full": "InternVL-2.5 8B (base multimodal foundation model used for OWMM fine-tuning)",
            "brief_description": "The pre-trained foundation model (InternVL-2.5 family) used as the base for OWMM-VLM fine-tuning; comprises ViT visual encoder and a LLM; serves as both a baseline and starting point for OWMM-VLM.",
            "citation_title": "Internlm2 technical report.",
            "mention_or_use": "use",
            "model_name": "InternVL-2.5-8B",
            "model_description": "Pretrained multimodal model used as the backbone for OWMM-VLM-8B (InternViT-300M + InternLM-2.5-7B). In experiments the base ViT is frozen and only projection MLP + LLM are fine-tuned for OWMM.",
            "visual_encoder_type": "InternViT-300M (part of InternVL family)",
            "visual_encoder_pretraining": "Pre-trained InternVL-2.5 family (paper does not provide exact pretraining dataset details here)",
            "grounding_mechanism": "As a baseline, the pre-trained model without OWMM fine-tuning performs poorly at affordance grounding; grounding is not specialized for OWMM tasks until instruction-fine-tuned.",
            "representation_level": "global/multi-image features in pretraining; not sufficient for OWMM region-level affordance without fine-tuning",
            "spatial_representation": "implicit image-level features; not explicitly producing planner-ready bounding boxes before OWMM fine-tuning",
            "embodied_task_type": "used as baseline for single-step OWMM capabilities and as the base for fine-tuned models",
            "embodied_task_name": "OWMM single-step evaluation when used with minimal examples (0k) or limited in-context learning",
            "visual_domain": "pretraining domain unspecified in paper; evaluation on Habitat simulation egocentric frames",
            "performance_metric": "single-step metrics (decision-making %, image retrieval %, affordance grounding scores)",
            "performance_value": "InternVL-2.5-8B baseline (no OWMM fine-tuning): Decision-making 17.52%; Image retrieval 1.27%; affordance grounding object 0.05, receptacle 0.18, navigation 0.14.",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Instruction-fine-tuning on synthetic OWMM episodes dramatically improves grounding and decision-making compared to the vanilla InternVL-2.5-8B baseline (see OWMM-VLM results), demonstrating that specialized fine-tuning is key.",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Comparison primarily between base InternVL-2.5 (vanilla) and OWMM-fine-tuned variants; finetuned models (OWMM-VLM) greatly outperform the base InternVL-2.5-8B on all grounding and decision metrics.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Vanilla InternVL model lacks task-specific affordance and multi-image reasoning capabilities, yielding near-zero image retrieval and affordance grounding scores without OWMM-specific data.",
            "failure_mode_analysis": "Fails at affordance grounding and image retrieval when not fine-tuned for OWMM; unable to produce planner-ready regions.",
            "domain_shift_handling": "The paper shows that OWMM instruction fine-tuning of the InternVL backbone addresses domain shift for embodied tasks.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "OWMM fine-tuning freezes ViT and adjusts projection and LLM; no direct comparison to full visual fine-tuning was reported.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Pretrained model uses ViT -&gt; projection -&gt; LLM fusion; OWMM fine-tuning follows same architecture but with task-specific supervised examples.",
            "sample_efficiency": "Base model with no OWMM data (0k) performs poorly; adding 15k synthetic OWMM samples yields large improvements (decision-making to 73.27%), indicating task-specific fine-tuning is sample-efficient.",
            "key_findings_grounding": "Pretrained multimodal backbones are insufficient for precise embodied affordance grounding and multi-image decision-making without targeted instruction-fine-tuning on synthetic, multi-turn episodes; fine-tuning teaches state-tracking and embodied priors needed for OWMM.",
            "uuid": "e1953.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Robopoint: A vision-language model for spatial affordance prediction for robotics.",
            "rating": 2
        },
        {
            "paper_title": "Pivot: Iterative visual prompting elicits actionable knowledge for vlms.",
            "rating": 2
        },
        {
            "paper_title": "Internlm2 technical report.",
            "rating": 2
        },
        {
            "paper_title": "Closed-loop open-vocabulary mobile manipulation with gpt-4v.",
            "rating": 2
        },
        {
            "paper_title": "Openvla: An open-source vision-language-action model.",
            "rating": 1
        },
        {
            "paper_title": "RoboPoint and PIVOT: (see respective papers above for grounding-focused evaluations).",
            "rating": 1
        }
    ],
    "cost": 0.019815,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis
21 Jun 2025</p>
<p>Junting Chen 
Haotian Liang 
Shanghai AI Laboratory</p>
<p>Lingxiao Du 
Shanghai AI Laboratory</p>
<p>Weiyun Wang 
Shanghai AI Laboratory</p>
<p>Mengkang Hu 
Yao Mu 
Wenhai Wang 
Shanghai AI Laboratory</p>
<p>Jifeng Dai 
Ping Luo 
Wenqi Shao 
Shanghai AI Laboratory</p>
<p>Lin Shao 
School of Computing
National University of Singapore</p>
<p>The Univeristy of Hongkong</p>
<p>Shanghai Jiaotong University</p>
<p>Tsinghua University</p>
<p>OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis
21 Jun 2025CE69D2C6D6B65FF1B69D7C90DFE017F4arXiv:2506.04217v2[cs.RO]
The rapid progress of navigation, manipulation, and vision models has made mobile manipulators capable in many specialized tasks.However, the open-world mobile manipulation (OWMM) task remains a challenge due to the need for generalization to open-ended instructions and environments, as well as the systematic complexity to integrate high-level decision making with low-level robot control based on both global scene understanding and current agent state.To address this complexity, we propose a novel multi-modal agent architecture that maintains multi-view scene frames and agent states for decision-making and controls the robot by function calling.A second challenge is the hallucination from domain shift.To enhance the agent performance, we further introduce an agentic data synthesis pipeline for the OWMM task to adapt the VLM model to our task domain with instruction finetuning.We highlight our fine-tuned OWMM-VLM as the first dedicated foundation model for mobile manipulators with global scene understanding, robot state tracking, and multi-modal action generation in a unified model.Through experiments, we demonstrate that our model achieves SOTA performance compared to other foundation models including GPT-4o and strong zero-shot generalization in real world.The project page is at https://github.com/HHYHRHY/OWMM-Agent.</p>
<p>= =</p>
<p>Retrieved Images wtih associated map poses relevant to tasks = Figure 1: OWMM-Agent Operates Fetch Robot for Tidying Task.OWMM-Agent receives natural language instructions and leverages both long-term environment memory (scene images) and transient robot state memory (textual summary) to generate sequential multi-modal actions to finish the task.By multi-turn, multi-image, and multi-modal VLM reasoning, the agent conducts global scene aware reasoning, updates robot state memory, and actuates itself to desired coordinates without any other learning-based models in unstructured environments.</p>
<p>Introduction</p>
<p>The vision of generalist home assistant robots has brought open-world mobile manipulation (OWMM) to the forefront of embodied AI research [38,34,37,42,26].OWMM tasks require mobile manipulators to interpret open-ended natural language instructions and operate in unstructured, previously unseen environments.Although advancements in navigation, manipulation, and vision models have effectively enabled mobile manipulators to perform many specialized tasks under constraints, achieving robust autonomy in these settings remains challenging.</p>
<p>A central difficulty in OWMM is the need for comprehensive global scene understanding and reasoning conditioned on natural language instructions and agent state.On one hand, prior approaches often construct 2D semantic maps [28] or 3D semantic fields with CLIP-based features [19,26], retrieving targets by computing embedding distances between the semantic map and language instructions.While these methods have enabled progress, they are limited by the capacity of embedding models, which can struggle with complex, compositional instructions, compared to foundational generative models like large language models (LLM) or vision-language models (VLM).Additionally, they often require time-consuming dense 3D reconstruction, making them less suitable for complex, open-ended and dynamic environments.On the other hand, the recent advances in LLMs and VLMs, with strong generalization capability, versatility, and reasoning capability, offer promising opportunities and potentially a fundamental pathway to solve all sorts of scene understanding, task planning, and robot control issues in open-world intelligent robot systems [20,15].</p>
<p>Based on the aforementioned observations, we propose a novel VLM agent framework, OWMM-Agent, to address these challenges and leverage the power of VLMs for OWMM task.More specifically, we formulate the high-level OWMM task for the internal VLM model as a multi-turn, multi-image, and multi-modal reasoning problem.The VLM model generates end-to-end chain-of-thought (CoT) thinking process, tracked agent states, and multi-modal actions with coordinates based on all raw multi-modal input.Then the agent calls the coordinate-based planners to actuate the robot.Our approach is built on two insights: 1) We do not need the detailed geometric representation of the environment for instruction-conditioned global scene understanding, and we could easily access precise and even dynamic geometric information when the robot moves to the task-relevant local region.2) By leveraging the strong vision-language grounding capabilities, we can effectively bridge the high-level reasoning process in language and low-level robot control targets in coordinates, with the help of 2D-to-3D reverse projection.</p>
<p>However, directly applying pre-trained VLMs to our embodied agent presents challenges of domain shift: 1) Rare grounding tasks: Robotic planners and controllers require multi-modal inputs, including both tools and coordinates in the visual space for robot control.The base models could be powerful for object-centric grounding such as detecting novel objects, but they suffer in other grounding tasks including detecting non-blocked navigable areas in the ego-centric image.2) State tracking: The agent must infer and track its own state from observations and history records to make contextually appropriate decisions.3) Embodiment priors: Effective decision-making in egocentric settings demands strong embodiment-dependent priors, such as knowledge of the robot's kinematic constraints, such as maximum reach for picking actions.</p>
<p>To address the problem of domain adaptation, we further introduce an agentic data synthesis pipeline tailored for OWMM, to generate large-scale and instruction-driven episodes that teach the VLM agent to track its state, reason over multi-view observations, and generate multi-modal action affordances grounded in both the global scene and the agent's embodiment.This pipeline minimizes human annotation effort by utilizing predefined task sequence templates and ground-truth symbolic world representations from simulation.With extensive experiments in simulation, we demonstrate that OWMM-VLM consistently outperforms baseline models.In the real-world experiment, we find that our model has strong zero-shot generalization to real-world observations, with 27/30 = 90% action generation success rate on our fetch robot in the lab environment, even being fine-tuned on the simulated data.We also provide ablation studies on models and dataset analysis to provide insights into the model design and training data construction.In summary, our contributions are as follows:</p>
<p>• We propose OWMM-Agent, a unified VLM-based agent architecture for open-world mobile manipulation, capable of global scene understanding, state tracking, and end-to-end action generation.• We introduce a simulation-based agentic data synthesis pipeline that enables scalable data collection for instruction fine-tuning for domain adaptation with minimized human effort, with detailed analysis on the quality of the generated dataset.</p>
<p>• We introduce a foundation model for OWMM, capable of multi-image reasoning and executable multi-modal action generation, with extensive experiments analyzing the model's performance.</p>
<p>Related Works</p>
<p>Open World Mobile Manipulation</p>
<p>Open-vocabulary Mobile Manipulation (OVMM) focuses on navigating and manipulating novel objects in unseen environments with language instructions.Referred to as Open Vocabulary Mobile Manipulation (OVMM) by [37,42,19] or Open World Mobile Manipulation (OWMM) by [27,38,34], we use the term OWMM for this paper.</p>
<p>The original OWMM baseline and Melnik et al. [23] assume the agent starts without scene observation and must explore to build a representation for decision-making.Recent works Liu et al. [19], Qiu et al. [26], Zhi et al. [42] suggest a two-stage approach: first using SLAM [7] to create 3D semantic maps, then performing OVMM using open-vocabulary models like GPT-4V and GPT-4o [11].Zhi et al. [42] introduces COME-robot, a closed-loop OVMM framework using GPT-4V for reasoning and replanning, producing code for preset functions and object captions as in Code-as-Policy [17].Unlike relying on pre-trained skill models requiring inputs like skill names and object captions, our model directly produces target positions for position-based motion planners and controllers.</p>
<p>Large Foundational Models for Robotics</p>
<p>Recent advances in large fundamental models show significant potential in robotic control and generalization.One major research focus is to adapt pre-trained Visual Language Model (VLM) to robot scenario.RoboPoint [39] introduces a synthetic data pipeline for instruction-tuning VLMs in robotics, supporting accurate spatial affordance prediction in object manipulation and navigation.MOKA [18] uses a novel VLM approach in robotic manipulation with point-based affordance and motion representation, using visual prompts to turn key points and waypoint predictions into visual question-answering tasks for VLMs.Our proposed model OWMM-VLM also falls into this category.</p>
<p>The other popular research topic is Vision-Language-Action (VLA) models, focusing on using relatively smaller transformer backbones to directly generate robotic actions with high freqeuncy.</p>
<p>OpenVLA [14] is a 7B-parameter open-source model trained on 970,000 real-world demonstrations using Llama 2 [32] architecture, excelling in general manipulation tasks.Octo [31] advances generalist robot policies, handling language commands and goal images while adapting quickly to new inputs and actions with standard GPUs.π 0 [2] presents a flow-matching architecture based on a pre-trained VLM, excelling in dexterous tasks.These models mark significant progress in making robotic systems more versatile, scalable, and adaptable to different trajectories.</p>
<p>Embodied LLM/VLM Agents and Post-training Adaptation</p>
<p>Large Language Models (LLMs) and Vision-Language Models (VLMs) have achieved remarkable success across various agent applications.In web navigation, agents like AutoWebGLM [16] leverage curriculum learning and reinforcement learning to surpass GPT-4 on realistic browsing tasks.Similarly, AppAgent [40] and AssistGUI [9] successfully adapt multimodal LLMs to effectively interact with smartphone and desktop environments, enabling complex GUI manipulation through imitation learning.In embodied AI, foundational models have recently begun to be integrated into physical or simulated robotic agents.Approaches like Steve-Eye [41] provide integrated multimodal perception and planning capabilities for open-world tasks, while TANGO [44] demonstrates that pretrained LLMs, combined with basic robot primitives, can solve diverse embodied tasks without task-specific fine-tuning.</p>
<p>However, direct application of foundational models in embodied settings often causes hallucinations and grounding issues.To address this, recent works introduce specialized post-training methods to adapt models to embodied domains.For instance, KNOWAGENT [43] employs external knowledge bases to constrain model-generated plans, significantly reducing unrealistic outputs.Factually Augmented RLHF [30] utilizes reinforcement learning from human feedback enhanced by factual grounding to align model outputs with reality.Similarly, AdaVIB [1] incorporates adaptive information bottlenecks to suppress irrelevant visual features, thereby mitigating visual hallucinations and improving task accuracy.</p>
<p>Methodology</p>
<p>In this section, we introduce the definition of OWMM in section 3.1.After that, we elaborate on the agent framework in section 3.2.Finally, the training method of OWMM-VLM is presented in section 3.3.The overview of our method is shown in Figure 2.</p>
<p>OWMM Task Definition</p>
<p>Following the common OVMM/OWMM problem setting [37,19,34], the robot needs to follow the instruction in the pattern of "Move 〈A〉(in 〈B〉) and place it on/in 〈C〉", where 〈A〉〈B〉〈C〉 are novel objects/initial receptacles/goal receptacles in the unseen environment from the training data.Following the problem setting in [19,26], we assume a pre-mapping phase separating active exploration and the SLAM module from the OWMM task focus.This is practical, as most robotic vacuums automate room mapping before cleaning.</p>
<p>Thus, we introduce a pose graph G and associated RGB images I as the output of the pre-mapping stage on the basis of [37], and define our OWMM problem as follows: In an OWMM task episode of max timestep T , at each timestep t, 0 ≤ t ≤ T , an agent takes inputs composed of 1) a natural language instruction L; 2) a pre-mapping camera pose graph G = {V, E} of n poses, where V = v 0 , . . ., v n edges are not used; 3) and associated RGB images I = {I 0 , . . ., I n }, each image I i ∈ R 3×w×h are taken at head camera view pose v i in G; The pre-mapping camera 4) the agent's current head camera RGB image I c t and depth image D c t .With these inputs, an agent needs to generate a low-level continuous action a t that directly actuates the robot kinematically, including joint velocities of the robot arm and the base velocity of the robot.Let's F agent note the logical function of the agent policy model, and we have
a t = F agent (L, G, I, I c t , D c t , x t ),(1)
where x t stands for the robot state at time t.</p>
<p>OWMM Agent</p>
<p>Running large VLM models at 25Hz and gathering sufficient data for training a generalist VLA model from open-set language and visual observations remain challenging.To address this latency issue in the OWMM agent, we have the large VLM to produce high-level actions.The agent employs a unified VLM model F vlm to convert visual and lingual inputs into action types and positional commands, using a classical planner for navigation and a motion planner for manipulation, similar to Rekep [10].The model's output represents a high-level action A t spanning several simulation steps, while planners resolve trajectories and low-level actions a t for each step.
A t , H t = F vlm (L, G, I, I c t , H t−1 ),(2)a t = A t (x t , D c t ),(3)
where H t , H t are the high-level robot history, updated by the VLM model by itelf.a t = A t (x t , D c t ) indicates that the high-level action itself can be converted to executable code with the action handle linked to different planners and positional targets.In this regard, part of the high-level action A t can be seen as a special type of language model program, as proposed in [17].Then the linked planner takes the state of the robot x t , and point clouds converted from depth map D c t as an additional input to calculate the low-level action a t .</p>
<p>To translate high-level action A t into low-level action a t , the agent has a path planner [8] for navigation and a motion planner [29] for arm manipulation.These planners generate waypoints that satisfy mechanical constraints for base chassis and arm joints through sampling-based methods.There is also a gripper controller to grasp/ungrasp the object.The high-level actions that aim to actuate the robot will be associated with planners and controllers through predefined functions.Intuitively, a VLM model requires three core multi-modal capabilities to accomplish the OWMM task: (1) Image Retrieval.Given the graph of posed frames and an egocentric frame, the VLM model needs to retrieve a posed frame that contains the relative objects or receptacles that the robot needs to navigate to.(2) Ego-centric Decision-making.Given multiple posed frames and an egocentric frame, the VLM model needs to decide which action to conduct based on the task context, robot history, and current egocentric observation.This capability is closely associated with the idea of spatial intelligence [36], that VLM models should understand the spatial relationship between themselves and the scene objects in order to make decisions on actions.(3) Affordance Grounding.If the agent decides to interact with the near surroundings perceived in the egocentric frame, it should also generate the target positions that correspond to the intention of the task.</p>
<p>OWMM-VLM model</p>
<p>Following this insight, we train a versatile VLM model that takes the task instruction L, multimodal observations I, I c t , and history H t−1 , and generates all high-level actions.We design four types of high-level actions: 1) Posed image retrieval, 2) Navigate to point, 3) Pick, and 4) Place, which are associated with planners and the grip controller.However, due to the extended time horizon of the OWMM task, simply generating the executable action is insufficient.We instruct the VLM model to monitor the state through robot history and to infer the subsequent action by considering both the history and the present observations.Figure 3 demonstrates our model architecture as well as its input and output.For more details on model implementation, see Appendix C.</p>
<p>Dataset</p>
<p>In this section, we elaborate on our data construction pipeline and quality verification method in section 4.1.A detailed analysis of the data is provided in section 4.2.</p>
<p>Dataset Construction</p>
<p>For effective OWMM-VLM model training, generating the ground truth for OWMM is essential, covering navigation, object grasping, and manipulation affordances.Previous research [39,21] often generates question-answer pairs from images or videos, lacking comprehensive action sequence representations and necessary affordance information for contextual understanding.</p>
<p>To address this challenge, we developed a data collection pipeline.Using Habitat simulation [25], we first constructed task sequences to complete the OWMM task based on the Planning Domain Definition Language (PDDL) [22].We then directed the robot to execute task sequences within the simulator, recording key information at each step.With data selection strategy and filtering pipeline, we constructed question-answer pairs based on the definition of the problem mentioned in section 3.1.We constructed the reasoning and summarization components based on predefined templates.Specifically, the summarization from each step is systematically incorporated into the "Robot's History" framework to inform the question of the next step.See Appendix D for details.</p>
<p>Finally, we collected scene graph frames for each episode.We first set the robot at the location of the receptacle where objects were initially located and the goal receptacle, sampling the robot's head-view images.Subsequently, we randomly positioned the robot and captured its head-view images.More details can be found in Appendix D.</p>
<p>Dataset Analysis</p>
<p>We used 143 scenes from The Habitat Synthetic Scenes Dataset (HSSD) [13] and combined objects from YCB Objects [4] and Google Scanned Objects [6] to create a dataset with 157 unique manipulation objects.We collected 1471 receptacles from our selected scenes.In each scene, objects were randomly placed for the robot to pick and relocate to another receptacle, resulting in 400 episodes per scene for our experiments.</p>
<p>Based on the data collection pipeline described in 4. In our datasets, we also apply a re-labeling process for objects and receptacles, unlike HomeRobot's fixed criteria [37].We kept the original object labels and used GPT-4o to rewrite receptacle labels.These labels were diverse and descriptive, suited for open-world scenarios.</p>
<p>Experiments</p>
<p>In this section, we present the evaluation results in both simulation and real-world data.We present the experimental results of single-step evaluation for OWMM-VLM in our simulated benchmark in section 5.1 and episodic evaluation for the OWMM-Agent in our simulated benchmark in section 5.2.</p>
<p>We then present the real-world evaluation in section 5.3.Due to the page limit, we discuss the data scaling law and how data diversity impacts the model performance in Appendix D.2.For the ablation study on model design, such as the choice of generating bounding boxes rather than points, please see Appendix G.We further provide the qualitative comparisons of different models in Appendix H.</p>
<p>Single-step Evaluation</p>
<p>In the single-step evaluation, we assess three core VLM capabilities for the OWMM task: 1) Egocentric Decision-making: We evaluate the success rate of choosing correct action categories.2) Image Retrieval: We measure the image retrieval success rate.3) Affordance Grounding: Instead of predicting points directly like in [39,24], OWMM-VLM outputs a bounding box, from which we compute the center as the target point.With the target point, we compute the score for affordance grounding by s = Σ i 1 valid (i) × (1 − norm_dist i ), where 1 valid (i) is the indicator function of whether the model generates: an action matched with ground truth and a valid bounding box or point on the i − th test case. 1 if both conditions are satisfied simultaneously, and 0 otherwise.norm_dist i ∈ [0, 1] is the distance between the predicted target point and the ground truth point, normalized by the diagonal of the image.In short, s ∈ [0, 1] measures VLM's ability to generate accurate grounding with the correct format.Higher scores indicate better performance.</p>
<p>Regarding the baseline methods, we have evaluated both 1) multitasking foundation VLM models, including GPT-4o [11] and InternVL-2.5-8B that share the same unified input and output configuration as ours and 2) modularized agent with multiple models, including GPT-4o+PIVOT [24] and GPT-4o+Robopoint [39].For Robopoint and PIVOT, which specialize in grounding, GPT-4o serves as the higher-level module for decision-making and image retrieval.If GPT-4o's actions need grounding, its outputs are combined with task details as input to Robopoint and PIVOT for grounding.</p>
<p>The results are reported in Table 2. Our model excels in decision-making, achieving state-of-the-art results in image retrieval and affordance grounding.GPT-4o and InternVL2.5, as generalist models, perform poorly at affordance grounding.In contrast, RoboPoint and Pivot that concentrated on affordance grounding, exhibit capabilities on par with our model in this task, indicating that existing specialized approaches already provide good effect on robot's action affordance.</p>
<p>Moreover, our model demonstrates a marked improvement over GPT-4o in decision-making tasks.This advantage directly translates into higher overall accuracy compared to methods that employ GPT-4o as the agent.In other words, using the data from our data synthesis pipeline to conduct a supervised fine-tuning yields a significant enhancement in robotic decision-making performance.</p>
<p>Episodic Evaluation</p>
<p>In episodic evaluation, we assess how well each model completes an OWMM task episode in the simulator.Task success is measured by placing objects in goal receptacles using distance thresholds of 0.85m or 1.7m.The 0.85m threshold relates to half the average diagonal length of goal receptacles' 3D bounding boxes in our test set.</p>
<p>Additionally, we introduce three metrics to assess subgoals: 1) Image retrieval: Success rate in locating object and goal receptacles from multiple posed images.2) Object Picked: The success rate of the robot grasping an item when its end effector is either within 0.15m or 0.8m of the target, with the latter matching standard HomeRobot setups [37].3) Robot close to: The success rate of robot staying within 1.5m or 2.0m of the object or goal receptacle before picking or placing.Additionally, we propose the "dead loop" metric to quantify the number of cyclic stagnations occurring during test episodes.As mentioned in 5.1, GPT-4o may erroneously output image retrieval decisions when the expected action is navigation, thereby inducing cyclic stagnation.Detailed experimental results are presented in Table 3. See Appendix F for extra details about evaluation settings.</p>
<p>Real world Evaluation</p>
<p>In our real-robot experiments, we adopted the mobile manipulation system described in Robi Butler [33] within a real-world home environment.For safety reasons, we cannot allow the agent to fully operate the fetch robot in the real world.When OWMM-VLM generates a multi-modal action to execute, the agent prompts the visualization of the action and waits for human confirmation, and the fetch robot only executes the action with human consent.</p>
<p>We first had the robot navigate through the scene with human control to perform SLAM process.We then select 10 test samples from the sequence of the robot's head view during its run.We used human operators to judge the model's output according to several criteria: whether the chosen action was correct, whether the predicted affordance was accurate, and whether the target was reachable, among other factors.The results of these experiments are presented in Table 4.The results show that the model trained on synthetically generated data in the simulator also demonstrates strong zero-shot generalization capability in real-world scenarios.Table 5 presents the agent action prediction result on real-world data.</p>
<p>Conclusion</p>
<p>In this paper, we introduced OWMM-Agent, a novel agent architecture featuring the OWMM-VLM, a vision-language model fine-tuned via a simulation-based agentic data synthesis pipeline for Open-World Mobile Manipulation (OWMM) tasks.This approach enables the VLM to learn state tracking, multi-view reasoning, and multi-modal action generation grounded in global scene understanding and agent embodiment.Extensive experiments demonstrated that our OWMM-VLM, particularly the 38B variant, achieves state-of-the-art performance in single-step multi-modal capabilities like egocentric decision-making and affordance grounding, outperforming generalist VLMs and specialized robotics models.Episodic evaluations in simulated environments further confirmed the OWMM-Agent's superior success rates and robustness against common failure modes like dead loops, while real-world tests on a Fetch robot indicated strong zero-shot generalization.Ablation studies underscored the importance of our design choices, such as bounding box prediction and integrated reasoning, and revealed that while data scaling is crucial, egocentric spatial intelligence can be learned effectively even with limited object and scene diversity if data volume is sufficient.Future work will focus on addressing limitations like pre-mapping reliance and enhancing cross-embodiment adaptability for more complex manipulation tasks.Please also refer to the appendix for discussions about the potential impact of this research in Appendix A and extended discussions on limitations in Appendix B.</p>
<p>A Impact Statement</p>
<p>This work contributes to the long-term vision of creating generalist household robots capable of assisting with daily activities in homes and other human-centric spaces.Ethically, deploying such systems raises considerations regarding safety, privacy, and workforce displacement.Ensuring safe interactions with humans and securing data used for training are critical priorities.In addition, while automation may replace certain household jobs, it also creates opportunities for new roles in robot design, deployment, and maintenance.Future societal implications include increased accessibility to robotic assistance for individuals with disabilities or aging populations.By addressing current limitations through continued research into adaptability and real-world robustness, OWMM-VLM can pave the way toward more inclusive and effective robotic solutions for societal benefit.</p>
<p>B Limitations</p>
<p>In this work, we have proposed a novel embodied agent architecture with a foundational VLM model to address the open-world mobile manipulation problem.However, we also identify some limitations of our approach.</p>
<p>Pre-mapping: Although our method does not require 3D reconstruction of the environment, we still assume a pre-mapping phase with a camera pose graph and 2D occupancy map for path planning in navigation.</p>
<p>Complex manipulation: Following the grasping setup in [37], our agent and model can be directly applied robot with suction as end effector.However, our model fells short in the circumstances when the robot needs to control complex end effectors like dexhands.</p>
<p>Cross-embodiment: As demonstrated in the experiments, our model learns the object-scale prior for spatial understanding and reasoning.However, when deploying the model onto other robots with different mechanical compositions such as maximum arm stretch distance, our model could fail, i.e. the cross-embodiment issue.</p>
<p>C Implementation Details</p>
<p>Regarding the model's architecture, we have trained two variants consisting of 8 billion and 38 billion parameters, based on the pre-trained model from InternVL-2.5 [5].The 8B model is composed of InternViT-300M and InternLM-2.5-7B[3], and the 38B model is composed of InternViT-6B and Qwen2.5 [35].We directly finetune the base model on our OWMM dataset.The OWMM-VLM model is trained to autoregressively generate the response tokens consisting of the output action and its corresponding task context in JSON format.Specifically, we freeze the parameters in ViT and only adjust the parameters in MLP and LLM.As for the training time, OWMM-VLM-8B is trained on 8X NVIDIA A100 GPUs for about 7 hours, and OWMM-VLM-38B is trained on 24X NVIDIA A100 GPUs for about 18 hours.Both our models were trained for 1 epoch.For the testing, we deploy OWMM-VLM and RoboPoint [39] locally and use the openAI API to access GPT-4o and PIVOT [24].</p>
<p>D Details of Datasets D.1 Extra Dataset Construction Details</p>
<p>Our evaluation pipeline is constructed using the HomeRobot [37] framework, which serves as a software structure designed to enable comprehensive benchmarking in both simulated and realworld settings.Specifically, we use the simulation part of HomeRobot project, built on Habitat platform [25], with 200 scenes, 150 categories, and 7892 object instances.The original episodic data in HomeRobot are generated with Stretch Robot [12], which has a special telescopic arm instead of a normal articulated arm with rotary joints.This adds additional difficulty in base control as it requires the mobile chassis to rotate accurately to align the arm with the target object for manipulation.However, the baseline VLMs and methods we are going to compare with are designed for robots with conventional articulated arms [39,24], providing a broad range of chassis poses that allow for successful arm manipulation.</p>
<p>Therefore, we recreate the OWMM episodic training and testing datasets in the simulation using the Fetch Robot, which is a mobile robot equipped with a standard articulated arm and has also been integrated into the Habitat platform.We partitioned the scenes into training and testing sets using a ratio of 113:30.Besides, we allocated 157 objects between the training and validation sets with a ratio of 137:20, ensuring that the testing set contained entirely unseen objects.This division resulted in a total of 152k training data entries and 4k testing data entries, establishing a robust dataset for training and testing in our OWMM task.</p>
<p>In dataset construction pipeline,we first sample key imformation at each step.This information included the robot's coordinates, current action, the positions of objects and receptacles, and the extrinsic parameters of the robot's head-view camera.In particular, at this stage, we did not collect the robot's head-view images to enhance the data collection efficiency.We recollected the robot's head-view images of these steps within the simulator after selection strategy.</p>
<p>In the key step data selection strategy, for navigation actions, among all steps that the robot is moving, we select the step that the receptacle is visible from the robot's head-view image as the start point of the navigation action.The point at which the robot stops moving is considered the end point of the navigation action.Within these steps, we sample the waypoint step data at specified intervals.For grasp and manipulation actions, we select the first three frames during which the robot executes the action as the pre-defined action data.</p>
<p>The data filtering pipeline ensures the following matters: for navigation actions, both the receptacle and the next waypoint are within the robot's head-view image.For grasp actions,the object to be grasped is reachable by the robotic arm, and the object is within the robot's head-view image.For manipulation actions,the receptacle intended for object placement is reachable by the robotic arm, and the receptacle is within the robot's head-view image.</p>
<p>To enhance the diversity of the dataset, we paraphrased reasoning and summarization parts of the answers using GPT-4o mini.This analysis tries to answer two questions: 1) How does the diversity of objects and environments affect the model's performance on unseen objects and environments in the test set?We examine dataset diversity using three 45k-sample sets: 100% scenes and objects, 100% scenes with 30% objects, and 30% scenes with 100% objects.We control the total number of training samples while changing the number of object instances or scenes appearing in the training data.2) How does the model's performance change as the training data scales up?For data scaling, we use five data sizes: 0k (no fine-tuning), 15k (10%), 45k (30%), 76k (50%), and 152k (100%).At 0k, we give the Internvl-2.5-8Bmodel limited input-output pairs, allowing it to generate structured outputs via in-context learning.We evaluate the performance in image retrieval, egocentric decision-making, and three affordance grounding subtasks.Results are shown in Table 6 and Figure 5.</p>
<p>The results for the first question show that object and scene diversity have negligible effects on multi-modal capabilities, as metric fluctuations remain within a 5% range.For the second quesion, data scaling is crucial for enhancing OWMM-VLM's performance.As seen in Figure 5, increasing the dataset from 0k to 152k samples shows a logarithmic improvement, especially at lower sizes (0k to 15k, and 15k to 45k).However, benefits diminish near 152k.While larger datasets aid generalization, marginal gains decrease beyond a threshold.As performance gains plateau, egocentric decision making approaches a success rate of 1.0, whereas image retrieval lingers at approximately 0.8.This difference is likely due to the model's limited capacity with 8 billion parameters.We also draw two extra observations from the experiment:</p>
<p>1) The embodiment prior for deciding the current action based on the ego-centric RGB image, especially how close the robot should be to interact with the target objects, can be learned in a data-driven approach.</p>
<p>2) The ability to comprehend multiple images or the multimodal context length may present one of the bottlenecks for VLM models to function as the core cognitive model for intelligent robots, particularly when scene-level understanding is essential.</p>
<p>E Details of Baseline Setting</p>
<p>As Robopoint and PIVOT are designed for single-image QA task, we adjusted some settings to enable them fully utilizing their capabilities under the OWMM task.</p>
<p>E.1 Single Image Grounding</p>
<p>For the single-step evaluation, we first extracted robot's task instruction from the original prompt of the current step.Based on the ground truth action of the current step and whether the robot picks up an object, we designed new task instructions, as shown in Table 7.For Robopoint, we appended the following context: "Find a few spots for robot to execute the action.Your answer should be formatted as a list of tuples, i.e. [(x1, y1), (x2, y2), ...], where each tuple contains the x and y coordinates of a point satisfying the conditions above.The coordinates should be between 0 and 1, indicating the normalized pixel locations of the points in the image."This configuration aligns with Robopoint's original settings.</p>
<p>Ground Truth Action</p>
<p>New Task Instruction</p>
<p>Pick</p>
<p>The robot needs to pick {object item} on {target rec}</p>
<p>Place</p>
<p>The robot needs to place {object item} on {goal rec} Nav to point(object picked)</p>
<p>The robot needs to navigate closer to the {goal rec} for placing {object item} Nav to point(object not picked)</p>
<p>The robot needs to navigate closer to the {target rec} for picking {object item} Table 7: Redefined Task Instructions.{object item}, {target rec} and {goal rec} are from robot's task instruction.</p>
<p>For PIVOT, we configured the following parameters: n_samples_init=10, n_samples_opt=6, n_iters=2.In our evaluation settings, as the input consists of a single RGB image and task instructions, we randomly sample initial points in the image from a 2D Gaussian distribution.The distribution is parameterized with a mean of (256, 256) and standard deviation of (100, 100).</p>
<p>E.2 Agent Setting</p>
<p>We employed GPT-4o for agent construction.GPT-4o first receives our instruction inputs and returns JSON-formatted responses.When gpt's output action is "search scene frame", we directly adopt GPT-4o's response as the agent's current-step output.For actions "nav to point", "pick", or "place", the system sends both the action name and robot's current-view RGB image (single frame) to Robopoint/PIVOT for action affordance.The reformulated task instruction sent to Robopoint/PIVOT follows this template: "The robot needs to {task_instruction}.Now the robot needs to {gpt_output_action}. {robot_history}" where {task_instruction} is the original task instruction,{gpt_output_action} is gpt's output action,{robot_history} is the summarization of previous step.In single-step evaluation, Robopoint and PIVOT process these new task instructions using the same methodology described in Appendix E.1.In episodic evaluation, we transmit depth information to PIVOT while maintaining consistency with its original configuration.</p>
<p>F Extra Details of Episodic Evaluation</p>
<p>As mentioned in section 5.2, we designed the following metrics for episodic evaluation.More detailed specifications of these metrics are outlined below: Object to Goal Distance: We used the object to goal distance as the metric to determine whether objects are successfully placed in goal receptacles.To establish appropriate thresholds, we first calculated the 3D bounding box diagonal distances of all goal receptacles in the test set, filtering out those with distances less than 0.75m or greater than 3m.Table 8 show some examples of goal receptacles in our test set.Subsequently, we computed the average diagonal distance (1.7m) from the remaining valid receptacles.Based on this value, we selected half of the average (0.85m) as the strict threshold criterion and the full average (1.7m) as the relaxed threshold criterion.This threshold approach ensures successful placement recognition when robots position objects near goal receptacles,and reasonable constraint boundaries to prevent excessive leniency in evaluation.</p>
<p>F.1 Simulation</p>
<p>For one simulation step, the robot state delta is calculated by forward kinematics, as implemented by the Habitat 3.0 environment [25].The robot state and observations updates can be expressed mathematically as:</p>
<p>x t+1 = f k (x t , a t , ∆t) I c t+1 , D c t+1 = f obs (x t+1 ) where x t stands for robot current state (e.g., joint angles, positions), a t stands for velocities, and f k represents the kinematic model function that computes the next robot state within the discretized</p>
<p>Figure 2 :
2
Figure 2: The Overview of OWMM Agent Framework.The left panel represents the world space, including a graph of posed frames generated during the pre-mapping phase and a real-time egocentric frame captured by the robot.The right panel showcases the Agent Space, where OWMM-VLM processes task instructions, robot history, and visual inputs to perform chain-of-thought reasoning and generate high-level actions with region coordinates, which are then sent to robot planners for navigation and manipulation.</p>
<p>Figure 3 :
3
Figure 3: Overview of OWMM-VLM.Our model is fine-tuned on InternVL-2.5[5], comprising a ViT, a 2-layer projection MLP, and a LLM.During training, ViT parameters are frozen while the projection MLP and the LLM parameters are trainable.The model is required to generate multi-modal actions in JSON format conditioned on scene images, task instructions, and robot history.</p>
<p>Figure 4 :
4
Figure 4: Word Cloud Distribution of Objects and Receptacles in our dataset</p>
<p>Figure 5 :
5
Figure 5: OVMM-VLM-8B Sub-task Performance with the Increase of Training Data Size.The task scores consistently improve as the training data size increases.</p>
<ol>
<li>Long-term Environent Memory (Images) 2. Transient Robot State Memory (Texts) Multimodal Memory Multimodal Tools = VLM
Please clean upthe table andplace back fruitsback to bowls!</li>
</ol>
<p>vlm Nav to frame Embodied VLM Agent vlm Pick at position vlm Nav to frame vlm</p>
<p>Table 1 :
1
Dataset Overview for Instruction Fine-tuning.Our dataset consists of four subsets, each corresponding to one of the four primary task actions: Pick, Place, Navigate to Point, and Search Scene Frame.The dataset is designed to encompass diverse scenarios and objects, ensuring comprehensive coverage of open-world mobile manipulation tasks.
Task ActionPickPlaceNav to pointSearch scene frameData Size64.7K68.9K59.6K378.8KTask DescriptionMove Arm Hammer Di-Move Shark from theMove wood block fromMove flat screwdriveraper Pail Refills 12 PackConlay kitchen to thethe 7-piece dining setfrom the Modern Indus-from the Brunel-style barcomfortable sofa.with grey chairs to thetrial Dresser, Natural Ma-stool to the white 2-seaterLow kitchen element,terial to the Stacked shelfsofa.Natural element.system.Context DescriptionI have embarked on mytask and am steadilyadvancing toward theBrunel-style bar stool,where the Arm HammerDiaper Pail Refills 12Pack MFWkmoweejt issituated.Action Information[[68, 755, 239, 967]][[447, 539, 999, 999]][[246, 666, 285, 705]]4
I have embarked on my task and successfully navigated to the Conlay kitchen, retrieving the Shark with ease.Now, I am inching closer to the cozy haven of the comfortable sofa, where I will soon place the Shark.The task has started and I have navigated to 7piece dining set with grey chairs and picked up the wood block , I am getting closer to Low kitchen element, Natural element where I should place wood block.The journey has commenced, and I have successfully navigated to the Modern Industrial Dresser, Natural Material, where I have now picked up the flat screwdriver.</p>
<p>1, we collected episodes from each scene and ultimately gathered 21,046 valid episodes, obtaining approximately 235k annotations.As shown in Table 5, the dataset is composed of: pick action dataset of 64.7K image-text pairs, place action dataset of 68.9K image-text pairs, navigation dataset of 59.6K image-text pairs, and a search scene frame dataset with 378.8K multi-image-text pairs.</p>
<p>Table 2 :
2
Single-step evaluation of VLM models on OWMM core multi-modal capabilities.The OWMM-VLM-38B model achieves the best performance across all metrics, demonstrating its superior ability to integrate scene understanding, decision-making, and action generation.<em>: Since PIVOT and RoboPoint are designed for a single image, we also report the single image grounding results for fairness.
Model/ Task ScoreEgo-centricImageAffordanceAffordanceAffordanceTimeDecision-Retrieval↑GroundingGroundingGroundingConsumption(s)↓making↑(object)↑(receptacle)↑(navigation)↑OWMM-VLM-38B(ours)97.85%87.54%0.97(±0.14) 0.94(±0.19) 0.88(±0.17)36.58OWMM-VLM-8B(ours)96.72%79.04%0.93(±0.14)0.91(±0.20)0.83(±0.21)16.58GPT-4o[11]48.53%46.46%0.56(±0.38)0.35(±0.40)0.07(±0.21)160.74Internvl2.5-8B[5]17.52%1.27%0.05(±0.19)0.18(±0.31)0.14(±0.26)16.06GPT-4o+PIVOT[24]52.72%55.38%0.67(±0.38)0.45(±0.44)0.05(±0.18)22.91GPT-4o+Robopoint[39]49.56%49.72%0.64(±0.41)0.38(±0.42)0.06(±0.20)14.19Test of Single Image Grounding(</em>)Robopoint[39]<em>--0.91(±0.33)0.83(±0.11)0.72(±0.11)-PIVOT(GPT-4o)[24]</em>--0.86(±0.13)0.84(±0.12)0.74(±0.13)-</p>
<p>Table 3 :
3
Agent success rate in OWMM Task.OWMM-VLM-38B model consistently outperforms others across all metrics.
MethodFull TaskImage Re-Robot closeObjectImage Re-Robot closeDead Looptrieval(Object)to ObjectPickedtrieval(Goal)to GoalOWMM-VLM-38B(ours)21.90%88.56%84.64%38.56%30.39%23.53%0/308OWMM-VLM-8B (ours)9.45%81.43%74.59%17.92%15.96%10.42%0/308GPT-4o+PIVOT0.33%59.15%10.13%0.65%0.33%0.00%195/308GPT-4o+Robopoint0.33%56.86%11.11%1.31%0.00%0.00%184/308Experiment with more lenient distance toleranceOWMM-VLM-38B(ours)51.52%89.23%88.22%62.96%51.52%44.78%0/308OWMM-VLM-8B (ours)38.59%83.22%81.21%52.35%39.93%33.56%0/308GPT-4o+PIVOT1.68%60.27%12.12%5.39%1.68%1.35%204/308GPT-4o+Robopoint3.03%52.86%10.10%4.04%2.69%1.35%209/308</p>
<p>Table 4 :
4
Real world single evaluation.OWMM-VLM-38B model achieved the best performance, and OWMM-VLM-8B model also outperformed the baseline.While the baseline model demonstrated relatively strong affordance grounding capabilities for objects, its poor performance in action decisionmaking led to incorrect navigation.
MethodImageAffordanceAffordance Grounding(navigation) Total AccRetrievalGrounding(object&amp;receptacle)OWMM-VLM-38B(ours)7/1010/1010/1090.00%OWMM-VLM-8B (ours)5/1010/109/1080.00%GPT-4o+PIVOT8/106/100/1046.67%GPT-4o+Robopoint8/106/100/1046.67%</p>
<p>Table 5 :
5
Demonstration of single step evaluation in real world.These demos showcase OWMM-VLM-38B's outputs, illustrating that even though its training data are drawn entirely from our data-synthesis approach in the simulator, the model delivers outstanding decision-making and affordance-grounding performance in real-world settings.
Context DescriptionThe task has started and I amThe task has started and I haveThe task has started and I amgetting closer to the Minimalistnavigated to the black desk andgetting closer to GenuineBlack Workstation Desk wherepicked up the banana, I amLeather Sofa where the chip boxthe NutriSoy Bean Milk Box isgetting closer to the Whiteis located.located.Rectangular Office MeetingTable where I should place thebanana.Action Information[576, 263, 769, 548]][[0, 445, 1000, 999]][[539, 978, 578, 999]]
Model's Output Action Pick Place Nav to point Task Description Move the NutriSoy Bean Milk Box from the Minimalist Black Workstation Desk to the White Rectangular Office Meeting Table.Move the banana from the black desk to the White Rectangular Office Meeting Table.Move the chip box from Genuine Leather Sofa to the white table.</p>
<p>Table 6 :
6
Results with different data diversity data scales.The best performance across training sets with different scales is indicated with bold font.Besides, underline highlights the best performance across three 45k-sample training sets with different diversity.
Data Composition/ Task ScoreEgo-centricImage Retrieval↑AffordanceAffordanceAffordanceDecision-GroundingGroundingGroundingmaking↑(object)↑(receptacle)↑(navigation)↑0k(0%)17.52%1.27%0.05(±0.19)0.18(±0.31)0.14(±0.26)15k(10%)73.27%41.36%0.69(±0.43)0.84(±0.29)0.45(±0.41)45k(30%)91.01%70.68%0.88(±0.24)0.84(±0.31)0.74(±0.31)45k(100% scene + 30% object)91.56%71.95%0.87(±0.26)0.89(±0.23)0.72(±0.33)45k(30% scene + 100% object)88.96%69.12%0.87(±0.26)0.84(±0.31)0.69(±0.36)76k(50%)95.79%76.20%0.91(±0.19)0.88(±0.24)0.84(±0.20)152k(100%)96.72%79.04%0.93(±0.14)0.91(±0.20)0.83(±0.21)
AcknowledgementsWe sincerely thank Anxing Xiao and David Hsu from the National University of Singapore for their crucial support and guidance in the successful real-world deployment of this project, as well as their generosity in providing the Fetch robot.G Ablation Study on OWMM-VLMThe ablation study evaluates the contributions of the components of the OWMM-VLM model.We focus on grounding output formats, comparing the bounding box and point coordinate, and we assess the inclusion of reasoning and summarization in the outputs.Furthermore, we examine the beam search option provided by the base model Internvl-2.5-8B[5].The results are in Table9.From the table, we have these observations and indications:1) Beam Search.Beam search is a decoding algorithm widely used in language generation, maintaining a beam number of top candidate sequences at each step.Beam search enhances Ego-centric Decision-making and Affordance Grounding tasks, with minimal impact on Image Retrieval, but increases temporal and spatial overhead in inference, especially on the 38B variant.Hence, its effect is briefly shown only in the ablation study.2) Grounding Format.Replacing bounding box predictions with direct output coordinates reduces performance in Affordance Grounding, especially for objects (0.9251 → 0.6542) and receptacles (0.9060 → 0.6479).It is postulated that the large-scale visual grounding data in the pre-trained model allow our model to utilize this prior knowledge.The consistency in output format between the base model and the instruction fine-tuning dataset aids the training process.3) Reasoning and Summarization.Removing reasoning and summarization capabilities leads to the worst performance across most metrics, with a decrease in Image Retrieval (0.7904 → 0.6586) and Ego-centric Decision-making (0.9672 → 0.9049).This highlights the critical role of reasoning and summarization in maintaining contextual coherence and task understanding.H Qualitative EvaluationWe provide the qualitative evaluation of our OWMM-VLM model compared to other baseline models.Pick Place Nav to point
Mitigating hallucinations in large vision-language models by adaptively constraining information flow. Jiaqi Bai, Hongcheng Guo, Zhongyuan Peng, Jian Yang, Zhoujun Li, Mohan Li, Zhihong Tian, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>pi0: A vision-language-action flow model for general robot control. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, arXiv:2403.172972024Internlm2 technical report. arXiv preprint</p>
<p>The ycb object and model set: Towards common benchmarks for manipulation research. Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, Aaron M Dollar, 2015 international conference on advanced robotics (ICAR). IEEE2015</p>
<p>Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Zhaoyang Hao Tian, Liu, arXiv:2412.052712024arXiv preprint</p>
<p>Google scanned objects: A high-quality dataset of 3d scanned household items. Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B Mchugh, Vincent Vanhoucke, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Simultaneous localization and mapping: part i. Hugh Durrant, - Whyte, Tim Bailey, IEEE robotics &amp; automation magazine. 1322006</p>
<p>A survey on coverage path planning for robotics. Enric Galceran, Marc Carreras, Robotics and Autonomous systems. 61122013</p>
<p>Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, arXiv:2312.13108Task-oriented desktop graphical user interface automation. 2023arXiv preprint</p>
<p>Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, Li Fei-Fei, arXiv:2409.01652Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation. 2024arXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>The design of stretch: A compact, lightweight mobile manipulator for indoor human environments. Aaron Charles C Kemp, Henry M Edsinger, Blaine Clever, Matulevich, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for objectgoal navigation. Mukul Khanna, Yongsen Mao, Hanxiao Jiang, Sanjay Haresh, Brennan Shacklett, Dhruv Batra, Alexander Clegg, Eric Undersander, Angel X Chang, Manolis Savva, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>A survey on integration of large language models with intelligent robots. Yeseung Kim, Dohyun Kim, Jieun Choi, Jisang Park, Nayoung Oh, Daehyung Park, Intelligent Service Robotics. 1752024</p>
<p>Autowebglm: A large language model-based web navigating agent. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, Jie Tang, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Moka: Open-vocabulary robotic manipulation through mark-based visual prompting. Fangchen Liu, Kuan Fang, Pieter Abbeel, Sergey Levine, First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024. 2024</p>
<p>Peiqi Liu, Yaswanth Orru, Jay Vakil, Chris Paxton, Nur Muhammad Mahi, Lerrel Shafiullah, Pinto, arXiv:2401.12202Ok-robot: What really matters in integrating open-knowledge models for robotics. 2024arXiv preprint</p>
<p>A survey on visionlanguage-action models for embodied ai. Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King, arXiv:2405.140932024arXiv preprint</p>
<p>Embodied question answering in the era of foundation models. Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Pddl-the planning domain definition language. Drew Mcdermott, Malik Ghallab, Adele E Howe, Craig A Knoblock, Ashwin Ram, Manuela M Veloso, Daniel S Weld, David E Wilkins, 199859656859</p>
<p>Andrew Melnik, Michael Büttner, Leon Harz, Lyon Brown, Chand Gora, Nandi, P S Arjun, Gaurav Kumar Yadav, Rahul Kala, Robert Haschke, arXiv:2312.08611Uniteam: Open vocabulary mobile manipulation challenge. 2023arXiv preprint</p>
<p>Pivot: Iterative visual prompting elicits actionable knowledge for vlms. Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, arXiv:2402.078722024arXiv preprint</p>
<p>Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, arXiv:2310.13724So Yeon Min, et al. Habitat 3.0: A co-habitat for humans, avatars and robots. 2023arXiv preprint</p>
<p>Open-vocabulary mobile manipulation in unseen dynamic environments with 3d semantic maps. Dicong Qiu, Wenzong Ma, Zhenfu Pan, Hui Xiong, Junwei Liang, arXiv:2406.181152024arXiv preprint</p>
<p>Learning generalizable feature fields for mobile manipulation. Ri-Zhao Qiu, Yafei Hu, Ge Yang, Yuchen Song, Yang Fu, Jianglong Ye, Jiteng Mu, Ruihan Yang, Nikolay Atanasov, Sebastian Scherer, arXiv:2403.075632024arXiv preprint</p>
<p>Learning hierarchical interactive multi-object search for mobile manipulation. Fabian Schmalstieg, Daniel Honerkamp, Tim Welschehold, Abhinav Valada, IEEE Robotics and Automation Letters. 2023</p>
<p>The open motion planning library. Mark Ioan A Sucan, Lydia E Moll, Kavraki, IEEE Robotics &amp; Automation Magazine. 1942012</p>
<p>Aligning large multimodal models with factually augmented rlhf. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, arXiv:2309.145252023arXiv preprint</p>
<p>Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, arXiv:2405.12213An open-source generalist robot policy. 2024arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Robi butler: Multimodal remote interaction with a household robot assistant. Anxing Xiao, Nuwan Janaka, Tianrun Hu, Anshul Gupta, Kaixin Li, Cunjun Yu, David Hsu, 2025</p>
<p>Adaptive mobile manipulation for articulated objects in the open world. Haoyu Xiong, Russell Mendonca, Kenneth Shaw, Deepak Pathak, arXiv:2401.144032024arXiv preprint</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, arXiv:2407.10671Fei Huang, et al. Qwen2 technical report. 2024arXiv preprint</p>
<p>Thinking in space: How multimodal large language models see, remember, and recall spaces. Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, Saining Xie, arXiv:2412.141712024arXiv preprint</p>
<p>Homerobot: Open vocab mobile manipulation. Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alex William Clegg, John Turner, Zsolt Kira, Manolis Savva, Angel Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton, Conference on Robot Learning. 2023</p>
<p>Sriram Yenamandra, Arun Ramachandran, Mukul Khanna, Karmesh Yadav, Jay Vakil, Andrew Melnik, Michael Büttner, Leon Harz, Lyon Brown, Gora Chand Nandi, arXiv:2407.06939Towards open-world mobile manipulation in homes: Lessons from the neurips 2023 homerobot open vocabulary mobile manipulation challenge. 2024arXiv preprint</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, Dieter Fox, arXiv:2406.107212024arXiv preprint</p>
<p>Appagent: Multimodal agents as smartphone users. Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, Gang Yu, Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. the 2025 CHI Conference on Human Factors in Computing Systems2025</p>
<p>Steve-eye: Equipping llm-based embodied agents with visual perception in open worlds. Sipeng Zheng, Jiazheng Liu, Yicheng Feng, Zongqing Lu, arXiv:2310.132552023arXiv preprint</p>
<p>Peiyuan Zhi, Zhiyuan Zhang, Muzhi Han, Zeyu Zhang, Zhitian Li, Ziyuan Jiao, Baoxiong Jia, Siyuan Huang, arXiv:2404.10220Closed-loop open-vocabulary mobile manipulation with gpt-4v. 2024arXiv preprint</p>
<p>Knowagent: Knowledge-augmented planning for llm-based agents. Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Shiwei Lyu, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen, Ningyu Zhang, arXiv:2403.031012024arXiv preprint</p>
<p>Tango: Trainingfree embodied ai agents for open-world tasks. Filippo Ziliotto, Tommaso Campari, Luciano Serafini, Lamberto Ballan, arXiv:2412.104022024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
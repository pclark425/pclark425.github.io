<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4562 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4562</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4562</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-a6cad00e214c0f3ccdb0145bc4c384475d8b9828</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a6cad00e214c0f3ccdb0145bc4c384475d8b9828" target="_blank">Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work advances LLM evaluation methodologies by bridging Bayesian inference with practical constraints in real-world deployment scenarios by formalizing model ranking as a Bayesian hypothesis testing problem over mutually exclusive capability intervals.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) exhibit probabilistic output characteristics, yet conventional evaluation frameworks rely on deterministic scalar metrics. This study introduces a Bayesian approach for LLM capability assessment that integrates prior knowledge through probabilistic inference, addressing limitations under limited-sample regimes. By treating model capabilities as latent variables and leveraging a curated query set to induce discriminative responses, we formalize model ranking as a Bayesian hypothesis testing problem over mutually exclusive capability intervals. Experimental evaluations with GPT-series models demonstrate that the proposed method achieves superior discrimination compared to conventional evaluation methods. Results indicate that even with reduced sample sizes, the approach maintains statistical robustness while providing actionable insights, such as probabilistic statements about a model's likelihood of surpassing specific baselines. This work advances LLM evaluation methodologies by bridging Bayesian inference with practical constraints in real-world deployment scenarios.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4562.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4562.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian capability-ranking evaluation for LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic evaluation framework that treats an LLM's latent capability as a continuous latent variable and performs Bayesian inference over mutually exclusive capability intervals using priors derived from anchor-model response distributions and a curated discriminative query set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Bayesian capability-ranking evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Treat each model's capability θ as a latent scalar. Use a curated set Q of M discriminative binary queries and empirical conditional probabilities Pr(Q_j=1 | L_i) from N anchor models to form priors and likelihoods. Partition capability space into N+1 mutually exclusive intervals determined by anchor model cumulative success rates (θ_1..θ_N). Assume a uniform prior over θ within each interval (maximum entropy) so prior probability of interval ∝ (θ_{i+1}-θ_i). Under conditional independence of queries, compute likelihood of observed binary outcomes q (single-trial) or K successes out of O trials per query (multi-trial Bernoulli) by averaging the anchor-boundary probabilities: Pr(Q_j=q_j | θ in interval) ≈ (Pr(Q_j|L_i) + Pr(Q_j|L_{i+1}))/2, and multiply across j. Normalize to get posterior Pr(interval | q). The method outputs probabilistic interval membership (e.g., probability model lies between anchor i and i+1), enabling statements like "Model X has p% probability of exceeding baseline Y." Implements numerical stabilizers (ε-adjustment for observed 0/1 probabilities) and supports single-trial (Bayes@1) and multi-trial aggregation (Bayes@10).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Probabilistic interval membership (likelihood of belonging to capability intervals defined by anchors); discriminative power between anchors; statistical robustness under limited sample sizes; posterior probability sharpness (confidence); consistency across trial aggregations (single vs multi-trial).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (questions span sciences, engineering, mathematics, humanities, arts, reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>not applicable (evaluates model capability/performance ranking rather than specific scientific theories)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>The Bayesian method achieved superior discrimination vs. conventional metrics in the paper's experiments. Examples: anchor cumulative success rates ranged ~14% to 82% across six GPT-series anchors; the method retained stable interval identification down to M=20 questions (≥~65% confidence for most likely interval), with sharpness attenuating as M decreased to 10 or 5 (peak interval probabilities fell below 50% at very small M). Multi-trial Bayes@10 improved interval precision compared to Bayes@1. The approach produced actionable probabilistic statements (e.g., probability of exceeding specific baselines) and resolved ambiguities that accuracy/Pass@N/mean±std obscured (paper gives multiple model-specific comparisons showing Bayesian intervals disagree with naive mean±std or Pass@10 rankings).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated/statistical: purely model-output-driven using measured response frequencies from anchor models and the test model; no human raters were used for the core posterior computations (though the paper references human-preference platforms in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical experiments comparing Bayesian posterior interval assignments to conventional metrics (accuracy, Pass@N, mean±std) across varying query budgets M (50,30,20,10,5) and single vs multi-trial regimes (O up to 10); robustness analysis (how posterior sharpness changes with M); demonstration on several test models and anchor sets (GPT-series anchors, multiple open-source test models).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires representative, high-quality anchor models and discriminative query sets; assumes conditional independence of queries (may fail for correlated items); binary scoring limits applicability to open-ended tasks; interval priors rely on monotonic anchor ordering which may be disrupted by new architectures; extreme observed probabilities require ε-adjustment; does not model question interdependencies or multi-dimensional rubrics without extension.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>A curated multi-source query set assembled from superGPQA, MMLU-Pro, GPQA-Diamond, MATH, ZebraLogic, KOR-Bench, and Procbench: initially 170 items then reduced to a 50-question set (further ablations to 30/20/10/5); the refined set was syntactically paraphrased to reduce memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4562.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4562.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayes@10 (multi-trial)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-trial Bayesian aggregation (Bayes@O, e.g., Bayes@10)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of the Bayesian evaluation that aggregates O independent trials per query by modeling per-query outcomes as Binomial/Bernoulli experiments and using the binomial likelihood for anchor boundary probabilities, improving precision over single-trial inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Multi-trial Bayesian aggregation (Bayes@O)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each query Q_j, collect O independent trials yielding K successes. For each anchor L_i, estimate Pr(vec Q_j = vec q_j | L_i) = C(O,K) * p_i^K * (1-p_i)^(O-K) where p_i = Pr(Q_j=1 | L_i). For an interval (θ_i,θ_{i+1}], approximate likelihood as average of the two boundary anchors' binomial likelihoods. Multiply across queries, apply uniform-θ interval priors, normalize to posterior interval probabilities. Aggregation across trials reduces variance and sharpens posterior distributions relative to single-trial Bayes@1.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Posterior interval probability sharpness; reduction in uncertainty with increasing O; stability and consistency of categorical assignments across trials.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multi-domain (same curated question set as Bayesian evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Bayes@10 provided superior interval precision vs Bayes@1 in experiments. The paper reports Bayes@10 maintains consistent category estimation boundaries across models (except for particularly variable models like Qwen2.5-72B) and reduces ambiguity compared to single-trial inference.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (statistical aggregation of repeated model outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical comparison between Bayes@1 and Bayes@10 across the curated query set with O=10 trials per question; assessments of interval probability sharpness and resolution under reduced M.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires the ability to run multiple independent trials (cost/time); sensitive to trial-to-trial variability of the evaluated model (high variance models may still produce wide posteriors); still relies on anchor quality and independence assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same curated multi-source query set; experiments used O=10 trials per anchor/model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4562.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4562.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pass@N</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pass@N (upper-bound performance aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A performance metric that reports whether at least one of N sampled outputs for a given input passes an evaluation test, intended to approximate a model's upper-bound or best-case output quality across stochastic decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating large language models trained on code</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Pass@N</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate N independent outputs per input (using stochastic decoding); count an input as successful if any of the N outputs is correct; aggregate success rate across inputs to estimate the model's best-case performance over N samples. Often used for problems with multiple possible valid outputs (e.g., code generation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Best-case/upper-bound pass rate across N samples; emphasizes rare high-quality outputs rather than consistent reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general NLP benchmarks and code generation contexts (used here as a comparison metric on the curated question set)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>In the paper, Pass@10 inflated overall accuracy metrics while reducing discernibility between models; it sometimes suggested parity where Bayesian analysis found categorical differences (e.g., Pass@10 suggested Llama-4-Maverick comparable to DeepSeek-R1, contradicting Bayesian rankings).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (metric computed from multiple sampled outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared empirically against Bayesian posteriors and mean±std reporting using O=10 trials and M=20 questions; differences in model discrimination and interpretability were highlighted.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>No statistical interpretation of uncertainty; sensitive to decoding hyperparameters (temperature, top-k/p); emphasizes rare best-case outputs and can obscure consistent performance differences; may reduce discriminability between models.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used as a comparative metric on the paper's curated multi-source question set (M=20 used in method comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4562.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4562.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Accuracy reporting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-trial accuracy reporting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The simple metric of reporting the fraction of correct answers from a single evaluation pass over a dataset (one trial per question), producing a deterministic scalar score without quantifying uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Accuracy (single-trial)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Run one evaluation trial per input and compute the fraction of correct responses (binary scoring). Produces a single scalar score representing empirical accuracy on the sampled queries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Point estimate of correctness rate; no direct quantification of uncertainty or distribution across trials.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general/multi-domain (applied to the curated question set)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Accuracy reporting gives a single score but lacks statistical interpretation on reliability. The paper shows accuracy can obscure performance hierarchies resolvable by Bayesian analysis (e.g., two models both achieving 65% accuracy could be distinguished by Bayesian interval posteriors).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (single-run metric).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared empirically to Bayesian and mean±std and Pass@N on the curated question set across multiple test models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Does not quantify uncertainty; vulnerable to stochastic output noise; limited discriminatory power particularly under small-sample regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Curated multi-source question set used in the paper (evaluated at M=20 in method comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4562.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4562.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mean±std reporting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mean and standard error / standard deviation reporting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Report the sample mean and standard deviation (or standard error) of model accuracy across multiple independent trials to present central tendency and trial-to-trial variability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Mean ± std (multi-trial)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Perform O independent trials per input (or per model on dataset), compute mean accuracy and standard deviation/standard error across trials to present a central estimate and variability range (e.g., mean ± std).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Central tendency (mean accuracy) and trial variability (std/error); overlapping intervals sometimes used to assess indistinguishability between models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general/multi-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Mean±std provides partial statistical transparency but can fail to resolve performance disparities: models with overlapping variances (means near 60% with overlapping std) appeared comparable, while Bayesian analysis revealed categorical disparities (e.g., placing Llama-4-Maverick below other models despite similar means). Qwen2.5-72B showed the largest std in experiments, indicating high trial variability.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (statistical summary across repeated runs).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical comparison to Bayesian posteriors and Pass@N on the curated question set with O=10 trials.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Overlap of mean ± std intervals does not provide direct probabilistic statements about relative ordering; does not incorporate prior knowledge; can mask structured differences that Bayesian interval inference reveals.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to curated multi-source question set (M=20) in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4562.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4562.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rasch / IRT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rasch model (Item Response Theory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classic item response theory model used in educational measurement to estimate latent ability and item difficulty using probabilistic models relating latent traits to item response probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Probabilistic models for some intelligence and attainment tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Rasch model (IRT)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Model the probability of a correct response as a logistic function of the difference between a subject's latent ability and an item's difficulty parameter; typically assumes particular parametric forms and often Gaussian assumptions over populations for inference.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Latent ability estimates, item difficulty parameters, fit to logistic/parametric IRT assumptions; measurement invariance across items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>educational measurement / psychometrics (discussed as a related approach to LLM evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>latent-trait measurement</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper discusses IRT/Rasch as a related probabilistic strategy but highlights limitations for LLM evaluation: IRT typically assumes distributional forms (e.g., Gaussian latent traits) and requires assumptions about test-taker populations that may be invalid for limited numbers of LLMs. The authors argue their Bayesian approach relaxes such distributional constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated/statistical (but developed for human test populations); here referenced only.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not applied in the paper; discussed conceptually and contrasted with the proposed Bayesian method.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires distributional assumptions about latent traits (often Gaussian) and enough test-takers/models to justify central-limit-type reasoning; may be ill-suited when anchor/model populations are small or non-Gaussian.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4562.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4562.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elo (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elo rating system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pairwise-comparison rating system originally designed for competitive games that updates player ratings based on head-to-head outcomes to model pairwise win probabilities and rank participants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Elo rating (conceptual for LLM evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Iteratively update model ratings from many pairwise comparisons, modeling the probability one model outperforms another; in principle it can capture nuanced performance trajectories from head-to-head outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pairwise win probabilities aggregated into rating scores; temporal dynamics of rating adjustments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general model ranking / comparative evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>pairwise comparative ranking</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Mentioned as a sophisticated probabilistic approach. The paper notes practical deployment is hindered for LLM evaluation because Elo would require extensive human-labeled comparisons across thousands of queries, making it prohibitively costly in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Typically hybrid/human-in-the-loop when used for LLMs (relies on many pairwise judgments); paper treats it conceptually and does not implement it.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not implemented in the paper; discussed as related work with practical cost limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>High human-labeling cost for sufficient pairwise comparisons; scalability issues for large model populations and diverse tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating large language models trained on code <em>(Rating: 2)</em></li>
                <li>Probabilistic models for some intelligence and attainment tests <em>(Rating: 2)</em></li>
                <li>Towards reproducible llm evaluation: Quantifying uncertainty in llm benchmark scores <em>(Rating: 2)</em></li>
                <li>Adding error bars to evals: A statistical approach to language model evaluations <em>(Rating: 2)</em></li>
                <li>Chatbot arena: An open platform for evaluating llms by human preference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4562",
    "paper_id": "paper-a6cad00e214c0f3ccdb0145bc4c384475d8b9828",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Bayesian evaluation",
            "name_full": "Bayesian capability-ranking evaluation for LLMs",
            "brief_description": "A probabilistic evaluation framework that treats an LLM's latent capability as a continuous latent variable and performs Bayesian inference over mutually exclusive capability intervals using priors derived from anchor-model response distributions and a curated discriminative query set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Bayesian capability-ranking evaluation",
            "evaluation_method_description": "Treat each model's capability θ as a latent scalar. Use a curated set Q of M discriminative binary queries and empirical conditional probabilities Pr(Q_j=1 | L_i) from N anchor models to form priors and likelihoods. Partition capability space into N+1 mutually exclusive intervals determined by anchor model cumulative success rates (θ_1..θ_N). Assume a uniform prior over θ within each interval (maximum entropy) so prior probability of interval ∝ (θ_{i+1}-θ_i). Under conditional independence of queries, compute likelihood of observed binary outcomes q (single-trial) or K successes out of O trials per query (multi-trial Bernoulli) by averaging the anchor-boundary probabilities: Pr(Q_j=q_j | θ in interval) ≈ (Pr(Q_j|L_i) + Pr(Q_j|L_{i+1}))/2, and multiply across j. Normalize to get posterior Pr(interval | q). The method outputs probabilistic interval membership (e.g., probability model lies between anchor i and i+1), enabling statements like \"Model X has p% probability of exceeding baseline Y.\" Implements numerical stabilizers (ε-adjustment for observed 0/1 probabilities) and supports single-trial (Bayes@1) and multi-trial aggregation (Bayes@10).",
            "evaluation_criteria": "Probabilistic interval membership (likelihood of belonging to capability intervals defined by anchors); discriminative power between anchors; statistical robustness under limited sample sizes; posterior probability sharpness (confidence); consistency across trial aggregations (single vs multi-trial).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "multi-domain (questions span sciences, engineering, mathematics, humanities, arts, reasoning)",
            "theory_type": "not applicable (evaluates model capability/performance ranking rather than specific scientific theories)",
            "human_comparison": false,
            "evaluation_results": "The Bayesian method achieved superior discrimination vs. conventional metrics in the paper's experiments. Examples: anchor cumulative success rates ranged ~14% to 82% across six GPT-series anchors; the method retained stable interval identification down to M=20 questions (≥~65% confidence for most likely interval), with sharpness attenuating as M decreased to 10 or 5 (peak interval probabilities fell below 50% at very small M). Multi-trial Bayes@10 improved interval precision compared to Bayes@1. The approach produced actionable probabilistic statements (e.g., probability of exceeding specific baselines) and resolved ambiguities that accuracy/Pass@N/mean±std obscured (paper gives multiple model-specific comparisons showing Bayesian intervals disagree with naive mean±std or Pass@10 rankings).",
            "automated_vs_human_evaluation": "Automated/statistical: purely model-output-driven using measured response frequencies from anchor models and the test model; no human raters were used for the core posterior computations (though the paper references human-preference platforms in related work).",
            "validation_method": "Empirical experiments comparing Bayesian posterior interval assignments to conventional metrics (accuracy, Pass@N, mean±std) across varying query budgets M (50,30,20,10,5) and single vs multi-trial regimes (O up to 10); robustness analysis (how posterior sharpness changes with M); demonstration on several test models and anchor sets (GPT-series anchors, multiple open-source test models).",
            "limitations_challenges": "Requires representative, high-quality anchor models and discriminative query sets; assumes conditional independence of queries (may fail for correlated items); binary scoring limits applicability to open-ended tasks; interval priors rely on monotonic anchor ordering which may be disrupted by new architectures; extreme observed probabilities require ε-adjustment; does not model question interdependencies or multi-dimensional rubrics without extension.",
            "benchmark_dataset": "A curated multi-source query set assembled from superGPQA, MMLU-Pro, GPQA-Diamond, MATH, ZebraLogic, KOR-Bench, and Procbench: initially 170 items then reduced to a 50-question set (further ablations to 30/20/10/5); the refined set was syntactically paraphrased to reduce memorization.",
            "uuid": "e4562.0",
            "source_info": {
                "paper_title": "Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Bayes@10 (multi-trial)",
            "name_full": "Multi-trial Bayesian aggregation (Bayes@O, e.g., Bayes@10)",
            "brief_description": "An extension of the Bayesian evaluation that aggregates O independent trials per query by modeling per-query outcomes as Binomial/Bernoulli experiments and using the binomial likelihood for anchor boundary probabilities, improving precision over single-trial inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Multi-trial Bayesian aggregation (Bayes@O)",
            "evaluation_method_description": "For each query Q_j, collect O independent trials yielding K successes. For each anchor L_i, estimate Pr(vec Q_j = vec q_j | L_i) = C(O,K) * p_i^K * (1-p_i)^(O-K) where p_i = Pr(Q_j=1 | L_i). For an interval (θ_i,θ_{i+1}], approximate likelihood as average of the two boundary anchors' binomial likelihoods. Multiply across queries, apply uniform-θ interval priors, normalize to posterior interval probabilities. Aggregation across trials reduces variance and sharpens posterior distributions relative to single-trial Bayes@1.",
            "evaluation_criteria": "Posterior interval probability sharpness; reduction in uncertainty with increasing O; stability and consistency of categorical assignments across trials.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "multi-domain (same curated question set as Bayesian evaluation)",
            "theory_type": "n/a",
            "human_comparison": false,
            "evaluation_results": "Bayes@10 provided superior interval precision vs Bayes@1 in experiments. The paper reports Bayes@10 maintains consistent category estimation boundaries across models (except for particularly variable models like Qwen2.5-72B) and reduces ambiguity compared to single-trial inference.",
            "automated_vs_human_evaluation": "Automated (statistical aggregation of repeated model outputs).",
            "validation_method": "Empirical comparison between Bayes@1 and Bayes@10 across the curated query set with O=10 trials per question; assessments of interval probability sharpness and resolution under reduced M.",
            "limitations_challenges": "Requires the ability to run multiple independent trials (cost/time); sensitive to trial-to-trial variability of the evaluated model (high variance models may still produce wide posteriors); still relies on anchor quality and independence assumptions.",
            "benchmark_dataset": "Same curated multi-source query set; experiments used O=10 trials per anchor/model.",
            "uuid": "e4562.1",
            "source_info": {
                "paper_title": "Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Pass@N",
            "name_full": "Pass@N (upper-bound performance aggregation)",
            "brief_description": "A performance metric that reports whether at least one of N sampled outputs for a given input passes an evaluation test, intended to approximate a model's upper-bound or best-case output quality across stochastic decoding.",
            "citation_title": "Evaluating large language models trained on code",
            "mention_or_use": "use",
            "evaluation_method_name": "Pass@N",
            "evaluation_method_description": "Generate N independent outputs per input (using stochastic decoding); count an input as successful if any of the N outputs is correct; aggregate success rate across inputs to estimate the model's best-case performance over N samples. Often used for problems with multiple possible valid outputs (e.g., code generation).",
            "evaluation_criteria": "Best-case/upper-bound pass rate across N samples; emphasizes rare high-quality outputs rather than consistent reliability.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general NLP benchmarks and code generation contexts (used here as a comparison metric on the curated question set)",
            "theory_type": "n/a",
            "human_comparison": false,
            "evaluation_results": "In the paper, Pass@10 inflated overall accuracy metrics while reducing discernibility between models; it sometimes suggested parity where Bayesian analysis found categorical differences (e.g., Pass@10 suggested Llama-4-Maverick comparable to DeepSeek-R1, contradicting Bayesian rankings).",
            "automated_vs_human_evaluation": "Automated (metric computed from multiple sampled outputs).",
            "validation_method": "Compared empirically against Bayesian posteriors and mean±std reporting using O=10 trials and M=20 questions; differences in model discrimination and interpretability were highlighted.",
            "limitations_challenges": "No statistical interpretation of uncertainty; sensitive to decoding hyperparameters (temperature, top-k/p); emphasizes rare best-case outputs and can obscure consistent performance differences; may reduce discriminability between models.",
            "benchmark_dataset": "Used as a comparative metric on the paper's curated multi-source question set (M=20 used in method comparison).",
            "uuid": "e4562.2",
            "source_info": {
                "paper_title": "Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Accuracy reporting",
            "name_full": "Single-trial accuracy reporting",
            "brief_description": "The simple metric of reporting the fraction of correct answers from a single evaluation pass over a dataset (one trial per question), producing a deterministic scalar score without quantifying uncertainty.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Accuracy (single-trial)",
            "evaluation_method_description": "Run one evaluation trial per input and compute the fraction of correct responses (binary scoring). Produces a single scalar score representing empirical accuracy on the sampled queries.",
            "evaluation_criteria": "Point estimate of correctness rate; no direct quantification of uncertainty or distribution across trials.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general/multi-domain (applied to the curated question set)",
            "theory_type": "n/a",
            "human_comparison": false,
            "evaluation_results": "Accuracy reporting gives a single score but lacks statistical interpretation on reliability. The paper shows accuracy can obscure performance hierarchies resolvable by Bayesian analysis (e.g., two models both achieving 65% accuracy could be distinguished by Bayesian interval posteriors).",
            "automated_vs_human_evaluation": "Automated (single-run metric).",
            "validation_method": "Compared empirically to Bayesian and mean±std and Pass@N on the curated question set across multiple test models.",
            "limitations_challenges": "Does not quantify uncertainty; vulnerable to stochastic output noise; limited discriminatory power particularly under small-sample regimes.",
            "benchmark_dataset": "Curated multi-source question set used in the paper (evaluated at M=20 in method comparisons).",
            "uuid": "e4562.3",
            "source_info": {
                "paper_title": "Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Mean±std reporting",
            "name_full": "Mean and standard error / standard deviation reporting",
            "brief_description": "Report the sample mean and standard deviation (or standard error) of model accuracy across multiple independent trials to present central tendency and trial-to-trial variability.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Mean ± std (multi-trial)",
            "evaluation_method_description": "Perform O independent trials per input (or per model on dataset), compute mean accuracy and standard deviation/standard error across trials to present a central estimate and variability range (e.g., mean ± std).",
            "evaluation_criteria": "Central tendency (mean accuracy) and trial variability (std/error); overlapping intervals sometimes used to assess indistinguishability between models.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general/multi-domain",
            "theory_type": "n/a",
            "human_comparison": false,
            "evaluation_results": "Mean±std provides partial statistical transparency but can fail to resolve performance disparities: models with overlapping variances (means near 60% with overlapping std) appeared comparable, while Bayesian analysis revealed categorical disparities (e.g., placing Llama-4-Maverick below other models despite similar means). Qwen2.5-72B showed the largest std in experiments, indicating high trial variability.",
            "automated_vs_human_evaluation": "Automated (statistical summary across repeated runs).",
            "validation_method": "Empirical comparison to Bayesian posteriors and Pass@N on the curated question set with O=10 trials.",
            "limitations_challenges": "Overlap of mean ± std intervals does not provide direct probabilistic statements about relative ordering; does not incorporate prior knowledge; can mask structured differences that Bayesian interval inference reveals.",
            "benchmark_dataset": "Applied to curated multi-source question set (M=20) in comparisons.",
            "uuid": "e4562.4",
            "source_info": {
                "paper_title": "Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Rasch / IRT",
            "name_full": "Rasch model (Item Response Theory)",
            "brief_description": "A classic item response theory model used in educational measurement to estimate latent ability and item difficulty using probabilistic models relating latent traits to item response probabilities.",
            "citation_title": "Probabilistic models for some intelligence and attainment tests",
            "mention_or_use": "mention",
            "evaluation_method_name": "Rasch model (IRT)",
            "evaluation_method_description": "Model the probability of a correct response as a logistic function of the difference between a subject's latent ability and an item's difficulty parameter; typically assumes particular parametric forms and often Gaussian assumptions over populations for inference.",
            "evaluation_criteria": "Latent ability estimates, item difficulty parameters, fit to logistic/parametric IRT assumptions; measurement invariance across items.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "educational measurement / psychometrics (discussed as a related approach to LLM evaluation)",
            "theory_type": "latent-trait measurement",
            "human_comparison": false,
            "evaluation_results": "Paper discusses IRT/Rasch as a related probabilistic strategy but highlights limitations for LLM evaluation: IRT typically assumes distributional forms (e.g., Gaussian latent traits) and requires assumptions about test-taker populations that may be invalid for limited numbers of LLMs. The authors argue their Bayesian approach relaxes such distributional constraints.",
            "automated_vs_human_evaluation": "Automated/statistical (but developed for human test populations); here referenced only.",
            "validation_method": "Not applied in the paper; discussed conceptually and contrasted with the proposed Bayesian method.",
            "limitations_challenges": "Requires distributional assumptions about latent traits (often Gaussian) and enough test-takers/models to justify central-limit-type reasoning; may be ill-suited when anchor/model populations are small or non-Gaussian.",
            "benchmark_dataset": "",
            "uuid": "e4562.5",
            "source_info": {
                "paper_title": "Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Elo (mentioned)",
            "name_full": "Elo rating system",
            "brief_description": "A pairwise-comparison rating system originally designed for competitive games that updates player ratings based on head-to-head outcomes to model pairwise win probabilities and rank participants.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "Elo rating (conceptual for LLM evaluation)",
            "evaluation_method_description": "Iteratively update model ratings from many pairwise comparisons, modeling the probability one model outperforms another; in principle it can capture nuanced performance trajectories from head-to-head outcomes.",
            "evaluation_criteria": "Pairwise win probabilities aggregated into rating scores; temporal dynamics of rating adjustments.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general model ranking / comparative evaluation",
            "theory_type": "pairwise comparative ranking",
            "human_comparison": false,
            "evaluation_results": "Mentioned as a sophisticated probabilistic approach. The paper notes practical deployment is hindered for LLM evaluation because Elo would require extensive human-labeled comparisons across thousands of queries, making it prohibitively costly in practice.",
            "automated_vs_human_evaluation": "Typically hybrid/human-in-the-loop when used for LLMs (relies on many pairwise judgments); paper treats it conceptually and does not implement it.",
            "validation_method": "Not implemented in the paper; discussed as related work with practical cost limitations.",
            "limitations_challenges": "High human-labeling cost for sufficient pairwise comparisons; scalability issues for large model populations and diverse tasks.",
            "benchmark_dataset": "",
            "uuid": "e4562.6",
            "source_info": {
                "paper_title": "Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 2
        },
        {
            "paper_title": "Probabilistic models for some intelligence and attainment tests",
            "rating": 2
        },
        {
            "paper_title": "Towards reproducible llm evaluation: Quantifying uncertainty in llm benchmark scores",
            "rating": 2
        },
        {
            "paper_title": "Adding error bars to evals: A statistical approach to language model evaluations",
            "rating": 2
        },
        {
            "paper_title": "Chatbot arena: An open platform for evaluating llms by human preference",
            "rating": 1
        }
    ],
    "cost": 0.016448249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CONFIDENCE IN LARGE LANGUAGE MODEL EVALUATION: A BAYESIAN APPROACH TO LIMITED-SAMPLE CHALLENGES</h1>
<p>Xiao Xiao, Yu Su, Sijing Zhang, Zhang Chen, Yadong Chen, Tian Liu*<br>Tencent Hunyuan</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) exhibit probabilistic output characteristics, yet conventional evaluation frameworks rely on deterministic scalar metrics. This study introduces a Bayesian approach for LLM capability assessment that integrates prior knowledge through probabilistic inference, addressing limitations under limited-sample regimes. By treating model capabilities as latent variables and leveraging a curated query set to induce discriminative responses, we formalize model ranking as a Bayesian hypothesis testing problem over mutually exclusive capability intervals. Experimental evaluations with GPT-series models demonstrate that the proposed method achieves superior discrimination compared to conventional evaluation methods. Results indicate that even with reduced sample sizes, the approach maintains statistical robustness while providing actionable insights, such as probabilistic statements about a model's likelihood of surpassing specific baselines. This work advances LLM evaluation methodologies by bridging Bayesian inference with practical constraints in real-world deployment scenarios.</p>
<h2>1 Introduction</h2>
<p>Contemporary large language models (LLMs) [Brown et al., 2020][Ouyang et al., 2022][Radford et al., 2018][Radford et al., 2019][Zhao et al., 2023] exhibit probabilistic output characteristics, yet current evaluation systems often employ deterministic methodologies for effectiveness determination. Such assessment mechanisms generate scalar scores as quantitative measures, theoretically grounded in the assumption of a unidimensional latent variable (analogous to the concept of intelligence quotient in humans) that supposedly underlies model capabilities. This quantitative metric then serves as the basis for cross-model comparisons and performance rankings.
This methodological approach occasionally presents a paradoxical evaluation challenge. Consider two LLM A and B, administering an exam comprising two questions X and Y. Model A correctly answers only X, whereas model B correctly answers only Y, both achieving $50 \%$ accuracy. An uninformed evaluator might equate their performance, perceiving equivalence in competence. However, in most practical settings, questions are administered to multiple LLMs, yielding empirical data on their discriminative capability. For instance, question X is answered correctly by $90 \%$ of models, while question Y is solved by only $20 \%$. With this context, the evaluation may shift. This raises critical questions about comparative assessment: How should we quantify the likelihood that one model surpasses the other, given their differential performance on a limited number of evaluation tasks?
There are several potential strategies to address this challenge. First, assigning differential score weights to questions X and Y based on their perceived difficulty level, though such scoring remains inherently</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>subjective and fails to account for the statistical properties of LLMs. Second, Pass@N frameworks [Chen et al., 2021][Chowdhery et al., 2022][Touvron et al., 2023] assess LLMs' upper-bound performance but face implementation challenges due to unspecified decoding strategies (e.g., temperature, top-k, top-p sampling), often requiring tradeoffs between optimizing rare high-quality outputs and ensuring consistent performance. Third, repeated testing with mean and standard error reporting [Blackwell et al., 2024][Miller, 2024] offers a statistical approach but neglects prior knowledge integration.
Educational research employs Rasch models [Rasch, 1993]-a specialized item response theory (IRT) tool-to estimate latent student ability through probabilistic measurement. However, determining question difficulty level in IRT often needs distribution assumptions about the test takers capability that is unknown a priori in LLMs [Polo et al., 2024][Truong et al., 2025]. Elo rating systems-a method originally designed to rank players in competitive games by modeling pairwise outcome probabilities-represent another sophisticated probabilistic approach. By iteratively adjusting scores based on head-to-head comparisons, Elo could theoretically capture nuanced performance trajectories. However, its practical deployment for LLM evaluation is hindered by prohibitive costs, requiring extensive human-labeled comparisons on thousands of queries [Chiang et al., 2024]. In practice, constrained sample sizes often limit the ability of typical practitioners or end-users to reliably assess model capabilities through empirical evaluation.
In this study, we present a Bayesian approach that incorporates prior knowledge when assessing a model's latent capabilities. The referenced prior knowledge specifically refers to the response distribution patterns exhibited by diverse LLMs across a curated question dataset. In scenarios where prior information is unavailable, our methodology rigorously adheres to the principle of maximum entropy, a foundational concept in statistical inference that ensures the derivation of statistically optimal and objectively calibrated estimates under conditions of uncertainty.</p>
<h1>2 Bayesian Formulation</h1>
<h3>2.1 Problem Statement</h3>
<p>We begin by establishing the mathematical foundation for model ranking. Let a set of $N$ anchor models be defined as $\mathcal{L}=\left{L_{1}, L_{2}, \ldots, L_{N}\right}$. For each $L_{i} \in \mathcal{L}$, its performance on real world tasks is determined by an underlying capability parameter $\theta_{i} \in \mathbb{R}^{+}$, which is analogous to human IQ. Without loss of generality, we assume these capabilities follow a monotonically increasing order such that $\theta_{1}&lt;\theta_{2}&lt;\cdots&lt;\theta_{N}$. For an out-of-set test model $L_{x} \notin \mathcal{L}$, our objective is to estimate its probabilistic membership across $N+1$ mutually exclusive ranking intervals: $\theta_{x} \leq \theta_{1}, \theta_{i}&lt;\theta_{x} \leq \theta_{i+1}$ for $i \in{1,2, \ldots, N-1}$ and $\theta_{N}&lt;\theta_{x}$ through systematic experimentation.
To enable capability discrimination, consider a curated query set $Q$ comprising $M$ discriminative queries representing real-world operational scenarios. When applied to a model, each query $Q_{j}$ constitutes a binary random variable $\left(Q_{j} \in{0,1}\right)$ corresponding to incorrect/correct model responses, and the entire query set constitutes a readout of binary sequence $\left{Q_{1}, Q_{2}, \ldots, Q_{M}\right}$. The design of the curated query set aims to induce measurable differentiation between the anchor models with different latent capabilities.</p>
<h3>2.2 Model ranking via Bayesian inference</h3>
<p>Given an anchor model set $\mathcal{L}$ and query set $Q$, the conditional probability of correct response for each anchor model $L_{i}(i \in{1,2, \ldots, N})$ on query $Q_{j}(j \in{1,2, \ldots, M})$ is expressed as $\operatorname{Pr}\left(Q_{j}=1 \mid L_{i}\right)$, which is empirically measurable through controlled experiments. To facilitate subsequent derivations, we introduce two boundary anchor models: $L_{0}$ with $\theta_{0}$ and $L_{N+1}$ with $\theta_{N+1}$, where $\forall j \in{1,2, \ldots, M}, \operatorname{Pr}\left(Q_{j}=1 \mid L_{0}\right)=0$ and $\operatorname{Pr}\left(Q_{j}=1 \mid L_{N+1}\right)=1$.
Following evaluation of test model $L_{x}$ on query set $Q$, we observe binary outcomes $q=\left{q_{1}, q_{2}, \ldots, q_{M}\right}$. The ranking problem constitutes Bayesian inference over mutually exclusive hypothesis spaces:</p>
<p>$$
\operatorname{Pr}\left(\theta_{i}&lt;\theta_{x} \leq \theta_{i+1} \mid Q=q\right)=\frac{\operatorname{Pr}\left(Q=q \mid \theta_{i}&lt;\theta_{x} \leq \theta_{i+1}\right) \operatorname{Pr}\left(\theta_{i}&lt;\theta_{x} \leq \theta_{i+1}\right)}{\operatorname{Pr}(Q=q)} \quad \text { for } i \in{0,1, \ldots, N}
$$</p>
<h1>2.3 Parameter Estimation</h1>
<p>Computation of posterior probabilities in Equation 1 requires estimation of three components on the right-hand side, denoted as $\operatorname{Pr}<em i="i">{\text {est }}\left(\theta</em>}&lt;\theta_{x} \leq \theta_{i+1}\right), \operatorname{Pr<em i="i">{\text {est }}\left(Q=q \mid \theta</em>(Q)$ for $i \in{0,1, \ldots, N}$.
First, following the principle of maximum entropy[Jaynes, 2003] without assuming specific $\theta$ distributions:}&lt;\theta_{x} \leq \theta_{i+1}\right)$, and $\operatorname{Pr}_{\text {est }</p>
<p>$$
\operatorname{Pr}<em i="i">{\mathrm{est}}\left(\theta</em>\right)
$$}&lt;\theta_{x} \leq \theta_{i+1}\right) \propto\left(\theta_{i+1}-\theta_{i</p>
<p>This ensures uniform prior distribution of $\theta$.
Second, under conditional query independence assumption, the joint likelihood decomposes as:</p>
<p>$$
\operatorname{Pr}<em i="i">{\mathrm{est}}\left(Q=q \mid \theta</em>}&lt;\theta_{x} \leq \theta_{i+1}\right)=\prod_{j=1}^{M} \operatorname{Pr<em j="j">{\mathrm{est}}\left(Q</em>\right)
$$}=q_{j} \mid \theta_{i}&lt;\theta_{x} \leq \theta_{i+1</p>
<p>Conditional probabilities are estimated through:</p>
<p>$$
\operatorname{Pr}<em j="j">{\mathrm{est}}\left(Q</em>
$$}=q_{j} \mid \theta_{i}&lt;\theta_{x} \leq \theta_{i+1}\right)=\frac{\operatorname{Pr}\left(Q_{j}=q_{j} \mid L_{i}\right)+\operatorname{Pr}\left(Q_{j}=q_{j} \mid L_{i+1}\right)}{2</p>
<p>Eq. 4 again follows the principle of maximum entropy, which postulates that, in the absence of further information about the distribution of $\theta$, the most unbiased assumption is that $\theta$ is uniformly distributed in the interval $\left(\theta_{i}, \theta_{i+1}\right]$. Under this assumption, the linear relationship between $\theta$ and the probability of answer correctness leads to the simplest and most symmetric solution, which is the average of the probabilities at the interval boundaries.
Finally, $\operatorname{Pr}_{\text {est }}(Q=q)$ serves as a normalization factor ensuring probability conservation:</p>
<p>$$
\operatorname{Pr}<em i="0">{\mathrm{est}}(Q=q)=\sum</em>}^{N} \operatorname{Pr<em i="i">{\mathrm{est}}\left(Q=q \mid \theta</em>}&lt;\theta_{x} \leq \theta_{i+1}\right) \operatorname{Pr<em i="i">{\mathrm{est}}\left(\theta</em>\right)
$$}&lt;\theta_{x} \leq \theta_{i+1</p>
<h2>3 Experimental Setup</h2>
<h3>3.1 Anchor Model Selection</h3>
<p>In this study, we chose the GPT-series as our anchor models due to their established recognition. Specifically, our anchor models include GPT-3.5 Turbo [OpenAI, 2023], GPT-4 [OpenAI, 2023], GPT-4o [OpenAI, 2024], GPT-4.5 [OpenAI, 2025], o1 [OpenAI, 2024], and o3-mini-high [OpenAI, 2025]. The temporal progression of model releases creates an implicit capability hierarchy, with subsequent iterations expecting enhanced performance profiles.
Nevertheless, existing research indicates that chain-of-thought reasoning models typically outperform generalpurpose foundation models on benchmark ranking tasks (e.g., Chatbot Arena, SuperCLUE) [Chiang et al., 2024][Xu et al., 2023], while distilled variants (i.e., Turbo and mini series) may exhibit performance degradation relative to their full-scale counterparts [Gu et al., 2023][Wu et al., 2024]. To empirically validate the actual capability ordering within this anchor model cohort, we conducted evaluations on our curated query set rather than relying solely on release-order assumptions.</p>
<h3>3.2 Query Set Construction</h3>
<p>To construct the query set, we initially assembled 170 questions by selecting 30 items each from superGPQA [M-A-P Team et al., 2025], MMLU-Pro [Wang et al., 2024], GPQA-Diamond [Rein et al., 2023], MATH [Hendrycks et al., 2021], and ZebraLogic [Lin et al., 2025], along with 10 questions each from KOR-Bench</p>
<p>[Ma et al., 2024] and Procbench [Fujisawa et al., 2024]. The selection process followed two criteria. First, the initial pool ensured comprehensive task coverage across material science, earth and environmental sciences, engineering and technology, chemistry, medicine, biology, humanities, art, mathematics, physics, and reasoning. Second, based on previous experimental results from model series released in early 2025 ([Anthropic, 2025] [Google DeepMind, 2025] [Li et al., 2025] [Doubao Team, 2025] [Qwen Team, 2025]), we implemented discriminative query selection such that no less than one but no more than half of these models could correctly answer each question.
To emphasize the capability ranking efficacy of our Bayesian evaluation approach even with reduced question sets, we subsequently reduced the query set from 170 to 50 through systematic triage while maintaining source diversity, task diversity, and anchor model performance diversity. This refined set underwent syntactic paraphrasing [Dong et al., 2023][Yin et al., 2024] to mitigate recently reported memorization susceptibility in language models [Cheng et al., 2025][Deng et al., 2023][Golchin and Surdeanu, 2023].</p>
<h1>3.3 Bayesian Ranking</h1>
<p>The evaluation employed five open-source language models-Llama-4-Maverick [Meta, 2025], DeepSeek-V3-0324 [DeepSeek-AI, 2025], DeepSeek-R1 [DeepSeek-AI, 2025], QwQ-32B [Qwen Team, 2025] and Qwen2.5-72B [Qwen Team, 2025]-as representative test models. For each test model, evaluations were conducted on the curated query set to derive binary outcome measures. These outcome indicators underwent subsequent mathematical calculations detailed in Section 2 to generate probabilistic ranking scores.
Specifically regarding prior knowledge acquisition, we repeatedly tested each anchor model $L_{i}$ on each question $Q_{j}$ for $O=10$ times and recorded the number of successes to calculate the conditional probability $\operatorname{Pr}\left(Q_{j} \mid L_{i}\right)$. To ensure numerical stability during calculation, we introduced $\epsilon$-adjustment mechanisms ( $\epsilon=0.01$ ) that apply $\pm 1 \%$ boundary modulation when observing extremal probability values $\left(\operatorname{Pr}\left(Q_{j} \mid L_{i}\right) \in{0,1}\right)$, thereby preventing division-by-zero errors in subsequent computations. This information was then systematically organized in a capability matrix $\Pi \in \mathbb{R}^{N \times M}$ :</p>
<p>$$
\Pi=\left[\begin{array}{cccc}
P\left(Q_{1} \mid L_{1}\right) &amp; P\left(Q_{2} \mid L_{1}\right) &amp; \cdots &amp; P\left(Q_{M} \mid L_{1}\right) \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
P\left(Q_{1} \mid L_{N}\right) &amp; P\left(Q_{2} \mid L_{N}\right) &amp; \cdots &amp; P\left(Q_{M} \mid L_{N}\right)
\end{array}\right]
$$</p>
<p>The comparative ranking among anchor models was ultimately determined through cumulative success rates aggregated across the entire question set, with $\theta_{i}$ representing the cumulative success rate for $L_{i}$. Optimal anchor model-query set pairing produces uniform interval sizes $\left(\theta_{i+1}-\theta_{i}\right)$ that ensure consistent discriminatory power across the entire capability spectrum.
Furthermore, we conducted additional experiments by reducing the query set size to $30,20,10$, and 5 to systematically evaluate the robustness of the proposed methods.</p>
<h3>3.4 Methods comparison</h3>
<p>We compare the Bayesian method with three other commonly employed evaluation methods, namely: simple accuracy reporting, Pass@N reporting, and mean $\pm$ std reporting.
While accuracy reporting requires a single trial of evaluating a test model on the query set, the Pass@N selection method and mean $\pm$ std reporting protocol require multiple trials. Therefore, we conducted $O=10$ independent trials on each evaluated model $L_{x}$ against the query set $Q$.
In the proposed Bayesian evaluation approach, we integrate multi-trial observations through stochastic modeling of the query $\tilde{Q}<em j="j">{j}=\left{Q</em>}^{(1)}, Q_{j}^{(2)}, \ldots, Q_{j}^{(O)}\right}$ as a collection of independent Bernoulli trials, where $O$ represents the total number of trials conducted for query $Q_{j}$. For the evaluation protocol, the corresponding measurement is defined as $\tilde{q<em j="j">{j}=\left{q</em>\right}$, containing $K$ correct outcomes. This formulation generalizes Eq. 4 to accommodate multiple trials:}^{(1)}, q_{j}^{(2)}, \ldots, q_{j}^{(O)</p>
<p>$$
\operatorname{Pr}<em j="j">{\mathrm{est}}\left(\vec{Q}</em>}=\vec{q<em i="i">{j} \mid \theta</em>}&lt;\theta_{x} \leq \theta_{i+1}\right)=\frac{\operatorname{Pr<em j="j">{\mathrm{est}}\left(\vec{Q}</em>}=\vec{q<em i="i">{j} \mid L</em>}\right)+\operatorname{Pr<em j="j">{\mathrm{est}}\left(\vec{Q}</em>}=\vec{q<em i_1="i+1">{j} \mid L</em>
$$}\right)}{2</p>
<p>where the likelihood function for each anchor model $L_{i}$ under the multi-trial paradigm is estimated as:</p>
<p>$$
\operatorname{Pr}<em j="j">{\mathrm{est}}\left(\vec{Q}</em>}=\vec{q<em i="i">{j} \mid L</em>
$$}\right)=\binom{O}{K} \operatorname{Pr}\left(Q_{j}=1 \mid L_{i}\right)^{K} \operatorname{Pr}\left(Q_{j}=0 \mid L_{i}\right)^{(O-K)</p>
<p>This generalized formulation explicitly incorporates the multi-trial nature of the evaluation process. Notably, when $O=1$, Equation 7 analytically reduces to the single-trial probability expression, thereby establishing mathematical consistency between single-trial and multi-trial evaluation regimes.</p>
<h1>4 Results</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Anchor Model Performance
(a) Success rates of the six anchor models (measured over $O=10$ trials per question) for $M=50$ evaluation questions. (b) The success rate of the first 20 questions are shown here, where the complete success rate distribution is provided in Appendix. Extreme probability values ${0 \%, 100 \%}$ were modulated to ${1 \%, 99 \%}$ to ensure numerical stability during subsequent computations.</p>
<p>The capability matrix was systematically visualized in Figure 1, where anchor model performance was shown as their respective probability of correctly answering the questions. Anchor models demonstrated quasiuniform spacing in their cumulative success rate ( $14 \%$ to $82 \%$ ), revealing effective capability differentiation within the query set (complete query set provided in Appendix B).
Notably, model performance exhibited temporal progression aligned with release timelines (GPT-3.5 turbo: $14 \%$ vs. o1: $82 \%$ ). However, reasoning-enhanced systems (o1/o3-mini-high) outperformed non-reasoning models in general, and o3-mini-high—a purported distilled variant of o3-showed degraded performance despite its later release relative to o1.
Bayesian inference positioned DeepSeek-R1 between o3-mini-high and o1 when evaluated with the full query set (Fig. 2). Knowledge-intensive domains (sciences/humanities/arts) emerged as performance differentiators, where ol demonstrated superior and more consistent performance relative to DeepSeek-R1. Robustness analysis confirmed stable interval identification down to $\mathrm{M}=20$ questions, though probability sharpness progressively attenuated. When the number of questions is reduced to $\mathrm{M}=10$ and less, the peak interval probability fell below $50 \%$, accompanied by increased ambiguity across adjacent categories.
Figure 2 also showed more validations with additional test models (DeepSeek-V3-0324, QwQ-32B, Qwen2.572B, Llama-4-Maverick) corroborating this pattern: full query sets enabled unambiguous capability localiza-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Anch</th>
<th style="text-align: center;">Interval</th>
<th style="text-align: center;">$M=5$</th>
<th style="text-align: center;">$M=10$</th>
<th style="text-align: center;">$M=20$</th>
<th style="text-align: center;">$M=30$</th>
<th style="text-align: center;">$M=50$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Higher</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$17 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$2 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">31\%</td>
<td style="text-align: center;">35\%</td>
<td style="text-align: center;">65\%</td>
<td style="text-align: center;">96\%</td>
</tr>
<tr>
<td style="text-align: center;">o3-mini-high</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">14\%</td>
<td style="text-align: center;">48\%</td>
<td style="text-align: center;">33\%</td>
<td style="text-align: center;">4\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4.5-preview</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16\%</td>
<td style="text-align: center;">11\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">15\%</td>
<td style="text-align: center;">4\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">6\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5 Turbo</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">Lower</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Anchor model</th>
<th style="text-align: center;">Interval</th>
<th style="text-align: center;">$M=5$</th>
<th style="text-align: center;">$M=10$</th>
<th style="text-align: center;">$M=20$</th>
<th style="text-align: center;">$M=30$</th>
<th style="text-align: center;">$M=50$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Higher</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">10\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">7\%</td>
<td style="text-align: center;">6\%</td>
<td style="text-align: center;">3\%</td>
<td style="text-align: center;">1\%</td>
</tr>
<tr>
<td style="text-align: center;">o3-mini-high</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">37\%</td>
<td style="text-align: center;">49\%</td>
<td style="text-align: center;">72\%</td>
<td style="text-align: center;">99\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4.5-preview</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">41\%</td>
<td style="text-align: center;">42\%</td>
<td style="text-align: center;">25\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3\%</td>
<td style="text-align: center;">3\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5 Turbo</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">Lower</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Anchor model</th>
<th style="text-align: center;">Interval</th>
<th style="text-align: center;">$M=5$</th>
<th style="text-align: center;">$M=10$</th>
<th style="text-align: center;">$M=20$</th>
<th style="text-align: center;">$M=30$</th>
<th style="text-align: center;">$M=50$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Higher</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">8\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">18\%</td>
<td style="text-align: center;">15\%</td>
<td style="text-align: center;">1\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">o3-mini-high</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">61\%</td>
<td style="text-align: center;">77\%</td>
<td style="text-align: center;">69\%</td>
<td style="text-align: center;">100\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4.5-preview</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">6\%</td>
<td style="text-align: center;">6\%</td>
<td style="text-align: center;">12\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3\%</td>
<td style="text-align: center;">2\%</td>
<td style="text-align: center;">17\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4\%</td>
<td style="text-align: center;">1\%</td>
<td style="text-align: center;">1\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5 Turbo</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">Lower</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Anchor model</th>
<th style="text-align: center;">Interval</th>
<th style="text-align: center;">$M=5$</th>
<th style="text-align: center;">$M=10$</th>
<th style="text-align: center;">$M=20$</th>
<th style="text-align: center;">$M=30$</th>
<th style="text-align: center;">$M=50$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Higher</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">1\%</td>
<td style="text-align: center;">1\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">o3-mini-high</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">9\%</td>
<td style="text-align: center;">14\%</td>
<td style="text-align: center;">10\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4.5-preview</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">44\%</td>
<td style="text-align: center;">53\%</td>
<td style="text-align: center;">79\%</td>
<td style="text-align: center;">95\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">35\%</td>
<td style="text-align: center;">30\%</td>
<td style="text-align: center;">11\%</td>
<td style="text-align: center;">5\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">10\%</td>
<td style="text-align: center;">2\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5 Turbo</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">Lower</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Anchor model</th>
<th style="text-align: center;">Interval</th>
<th style="text-align: center;">$M=5$</th>
<th style="text-align: center;">$M=10$</th>
<th style="text-align: center;">$M=20$</th>
<th style="text-align: center;">$M=30$</th>
<th style="text-align: center;">$M=50$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Higher</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">17\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">16\%</td>
<td style="text-align: center;">18\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">o3-mini-high</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">14\%</td>
<td style="text-align: center;">24\%</td>
<td style="text-align: center;">2\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4.5-preview</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">31\%</td>
<td style="text-align: center;">42\%</td>
<td style="text-align: center;">67\%</td>
<td style="text-align: center;">91\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">15\%</td>
<td style="text-align: center;">14\%</td>
<td style="text-align: center;">21\%</td>
<td style="text-align: center;">2\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">8\%</td>
<td style="text-align: center;">1\%</td>
<td style="text-align: center;">11\%</td>
<td style="text-align: center;">7\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5 Turbo</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">Lower</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 2: Bayesian Probability Ranking Analysis
Probability distributions of the test model's ranking relative to six anchor models across varying question counts ( $M$ ). The anchor models partitioned the ranking space into seven mutually exclusive intervals, with probabilities quantifying the likelihood of the test model falling into each interval.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Method comparison
Performance comparison between the proposed Bayesian approach and conventional evaluation metrics. Bayesian@1 and accuracy reporting are Single-trial evaluations, while Bayes@10, Pass@10 and Mean $\pm$ std are aggregated from $\mathrm{O}=10$ independent trials. All results are obtained from $\mathrm{M}=20$ questions.
tion, while $M=20$ retained at least $65 \%$ confidence in most likely interval resolution. Sample size reduction to $M=10$ and below induced non-linear uncertainty propagation.</p>
<p>Figure 3 compared Bayesian evaluation metrics (single/multi-trial) against conventional reporting paradigms using $M=20$ questions, yielding four observations:</p>
<ol>
<li>Multi-trial Bayesian@10 demonstrated superior interval precision compared to Bayesian@1, while maintaining consistent category estimation boundaries across all models except Qwen2.5-72B. Notably, Qwen2.5-72B exhibited the largest standard error in mean+/std reporting, suggesting higher trial-to-trial variability.</li>
<li>Accuracy reporting gives a single score for each model but does not provide any statistical interpretation on the reliability of the scores. Furthermore, it obscured performance hierarchies resolvable through Bayesian analysis, as evidenced by the $65 \%$ accuracy achieved by both DeepSeek-V3-0324 and Qwen2.5-72B.</li>
<li>Pass@10 aggregation inflated overall accuracy metrics while reducing discernibility between models. It does not provide any statistica interpretation either. In addition, Pass@10 suggested comparable performance for Llama-4-Maverick relative to DeepSeek-R1, which is the only evaluation metric suggesting such parity.</li>
<li>In traditional mean $\pm$ std reporting, DeepSeek-V3-0324, QwQ-32B, and Llama-4-Maverick all have means near $60 \%$ with overlapping variances, suggesting comparable performance. However, Bayesian analysis indicated a categorical performance disparity, placing Llama-4-Maverick below the other two models despite their similar mean estimates.</li>
</ol>
<h1>5 Discussion</h1>
<p>Our systematic comparisons demonstrate Bayesian inference's superior discriminatory capacity over conventional evaluation methodologies through two dimensions: statistical robustness under constrained query regimes, and resolution of performance ambiguities.
The Bayesian approach exhibits marked stability in discriminative performance when subjected to limited query budgets $(M=20)$, outperforming conventional accuracy metrics that remain vulnerable to stochastic output variance while lacking rigorous statistical interpretation, particularly under low-sample conditions. While Pass@N frameworks establish meaningful performance ceilings, they exhibit critical limitations in resolving models with comparable best-case performances - exemplified by Llama-4 showing a $10 \%$ advantage in Pass@10 metrics yet underperforming QwQ-32B by one category with the Bayesian approach.
Mean $\pm$ standard error reporting offered partial statistical transparency but suffered from resolution limitations in cross-model analyses. Notably, our Bayesian method resolved performance disparities between models with overlapping uncertainty intervals (QwQ-32B vs. Llama-4-Maverick), demonstrating superior discriminative power. The approach's incorporation of anchor model priors effectively addressed small-sample equivalence fallacies, where conventional metrics misattributed systematic capability differences to stochastic sampling noise.</p>
<p>The proposed evaluation method enables progressive capability refinement through strategic anchor augmentation. For instance, incorporating QwQ-32B into the anchor ensemble could resolve its current equivalence with DeepSeek-V3-0324, demonstrating the method's adaptive capacity for step-by-step model discrimination through dynamic anchor integration.
While Item Response Theory (IRT) approaches like the Rasch model[Rasch, 1993] have proven successful in educational assessment, their direct adaptation to LLM evaluation presents critical limitations. Both IRT and our approaches leverage anchor model priors when available, but diverge fundamentally in handling information uncertainty. Conventional IRT implementations assume Gaussian latent trait distributions across test populations - an assumption theoretically justified in human testing through central limit theorem applications but questionable with limited number of LLM models. In contrast, our Bayesian approach does not require such assumptions, accommodating non-Gaussian capability distributions while maintaining computational tractability. This relaxation of distributional constraints enhances flexibility compared to IRT-based methods, while maintaining granular, uncertainty-aware comparisons.
The evaluation method's ability to deliver statistically calibrated rankings with about 20 queries aligns with real-world usage patterns. Experienced users naturally employ prior knowledge through iterative LLM interactions, instinctively comparing outputs against previously encountered models. Users also gain valuable insights from limited trials while quantifying uncertainty-e.g., "Model A is definitely better than GPT-3.5 but may be on par with GPT-4." Our methodology formalizes this cognitive process through explicit Bayesian inference. It systematically integrates priors derived from query set selection and anchor model selection, both of which reflect inherent human preference and experience. This methodology also recognizes expert judgment value - particularly critical when reliable prior information represents both scarce resource in the real-world and competitive advantage in cutting-edge LLM development.</p>
<h3>5.1 Limitations and Future Directions</h3>
<p>First, valid priors require high-quality query sets and representative anchors. While GPT-series models provided stable baselines, emerging architectures may disrupt assumed capability ranking orders. Automated anchor selection protocols and dynamic anchor updates warrant further investigation.
Second, the factorization in Eqs. 3 assumes assumes conditional query independence - a valid approximation in a well curated query set but may be invalid with semantically correlated queries. Future work should model question interdependencies or introduce non-correlated hierarchical priors (e.g., grouping questions by skill type) while examining independence assumptions across hierarchical levels.</p>
<p>Third, the binary scoring paradigm struggles with open-ended humanities questions. Extending the approach to multi-dimensional rubrics (e.g., correctness, creativity, coherence, empathy) would enhance versatility for complex tasks.</p>
<h1>6 Conclusion</h1>
<p>The study presents a method for evaluating LLMs by systematically incorporating measurable prior knowledge within Bayesian inference, thereby more effectively accommodating the inherent stochastic characteristics of these models. Notably, the proposed methodology demonstrates enhanced discriminative capability through prior knowledge integration even with limited sample sizes, while concurrently ensuring result stability in its analytical outcomes.</p>
<h2>References</h2>
<p>[1] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. URL https: //arxiv.org/abs/2005.14165.
[2] Long Ouyang, Jeff Wu, Xu Jiang, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. URL https://arxiv.org/abs/2203.02155.
[3] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. Technical report, OpenAI, 2018. URL https://cdn.openai.com/ research-covers/language-unsupervised/language_understanding_paper.pdf.
[4] Alec Radford, Jeff Wu, Rewon Child, et al. Language models are unsupervised multitask learners. Technical report, OpenAI, 2019. URL https://cdn.openai.com/better-language-models/language_ models_are_unsupervised_multitask_learners.pdf.
[5] Wayne Xin Zhao, Kun Zhou, Junyi Li, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. URL https://arxiv.org/abs/2303.18223.
[6] Mark Chen, Jerry Tworek, Heewoo Jun, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.
[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. URL https://arxiv.org/abs/2204.02311.
[8] Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. URL https://arxiv.org/abs/2302.13971.
[9] Robert E Blackwell, James Barry, and Anthony G Cohn. Towards reproducible llm evaluation: Quantifying uncertainty in llm benchmark scores. arXiv preprint arXiv:2410.03492, 2024. URL https://arxiv.org/abs/2410.03492.
[10] Ethan Miller. Adding error bars to evals: A statistical approach to language model evaluations. arXiv preprint arXiv:2411.00640, 2024. URL https://arxiv.org/abs/2411.00640.
[11] Georg Rasch. Probabilistic models for some intelligence and attainment tests. MESA Press, Chicago, 50th anniversary edition, 1993. URL https://eric.ed.gov/?id=ED419814.
[12] Federico Maria Polo, Lucas Weber, Leshem Choshen, et al. tinybenchmarks: Evaluating llms with fewer examples. arXiv preprint arXiv:2402.14992, 2024. URL https://arxiv.org/abs/2402.14992.
[13] Sang Truong, Yifan Tu, Percy Liang, et al. Reliable and efficient amortized model-based evaluation. arXiv preprint arXiv:2503.13335, 2025. URL https://arxiv.org/abs/2503.13335.
[14] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024. URL https://arxiv.org/abs/ 2403.04132.
[15] Edwin T Jaynes. Probability theory: The logic of science. Cambridge University Press, 2003. URL https://www.cambridge.org/core/books/probability-theory/ 9CA08E224FF30123304E6D8935CF1A99.</p>
<p>[16] OpenAI. Introducing chatgpt and whisper apis, March 2023. URL https://openai.com/blog/ introducing-chatgpt-and-whisper-apis/. Accessed: 2023-03-01.
[17] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.org/ abs/2303.08774.
[18] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. URL https://arxiv.org/ abs/2410.21276.
[19] OpenAI. Introducing gpt-4.5, February 2025. URL https://openai.com/blog/ introducing-gpt-4-5/. Accessed: 2025-02-27.
[20] OpenAI. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. URL https://arxiv.org/ abs/2412.16720.
[21] OpenAI. Openai o3-mini, January 2025. URL https://openai.com/blog/openai-o3-mini/. Accessed: 2025-01-31.
[22] Liang Xu, Aiping Li, Lei Zhu, et al. Superclue: A comprehensive chinese large language model benchmark. arXiv preprint arXiv:2307.15020, 2023. URL https://arxiv.org/abs/2307.15020.
[23] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. arXiv preprint arXiv:2306.08543, 2023. URL https://arxiv.org/abs/2306.08543.
[24] Minghao Wu, Abdul Waheed, Chiyu Zhang, et al. Lamini-lm: A diverse herd of distilled models from large-scale instructions. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2024), pages 944-964. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.eacl-long.57/.
[25] M-A-P Team, Xiaocong Du, Yuan Yao, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739, 2025. URL https://arxiv.org/abs/2502.14739.
[26] Yizhong Wang, Xiang Ma, Guiming Zhang, et al. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. URL https://arxiv. org/abs/2406.01574.
[27] David Rein, Benjamin L Hou, Asa Cooper Stickland, et al. Gpqa: A graduate-level google-proof q\&amp;a benchmark. arXiv preprint arXiv:2311.12022, 2023. URL https://arxiv.org/abs/2311.12022.
[28] Dan Hendrycks, Collin Burns, Saurav Kadavath, et al. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. URL https://arxiv.org/abs/2103.03874.
[29] Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, et al. Zebralogic: On the scaling limits of llms for logical reasoning. arXiv preprint arXiv:2502.01100, 2025. URL https://arxiv.org/abs/2502. 01100 .
[30] Kaijun Ma, Xiaocong Du, Yizhong Wang, et al. Kor-bench: Benchmarking language models on knowledge-orthogonal reasoning tasks. arXiv preprint arXiv:2410.06526, 2024. URL https://arxiv. org/abs/2410.06526.
[31] Itsuki Fujisawa, Shun Nobe, Hiroki Seto, et al. Procbench: Benchmark for multi-step reasoning and following procedure. arXiv preprint arXiv:2410.03117, 2024. URL https://arxiv.org/abs/2410. 03117 .
[32] Anthropic. Claude 3.7 sonnet and claude code, February 2025. URL https://www. anthropic.com/ news/claude-3-7-sonnet. Accessed: 2025-02-24.
[33] Google DeepMind. Gemini 2.5: Our most intelligent ai model, March 2025. URL https://blog. google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/. Accessed: 2025-03-25.
[34] Yuhang Li, Rui Xie, Zhen Yang, et al. Transmamba: Flexibly switching between transformer and mamba. arXiv preprint arXiv:2503.24067, 2025. URL https://arxiv.org/abs/2503.24067.
[35] Doubao Team. Doubao-1.5-pro: Model release, January 2025. URL https://team.doubao.com/ en/special/doubao_1_5_pro. Accessed: 2025-01-22.</p>
<p>[36] Qwen Team. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2025. URL https://arxiv. org/abs/2412.15115.
[37] Qingxiu Dong, Jing Xu, Lingpeng Kong, Zhifang Sui, and Lei Li. Statistical knowledge assessment for large language models. arXiv preprint arXiv:2305.10519, 2023. URL https://arxiv.org/abs/ 2305.10519.
[38] Xunjian Yin, Xinyu Zhang, Jian Ruan, and Xiaojun Wan. Benchmarking knowledge boundary for large language models: A different perspective on model evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2270-2286. Association for Computational Linguistics, 2024. doi: 10.18653/v1/2024.acl-long.124. URL https://doi.org/10.18653/v1/2024.acl-long. 124.
[39] Yu Cheng, Yupeng Chang, and Yuxin Wu. A survey on data contamination for large language models. arXiv preprint arXiv:2502.14425, 2025. URL https://arxiv.org/abs/2502.14425.
[40] Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. Investigating data contamination in modern benchmarks for large language models. arXiv preprint arXiv:2311.09783, 2023. URL https://arxiv.org/abs/2311.09783.
[41] Shahriar Golchin and Mihai Surdeanu. Data contamination quiz: A tool to detect and estimate contamination in large language models. arXiv preprint arXiv:2311.06233, 2023. URL https: //arxiv.org/abs/2311.06233.
[42] Meta. The llama 4 herd: The beginning of a new era of natively multimodal ai innovation, April 2025. URL https://ai.meta.com/blog/llama-4-multimodal-intelligence/. Accessed: 2025-0405 .
[43] DeepSeek-AI. Deepseek-v3-0324 release, March 2025. URL https://api-docs.deepseek.com/ news/news250325. Accessed: 2025-03-24.
[44] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https://arxiv.org/abs/2501.12948.
[45] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, 2025. URL https://qwenlm. github.io/blog/qwq-32b/. Accessed: 2025.</p>
<h1>Appendix A Complete success rate distribution</h1>
<p>Complete success rate distribution of anchor models with 50 questions. Extreme probability values ${0 \%, 100 \%}$ were modulated to ${1 \%, 99 \%}$ to ensure numerical stability during subsequent computations.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Q1</th>
<th style="text-align: center;">Q2</th>
<th style="text-align: center;">Q3</th>
<th style="text-align: center;">Q4</th>
<th style="text-align: center;">Q5</th>
<th style="text-align: center;">Q6</th>
<th style="text-align: center;">Q7</th>
<th style="text-align: center;">Q8</th>
<th style="text-align: center;">Q9</th>
<th style="text-align: center;">Q10</th>
<th style="text-align: center;">Q11</th>
<th style="text-align: center;">Q12</th>
<th style="text-align: center;">Q13</th>
<th style="text-align: center;">Q14</th>
<th style="text-align: center;">Q15</th>
<th style="text-align: center;">Q16</th>
<th style="text-align: center;">Q17</th>
<th style="text-align: center;">Q18</th>
<th style="text-align: center;">Q19</th>
<th style="text-align: center;">Q20</th>
<th style="text-align: center;">Q21</th>
<th style="text-align: center;">Q22</th>
<th style="text-align: center;">Q23</th>
<th style="text-align: center;">Q24</th>
<th style="text-align: center;">Q25</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">01</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$40 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">O3-mini-high</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4.5-preview</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$20 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;">$40 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$40 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$50 \%$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Q28</th>
<th style="text-align: center;">Q27</th>
<th style="text-align: center;">Q28</th>
<th style="text-align: center;">Q29</th>
<th style="text-align: center;">Q30</th>
<th style="text-align: center;">Q31</th>
<th style="text-align: center;">Q32</th>
<th style="text-align: center;">Q33</th>
<th style="text-align: center;">Q34</th>
<th style="text-align: center;">Q35</th>
<th style="text-align: center;">Q36</th>
<th style="text-align: center;">Q37</th>
<th style="text-align: center;">Q38</th>
<th style="text-align: center;">Q39</th>
<th style="text-align: center;">Q40</th>
<th style="text-align: center;">Q41</th>
<th style="text-align: center;">Q42</th>
<th style="text-align: center;">Q43</th>
<th style="text-align: center;">Q44</th>
<th style="text-align: center;">Q45</th>
<th style="text-align: center;">Q46</th>
<th style="text-align: center;">Q47</th>
<th style="text-align: center;">Q48</th>
<th style="text-align: center;">Q49</th>
<th style="text-align: center;">Q50</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">01</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
</tr>
<tr>
<td style="text-align: center;">O3-mini-high</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$40 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4.5-preview</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$50 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$40 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
</tr>
</tbody>
</table>
<h1>Appendix B Complete Query Set</h1>
<p>When query set size was reduced to $M$, only the first $M$ questions were used from the complete 50 -question-set.</p>
<p>Table 1: Complete query set with 50 questions</p>
<table>
<thead>
<tr>
<th style="text-align: center;">NO</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">Domains</th>
<th style="text-align: center;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">What is the large documentary called, that Sergey Apollinariyevich Grigoryev co-produced with Chinese film workers when he came to China in 1950? <br> __A) Revolutionary Modern China. <br> __B) Progressive China. <br> __C) Liberated China. <br> __D) Revolutionary New China. <br> __E) Free China. <br> __F) Modern China. <br> __G) Revolutionary Free China. <br> __H) New China. <br> __I) Revolutionary Liberated China. <br> __J) Revolutionary China</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">Arts</td>
<td style="text-align: center;">SuperGPQA</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">The definition of an algorithm: The definition of $a(n)$ is hypotenuse numbers (numbers that can be expressed as the square root of the sum of the squares of two non-zero integers). Given the input x_list (a series of values): $[55,56,57,58,59,60,61,62,63,64]$, determine the corresponding output sequence y_list. <br> A) $[118,121,124,129,131,135,136,138,143,145]$ <br> B) $[115,118,120,127,129,134,138,139,144,146]$ <br> C) $[114,118,121,126,129,133,137,141,142,148]$ <br> D) $[119,120,122,123,125,130,135,136,137,140]$ <br> E) $[116,119,120,121,123,126,131,133,135,138]$ <br> F) $[122,124,125,127,131,132,138,140,141,145]$ <br> G) $[121,122,124,126,128,132,136,139,140,142]$ <br> H) $[124,128,130,134,137,143,147,149,150,157]$ <br> I) $[120,125,127,131,135,136,139,141,142,149]$ <br> J) $[117,123,126,128,133,137,139,141,143,147]$</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">Engineering</td>
<td style="text-align: center;">MMLU-Pro</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">What is the major outcome of the reaction between 4,4-dimethylcyclopent-1-enol and bromine? <br> A. 2-bromo-4,4-dimethylcyclopentanone <br> B. (1R,2R)-1,2-dibromo-4,4-dimethylcyclopentanol <br> C. (1R,2S)-1,2-dibromo-4,4-dimethylcyclopentanol <br> D. 4-bromo-4,4-dimethylcyclopentanone</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">Chemistry</td>
<td style="text-align: center;">GPQA- <br> Diamond</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">What are the roots of $x^{2}-3 x^{2}-10 x+24$. List your answer as numbers separated by commas.</td>
<td style="text-align: center;">$2,-3,4$</td>
<td style="text-align: center;">Mathematics</td>
<td style="text-align: center;">MATH</td>
</tr>
<tr>
<td style="text-align: center;">Continued on next page</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1: Complete query set with 50 questions</p>
<table>
<thead>
<tr>
<th style="text-align: center;">NO</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">Domains</th>
<th style="text-align: center;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Encryption Process Description <br> - Input Data: - Plaintext: A string consisting solely of uppercase letters with no spaces or punctuation. - Output (Ciphertext): A string without punctuation. <br> - Setup: Use the following Multitap Code Table as a reference:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">KOR-Bench</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Letter</td>
<td style="text-align: center;">Multitap Code</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">$2^{1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">$2^{2}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">$2^{3}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">$3^{1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E</td>
<td style="text-align: center;">$3^{2}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">$3^{3}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">G</td>
<td style="text-align: center;">$4^{1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">$4^{2}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">$4^{3}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">J</td>
<td style="text-align: center;">$5^{1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">K</td>
<td style="text-align: center;">$5^{2}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">$5^{3}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">$6^{1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">$6^{2}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">O</td>
<td style="text-align: center;">$6^{3}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">$7^{1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q</td>
<td style="text-align: center;">$7^{2}$</td>
<td style="text-align: center;">$\left[\left[8^{2} 6^{3}\right]\right]$</td>
<td style="text-align: center;">Reasoning</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">$7^{3}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">$7^{4}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">$8^{1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">U</td>
<td style="text-align: center;">$8^{2}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">V</td>
<td style="text-align: center;">$8^{3}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">W</td>
<td style="text-align: center;">$9^{1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">$9^{2}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">$9^{3}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Z</td>
<td style="text-align: center;">$9^{4}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">- Encryption Steps: For every character ' $p$ ' in the plaintext: 1. Confirm that ' $p$ ' is an uppercase letter and appears in the Multitap Code Table. 2. Substitute ' $p$ ' with its corresponding Multitap Code from the table. Decryption Process Description <br> - Input Data: - Ciphertext: A string devoid of punctuation. - Output (Plaintext): A string of uppercase letters. <br> - Setup: The same Multitap Code Table is used as a reference. <br> - Decryption Steps: For every multitap code 'c' in the ciphertext: 1. Confirm that ' $c$ ' matches one of the codes in the table. 2. Replace ' $c$ ' with the corresponding uppercase letter from the Multitap Code Table. <br> - #Question <br> Given the plaintext: '"UO"', perform the encryption according to the above rules. Wrap your final encrypted output in double square brackets, e.g. '[[encrypted answer]]'.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">KOR-Bench</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Continued on next page</td>
</tr>
</tbody>
</table>
<p>Table 1: Complete query set with 50 questions</p>
<table>
<thead>
<tr>
<th style="text-align: center;">NO</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">Domains</th>
<th style="text-align: center;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">When the furnace operates at normal temperature and the molten iron is flowing well, can the iron tap hole be ()? <br> A) Constricted <br> B) Smaller <br> C) No requirements <br> D) Elevated <br> E) Unchanged <br> F) Increased <br> G) Larger <br> H) Lowered <br> I) Expansive <br> J) Lessened</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">Materials <br> Science</td>
<td style="text-align: center;">SuperGPQA</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">When was the EP "Dirty Laundry" released by Adore Delano? <br> A) February 19, 2022 <br> B) July 9, 2021 <br> C) November 29, 2022 <br> D) August 29, 2021 <br> E) July 9, 2020 <br> F) December 5, 2020 <br> G) October 19, 2023 <br> H) March 1, 2021 <br> I) October 19, 2020 <br> J) May 25, 2021</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">Arts</td>
<td style="text-align: center;">MMLU-Pro</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Acetic acid is treated with bromine, pyridine, and acetic anhydride with heating, forming product 1 . <br> 1 is heated with ethanol and a small amount of sulfuric acid, forming product 2 . <br> 2 is treated with sodium cyanide, forming product 3 . <br> 3 is then treated with excess sodium hydride and 1,5-dibromopentane, forming final product 4 . <br> how many distinct hydrogen signals will be observable in the 1 H NMR spectrum of 4 (some of them maybe very close in chemical shift and thus not practically distinguishable, but the desired answer is the number of chemically distinct hydrogens) <br> A. 12 <br> B. 8 <br> C. 5 <br> D. 10</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">Chemistry</td>
<td style="text-align: center;">GPQA- <br> Diamond</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">What is the sum of all complex values of $a$, such that the polynomial $x^{4}+\left(a^{2}-1\right) x^{2}+a^{3}$ has just two distinct complex roots.</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Mathematics</td>
<td style="text-align: center;">MATH</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Problem Statement <br> You're given a string composed of digits and lowercase letters, along with a starting character and a positive integer. Your task is to locate the character that lies a specified number of steps to the right of the starting character, moving one position per step and wrapping around to the beginning of the string when you reach the end. <br> At each step: <br> 1. Move one position to the right in the string. <br> 2. Decrement the step count by one. <br> 3. Continue until the step count reaches zero. <br> You should report: <br> - The ${ }^{<em> </em>}$ final ${ }^{<em> </em>}$ character found when the step count reaches zero. <br> - The ${ }^{<em> </em>}$ initial ${ }^{<em> </em>}$ state as a pair [starting character, initial step count]. <br> - The ${ }^{<em> </em>}$ intermediate ${ }^{<em> </em>}$ states as a list of [character, remaining steps] pairs ${ }^{<em> </em>}$ after each move ${ }^{<em> </em>}$, excluding the initial and final states. <br> Input <br> String: e6 <br> Command: ['6', '2']</td>
<td style="text-align: center;">[ "final": "6", <br> "init": [ "6", <br> "2"], <br> "intermediate": <br> [ [ "e", "1"] ] ]</td>
<td style="text-align: center;">Computer <br> Science</td>
<td style="text-align: center;">Procbench</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Continued on next page</td>
</tr>
</tbody>
</table>
<p>Table 1: Complete query set with 50 questions</p>
<table>
<thead>
<tr>
<th style="text-align: center;">NO</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">Domains</th>
<th style="text-align: center;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">What is the university called where American music producer George Avakian began teaching in 1948? <br> A) Columbia University. <br> B) University of Pennsylvania. <br> C) University of New England. <br> D) University of New York. <br> E) Harvard University. <br> F) University of New Hampshire. <br> G) Princeton University. <br> H) University of New Jersey. <br> I) New York University. <br> J) Yale University.</td>
<td style="text-align: center;">I</td>
<td style="text-align: center;">Arts</td>
<td style="text-align: center;">SuperGPQA</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">What are the three theories about the origin of Proto-Malay language mentioned in the 'Encyclopedia of Malaysia: Early History'? <br> A) The Yunnan theory, the Seafarer theory, and the Borneo theory <br> B) The Yunnan theory, the Seafarer theory, and the Melanesian theory <br> C) The Yunnan theory, the Seafarer theory, and the Malay Archipelago theory <br> D) The Yunnan theory, the Seafarer theory, and the Philippine theory <br> E) The Yunnan theory, the Seafarer theory, and the Javanese theory <br> F) The Yunnan theory, the Seafarer theory, and the Micronesian theory <br> G) The Yunnan theory, the Seafarer theory, and the Taiwan theory <br> H) The Yunnan theory, the Seafarer theory, and the Sumatran theory <br> I) The Yunnan theory, the Seafarer theory, and the Polynesian theory <br> J) The Yunnan theory, the Seafarer theory, and the Indonesian theory</td>
<td style="text-align: center;">G</td>
<td style="text-align: center;">Humanities</td>
<td style="text-align: center;">MMLU-Pro</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">A textile dye containing an extensively conjugated pi-electrons emits light with energy of 2.3393 eV . What color of light is absorbed by the organic compound? <br> A. Violet <br> B. Yellow <br> C. Red <br> D. Blue</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">Chemistry</td>
<td style="text-align: center;">GPQA- <br> Diamond</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">Consider four integers where each leaves a different remainder when divided by 6 . If multiplying these integers together results in a value $Y$ that is not divisible by 6 , determine the remainder obtained when $Y$ is divided by 6 .</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Mathematics</td>
<td style="text-align: center;">MATH</td>
</tr>
<tr>
<td style="text-align: center;">Continued on next page</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1: Complete query set with 50 questions</p>
<table>
<thead>
<tr>
<th style="text-align: center;">NO</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">Domains</th>
<th style="text-align: center;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">#Rule: <br> - Literal: A propositional variable and its negation together are called literals. <br> - Complement: For any literal L, its complement is written as L'. That is, if L is p then $\mathrm{L}^{\prime}$ is $\neg \mathrm{p}$; if L is $\neg \mathrm{p}$ then $\mathrm{L}^{\prime}$ is p . <br> - Resolution: <br> Suppose you have two simple disjunctive clauses: <br> - $\mathrm{C} 1=\mathrm{C} 3 \vee \mathrm{~L}$ <br> - $\mathrm{C} 2=\mathrm{C} 4 \vee \mathrm{~L}$ <br> Then, C 1 and C 2 can be resolved, and the operation is defined as $\operatorname{dispel}(\mathrm{C} 1, \mathrm{C} 2)=\mathrm{C} 3 \vee \mathrm{C} 4$. If the result is empty, then dispel $(\mathrm{C} 1, \mathrm{C} 2)=$ 0 . <br> - Resolution Algorithm: <br> 1. Input: A conjunctive normal form S. <br> 2. Output: Output "Plausible" if S has a satisfying assignment; otherwise, output "Implausible." <br> 3. Procedure: <br> - Initialization: <br> - Let S 0 and S 2 be empty sets. <br> - Let S 1 be the set of all simple disjunctive clauses in S . <br> - Resolution with S 0 and S 1 : <br> - For each clause C 1 in S 0 and each clause C 2 in S 1 , if they can be resolved, compute $\mathrm{C}=\operatorname{dispel}(\mathrm{C} 1, \mathrm{C} 2)$. <br> - If $\mathrm{C}=0$, output "Implausible" and stop. <br> - If C is new (i.e., not present in S 0 or S 1 ), add C to S 2 . <br> - Resolution within S1: <br> - For every pair of clauses C 1 and C 2 in S 1 , if they can be resolved, compute $\mathrm{C}=\operatorname{dispel}(\mathrm{C} 1, \mathrm{C} 2)$. <br> - If $\mathrm{C}=0$, output "Implausible" and stop. <br> - If C is new, add C to S 2 . <br> - Check S2: <br> - If S2 is empty, output "Plausible" and end. <br> - Otherwise, move S1 into S0, set S1 equal to S2, clear S2, and repeat from the earlier resolution steps. <br> #Question: <br> Given the clauses <br> - $\mathrm{C} 1=\neg \mathrm{p} \vee \mathrm{q} \vee \mathrm{r}$ <br> - $\mathrm{C} 2=\mathrm{p} \vee \neg \mathrm{r} \vee \neg \mathrm{s}$ <br> calculate dispel(C1, C2). <br> Provide the answer in the format ' $[]]^{\prime}$, or ' $[] ;[] ; \ldots$ ' if multiple answers exist.</td>
<td style="text-align: center;">$\begin{aligned} &amp; {[[\mathrm{q} \vee \mathrm{r} \vee \neg \mathrm{r} \vee \neg \mathrm{s}]} \ &amp; {[\neg \mathrm{p} \vee \mathrm{q} \vee \mathrm{p} \vee \neg \mathrm{s}]} \end{aligned}$</td>
<td style="text-align: center;">Reasoning</td>
<td style="text-align: center;">KOR-Bench</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">The sequence of shell plating starts from the flat keel's K strake and extends toward the $\qquad$ , with each strake labeled sequentially as $\mathrm{A}, \mathrm{B}$, $\mathrm{C}, \mathrm{D}$, etc., until reaching the $\qquad$ . <br> Options: <br> A) Upwards / adjacent strakes <br> B) Port and starboard directions / deck strakes <br> C) Forward direction / subsequent strakes <br> D) The port and starboard sides / side plates</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">Engineering</td>
<td style="text-align: center;">SuperGPQA</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Continued on next page</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1: Complete query set with 50 questions</p>
<table>
<thead>
<tr>
<th style="text-align: center;">NO</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">Domains</th>
<th style="text-align: center;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">Very Low Birth Weight (VLBW) has the meaning of $\qquad$ . <br> __A) Less than 1500 g within 1 hour of birth <br> __B) Less than 1000 g within 24 hours of birth <br> __C) Less than 1500 g after 24 hours of birth <br> __D) More than 1500 g after 24 hours of birth <br> __E) More than 1700 g within 24 hours of birth <br> __F) More than 2000 g within 1 hour of birth <br> __G) Less than 1300 g after 30 minutes of birth <br> __H) Less than 1200 g after 1 hour of birth <br> __I) More than 1000 g within 1 hour of birth <br> __J) More than 1500 g within 1 hour of birth</td>
<td style="text-align: center;">A</td>
<td style="text-align: center;">Medical <br> Science</td>
<td style="text-align: center;">MMLU-Pro</td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">Which particle is not associated with a spontaneously-broken symmetry? <br> A. Pion <br> B. Magnon <br> C. Phonon <br> D. Skyrmion</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">Physics</td>
<td style="text-align: center;">GPQA- <br> Diamond</td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">$G$ and $H$ are the centroid and orthocenter of triangle $A B C$, respectively. Let $F$ be the midpoint of $\overline{G H}$. Show $A F^{2}+B F^{2}+C F^{2}$ in terms of the side lengths $a, b, c$ and circumradius $R$ of triangle $A B C$.</td>
<td style="text-align: center;">$3 R^{2}$</td>
<td style="text-align: center;">Mathematics</td>
<td style="text-align: center;">MATH</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">Rule: <br> In a simple conjunctive form (or a simple disjunctive form) that contains n propositional variables, if each propositional variable and its negation appear exactly once, and the propositional variables or their negations are arranged in ascending order of subscripts or in lexicographical order, such a form is called a paired conjunctive term (or paired disjunctive term). <br> If the true assignment of a paired conjunctive term corresponds to a binary number equal to hexadecimal number $i$, this paired conjunctive term is denoted as mi (with lowercase m ). For example, the true assignment of $\mathrm{p} / \mathrm{sq}$ is 11 , and the binary number 11 corresponds to hexadecimal 3 , which is denoted as m 3 . <br> Similarly, if the false assignment of a paired disjunctive term corresponds to a binary number equal to hexadecimal number $i$, this paired disjunctive term is denoted as Mi (with uppercase M ). For instance, the false assignment of $\neg \mathrm{p} \vee \neg \mathrm{q} \vee \neg \mathrm{r}$ is 111 , and the binary number 111 (hexadecimal 7) is denoted as M7. <br> The disjunctive normal form (or conjunctive normal form) composed of all paired conjunctive terms (or paired disjunctive terms) is called the principal disjunctive normal form (or principal conjunctive normal form). <br> Given a formula A containing n propositional variables: <br> - If the principal disjunctive normal form of A includes all $2^{n}$ paired conjunctive terms, then A is a tautology. <br> - If the principal disjunctive normal form of A includes no paired conjunctive terms, then A is a contradiction. <br> - If the principal disjunctive normal form of A includes m 0 , then A is a basic formula. <br> - If the indices i of the paired conjunctive terms in the principal disjunctive normal form of A are all even, then A is an all-even formula. <br> - If the indices i of the paired conjunctive terms in the principal disjunctive normal form of A are all odd, then A is an all-odd formula. <br> Question: <br> Given that formula A contains 4 propositional variables, what should it be denoted as if it is both a tautology and a basic form? <br> Provide your answer wrapped in double square brackets.</td>
<td style="text-align: center;">$\left[\left[m 0 \vee m 1 \vee m 2 \vee\right.\right.$ $\left.m 3 \vee m 4 \vee m 5 \vee\right.$ $\left.m 6 \vee m 7 \vee m 8 \vee\right.$ $\left.m 9 \vee m A \vee m B \vee\right.$ $\left.m C \vee m D \vee m E \vee\right.$ $\left.m F]\right]$</td>
<td style="text-align: center;">Reasoning</td>
<td style="text-align: center;">KOR-Bench</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Continued on next page</td>
</tr>
</tbody>
</table>
<p>Table 1: Complete query set with 50 questions</p>
<table>
<thead>
<tr>
<th>NO</th>
<th>Question</th>
<th>Answer</th>
<th>Domains</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr>
<td>21</td>
<td>When was "Discussion sur l'évolution de l'univers" by Georges Lemaitre published? <br> A) 1931 <br> B) 1927 <br> C) 1933 <br> D) 1935 <br> E) 1937 <br> F) 1929 <br> G) 1932 <br> H) 1930 <br> I) 1936 <br> J) 1934</td>
<td>C</td>
<td>Physics</td>
<td>SuperGPQA</td>
</tr>
<tr>
<td>22</td>
<td>Which weather condition is characterized by the presence of a layer of high, thin clouds often preceding a warm front? <br> A) Cumulus clouds <br> B) Cumulonimbus clouds <br> C) Altocumulus clouds <br> D) Cirrus clouds <br> E) Stratocumulus clouds <br> F) Stratus clouds <br> G) Altostratus clouds <br> H) Cirrocumulus clouds <br> I) Cirrostratus clouds <br> J) Nimbostratus clouds</td>
<td>D</td>
<td>Earth Science</td>
<td>SuperGPQA</td>
</tr>
<tr>
<td>23</td>
<td>The 'Encyclopedia of Malaysia: Early History' mentioned which three theories about the origin of Proto-Malay language? <br> A) The Yunnan theory, the Seafarer theory, and the Taiwan theory <br> B) The Yunnan theory, the Seafarer theory, and the Melanesian theory <br> C) The Yunnan theory, the Seafarer theory, and the Malay Archipelago theory <br> D) The Yunnan theory, the Seafarer theory, and the Philippine theory <br> E) The Yunnan theory, the Seafarer theory, and the Borneo theory <br> F) The Yunnan theory, the Seafarer theory, and the Javanese theory <br> G) The Yunnan theory, the Seafarer theory, and the Micronesian theory <br> H) The Yunnan theory, the Seafarer theory, and the Sumatran theory <br> I) The Yunnan theory, the Seafarer theory, and the Polynesian theory <br> J) The Yunnan theory, the Seafarer theory, and the Indonesian theory</td>
<td>A</td>
<td>Humanities</td>
<td>MMLU-Pro</td>
</tr>
<tr>
<td>24</td>
<td>What are the building blocks of a pattern training method? <br> A) Core elements, inspection standards, assessment criteria, training techniques. <br> B) Key pattern, setup procedures, testing benchmarks, educational systems. <br> C) Target model, inspection means, evaluation criteria, training methods. <br> D) Pattern design, implementation tools, review standards, training models. <br> E) Principal components, examination vehicles, judging norms, learning techniques. <br> F) Primary algorithm, review guidelines, execution tactics, learning processes. <br> G) Basic units, examination means, evaluation frameworks, instructional methods. <br> H) Essential components, inspection measures, validation guidelines, learning models. <br> I) Basic algorithms, inspection tools, evaluation metrics, pattern methods. <br> J) Foundational structure, evaluation tools, training regimens, feedback mechanisms.</td>
<td>C</td>
<td>Humanities</td>
<td>MMLU-Pro</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Continued on next page</td>
</tr>
</tbody>
</table>
<p>Table 1: Complete query set with 50 questions</p>
<table>
<thead>
<tr>
<th>NO</th>
<th>Question</th>
<th>Answer</th>
<th>Domains</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr>
<td>25</td>
<td>1-bromobenzene-2-d is treated with NaNH2 in condensed ammonia solvent. This reaction contains how many possible organic products? <br> A. 2 <br> B. 1 <br> C. 3 <br> D. 4</td>
<td>C</td>
<td>Chemistry</td>
<td>GPQA- <br> Diamond</td>
</tr>
<tr>
<td>26</td>
<td>In a specific region of the sky, astronomers have observed that the number of stars varies with parallax as $1 / \mathrm{plx}^{5}$. How does the number of stars in that region of the sky change with distance (per unit range of distance, $r$ )? <br> A. $\sim r^{3}$ <br> B. $\sim r^{3}$ <br> C. $\sim r^{4}$ <br> D. $\sim r^{5}$</td>
<td>B</td>
<td>Physics</td>
<td>GPQA- <br> Diamond</td>
</tr>
<tr>
<td>27</td>
<td>List all possible values of the determinant of $\left(\begin{array}{ccc}\sec ^{2} x &amp; 1 &amp; 1 \ \cos ^{2} x &amp; \cos ^{2} x &amp; \csc ^{2} x \ 1 &amp; \cos ^{2} x &amp; \cot ^{2} x\end{array}\right)$, as $x$ ranges over all real numbers (where the determinant is defined).</td>
<td>$(0,1)$</td>
<td>Mathematics</td>
<td>MATH</td>
</tr>
<tr>
<td>28</td>
<td>Felix generates a two-digit integer by rolling a six-sided dice twice. The result of his first roll is the tens digit, and the result of his second roll is the ones digit. Find the probability that the resulting integer is divisible by 8 ? Express your answer as a common fraction.</td>
<td>$\frac{5}{36}$</td>
<td>Mathematics</td>
<td>MATH</td>
</tr>
<tr>
<td>29</td>
<td>Rules: <br> 1. The game begins with an initial word and identifies a target word at the end. <br> 2. You may change only one letter at each step, with every resulting intermediate word being valid. <br> 3. Transform the starting word into the target word using the fewest number of steps possible. <br> 4. The puzzle provides the starting and target words. Your task is to determine the minimum number of transformations required. <br> Task: <br> Convert the word "HEAD" into "TALE". Express the minimum number of steps needed in double square brackets. For instance, if it requires 3 steps, write your answer as [[3]].</td>
<td>[[5]]</td>
<td>Reasoning</td>
<td>KOR-Bench</td>
</tr>
<tr>
<td>Continued on next page</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1: Complete query set with 50 questions</p>
<table>
<thead>
<tr>
<th style="text-align: center;">NO</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">Domains</th>
<th style="text-align: center;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Example Puzzle: <br> "example_puzzle": "There are three houses arranged in a row, numbered 1 to 3 from left to right as seen from the opposite side of the street. Each house is inhabited by a different person, and every house is linked with a unique attribute in two categories: the resident's name and their preferred drink. The available names are Peter, Eric, and Arnold, while the drinks are tea, water, and milk. The clues provided are as follows: <br> 1. Peter occupies the second house. <br> 2. Arnold lives immediately to the left of the person who drinks only water. <br> 3. The person who drinks only water is immediately to the left of the person whose favorite drink is milk. <br> Please explain your reasoning and provide your final answer using the JSON format below: <br> { "reasoning": "", "solution": { "House 1": { "Name": "", "Drink": ""}, "House 2": { "Name": "", "Drink": ""}, "House 3": { "Name": "", "Drink": "" } }]", <br> "example_puzzle_answer": "{"reasoning": "Given Clue 1, Peter must be in House 2. Clue 2 indicates that Arnold is immediately to the left of the person who drinks only water; since the third house cannot have anyone to its left, Arnold must reside in House 1. Therefore, the resident in House 2 (Peter) is the one who drinks water, leaving House 3 for Eric. Clue 3 then implies that Eric's favorite drink is milk, so by elimination Arnold must prefer tea.", "solution": {"House 1": {"Name": "Arnold", "Drink": "tea"}, "House 2": {"Name": "Peter", "Drink": "water"}, "House 3": {"Name": "Eric", "Drink": "milk"}}}", <br> Puzzle_to_solve: <br> "Imagine five houses lined up from left to right, numbered 1 to 5 as seen from the other side of the street. Each house is occupied by a different individual, and every house features a distinct characteristic in the following four categories: <br> - Resident's name: Arnold, Peter, Bob, Alice, Eric <br> - Car model: toyota camry, tesla model 3, bmw 3 series, honda civic, ford f150 <br> - Favorite color: blue, red, green, white, yellow <br> - Mother's name: Janelle, Penny, Holly, Kailyn, Aniya <br> The clues are provided below: <br> 1. The person who drives a Honda Civic is the one whose favorite color is yellow. <br> 2. The individual whose favorite color is red drives a Tesla Model 3. <br> 3. The owner of the BMW 3 Series does not reside in the fourth house. <br> 4. The person whose mother is named Aniya is the one who prefers blue. <br> 5. Eric's favorite color is green. <br> 6. The resident with red as their favorite color lives somewhere to the left of the owner of a Ford F-150. <br> 7. Alice is immediately to the left of Eric. <br> 8. The resident whose mother is Holly lives in the first house. <br> 9. Arnold has white as his favorite color. <br> 10. The person whose mother is Janelle loves white. <br> 11. The resident whose mother is Kailyn is Alice. <br> 12. Arnold resides somewhere to the left of Peter. <br> 13. Eric is the owner of the BMW 3 Series. <br> 14. Bob lives adjacent to the house where the owner of the Ford F-150 resides. <br> Please detail your reasoning process and provide your final solution using the JSON structure below: { "reasoning": "", "solution": { "House 1": { "Name": "", "CarModel": "", "Color": "", "Mother": "" }, "House 2": { "Name": "", "CarModel": "", "Color": "", "Mother": "" }, "House 3": { "Name": "", "CarModel": "", "Color": "", "Mother": "" }, "House 4": { "Name": "", "CarModel": "", "Color": "", "Mother": "" }, "House 5": { "Name": "", "CarModel": "", "Color": "", "Mother": "" } }]"</td>
<td style="text-align: center;">{"header": <br> {"House", <br> "Name", <br> "CarModel", <br> "Color", <br> "Mother"}, <br> "rows": [["1", <br> "Bob", "tesla <br> model 3", "red", <br> "Holly"], ["2", <br> "Arnold", "ford <br> f150", "white", <br> "Janelle"], ["3", <br> "Peter", "toyota <br> camry", "blue", <br> "Aniya"], ["4", <br> "Alice", "honda <br> civic", "yellow", <br> "Kailyn"], ["5", <br> "Eric", "bmw 3 <br> series", "green", <br> "Penny"]]]</td>
<td style="text-align: center;">Reasoning</td>
<td style="text-align: center;">ZebraLogic</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding author email: tianxliu@tencent.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
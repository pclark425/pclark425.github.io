<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-750 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-750</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-750</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-247244762</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2203.02016v3.pdf" target="_blank">Interventions, Where and How? Experimental Design for Causal Models at Scale</a></p>
                <p><strong>Paper Abstract:</strong> Causal discovery from observational and interventional data is challenging due to limited data and non-identiﬁability: factors that introduce uncertainty in estimating the underlying structural causal model (SCM). Selecting experiments (interventions) based on the uncertainty arising from both factors can expedite the identiﬁcation of the SCM. Existing methods in experimental design for causal discovery from limited data either rely on linear assumptions for the SCM or select only the intervention target. This work incorporates recent advances in Bayesian causal discovery into the Bayesian optimal experimental design framework, allowing for active causal discovery of large, nonlinear SCMs while selecting both the interventional target and the value. We demonstrate the performance of the proposed method on synthetic graphs (Erdos-Rènyi, Scale Free) for both linear and nonlinear SCMs as well as on the in-silico single-cell gene regulatory network dataset, DREAM.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e750.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e750.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CBED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Bayesian Experimental Design (CBED)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian optimal experimental design method for causal discovery that jointly selects intervention targets and continuous intervention values by estimating mutual information (MI) over outcomes and optimizing it with Bayesian optimization; includes batch strategies (Greedy-CBED and Soft-CBED).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CBED (with Greedy-CBED and Soft-CBED batching)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>CBED formulates intervention selection as maximization of the mutual information I(Y; Φ | do(X_j = v), D) between experimental outcomes and the posterior over SCMs. It estimates MI via a Monte Carlo/BALD-style estimator using posterior samples over SCMs and likelihood evaluations, treats MI as a black-box function of the continuous intervention value v for each target j, and applies Gaussian-process-based Bayesian optimization (GP-UCB) per target to find high-MI values. For batch selection it uses either a greedy submodular maximization (Greedy-CBED) that sequentially adds (target, value) pairs with largest marginal MI, or a soft top-k sampling (Soft-CBED) that forms a candidate set from BO evaluations and samples B experiments proportionally to a softmax of MI scores to form a batch in one shot.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic graphs (Erdos-Rényi, Scale-Free) and in-silico single-cell gene regulatory networks (DREAM / GeneNetWeaver)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated/virtual lab environments that support active experimentation (do-interventions). Synthetic causal graphs up to 50 variables and GeneNetWeaver steady-state and knockout simulations emulate a wet-lab setting allowing batched interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detects and resolves spurious correlations through Bayesian model uncertainty and information-theoretic selection: uses posterior disagreement (MI / BALD) to detect where models disagree (indicative of spurious or ambiguous signals) and issues targeted interventions to refute incorrect hypotheses; batching strategies allow efficient simultaneous testing.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious edge orientations and finite-sample induced non-identifiability (ambiguous/ spurious correlations due to observational equivalence and limited data). Does not assume or handle hidden confounders (causal sufficiency assumed).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Mutual information over outcomes (equivalently BALD-style disagreement) computed from posterior samples over SCMs; high MI indicates disagreement among plausible models about experimental outcomes, flagging potentially spurious/ambiguous relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Implicit Bayesian downweighting via posterior updates: after interventions, models that poorly predict outcomes receive lower posterior weight; ABCD-style importance-weighted estimators are discussed as equivalent estimators for MI but CBED primarily relies on posterior reweighting through likelihoods rather than an explicit ad-hoc downweighting rule.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Active interventions chosen to maximize expected information gain (MI) — executing high-MI interventions produces outcomes that selectively reduce posterior mass on models producing wrong predictions, thereby refuting spurious causal relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Information-theoretic active learning: maximize mutual information between outcomes and SCM (MI) for each candidate (target, value); continuous value optimization per target via GP-UCB BO; batch selection via greedy submodular maximization or soft top-k sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Soft-CBED with GP-UCB consistently outperforms baselines on interventional-aware metric E-SID in nonlinear 50-variable synthetic graphs and DREAM datasets; recovers DAG structure faster than baselines, and GP-UCB value acquisition improves performance relative to fixed or support-sampled values (qualitative claim supported by experiments; specific numeric scores reported in paper figures/tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Using fixed intervention values or sampling values from the observational support performs worse: fixed-value and sampling-from-support baselines yield slower or inferior recovery (higher E-SID); naively sampling from support can be worse than fixed values due to lower epistemic uncertainty in high-density regions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Actively selecting both intervention targets and continuous values via MI maximization and BO (GP-UCB) yields substantially better causal-graph recovery (lower E-SID) than baselines that select only targets or fix values; MI/BALD-style disagreement effectively highlights where spurious or ambiguous relationships exist and targeted interventions refute them; Soft-CBED achieves near-greedy performance with much lower runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interventions, Where and How? Experimental Design for Causal Models at Scale', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e750.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e750.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Greedy-CBED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy batch Causal Bayesian Experimental Design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch extension of CBED that greedily builds a batch by repeatedly adding the (target, value) pair that gives the largest marginal mutual information, leveraging submodularity to guarantee a (1-1/e) approximation to the optimal batch.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Greedy-CBED</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Sequentially constructs a batch of size B by, at each step, running per-target GP-UCB to select the best value for each target given the current batch and then adding the (target, value) pair with maximal marginal MI; repeats until B elements selected. Exploits submodularity and monotonicity of MI to justify greedy approximation guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same as CBED (synthetic graphs and DREAM / GeneNetWeaver)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Batch experimental setting (user prepares multiple interventions before executing), enabling parallel execution without wait-for-feedback between interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Same as CBED: detects ambiguous/spurious relationships by MI and resolves them via targeted interventions; greedy batch picks complementary interventions to maximize marginal information and thus reduce spurious hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Ambiguous edge orientations and finite-sample spurious correlations arising from observational equivalence; not designed for hidden confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Mutual information marginal gains for candidate interventions computed from posterior samples.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Posterior updating after batch outcomes, which reduces weight of models inconsistent with observed interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Choosing batches that maximize marginal MI to produce outcomes that most reduce posterior support for spurious models.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Greedy submodular maximization over MI with per-target GP-UCB value selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Performs competitively with Soft-CBED and often marginally better in some settings but at higher computational cost; both batch strategies outperform repeating a single optimal (target,value) multiple times.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Greedy-CBED is near-optimal under submodularity assumptions and achieves strong graph recovery but is more computationally expensive than Soft-CBED; batching substantially improves recovery vs single-design repeated execution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interventions, Where and How? Experimental Design for Causal Models at Scale', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e750.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e750.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Soft-CBED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Soft top-k batch Causal Bayesian Experimental Design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch CBED variant that constructs a finite candidate set from BO evaluations and samples a batch without replacement proportionally to a softmax over MI scores, greatly reducing compute while retaining competitive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Soft-CBED (soft top-k batching)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Collects T GP-UCB evaluations per node to form d×T candidate (target,value) experiments, scores each candidate by MI, then samples B experiments without replacement proportional to exp(score/temperature) (softmax) to produce a batch in one shot. This reduces the number of GP-UCB runs from O(Bd) (greedy) to O(d) and yields comparable performance.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic graphs and DREAM GeneNetWeaver simulations</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Batch-intervention virtual lab scenarios with constrained computational budget for experiment planning.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Same detection/refutation via MI; soft sampling can diversify experiments to test spurious hypotheses efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Ambiguous/ spurious causal edges due to finite data and Markov equivalence; does not target hidden confounders specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>MI scores from posterior samples on candidate interventions; candidates with high MI indicate suspected spurious/ambiguous signals.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Posterior update after outcomes; sampling softly spreads tests across hypotheses to avoid overcommitting to potentially spurious signals.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Executing a diverse, high-MI sampled batch to obtain outcomes that reduce posterior mass on incorrect graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Soft top-k sampling from candidate set scored by MI (softBALD-like), with per-target BO for value acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Soft-CBED matches Greedy-CBED's recovery accuracy (E-SID) in experiments while significantly reducing runtime; effective in DREAM and 50D synthetic nonlinear SCMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Soft-CBED provides a computationally efficient batching strategy with near-greedy causal discovery performance, making MI-driven refutation of spurious hypotheses practical at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interventions, Where and How? Experimental Design for Causal Models at Scale', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e750.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e750.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ABCD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ABCD (Budgeted experimental design for targeted causal structure discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior BOECD baseline that estimates MI via weighted importance sampling and selects intervention targets under budget constraints; does not optimize continuous intervention values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Budgeted experimental design for targeted causal structure discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ABCD</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>ABCD decomposes the BOED objective and estimates mutual information over SCMs using weighted importance sampling with weights equal to intervention likelihoods; it selects intervention targets under a budget and uses fixed intervention values (does not perform value optimization). The paper shows that for their specific choice of importance weights, ABCD's MI estimator is algebraically equivalent to the outcomes-entropy estimator used in CBED.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Benchmark synthetic and simulated causal discovery tasks (as used in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Targeted experimental design under a limited intervention budget; typically selects targets but not values.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Ambiguous graph hypotheses via limited data; ABCD seeks informative targets to reduce model uncertainty but does not explicitly address distractor features or continuous value selection.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Importance-weighted MI estimation identifies targets that maximize expected posterior change (proxy for where models disagree).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Weighted importance sampling is used to approximate updated SCM posterior weights after hypothetical outcomes; not an explicit downweighting rule beyond importance weighting for estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Intervene on high-MI targets (with fixed values) to collect outcomes that update posterior over graphs and thereby refute inconsistent models.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Budgeted target selection using an MI estimator based on importance sampling; no continuous value optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>ABCD is a competitive baseline for target selection but is outperformed by CBED variants when continuous value optimization and soft batching are used; equivalence of MI estimators is proven in the appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ABCD's MI estimator with specific importance weights is algebraically the same as CBED's outcome-entropy estimator; however ABCD's lack of value acquisition and batching strategies makes it suboptimal compared to CBED in experiments reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interventions, Where and How? Experimental Design for Causal Models at Scale', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e750.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e750.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AIT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Intervention Targeting (AIT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-network-based active method that scores candidate intervention targets by a discrepancy (F-score-type) between predicted outcomes across sampled graphs and selects targets with large model disagreement; does not originally select continuous intervention values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AIT (Active Intervention Targeting)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>AIT computes a discrepancy score per target based on variability of predicted interventional outcomes across posterior graph samples (an f-score-like metric). The paper proves that AIT's discrepancy is a Monte Carlo estimate of an approximation to mutual information when outcomes are Gaussian. In this work AIT is used as a baseline and augmented with Soft batching and GP-UCB for value acquisition in ablations (softAIT).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic graphs and DREAM benchmarks used as baselines in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive/virtual lab settings where interventions can be chosen (original AIT did not include batching/value selection; this paper augments AIT for comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detects ambiguous/spurious relations by looking at disagreement (discrepancy) in predicted outcomes across sampled graphs; like CBED, resolves them by intervening on high-discrepancy targets to gather refuting data.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Ambiguous or spurious edge orientations due to multiple plausible graphs from finite data; not designed for hidden confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Discrepancy score (variance/mean-difference based F-score style) computed from interventional samples across model samples; provably relates to MI under Gaussian outcome assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Posterior updates after observing outcomes downweight graphs inconsistent with new data; no explicit additional downweighting scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Perform interventions on high-discrepancy targets to obtain outcomes that eliminate graphs causing the discrepancy.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Select targets with high discrepancy/MIs; in this paper AIT is augmented with soft batching and optionally GP-UCB for value selection (softAIT) for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When augmented with soft batching (softAIT) and GP-UCB, AIT performance improves substantially and can recover ground-truth graphs up to ~4x faster than vanilla AIT in reported experiments; softAIT is competitive with Soft-CBED in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Original AIT without value acquisition and batching converges slower and performs worse than Soft-CBED in reported nonlinear 50D experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The AIT discrepancy is interpretable as a Monte Carlo approximation to MI under Gaussian outcomes; augmenting AIT with value acquisition and soft batching narrows the gap to CBED, highlighting the importance of continuous value selection and batching for refuting spurious hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interventions, Where and How? Experimental Design for Causal Models at Scale', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e750.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e750.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BALD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Active Learning by Disagreement (BALD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An information-theoretic acquisition strategy that selects queries maximizing mutual information between model parameters and predicted outcomes, operationalized as posterior predictive entropy minus expected conditional entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>BALD-style MI estimator</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>BALD formalizes MI between model parameters and outputs as H[Y] - E_{p(Φ|D)}[H[Y|Φ]], capturing where models disagree most; CBED derives an estimator analogous to BALD for MI over experimental outcomes using posterior samples and likelihood evaluations to guide interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General active learning / experimental design settings (used conceptually in CBED)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applies to interactive settings where one can query outcomes (interventions) and has a posterior distribution over models.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detection of spurious/ambiguous hypotheses through posterior disagreement (high predictive entropy) which flags areas where interventions are informative for refutation.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Model uncertainty induced spurious correlations or ambiguous causal structures from finite data.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>High predictive entropy under the posterior indicates disagreement about outcomes (potential spurious signals); BALD measures this directly.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Posterior conditioning on observed outcomes naturally downweights models inconsistent with outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Querying high-BALD points (here: interventions) produces outcomes that reduce posterior disagreement and thus refute spurious models.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Maximize mutual information (BALD) to select interventions/queries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Used as the conceptual basis for CBED's MI estimator; empirically leads to informative experiment selection and improved recovery in CBED variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BALD-like estimators identify interventions that maximize model disagreement; implementing BALD for interventional outcomes and optimizing values via BO yields strong practical gains in refuting spurious causal hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interventions, Where and How? Experimental Design for Causal Models at Scale', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e750.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e750.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process Upper Confidence Bound</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian optimization acquisition function that trades off exploration and exploitation by selecting points maximizing µ(v) + β σ(v) under a GP posterior, here used to optimize continuous intervention values for each target.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GP-UCB (per-target Bayesian optimization for intervention value selection)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Treats the MI as a costly black-box function over continuous intervention values for a fixed target, models it with a one-dimensional GP per target, and uses the UCB acquisition to propose the next value to evaluate; runs in parallel per target and returns per-target high-MI candidate values for batch construction.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same experimental environments used in CBED (synthetic graphs; DREAM simulations)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>One-dimensional continuous intervention domains (e.g., dosage or perturbation magnitude) per target, constrained to application-specific ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Bayesian optimization with GP surrogate and UCB acquisition to find intervention values that maximize expected MI.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>GP-UCB value acquisition improves causal discovery substantially compared to fixed-value or sampling-from-support heuristics (empirical improvement in E-SID and convergence speed reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Fixed-value and sampling-from-support strategies yield worse recovery; sampling from support can be particularly poor due to lower epistemic uncertainty in high-density regions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Optimizing continuous intervention values with GP-UCB is important: mutual information depends on value and BO-driven value selection yields better refutation of spurious correlations and faster graph recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interventions, Where and How? Experimental Design for Causal Models at Scale', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e750.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e750.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiBS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Bayesian Structure Learning (DiBS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalable variational posterior model over DAGs using latent continuous variables and Stein variational gradient descent for posterior inference, used in this paper as the SCM posterior model for nonlinear settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DiBS (variational posterior over DAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>DiBS represents uncertainty over graph structures via continuous latent variables and approximates the posterior using SVGD, enabling scalable sampling of plausible DAGs for downstream MI estimation and active experiment selection in nonlinear SCM settings.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used as posterior model for experiments on synthetic graphs and DREAM simulations</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Provides posterior samples over SCMs/graphs enabling Bayesian MI estimators and acquisition functions to identify spurious/ambiguous relations for intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Provides model uncertainty over graph structures to expose ambiguous/spurious edges due to finite data (but does not explicitly remove distractor variables).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>By producing diverse posterior samples, DiBS enables MI/discrepancy computations that detect disagreement-induced spurious hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Posterior reweighting occurs via Bayesian update after interventions; DiBS provides the approximate posterior to update.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Used as the posterior model whose samples are interrogated by high-MI interventions to refute incorrect graph hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Serves as the posterior engine driving CBED/AIT MI computations and thus supports active experimental design.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Using DiBS as the posterior model enables CBED to scale to nonlinear SCMs and achieve strong recovery on 50D tasks; empirical improvements shown in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Scalable Bayesian posterior models like DiBS are crucial enablers for MI-based active experimental design in nonlinear, higher-dimensional SCMs; they expose model disagreement necessary for detecting spurious/ambiguous causal relations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interventions, Where and How? Experimental Design for Causal Models at Scale', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e750.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e750.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAG-bootstrap / GIES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAG-bootstrap with GIES (Greedy Interventional Equivalence Search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bootstrap-based baseline that samples DAGs by bootstrapping data and running interventional structure learning (GIES) to form an approximate posterior over graphs for MI estimation or baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DAG-bootstrap (with GIES)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Bootstrap observational and interventional data to generate multiple graph hypotheses using GIES, treat these as approximate posterior samples p(G|D) for downstream MI estimation; used as an alternative posterior sampling method in linear SCM experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic graphs used in some experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interactive—used to obtain posterior samples for planning interventions rather than actively optimized within a continuous-value BO loop in this paper's linear experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Provides multiple graph hypotheses reflecting uncertainty from finite data; does not itself identify distractor variables.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Used to produce candidate graphs whose disagreement can be reduced by subsequent interventions selected by CBED/AIT.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DAG-bootstrap provides a simple way to obtain diverse graph hypotheses for MI estimation but scales less well to nonlinear/posterior-rich settings compared to variational methods like DiBS.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interventions, Where and How? Experimental Design for Causal Models at Scale', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e750.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e750.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active ICP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Invariant Causal Prediction (Active ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active method that builds on invariant causal prediction to select experiments that test invariances/stability and thus identify causal parents via stability criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Active ICP</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses the invariant causal prediction framework to choose interventions actively so as to validate invariance properties and select causal parents whose conditional distributions remain stable across interventions; focuses on stability rather than MI maximization.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General causal discovery settings (mentioned as related work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive experimental settings where invariance tests can be performed across different interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detects spurious predictors by testing instability across environments (if a variable's conditional distribution changes, it is less likely to be a causal parent), thereby downweighting non-invariant/distractor variables.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Non-invariant associations (selection bias/ distribution shifts and spurious correlations due to interventions or environment changes).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Tests for invariance/stability of conditional distributions across environments/interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Variables failing invariance tests are de-prioritized or rejected as causes.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Design interventions to create environments that expose instability in spurious associations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Active selection of interventions to maximize informativeness for invariance tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Active ICP conceptually targets spurious/unstable predictors by actively seeking environments where non-causal associations break, but it differs from MI-based BOECD approaches and was mentioned as related work rather than used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interventions, Where and How? Experimental Design for Causal Models at Scale', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e750.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e750.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>vonKugelgen-GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>von Kügelgen et al. GP approach</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that models posteriors over conditional mechanisms with Gaussian Processes and then uses BO to choose continuous intervention values; not shown to scale beyond bivariate graphs in that work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>von Kügelgen et al. GP posterior + BO</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Models conditional mechanisms with multi-dimensional Gaussian processes to represent posterior predictive distributions and uses BO to find informative intervention values; scaling issues arise for larger graphs due to GP dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Small-scale / bivariate simulated causal systems (as discussed in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive experimental design with continuous intervention values, but prior work limited to very small graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>BO over GP-modeled MI-like objectives for value selection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates BO over value space with GP-modeled conditionals but limited scalability motivated the CBED design choices (one-dimensional per-target GPs and variational posterior models).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interventions, Where and How? Experimental Design for Causal Models at Scale', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Budgeted experimental design for targeted causal structure discovery <em>(Rating: 2)</em></li>
                <li>Active intervention targeting for causal discovery (Scherrer et al., 2021) <em>(Rating: 2)</em></li>
                <li>SoftBALD: Stochastic acquisition / soft top-k sampling (Kirsch et al., 2021) <em>(Rating: 2)</em></li>
                <li>Variational causal networks: Approximate bayesian inference over causal structures <em>(Rating: 2)</em></li>
                <li>DiBS: Differentiable Bayesian Structure Learning (Lorch et al., 2021) <em>(Rating: 2)</em></li>
                <li>Active invariant causal prediction: Experiment selection through stability <em>(Rating: 1)</em></li>
                <li>Efficient bayesian experimental design for implicit models (Kleinegesse & Gutmann) <em>(Rating: 1)</em></li>
                <li>von Kügelgen et al. (2019) GP-based posterior + BO for interventions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-750",
    "paper_id": "paper-247244762",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "CBED",
            "name_full": "Causal Bayesian Experimental Design (CBED)",
            "brief_description": "A Bayesian optimal experimental design method for causal discovery that jointly selects intervention targets and continuous intervention values by estimating mutual information (MI) over outcomes and optimizing it with Bayesian optimization; includes batch strategies (Greedy-CBED and Soft-CBED).",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "CBED (with Greedy-CBED and Soft-CBED batching)",
            "method_description": "CBED formulates intervention selection as maximization of the mutual information I(Y; Φ | do(X_j = v), D) between experimental outcomes and the posterior over SCMs. It estimates MI via a Monte Carlo/BALD-style estimator using posterior samples over SCMs and likelihood evaluations, treats MI as a black-box function of the continuous intervention value v for each target j, and applies Gaussian-process-based Bayesian optimization (GP-UCB) per target to find high-MI values. For batch selection it uses either a greedy submodular maximization (Greedy-CBED) that sequentially adds (target, value) pairs with largest marginal MI, or a soft top-k sampling (Soft-CBED) that forms a candidate set from BO evaluations and samples B experiments proportionally to a softmax of MI scores to form a batch in one shot.",
            "environment_name": "Synthetic graphs (Erdos-Rényi, Scale-Free) and in-silico single-cell gene regulatory networks (DREAM / GeneNetWeaver)",
            "environment_description": "Simulated/virtual lab environments that support active experimentation (do-interventions). Synthetic causal graphs up to 50 variables and GeneNetWeaver steady-state and knockout simulations emulate a wet-lab setting allowing batched interventions.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detects and resolves spurious correlations through Bayesian model uncertainty and information-theoretic selection: uses posterior disagreement (MI / BALD) to detect where models disagree (indicative of spurious or ambiguous signals) and issues targeted interventions to refute incorrect hypotheses; batching strategies allow efficient simultaneous testing.",
            "spurious_signal_types": "Spurious edge orientations and finite-sample induced non-identifiability (ambiguous/ spurious correlations due to observational equivalence and limited data). Does not assume or handle hidden confounders (causal sufficiency assumed).",
            "detection_method": "Mutual information over outcomes (equivalently BALD-style disagreement) computed from posterior samples over SCMs; high MI indicates disagreement among plausible models about experimental outcomes, flagging potentially spurious/ambiguous relationships.",
            "downweighting_method": "Implicit Bayesian downweighting via posterior updates: after interventions, models that poorly predict outcomes receive lower posterior weight; ABCD-style importance-weighted estimators are discussed as equivalent estimators for MI but CBED primarily relies on posterior reweighting through likelihoods rather than an explicit ad-hoc downweighting rule.",
            "refutation_method": "Active interventions chosen to maximize expected information gain (MI) — executing high-MI interventions produces outcomes that selectively reduce posterior mass on models producing wrong predictions, thereby refuting spurious causal relationships.",
            "uses_active_learning": true,
            "inquiry_strategy": "Information-theoretic active learning: maximize mutual information between outcomes and SCM (MI) for each candidate (target, value); continuous value optimization per target via GP-UCB BO; batch selection via greedy submodular maximization or soft top-k sampling.",
            "performance_with_robustness": "Soft-CBED with GP-UCB consistently outperforms baselines on interventional-aware metric E-SID in nonlinear 50-variable synthetic graphs and DREAM datasets; recovers DAG structure faster than baselines, and GP-UCB value acquisition improves performance relative to fixed or support-sampled values (qualitative claim supported by experiments; specific numeric scores reported in paper figures/tables).",
            "performance_without_robustness": "Using fixed intervention values or sampling values from the observational support performs worse: fixed-value and sampling-from-support baselines yield slower or inferior recovery (higher E-SID); naively sampling from support can be worse than fixed values due to lower epistemic uncertainty in high-density regions.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Actively selecting both intervention targets and continuous values via MI maximization and BO (GP-UCB) yields substantially better causal-graph recovery (lower E-SID) than baselines that select only targets or fix values; MI/BALD-style disagreement effectively highlights where spurious or ambiguous relationships exist and targeted interventions refute them; Soft-CBED achieves near-greedy performance with much lower runtime.",
            "uuid": "e750.0",
            "source_info": {
                "paper_title": "Interventions, Where and How? Experimental Design for Causal Models at Scale",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Greedy-CBED",
            "name_full": "Greedy batch Causal Bayesian Experimental Design",
            "brief_description": "A batch extension of CBED that greedily builds a batch by repeatedly adding the (target, value) pair that gives the largest marginal mutual information, leveraging submodularity to guarantee a (1-1/e) approximation to the optimal batch.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Greedy-CBED",
            "method_description": "Sequentially constructs a batch of size B by, at each step, running per-target GP-UCB to select the best value for each target given the current batch and then adding the (target, value) pair with maximal marginal MI; repeats until B elements selected. Exploits submodularity and monotonicity of MI to justify greedy approximation guarantees.",
            "environment_name": "Same as CBED (synthetic graphs and DREAM / GeneNetWeaver)",
            "environment_description": "Batch experimental setting (user prepares multiple interventions before executing), enabling parallel execution without wait-for-feedback between interventions.",
            "handles_distractors": true,
            "distractor_handling_technique": "Same as CBED: detects ambiguous/spurious relationships by MI and resolves them via targeted interventions; greedy batch picks complementary interventions to maximize marginal information and thus reduce spurious hypotheses.",
            "spurious_signal_types": "Ambiguous edge orientations and finite-sample spurious correlations arising from observational equivalence; not designed for hidden confounding.",
            "detection_method": "Mutual information marginal gains for candidate interventions computed from posterior samples.",
            "downweighting_method": "Posterior updating after batch outcomes, which reduces weight of models inconsistent with observed interventions.",
            "refutation_method": "Choosing batches that maximize marginal MI to produce outcomes that most reduce posterior support for spurious models.",
            "uses_active_learning": true,
            "inquiry_strategy": "Greedy submodular maximization over MI with per-target GP-UCB value selection.",
            "performance_with_robustness": "Performs competitively with Soft-CBED and often marginally better in some settings but at higher computational cost; both batch strategies outperform repeating a single optimal (target,value) multiple times.",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Greedy-CBED is near-optimal under submodularity assumptions and achieves strong graph recovery but is more computationally expensive than Soft-CBED; batching substantially improves recovery vs single-design repeated execution.",
            "uuid": "e750.1",
            "source_info": {
                "paper_title": "Interventions, Where and How? Experimental Design for Causal Models at Scale",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Soft-CBED",
            "name_full": "Soft top-k batch Causal Bayesian Experimental Design",
            "brief_description": "A batch CBED variant that constructs a finite candidate set from BO evaluations and samples a batch without replacement proportionally to a softmax over MI scores, greatly reducing compute while retaining competitive performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Soft-CBED (soft top-k batching)",
            "method_description": "Collects T GP-UCB evaluations per node to form d×T candidate (target,value) experiments, scores each candidate by MI, then samples B experiments without replacement proportional to exp(score/temperature) (softmax) to produce a batch in one shot. This reduces the number of GP-UCB runs from O(Bd) (greedy) to O(d) and yields comparable performance.",
            "environment_name": "Synthetic graphs and DREAM GeneNetWeaver simulations",
            "environment_description": "Batch-intervention virtual lab scenarios with constrained computational budget for experiment planning.",
            "handles_distractors": true,
            "distractor_handling_technique": "Same detection/refutation via MI; soft sampling can diversify experiments to test spurious hypotheses efficiently.",
            "spurious_signal_types": "Ambiguous/ spurious causal edges due to finite data and Markov equivalence; does not target hidden confounders specifically.",
            "detection_method": "MI scores from posterior samples on candidate interventions; candidates with high MI indicate suspected spurious/ambiguous signals.",
            "downweighting_method": "Posterior update after outcomes; sampling softly spreads tests across hypotheses to avoid overcommitting to potentially spurious signals.",
            "refutation_method": "Executing a diverse, high-MI sampled batch to obtain outcomes that reduce posterior mass on incorrect graphs.",
            "uses_active_learning": true,
            "inquiry_strategy": "Soft top-k sampling from candidate set scored by MI (softBALD-like), with per-target BO for value acquisition.",
            "performance_with_robustness": "Soft-CBED matches Greedy-CBED's recovery accuracy (E-SID) in experiments while significantly reducing runtime; effective in DREAM and 50D synthetic nonlinear SCMs.",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Soft-CBED provides a computationally efficient batching strategy with near-greedy causal discovery performance, making MI-driven refutation of spurious hypotheses practical at scale.",
            "uuid": "e750.2",
            "source_info": {
                "paper_title": "Interventions, Where and How? Experimental Design for Causal Models at Scale",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "ABCD",
            "name_full": "ABCD (Budgeted experimental design for targeted causal structure discovery)",
            "brief_description": "A prior BOECD baseline that estimates MI via weighted importance sampling and selects intervention targets under budget constraints; does not optimize continuous intervention values.",
            "citation_title": "Budgeted experimental design for targeted causal structure discovery",
            "mention_or_use": "use",
            "method_name": "ABCD",
            "method_description": "ABCD decomposes the BOED objective and estimates mutual information over SCMs using weighted importance sampling with weights equal to intervention likelihoods; it selects intervention targets under a budget and uses fixed intervention values (does not perform value optimization). The paper shows that for their specific choice of importance weights, ABCD's MI estimator is algebraically equivalent to the outcomes-entropy estimator used in CBED.",
            "environment_name": "Benchmark synthetic and simulated causal discovery tasks (as used in comparisons)",
            "environment_description": "Targeted experimental design under a limited intervention budget; typically selects targets but not values.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Ambiguous graph hypotheses via limited data; ABCD seeks informative targets to reduce model uncertainty but does not explicitly address distractor features or continuous value selection.",
            "detection_method": "Importance-weighted MI estimation identifies targets that maximize expected posterior change (proxy for where models disagree).",
            "downweighting_method": "Weighted importance sampling is used to approximate updated SCM posterior weights after hypothetical outcomes; not an explicit downweighting rule beyond importance weighting for estimation.",
            "refutation_method": "Intervene on high-MI targets (with fixed values) to collect outcomes that update posterior over graphs and thereby refute inconsistent models.",
            "uses_active_learning": true,
            "inquiry_strategy": "Budgeted target selection using an MI estimator based on importance sampling; no continuous value optimization.",
            "performance_with_robustness": "ABCD is a competitive baseline for target selection but is outperformed by CBED variants when continuous value optimization and soft batching are used; equivalence of MI estimators is proven in the appendix.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "ABCD's MI estimator with specific importance weights is algebraically the same as CBED's outcome-entropy estimator; however ABCD's lack of value acquisition and batching strategies makes it suboptimal compared to CBED in experiments reported here.",
            "uuid": "e750.3",
            "source_info": {
                "paper_title": "Interventions, Where and How? Experimental Design for Causal Models at Scale",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "AIT",
            "name_full": "Active Intervention Targeting (AIT)",
            "brief_description": "A neural-network-based active method that scores candidate intervention targets by a discrepancy (F-score-type) between predicted outcomes across sampled graphs and selects targets with large model disagreement; does not originally select continuous intervention values.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "AIT (Active Intervention Targeting)",
            "method_description": "AIT computes a discrepancy score per target based on variability of predicted interventional outcomes across posterior graph samples (an f-score-like metric). The paper proves that AIT's discrepancy is a Monte Carlo estimate of an approximation to mutual information when outcomes are Gaussian. In this work AIT is used as a baseline and augmented with Soft batching and GP-UCB for value acquisition in ablations (softAIT).",
            "environment_name": "Synthetic graphs and DREAM benchmarks used as baselines in comparisons",
            "environment_description": "Interactive/virtual lab settings where interventions can be chosen (original AIT did not include batching/value selection; this paper augments AIT for comparisons).",
            "handles_distractors": true,
            "distractor_handling_technique": "Detects ambiguous/spurious relations by looking at disagreement (discrepancy) in predicted outcomes across sampled graphs; like CBED, resolves them by intervening on high-discrepancy targets to gather refuting data.",
            "spurious_signal_types": "Ambiguous or spurious edge orientations due to multiple plausible graphs from finite data; not designed for hidden confounders.",
            "detection_method": "Discrepancy score (variance/mean-difference based F-score style) computed from interventional samples across model samples; provably relates to MI under Gaussian outcome assumptions.",
            "downweighting_method": "Posterior updates after observing outcomes downweight graphs inconsistent with new data; no explicit additional downweighting scheme.",
            "refutation_method": "Perform interventions on high-discrepancy targets to obtain outcomes that eliminate graphs causing the discrepancy.",
            "uses_active_learning": true,
            "inquiry_strategy": "Select targets with high discrepancy/MIs; in this paper AIT is augmented with soft batching and optionally GP-UCB for value selection (softAIT) for experiments.",
            "performance_with_robustness": "When augmented with soft batching (softAIT) and GP-UCB, AIT performance improves substantially and can recover ground-truth graphs up to ~4x faster than vanilla AIT in reported experiments; softAIT is competitive with Soft-CBED in some settings.",
            "performance_without_robustness": "Original AIT without value acquisition and batching converges slower and performs worse than Soft-CBED in reported nonlinear 50D experiments.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "The AIT discrepancy is interpretable as a Monte Carlo approximation to MI under Gaussian outcomes; augmenting AIT with value acquisition and soft batching narrows the gap to CBED, highlighting the importance of continuous value selection and batching for refuting spurious hypotheses.",
            "uuid": "e750.4",
            "source_info": {
                "paper_title": "Interventions, Where and How? Experimental Design for Causal Models at Scale",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "BALD",
            "name_full": "Bayesian Active Learning by Disagreement (BALD)",
            "brief_description": "An information-theoretic acquisition strategy that selects queries maximizing mutual information between model parameters and predicted outcomes, operationalized as posterior predictive entropy minus expected conditional entropy.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "method_name": "BALD-style MI estimator",
            "method_description": "BALD formalizes MI between model parameters and outputs as H[Y] - E_{p(Φ|D)}[H[Y|Φ]], capturing where models disagree most; CBED derives an estimator analogous to BALD for MI over experimental outcomes using posterior samples and likelihood evaluations to guide interventions.",
            "environment_name": "General active learning / experimental design settings (used conceptually in CBED)",
            "environment_description": "Applies to interactive settings where one can query outcomes (interventions) and has a posterior distribution over models.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detection of spurious/ambiguous hypotheses through posterior disagreement (high predictive entropy) which flags areas where interventions are informative for refutation.",
            "spurious_signal_types": "Model uncertainty induced spurious correlations or ambiguous causal structures from finite data.",
            "detection_method": "High predictive entropy under the posterior indicates disagreement about outcomes (potential spurious signals); BALD measures this directly.",
            "downweighting_method": "Posterior conditioning on observed outcomes naturally downweights models inconsistent with outcomes.",
            "refutation_method": "Querying high-BALD points (here: interventions) produces outcomes that reduce posterior disagreement and thus refute spurious models.",
            "uses_active_learning": true,
            "inquiry_strategy": "Maximize mutual information (BALD) to select interventions/queries.",
            "performance_with_robustness": "Used as the conceptual basis for CBED's MI estimator; empirically leads to informative experiment selection and improved recovery in CBED variants.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "BALD-like estimators identify interventions that maximize model disagreement; implementing BALD for interventional outcomes and optimizing values via BO yields strong practical gains in refuting spurious causal hypotheses.",
            "uuid": "e750.5",
            "source_info": {
                "paper_title": "Interventions, Where and How? Experimental Design for Causal Models at Scale",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "GP-UCB",
            "name_full": "Gaussian Process Upper Confidence Bound",
            "brief_description": "A Bayesian optimization acquisition function that trades off exploration and exploitation by selecting points maximizing µ(v) + β σ(v) under a GP posterior, here used to optimize continuous intervention values for each target.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "GP-UCB (per-target Bayesian optimization for intervention value selection)",
            "method_description": "Treats the MI as a costly black-box function over continuous intervention values for a fixed target, models it with a one-dimensional GP per target, and uses the UCB acquisition to propose the next value to evaluate; runs in parallel per target and returns per-target high-MI candidate values for batch construction.",
            "environment_name": "Same experimental environments used in CBED (synthetic graphs; DREAM simulations)",
            "environment_description": "One-dimensional continuous intervention domains (e.g., dosage or perturbation magnitude) per target, constrained to application-specific ranges.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "Bayesian optimization with GP surrogate and UCB acquisition to find intervention values that maximize expected MI.",
            "performance_with_robustness": "GP-UCB value acquisition improves causal discovery substantially compared to fixed-value or sampling-from-support heuristics (empirical improvement in E-SID and convergence speed reported).",
            "performance_without_robustness": "Fixed-value and sampling-from-support strategies yield worse recovery; sampling from support can be particularly poor due to lower epistemic uncertainty in high-density regions.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Optimizing continuous intervention values with GP-UCB is important: mutual information depends on value and BO-driven value selection yields better refutation of spurious correlations and faster graph recovery.",
            "uuid": "e750.6",
            "source_info": {
                "paper_title": "Interventions, Where and How? Experimental Design for Causal Models at Scale",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "DiBS",
            "name_full": "Differentiable Bayesian Structure Learning (DiBS)",
            "brief_description": "A scalable variational posterior model over DAGs using latent continuous variables and Stein variational gradient descent for posterior inference, used in this paper as the SCM posterior model for nonlinear settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "DiBS (variational posterior over DAGs)",
            "method_description": "DiBS represents uncertainty over graph structures via continuous latent variables and approximates the posterior using SVGD, enabling scalable sampling of plausible DAGs for downstream MI estimation and active experiment selection in nonlinear SCM settings.",
            "environment_name": "Used as posterior model for experiments on synthetic graphs and DREAM simulations",
            "environment_description": "Provides posterior samples over SCMs/graphs enabling Bayesian MI estimators and acquisition functions to identify spurious/ambiguous relations for intervention.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Provides model uncertainty over graph structures to expose ambiguous/spurious edges due to finite data (but does not explicitly remove distractor variables).",
            "detection_method": "By producing diverse posterior samples, DiBS enables MI/discrepancy computations that detect disagreement-induced spurious hypotheses.",
            "downweighting_method": "Posterior reweighting occurs via Bayesian update after interventions; DiBS provides the approximate posterior to update.",
            "refutation_method": "Used as the posterior model whose samples are interrogated by high-MI interventions to refute incorrect graph hypotheses.",
            "uses_active_learning": true,
            "inquiry_strategy": "Serves as the posterior engine driving CBED/AIT MI computations and thus supports active experimental design.",
            "performance_with_robustness": "Using DiBS as the posterior model enables CBED to scale to nonlinear SCMs and achieve strong recovery on 50D tasks; empirical improvements shown in the paper.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Scalable Bayesian posterior models like DiBS are crucial enablers for MI-based active experimental design in nonlinear, higher-dimensional SCMs; they expose model disagreement necessary for detecting spurious/ambiguous causal relations.",
            "uuid": "e750.7",
            "source_info": {
                "paper_title": "Interventions, Where and How? Experimental Design for Causal Models at Scale",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "DAG-bootstrap / GIES",
            "name_full": "DAG-bootstrap with GIES (Greedy Interventional Equivalence Search)",
            "brief_description": "A bootstrap-based baseline that samples DAGs by bootstrapping data and running interventional structure learning (GIES) to form an approximate posterior over graphs for MI estimation or baseline comparisons.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "DAG-bootstrap (with GIES)",
            "method_description": "Bootstrap observational and interventional data to generate multiple graph hypotheses using GIES, treat these as approximate posterior samples p(G|D) for downstream MI estimation; used as an alternative posterior sampling method in linear SCM experiments.",
            "environment_name": "Synthetic graphs used in some experiments",
            "environment_description": "Non-interactive—used to obtain posterior samples for planning interventions rather than actively optimized within a continuous-value BO loop in this paper's linear experiments.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Provides multiple graph hypotheses reflecting uncertainty from finite data; does not itself identify distractor variables.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": "Used to produce candidate graphs whose disagreement can be reduced by subsequent interventions selected by CBED/AIT.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "DAG-bootstrap provides a simple way to obtain diverse graph hypotheses for MI estimation but scales less well to nonlinear/posterior-rich settings compared to variational methods like DiBS.",
            "uuid": "e750.8",
            "source_info": {
                "paper_title": "Interventions, Where and How? Experimental Design for Causal Models at Scale",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Active ICP",
            "name_full": "Active Invariant Causal Prediction (Active ICP)",
            "brief_description": "An active method that builds on invariant causal prediction to select experiments that test invariances/stability and thus identify causal parents via stability criteria.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "method_name": "Active ICP",
            "method_description": "Uses the invariant causal prediction framework to choose interventions actively so as to validate invariance properties and select causal parents whose conditional distributions remain stable across interventions; focuses on stability rather than MI maximization.",
            "environment_name": "General causal discovery settings (mentioned as related work)",
            "environment_description": "Interactive experimental settings where invariance tests can be performed across different interventions.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detects spurious predictors by testing instability across environments (if a variable's conditional distribution changes, it is less likely to be a causal parent), thereby downweighting non-invariant/distractor variables.",
            "spurious_signal_types": "Non-invariant associations (selection bias/ distribution shifts and spurious correlations due to interventions or environment changes).",
            "detection_method": "Tests for invariance/stability of conditional distributions across environments/interventions.",
            "downweighting_method": "Variables failing invariance tests are de-prioritized or rejected as causes.",
            "refutation_method": "Design interventions to create environments that expose instability in spurious associations.",
            "uses_active_learning": true,
            "inquiry_strategy": "Active selection of interventions to maximize informativeness for invariance tests.",
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Active ICP conceptually targets spurious/unstable predictors by actively seeking environments where non-causal associations break, but it differs from MI-based BOECD approaches and was mentioned as related work rather than used in experiments.",
            "uuid": "e750.9",
            "source_info": {
                "paper_title": "Interventions, Where and How? Experimental Design for Causal Models at Scale",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "vonKugelgen-GP",
            "name_full": "von Kügelgen et al. GP approach",
            "brief_description": "A prior approach that models posteriors over conditional mechanisms with Gaussian Processes and then uses BO to choose continuous intervention values; not shown to scale beyond bivariate graphs in that work.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "method_name": "von Kügelgen et al. GP posterior + BO",
            "method_description": "Models conditional mechanisms with multi-dimensional Gaussian processes to represent posterior predictive distributions and uses BO to find informative intervention values; scaling issues arise for larger graphs due to GP dimensionality.",
            "environment_name": "Small-scale / bivariate simulated causal systems (as discussed in related work)",
            "environment_description": "Interactive experimental design with continuous intervention values, but prior work limited to very small graphs.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "BO over GP-modeled MI-like objectives for value selection",
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Demonstrates BO over value space with GP-modeled conditionals but limited scalability motivated the CBED design choices (one-dimensional per-target GPs and variational posterior models).",
            "uuid": "e750.10",
            "source_info": {
                "paper_title": "Interventions, Where and How? Experimental Design for Causal Models at Scale",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Budgeted experimental design for targeted causal structure discovery",
            "rating": 2,
            "sanitized_title": "budgeted_experimental_design_for_targeted_causal_structure_discovery"
        },
        {
            "paper_title": "Active intervention targeting for causal discovery (Scherrer et al., 2021)",
            "rating": 2,
            "sanitized_title": "active_intervention_targeting_for_causal_discovery_scherrer_et_al_2021"
        },
        {
            "paper_title": "SoftBALD: Stochastic acquisition / soft top-k sampling (Kirsch et al., 2021)",
            "rating": 2,
            "sanitized_title": "softbald_stochastic_acquisition_soft_topk_sampling_kirsch_et_al_2021"
        },
        {
            "paper_title": "Variational causal networks: Approximate bayesian inference over causal structures",
            "rating": 2,
            "sanitized_title": "variational_causal_networks_approximate_bayesian_inference_over_causal_structures"
        },
        {
            "paper_title": "DiBS: Differentiable Bayesian Structure Learning (Lorch et al., 2021)",
            "rating": 2,
            "sanitized_title": "dibs_differentiable_bayesian_structure_learning_lorch_et_al_2021"
        },
        {
            "paper_title": "Active invariant causal prediction: Experiment selection through stability",
            "rating": 1,
            "sanitized_title": "active_invariant_causal_prediction_experiment_selection_through_stability"
        },
        {
            "paper_title": "Efficient bayesian experimental design for implicit models (Kleinegesse & Gutmann)",
            "rating": 1,
            "sanitized_title": "efficient_bayesian_experimental_design_for_implicit_models_kleinegesse_gutmann"
        },
        {
            "paper_title": "von Kügelgen et al. (2019) GP-based posterior + BO for interventions",
            "rating": 1,
            "sanitized_title": "von_kügelgen_et_al_2019_gpbased_posterior_bo_for_interventions"
        }
    ],
    "cost": 0.020925999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Interventions, Where and How? Experimental Design for Causal Models at Scale</p>
<p>Panagiotis Tigas 
OATML
University of Oxford</p>
<p>Yashas Annadani 
KTH Royal Institute of Technology
Stockholm</p>
<p>Max Planck Institute for Intelligent Systems</p>
<p>Andrew Jesson 
OATML
University of Oxford</p>
<p>Bernhard Schölkopf 
Max Planck Institute for Intelligent Systems</p>
<p>Yarin Gal 
OATML
University of Oxford</p>
<p>Stefan Bauer 
KTH Royal Institute of Technology
Stockholm</p>
<p>CIFAR Azrieli Global Scholar</p>
<p>Interventions, Where and How? Experimental Design for Causal Models at Scale</p>
<p>Causal discovery from observational and interventional data is challenging due to limited data and non-identifiability: factors that introduce uncertainty in estimating the underlying structural causal model (SCM). Selecting experiments (interventions) based on the uncertainty arising from both factors can expedite the identification of the SCM. Existing methods in experimental design for causal discovery from limited data either rely on linear assumptions for the SCM or select only the intervention target. This work incorporates recent advances in Bayesian causal discovery into the Bayesian optimal experimental design framework, allowing for active causal discovery of large, nonlinear SCMs while selecting both the interventional target and the value. We demonstrate the performance of the proposed method on synthetic graphs (Erdos-Rènyi, Scale Free) for both linear and nonlinear SCMs as well as on the in-silico single-cell gene regulatory network dataset, DREAM. simple baseline for batch active learning with stochastic acquisition functions. arXiv preprint arXiv:2106.12059, 2021.Steven Kleinegesse and Michael U Gutmann. Efficient bayesian experimental design for implicit models. . Near-optimal nonmyopic value of information in graphical models. arXiv preprint arXiv:1207.1394, 2012. Harold J Kushner. A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise. 1964. . Policybased bayesian experimental design for non-differentiable implicit models. arXiv preprint arXiv:2203.04272, 2022. Dennis V Lindley. On a measure of the information provided by an experiment. The Annals of Mathematical Statistics, pages 986-1005, 1956. Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. arXiv preprint arXiv:1608.04471, 2016. . Learning latent permutations with gumbel-sinkhorn networks. arXiv preprint arXiv:1802.08665, 2018. Jonas Močkus. On bayesian methods for seeking the extremum. In Optimization techniques IFIP technical conference, pages 400-404. Springer, 1975. Kevin P Murphy. Active learning of causal bayes net structure. 2001. George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maximizing submodular set functions-i. Mathematical programming, 14(1):265-294, 1978. Robert Osazuwa Ness, Karen Sachs, Parag Mallick, and Olga Vitek. A bayesian active learning experimental design for inferring signaling networks.</p>
<p>What is the structure of the protein-signaling network derived from a single cell? How do different habits influence the presence of disease? Such questions refer to causal effects in complex systems governed by nonlinear, noisy processes. On most occasions, passive observation of such systems is insufficient to uncover the real cause-effect relationship and costly experimentation is required to disambiguate between competing hypotheses. As such, the design of experiments is of significant interest; an efficient experimentation protocol helps reduce the costs involved in experimentation while aiding the process of producing knowledge through the (closed-loop / policy-driven) scientific method (Fig. 1).</p>
<p>In the language of causality (Pearl, 2009), the causal relationships are represented qualitatively by a directed acyclic graph (DAG), where the nodes correspond to different variables of the system of study and the edges represent the flow of information between the variables. The abstraction of DAGs allows us to represent the space of possible explanations (hypotheses) for the observations at hand. Representing such hypotheses as Bayesian probabilities (beliefs) allows us to formalize the problem of the scientific method as one of Bayesian inference, where the goal is to estimate the posterior distribution p(DAGs | Observations). A posterior distribution over the DAGs allows us to employ information-theoretic acquisition functions that guide experimentation towards the most informative variables for disambiguating between competing hypotheses. Such design procedures belong to the field of Bayesian Optimal Experimental Design (Lindley, 1956) for Causal Discovery (BOECD) (Tong andKoller, 2001, Murphy, 2001).</p>
<p>In the Bayesian Optimal Experimental Design (BOED) (Lindley, 1956) framework, one seeks the experiment that maximizes the expected information gain about some parameter(s) of interest. In causal discovery, an experiment takes the form of a causal intervention, and the parameters of interest are the Structural Causal Model (SCM) and its associated DAG.</p>
<p>An intervention in a causal model refers to the variable (or target) we manipulate and the value (or strength) at which we set the variable. Hence, the design space in the case of learning causal models is the set of all subsets of the intervention targets and the possibly countably infinite set of intervention values of the chosen targets. The intervention value encapsulates important semantics in many causal inference applications. For instance, in medical applications, an intervention can correspond to the administration of different drugs and the intervention value takes the form of a dosage level for each drug. Even though the appropriate choice of this value is crucial for identifying the underlying causal model, existing work on active causal discovery focuses exclusively on selecting the intervention target (Agrawal et al., 2019, Cho et al., 2016. There, the intervention value is generally some arbitrary fixed value (like 0) which is suboptimal (see Fig. 2a). Hence, a holistic treatment of selecting the intervention value and the target in the general case of nonlinear causal models has been missing. We present a Bayesian experimental design method (CBED -pronounced "seabed") to acquire optimal intervention targets and values by performing Bayesian optimization.</p>
<p>Additionally, some settings call for the selection of a batch of interventions. The problem of batched interventions is computationally expensive as it requires evaluating all possible combinations of interventions. We extend CBED to the batch setting and propose two different batching strategies for tractable, Bayes optimal acquisition of both intervention targets and values. The first strategy -Greedy-CBED -builds up the intervention set greedily. A greedy heuristic is still near-optimal due to submodularity properties of mutual information (Krause and Guestrin, 2012, Agrawal et al., 2019, Kirsch et al., 2019. The second strategy -Soft-CBED -constructs a set of interventions by stochastic sampling from a finite set of candidates, thereby significantly increasing computational efficiency while recovering the DAG structure and the parameters of the SCM as fast as the greedy strategy. This strategy is well suited for resource-constrained settings.</p>
<p>Throughout this work, we make the following standard assumptions for causal discovery (Peters et al., 2017): Assumption 1 (Causal Sufficiency). There are no hidden confounders, and all the random variables of interest are observable. Assumption 2 (Finite Samples). There is a finite number of observational/ interventional samples available.</p>
<p>Assumption 3 (Nonlinear SCM with Additive Noise). The structural causal model has nonlinear conditional expectations with additive Gaussian noise. Assumption 4 (Single Target). Each intervention is atomic and applied to a single target of the SCM.</p>
<p>Additionally, we assume that interventions are planned and executed in batches of size B, with a fixed budget of total interventions given by Number of Batches × B. We also assume that the underlying graph is sparse, as is the case in all the real-world settings (Bengio et al., 2019, Schmidt et al., 2007. Experimental design is preferable in sparse graph settings as the number of informative intervention targets and values would be significantly less compared to dense graphs. Many nodes corresponding to a sparse graph would have very less probability of having parent sets, and hence preforming experiments with a random policy is not maximally informative. Finally, we are interested in recovering the full graph G with a small number of batches. As with all causal inference tasks, the assumptions that we make above have to be carefully verified for the application of interest.</p>
<p>We show that our methods, Greedy-CBED and Soft-CBED, perform better than the state-of-the-art active causal discovery baselines in linear and nonlinear SCM settings. In addition, our approach achieves superior results in the real-world inspired nonlinear dataset, DREAM (Greenfield et al., 2010).</p>
<p>Background</p>
<p>Notation. Let V = {1, . . . , d} be the vertex set of any DAG g = (V, E) and X V = {X 1 , . . . , X d } ⊆ X be the random variables of interest indexed by V. We have an initial observational
dataset D = {x V (i) } n i=1 comprised of instances x V ∼ P (X 1 = x 1 , . . . , X d = x d ) = p(x 1 , . . . , x d ).
Causal Bayesian Network. A causal Bayesian network (CBN) is the pair (g, P ) such that for any W ⊂ V,
P (X V |do(X W = x W )) = i∈V\W P (X i | X pa g (i) )1(X W = x W )
where do(X W ) represents a hypothetical intervention on the variables X W , 1(·) is an indicator function and pa g (i) denotes parents of variable X i in DAG g. A perfect intervention on any variable X j completely removes all dependencies with its parents, i.e. P (X j | X pa g (j) ) = P (X j ) thereby resulting in a mutilated DAG g = (V, E(pa g (j), j)).</p>
<p>Structural Causal Model. From the data generative mechanism point of view, the DAG g on X V matches a set of structural equations:
X i := f i (X pa g (i) , i ) ∀i ∈ V(1)
where f i 's are (potentially nonlinear) causal mechanisms that remain invariant when intervening on any variable X j = X i . i 's are exogenous noise variables with arbitrary distribution that are mutually independent, i.e i ⊥ ⊥ j ∀i = j.</p>
<p>(1) represents the conditional distributions in a Causal Bayesian Network and can additionally reveal the effect of interventions if the mechanisms are known (Peters et al., 2017, Pearl, 2009). These equations together form the structural causal model (SCM), with an associated DAG g. Though the mechanisms f can be nonparametric in the general case, we assume that there exists a parametric approximation to these mechanisms with parameters γ ∈ Γ. In the case of linear SCMs, γ corresponds to the weights of the edges in E. In the nonlinear case, they could represent the parameters of a nonlinear function that parameterizes the mean of a Gaussian distribution.</p>
<p>A common form of (1) corresponds to Gaussian additive noise models (ANM) 2 :
X i := f i (X pa g (i) ; γ i ) + i , i ∼ N (0, σ 2 i )(2)
An ANM is fully specified by a a DAG g, mechanisms, f (·; γ) = [f 1 (·; γ 1 ), . . . , f d (·; γ d )], parameterized by γ = [γ 1 , . . . , γ d ], and variances, σ 2 = σ 2 1 , . . . , σ 2 d . For notational brevity, henceforth we denote θ = (γ, σ 2 ) and all the parameters of interest with φ = (g, θ).</p>
<p>Bayesian Causal Discovery. A common assumption in causal inference is that causal relations are known qualitatively and can be represented by a DAG. While this qualitative information can be obtained from domain knowledge in some scenarios, it's infeasible in most applications. The goal of causal discovery is to recover the SCM and the associated DAG, given a dataset D. In general, without further assumptions about the nature of mechanisms f (e.g., linear vs. nonlinear), the true SCM may not be identifiable (Peters et al., 2012) from observational data alone. This non-identifiability is because there could be multiple DAGs (and hence multiple factorizations of P (X V )) which explain the data equally well. Such DAGs are said to be Markov Equivalent. Interventions can improve identifiability. In addition to identifiability issues, estimating the functional relationships between nodes using finite data is another source of uncertainty. Bayesian parameter estimation over the unknown SCM provides a principled way to quantify these uncertainties and obtain a posterior distribution over the SCM given observational data. An experimenter can then use the knowledge encoded by the posterior to design informative experiments that efficiently acquire interventional data to resolve unknown edge orientations and functional uncertainty.</p>
<p>Bayesian Inference of SCMs and DAGs. The key challenge in performing Bayesian inference jointly over SCMs and DAGs is that the space of DAGs is discrete and superexponential in the number of variables (Peters et al., 2017). However, recent techniques based on variational inference (Annadani et al., 2021, Lorch et al., 2021, Cundy et al., 2021 provide a tractable and scalable way of performing posterior inference of these parameters. Given a tractable distribution q ψ (φ) which approximates the posterior p(φ | D), variational inference maximizes a lower bound on the (log-) evidence:
log p(D) ≥ L(ψ ∈ Ψ) = E q ψ (φ) [log p(D | φ)] − D KL (q ψ (φ)||p(φ))
The key idea in these techniques is the way the variational family Ψ for DAGs is parameterized.  ley, 1956, Chaloner andVerdinelli, 1995) is an information theoretic approach to the problem of selecting the optimal experiment to estimate any parameter θ. For BOED, the utility of the experiment ξ is the mutual information (MI) between the observation y and θ:
U BOED (ξ) I(Y; θ | ξ, D) = E p(y|θ,ξ)p(θ|D) [log p(y | ξ, D) − log p(y | θ, ξ, D)]
The goal of BOED is to select the experiment that maximizes this objective ξ * = arg max ξ U BOED (ξ). A common setting, called static, fixed or batch design, is to optimize B designs {ξ 1 , . . . , ξ B } at the same time. The designs are then executed and the experimental outcomes are collected to update the model parameters in a Bayesian fashion.</p>
<p>Method</p>
<p>The true SCM and the associated DAGφ = (g,θ) over random variables X V is a matter of fact, but our belief inφ is uncertain for many reasons. Primarily, it is only possible to learn the DAG g up to a Markov equivalence class (MEC) from observational data D. Uncertainty also arises from D from being a finite sample, which we model by introducing the the random variable Φ, of which φ is an outcome. Let φ ∼ p(φ|D) ∝ p(D | φ)p(φ) be an instance of the random variable Φ that is sampled from our posterior over SCMs after observing the dataset D.</p>
<p>We would like to design an experiment to identify an intervention ξ := {(j, v)} := do(X j = v) that maximizes the information gain about Φ after observing the outcome of the intervention y ∼ P (
X 1 = x 1 , . . . , X d = x d | do(X j = v) = p(y | ξ).
Here, y is an instance of the random variable Y ⊆ X distributed according to the distribution specified by the mutilated true graphg under intervention. Looking at one intervention at a time, one can formalize BOECD as gain in information about Φ after observing the outcome of an experiment y. The experiment ξ := {(j, v)} that maximizes the information gain is the experiment that maximizes the mutual information between Φ and Y:
{(j * , v * )} = arg max j,v {I(Y; Φ | {(j, v)}, D)}(3)
The above objective considers taking arg max over not just the discrete set of intervention targets j ∈ V, but also over the uncountable set of intervention values v ⊂ X j . While the existing works in BOECD consider only the design of intervention targets to limit the complexity (Tong and Koller, 2001, Murphy, 2001, Agrawal et al., 2019, our approach tackles both the problems. We first outline the methodology for a single design and in Section 3.2 demonstrate how to extend this single design to a batch setting. </p>
<p>(c)</p>
<p>Sampled nodes from softmax scores without replacement. Figure 2: (a) Each graph shows how the mutual information (MI) (y-axis) changes for intervening on that node (plot color matching the node color) with different values (x-axis). The SCM in this example is a nonlinear SCM with Additive Gaussian noise. We can see that by intervening on node X2 with the value v * , the mutual information gets maximized. (b) The posterior distribution of a GP on the Mutual Information function as a response to different intervention values after four Bayesian Optimization (BO) steps. (c) For each t iteration of the BO algorithm and each node j, we get a utility function evaluationÛ t j (the utility being the MI in our case). Then we sample without replacement proportionally to the scores to prepare a batch (3.2).</p>
<p>Single Design</p>
<p>To maximize the objective in Equation 3, we need to (1) estimate MI for candidate interventions and (2) maximize the estimated MI by optimizing over the domain of intervention value for every candidate interventional target.</p>
<p>Estimating the MI. As mutual information is intractable, there are various ways to estimate it depending on whether we can sample from the posterior and whether the likelihood can be evaluated (Foster et al., 2020, Poole et al., 2019, Houlsby et al., 2011. Since the models we consider allow both posterior sampling and likelihood evaluation, it suffices to obtain an estimator which requires only likelihood evaluation and Monte Carlo approximations of the expectations. To do so, we derive an estimator similar to Bayesian Active Learning by Disagreement (BALD) (Houlsby et al., 2011), which considers MI as a difference of conditional entropies over the outcomes Y:
I(Y; Φ | {(j, v)}, D) = H(Y | {(j, v)}, D) − H(Y | Φ, {(j, v)}, D) = − E p(y|{(j,v)},D) log E p(φ|D) [p(y | φ, {(j, v)})] + E p(φ|D) E p(y|φ,{(j,v)}) [log (p(y | φ, {(j, v)}))] (4)
where H(·) is the entropy. See Appendix B.1 for the derivation. A Monte Carlo estimator of the above equation can be used as an approximation (Appendix B.2). Equation (4) has an intuitive interpretation. It assigns high mutual information to interventions that the model disagrees the most regarding the outcome. We denote the MI for a single design as
I({(j, v)}) := I(Y; Φ | {(j, v)}, D).
Selecting the Intervention Value. As shown in (3), maximizing the objective is achieved not only by selecting the intervention target but also by setting the appropriate intervention value. Although optimizing the intervention target is tractable (discrete and finite number of nodes to select from), selecting the value to intervene is usually intractable since they are continuous. For any given target node j, MI is a nonlinear function over v ∈ X j (See Fig 2) and hence solving with gradient ascent techniques only yields a local maximum. Given that MI is expensive to evaluate, we treat MI for a given target node j as a black-box function and obtain its maximum using Bayesian Optimization (BO) (Kushner, 1964, Zhilinskas, 1975, Močkus, 1975. BO seeks to find the maximum of this function max v∈Xj I({(j, v)}) over the entire set X j with as few evaluations as possible. See appendix E for details. (v). Subscript j signifies the fact that we maintain different µ and σ per intervention target and superscript t represents the BO step. Querying proceeds by having an acquisition function defined on this posterior, which suggests the next point to query. For BO, we use an acquisition function called as the Upper Confidence Bound (UCB) (Srinivas et al., 2010) which suggests the next point to query by trading-off exploration and exploitation with a hyperparameter
β t j : v (t+1) * j = arg max v µ t j (v) + β t+1 j σ t j (v).
We run GP-UCB independently on every candidate intervention target j = {1, . . . , d} by querying points within a fixed domain [−k, k] ⊂ R. Note that the domain can be chosen based on the application, for example, if we must constrain dosage levels within a fixed range. Each GP is one-dimensional in our setup; hence a few evaluations of UCB are sufficient to get a good value maxima candidate. Further, GP-UCB for each candidate target is parallelizable, making it efficient. We finally select the design with the highest MI across the candidate intervention targets.</p>
<p>Batch Design</p>
<p>In many applications, it is desirable to select the most informative set of interventions instead of a single intervention at a time. Take, for example, a biologist entering a wet lab with a script of experiments to execute. Batching experiments removes the bottleneck of waiting for an experiment to finish and get analyzed until executing the next one. Given a budget per batch B which denotes the number of experiments in a batch, the problem of selecting the batch then becomes
arg max Ξ I(Y; Φ | Ξ, D), such that cardinality(Ξ) = B, where Ξ is a set of interventions B i=1 (j i , v i )
and Y denotes the random variable for the outcomes of the interventions of the batch. We denote the MI for a batch design as I(Ξ) := I(Y; Φ | Ξ, D).</p>
<p>Greedy Algorithm. Computing the optimal solution I(Ξ * ) is computationally infeasible. However, as the conditional mutual information is submodular and non-decreasing (see Appendix B.4 for proof), we can derive a simple greedy algorithm (Algorithm 1) that can achieve at least a (1 − 1/e) ≈ 0.64 approximation of the optimal solution (Krause and Guestrin, 2012, Nemhauser et al., 1978. We denote this strategy as Greedy-CBED.</p>
<p>Soft Top-K. Although the greedy algorithm is tractable, it requires O(Bd) instances of GP-UCB. Kirsch et al. (2021) show that a soft top-k selection strategy performs similarly to the greedy algorithm, reducing the computation requirements to O(d) runs of GP-UCB. To achieve this, we construct a finite set of candidate intervention target-value pairs by keeping all the T evaluations of GP-UCB for each node j = {1, . . . , d}. Therefore, for d nodes, our candidate set is comprised of d × T experiments. We score each experiment in this candidate set using the MI estimate. We then sample without replacement B times proportionally to the softmax of the MI scores (Algorithm 2). We denote this strategy as Soft-CBED.</p>
<p>Comparison with existing active causal discovery methods</p>
<p>We outline how our approach compares with two main existing active causal discovery methods.</p>
<p>ABCD (Agrawal et al., 2019). The estimator of MI used in ABCD is based on weighted importance sampling. However, for the specific choice of the importance sampling weights used in ABCD, their MI estimator ends up with the same approximation as in our method (see Appendix B.5). Nevertheless, ABCD does not select intervention values but suboptimally sets them to a fixed value. In addition, our proposed Soft-CBED is a faster and more efficient batch strategy, especially when values also have to be acquired. From this perspective, our approach is an extension of ABCD with nonlinear assumptions, value acquisition, and a soft top-k batching strategy.</p>
<p>AIT (Scherrer et al., 2021). AIT is an F-score-based intervention target acquisition strategy. Although it is not a BOECD method, we prove here that it can be viewed as a Monte Carlo estimate of the approximation to MI when the outcomes Y are Gaussian. Nevertheless, AIT does not select intervention values like ABCD and does not have a batch strategy.<br />
Ξ ← ∅ 2 for n = 1 . . . B do 3 for j = 1 . . . d do
Select optimal intervention value per node j using GP-UCB Initialize µ 0 j and σ 0
4 V j ← arg max v I(Ξ ∪ {(j, v)}) 5 U j ← I(Ξ ∪ {(j, V j )}) 6 j * ← arg max j U j 7 v * ← V j * 8 Ξ ← Ξ ∪ {(j * , v * )} 9 return Ξj 3 for t = 1 . . . T do 4 V t j ← arg max v µ t−1 j (v) + β t σ t−1 j (v) 5Û t j ← I({(j, V t j )}) 6
Update the GP to obtain µ t j and σ t j
7 {(t i , j i )} i∈{1,...,B} ← B samples without replacement ∝ exp (Û t j /ζ) 8 Ξ ← {(j i , V ti ji )} i∈{1,...,B} 9 return Ξ</p>
<p>Related Work</p>
<p>Early efforts of using Bayesian Optimal Experimental Design for Causal Discovery (BOECD) can be found in the works of Murphy (2001) and Tong and Koller (2001). However, these approaches deal with simple settings like limiting the graphs to topologically ordered structures, intervening sequentially, linear models, and discrete variables.  (2001) and Tong and Koller (2001) in the setting where interventions can be applied in batches with continuous variables. To achieve this, they (approximately) solve the submodular problem of maximizing the batched mutual information between interventions (experiments), outcomes, and observational data, given a DAG. DAG hypotheses are sampled using DAG-bootstrap (Friedman et al., 2013). Our work differs from ABCD in a few ways: we work with both linear and nonlinear SCMs by using state-of-the-art posterior models over DAGs (Lorch et al., 2021), we apply BO to select the value to intervene with, but we also prepare the batch using softBALD (Kirsch et al., 2021) which is significantly faster than the greedy approximation of ABCD method.</p>
<p>In von Kügelgen et al. (2019) the authors proposed the use of Gaussian Processes to model the posterior over DAGs and then use BO to identify the value to intervene with, however, this method was not shown to be scalable for larger than bivariate graphs since they rely on multi-dimensional Gaussian Processes for modeling the conditional distributions.</p>
<p>A new body of work has emerged in the field of differentiable causal discovery, where the problem of finding the structure, usually from observational data, is solved with gradient ascent and functional approximators, like neural networks (Zheng et al., 2018, Ke et al., 2019, Brouillard et al., 2020, Bengio et al., 2019. In recent works (Cundy et al., 2021, Lorch et al., 2021, Annadani et al., 2021, the authors proposed a variational approximation of the posterior over the DAGs which allowed for modeling a distribution rather than a point estimate of the DAG that best explains the observational data D. Such work can be used to replace DAG-bootstrap (Friedman et al., 2013), allowing for the modeling of posterior distributions with greater support.</p>
<p>Besides the BOECD-based approaches, a few active causal learning works have been proposed (He and Geng, 2008, Gamella and Heinze-Deml, 2020, Scherrer et al., 2021, Shanmugam et al., 2015, Squires et al., 2020, Kocaoglu et al., 2017. Active ICP (Gamella and Heinze-Deml, 2020) uses ICP (Peters et al., 2016) for causal learning while using an active policy to select the target, however, 
Erdos Renyi D=50 ESID Scale Free D=50 Interventional samples (a) (b) ESID Random Fixed SoftAIT GP-UCB AIT GP-UCB CBED GP-UCB SoftCBED GP-UCB SoftCBED Fixed SoftCBED GP-UCB SoftCBED Sample Dist</p>
<p>Experiments</p>
<p>We evaluate the performance of our method on synthetic and real-world causal experimental design problems and a range of baselines. We aim to investigate the following aspects empirically: (1) competitiveness of the overall proposed strategies of Greedy-CBED and Soft-CBED at scale (50 nodes) on synthetic datasets; (2) performance of the value acquisition strategy based on GP-UCB; and (3) performance of the proposed approach on a real-world inspired dataset.</p>
<p>Acquisition Functions</p>
<p>Random. Random baseline acquires interventional targets at random.</p>
<p>AIT / softAIT. active intervention targeting (AIT) (Scherrer et al., 2021) uses an f-score based acquisition strategy to select the intervention targets. See appendix B.6 for more details. Since the original proposed approach does not consider a batch setting, we introduce a variant that augments AIT with the proposed soft batching, as described in section 3.2.</p>
<p>CBED / GreedyCBED / SoftCBED . These are the Monte Carlo estimates of MI, as described in section 3. CBED selects a single intervention (target and value) that maximizes the MI and this intervention is applied for the whole batch. In Greedy-CBED , the batch is built up in a greedy fashion selecting the target, value pairs one at a time (Algorithm 1). Soft-CBED is sampling (target, value) pairs proportionally to the MI scores to select a batch, as described in section 3.2 and Algorithm 2.</p>
<p>Value Selection Strategies</p>
<p>Fixed: This value selection strategy assumes setting the value of the intervention to a fixed value. In the experiments, we fixed the value to 0. Sample-Dist: This value selection strategy samples from the support of the observational data. GP-UCB: This strategy uses the proposed GP-UCB Bayesian optimization strategy to select the value that maximizes MI.</p>
<p>Tasks</p>
<p>Synthetic Graphs. In this setting, we generate Erdős-Rényi (Erdős and Rényi, 1959) (ER) and Scale-Free (SF) graphs (Barabási and Albert, 1999) of size 20 and 50. For linear SCMs, we sample the edge weights γ uniformly at random. For the nonlinear SCM, we parameterize each variable to be a Gaussian whose mean is a nonlinear function of its parents. We model the nonlinear function with a neural network. In all settings, we set noise variance σ 2 = 0.1. For both types of graphs, we set the expected number of edges per vertex to 1. We provide more details about the experiments in appendix D.1. </p>
<p>Single-Cell</p>
<p>Metrics</p>
<p>E-SHD: Defined as the expected structural hamming distance between samples from the posterior model over graphs and the true graph E-SHD := E g∼p(G|D) SHD(g,g)</p>
<p>E-SID: As the SHD is agnostic to the notion of intervention, (Peters and Bühlmann, 2015) proposed the expected structural interventional distance (E-SID) which quantifies the differences between graphs with respect to the causal inference statements and interventional distributions.</p>
<p>AUROC:</p>
<p>The area under the receiver operating characteristic curve of the binary classification task of predicting the presence/ absence of all edges.</p>
<p>AUPRC:</p>
<p>The area under the precision-recall curve of the binary classification task of predicting the presence/ absence of all edges. For each of the acquisition objectives and datasets, we present the mean and standard error of the expected structural hamming distance E-SHD, expected structural interventional distance E-SID (Peters and Bühlmann, 2015), area under the receiver operating characteristic curve AUROC and area under the precision-recall curve AUPRC. We evaluate these metrics as a function of the number of acquired interventional samples (or experiments), which helps quantitatively compare different acquisition strategies. Apart from E-SID, we relegate results with other metrics to the appendix I.</p>
<p>Results</p>
<p>On the synthetic graphs (Figure 3(a)), we can see that for ER and SF graphs with 50D variables and nonlinear functional relationships, the proposed approach based on soft top-k to select a batch with GP-UCB outperforms all the baselines in terms of the E-SID metric. On the other hand, AIT alone does not converge to the ground truth graph fast even after combining with the proposed value acquisition, but when further augmented with the proposed soft strategy, the softAIT recovers the ground truth causal graph upto 4 times faster and performs competitively to Soft-CBED.</p>
<p>We observe similar performance across other metrics as well. In addition, we found this trend to hold for 20D variables and linear models. Full results are presented in the appendix I.</p>
<p>Ecoli1</p>
<p>Ecoli2</p>
<p>Yeast1 Yeast2</p>
<p>Interventional samples SoftCBED (ours) Random softAIT ESID Figure 4: Comparison of acquisition functions on DREAM dataset, for 50 dimensions and batch size 10 on E-SID↓ metric (6 seeds, with standard error of the mean).</p>
<p>Next, we examine the importance of having a value selection strategy for active causal discovery. We use the MI estimator in Equation 4; moreover, we test the proposed GP-UCB with two heuristics -the fixed value strategy and sampling values from the support. As we can see in Figure 3(b), selecting the value using GP-UCB clearly benefits the causal discovery process. We expect this finding as the mutual information is not constant with respect to the intervened value. To make this point clear, we demonstrate in the appendix G the influence of the value in a simple two variables graph. In addition, we note that naively sampling from the support of the observed dataset performs worse than fixing the value to 0. We hypothesize that this is due to lower epistemic uncertainty in the high density regions of the support, hinting that these regions might be less informative.</p>
<p>In order to further understand how the soft batch strategy compares with other batch selection strategies, we compare the results of Soft-CBED with Greedy-CBED and CBED. We observe (Figure 3(c)) that Greedy-CBED and Soft-CBED give very similar results overall. While Greedy-CBED is optimal under certain conditions (Kirsch et al., 2019), Soft-CBED remains competitive and has the advantage that the batch can be selected in a one-shot manner. This is also evident from the runtime performance of both these batching strategies in Table 1. Both these batch selection strategies perform significantly better than selecting one intervention target/value pair, and executing them B times (CBED).</p>
<p>Finally, on the DREAM task, we see that our method outperforms softAIT and random baselines on the E-SID metric (see Figure 4). In these experiments, since the intervention is emulating the gene knockout setting, we only use the fixed value strategy, with a value of 0.0. Although random baseline still remains a competitive choice, in certain settings, Soft-CBED objective is significantly better (Ecoli1, Ecoli2 datasets).</p>
<p>Summary and Conclusions</p>
<p>This paper studies the problem of efficiently selecting the Bayes optimal experiments to discover causal models. Our proposed framework simultaneously answers the questions of where and how to intervene in a batched setting. We present a Bayesian optimization strategy to acquire interventional targets and values. Further, we propose two different batching strategies: one based on greedy selection and the other based on soft top-k selection. The proposed methodology for selecting intervention target-value pairs in a batched setting provides superior performance over the state-ofthe-art for causal models up to 50D variables. We validate this using synthetic datasets and using real-world inspired datasets of single-cell regulatory networks, showing the potential impact on areas like biology and other experimental sciences.  </p>
<p>References</p>
<p>A Potential negative societal impacts</p>
<p>Causal Experimental Design has the potential to impact several sectors; healthcare, biology, mechanical and material engineering, computational advertisement, to name a few. As any AI powered field, it can have negative societal impact when being used by malicious actors.</p>
<p>B Theoretical Results</p>
<p>B.1 Deriving the Mutual Information over Outcomes</p>
<p>In the following lemma, we derive the mutual information over outcomes given in (4).</p>
<p>Lemma B.1.
I(Y; Φ | {(j, v)}, D) = − E p(y|{(j,v)},D) log E p(φ|{(j,v)},D) [p(y | φ, {(j, v)})] + E p(φ|D) E p(y|φ,{(j,v)}) <a href="5">log (p(y | φ, {(j, v)}))</a>
Proof.
I(Y;Φ | {(j, v)}, D) = H(Y | {(j, v)}, D) − H(Y | Φ, {(j, v)}, D) (6a) = H(Y | {(j, v)}, D) − E p(φ|D) [H(Y | φ, {(j, v)})] (6b) = − E p(y|{(j,v)},D) [log (p(y | {(j, v)}, D))] + E p(φ|D) E p(y|φ,{(j,v)}) [log (p(y | φ, {(j, v)}))] (6c) = − E p(y|{(j,v)},D) log φ p(y, φ | {(j, v)}, D)dφ + E p(φ|D) E p(y|φ,{(j,v)}) [log (p(y | φ, {(j, v)}))] (6d) = − E p(y|{(j,v)},D) log φ p(φ | {(j, v)}, D)p(y | φ, {(j, v)}, D)dφ + E p(φ,|D) E p(y|φ,{(j,v)}) [log (p(y | φ, {(j, v)}))] (6e) = − E p(y|{(j,v)},D) log E p(φ|{(j,v)},D) [p(y | φ, {(j, v)})] + E p(φ|D) E p(y|φ,{(j,v)}) [log (p(y | φ, {(j, v)}))] .
(6f)</p>
<p>B.2 Estimating the Mutual Information over Outcomes</p>
<p>For models that allow for evaluation of the experimental outcome density (likelihood), p(y | φ, {(j, v)}), we can use the following estimator for I(Y; Φ | {(j, v)}, D):
I(Y; Φ | {(j, v)}, D) = H(Y | {(j, v)}, D) − H(Y | Φ, {(j, v)}, D)(7)
Algorithm 3: Mutual Information Computation Input : Posterior q(φ | D ∪ D ), Number of posterior samples c, Number of interventional samples m, Intervention {(j, v)}. We notate as the interventional and Y the observational data. Sample from the posterior
1 { φ i ∼ q(φ | D ∪ D )} c i=1
Sample from mutilated SCMs  <br />
2 { y i,j,k ∼ p(y | φ i , {(j, v)})} m k=1 3 return − 1 c×m c i=1 m k=1 log 1 c c l=1 p( y i,k | φ l , {(j, v)}) + 1 c×m c i=1 m k=1 log p( y i,k | φ i , {(j, v)})− 1 c o × m co i=1 m k=1 log 1 c in cin l=1 p( y i,k | φ l , {(j, v)}) ,(8)where y i,k ∼ p(y | φ l , {(j, v)}) isH(Y | {(j, v)}, D) = − E p(y|{(j,v)},D) log E p(φ|D) [p(y | φ, {(j, v)})] , ≤ − E p(y|{(j,v)},D) E p(φ|D) [log (p(y | φ, {(j, v)}))] ,− 1 c o × c in × m co i=1 m k=1 cin l=1 log p( y i,k | φ l , {(j, v)}) ,(9)− 1 c o × m co i=1 m k=1 log p( y i,k | φ i , {(j, v)}) ,(10)
where
y i,k ∼ p(y | φ i , {(j, v)}) is one of m samples from the density parameterised by the ith of c o graphs φ i ∼ p(φ | D) augmented by intervention {(j, v)}.</p>
<p>B.3 Monte Carlo Estimator of the Batch Mutual Information</p>
<p>While Equation 4 pertains to MI for a single design, we present here the MI estimator for the batch design.
I(Y; Φ | Ξ, D) = {(j,v)}∈Ξ I(Y; Φ | {(j, v)}, D) (11) = {(j,v)}∈Ξ H(Y | {(j, v)}, D) − H(Y | Φ, {(j, v)}, D) = − {(j,v)}∈Ξ E p(y|{(j,v)},D) log E p(φ|D) [p(y | φ, {(j, v)})] + E p(φ|D) E p(y|φ,{(j,v)}) [log (p(y | φ, {(j, v)}))]</p>
<p>B.4 Mutual Information Submodularity and Monotonicity Proofs</p>
<p>Theorem B.2. I(Y ; ω | X) is submodular.</p>
<p>Proof. The proof follows the structure of (Kirsch et al., 2019, Appendix A).
I(Y ∪ {y 1 }; ω | X ∪ {x 1 }) + I(Y ∪ {y 2 }; ω | X ∪ {x 2 }) ≥ I(Y ∪ {y 1 , y 2 }; ω | X ∪ {x 1 , x 2 }) + I(Y ; ω | X) (conditioning on RVs that are independent of the non-conditioning RVs) ⇔ I(Y ∪ {y 1 }; ω | X ∪ {x 1 , x 2 }) + I(Y ∪ {y 2 }; ω | X ∪ {x 1 , x 2 }) ≥ I(Y ∪ {y 1 , y 2 }; ω | X ∪ {x 1 , x 2 }) + I(Y ; ω | X ∪ {x 1 , x 2 }) (substituting X ∪ {x 1 , x 2 } with X + ) ⇔ I(Y ∪ {y 1 }; ω | X + ) + I(Y ∪ {y 2 }; ω | X + ) ≥ I(Y ∪ {y 1 , y 2 }; ω | X + ) + I(Y ; ω | X + )
(subtract 2 * I(Y ; ω | X + ) from both sides and use the identity I(A, B; C) − I(B; C) = I(A; C | B) ) ⇔ I(y 1 ; ω | Y, X + ) + I(y 2 ; ω | Y, X + ) ≥ I(y 1 , y 2 ; ω | Y, X + ) ⇔ I(y 1 ; ω | Y, X + ) + I(y 2 ; ω | Y, X + ) = (h(y 1 | Y, X + ) + h(y 2 | Y, X + )) ≥h(y1,y2|Y,X + ) (Thomas and Joy, 2006, p.253)
− (h(y 1 | Y, X + , ω) + h(y 2 | Y, X + , ω)) =h(y1,y2|ω,Y,X + ) (because y1⊥ ⊥y2|ω) ≥ h(y 1 , y 2 | Y, X + ) − h(y 1 , y 2 | ω, Y, X + ) = I(y 1 , y 2 ; ω | Y, X + ) Theorem B.3. I(Y ; ω | X) is non-decreasing.
Proof. 
I(Y ∪ {y}; ω | X ∪ {x}) − I(Y ; ω | X) = (</p>
<p>B.5 Relation to MI Approximation in ABCD</p>
<p>Here we demonstrate that though ABCD (Agrawal et al., 2019) uses an importance weighted estimate of mutual information, for the specific choice of importance weights used in ABCD, the MI estimate turns out to be the same as the one used in this work.</p>
<p>We note that ABCD decomposes the MI as entropy over the SCM as opposed to the entropy over outcomes used in this work.</p>
<p>B.5.1 Entropy Over SCM</p>
<p>The mutual information in (3) can be written as:
I(Y; Φ | {(j, v)}, D) = H(Φ | {(j, v)}, D) − H(Φ | Y, {(j, v)}, D)(12)
where H(·) is the expected entropy. As the posterior p(g, θ|D) does not change as a result of conditioning on the design choice {(j, v)}, the first entropy term is constant wrt {(j, v)}. Hence, selecting the most informative target corresponds to minimising the conditional entropy of the parameters Φ.
H(Φ | Y, {(j, v)}, D) = − E p(y|{(j,v)},D) E p(φ|y,{(j,v)},D) <a href="13">log p(φ | y, {(j, v)}, D)</a>
The above equation cannot be estimated from samples of q(φ | D) ≈ p(φ | D) since the posterior of the SCM would change when the interventional outcome y is conditioned on. To address this problem, ABCD (Agrawal et al., 2019) proposes to use weighted importance sampling with weights w = p(y | φ, {(j, v)}, D) and use samples from q(φ | D).</p>
<p>Definition B.4. The weighted importance sampling estimate of entropy over SCM (12) with weights w(φ) is given by
I WIS = 1 c o co i=1 E p(y|{(j,v)},D) log w( φ i ) − E p(y|{(j,v)},D) log E p(φ|D,{(j,v)}) w(φ)(14)
B.5.2 Entropy Over Outcomes.</p>
<p>We can instead consider an alternative factorisation of (3) which would not require importance sampling and also compute entropies in the lower dimensional space of experimental outcomes, as given in Equation 4. Definition B.5. The Monte Carlo estimate of entropy over outcomes (4) is given by
I MC = 1 c o × m co i=1 m k=1 log p( y i,k | φ i , {(j, v)}) − 1 c o × m co i=1 m k=1 log 1 c in cin l=1 p( y i,k | φ l , {(j, v)})(15)</p>
<p>B.5.3 Relation between Approximations with Entropy over SCM and Entropy over Outcomes</p>
<p>We prove below that for specific choice of importance weights w(φ) :== p(y | φ, {(j, v)}, D) used in ABCD, the MI approximations due to the above two factorizations are the same.</p>
<p>Theorem B.1. Let I WIS (14) be the weighted importance sampling estimate of entropy over SCM (12) with weights w(φ) and I MC (15) be the Monte Carlo estimate of entropy over outcomes (4). Then,
I WIS = I MC if w(φ) = p(y | φ, {(j, v)}, D).
Proof. Consider the entropy over SCM:
I(Y; Φ | {(j, v)}, D) = H(Φ | {(j, v)}, D) − H(Φ | Y, {(j, v)}, D) I(Y; Φ | {(j, v)}, D) = H(Φ | {(j, v)}, D) + E p(y|{(j,v)},D) E p(φ|y,{(j,v)},D) <a href="16">log p(φ | y, {(j, v)}, D)</a>
Consider the importance weighted estimate of the above equation with weights w(φ). We can rewrite p(φ | y, {(j, v)}, D) as: using (17) in (16),
p(φ | y, {(j, v)}, D) = w(φ)p(φ | D, {(j, v)}) Ep(φ|D,{(j,v)}) <a href="17">w(φ)</a>Let { φ i ∼ p(φ | D)} co i=1 ,I WIS (Y; Φ | {(j, v)}, D) = H(Φ | {(j, v)}, D) + 1 c o co i=1 E p(y|{(j,v)},D) log w( φ i )p( φ i | D, {(j, v)}) Ep(φ|D,{(j,v)}) <a href="18a">w(φ)</a>
Furthermore, using a Monte-Carlo estimate on first term with φ i , we get
I WIS (Y; Φ | {(j, v)}, D) = 1 c o co i=1 − log p( φ i | D, {(j, v)}) + E p(y|{(j,v)},D) log w( φ i )p( φ i | D, {(j, v)}) Ep(φ|D,{(j,v)}) <a href="18b">w(φ)</a>
Focusing on the second term,
E p(y|{(j,v)},D) log w( φ i )p( φ i | D, {(j, v)}) Ep(φ|D,{(j,v)}) [w(φ)] = E p(y|{(j,v)},D) log w( φ i ) + log p( φ i | D, {(j, v)}) − E p(y|{(j,v)},D) log E p(φ|D,{(j,v)}) w(φ)(19)
Plugging the above result back in (18b) and noticing that second term in the above equation cancels with first term in (18b), we get:
I WIS = 1 c o co i=1 E p(y|{(j,v)},D) log w( φ i ) − E p(y|{(j,v)},D) log E p(φ|D,{(j,v)}) w(φ)(20)
I MC is given by (8)+(10). We can notice that I WIS = I MC if w(φ) = p(y | φ, {(j, v)}, D) and approximating remaining expectations in the above equation with Monte Carlo samples.</p>
<p>B.6 Information Theoretic Interpretation of Neural Causal Models with Active Interventions</p>
<p>Here we provide the proof for Theorem 3.1. Definition B.6. The discrepancy score for a target j in AIT (Scherrer et al., 2021) is given by:
D j ≡ VBG j VWG j ≡ k µ j k − µ j 2 i k y i,j,k − µ j k 2(21)
where y i,j,k is the interventional sample from a hypothetical intervention on node j on a graph i sampled from the model. µ j i is the sample mean over samples k in y i,j,k and µ j is the mean over all graphs and samples.</p>
<p>We restate Theorem 3.1 for the sake of completeness. Theorem 3.1. Let Y be a Gaussian random variable. Then the discrepancy score of Scherrer et al. (2021) is a Monte Carlo estimate of an approximation to mutual information (Eq. (4)). See Appendix B.6 for proof.</p>
<p>Proof. Mutual Information over outcomes is given by
I(Y; Φ | {(j, v)}, D) = H(Y | {(j, v)}, D) − H(Y | Φ, {(j, v)}, D) (22a) = H(Y | {(j, v)}, D) − E p(φ|D) <a href="22b">H(Y | φ, {(j, v)})</a>
Making use of the fact that Y is Gaussian, its entropies are given by:
(22c) = 1 2 log (2πVar(Y | {(j, v)}, D)) − 1 2 E p(φ|D) [log (2πVar(Y | φ, {(j, v)}))] (22d) ≥ 1 2 log (2πVar(Y | {(j, v)}, D)) − 1 2 log E p(φ|D) [2πVar(Y | φ, {(j, v)})] (22e) = 1 2 log Var(Y | {(j, v)}, D)) Ep(φ|D) [Var(Y | φ, {(j, v)})] (22f) = 1 2 log   Ep(y|{(j,v)},D) y − Ep(y|{(j,v)},D) [y] 2 Ep(φ|D) Ep(y|φ,{(j,v)}) y − Ep(y|φ,{(j,v)}) [y] 2   (22g) = 1 2 log     Ep(φ|D) Ep(y|φ,{(j,v)}) y − µ k 2 Ep(φ|D) Ep(y|φ,{(j,v)}) y − µ k φ 2     (22h) ≤ Ep(φ|D) Ep(y|φ,{(j,v)}) y − µ k 2 Ep(φ|D) Ep(y|φ,{(j,v)}) y − µ k φ 2 (22i)
Since the bounds are in the opposite direction in (22e) and (22i), we cannot obtain a single common bound to MI but instead only a rough approximation given by (22i). We can now define a Monte Carlo estimate of the above approximation:
I(Y; Φ | {(j, v)}, D) ≡ i k y i,j,k − µ j 2 i k y i,j,k − µ j k 2 (23a)
Consider an m-sample Monte Carlo estimate over samples y, i.e i = 1, . . . , m, then  Erdős-Rényi 10000 20 Frobenius Squared Exponential (hlatent = 5.0, htheta = 500) Yeast1
I(Y; Φ | {(j, v)}, D) = m 2 k i y i,j,k m − µ j m 2 i k y i,j,k − µ j k 2 (23b) = m 2 k µ j k − µ j m 2 i k y i,j,k − µ j k 2 (23c)
Erdős-Rényi 10000 20 Frobenius Squared Exponential (hlatent = 5.0, htheta = 500) Yeast2</p>
<p>Erdős-Rényi 10000 20 Frobenius Squared Exponential (hlatent = 5.0, htheta = 500)</p>
<p>Ecoli1</p>
<p>Erdős-Rényi 10000 20 Frobenius Squared Exponential (hlatent = 5.0, htheta = 500) Ecoli2</p>
<p>Erdős-Rényi 10000 20 Frobenius Squared Exponential (hlatent = 5.0, htheta = 500) Yeast1</p>
<p>Erdős-Rényi 10000 20 Frobenius Squared Exponential (hlatent = 5.0, htheta = 500) Yeast2</p>
<p>Erdős-Rényi 10000 20 Frobenius Squared Exponential (h latent = 5.0, h theta = 500) </p>
<p>C.2 DAG Bootstrap</p>
<p>The DAG bootstrap bootstraps observations and interventions to infer a different causal structure per bootstrap. We used GIES as the causal inference algorithm because of the adaptation of GES on interventional data as well. In our experiments, we used the pcalg R implementation https: //github.com/cran/pcalg/blob/master/R/gies.R to discover 100 graphs. Each graph can be seen as a posterior sample from p(G | D). For each of the sampled graphs G i we compute the appropriate θ MLE under linear Gaussian assumption for the conditional distributions.</p>
<p>D Datasets and Experiment details D.1 Synthetic Graphs Experiments</p>
<p>In the synthetic data experiments, we focus on two types of graphs. The Erdős-Rényi and Scale Free.</p>
<p>Erdos-Renyi model:</p>
<p>We used networkx 3 and method fast_gnp_random_graph (Batagelj and Brandes, 2005) to generate graphs based on the Erdős-Rényi model. We set expected number of edges per vertex to 1.</p>
<p>Scale Free (Barabasi-Albert) graphs:</p>
<p>We used igraph 4 package to generate the graphs. We set the expected number of edges per vertex to 1.</p>
<p>For all the synthetic graph experiments, we used batch size of 10 and number of iterations of 10.</p>
<p>D.2 DREAM Experiments</p>
<p>For the DREAM experiments, we used GeneNetWeaver (Schaffter et al., 2011), a simulator of gene regulatory networks, based on stochastic differential equations. This simulator was used to generate data for Dialogue for Reverse Engineering Assessments and Methods (DREAM) (Sachs et al., 2005) competition with three network inference challenges (DREAM3, DREAM4 and DREAM5). We used the GeneNetWeaver v3.1 5 .</p>
<p>Each experiment is parametrized as an xml file describing the network topology but also the crucial parameters of the stochastic differential equation that GeneNetWeaver simulates. In our experiments, we used Ecoli1, Ecoli2, Yeast1 and Yeast2 networks for 10 and 50 nodes.</p>
<p>Each experiment was initialized with 100 observational data. For the observational data, we used the steady state 6 of wild-type experiments. For the interventional data, we used the steady-state of knock-out experiments. Each observational or interventional sample was conducted by running the simulator with a different seed per draw. </p>
<p>E Bayesian Optimisation</p>
<p>Bayesian Optimisation (BO) (Kushner, 1964, Zhilinskas, 1975, Močkus, 1975) is a global optimisation technique for optimising black-box functions. More formally, for any function U defined on a set X which is expensive to evaluate, BO seeks to find the maximum of the function over the entire set X with as few evaluations as possible.
max x∈X U(x)
BO typically proceeds by placing a prior on the unknown function and obtaining the posterior over this function with the queried points x * = {x * 1 , . . . , x * t }. A common prior is a Gaussian Process (GP) (Rasmussen, 2003) with mean 0 and covariance function defined by a kernel k(x, x ). Let
U x * = [U(x * 1 ), . . . , U(x * t )]
denote the vector of function evaluations, K the kernel matrix with
K i,j = k(x * i , x * j ) and k t+1 = [k(x * 1 , x t+1 ), . . . , k(x * t , x t+1 )].
The posterior predictive of point x t+1 can be obtained in closed form:
p(U) ∼ GP(0, k) p(U | x * , U x * , x t+1 ) = N (µ(x t+1 ), σ 2 (x t+1 )) µ(x t+1 ) = k T t+1 (K + I) −1 U x * σ 2 (x t+1 ) = k(x t+1 , x t+1 ) − k T t+1 (K + I) −1 k t+1
For the Gaussian Process, we used the following hyperparameters. Matern kernel, length scale 1.0, length scale bounds (lower=1e − 5, upper=1e5), Nu (smoothness of learned function) 2.5. Also added 1e − 6 to the diagonal of the kernel matrix. H metrics E-SHD: Defined as the expected structural hamming distance between samples from the posterior model over graphs and the true graph E-SHD := E g∼p(G|D) SHD(g,g)</p>
<p>F Related Work</p>
<p>E-SID: As the SHD is agnostic to the notion of intervention, (Peters and Bühlmann, 2015) proposed the expected structural interventional distance (E-SID) which quantifies the differences between graphs with respect to the causal inference statements and interventional distributions.</p>
<p>AUROC:</p>
<p>The area under the receiver operating characteristic curve of the binary classification task of predicting the presence/ absence of all edges.</p>
<p>AUPRC:</p>
<p>The area under the precision-recall curve of the binary classification task of predicting the presence/ absence of all edges.</p>
<p>I Complete list of Synthetic task results</p>
<p>Unless stated otherwise, for all the synthetic experiments we run 100 seeds, with standard error of the mean shaded.  </p>
<p>J Code Dependencies</p>
<p>We are using the following dependencies.  </p>
<p>L License</p>
<p>We summarize the licenses on table 5.</p>
<p>t e r v e n e w it h Figure 1 :
h1Intervention-Inference-Design loop of Bayesian Optimal Experimental Design for Causal Discovery framework.</p>
<p>Unfortunately, evaluating and optimizing this objective is challenging because of the nested expectations (Rainforth et al., 2018) and several estimators have been introduced (Foster et al., 2019, Kleinegesse and Gutmann, 2019) that lower bound the BOED objective which then can be combined with various optimization methods to select the designs(Foster et al., 2020, Ivanova et al., 2021,  Foster et al., 2021, Lim et al., 2022.</p>
<p>BO typically proceeds by placing a Gaussian Process (GP) (Rasmussen, 2003) prior on the function I({j, ·}) and obtain the posterior of this function with the queried points v * = {v (1) * , . . . , v (T ) * }. Let the value of the mutual information queried at each optimization step t beÛ t j = I({(j, v (t) * }). The posterior predictive of a point v (t+1) can be obtained in closed form as a Gaussian with mean µ (t+1) j (v) and variance σ (t+1) j</p>
<p>Theorem 3 . 1 .
31Let Y be a Gaussian random variable. Then the discrepancy score of Scherrer et al.(2021)is a Monte Carlo estimate of an approximation to mutual information (Eq. (4)). See Appendix B.6 for proof.</p>
<p>In
Cho et al. (2016) and Ness et al. (2017), BOECD was applied for learning biological networks structure. BOECD was also explored in Greenewald et al. (2019) under the assumption that undirected edges of the graph always forms a tree. More recently, ABCD framework (Agrawal et al., 2019) extended the work of Murphy</p>
<p>Figure 3 :
3Results on the E-SID ↓ metric (100 seeds, with standard error of the mean shaded) for 50 variables involving nonlinear functional relationships and additive Gaussian noise. (a) We show that Soft-CBED with GP-UCB value selection strategy significantly outperforms the baselines. (b) We isolate the effect of the value selection strategy. We show that intervening with a fixed value and sampling from the support of data both perform worse than having an optimizer like GP-UCB. (c) we compare non-batch (CBED) vs batch-based acquisition functions (Greedy-CBED, Soft-CBED). As we can see, Soft-CBED performs as well as Greedy-CBED. For all experiments, we use the DiBS (Lorch et al., 2021) posterior model.this work is not applicable in the setting where the full graph needs to be recovered. InZhang et al.  (2021), the authors propose an active learning method to the problem of identifying the interventions that push a dynamical causal network towards a desired state. A few approaches tackle the problem of actively acquiring interventional data to orient edges of a skeletal graph(Shanmugam et al., 2015,  Squires et al., 2020, Kocaoglu et al., 2017. Closer to our proposal belongsAIT (Scherrer et al.,  2021), which uses a neural network-based posterior model over the graphs but evaluates the F-score to select the interventions.</p>
<p>Protein-Signalling Network. The DREAM family of benchmarks(Greenfield et al.,  2010)  are designed to evaluate causal discovery algorithms of the regulatory networks of a single cell. A set of ODEs and SDEs generates the dataset, simulating the reverse-engineered networks of single cells. We use GeneNetWeaver(Schaffter et al., 2011)  to simulate the steady-state wind-type expression and single-gene knockouts. Refer to appendix D.2 for the exact settings.</p>
<p>Checklist 1 .
1For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See end of Section 1. Our limitations arise from the fact that the proposed methodology and conclusion hold when the assumptions laid out are satisfied. (c) Did you discuss any potential negative societal impacts of your work? [Yes] Negative societal impact is discussed in appendix A and will be added to the extra page of the camera ready after the reviewing process. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] The assumptions are laid out in Introduction as well as in Theorem 3.1. (b) Did you include complete proofs of all theoretical results? [Yes] The proofs are in Appendix. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Appendix D.1, D.2 and H (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] See Figure 3 and 4. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Please check appendix K for details. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] Please check appendix L for details. (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] Data are simulated. (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No] Data are simulated. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</p>
<p>Definition B. 1 .
1The Monte Carlo estimator, H(Y | {(j, v)}, D), of the marginal entropy of the experimental outcomes, H(Y | {(j, v)}, D), is given by:</p>
<p>one of m samples from the density parameterised by the ith of c o SCMs φ i ∼ p(φ | D) augmented by intervention {(j, v)}. The likelihood of the sample y i,k is then evaluated under the parameterisation of the lth of c in additional SCMs φ l ∼ p(φ | D) augmented by intervention {(j, v)}. H(Y | {(j, v)}, D) is a consistent but biased estimator of H(Y | {(j, v)}, D) due to the expectation inside of the nonlinear log function. Alternatively, we can look at the following lower bound on H(Y | {(j, v)}, D):</p>
<p>by Jensen's inequality. We can then define an unbiased estimator of this lower bound. Definition B.2. The unbiased Monte Carlo estimator, H * (Y | {(j, v)}, D), of the lower bound on the marginal entropy of the experimental outcomes, − Ep(y|{(j,v)},D) Ep(φ,|D) [log (p(y | φ, {(j, v)}))] , is given by:</p>
<p>Finally
, we define our estimator for H(Y | Φ, {(j, v)}, D). Definition B.3. The Monte Carlo estimator, H(Y | Φ, {(j, v)}, D), of the entropy of the experimental outcomes conditioned on Φ, H(Y | Φ, {(j, v)}, D), is given by:</p>
<p>conditioning on RVs that are independent of the non-conditioning RVs) I(Y ∪ {y}; ω | X ∪ {x}) − I(Y ; ω | X ∪ {x}) = (use the identity I(A, B; C) − I(B; C) = I(A; C | B)) I({y}; ω | Y, X ∪ {x}) ≥ 0</p>
<p>For a single sample Monte-Carlo estimate m = 1, I(Y; Φ | {(j, v)}, D) = D j .</p>
<p>Figure 6 :Figure 7 :Figure 8 :Figure 9 :
6789Results of Erdős-Rényi (Erdős and Rényi, 1959) linear SCMs with 20 variables. Experiments were performed with DAG Bootstrap as the underlying posterior model. Results of scale-free linear SCMs with 20 variables. Experiments were performed with DAG Bootstrap as the underlying posterior model. Results of Erdős-Rényi (Erdős and Rényi, 1959) linear SCMs with 50 variables. Experiments were performed with DAG Bootstrap as the underlying posterior model. Results of scale-free linear SCMs with 50 variables. Experiments were performed with DAG Bootstrap as the underlying posterior model.</p>
<p>Figure 10 :Figure 11 :Figure 12 :Figure 13 :
10111213Results of Erdős-Rényi (Erdős and Rényi, 1959) nonlinear SCMs with 20 variables. Experiments were performed with DiBS as the underlying posterior model. Results of scale-free nonlinear SCMs with 20 variables. Experiments were performed with DiBS as the underlying posterior model. Results of Erdős-Rényi (Erdős and Rényi, 1959) nonlinear SCMs with 50 variables. Experiments were performed with DiBS as the underlying posterior model. Results of scale-free nonlinear SCMs with 50 variables. Experiments were performed with DiBS as the underlying posterior model.</p>
<p>Bayesian Optimal Experimental Design. Bayesian Optimal Experimental Design (BOED) (LindThe variational family for the Variational Causal Network (VCN) method (Annadani et al., 2021) 
is an autoregressive Bernoulli distribution over the adjacency matrix. They further enforce the 
acyclicity constraint (Zheng et al., 2018) through the prior. BCD-Nets (Cundy et al., 2021) consider 
a distribution over node orderings through a Boltzmann distribution and perform inference with 
Gumbel-Sinkhorn (Mena et al., 2018) operator. DiBS (Lorch et al., 2021) consider latent variables 
over entries of adjacency matrix and perform inference over these latent variables using SVGD (Liu 
and Wang, 2016). We demonstrate empirical results in BOECD using the DiBS model in this work 
because it is easily extendable to nonlinear SCMs. </p>
<p>Table 1 :
1Performancecomparison be-
tween different value selection and 
batch strategies for CBED. Experiments 
are performed using an AMD EPYC 
7662 64-Core CPU and Tesla V100 
GPU. </p>
<p>Strategy 
Value 
Batch 
Runtime(s) </p>
<p>Fixed 
Greedy 
32.56 
Soft 
6.42 </p>
<p>GP-UCB </p>
<p>Greedy 
284.98 
Soft 
24.17 </p>
<p>Raj Agrawal, Chandler Squires, Karren Yang, Karthikeyan Shanmugam, and Caroline Uhler. Abcdstrategy: Budgeted experimental design for targeted causal structure discovery. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3400-3409. PMLR, 2019. Hyunghoon Cho, Bonnie Berger, and Jian Peng. Reconstructing causal biological networks through active learning. PloS one, 11(3):e0150611, 2016. Chris Cundy, Aditya Grover, and Stefano Ermon. Bcd nets: Scalable variational approaches for bayesian causal discovery. Advances in Neural Information Processing Systems, 34, 2021. Adam Foster, Martin Jankowiak, Matthew O'Meara, Yee Whye Teh, and Tom Rainforth. A unified stochastic gradient approach to designing bayesian-optimal experiments. In International Conference on Artificial Intelligence and Statistics, pages 2959-2969. PMLR, 2020. Adam Foster, Desi R Ivanova, Ilyas Malik, and Tom Rainforth. Deep adaptive design: Amortizing sequential bayesian experimental design. arXiv preprint arXiv:2103.02438, 2021.Yashas Annadani, Jonas Rothfuss, Alexandre Lacoste, Nino Scherrer, Anirudh Goyal, Yoshua Bengio, 
and Stefan Bauer. Variational causal networks: Approximate bayesian inference over causal 
structures. arXiv preprint arXiv:2106.07635, 2021. </p>
<p>Albert-László Barabási and Réka Albert. Emergence of scaling in random networks. science, 286 
(5439):509-512, 1999. </p>
<p>Vladimir Batagelj and Ulrik Brandes. Efficient generation of large random networks. Physical 
Review E, 71(3):036113, 2005. </p>
<p>Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sébastien Lachapelle, Olexa Bilaniuk, 
Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle causal 
mechanisms. arXiv preprint arXiv:1901.10912, 2019. </p>
<p>Philippe Brouillard, Sébastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and Alexandre 
Drouin. Differentiable causal discovery from interventional data. arXiv preprint arXiv:2007.01754, 
2020. </p>
<p>Kathryn Chaloner and Isabella Verdinelli. Bayesian experimental design: A review. Statistical 
Science, pages 273-304, 1995. </p>
<p>P Erdős and A Rényi. On random graphs i. publicationes mathematicae (debrecen). 1959. </p>
<p>Adam Foster, Martin Jankowiak, Eli Bingham, Paul Horsfall, Yee Whye Teh, Tom Rainforth, 
and Noah Goodman. Variational bayesian optimal experimental design. arXiv preprint 
arXiv:1903.05480, 2019. </p>
<p>Nir Friedman, Moises Goldszmidt, and Abraham Wyner. Data analysis with bayesian networks: A 
bootstrap approach. arXiv preprint arXiv:1301.6695, 2013. </p>
<p>Juan L Gamella and Christina Heinze-Deml. Active invariant causal prediction: Experiment selection 
through stability. arXiv preprint arXiv:2006.05690, 2020. </p>
<p>Kristjan Greenewald, Dmitriy Katz, Karthikeyan Shanmugam, Sara Magliacane, Murat Kocaoglu, 
Enric Boix Adsera, and Guy Bresler. Sample efficient active learning of causal trees. Advances in 
Neural Information Processing Systems, 32, 2019. </p>
<p>Alex Greenfield, Aviv Madar, Harry Ostrer, and Richard Bonneau. Dream4: Combining genetic 
and dynamic information to identify biological networks and dynamical models. PloS one, 5(10): 
e13397, 2010. </p>
<p>Yang-Bo He and Zhi Geng. Active learning of causal networks with intervention experiments and 
optimal designs. Journal of Machine Learning Research, 9(Nov):2523-2547, 2008. </p>
<p>Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. Bayesian active learning for 
classification and preference learning. arXiv preprint arXiv:1112.5745, 2011. </p>
<p>Table 2 :
2Settings of DREAM experiments for nodes 10 and 50.</p>
<p>Settings of DREAM experiments for nodes 10 and 50.Dataset 
Model 
Starting Observational Samples Batch Size Number of Batches </p>
<p>10 nodes </p>
<p>Ecoli1 DiBS non linear 
100 
5 
20 
Ecoli2 DiBS non linear 
100 
5 
20 
Yeast1 DiBS non linear 
100 
5 
20 
Yeast2 DiBS non linear 
100 
5 
20 </p>
<p>50 nodes </p>
<p>Ecoli1 DiBS non linear 
100 
20 
20 
Ecoli2 DiBS non linear 
100 
20 
20 
Yeast1 DiBS non linear 
100 
20 
20 
Yeast2 DiBS non linear 
100 
20 
20 
Table 3: </p>
<p>Table 4 :
4Comparison of the proposed experimental design for causal discovery with existing experimental design for causal discovery techniques. Mutual Information per value for two Variables graphFigure 5: Estimation of the Mutual Information using two variables model G. In green we represent the interventional data. We train an ensemble of a linear (left plot) and a non-linear (right plot) function approximator (NN) parametrizing a Gaussian Distribution. We can see that in both cases, MI is influenced by the value of intervention do(X1 = x1). In this experiment we used the BALD estimator of the MI.Method 
Nonlinear BOED Scalable Continuous Finite Data Setting the value </p>
<p>Murphy (2001), Tong and Koller (2001) 
Agrawal et al. (2019) 
Scherrer et al. (2021) 
Gamella and Heinze-Deml (2020) 
von Kügelgen et al. (2019) 
Sussex et al. (2021) 
Ours </p>
<p>Table 5 :
5Set-up and dataset details for non-convex, non-linear regression prblem. jaxlib/jax https://jax.readthedocs.io/en/latest/ Apache causaldag https://github.com/FenTechSolutions/CausalDiscoveryToolbox MIT pytorch https://github.com/pytorch/pytorch BSD xarray https://github.com/pydata/xarray Apache cdt https://github.com/FenTechSolutions/CausalDiscoveryToolbox MIT bayesian-optimization https://github.com/fmfn/BayesianOptimization MIT pgmpy https://github.com/pgmpy/pgmpy MIT igraph https://github.com/igraph/igraph GPL-2.0 numpy https://github.com/numpy/numpy BSD SciPy https://github.com/scipy/scipy BSD scikit-learn https://github.com/scikit-learn/scikit-learn BSD networkx https://github.com/networkx/networkx BSDName 
URL </p>
<p>Table 6 :
6Total number of GPU hours (back-of-the-envelope estimation). Experiments are performed on an AMD EPYC 7662 64-core CPU and Tesla V100 GPU. per acq. Iterations Seeds Experiments total (hours)Runtime greedy ucb (CBED) 
284.98 
20 
100 
2 
316.64 
greedy fixed (CBED) 32.56 
20 
100 
2 
36.17 
D=50 
soft ucb (CBED) 
24.17 
20 
100 
2 
26.85 
soft fixed (CBED) 
6.42 
20 
100 
2 
7.13 
6.42 
20 
100 
2 
7.13 
greedy ucb (AIT) 
284.98 
20 
100 
2 
316.64 
soft ucb (AIT) 
24.17 
20 
100 
2 
26.85 </p>
<p>greedy ucb (CBED) 
113.992 
20 
100 
2 
126.65 
greedy fixed (CBED) 13.024 
20 
100 
2 
14.47 
D=20 
soft ucb (CBED) 
9.668 
20 
100 
2 
10.74 
soft fixed (CBED) 
2.568 
20 
100 
2 
2.85 
soft sampled (CBED) 2.568 
20 
100 
2 
2.85 
greedy ucb (AIT) 
113.992 
20 
100 
2 
126.65 
soft ucb (AIT) 
9.668 
20 
100 
2 
10.74 </p>
<p>DREAM soft fixed (CBED) 
6.42 
20 
6 
4 
0.856 
soft fixed (AIT) 
6.42 
20 
6 
4 
0.856 </p>
<p>sum 
889.31 </p>
<p>ANM's can have noise variables that are non-Gaussian as well, but we restrict our exposition to the Gaussian case.
https://networkx.org/documentation/networkx-1.10/reference/generated/networkx. generators.random_graphs.fast_gnp_random_graph.html 4 https://igraph.org/python/api/latest/igraph._igraph.GraphBase.html#Barabasi 5 https://github.com/tschaffter/genenetweaver 6 Steady state is considered the result of the simulation of the SDE for maximum 2000 steps.
Acknowledgments and Disclosure of FundingWe would like to thank Nino Scherrer, Tom Rainforth, Desi R. Ivanova and all anonymous reviewers for sharing their valuable feedback and insights. Panagiotis Tigas is supported by the UK EPSRC CDT in Autonomous Intelligent Machines and Systems (grant reference EP/L015897/1). We are grateful for compute from the Berzelius Cluster and the Swedish National Supercomputer Centre.
. Adam Desi R Ivanova, Steven Foster, Kleinegesse, Thomas Michael U Gutmann, Rainforth, Desi R Ivanova, Adam Foster, Steven Kleinegesse, Michael U Gutmann, and Thomas Rainforth.</p>            </div>
        </div>

    </div>
</body>
</html>
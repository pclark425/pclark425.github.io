<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1858 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1858</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1858</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-55dff8d4cdeab9c86dccb1c8b739ac0518cdbed0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/55dff8d4cdeab9c86dccb1c8b739ac0518cdbed0" target="_blank">Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models</a></p>
                <p><strong>Paper Venue:</strong> Robotics: Science and Systems</p>
                <p><strong>Paper TL;DR:</strong> Data-driven Instruction Augmentation for Language-conditioned control (DIAL) is introduced, utilizing semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration data and then train language- Conditioned policies on the augmented datasets, allowing for more efficient label coverage of large-scale datasets.</p>
                <p><strong>Paper Abstract:</strong> In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot-language data that was either collected with specific tasks in mind or expensively re-labelled by humans with rich language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration data and then train language-conditioned policies on the augmented datasets. This method enables cheaper acquisition of useful language descriptions compared to expensive human labels, allowing for more efficient label coverage of large-scale datasets. We apply DIAL to a challenging real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations do not contain crowd-sourced language annotations. DIAL enables imitation learning policies to acquire new capabilities and generalize to 60 novel instructions unseen in the original dataset.</p>
                <p><strong>Cost:</strong> 0.028</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1858.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1858.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FT-CLIP (DIAL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned CLIP used in DIAL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The OpenAI CLIP model (ViT-based) initialized from pretrained weights and fine-tuned on a small set of robot trajectories with crowd-sourced first/last-frame language annotations; used both to propose natural-language hindsight labels for a large offline robot dataset and as the task-language encoder for downstream behavior-cloning policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>FT-CLIP (fine-tuned CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Vision-language model: CLIP ViT image encoder + CLIP Transformer text encoder initialized from OpenAI ViT-B/32 checkpoint, with an MLP fusion network that concatenates image embeddings for initial and final frames to form an episode embedding. Fine-tuned contrastively on (initial-final image, crowd-sourced instruction) pairs using symmetric cross-entropy softmax loss with temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Image–text web pretraining (vision-language) followed by in-domain fine-tuning on paired robot images and language</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Initialized from OpenAI CLIP ViT-B/32 pretrained checkpoint (internet-scale image-text pretraining; exact dataset not specified in this paper). Fine-tuned on D_A: 5,600 episode–instruction pairs (2,800 episodes × 2 captions) collected from real robot teleoperation; fine-tuning used batch size 64, 100,000 iterations, and fine-tuned both image & text encoders plus a 200-dim MLP fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Real-world robotic manipulation in office kitchen (mobile manipulator)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Mobile manipulator with 7-DoF arm and parallel-jaw gripper operating on real kitchen countertops (varied scenes, drawers, receptacles). Tasks include picking, placing, moving objects relative to others; policies take RGB image history and language goal and output closed-loop control.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A for pretraining stage (CLIP pretraining uses image–text pairs; no action space), fine-tuning uses static episode descriptions (no actions in CLIP contrastive training).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Downstream policy outputs discrete action tokens mapped to continuous motor controls for a 7-DoF arm and gripper (closed-loop control from pixels via the RT-1 policy architecture).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Indirect learned mapping: FT-CLIP provides language and episode embeddings and is used to (1) retrieve candidate natural language labels for trajectories (cosine similarity softmax over candidate texts) and (2) provide the language embedding as input to a behavior-cloning policy (RT-1) that learns to map (image history, language embedding) → discrete action tokens → continuous low-level controls.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>First-person RGB (over-the-shoulder) images; DIAL fine-tuning uses initial and final frames for each episode; downstream policies use a short history of RGB frames.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>DIAL (using FT-CLIP) achieved large gains on novel evaluation instructions: overall success 60.0% on 60 held-out novel instructions (Table III). Best spatial-task performance reported 68.0% (Min-p p=0.2 / combined dataset variant, Table IV). Using FT-CLIP as the task encoder improved novel-instruction success relative to USE in their ablations (see Table V).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baselines without DIAL or without FT-CLIP (e.g., RT-1 or USE-task-encoder baselines) achieved lower novel-instruction success: overall 42.5% (no instruction augmentation baseline, Table III); spatial-task baselines ranged 30–46% depending on dataset combination (Table II/III).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Training datasets used with FT-CLIP in reported experiments included: D_A = 5,600 annotated episodes (fine-tune set); D_B = 80,000 teleoperated episodes (original demos); D_C (DIAL relabeled) examples varied by hyperparameter—e.g., Min-p p=0.2 produced D_C = 38,516 relabeled episodes. Common experimental trainings used combinations such as {D_A, D_B, D_C} totaling up to ~124k episodes in that setting.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Baseline RT-1 / no-augmentation trainings used D_B = 80,000 episodes and/or D_A = 5,600 episodes depending on the ablation. The paper does not report sample-to-threshold curves, so no samples-to-performance-threshold numbers are given.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not reported as samples-to-threshold. Empirically, adding DIAL relabeled data (e.g. +38.5k D_C) and using FT-CLIP yielded an absolute overall success increase of ~17.5 percentage points (42.5% → 60.0%), which the paper frames as a >41% relative improvement in novel-task success.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Key factors: (1) Internet-scale CLIP pretraining provides a broad semantic prior; (2) fine-tuning on a small set of in-domain episode–language pairs (D_A) aligns CLIP to embodied viewpoints and instruction phrasing; (3) CLIP’s multimodal embedding allows visually grounded retrieval of candidate captions (enabling many new semantic labels); (4) using the same FT-CLIP text encoder as the policy task representation yields consistent downstream conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limitations: (1) domain gap — frozen pretrained CLIP underperforms and fine-tuning on in-domain data is necessary (Appendix A2); (2) CLIP-based relabeling only observes first and final frames, so temporal/multi-step skills are not captured; (3) label noise from weakly-relabeled instructions can degrade performance if over-augmented (Top-k with large k reduced accuracy); (4) motor-control failures remain (some evaluation failures due to control rather than instruction understanding).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A vision–language model pretrained on image–language data (CLIP) and fine-tuned on a small set of in-domain episode-level language annotations can be used to automatically relabel large offline robot datasets (DIAL), producing relabeled datasets that substantially improve behavior-cloned policies' ability to follow novel, unseen natural-language instructions in real-world 3D manipulation; success depends strongly on fine-tuning, conservative candidate selection (Min-p), and the availability of sufficient relabeled examples (tens of thousands) while managing label noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1858.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1858.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Language–Image Pre-training (CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large vision–language model pretrained contrastively on image–text pairs from the internet to produce joint image and text embeddings; used in this paper as the initialization for the FT-CLIP model and as a baseline when frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>OpenAI CLIP (ViT-B/32 checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pretrained vision–language model consisting of a ViT-based image encoder and a Transformer-based text encoder trained contrastively on image-text pairs to produce aligned embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Internet-scale image–text pairs (vision-language supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Paper uses OpenAI CLIP ViT-B/32 checkpoint as initialization; the original CLIP was pretrained on large image–text datasets (details are provided in the CLIP paper, not repeated here).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used as initialization or frozen encoder for robotic manipulation tasks (this paper and prior works such as CLIPort).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Robotic manipulation in real/simulated environments: CLIP embeddings are used as visual and/or language representations for instruction-conditioned control or for relabeling datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A (pretraining is passive image–text pairs; no action semantics in pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Robotic continuous motor controls in downstream tasks (7-DoF arm, gripper in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>In this paper CLIP is fine-tuned and its embeddings are used for retrieval of candidate instructions (cosine similarity) and used as task embeddings input to a learned BC policy; in other works CLIP has been used as frozen encoders feeding downstream policy models (e.g., CLIPort).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images; CLIP operates on RGB image inputs for embedding computation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>When fine-tuned (FT-CLIP) and used in DIAL, strong gains observed (see FT-CLIP entry). When used frozen or without in-domain fine-tuning, CLIP's instruction-labeling accuracy and downstream performance were significantly lower (Appendix A2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Provides strong semantic priors useful for visual grounding of language when combined with modest in-domain fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Out-of-distribution viewpoints / task distributions cause frozen pretrained CLIP to underperform; in-domain fine-tuning was required.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained CLIP is a valuable prior for visual grounding of language in embodied tasks but must be fine-tuned on a small set of in-domain episode–language pairs to be effective for instruction relabeling and downstream policy conditioning in real-world manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1858.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1858.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (candidates)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (language model used to propose candidate instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pretrained language model used in this work to generate a corpus of candidate natural-language instructions (hallucinated rephrasings/variants) that FT-CLIP then scores against episode embeddings for relabeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-3 (instruction proposal corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Generative pre-trained transformer model used here to produce diverse candidate textual instructions (9,336 proposals) derived from prompts that rephrase or imagine variations on structured teleoperator commands.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale internet text (unsupervised language modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper cites GPT-3 and uses it as a candidate-instruction generator; exact GPT-3 pretraining corpora are not detailed in this paper (see Brown et al. for GPT-3 pretraining specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used to propose candidate language labels for relabeling real-world robotic manipulation episodes (office kitchen domain).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>The generated textual candidates are scored by FT-CLIP against episode embeddings; selected candidates augment the training data for language-conditioned robot policies.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A (text generation; no action outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Downstream embodied actions are continuous robot controls; GPT-3 does not directly produce actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Indirect: GPT-3 generates candidate instructions (text); FT-CLIP embeds candidates and episodes and retrieves matches; retrieved text labels are used as conditioning inputs to a behavior-cloning policy that maps language to low-level control.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>None for GPT-3 generation; filtering is performed by FT-CLIP which uses visual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Of the 3,675 unique instructions selected by CLIP from the union candidate set of 18,719, 13.7% were sourced from GPT-3 proposals (L_GPT-3). GPT-3 proposals therefore contributed non-zero but minority share of selected labels; downstream aggregate performance improvements reported for DIAL include the use of GPT-3 candidates in L.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>GPT-3 provides high linguistic diversity and produce rephrasings and semantic variants that can increase coverage of candidate instructions; the CLIP retrieval stage filters irrelevant hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>GPT-3 can hallucinate inaccurate or overly-specific instructions; majority of selected instructions still came from human-sourced candidates, indicating GPT-3 proposals were less frequently chosen by CLIP scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large pretrained LMs like GPT-3 can be used to synthetically expand the space of candidate natural-language instructions for automatic relabeling pipelines, but their utility depends on downstream multimodal filtering (FT-CLIP) to discard hallucinated/irrelevant proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1858.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1858.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>USE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Universal Sentence Encoder (USE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sentence embedding model pretrained on large text corpora and previously used as the language task representation in RT-1; used in this paper as a baseline task encoder for ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Universal sentence encoder</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Universal Sentence Encoder (USE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pretrained text embedding model that maps sentences to fixed-size embeddings, used for language conditioning in prior RT-1 policies and included here as a baseline encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale text corpora used to train sentence embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper references USE as the original RT-1 task encoder; exact corpora/details are in the USE reference and are not re-stated here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>RT-1 language-conditioned robotic manipulation policies (used as baseline in ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Policies conditioned on USE embeddings operate in the same real-world robotic manipulation domain (7-DoF mobile manipulator).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A (text embeddings; no action space in pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Continuous robotic control (same as RT-1 policies).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>USE embeddings are provided as task conditioning inputs to behavior-cloning policies mapping images+language embeddings to actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB image inputs for policies; USE itself requires text inputs only.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>As a baseline task encoder, USE yielded lower novel-instruction performance versus FT-CLIP in the ablations (e.g., Table V: DIAL k=1 with USE had lower spatial/rephrased/overall scores than FT-CLIP variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Simple text embeddings are useful for conditioning but may lack visual grounding that FT-CLIP offers.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Lacks direct visual grounding and may underperform when the downstream task requires visually disambiguating spatial/object instances.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a visual-language-aligned task encoder (FT-CLIP) improved downstream instruction-following performance relative to a pure text sentence encoder (USE) in the real-world manipulation setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1858.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1858.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DistilBERT (LOReL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DistilBERT (as used in LOReL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distilled text transformer model referenced in related work (LOReL) used as a text embedding component for learning reward models from offline robot datasets (related mention in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>DistilBERT (in LOReL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Distilled BERT text embedding used by LOReL (mentioned in related work) combined with a learned neural reward model to map text-language labels to rewards from offline robotic data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained on text corpora (BERT-style pretraining; distilled).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper cites LOReL which uses a pretrained DistilBERT sentence embedding as an input to a reward model trained on crowd-sourced annotations; exact pretraining corpora/details are in the DistilBERT reference, not repeated here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Offline robot datasets (reward learning) — mentioned as related work, not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>LOReL learns reward functions from offline robot datasets with crowd-sourced annotations, for use in offline RL.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A (text embedding).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Robotic control in referenced work; not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>DistilBERT produces sentence embeddings which are fed to a learned reward network that scores episodes; not directly used as policy-conditioned input in the main experiments of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Using a pretrained text embedder enables reward models to leverage language semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not evaluated in this paper; mention only in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as an example where pretrained text models (DistilBERT) are combined with learned modules to ground language into robotic reward functions (related work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1858.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1858.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIPort: What and Where pathways for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that uses frozen CLIP image and text encoders together with Transporter-like architectures to do imitation learning for manipulation; mentioned as related work for using pretrained VLMs in robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cliport: What and where pathways for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Combines frozen CLIP vision and text encoders with Transporter-style spatial reasoning networks for imitation learning on manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>CLIP pretrained on image–text internet data (used as frozen encoder in CLIPort).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Original CLIP pretraining details in CLIP paper; CLIPort uses frozen CLIP as a perceptual/textual backbone in imitation-learning pipelines (referenced but not evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotic manipulation via imitation learning (simulated/real setups in CLIPort paper).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Manipulation tasks requiring object 'what' and 'where' reasoning; CLIP embeddings drive perception and ground language.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A (CLIP pretraining has no action space).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Manipulation actions in imitation learning pipelines (discrete/continuous gripper motions depending on implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Frozen CLIP encoders produce aligned visual and text features used by Transporter/Transporter-like modules to produce low-level action outputs (architecture-specific mapping).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images and language instructions for conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Visual–language alignment in CLIP provides useful grounding for mapping language to spatial manipulation primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not evaluated within this paper; cited as relevant prior art.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates prior successful use of pretrained VLMs as perception and language backbones for robotic manipulation (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1858.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1858.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MineCLIP / MineDojo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MineCLIP (from MineDojo / Minedojo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related prior work that fine-tunes CLIP encoders with contrastive losses on large offline video datasets (Minecraft) and uses the resulting representations for language-conditioned control — cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Minedojo: Building open-ended embodied agents with internet-scale knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>MineCLIP (as described in MineDojo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Fine-tuning CLIP image/text encoders contrastively on a large offline dataset of Minecraft videos to obtain representations used for language-conditioned control and RL.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretraining: CLIP image–text internet pretraining; fine-tuning on large-scale Minecraft video datasets (visual domain).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Referenced as MineDojo/MineCLIP; specifics (video dataset size) are in the cited MineDojo work and not reiterated here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Language-conditioned control in Minecraft / game-based embodied environments (as in MineDojo experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Interactive, open-ended game environment (Minecraft) where language-conditioned agents perform tasks; representations used for RL and control.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A for CLIP pretraining; Minecraft has discrete action commands in-game.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete game-control actions (in Minecraft) for agents trained in MineDojo.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>MineCLIP representations feed into RL/control modules (e.g., language-conditioned policies) to map from language/visual inputs to game actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Visual observations (game frames) and text descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Contrastive VLM pretraining provides semantic priors useful for downstream embodied tasks in game environments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Mentioned as related work; specifics not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Example of adapting CLIP-style models (pretrained on image–text data) to an embodied interactive domain (Minecraft) and using them for downstream control (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1858.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1858.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs as Planners</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models used as zero-shot planners for embodied agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related works cited that use large pretrained language models as high-level planners or to extract actionable steps for embodied agents in simulated and real robotics settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Large language models used as planners (e.g., GPT-family variants cited in Huang et al., Inner Monologue)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pretrained LLMs are used to decompose high-level instructions into stepwise plans or programs which are then executed by downstream controllers or planners in embodied environments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale internet text pretraining (autoregressive/transformer LMs).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Cited works use public LLMs to produce plans or programs for embodied tasks (details in referenced papers).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Simulated and real-world robotics long-horizon planning tasks (as in cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>LLMs propose multi-step plans or code/programs that a lower-level controller or executor maps to concrete sensory-motor actions in an embodied environment.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A (text/planning outputs); planning outputs are symbolic/high-level steps.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Low-level continuous/discrete robotic controls handled by downstream controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>LLM outputs are interpreted by downstream execution modules (planners, policy libraries, or grounding modules) that map textual plan steps into executable motor primitives or subpolicies.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Embodied systems require vision and proprioception; LLMs themselves need only text inputs but may be provided scene descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Semantic knowledge and compositionality in LLM outputs can bootstrap long-horizon planning when appropriately grounded by perception and low-level controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Bridging high-level textual plans to low-level control requires robust grounding and environment models; cited as an active area of research.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited literature shows pretrained LLMs can act as zero-shot planners for embodied tasks, but require additional grounding/execution modules to map verbal plans into motor actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Cliport: What and where pathways for robotic manipulation <em>(Rating: 2)</em></li>
                <li>Minedojo: Building open-ended embodied agents with internet-scale knowledge <em>(Rating: 2)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Universal sentence encoder <em>(Rating: 2)</em></li>
                <li>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter <em>(Rating: 1)</em></li>
                <li>R3M: A universal visual representation for robot manipulation <em>(Rating: 1)</em></li>
                <li>Learning language-conditioned robot behavior from offline data and crowd-sourced annotation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1858",
    "paper_id": "paper-55dff8d4cdeab9c86dccb1c8b739ac0518cdbed0",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "FT-CLIP (DIAL)",
            "name_full": "Fine-tuned CLIP used in DIAL",
            "brief_description": "The OpenAI CLIP model (ViT-based) initialized from pretrained weights and fine-tuned on a small set of robot trajectories with crowd-sourced first/last-frame language annotations; used both to propose natural-language hindsight labels for a large offline robot dataset and as the task-language encoder for downstream behavior-cloning policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "FT-CLIP (fine-tuned CLIP)",
            "model_agent_description": "Vision-language model: CLIP ViT image encoder + CLIP Transformer text encoder initialized from OpenAI ViT-B/32 checkpoint, with an MLP fusion network that concatenates image embeddings for initial and final frames to form an episode embedding. Fine-tuned contrastively on (initial-final image, crowd-sourced instruction) pairs using symmetric cross-entropy softmax loss with temperature.",
            "pretraining_data_type": "Image–text web pretraining (vision-language) followed by in-domain fine-tuning on paired robot images and language",
            "pretraining_data_details": "Initialized from OpenAI CLIP ViT-B/32 pretrained checkpoint (internet-scale image-text pretraining; exact dataset not specified in this paper). Fine-tuned on D_A: 5,600 episode–instruction pairs (2,800 episodes × 2 captions) collected from real robot teleoperation; fine-tuning used batch size 64, 100,000 iterations, and fine-tuned both image & text encoders plus a 200-dim MLP fusion.",
            "embodied_task_name": "Real-world robotic manipulation in office kitchen (mobile manipulator)",
            "embodied_task_description": "Mobile manipulator with 7-DoF arm and parallel-jaw gripper operating on real kitchen countertops (varied scenes, drawers, receptacles). Tasks include picking, placing, moving objects relative to others; policies take RGB image history and language goal and output closed-loop control.",
            "action_space_text": "N/A for pretraining stage (CLIP pretraining uses image–text pairs; no action space), fine-tuning uses static episode descriptions (no actions in CLIP contrastive training).",
            "action_space_embodied": "Downstream policy outputs discrete action tokens mapped to continuous motor controls for a 7-DoF arm and gripper (closed-loop control from pixels via the RT-1 policy architecture).",
            "action_mapping_method": "Indirect learned mapping: FT-CLIP provides language and episode embeddings and is used to (1) retrieve candidate natural language labels for trajectories (cosine similarity softmax over candidate texts) and (2) provide the language embedding as input to a behavior-cloning policy (RT-1) that learns to map (image history, language embedding) → discrete action tokens → continuous low-level controls.",
            "perception_requirements": "First-person RGB (over-the-shoulder) images; DIAL fine-tuning uses initial and final frames for each episode; downstream policies use a short history of RGB frames.",
            "transfer_successful": true,
            "performance_with_pretraining": "DIAL (using FT-CLIP) achieved large gains on novel evaluation instructions: overall success 60.0% on 60 held-out novel instructions (Table III). Best spatial-task performance reported 68.0% (Min-p p=0.2 / combined dataset variant, Table IV). Using FT-CLIP as the task encoder improved novel-instruction success relative to USE in their ablations (see Table V).",
            "performance_without_pretraining": "Baselines without DIAL or without FT-CLIP (e.g., RT-1 or USE-task-encoder baselines) achieved lower novel-instruction success: overall 42.5% (no instruction augmentation baseline, Table III); spatial-task baselines ranged 30–46% depending on dataset combination (Table II/III).",
            "sample_complexity_with_pretraining": "Training datasets used with FT-CLIP in reported experiments included: D_A = 5,600 annotated episodes (fine-tune set); D_B = 80,000 teleoperated episodes (original demos); D_C (DIAL relabeled) examples varied by hyperparameter—e.g., Min-p p=0.2 produced D_C = 38,516 relabeled episodes. Common experimental trainings used combinations such as {D_A, D_B, D_C} totaling up to ~124k episodes in that setting.",
            "sample_complexity_without_pretraining": "Baseline RT-1 / no-augmentation trainings used D_B = 80,000 episodes and/or D_A = 5,600 episodes depending on the ablation. The paper does not report sample-to-threshold curves, so no samples-to-performance-threshold numbers are given.",
            "sample_complexity_gain": "Not reported as samples-to-threshold. Empirically, adding DIAL relabeled data (e.g. +38.5k D_C) and using FT-CLIP yielded an absolute overall success increase of ~17.5 percentage points (42.5% → 60.0%), which the paper frames as a &gt;41% relative improvement in novel-task success.",
            "transfer_success_factors": "Key factors: (1) Internet-scale CLIP pretraining provides a broad semantic prior; (2) fine-tuning on a small set of in-domain episode–language pairs (D_A) aligns CLIP to embodied viewpoints and instruction phrasing; (3) CLIP’s multimodal embedding allows visually grounded retrieval of candidate captions (enabling many new semantic labels); (4) using the same FT-CLIP text encoder as the policy task representation yields consistent downstream conditioning.",
            "transfer_failure_factors": "Limitations: (1) domain gap — frozen pretrained CLIP underperforms and fine-tuning on in-domain data is necessary (Appendix A2); (2) CLIP-based relabeling only observes first and final frames, so temporal/multi-step skills are not captured; (3) label noise from weakly-relabeled instructions can degrade performance if over-augmented (Top-k with large k reduced accuracy); (4) motor-control failures remain (some evaluation failures due to control rather than instruction understanding).",
            "key_findings": "A vision–language model pretrained on image–language data (CLIP) and fine-tuned on a small set of in-domain episode-level language annotations can be used to automatically relabel large offline robot datasets (DIAL), producing relabeled datasets that substantially improve behavior-cloned policies' ability to follow novel, unseen natural-language instructions in real-world 3D manipulation; success depends strongly on fine-tuning, conservative candidate selection (Min-p), and the availability of sufficient relabeled examples (tens of thousands) while managing label noise.",
            "uuid": "e1858.0",
            "source_info": {
                "paper_title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "OpenAI CLIP",
            "name_full": "Contrastive Language–Image Pre-training (CLIP)",
            "brief_description": "A large vision–language model pretrained contrastively on image–text pairs from the internet to produce joint image and text embeddings; used in this paper as the initialization for the FT-CLIP model and as a baseline when frozen.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_agent_name": "OpenAI CLIP (ViT-B/32 checkpoint)",
            "model_agent_description": "Pretrained vision–language model consisting of a ViT-based image encoder and a Transformer-based text encoder trained contrastively on image-text pairs to produce aligned embeddings.",
            "pretraining_data_type": "Internet-scale image–text pairs (vision-language supervision)",
            "pretraining_data_details": "Paper uses OpenAI CLIP ViT-B/32 checkpoint as initialization; the original CLIP was pretrained on large image–text datasets (details are provided in the CLIP paper, not repeated here).",
            "embodied_task_name": "Used as initialization or frozen encoder for robotic manipulation tasks (this paper and prior works such as CLIPort).",
            "embodied_task_description": "Robotic manipulation in real/simulated environments: CLIP embeddings are used as visual and/or language representations for instruction-conditioned control or for relabeling datasets.",
            "action_space_text": "N/A (pretraining is passive image–text pairs; no action semantics in pretraining).",
            "action_space_embodied": "Robotic continuous motor controls in downstream tasks (7-DoF arm, gripper in this work).",
            "action_mapping_method": "In this paper CLIP is fine-tuned and its embeddings are used for retrieval of candidate instructions (cosine similarity) and used as task embeddings input to a learned BC policy; in other works CLIP has been used as frozen encoders feeding downstream policy models (e.g., CLIPort).",
            "perception_requirements": "RGB images; CLIP operates on RGB image inputs for embedding computation.",
            "transfer_successful": null,
            "performance_with_pretraining": "When fine-tuned (FT-CLIP) and used in DIAL, strong gains observed (see FT-CLIP entry). When used frozen or without in-domain fine-tuning, CLIP's instruction-labeling accuracy and downstream performance were significantly lower (Appendix A2).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Provides strong semantic priors useful for visual grounding of language when combined with modest in-domain fine-tuning.",
            "transfer_failure_factors": "Out-of-distribution viewpoints / task distributions cause frozen pretrained CLIP to underperform; in-domain fine-tuning was required.",
            "key_findings": "Pretrained CLIP is a valuable prior for visual grounding of language in embodied tasks but must be fine-tuned on a small set of in-domain episode–language pairs to be effective for instruction relabeling and downstream policy conditioning in real-world manipulation.",
            "uuid": "e1858.1",
            "source_info": {
                "paper_title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "GPT-3 (candidates)",
            "name_full": "GPT-3 (language model used to propose candidate instructions)",
            "brief_description": "A large pretrained language model used in this work to generate a corpus of candidate natural-language instructions (hallucinated rephrasings/variants) that FT-CLIP then scores against episode embeddings for relabeling.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_agent_name": "GPT-3 (instruction proposal corpus)",
            "model_agent_description": "Generative pre-trained transformer model used here to produce diverse candidate textual instructions (9,336 proposals) derived from prompts that rephrase or imagine variations on structured teleoperator commands.",
            "pretraining_data_type": "Large-scale internet text (unsupervised language modeling)",
            "pretraining_data_details": "The paper cites GPT-3 and uses it as a candidate-instruction generator; exact GPT-3 pretraining corpora are not detailed in this paper (see Brown et al. for GPT-3 pretraining specifics).",
            "embodied_task_name": "Used to propose candidate language labels for relabeling real-world robotic manipulation episodes (office kitchen domain).",
            "embodied_task_description": "The generated textual candidates are scored by FT-CLIP against episode embeddings; selected candidates augment the training data for language-conditioned robot policies.",
            "action_space_text": "N/A (text generation; no action outputs).",
            "action_space_embodied": "Downstream embodied actions are continuous robot controls; GPT-3 does not directly produce actions.",
            "action_mapping_method": "Indirect: GPT-3 generates candidate instructions (text); FT-CLIP embeds candidates and episodes and retrieves matches; retrieved text labels are used as conditioning inputs to a behavior-cloning policy that maps language to low-level control.",
            "perception_requirements": "None for GPT-3 generation; filtering is performed by FT-CLIP which uses visual inputs.",
            "transfer_successful": true,
            "performance_with_pretraining": "Of the 3,675 unique instructions selected by CLIP from the union candidate set of 18,719, 13.7% were sourced from GPT-3 proposals (L_GPT-3). GPT-3 proposals therefore contributed non-zero but minority share of selected labels; downstream aggregate performance improvements reported for DIAL include the use of GPT-3 candidates in L.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "GPT-3 provides high linguistic diversity and produce rephrasings and semantic variants that can increase coverage of candidate instructions; the CLIP retrieval stage filters irrelevant hallucinations.",
            "transfer_failure_factors": "GPT-3 can hallucinate inaccurate or overly-specific instructions; majority of selected instructions still came from human-sourced candidates, indicating GPT-3 proposals were less frequently chosen by CLIP scoring.",
            "key_findings": "Large pretrained LMs like GPT-3 can be used to synthetically expand the space of candidate natural-language instructions for automatic relabeling pipelines, but their utility depends on downstream multimodal filtering (FT-CLIP) to discard hallucinated/irrelevant proposals.",
            "uuid": "e1858.2",
            "source_info": {
                "paper_title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "USE",
            "name_full": "Universal Sentence Encoder (USE)",
            "brief_description": "A sentence embedding model pretrained on large text corpora and previously used as the language task representation in RT-1; used in this paper as a baseline task encoder for ablations.",
            "citation_title": "Universal sentence encoder",
            "mention_or_use": "use",
            "model_agent_name": "Universal Sentence Encoder (USE)",
            "model_agent_description": "Pretrained text embedding model that maps sentences to fixed-size embeddings, used for language conditioning in prior RT-1 policies and included here as a baseline encoder.",
            "pretraining_data_type": "Large-scale text corpora used to train sentence embeddings",
            "pretraining_data_details": "The paper references USE as the original RT-1 task encoder; exact corpora/details are in the USE reference and are not re-stated here.",
            "embodied_task_name": "RT-1 language-conditioned robotic manipulation policies (used as baseline in ablations).",
            "embodied_task_description": "Policies conditioned on USE embeddings operate in the same real-world robotic manipulation domain (7-DoF mobile manipulator).",
            "action_space_text": "N/A (text embeddings; no action space in pretraining).",
            "action_space_embodied": "Continuous robotic control (same as RT-1 policies).",
            "action_mapping_method": "USE embeddings are provided as task conditioning inputs to behavior-cloning policies mapping images+language embeddings to actions.",
            "perception_requirements": "RGB image inputs for policies; USE itself requires text inputs only.",
            "transfer_successful": null,
            "performance_with_pretraining": "As a baseline task encoder, USE yielded lower novel-instruction performance versus FT-CLIP in the ablations (e.g., Table V: DIAL k=1 with USE had lower spatial/rephrased/overall scores than FT-CLIP variants).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Simple text embeddings are useful for conditioning but may lack visual grounding that FT-CLIP offers.",
            "transfer_failure_factors": "Lacks direct visual grounding and may underperform when the downstream task requires visually disambiguating spatial/object instances.",
            "key_findings": "Using a visual-language-aligned task encoder (FT-CLIP) improved downstream instruction-following performance relative to a pure text sentence encoder (USE) in the real-world manipulation setting.",
            "uuid": "e1858.3",
            "source_info": {
                "paper_title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "DistilBERT (LOReL)",
            "name_full": "DistilBERT (as used in LOReL)",
            "brief_description": "A distilled text transformer model referenced in related work (LOReL) used as a text embedding component for learning reward models from offline robot datasets (related mention in paper).",
            "citation_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "mention_or_use": "mention",
            "model_agent_name": "DistilBERT (in LOReL)",
            "model_agent_description": "Distilled BERT text embedding used by LOReL (mentioned in related work) combined with a learned neural reward model to map text-language labels to rewards from offline robotic data.",
            "pretraining_data_type": "Pretrained on text corpora (BERT-style pretraining; distilled).",
            "pretraining_data_details": "The paper cites LOReL which uses a pretrained DistilBERT sentence embedding as an input to a reward model trained on crowd-sourced annotations; exact pretraining corpora/details are in the DistilBERT reference, not repeated here.",
            "embodied_task_name": "Offline robot datasets (reward learning) — mentioned as related work, not evaluated here.",
            "embodied_task_description": "LOReL learns reward functions from offline robot datasets with crowd-sourced annotations, for use in offline RL.",
            "action_space_text": "N/A (text embedding).",
            "action_space_embodied": "Robotic control in referenced work; not specified in this paper.",
            "action_mapping_method": "DistilBERT produces sentence embeddings which are fed to a learned reward network that scores episodes; not directly used as policy-conditioned input in the main experiments of this paper.",
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Using a pretrained text embedder enables reward models to leverage language semantics.",
            "transfer_failure_factors": "Not evaluated in this paper; mention only in related work.",
            "key_findings": "Referenced as an example where pretrained text models (DistilBERT) are combined with learned modules to ground language into robotic reward functions (related work).",
            "uuid": "e1858.4",
            "source_info": {
                "paper_title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "CLIPort",
            "name_full": "CLIPort: What and Where pathways for robotic manipulation",
            "brief_description": "Prior work that uses frozen CLIP image and text encoders together with Transporter-like architectures to do imitation learning for manipulation; mentioned as related work for using pretrained VLMs in robotics.",
            "citation_title": "Cliport: What and where pathways for robotic manipulation",
            "mention_or_use": "mention",
            "model_agent_name": "CLIPort",
            "model_agent_description": "Combines frozen CLIP vision and text encoders with Transporter-style spatial reasoning networks for imitation learning on manipulation tasks.",
            "pretraining_data_type": "CLIP pretrained on image–text internet data (used as frozen encoder in CLIPort).",
            "pretraining_data_details": "Original CLIP pretraining details in CLIP paper; CLIPort uses frozen CLIP as a perceptual/textual backbone in imitation-learning pipelines (referenced but not evaluated here).",
            "embodied_task_name": "Robotic manipulation via imitation learning (simulated/real setups in CLIPort paper).",
            "embodied_task_description": "Manipulation tasks requiring object 'what' and 'where' reasoning; CLIP embeddings drive perception and ground language.",
            "action_space_text": "N/A (CLIP pretraining has no action space).",
            "action_space_embodied": "Manipulation actions in imitation learning pipelines (discrete/continuous gripper motions depending on implementation).",
            "action_mapping_method": "Frozen CLIP encoders produce aligned visual and text features used by Transporter/Transporter-like modules to produce low-level action outputs (architecture-specific mapping).",
            "perception_requirements": "RGB images and language instructions for conditioning.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Visual–language alignment in CLIP provides useful grounding for mapping language to spatial manipulation primitives.",
            "transfer_failure_factors": "Not evaluated within this paper; cited as relevant prior art.",
            "key_findings": "Demonstrates prior successful use of pretrained VLMs as perception and language backbones for robotic manipulation (cited as related work).",
            "uuid": "e1858.5",
            "source_info": {
                "paper_title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "MineCLIP / MineDojo",
            "name_full": "MineCLIP (from MineDojo / Minedojo)",
            "brief_description": "Related prior work that fine-tunes CLIP encoders with contrastive losses on large offline video datasets (Minecraft) and uses the resulting representations for language-conditioned control — cited in related work.",
            "citation_title": "Minedojo: Building open-ended embodied agents with internet-scale knowledge",
            "mention_or_use": "mention",
            "model_agent_name": "MineCLIP (as described in MineDojo)",
            "model_agent_description": "Fine-tuning CLIP image/text encoders contrastively on a large offline dataset of Minecraft videos to obtain representations used for language-conditioned control and RL.",
            "pretraining_data_type": "Pretraining: CLIP image–text internet pretraining; fine-tuning on large-scale Minecraft video datasets (visual domain).",
            "pretraining_data_details": "Referenced as MineDojo/MineCLIP; specifics (video dataset size) are in the cited MineDojo work and not reiterated here.",
            "embodied_task_name": "Language-conditioned control in Minecraft / game-based embodied environments (as in MineDojo experiments).",
            "embodied_task_description": "Interactive, open-ended game environment (Minecraft) where language-conditioned agents perform tasks; representations used for RL and control.",
            "action_space_text": "N/A for CLIP pretraining; Minecraft has discrete action commands in-game.",
            "action_space_embodied": "Discrete game-control actions (in Minecraft) for agents trained in MineDojo.",
            "action_mapping_method": "MineCLIP representations feed into RL/control modules (e.g., language-conditioned policies) to map from language/visual inputs to game actions.",
            "perception_requirements": "Visual observations (game frames) and text descriptions.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Contrastive VLM pretraining provides semantic priors useful for downstream embodied tasks in game environments.",
            "transfer_failure_factors": "Mentioned as related work; specifics not analyzed in this paper.",
            "key_findings": "Example of adapting CLIP-style models (pretrained on image–text data) to an embodied interactive domain (Minecraft) and using them for downstream control (cited as related work).",
            "uuid": "e1858.6",
            "source_info": {
                "paper_title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "LLMs as Planners",
            "name_full": "Language models used as zero-shot planners for embodied agents",
            "brief_description": "Related works cited that use large pretrained language models as high-level planners or to extract actionable steps for embodied agents in simulated and real robotics settings.",
            "citation_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "mention_or_use": "mention",
            "model_agent_name": "Large language models used as planners (e.g., GPT-family variants cited in Huang et al., Inner Monologue)",
            "model_agent_description": "Pretrained LLMs are used to decompose high-level instructions into stepwise plans or programs which are then executed by downstream controllers or planners in embodied environments.",
            "pretraining_data_type": "Large-scale internet text pretraining (autoregressive/transformer LMs).",
            "pretraining_data_details": "Cited works use public LLMs to produce plans or programs for embodied tasks (details in referenced papers).",
            "embodied_task_name": "Simulated and real-world robotics long-horizon planning tasks (as in cited works).",
            "embodied_task_description": "LLMs propose multi-step plans or code/programs that a lower-level controller or executor maps to concrete sensory-motor actions in an embodied environment.",
            "action_space_text": "N/A (text/planning outputs); planning outputs are symbolic/high-level steps.",
            "action_space_embodied": "Low-level continuous/discrete robotic controls handled by downstream controllers.",
            "action_mapping_method": "LLM outputs are interpreted by downstream execution modules (planners, policy libraries, or grounding modules) that map textual plan steps into executable motor primitives or subpolicies.",
            "perception_requirements": "Embodied systems require vision and proprioception; LLMs themselves need only text inputs but may be provided scene descriptions.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Semantic knowledge and compositionality in LLM outputs can bootstrap long-horizon planning when appropriately grounded by perception and low-level controllers.",
            "transfer_failure_factors": "Bridging high-level textual plans to low-level control requires robust grounding and environment models; cited as an active area of research.",
            "key_findings": "Cited literature shows pretrained LLMs can act as zero-shot planners for embodied tasks, but require additional grounding/execution modules to map verbal plans into motor actions.",
            "uuid": "e1858.7",
            "source_info": {
                "paper_title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Cliport: What and where pathways for robotic manipulation",
            "rating": 2
        },
        {
            "paper_title": "Minedojo: Building open-ended embodied agents with internet-scale knowledge",
            "rating": 2
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Universal sentence encoder",
            "rating": 2
        },
        {
            "paper_title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "rating": 1
        },
        {
            "paper_title": "R3M: A universal visual representation for robot manipulation",
            "rating": 1
        },
        {
            "paper_title": "Learning language-conditioned robot behavior from offline data and crowd-sourced annotation",
            "rating": 2
        }
    ],
    "cost": 0.02826725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models</h1>
<p>Ted Xiao ${ }^{1, <em>}$ Harris Chan ${ }^{1,2, </em>}$ Pierre Sermanet ${ }^{1}$ Ayzaan Wahid ${ }^{1}$ Anthony Brohan ${ }^{1}$<br>Karol Hausman ${ }^{1}$ Sergey Levine ${ }^{1}$ Jonathan Tompson ${ }^{1}$<br>${ }^{1}$ Robotics at Google ${ }^{2}$ University of Toronto<br>Project website: https://instructionaugmentation.github.io</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: DIAL consists of three steps: (1) Contrastive fine-tuning of a vision-language model (VLM) such as CLIP [39] on a small dataset of robot manipulation trajectories with crowd-sourced natural language annotation, (2) labeling a larger dataset of trajectories using the fine-tuned VLM (in dashed outline), and (3) training a language-conditioned policy using behavior cloning on the original and relabeled dataset. We evaluate the trained policy on unseen instructions. See Section III for more details.</p>
<h4>Abstract</h4>
<p>Robotic manipulation policies that follow natural language instructions are typically trained from corpora of robot-language data that were either collected with specific tasks in mind or expensively relabeled by humans with varied language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets with limited ground truth annotations? For example, if the original annotations contained templated task descriptions such as "pick apple", a pretrained VLM-based labeler could significantly expand the number of semantic concepts available in the data and introduce spatial concepts such as "the apple on the right side of the table" or alternative phrasings such as "the red colored fruit". To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>control (DIAL): we utilize semi-supervised language labels to propagate CLIP's semantic knowledge onto large datasets of unlabeled demonstration data, from which we then train languageconditioned policies. This method enables cheaper acquisition of useful language descriptions compared to expensive human labels, allowing for more efficient label coverage of large-scale datasets. We apply DIAL to a challenging real-world robotic manipulation domain where only $\mathbf{3 . 5 \%}$ of the $\mathbf{8 0 , 0 0 0}$ demonstrations contain crowd-sourced language annotations. Through a large-scale study of over 1,300 real world evaluations, we find that DIAL enables imitation learning policies to acquire new capabilities and generalize to $\mathbf{6 0}$ novel instructions unseen in the original dataset.</p>
<h2>I. INTRODUCTION</h2>
<p>Advances in deep learning architectures have made it possible to train end-to-end robotic control policies for following a wide range of textual instructions, often by integrating state-of-the-art language embeddings and pretrained encoders with</p>
<p>imitation learning on top of large, manually collected datasets of robotic demonstrations annotated with text commands [24]. However the performance of such methods is critically dependent on the quantity and breadth of instruction-labeled demonstration data that is available [32], and producing expert demonstrations of robot motion often requires expertise and time [33]. Can we squeeze more generalization capacity out of a given set of demonstrations, with minimal additional human effort? The key observation we make is that a given demonstration might illustrate more than one behavior - e.g., a motion that picks up the leftmost can in Figure 1 is an example of how to pick up a coke can, the left can in a row of three, a first step toward clearing the table, and more. Can we leverage this observation to relabel a given demonstration dataset to enable a robot to master a broader range of semantic behaviors, and can we do this in a largely automated and scalable way?</p>
<p>One possibility is to leverage large-scale pretrained language models (LLMs) [9, 15] and vision-language models (VLMs) [2, 39], which can be pretrained on Internet-scale data and then be applied to downstream domains. In robotics, they have been used as representations for perception [37, 42], as task representation for language [24, 30], or as planners [1, 22]. In contrast, we seek to apply pretrained VLMs to the datasets themselves: can we use VLMs for instruction augmentation, where we relabel existing offline trajectory datasets with additional language instructions?</p>
<p>In this work, we introduce Data-driven Instruction Augmentation for Language-conditioned Control (DIAL), a method that performs instruction augmentation with pretrained VLMs to weakly relabel offline control datasets. We implement an instantiation of our method with CLIP [39] on a challenging real-world robotic manipulation setting with 80,000 teleoperated demonstrations, which include 2,800 demonstrations that are labeled by crowd-sourced language annotators. By performing a large quantitative evaluation of over 1,300 real world robot evaluations, we compare our method with baselines and instruction augmentation methods that are not visually grounded. We find that DIAL enables policies to acquire understanding of new concepts not contained in the original task labels and improving performance on 60 novel evaluation instructions by over $41 \%$. Sample emergent capabilities of our method are shown in Figure 5.</p>
<h2>II. Related Work</h2>
<p>a) Language instruction following in robotics: Languageinstruction following agents have been extensively explored with engineered symbolic representations [17, 45], with reinforcement learning (RL) [5, 20, 29], and with imitation learning [3, 6, 24, 30]. Recent advances in deep learning with large amounts of data have led to advances in methods for learning instruction-conditioned policies [1, 28, 34, 43, 44]. Latent Motor Policies (LMP) [31] learns hierarchical goal-conditioned policies. Subsequent Language from Play (LfP) [30] uses language goals provided by large dataset of hindsight human labels on robotic play data. Similarly, Interactive Language [32] uses crowd-sourced hindsight labels on diverse demonstration
data for table-top object rearrangements. In contrast, our method does not rely on crowd-sourced language labels at scale, but instead leverages a modest number of language labels by using a learned model to provide weak hindsight labeling for the rest of the data.
b) Pretrained VLMs and LLMs for language-conditioned control: Prior works have leveraged pretrained VLMs and LLMs for language-conditioned control, as part of reward modeling [18, 36], as part of the agent architecture [37, 42], or as planners for long-horizon tasks [1, 22, 23]. MineCLIP [18] fine-tunes CLIP [39] encoders using a contrastive loss on a large offline dataset of Minecraft videos and optimizes a language-conditioned control policy on top of the finetuned CLIP representations through online RL. LOReL [36] learns a reward function from offline robot datasets with crowd sourced annotations using a neural network trained from scratch combined with a pretraind DistilBERT sentence embedding [41] using a binary cross entropy loss. CLIPort [42] uses a frozen CLIP vision and text encoders in combination with Transporter networks [47] for imitation learning. R3M [37] uses representations pretrained constrastively on Ego4D [21] human video datasets for robotic policy learning via imitation learning. For long-horizon language instructions, LLMs have been used as planners both in simulated [22] and real-world robotics settings [1]. Our approach fine-tunes CLIP on our real robot offline dataset and is used for instruction augmentation for a behavior cloning agent, instead of directly using the CLIP model as a reward model and optimizing an RL agent.
c) Hindsight relabeling for goal-conditioned reinforcement learning: The relabeling approach for goal-conditioned reinforcement learning [38] is used in tabular [26] and continuous [4] settings, where the desired goals are relabeled with achieved goals to generate positive examples in sparse reward environments. This method has been applied to environments with goals represented as images [12], task IDs [27], and language instructions [11, 13, 25]. Previous works with language goals used environment simulators [11, 25] or learned models [13, 40] to provide hindsight labels. Our work introduces the novel contribution of visual grounding by leveraging VLMs to generate unstructured natural language relabeling instructions, enabling scaling to complex real robot environments.</p>
<h2>III. Data-DRIVEN InStruction Augmentation for LANGUAGE-CONDITIONED CONTROL</h2>
<p>In this section, we describe our method, DIAL, which consists of three stages: (1) fine-tuning a VLM's vision and language representations on a small offline dataset of trajectories with crowd-sourced episode-level natural language descriptions, (2) generating alternative instructions for a larger offline dataset of trajectories with the VLM, and (3) learning a language-conditioned policy via behavioral cloning on this instruction-augmented dataset.</p>
<h2>A. Fine-tuning Vision-Language Model Representations</h2>
<p>We first collect a dataset of robot trajectories, from either human teleoperated demonstrations on a wide variety of tasks</p>
<p>[1], or from unstructured robotic “play” data [31]. We partition this dataset with uniform sampling into two subsets: a small subset to be annotated by human annotators, and a much larger subset to be labeled by the VLM finetuned on the former. The smaller portion is selected because the process of human labeling is time-consuming and requires significant effort and cost.</p>
<p>Let the small dataset of $N$ trajectories be $[\tau_{1},\ldots,\tau_{N}]$, $\tau_{n}=$ $([(s_{0}^{n},a_{0}^{n}),(s_{1}^{n},a_{1}^{n}),\ldots,(s_{T}^{n})])$, where $s_{t}^{n}$ and $a_{t}^{n}$ denote the observed state and action, respectively, at time $t$ for the $n$-th episode. We then collect a corresponding natural language annotation $l^{n}$ for the $n$-th episode describing what the robot agent did in the episode via crowd-sourcing.</p>
<p>When producing these descriptions, the crowd-sourced evaluators observe the first frame, $s_{0}$, and last frame, $s_{T}$, from the agent’s first-person view. We refer to these instructions as crowd-sourced instructions. Together, we denote the first dataset $\mathcal{D}<em 1="1">{A}=[(\tau</em>$.},l^{1}),\ldots,(\tau_{N},l^{N})]$ as the paired trajectories and crowd-sourced labels. Our method then fine-tunes a vision and language model representation on $\mathcal{D}_{A</p>
<p>Motivated by promising results of CLIP in robotics in prior works [35, 42], our instantiation of DIAL uses CLIP [39] for both instruction augmentation and task representation; nonetheless, other VLMs or captioning models could also be used to propose instruction augmentations. Given a batch of $B$ initial state $s_{0}$, final state $s_{T}$, and crowdsourced instruction $l$ tuple, the model is trained to predict which of the $B^{2}$ (initial-final state, crowd-sourced instruction) pairs co-occurred. We use CLIP’s Transformer-based text encoder $T_{enc}$ to embed the crowd-sourced instruction to a latent space $z_{l}^{n}=T_{enc}(l^{n})/\left|T_{enc}(l^{n})\right| \in \mathbb{R}^{d}$ and CLIP’s Vision Transformer-based (ViT) [16] image encoder $I_{e n c}$ to embed the initial and final state, and further concatenate these two embeddings and pass through a fully connected neural network $f_{\theta}$, producing the vision embedding $z_{s}^{n}=f_{\theta}\left(\left[I_{e n c}\left(s_{0}^{n}\right) ; I_{e n c}\left(s_{T}^{n}\right)\right]\right) /|f_{\theta}\left(\left[I_{e n c}\left(s_{0}^{n}\right) ; I_{e n c}\left(s_{T}^{n}\right)\right]\right)| \in$ $\mathbb{R}^{d}$. $B^{2}$ similarity logits are formed by applying dot product across all state-instruction pairs, and a symmetric cross entropy loss term is calculated by applying softmax normalization with temperature $\alpha$ across the states and texts:</p>
<p>$\mathcal{L}<em n="1">{\theta}=-\sum</em>\right)\right]$}^{B}\left[\log \left(\frac{e^{z_{l}^{n} \cdot z_{s}^{n} / \alpha}}{\sum_{k=1}^{B} e^{z_{l}^{k} \cdot z_{s}^{n} / \alpha}}\right)+\log \left(\frac{e^{z_{l}^{n} \cdot z_{s}^{n} / \alpha}}{\sum_{k=1}^{B} e^{z_{l}^{n} \cdot z_{s}^{k} / \alpha}</p>
<h2>B. Instruction Augmentation</h2>
<p>In the larger partition of the original dataset, which we denote as dataset $\mathcal{D}<em 1="1">{B}$, contains $M \gg N$ trajectories $\left[\hat{\tau}</em>}, \ldots, \hat{\tau<em m="m">{M}\right]$, where $\hat{\tau}</em>}=\left(\left[\left(\hat{s<em 0="0">{0}^{m}, \hat{a}</em>}^{m}\right),\left(\hat{s<em 1="1">{1}^{m}, \hat{a}</em>}^{m}\right), \ldots,\left(\hat{s<em A="A">{T}^{m}\right)\right]\right)$. In contrast to $\mathcal{D}</em>$ do not have any associated natural language labels.}$, we assume that trajectories in $\mathcal{D}_{B</p>
<p>We use the fine-tuned VLM model to propose natural language instructions $\hat{l}^{m}$ for each trajectory $\hat{\tau}<em B="B">{m}$ to augment $\mathcal{D}</em>$ as well as additional instructions drawing from GPT-3 [9] proposals of possible tasks, which we denote}$. While $\hat{l}^{m}$ could be drawn from any reasonable corpus, our specific instantiation of DIAL sources these candidate instructions from $\mathcal{D}_{A</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3: The construction of datasets: Dataset A ( $\mathcal{D}<em n="n">{A}$ ) (blue) consists of the $N$ trajectories $\left{\tau</em>\right}<em n="1">{n=1}^{N}$ labeled with crowdsourced instructions $\left{l^{n}\right}</em>}^{N}$ describing what the robot agent performed in the episode. Dataset B ( $\mathcal{D<em m="m">{B}$ ) (yellow) consists of a much larger set of trajectories, $\left{\hat{\tau}</em>\right}<em C="C">{m=1}^{M}$ without crowdsourced instructions. Dataset C ( $\mathcal{D}</em>}$ ) (red, dashed) contains Dataset B trajectories relabeled with VLM-sourced hindsight instruction(s) $\left{\hat{l<em k="k">{1}^{m}, \ldots, \hat{l}</em>\right}}^{m<em G="G" P="P" T-3="T-3">{m=1}^{M}$.
as $\mathcal{D}</em>}$ (the details of this procedure will be covered in Section IV-B). We use the CLIP text encoder to independently embed these candidate natural language instructions, i.e. $\hat{l}^{m} \in L=\left{l^{1}, \ldots, l^{N}\right} \sim \mathcal{D<em G="G" P="P" T-3="T-3">{A} \cup \mathcal{D}</em>$</p>
<p>$$
\left{z_{l}^{1}, \ldots, z_{l}^{N}\right}=\left{T_{e n c}\left(l^{1}\right), \ldots, T_{e n c}\left(l^{N}\right)\right}
$$</p>
<p>Similarly, we use the fine-tuned CLIP image encoder and MLP fusion to embed the initial and final observations from the second dataset:</p>
<p>$$
\left{\hat{z}<em s="s">{s}^{1}, \ldots, \hat{z}</em>}^{M}\right}=\left{f_{\theta}\left(\left[I_{e n c}\left(\hat{s<em c="c" e="e" n="n">{0}^{l}\right) ; I</em>}\left(\hat{s<em i="1">{T}^{l}\right)\right]\right)\right}</em>
$$}^{M</p>
<p>With these embeddings pre-computed, we can retrieve the most likely candidates using $k$-Nearest Neighbors [19] with cosine similarity between the vision-language embedding pairs $d\left(z_{l}^{n}, \hat{z}<em l="l">{s}^{m}\right)=\frac{z</em>}^{n} \cdot \hat{z<em l="l">{s}^{n}}{\left|z</em>}^{n} \cdot \hat{z<em C="C">{s}^{m}\right|}$ as the metric. We then use the cosine similarity to select a subset of candidate instructions to construct a new relabeled dataset $\mathcal{D}</em>}=$ $\left[\left(\hat{\tau<em 1="1">{1}, \hat{l}</em>}^{1}\right), \ldots,\left(\hat{\tau<em k="k">{1}, \hat{l}</em>}^{1}\right), \ldots,\left(\hat{\tau<em 1="1">{M}, \hat{l}</em>}^{M}\right), \ldots,\left(\hat{\tau<em k="k">{M}, \hat{l}</em>\right)\right]$. Figure 3 visualizes the three datasets generated. There are several potential strategies for candidate instruction selection:
a) Top-k selection: For each trajectory, we rank the candidate instructions in descending order based on their cosine similarity distances and output the top- $k$ instructions. The hyperparameter $k$ trades off precision and recall of the relabeled dataset. A smaller $k$ will return mostly relevant candidate instructions, while a larger $k$ value can recall a broader spectrum of potential hindsight descriptions for the episode at the expense of introducing erroneous instructions.}^{M</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: (a) A mobile manipulator robot receives RGB images from an onboard camera and uses a 7 DoF arm with parallel-jaw grippers. (b) Teleoperators receive instructions drawn from a set of 551 structured commands to perform a total of 80,000 demonstrations. 2,800 of these episodes are sent for crowdsourced language annotations. (c) A sample of scenes in the demonstration dataset which range across various countertops, drawers, and object arrangements in an office kitchen setting.</p>
<p><em>b) Min-p selection:</em> Instead of outputting a fixed number of candidate instructions per trajectory, we dynamically adjust this number based on a minimum probability <em>p</em> parameter, representing the minimum confidence for each instruction. We first convert the cosine similarity between the vision-language embedding pair to a probability that the <em>m</em>-th episode has language label <em>l<sup>n</sup></em> by taking the softmax over all the candidate instructions with temperature parameter α from CLIP:</p>
<p>$$P(\hat{l}^m = l^n | (\hat{s}<em>0^m, \hat{s}_T^m)) = \frac{\exp(d(z_i^n, \hat{z}</em><em>^m)/\alpha)}{\sum_{n'} \exp(d(z_i^n', \hat{z}_</em>^m)/\alpha) \tag{1}$$</p>
<p>We truncate the candidate instructions to the set <em>L<sup>(p)</sup></em> ⊂ <em>L</em> such that each instruction has a minimum hurdle probability <em>p</em> &gt; 0:</p>
<p>$$P(\hat{l}^m = l) \ge p, \quad \forall \, l \in L^{(p)} \tag{2}$$</p>
<p>Given <em>p</em>, the <em>maximum</em> number of candidates that can be output for a trajectory is <em>k</em> = 1/<em>p</em>. The <em>minimum</em> number of candidates, meanwhile, can be zero, if there are no candidate instructions satisfying this hurdle probability.</p>
<p>We will investigate in Section V-C the effects of these candidate instruction selection strategies on relabeled instruction accuracy, augmented dataset size, and downstream policy performance.</p>
<h3><em>C. Language Conditioned Policies with Behaviour Cloning</em></h3>
<p>Given a dataset <em>D</em> = [<em>D<sub>A</sub></em>, <em>D<sub>C</sub></em>] of robot trajectories and corresponding augmented language instructions, we can train a language-conditioned control policy with Behavior Cloning (BC). While instruction augmented offline datasets can be used by any downstream language-conditioned policy learning method such as offline RL or BC, we limit our work to the conceptually simpler BC in order to focus our analysis on the importance of instruction augmentation.</p>
<h3>IV. EXPERIMENTAL SETUP</h3>
<p>We first describe the setup for our experimental validation, including the environments, the configuration of the robot, and the datasets that we use in our experiments. Since our aim is to study the benefits of our proposed instruction augmentation approach, we describe other alternative methods for augmenting the dataset for comparison. Finally, we detail our evaluation protocol, which involves testing the degree to which policies learned with different types of instruction augmentation generalize to <em>novel previously unseen</em> instructions.</p>
<h4><em>A. Environment, Robot, and Datasets</em></h4>
<p>We implement DIAL in a challenging real-world robotic manipulation setting based on the kitchen environments described by Ahn et al. [1]. We focus on the practically-motivated setting where a dataset of teleoperated demonstrations is available, collected for downstream imitation learning [1, 24]. A mobile manipulator robot with a parallel-jaw gripper, an over-theshoulder RGB camera, and a 7 DoF arm is placed in an office kitchen to interact with common objects using concurrent [46] continuous closed-loop control from pixels. We collect a large-scale dataset of over 80,000 robot trajectories via human teleoperation (<em>D<sub>B</sub></em>), where teleoperators receive 551 structured commands motivated by common manipulation skills and objects in a kitchen environment, following prior work [1]. Afterwards, we leverage crowd-sourced human annotators to</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Instruction Samples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Spatial</td>
<td>[‘knock down the right soda’, ‘raise the left most can’, ‘raise bottle which is to the left of the can’]</td>
</tr>
<tr>
<td>Rephrased</td>
<td>[‘pick up the apple fruit’, ‘liftt the fruit’ [sic], ‘lift the yellow rectangle’]</td>
</tr>
<tr>
<td>Semantic</td>
<td>[‘move the lonely object to the others’, ‘push blue chip bag to the left side of the table’, ‘move the green bag away from the others’]</td>
</tr>
</tbody>
</table>
<p>label 2,800 robot trajectories with two hindsight instructions each, resulting in a total of 5,600 unique episodes with crowdsourced captions ( $\mathcal{D}_{A}$ ). Human annotators are shown the first and last frame of the episode and asked to provide a free-form text description describing how a robot should be commanded to go from the start to the end. Visualizations of the mobile manipulator, dataset collection procedure, and example scenes are shown in Figure 4 and further detailed in Appendix B.</p>
<h2>B. Instruction Augmentation</h2>
<p>We consider various methods of instruction augmentation which each result in different relabeled datasets that are then used for downstream policy learning.
a) DIAL implementations: We implement DIAL with a CLIP model that is fine-tuned on $\mathcal{D}<em A="A">{A}$ with the procedure described in Section III-A. After fine-tuning CLIP, we source 18,719 candidate instruction labels $(L)$ from the combination of $\mathcal{D}</em>$ of 80,000 robot trajectories that do not contain crowd-sourced annotations with $L$, we follow Section III-B to implement two variations of DIAL: Top- $k$ selection and Min- $p$ selection.}$ and a corpus of GPT-3 proposals of potential language instructions. To perform instruction augmentation that relabels dataset $\mathcal{D}_{B</p>
<p>The version of DIAL with Top- $k$ selection applies a fixed number $k$ instruction augmentations for every episode in the source dataset based on cosine similarity distances. By changing $k$, we produce three instruction augmented datasets: 80,000 relabeled demonstrations $(k=1)$, 240,000 relabeled demonstrations $(k=3)$, and 800,000 relabeled demonstrations $(k=10)$. The version of DIAL with Min- $p$ selection is more conservative and only performs instruction augmentation when confidence from CLIP is above some threshold $p$. By changing $p$, we produce three instruction augmented datasets: 128,422 relabeled demonstrations $(p=0.1)$, 38,516 relabeled demonstrations $(p=0.2)$, and 17,013 relabeled demonstrations $(p=0.3)$. Additional details can be found in Appendix C.
b) Non-visual instruction augmentation methods: We consider three instruction augmentation methods that do not utilize any visual information. First, we implement a "Gaussian Noise" baseline that adds random noise to existing crowdsourced instructions’ language embeddings. Second, we design a "Word-level Synonyms" baseline that replaces individual words in existing instructions with sampled synonyms from a predefined list. Finally, we introduce a "LLM-proposed Instructions" baseline that replaces entire instructions with alternative instructions as proposed by GPT-3. Implementation details for these baselines can be found in Appendix E.</p>
<h2>C. Policy Training</h2>
<p>Using these various instruction augmented datasets, we train vision-based language-conditioned behavioral cloning policies with the RT-1 architecture [7]. One main difference from RT-1 is that instead of utilizing USE [10] as the task representation for language conditioning, we instead use the language encoder of the fine-tuned CLIP model that was used for instruction</p>
<p>TABLE I: Samples from the 60 novel evaluation instructions we consider. 34 Spatial tasks focus on instructions involving reasoning about spatial relationships, such as specifying an object’s initial position relative to other objects in the scene. 16 Rephrased tasks are linguistic re-phrasings of the original 551 foresight tasks, such as referring to sodas and chips by their colors instead of their brand name. 10 Semantic tasks describe skills not contained in the original dataset, such as moving objects away from all other objects, since the original dataset only contains trajectories of moving objects towards other objects. A full list is provided in Table VII.
augmentation in Section III-B; full details are described further in Appendix G. Nonetheless, we treat the behavioral cloning policy as an independent component of our method and focus on studying instruction augmentation methods; we do not explore different policy architectures or losses in this work.</p>
<h2>D. Evaluation</h2>
<p>In contrast to many prior works [3, 6] on instruction following, we focus our evaluation only on novel instructions unseen during training. To source these novel instructions, we crowd-source instructions and prompt GPT-3 for evaluation task suggestions, and then filter out any instructions already contained in either the crowd-sourced language instructions in $\mathcal{D}<em B="B">{A}$, the original set of 551 structured teleoperator commands $\mathcal{D}</em>$; in total, we sample 60 novel evaluation instructions. We organize these evaluation instructions into three categories to allow for more detailed analysis of qualitative policy performance; examples are shown in Table I and a full list is provided in Table VII.}$, or the instruction augmentated dataset $\mathcal{D}_{C</p>
<h2>V. EXPERIMENTAL RESULTS</h2>
<p>In our experiments, we investigate whether DIAL can improve the policy performance on the unseen tasks described in Section IV-D when starting from fully or partially labeled source datasets. We ablate on the types of instruction augmentations described in Section IV-B, and analyze the importance of accuracy when augmenting instructions with DIAL.</p>
<h2>A. Does DIAL improve performance on unseen tasks?</h2>
<p>We investigate whether DIAL can enable languageconditioned behavior cloning policies to successfully perform</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5: Given the same starting scene, DIAL follows the instructions of (a) pick can which is on the right of the table, (b) pick the can in the middle, and (c) pick can which is on the left of the table. Non-DIAL methods do not adjust their behaviors based on language commands, demonstrating a lack of spatial understanding.</p>
<p>Novel instructions. For training various control policies, we consider three training datasets: $\mathcal{D}_A$ contains 5,600 episodes which contain crowd-sourced hindsight language instructions, $\mathcal{D}_B$ contains 80,000 episodes which contain structured commands given to teleoperators, and $\mathcal{D}_C$ contains 38,516 episodes with instructions predicted by DIAL with Min- $p = 0.2$ starting from $\mathcal{D}_A$ and $\mathcal{D}_B$. We refer to training on only $\mathcal{D}_A$ as the Interactive Language (IL) [32] setting, training on only $\mathcal{D}_B$ as the RT-1 [7] setting, and training on both $\mathcal{D}_A$ and $\mathcal{D}_B$ as the RT-1 + IL setting. Then, we refer training on $\mathcal{D}_C$ (either with or without $\mathcal{D}_A$ and $\mathcal{D}_B$) as DIAL. This experiment is practically motivated by the setting where large amounts of unstructured trajectory data are available but hindsight labels are expensive to collect, such as robot play data [14, 31, 32].</p>
<p>After policy training, we evaluate on task instructions not contained in $\mathcal{D}_A$, $\mathcal{D}_B$, or $\mathcal{D}_C$. Table II demonstrates that DIAL is able to solve over 40% more challenging novel tasks across the three evaluation categories compared to either RT-1 and/or IL, which do not use the instruction augmented data $\mathcal{D}_C$.</p>
<p>An example is shown in Figure 5, where DIAL successfully understands the spatial concepts of "left", "middle", and "right". Such spatial concepts are especially important to identify object instances in scenes with duplicate objects: while baseline methods ignore the language instruction and instead repeat the same motions or randomly select a target object, DIAL is able to consistently target the correct objects. In addition to <em>Spatial</em> tasks, DIAL is also able to outperform baseline policies at <em>Semantic</em> tasks that focus on semantic skills not contained in the original foresight instructions. We show more examples of evaluation successes in Figure 8.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Dataset Properties</th>
<th></th>
<th>Evaluation on Novel Instructions</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$\mathcal{D}_A$</td>
<td>$\mathcal{D}_B$</td>
<td>$\mathcal{D}_C$</td>
<td>Spatial</td>
</tr>
<tr>
<td>IL [32]</td>
<td>✓</td>
<td></td>
<td></td>
<td>30.0%</td>
</tr>
<tr>
<td>RT-1 [7]</td>
<td></td>
<td>✓</td>
<td></td>
<td>38.0%</td>
</tr>
<tr>
<td>RT-1 + IL</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td>46.0%</td>
</tr>
<tr>
<td>DIAL (ours)</td>
<td>✓</td>
<td></td>
<td>✓</td>
<td>58.0%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>✓</td>
<td>✓</td>
<td>50.0%</td>
</tr>
<tr>
<td></td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>68.0%</td>
</tr>
</tbody>
</table>
<p>TABLE II: Comparing the performance of language-conditioned policies trained on different types of labeled datasets. $\mathcal{D}_A$ contains 5,600 episodes with crowd-sourced language instructions and is representative of the Interactive Language (IL) [32] setting. $\mathcal{D}_B$ contains 80,000 episodes with structured teleoperator commands and is representative of the RT-1 [7] setting. DIAL additionally creates an augmented $\mathcal{D}_C$ with 38,516 relabeled instructions. We find that DIAL is able to significantly performance on novel evaluation instructions, especially in the IL setting where $\mathcal{D}_B$ is not available.</p>
<h3>B. How does DIAL compare to other instruction augmentation methods?</h3>
<p>We compare DIAL to non-visual instruction augmentation strategies outlined in Section IV-B. For this comparison, we apply the baseline instruction augmentation methods on both episodes with crowd-sourced annotations ($\mathcal{D}_A$) and on episodees with structured teleoperator commands ($\mathcal{D}_B$) to produce different instruction aug-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6: Comparing the task diversity of structured commands provided to teleoperators and DIAL instruction predictions. The visualization shows a t-SNE for 30,000 trajectories with their first and last frames embedded via a fine-tuned CLIP model. On the left, embeddings are colored based on the skill categories of structured teleoperator commands. On the right, embeddings are colored based on particular keywords that are present in the DIAL predicted instructions. While large clusters of episodes may all correspond to the same teleoperator command, DIAL predictions may highlight more nuanced semantic concepts.</p>
<table>
<thead>
<tr>
<th>Instruction Augmentation</th>
<th>Evaluation on Novel Instructions</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Spatial Tasks</td>
<td>Rephrased Tasks</td>
<td>Semantic Tasks</td>
<td>Overall</td>
</tr>
<tr>
<td>None</td>
<td>46.0%</td>
<td>60.0%</td>
<td>15.4%</td>
<td>42.5%</td>
</tr>
<tr>
<td>Gaussian Noise</td>
<td>36.0%</td>
<td>40.0%</td>
<td>23.1%</td>
<td>33.8%</td>
</tr>
<tr>
<td>Word-level Synonyms</td>
<td>28.0%</td>
<td>46.7%</td>
<td>7.7%</td>
<td>27.5%</td>
</tr>
<tr>
<td>LLM-proposed Instructions</td>
<td>28.0%</td>
<td>46.7%</td>
<td>23.1%</td>
<td>30.0%</td>
</tr>
<tr>
<td>DIAL (ours)</td>
<td>68.0%</td>
<td>66.7%</td>
<td>30.8%</td>
<td>60.0%</td>
</tr>
</tbody>
</table>
<p>TABLE III: Evaluating language-conditioned BC policies trained on datasets with different types of instruction augmentation. Each policy performs 80 evaluations over 60 novel task instructions. DIAL is consistently most performant, especially on <em>Spatial</em> Tasks requiring visual scene understanding.</p>
<p>mented datasets (D^{Gaussian}<em C="C">{C}, D^{Synonyms}</em>}, D^{LLM<em C="C">{C}, D^{DIAL}</em> being each of their respective instruction augmentated datasets. Table III indicates that DIAL significantly outperforms other baseline instruction augmentation methods. For both the }). Afterwards, we train separate language-conditioned policies policies: the "None" model trains on {D_{A}, D_{B}}, while the remaining models train on {D_{A}, D_{B}, D_{C}} with D_{C<em>Spatial</em> Tasks and <em>Rephrased</em> tasks, we observe that these baseline instruction augmentation methods without visual grounding resulted in worse performance compared to the no instruction augmentation.</p>
<h3><em>C. How sensitive is DIAL to hyperparameters and instruction prediction accuracy?</em></h3>
<p>We study the trade-off between increasing the amount of instruction augmentation and potentially relabeling with</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 7: The factual accuracy of the top 20 instruction augmentation predictions of 50 sampled episodes relabeled by a fine-tuned CLIP model. For k ∈ [1, 20], we measure the accuracy of the k-th instruction, the cumulative accuracy of the top k instructions, and CLIP's confidence score for the k-th instruction. While top instructions are often accurate, they become increasingly inaccurate along with CLIP's confidence.</p>
<p>incorrect or irrelevant instructions. By varying the hyperparameters of Top-k prediction and Min-p prediction, the two instruction prediction variations of DIAL discussed in Section IV-B, we can indirectly influence the size of the potential label inaccuracy of the relabeled datasets. To measure how instruction augmentation accuracy changes as we increase k,</p>
<table>
<thead>
<tr>
<th>DIAL Version</th>
<th>Dataset Properties</th>
<th></th>
<th>Evaluation on Novel Instructions</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Prediction Method</td>
<td>Relabeled Episodes</td>
<td>Relabeled Accuracy</td>
<td>Spatial Tasks</td>
<td>Rephrased Tasks</td>
<td>Semantic Tasks</td>
<td>Overall</td>
</tr>
<tr>
<td>Top-$k$, $k=1$</td>
<td>80, 000</td>
<td>68.0%</td>
<td>62.0%</td>
<td>40.0%</td>
<td>23.1%</td>
<td>50.0%</td>
</tr>
<tr>
<td>Top-$k$, $k=3$</td>
<td>240, 000</td>
<td>65.3%</td>
<td>62.0%</td>
<td>40.0%</td>
<td>15.4%</td>
<td>48.8%</td>
</tr>
<tr>
<td>Top-$k$, $k=10$</td>
<td>800, 000</td>
<td>57.0%</td>
<td>37.5%</td>
<td>50.0%</td>
<td>20.0%</td>
<td>35.0%</td>
</tr>
<tr>
<td>Min-$p$, $p=0.10$</td>
<td>128, 422</td>
<td>61.9%</td>
<td>44.0%</td>
<td>46.7%</td>
<td>23.1%</td>
<td>40.0%</td>
</tr>
<tr>
<td>Min-$p$, $p=0.20$</td>
<td>38, 516</td>
<td>68.8%</td>
<td>$\mathbf{68.0\%}$</td>
<td>$\mathbf{66.7\%}$</td>
<td>$\mathbf{30.8\%}$</td>
<td>$\mathbf{60.0\%}$</td>
</tr>
<tr>
<td>Min-$p$, $p=0.30$</td>
<td>17, 013</td>
<td>76.0%</td>
<td>62.0%</td>
<td>53.3%</td>
<td>46.2%</td>
<td>56.3%</td>
</tr>
</tbody>
</table>
<p>TABLE IV: Comparing DIAL with Top- $k$ prediction against DIAL with Min- $p$ prediction. By increasing $k$ or decreasing $p$, augmented datasets become larger but increasingly inaccurate. We provide analysis of the relationship between instruction accuracy and CLIP confidence in Figure 7 and Section V-C.
we ask human labelers to rate whether proposed instruction augmentation are factually accurate descriptions of a given episode. We show an example of the top 10 predicted instruction augmentations in an episode in Figure 12.</p>
<p>In Figure 7, we sample 50 episodes and ask human labelers to assess the predicted instruction accuracy as we increase the number of predictions produced by CLIP. While the initial predictions are often correct, the later predictions are often factually inaccurate. The top-20th instruction prediction is only factually accurate $20.0\%$ of the time.</p>
<p>When applying these different relabeled datasets to downstream policy learning, we find in Table IV that Min- $p$ instruction prediction, a more conservative approach than Top- $k$ prediction, performs the best across all evaluation instructions. We also find that finetuning CLIP is quite important for instruction augmentation, which is detailed in Appendix A2.</p>
<h2>VI. Conclusion, Limitations, and Future Work</h2>
<p>In this work, we introduced DIAL, a method that uses VLMs to label offline datasets for language-conditioned policy learning. Scaling DIAL to a real world robotic manipulation domain, we perform a large-scale study of over 1,300 evaluations and find that DIAL is able to outperform baselines by 40% on a challenging set of 60 novel evaluation instructions unseen during training. We compare DIAL against instruction augmentation methods that do not consider visual context, and also ablate the source datasets we use for instruction augmentation. Finally, we study the interplay between larger augmented datasets and lowered instruction accuracy; we find that control policies are able to utilize relabeled demonstrations even when some labels are inaccurate, suggesting that DIAL is able to provide a cheap and automated option to extract additional semantic knowledge from offline control datasets.</p>
<p>Limitations and Future Work: Although DIAL seems to improve policy understanding on many novel concepts not contained in the original training dataset, it sometimes fails, especially when evaluating tasks that may require new motor skills. In addition, since both our crowd-source annotators and VLMs only have access to the first and final states of an episode,
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 8: DIAL successfully completes various novel evaluation instructions requiring understanding of concepts such as relative spatial references, colors, and alternative phrasings. These concepts were not present in the structured teleoperator commands used for the original training demonstrations.
they do not capture skills involving temporal coherence nor is aware of how these instructions are accomplished. A natural next step is to apply DIAL to full video episodes. Another interesting direction is to view DIAL as goal-conditioning and attempting visual goals during training or evaluation. Moreover, on-policy or RL variations of DIAL may be able to effectively explore the task representation space autonomously.</p>
<h2>ACKNOWLEDGEMENT</h2>
<p>The authors would like to thank Kanishka Rao, Debidatta Dwibedi, Pete Florence, Yevgen Chebotar, Fei Xia, and Corey Lynch for valuable feedback and discussions. We would also like to thank Emily Perez, Dee M, Clayton Tan, Jaspiar Singh, Jornell Quiambao, and Noah Brown for navigating the ever-changing challenges of data collection and robot policy evaluation at scale. Additionally, Tom Small designed informative animations to visualize DIAL. Finally, we would like to thank the large team that built [8] and [1], upon which we develop DIAL.</p>
<h2>REFERENCES</h2>
<p>[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id=bdHkMjBJG_w.
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.
[3] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36743683, 2018.
[4] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017.
[5] Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Arian Hosseini, Pushmeet Kohli, and Edward Grefenstette. Learning to understand goal specifications by modelling reward. arXiv preprint arXiv:1806.01946, 2018.
[6] Valts Blukis, Yannick Terme, Eyvind Niklasson, Ross A Knepper, and Yoav Artzi. Learning to map natural language instructions to physical quadcopter control using simulated flight. arXiv preprint arXiv:1910.09664, 2019.
[7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana</p>
<p>Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale. In arXiv preprint arXiv:2212.06817, 2022.
[8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.
[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877-1901, 2020.
[10] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. Universal sentence encoder. arXiv preprint arXiv:1803.11175, 2018.
[11] Harris Chan, Yuhuai Wu, Jamie Kiros, Sanja Fidler, and Jimmy Ba. Actrce: Augmenting experience via teacher's advice for multi-goal reinforcement learning. arXiv preprint arXiv:1902.04546, 2019.
[12] Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, et al. Actionable models: Unsupervised offline reinforcement learning of robotic skills. arXiv preprint arXiv:2104.07749, 2021.
[13] Geoffrey Cideron, Mathieu Seurin, Florian Strub, and Olivier Pietquin. Higher: Improving instruction following with hindsight generation for experience replay. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI), pages 225-232. IEEE, 2020.
[14] Zichen Jeff Cui, Yibin Wang, Nur Muhammad, Lerrel Pinto, et al. From play to policy: Conditional behavior generation from uncurated robot data. arXiv preprint arXiv:2210.10047, 2022.
[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv</p>
<p>preprint arXiv:2010.11929, 2020.
[17] Felix Duvallet, Thomas Kollar, and Anthony Stentz. Imitation learning for natural language direction following through unknown environments. In 2013 IEEE International Conference on Robotics and Automation, pages 1047-1053. IEEE, 2013.
[18] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. arXiv preprint arXiv:2206.08853, 2022.
[19] Evelyn Fix and Joseph Lawson Hodges. Discriminatory analysis. nonparametric discrimination: Consistency properties. International Statistical Review/Revue Internationale de Statistique, 57(3):238-247, 1989.
[20] Justin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. From language to goals: Inverse reinforcement learning for vision-based instruction following. arXiv preprint arXiv:1902.07742, 2019.
[21] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1899519012, 2022.
[22] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022.
[23] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.
[24] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pages 991-1002. PMLR, 2022.
[25] Yiding Jiang, Shixiang Shane Gu, Kevin P Murphy, and Chelsea Finn. Language as an abstraction for hierarchical deep reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019.
[26] Leslie Pack Kaelbling. Learning to achieve goals. In IJCAI, volume 2, pages 1094-8. Citeseer, 1993.
[27] Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv preprint arXiv:2104.08212, 2021.
[28] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022.
[29] Jelena Luketina, Nantas Nardelli, Gregory Farquhar,</p>
<p>Jakob Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, and Tim Rocktäschel. A survey of reinforcement learning informed by natural language. arXiv preprint arXiv:1906.03926, 2019.
[30] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data. arXiv preprint arXiv:2005.07648, 2020.
[31] Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet. Learning latent plans from play. In Conference on robot learning, pages 1113-1132. PMLR, 2020.
[32] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. arXiv preprint arXiv:2210.06407, 2022.
[33] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Martín-Martín. What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv:2108.03298, 2021.
[34] Oier Mees, Jessica Borja-Diaz, and Wolfram Burgard. Grounding language with visual affordances over unstructured data. arXiv preprint arXiv:2210.01911, 2022.
[35] Oier Mees, Lukas Hermann, and Wolfram Burgard. What matters in language conditioned robotic imitation learning. arXiv preprint arXiv:2204.06252, 2022.
[36] Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, et al. Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. In Conference on Robot Learning, pages 1303-1315. PMLR, 2022.
[37] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022.
[38] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018.
[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021.
[40] Frank Röder, Manfred Eppe, and Stefan Wermter. Grounding hindsight instructions in multi-goal reinforcement learning for robotics. arXiv preprint arXiv:2204.04308, 2022.
[41] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.</p>
<p>[42] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, pages 894-906. PMLR, 2022.
[43] Andrew Silva, Nina Moorman, William Silva, Zulfiqar Zaidi, Nakul Gopalan, and Matthew Gombolay. Lanconlearn: Learning with language to enable generalization in multi-task manipulation. IEEE Robotics and Automation Letters, 7(2):1635-1642, 2021.
[44] Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, and Heni Ben Amor. Languageconditioned imitation learning for robot manipulation tasks. Advances in Neural Information Processing Systems, 33:13139-13150, 2020.
[45] Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R Walter, Ashis Gopal Banerjee, Seth Teller, and Nicholas Roy. Approaching the symbol grounding problem with probabilistic graphical models. AI magazine, 32(4):64-76, 2011.
[46] Ted Xiao, Eric Jang, Dmitry Kalashnikov, Sergey Levine, Julian Ibarz, Karol Hausman, and Alexander Herzog. Thinking while moving: Deep reinforcement learning with concurrent control. arXiv preprint arXiv:2004.06089, 2020.
[47] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, et al. Transporter networks: Rearranging the visual world for robotic manipulation. In Conference on Robot Learning, pages 726-747. PMLR, 2021.</p>
<h2>APPENDIX</h2>
<h2>A. Additional Experiments</h2>
<p>We present failure examples, study the importance of both pre-training and fine-tuning, and also explore whether VLMs used for DIAL could also be utilized as a task representation.</p>
<p>1) Failure Examples: In addition to the successful example trajectories visualized in Figure 5 and Figure 8, we also show some examples of failure cases in Figure 9.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 9: Samples of evaluation failures for DIAL. Errors are due to a combination of motor control and task confusion.
2) How important is VLM fine-tuning for DIAL?: Whereas Section V-C studies different prediction mechanisms for a fine-tuned CLIP model (FT-CLIP), we are also interested in comparing different CLIP models altogether. The main FTCLIP model used in DIAL is initialized from the pretrained OpenAI CLIP weights and then fine-tuned on $\mathcal{D}_{A}$ from Section III-A, which begs the question: are both a strong pretrained initialization and subsequent fine-tuning necessary for strong instruction labeling performance? To answer this question, we perform instruction augmentation with (1) the frozen pretrained OpenAI CLIP model and (2) a fine-tuned CLIP model that starts from a random weight initialization instead of from the OpenAI pretrained weights.</p>
<p>As we see in Figure 10, predicted instruction accuracy for both of these models is significantly lower than the main FT-CLIP model. The poor performance of the frozen pretrained CLIP model (1) suggests that the particular embodied captioning task required by DIAL is likely quite out of distribution for the pre-training used by the OpenAI CLIP model, so some amount of domain data is required. On the other hand, the poor performance of training CLIP from scratch on robot demonstration data (2) suggests that internet-scale
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 10: Estimated top- $K$ cumulative instruction labeling accuracy using CLIP variants. The fine-tuned CLIP model from OpenAI checkpoint [39] performed significantly better than frozen CLIP model and the fine-tuned model from random initial weights.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 11: CLIP architecture for fine-tuning using the contrastive loss. The image embeddings of the initial and final observation is concatenated and passed through an MLP to produce an episode embedding to dot product with the text embedding.
pre-training is required to start from a reasonable prior; there may not be sufficient robot domain data in the datasets we consider to fully train CLIP from scratch.
3) Is a VLM good at relabeling also a good task representation?: We study whether a VLM fine-tuned for instruction augmentation can also act as a better task representation for conditioning a policy in the form of a more powerful</p>
<p>language embedding. Across the various groundtruth and relabeled datasets we focus on, we find that fine-tuned CLIP (FT-CLIP) is the most effective task representation, as seen in Table V. FT-CLIP is a good representation not only for freeform language instructions like those contained in the fine-tuning dataset $\mathcal{D}<em B="B">{A}$, but also for structured metadata labels used to collect the demonstrations in $\mathcal{D}</em>$. Thus, we utilize FT-CLIP’s language encoder as the main task representation for language conditioning in all control policies we train, besides for policies that explicitly denote otherwise, such as in Table V.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Task Encoder</th>
<th>Evaluation on Novel Instructions</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Spatial</td>
<td>Rephrased</td>
<td>Semantic Overall</td>
</tr>
<tr>
<td>$\mathcal{D}_{A}$</td>
<td>USE</td>
<td>$22.5\%$</td>
<td>$50.0\%$</td>
<td>$0.0\%$</td>
</tr>
<tr>
<td>$\mathcal{D}_{A}$</td>
<td>FT-CLIP</td>
<td>$30.0\%$</td>
<td>$40.0\%$</td>
<td>$7.7\%$</td>
</tr>
<tr>
<td>$\mathcal{D}<em B="B">{A}, \mathcal{D}</em>$</td>
<td>CLIP</td>
<td>$45.0 \%$</td>
<td>$40.0 \%$</td>
<td>$10.0 \%$</td>
</tr>
<tr>
<td>$\mathcal{D}<em B="B">{A}, \mathcal{D}</em>$</td>
<td>FT-CLIP</td>
<td>$46.0 \%$</td>
<td>$60.0 \%$</td>
<td>$15.4 \%$</td>
</tr>
<tr>
<td>DIAL, $k=1$</td>
<td>USE</td>
<td>$50.0 \%$</td>
<td>$50.0 \%$</td>
<td>$20.0 \%$</td>
</tr>
<tr>
<td>DIAL, $k=1$</td>
<td>FT-CLIP</td>
<td>$62.0 \%$</td>
<td>$40.0 \%$</td>
<td>$23.1 \%$</td>
</tr>
</tbody>
</table>
<p>TABLE V: Comparing downstream policy performance when improving the task representation from USE [10] to Pretrained OpenAI CLIP (CLIP) [39] to fine-tuned CLIP (FT-CLIP), as described in Section III-A. We find that the FT-CLIP representation is the best task representation in all dataset settings: training on crowd-sourced language annotations $\left(\mathcal{D}<em A="A">{A}\right)$, training on structured teleoperator commands along with crowdsourced language annotations $\left{\mathcal{D}</em>}, \mathcal{D<em A="A">{B}\right}$, and using DIAL $\left{\mathcal{D}</em>}, \mathcal{D<em C="C">{B}, \mathcal{D}</em>\right}$ with Top- $k$ with $k=1$ (DIAL, $k=1$ ).</p>
<h1>B. Dataset Details</h1>
<p>Following the procedure in [7], we collect a large dataset of robot trajectories via teleoperation by uniformly sampling from one of the structured commands shown in Table VI and sending those commands to teleoperators that operate a mobile manipulator robot in the real world. After teleoperators discard unsafe or failed demonstrations, we save a dataset of 80,000 successful robot demonstration trajectories $\left(\mathcal{D}<em B="B">{B}\right)$. Then, we send 2,800 of the episodes from $\mathcal{D}</em>$. Finally, we produce various relabeled datasets via instruction augmentation, that we detail in Section IV-B for different prediction methods for DIAL and Section V-B for different non-visual instruction augmentation baselines.}$ to be labeled by a pool of human labelers, who see the first and last frame of the episode and are tasked with providing a natural language description of how a robot could be commanded to from the first frame to the last frame. We request two independent language instruction annotations for each episode, so we obtain a total of 5,600 episode-instruction pairs, which we save as $\mathcal{D}_{A</p>
<h2>C. DIAL Implementation Details</h2>
<p>We implement DIAL with a CLIP model that is fine-tuned on 5,600 annotated episodes $\left(\mathcal{D}<em c="c" e="e" n="n">{A}\right)$ with the procedure described in Section III-A. The architecture of the CLIP model is shown in Figure 11. The initial and final state observation images are embedded using the CLIP image encoder $I</em>}$, and the resulting embeddings are concatenated and passed through a 200 hidden dimension single-layer MLP to produce the final episode embedding. We use the CLIP text encoder $T_{e n c}$ to embed the crowd-sourced annotations to produce the corresponding text embeddings. To fine-tune, we loaded the CLIP encoder weights from the ViT-B/32 OpenAI checkpoint. We use a batch size of 64 and train for 100,000 iterations, but select the best checkpoint based on text prediction accuracy on a randomly held-out test set of $10 \%$ of the training dataset $\mathcal{D<em c="c" e="e" n="n">{A}$. We fine-tune both encoders $I</em>$ as well as the fusion MLP.}$ and $T_{e n c</p>
<p>After fine-tuning CLIP, we source 18,719 candidate instruction labels $(L)$ from $\mathcal{D}_{A}$ and a corpus of GPT-3 proposals of potential language instructions. The GPT-3 proposals are generated by using the prompt shown in Listing 1 to iterate over the 551 instructions used to collect teleoperated demonstrations. We note that that Listing 1 generates diverse instructions that may not be accurate for a given episode. Listing 1 is purposefully tuned to produce "hallucinated" descriptions that can add semantic properties in the proposed instructions that may or not be correct (for example, "pick up the orange" might be augmented into "retrieve the orange from the sink" or "raise the orange next to the vase"). The motivation behind this design decision is that GPT-3 predictions can be a lot less conservative when being used downstream by DIAL, since the CLIP model will ideally filter out irrelevant instructions. In contrast, the prompt in Listing 2 is used for producing Sentence-Level Synonyms, which should ideally always be factually equivalent to the original instruction.</p>
<p>Next, to relabel 80,000 robot trajectories that do not contain crowd-sourced annotations $\left(\mathcal{D}_{B}\right)$ with $L$, we follow Section III-B to implement two variations of DIAL: Top- $k$ selection and Min- $p$ selection. For these two variations, we use $k={1,3,10}$ and $p={0.1,0.2,0.3}$.</p>
<h2>D. DIAL Hyperparameters</h2>
<p>In Section V-C, we examined DIAL's sensitivity to hyperparameters, including top-K and min-P sampling, which influence dataset diversity and instruction accuracy (label noise). Such tradeoffs are common in data augmentation, where the benefits of increased augmentation must be balanced with the risks of too much label noise. These hyperparameter choices are highly domain-specific, and in our case we optimize this balance by selecting hyperparameters based on measuring offline VLM prediction accuracy. Notably, hyperparameter tuning in DIAL is substantially more cost-effective than manual full dataset labeling by humans, enabling scalable policy learning improvement from generated instructions. We plan to include further discussion on hyperparameter selection and future work on studying these tradeoffs in more detail.</p>
<h2>E. Instruction Augmentation Baselines</h2>
<p>a) Gaussian Noise: Given an instruction $l$, we add Gaussian noise to the language embedding produced by the</p>
<table>
<thead>
<tr>
<th>Structured Teleoperator Command Categories</th>
<th>Count</th>
<th>Description</th>
<th>Example Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pick Object</td>
<td>17</td>
<td>picking objects on a counter</td>
<td>pick water bottle</td>
</tr>
<tr>
<td>Move Object Near Object</td>
<td>342</td>
<td>moving an object near another</td>
<td>move pepsi can near rxbar blueberry</td>
</tr>
<tr>
<td>Place Object Upright</td>
<td>8</td>
<td>placing an elongated object vertically upright</td>
<td>place coke can upright</td>
</tr>
<tr>
<td>Knock Object Over</td>
<td>8</td>
<td>picking an object and laying it sideways on the counter</td>
<td>knock redbull can over</td>
</tr>
<tr>
<td>Open / Close Drawer</td>
<td>6</td>
<td>opening or closing a counter drawer</td>
<td>open the top drawer</td>
</tr>
<tr>
<td>Place Object into Receptacle</td>
<td>85</td>
<td>pick an object on the table and put it in a container or drawer</td>
<td>place brown chip bag into white bowl</td>
</tr>
<tr>
<td>Pick Object from Receptacle and Place on the Counter</td>
<td>85</td>
<td>pick an object out of a container or drawer and place it on the counter</td>
<td>pick green jalapeno chip bag from paper bowl and place on counter</td>
</tr>
<tr>
<td>Total</td>
<td>551</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>TABLE VI: Teleoperators receive an instruction sampled from a total of 551 unique structured commands covering 7 different manipulation skills. The 80,000 episodes in $\mathcal{D}_{B}$ each contain exactly one of these structured teleoperator commands.</p>
<table>
<thead>
<tr>
<th>First Frame</th>
<th>Last Frame</th>
</tr>
</thead>
<tbody>
<tr>
<td>Instruction Augmentation Prediction by CLIP</td>
<td></td>
</tr>
<tr>
<td>#1: pick up the green can and place it in the bowl which is at the left side of the table</td>
<td>0.2244</td>
</tr>
<tr>
<td>#2: lift green can from table and place it in white cup</td>
<td>0.1408</td>
</tr>
<tr>
<td>#3: pick up the green can which is close to the water bottle and place it in the bowl</td>
<td>0.1209</td>
</tr>
<tr>
<td>#4: place green can into the plastic white bowl</td>
<td>0.0699</td>
</tr>
<tr>
<td>#5: pick the green can from the bottom right of the table and place it into the white bowl</td>
<td>0.0664</td>
</tr>
<tr>
<td>#6: pick up the silver can and place it in the white bowl</td>
<td>0.0429</td>
</tr>
<tr>
<td>#7: bring the blue can and place it into white paper bowl</td>
<td>0.0417</td>
</tr>
<tr>
<td>#8: pick up the green can from the bottom left side of the table</td>
<td>0.0388</td>
</tr>
<tr>
<td>#9: pick up the green can from the bottom side of the table and drop it into bowl</td>
<td>0.0339</td>
</tr>
<tr>
<td>#10: pick up the red bull can and drop it in the white bowl</td>
<td>0.0243</td>
</tr>
</tbody>
</table>
<p>Fig. 12: The top 10 proposed instruction augmentations for a single episode with original structured teleoperator command place green can in white bowl. In some cases, the predicted captions provide additional semantic information such as describing the location of the can or the material of the bowl. As seen in Figure 7, the probability CLIP assigns to each the candidates quickly drops off past a few top predictions. Setting Min- $p=0.2$ would only take the first instruction augmentation prediction, while setting $k=3$ would take the top three predictions, including an incorrect prediction (#3).</p>
<p>CLIP text encoder $T_{e n c}$, directly obtaining the augmentation in the latent space $\tilde{z}_{l}$:</p>
<p>$$
\tilde{z}<em c="c" e="e" n="n">{l}=T</em>
$$}(l)+\epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma) \in \mathbb{R}^{d</p>
<p>In our implementation, we choose $\sigma=0.05$ and perform the Gaussian noise augmentation dynamically to the 512-dimension CLIP $T_{e n c}$ embedding resulting from passing in the original language instruction to the CLIP text encoder.
b) Word-level Synonyms: We replace individual words in existing instructions with sampled synonyms from a predefined list. The mapping between words present in the original structured 551 instructions and possible synonyms is shown in Listing 3.
c) Sentence-level Synonyms: We replace entire instructions with alternative instructions as proposed by GPT-3. We pre-compute valid sentence-level synonyms by using the prompt shown in Listing 1 to iterate over the 551 instructions used to collect teleoperated demonstrations.</p>
<h2>F. Augmented Dataset Details</h2>
<p>As noted in Section IV-B, DIAL uses CLIP to score predictions from 18,719 candidate text labels $L$, which are a union of 9,393 crowd-sourced instructions ( $L_{C C}$ ) from the</p>
<p>original $\mathcal{D}<em 3="3" G="G" P="P" T="T">{A}$ dataset as well as 9,336 instruction candidates ( $L</em>$, we would deduplicate that candidate from $L$ and consider the instruction to be human-sourced. To enhance diversity, future work could leverage improved VLMs and fine-tune on more diverse labels, including LLM-generated captions.}$ ) from GPT-3 curated with the prompt in Listing 1. During the instruction augmentation process, we find that CLIP selects 3,675 unique instructions from these 18,719 candidates $L$. Additionally, $86.3 \%$ of selected instructions were from $L_{C C}$ and $13.7 \%$ from $L_{G P T 3}$. These characteristics show that CLIP generally prefers human-like instructions while sometimes incorporating GPT-3 variations. We note that these prediction characteristics may result from the fact that our relabeling model CLIP was finetuned solely on crowd-sourced labels, and that the desired output distribution is unclear (is it a positive or negative property to prefer crowd-sourced annotations?). Furthermore, it's possible that the true distribution of accurate and desireable language descriptions was already well-covered by the human annotations; so if GPT-3 proposed an instruction already contained in the human-sourced $L_{C C</p>
<h2>G. Language-Conditioned Policy Training</h2>
<p>The policies used in this work are trained using the RT-1 [7] architecture on a large dataset of human-provided demonstrations. The policies receive natural language descriptions in the form of a 512-dimensional VLM embedding and a short history of images and outputs discrete action tokens which are then transformed to continuous action outputs. Apart from the experiments explicitly denoted as using USE [10] and frozen CLIP [39] language embeddings in Table V, all policies trained in this work utilize fine-tuned CLIP (FT-CLIP) language embeddings as described in Section III-A.</p>
<p>Note that the exact policy architecture is not the main focus of this work, so we utilize the exact same policy training procedure across each experiment and only vary the instruction augmented datasets that the policies are trained on.</p>
<h2>H. Evaluation Instructions</h2>
<p>We utilize an evaluation setup focusing solely on novel instructions unseen during training. To source these novel instructions, we 1) crowd-source instructions from a different set of humans than the original dataset labelers and 2) prompt GPT-3 with Listing 2 to produce reasonable tasks that might be asked of a home robot manipulating various objects on a kitchen counter. Then, we normalize all instructions by removing punctuation, removing non-alphanumeric symbols, converting all instructions to lower case, and removing leading and ending spaces. Afterwards, we filter out any instructions already contained in either the instruction augmentation process in Section III-B or in the original set of 551 foresight tasks in Table VI. Finally, as seen in Table VII, we organize them into various semantic categories to allow for more detailed analysis of quantitative policy performance.</p>
<p>Listing 1: GPT-3 Prompt for Proposing Candidate Tasks.
For the following tasks for a helpful home robot, rephrase them to imagine different variations of the task. These variations include different types of objects, different locations, different obstacles, and different strategies for how the task should be accomplished.</p>
<p>3 rephrases for: pick mountain dew
Answer: lift the mountain dew on the left side of the desk, grab the mountain dew soda next to the water, pick the farthest green soda can</p>
<p>4 rephrases for: move your arm to the right side of the desk Answer: bring your arm to the right of the counter, move right slightly, go far to the rightmost part of the table, reorient your gripper to point right</p>
<p>10 rephrases for: bring me the yogurt
Answer: retrieve the yogurt, bring the white snack, pick up the yogurt cup from the far right, lift the yogurt snack from the left, bring back the yogurt near the chip bag, lift the yogurt from the top of the counter, bring the yogurt closest to the apple, grab the yogurt, lift the close left yogurt on the bottom left, retrieve the yogurt on the bottom of the table</p>
<p>10 rephrases for: <INSTRUCTION_TO_AUGMENT>
Answer:</p>
<p>Listing 2: GPT-3 Prompt for "Sentence-level Synonyms".
You are a helpful home robot in an office kitchen. You are able to manipulate household objects in a safe and efficient manner. Here are some tasks you are able to accomplish in various environments:</p>
<p>5 tasks in a sink with a sponge, brush, plate, and a cup: move sponge near the cup, fill up the cup with water, clean the plate with the brush, pick up the plate, put the cup on the plate</p>
<p>3 tasks in a storage room with a box, a ladder, and a hammer: lift the hammer, push the ladder, put the hammer in the box</p>
<p>10 tasks on a table with an apple, a coke can, a sponge, and an orange: pick up the apple, pick up the coke can, use the sponge to clean the apple, use the sponge to clean the coke can, put the apple down, put the coke can down, pick up the orange, peel the orange, eat the orange, throw away the peel</p>
<p>10 tasks on a table with <OBJECT_1>, <OBJECT_2>, and <OBJECT_3>:</p>
<p>Listing 3: Synonym Mapping for "Word-level Synonyms".
SYNONYM_MAP = [
'rxbar blueberry': [
'rxbar blueberry', 'blueberry rxbar',
'the blueberry rxbar', 'the rxbar blueberry'
],
'rxbar chocolate': [
'rxbar chocolate', 'chocolate rxbar',
'the chocolate rxbar', 'the rxbar chocolate'
],
'pick': ['pick', 'pick up', 'raise', 'lift'],
'move': [
'move', 'push', 'move', 'displace', 'guide',
'manipulate', 'bring'
],
'knock': ['knock', 'push over', 'fiick', 'knockdown'],
'place': ['place', 'put', 'gently place', 'gently put'],
'open': ['open', 'widen', 'pull', 'widely open'],
'close': ['close', 'push close', 'completely close'],
'coke': [
'coke', 'coca cola', 'coke', 'coca cola',
'the coke', 'a coke', 'a coca cola', 'the coca cola'
],
'green': [
'green', 'bright green', 'grass colored', 'lime',
'a green', 'the green', 'a lime', 'the lime',
'the bright green', 'a bright green'
],
'blue ': ['blue ', 'dark blue ', 'the blue ', 'a blue '],
'pepsi': ['pepsi', 'blue pepsi', 'pepsi', 'a pepsi',
'the pepsi'],
'7up': ['7up', 'white 7up', '7up', '7-up', '7up',
'a 7up', 'the 7up'],
'redbull': [
'redbull', 'red bull', 'energy drink',
'redbull energy', 'redbull soda',
'the redbull', 'a redbull', 'a red bull',
'the red bull'
],
'blueberry': ['blueberry', 'blue berry'],
'chocolate': ['chocolate', 'brown chocolate'],
'brown': ['brown', 'coffee colored',
'the brown', 'a brown'],
'jalapeno': ['jalapeno', 'spicy', 'hot', 'fiery'],
'rice': ['rice'],
'chip': ['chip', 'snack', 'chips'],
'plastic': ['plastic'],
'water': ['water', 'water', 'agua'],
'bowl': ['bowl', 'half dome', 'chalice'],
'togo': ['togo', 'to-go', 'to go'],
'box': ['box', 'container', 'paper box'],
'upright': ['upright', 'right side up', 'correctly'],
'near': ['near', 'close to', 'nearby',
'very near', 'very close to'],
'can': ['can', 'soda can', 'aluminum can'],
'rxbar': ['rxbar', 'snack bar', 'granola bar',
'health bar', 'granola'],
'apple': [
'apple', 'red apple', 'the apple', 'the red apple',
'an apple', 'a red apple', 'small apple',
'the small apple'
],
'orange': [
'orange', 'the orange', 'orange fruit', 'an orange',
'a small orange', 'a large orange'
],
'sponge': [
'sponge', 'yellow sponge', 'the yellow sponge',
'a yellow sponge', 'a sponge', 'the sponge'
],
'bottle': ['bottle', 'plastic bottle',
'recycleable', 'clear'],
]</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Instruction Samples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Spatial</td>
<td style="text-align: center;">['grab the bottle on the left of the table', 'grab the can which is on the right side of the table', 'grab the chip on the left', 'grab the chip on the right', 'grab the right most apple', 'knock down the right soda', 'lift the apple which is on the left side of the table', 'lift the apple which is on the right side of the table', 'lift the chips on the left side', 'lift the chips on the right side', 'lift the left can', 'move the left soda to the can on the right side of the table', 'move the soda can which is on the right toward the chip bag', 'pick can which is on the left of the table', 'pick can which is on the right of the table', 'pick chip bag on the left', 'pick chip bag on the right', 'pick the can in the middle', 'pick the left coke can', 'pick the left fruit', 'pick the leftmost chip bag', 'pick the object on the right side of the table', 'pick the right coke can', 'pick the right object', 'pick the rightmost chip bag', 'pick up the left apple', 'pick up the left object', 'pick up the right can', 'pick up the right object', 'push the left side apple to the brown chips', 'raise bottle which is to the left of the can', 'raise the blue tin', 'raise the left most can', 'raise the thing which is on the left of the counter']</td>
</tr>
<tr>
<td style="text-align: center;">Rephrased</td>
<td style="text-align: center;">['grab and lift up the green bag', 'grab the blue pepsi', 'grab the white can', 'knock over the water', 'lift the orange soda', 'lift the yellow rectangle', 'liftt the fruit', 'move green packet near the red apple', 'move orange near to the chip bag', 'pick up the apple fruit', 'push green chips close to the coke', 'upright the lime green can', 'put the apple next to the candy bar', 'retrieve the can from the left side of the coffee table', 'set the apple down next to the chocolate bar', 'take the can from the left side of the counter']</td>
</tr>
<tr>
<td style="text-align: center;">Semantic</td>
<td style="text-align: center;">['move the can to the bottom of the table', 'move the green bag away from the others', 'move the lonely object to the others', 'move the right apple to the left of the counter', 'push blue chip bag to the left side of the table', 'push the can towards the left', 'push the can towards the right', 'push the left apple to the right side', 'use the sponge to clean the coke can', 'use the sponge to clean the apple']</td>
</tr>
</tbody>
</table>
<p>TABLE VII: Novel evaluation instructions sourced from humans or GPT-3, grouped by category. Spatial tasks focus on tasks involving Spatial relationships, Rephrased tasks contain tasks that directly map to a foresight skill, and Semantic tasks describe semantic concepts not contained in the relabeled or original datasets. In total, there are 60 instructions across the three categories.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Equal contribution</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
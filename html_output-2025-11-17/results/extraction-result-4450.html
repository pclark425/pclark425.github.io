<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4450 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4450</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4450</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-282057333</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.09901v1.pdf" target="_blank">Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics</a></p>
                <p><strong>Paper Abstract:</strong> Computing has long served as a cornerstone of scientific discovery. Recently, a paradigm shift has emerged with the rise of large language models (LLMs), introducing autonomous systems, referred to as agents, that accelerate discovery across varying levels of autonomy. These language agents provide a flexible and versatile framework that orchestrates interactions with human scientists, natural language, computer language and code, and physics. This paper presents our view and vision of LLM-based scientific agents and their growing role in transforming the scientific discovery lifecycle, from hypothesis discovery, experimental design and execution, to result analysis and refinement. We critically examine current methodologies, emphasizing key innovations, practical achievements, and outstanding limitations. Additionally, we identify open research challenges and outline promising directions for building more robust, generalizable, and adaptive scientific agents. Our analysis highlights the transformative potential of autonomous agents to accelerate scientific discovery across diverse domains.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4450.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4450.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Info-Theory Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Information-Theoretic Framework for Autonomous Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual evaluation framework that frames scientific discovery as information processing: measuring Information Entropy (uncertainty), Verifiability (testability) and Dissipation (cost of exploring non-solution paths) to assess agent progress and outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Information-Theoretic Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluates LLM-generated theories by quantifying (1) entropy reduction the agent achieves (how much the hypothesis/search space uncertainty is reduced), (2) verifiability of artifacts (the degree to which a hypothesis/description can be empirically or logically tested), and (3) dissipation (computational/experimental cost expended on unproductive searches). The method is conceptual: track transformations of information across stages (Human Intent → Natural Language → Computer Language → Physical Information) and score candidate theories by how much they reduce internal entropy while increasing verifiability per unit dissipation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Information Entropy reduction (uncertainty/hypothesis-space shrinkage), Verifiability (testability/statistical reproducibility), Dissipation (resource/costs of false paths), and derived trade-offs between them.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / cross-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>general (hypotheses, mechanistic explanations, predictive conjectures)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Conceptual proposal only; no quantitative results reported in this paper. Authors argue the framework gives a principled lens for measuring agent progress but do not supply empirical scores or datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Framework-level conceptual metric; intended to be instantiated with automated measurements (entropy estimates, reproducibility statistics) and human judgments for verifiability -- therefore hybrid in intended use.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>The paper validates the framework conceptually via theoretical grounding (information theory, Landauer's principle) and qualitative analysis of discovery phases, but does not present empirical validation against expert judgments or ground-truth discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Practical measurement challenges: defining/estimating information entropy for open scientific hypothesis spaces, operationalizing verifiability quantitatively across domains, attributing dissipation to specific agent actions, and aligning these abstract measures with human judgments of scientific value.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4450.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4450.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Five-Level Autonomy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Five-Level Framework for Scientific Agent Autonomy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical evaluation/classification framework that assigns agents to five levels (Level 1 human-led → Level 5 full AI autonomy) based on their ability to reduce information entropy, minimize dissipation, and produce high-verifiability artifacts across core scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Five-Level Autonomy Classification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Classifies an agent's autonomy by assessing capabilities across five dimensions (Hypothesis Discovery, Experimental Design, Tool Use, Tool Creation, Analysis & Refinement) and mapping capacity to reduce entropy, manage dissipation, and produce verifiable outputs. Movement between levels is grounded in observable shifts in handling uncertainty and evidence production (e.g., ability to autonomously design experiments or create new tools).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-dimension capability (can/cannot autonomously perform each core task), degree of entropy reduction achieved, dissipation management (efficiency), and final artifact verifiability (can the output be empirically tested/reproduced).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / cross-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation of agent-generated hypotheses/theories generally</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper provides a qualitative taxonomy and operational definitions for levels but no numeric thresholds or benchmarked level assignments for concrete agents in the paper itself.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Primarily conceptual / descriptive; intended to be applied using observable automated metrics (task completion, entropy measures) and human judgment for the highest-level verifiability checks (hybrid).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated conceptually against examples of existing domain agents and the information-theoretic framework; no empirical cross-agent validation provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires operational definitions and measurable thresholds to be useful empirically; mapping abstract entropy/verifiability notions to measurable signals across domains is nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4450.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4450.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Metric-Based Screening</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Metric-Based Screening for Hypothesis Validation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated, scalable approach to screening LLM-generated hypotheses using predefined objective metrics (e.g., plausibility, novelty, feasibility, cost) and formal statistical checks to triage and rank candidate theories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Metric-Based Screening</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Automatically evaluates candidate hypotheses via a set of quantitative criteria and statistical controls: plausibility (consistency with known facts), novelty (dissimilarity to literature), feasibility (practical/experimental viability), cost (resource estimates), and statistical tests for significance where applicable. Integrates retrieval-augmented evidence and can use LLM-derived priors to score surprise/novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Plausibility, novelty, feasibility, cost, statistical significance/reproducibility, and evidence traceability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general (applied in biomedical, materials, chemistry examples)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypotheses and predictive claims</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Discussed qualitatively; the paper cites specific systems (e.g., POPPER) that reportedly achieve human-level accuracy and other benchmarks but Metric-Based Screening itself is presented as a category rather than an implemented single system in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (metric computation and statistical checks) often combined with downstream human review for high-stakes validation (hybrid).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Typically validated by correlating automated scores with expert judgments or by measuring downstream empirical test outcomes in cited works; the paper cites such validation in referenced systems but does not itself quantify validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Designing appropriate metrics across domains, avoiding gaming/overfitting to metrics, handling LLM hallucinations, ensuring provenance and traceability of supporting evidence, and calibrating novelty measures against literature coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Referenced task-specific suites (e.g., HypoBench, MATDESIGN) but Metric-Based Screening is a methodology rather than a specific dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4450.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4450.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>POPPER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>POPPER (Automated Validation Framework via Agentic Sequential Falsifications)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated validation framework applying rigorous statistical checks and sequential falsification strategies to automatically validate generated hypotheses, reported to reach human-level accuracy at higher speed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated hypothesis validation with agentic sequential falsifications</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>POPPER (agentic sequential falsification)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Automates hypothesis validation by sequentially generating tests/falsifications and applying formal statistical controls to outcomes; iteratively rejects hypotheses that fail falsification tests, aiming to replicate the scientific falsification process in an automated pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Statistical validity of test outcomes, falsifiability (ability to generate definitive tests), empirical reproducibility, and speed relative to human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / applied in cited demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>testable scientific hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Authors report POPPER achieves human-level accuracy at significantly greater speed (qualitative claim in this paper); no numeric benchmark values are supplied here.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: pipeline runs sequential falsification tests automatically; often compared against human baseline performance.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated by comparing automated validation outcomes against human judgments/decisions in cited evaluations; specific statistical controls are applied to measure significance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires reliable simulators/experimental proxies to act as testbeds; may be sensitive to how tests are generated; performance depends on quality of available tools and data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4450.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4450.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCIMON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCIMON (Scientific Inspiration Machines Optimized for Novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system-oriented approach that emphasizes measuring and optimizing novelty of generated scientific ideas by explicitly comparing candidates against existing literature and iteratively revising them until they are sufficiently distinct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>SCIMON novelty-driven screening</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluates hypotheses primarily on novelty by computing measures of similarity to prior literature and iteratively altering proposals until novelty thresholds are met; integrates retrieval to determine prior-art similarity and uses novelty as a primary ranking criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Novelty (distance from prior literature), plausibility (secondary), and novelty-preserving revision until minimal resemblance to existing work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / cross-domain</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>novel hypotheses and conceptual proposals</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Qualitative descriptions in the paper; SCIMON is cited as an example of novelty-focused metric systems but specific numerical performance not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric scoring (literature-similarity measures) typically combined with human review for final novelty judgment (hybrid).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated by measuring difference from literature baselines and (in original work) by expert assessment of novelty; not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Novelty metrics can be brittle with incomplete literature coverage; high novelty may trade off plausibility; requires careful calibration to avoid generating spurious 'novel' but invalid ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4450.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4450.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HypoBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HypoBench (Benchmark for Hypothesis Discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standardized benchmark suite proposed for hypothesis discovery tasks across diverse scientific domains to enable consistent, comparable evaluation of hypothesis-generation systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hypobench: Towards systematic and principled benchmarking for hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>HypoBench benchmark evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provides task definitions, datasets, and evaluation metrics for hypothesis generation problems; allows systems to be scored across reproducible tasks using metrics such as plausibility, novelty, and downstream empirical validation where available.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Domain-specific quality metrics (novelty, plausibility, testability), ranking performance, and possibly downstream experimental success when linked to empirical data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cross-domain (benchmarking across multiple fields)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis-generation outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Serves as a benchmarking framework; the paper references it as an emerging standard but does not present concrete benchmark results here.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Benchmark permits both automated metric scoring and inclusion of human/expert evaluations as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Benchmark validity relies on curated tasks and (in origin paper) correlation of benchmark metrics with expert judgments; not further validated in this survey paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Benchmark coverage, domain transfer, and establishing gold-standard labels for inherently creative tasks remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>A task suite for hypothesis generation across domains (as described in the referenced HypoBench paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4450.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4450.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATDESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MATDESIGN (Materials Design Benchmark / Quality Metrics Suite)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task-specific benchmarking suite for materials-science hypothesis/design that introduces scalable quality metrics for evaluating candidate materials and agent proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>MATDESIGN quality-metric evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Provides domain-specific metrics to evaluate generated materials hypotheses (e.g., predicted property satisfaction, synthesizability proxies, computational cost), enabling scalable ranking and triage of materials candidates generated by agents.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Property-prediction accuracy, synthesizability/feasibility, novelty relative to known materials, and computational/experimental cost.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>design hypotheses and candidate materials (predictive/engineering proposals)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Referenced as an emerging task-specific suite; the paper does not report numeric results for MATDESIGN here.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Designed for automated scoring with optional human expert curation for high-value candidates (hybrid).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Intended to be validated by downstream simulation/experimental checks and expert comparison in origin work; not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Domain-specific proxy metrics may not fully capture real-world synthesizability or long-term performance; requires robust simulation/experimental oracles.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Domain-specific materials design tasks and metrics (as referenced but not reproduced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4450.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4450.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logit-Calibrated Prior</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logit-based Calibrated Prior Technique for Hypothesis Assessment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique using LLM-derived probabilistic priors (from model logits) calibrated into expectations to quantify how surprising or novel a proposed correlation/hypothesis is and rank candidates accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploiting llms for automatic hypothesis assessment via a logit-based calibrated prior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Logit-based Calibrated Prior scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Computes an LLM's predictive prior distribution (using logits) for observed relationships and calibrates these to estimate expectedness; hypotheses are scored by how much observed evidence deviates from this prior (surprise), allowing ranking by novelty and relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Surprise / novelty relative to LLM prior, evidence support, and calibrated confidence scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / empirical-data-driven domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>correlational hypotheses and candidate associations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported in the cited work to enable high-accuracy ranking of hypotheses in real-world data; in this paper, authors summarize that the technique can rank by novelty and relevance with high accuracy without giving numeric measures.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric based on model internal statistics; can be compared/correlated with human judgments (hybrid possible).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated (in cited work) by measuring ranking accuracy against held-out real-world datasets and/or expert labels; paper summarizes that the technique performs well in such comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Depends on the calibration quality and the representativeness of the LLM prior; may reflect biases in pretraining data and requires careful calibration for domain shift.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4450.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4450.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Frameworks for LLM Evaluation (limited-sample robustness)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bayesian approaches to evaluate LLM outputs by explicitly modeling uncertainty in estimates and enabling principled inference with limited samples, improving robustness of evaluation metrics in small-N settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Confidence in large language model evaluation: A bayesian approach to limited-sample challenges</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Bayesian evaluation for LLM outputs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Treats evaluation as probabilistic hypothesis testing: builds Bayesian models of metric estimates (e.g., accuracy, agreement) to compute posterior distributions and credible intervals, allowing more robust comparisons under limited sample sizes and producing calibrated uncertainty estimates for evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Posterior mean/median metric estimates, credible intervals (uncertainty), and Bayes factors when comparing models or treatments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation of generated theories/explanations (statistical assessment)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper cites Bayesian frameworks as providing probabilistic robustness in limited-sample evaluation scenarios; no numeric evaluation results for hypothesis generation provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated statistical approach that complements human judgments by quantifying uncertainty (hybrid use recommended).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated by showing better-calibrated confidence intervals and improved decision-making in low-sample experiments (as summarized from referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires appropriate priors and careful modeling choices; interpretability of Bayesian outputs to non-experts can be a barrier; computational overhead vs simple frequentist estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4450.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4450.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-Based Validation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent-Based Validation (Proposal-Critic Multi-Agent Evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive validation paradigm where proposing agents generate hypotheses and a panel of reviewer/critic agents (or simulators) iteratively challenge, test, and refine proposals, emulating peer review and experimental falsification loops.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Agent-Based Validation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Uses multiple agents with different roles (proposers, critics, experiment simulators) to iteratively evaluate hypotheses: critics flag logical flaws or missing evidence; simulators/experimental tools run tests; proposer refines the hypothesis based on feedback. This produces a refined set of hypotheses that have survived adversarial critique and (where available) simulated/empirical tests.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Empirical testability, logical coherence, resistance to adversarial critique, reproducibility of simulated/empirical test outcomes, and documented provenance from evidence sources.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / applied in examples across biomedicine, chemistry, materials</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypotheses and mechanistic explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Cited frameworks (e.g., ResearchAgent, SGA) demonstrate iterative refinement and improved feasibility/novelty of proposals; specific numeric gains vary by domain and are reported in original system papers (this survey cites qualitative improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: primarily automated multi-agent critique with optional human-in-the-loop at final validation stage.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated by closure-of-loop experiments where agent proposals are tested in simulators or real experiments, and by comparing refined outputs to expert assessments (as reported in referenced systems).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quality of validation depends on fidelity of simulators/tools and the critic agents' models; agent critics can share training biases; risk of collective blind spots absent true human expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4450.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4450.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Axes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Canonical Evaluation Axes for Hypothesis Assessment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concise list of commonly used evaluation dimensions for assessing generated scientific hypotheses: plausibility, novelty, feasibility, cost (and related reproducibility/statistical significance).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Canonical Evaluation Axes</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Defines multiple orthogonal criteria used to score hypotheses: plausibility (consistency with known facts), novelty (degree of newness relative to literature), feasibility (practical/experimental implementability), cost (estimated resource/time expense), and associated reproducibility/statistical significance metrics for empirical assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Plausibility, novelty, feasibility, cost, reproducibility/statistical rigor, traceability to evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypotheses and explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Presented as recommended axes for screening and triage; no quantitative results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Intended for automated scoring where possible (e.g., literature-similarity for novelty) and human expert scoring for subjective axes (hybrid).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Axes are commonly validated in prior work by correlating axis scores with expert judgments and downstream empirical success; not re-validated empirically in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Some axes (novelty, plausibility) require comprehensive literature coverage and domain knowledge; interdependence between axes complicates scalar aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automated hypothesis validation with agentic sequential falsifications <em>(Rating: 2)</em></li>
                <li>Hypobench: Towards systematic and principled benchmarking for hypothesis generation <em>(Rating: 2)</em></li>
                <li>Exploiting llms for automatic hypothesis assessment via a logit-based calibrated prior <em>(Rating: 2)</em></li>
                <li>Confidence in large language model evaluation: A bayesian approach to limited-sample challenges <em>(Rating: 2)</em></li>
                <li>Scimon: Scientific inspiration machines optimized for novelty <em>(Rating: 2)</em></li>
                <li>ResearchAgent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4450",
    "paper_id": "paper-282057333",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Info-Theory Framework",
            "name_full": "Information-Theoretic Framework for Autonomous Scientific Discovery",
            "brief_description": "A conceptual evaluation framework that frames scientific discovery as information processing: measuring Information Entropy (uncertainty), Verifiability (testability) and Dissipation (cost of exploring non-solution paths) to assess agent progress and outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Information-Theoretic Evaluation",
            "evaluation_method_description": "Evaluates LLM-generated theories by quantifying (1) entropy reduction the agent achieves (how much the hypothesis/search space uncertainty is reduced), (2) verifiability of artifacts (the degree to which a hypothesis/description can be empirically or logically tested), and (3) dissipation (computational/experimental cost expended on unproductive searches). The method is conceptual: track transformations of information across stages (Human Intent → Natural Language → Computer Language → Physical Information) and score candidate theories by how much they reduce internal entropy while increasing verifiability per unit dissipation.",
            "evaluation_criteria": "Information Entropy reduction (uncertainty/hypothesis-space shrinkage), Verifiability (testability/statistical reproducibility), Dissipation (resource/costs of false paths), and derived trade-offs between them.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / cross-domain",
            "theory_type": "general (hypotheses, mechanistic explanations, predictive conjectures)",
            "human_comparison": null,
            "evaluation_results": "Conceptual proposal only; no quantitative results reported in this paper. Authors argue the framework gives a principled lens for measuring agent progress but do not supply empirical scores or datasets.",
            "automated_vs_human_evaluation": "Framework-level conceptual metric; intended to be instantiated with automated measurements (entropy estimates, reproducibility statistics) and human judgments for verifiability -- therefore hybrid in intended use.",
            "validation_method": "The paper validates the framework conceptually via theoretical grounding (information theory, Landauer's principle) and qualitative analysis of discovery phases, but does not present empirical validation against expert judgments or ground-truth discoveries.",
            "limitations_challenges": "Practical measurement challenges: defining/estimating information entropy for open scientific hypothesis spaces, operationalizing verifiability quantitatively across domains, attributing dissipation to specific agent actions, and aligning these abstract measures with human judgments of scientific value.",
            "benchmark_dataset": null,
            "uuid": "e4450.0",
            "source_info": {
                "paper_title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Five-Level Autonomy",
            "name_full": "Five-Level Framework for Scientific Agent Autonomy",
            "brief_description": "A hierarchical evaluation/classification framework that assigns agents to five levels (Level 1 human-led → Level 5 full AI autonomy) based on their ability to reduce information entropy, minimize dissipation, and produce high-verifiability artifacts across core scientific tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Five-Level Autonomy Classification",
            "evaluation_method_description": "Classifies an agent's autonomy by assessing capabilities across five dimensions (Hypothesis Discovery, Experimental Design, Tool Use, Tool Creation, Analysis & Refinement) and mapping capacity to reduce entropy, manage dissipation, and produce verifiable outputs. Movement between levels is grounded in observable shifts in handling uncertainty and evidence production (e.g., ability to autonomously design experiments or create new tools).",
            "evaluation_criteria": "Per-dimension capability (can/cannot autonomously perform each core task), degree of entropy reduction achieved, dissipation management (efficiency), and final artifact verifiability (can the output be empirically tested/reproduced).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / cross-domain",
            "theory_type": "evaluation of agent-generated hypotheses/theories generally",
            "human_comparison": true,
            "evaluation_results": "Paper provides a qualitative taxonomy and operational definitions for levels but no numeric thresholds or benchmarked level assignments for concrete agents in the paper itself.",
            "automated_vs_human_evaluation": "Primarily conceptual / descriptive; intended to be applied using observable automated metrics (task completion, entropy measures) and human judgment for the highest-level verifiability checks (hybrid).",
            "validation_method": "Validated conceptually against examples of existing domain agents and the information-theoretic framework; no empirical cross-agent validation provided.",
            "limitations_challenges": "Requires operational definitions and measurable thresholds to be useful empirically; mapping abstract entropy/verifiability notions to measurable signals across domains is nontrivial.",
            "benchmark_dataset": null,
            "uuid": "e4450.1",
            "source_info": {
                "paper_title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Metric-Based Screening",
            "name_full": "Metric-Based Screening for Hypothesis Validation",
            "brief_description": "An automated, scalable approach to screening LLM-generated hypotheses using predefined objective metrics (e.g., plausibility, novelty, feasibility, cost) and formal statistical checks to triage and rank candidate theories.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Metric-Based Screening",
            "evaluation_method_description": "Automatically evaluates candidate hypotheses via a set of quantitative criteria and statistical controls: plausibility (consistency with known facts), novelty (dissimilarity to literature), feasibility (practical/experimental viability), cost (resource estimates), and statistical tests for significance where applicable. Integrates retrieval-augmented evidence and can use LLM-derived priors to score surprise/novelty.",
            "evaluation_criteria": "Plausibility, novelty, feasibility, cost, statistical significance/reproducibility, and evidence traceability.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general (applied in biomedical, materials, chemistry examples)",
            "theory_type": "hypotheses and predictive claims",
            "human_comparison": null,
            "evaluation_results": "Discussed qualitatively; the paper cites specific systems (e.g., POPPER) that reportedly achieve human-level accuracy and other benchmarks but Metric-Based Screening itself is presented as a category rather than an implemented single system in this paper.",
            "automated_vs_human_evaluation": "Automated (metric computation and statistical checks) often combined with downstream human review for high-stakes validation (hybrid).",
            "validation_method": "Typically validated by correlating automated scores with expert judgments or by measuring downstream empirical test outcomes in cited works; the paper cites such validation in referenced systems but does not itself quantify validation.",
            "limitations_challenges": "Designing appropriate metrics across domains, avoiding gaming/overfitting to metrics, handling LLM hallucinations, ensuring provenance and traceability of supporting evidence, and calibrating novelty measures against literature coverage.",
            "benchmark_dataset": "Referenced task-specific suites (e.g., HypoBench, MATDESIGN) but Metric-Based Screening is a methodology rather than a specific dataset.",
            "uuid": "e4450.2",
            "source_info": {
                "paper_title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "POPPER",
            "name_full": "POPPER (Automated Validation Framework via Agentic Sequential Falsifications)",
            "brief_description": "An automated validation framework applying rigorous statistical checks and sequential falsification strategies to automatically validate generated hypotheses, reported to reach human-level accuracy at higher speed.",
            "citation_title": "Automated hypothesis validation with agentic sequential falsifications",
            "mention_or_use": "mention",
            "evaluation_method_name": "POPPER (agentic sequential falsification)",
            "evaluation_method_description": "Automates hypothesis validation by sequentially generating tests/falsifications and applying formal statistical controls to outcomes; iteratively rejects hypotheses that fail falsification tests, aiming to replicate the scientific falsification process in an automated pipeline.",
            "evaluation_criteria": "Statistical validity of test outcomes, falsifiability (ability to generate definitive tests), empirical reproducibility, and speed relative to human validation.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / applied in cited demonstrations",
            "theory_type": "testable scientific hypotheses",
            "human_comparison": true,
            "evaluation_results": "Authors report POPPER achieves human-level accuracy at significantly greater speed (qualitative claim in this paper); no numeric benchmark values are supplied here.",
            "automated_vs_human_evaluation": "Automated: pipeline runs sequential falsification tests automatically; often compared against human baseline performance.",
            "validation_method": "Validated by comparing automated validation outcomes against human judgments/decisions in cited evaluations; specific statistical controls are applied to measure significance.",
            "limitations_challenges": "Requires reliable simulators/experimental proxies to act as testbeds; may be sensitive to how tests are generated; performance depends on quality of available tools and data.",
            "benchmark_dataset": null,
            "uuid": "e4450.3",
            "source_info": {
                "paper_title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "SCIMON",
            "name_full": "SCIMON (Scientific Inspiration Machines Optimized for Novelty)",
            "brief_description": "A system-oriented approach that emphasizes measuring and optimizing novelty of generated scientific ideas by explicitly comparing candidates against existing literature and iteratively revising them until they are sufficiently distinct.",
            "citation_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "mention",
            "evaluation_method_name": "SCIMON novelty-driven screening",
            "evaluation_method_description": "Evaluates hypotheses primarily on novelty by computing measures of similarity to prior literature and iteratively altering proposals until novelty thresholds are met; integrates retrieval to determine prior-art similarity and uses novelty as a primary ranking criterion.",
            "evaluation_criteria": "Novelty (distance from prior literature), plausibility (secondary), and novelty-preserving revision until minimal resemblance to existing work.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / cross-domain",
            "theory_type": "novel hypotheses and conceptual proposals",
            "human_comparison": null,
            "evaluation_results": "Qualitative descriptions in the paper; SCIMON is cited as an example of novelty-focused metric systems but specific numerical performance not reported here.",
            "automated_vs_human_evaluation": "Automated metric scoring (literature-similarity measures) typically combined with human review for final novelty judgment (hybrid).",
            "validation_method": "Validated by measuring difference from literature baselines and (in original work) by expert assessment of novelty; not quantified in this paper.",
            "limitations_challenges": "Novelty metrics can be brittle with incomplete literature coverage; high novelty may trade off plausibility; requires careful calibration to avoid generating spurious 'novel' but invalid ideas.",
            "benchmark_dataset": null,
            "uuid": "e4450.4",
            "source_info": {
                "paper_title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "HypoBench",
            "name_full": "HypoBench (Benchmark for Hypothesis Discovery)",
            "brief_description": "A standardized benchmark suite proposed for hypothesis discovery tasks across diverse scientific domains to enable consistent, comparable evaluation of hypothesis-generation systems.",
            "citation_title": "Hypobench: Towards systematic and principled benchmarking for hypothesis generation",
            "mention_or_use": "mention",
            "evaluation_method_name": "HypoBench benchmark evaluation",
            "evaluation_method_description": "Provides task definitions, datasets, and evaluation metrics for hypothesis generation problems; allows systems to be scored across reproducible tasks using metrics such as plausibility, novelty, and downstream empirical validation where available.",
            "evaluation_criteria": "Domain-specific quality metrics (novelty, plausibility, testability), ranking performance, and possibly downstream experimental success when linked to empirical data.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "cross-domain (benchmarking across multiple fields)",
            "theory_type": "hypothesis-generation outputs",
            "human_comparison": null,
            "evaluation_results": "Serves as a benchmarking framework; the paper references it as an emerging standard but does not present concrete benchmark results here.",
            "automated_vs_human_evaluation": "Benchmark permits both automated metric scoring and inclusion of human/expert evaluations as ground truth.",
            "validation_method": "Benchmark validity relies on curated tasks and (in origin paper) correlation of benchmark metrics with expert judgments; not further validated in this survey paper.",
            "limitations_challenges": "Benchmark coverage, domain transfer, and establishing gold-standard labels for inherently creative tasks remain challenging.",
            "benchmark_dataset": "A task suite for hypothesis generation across domains (as described in the referenced HypoBench paper).",
            "uuid": "e4450.5",
            "source_info": {
                "paper_title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "MATDESIGN",
            "name_full": "MATDESIGN (Materials Design Benchmark / Quality Metrics Suite)",
            "brief_description": "A task-specific benchmarking suite for materials-science hypothesis/design that introduces scalable quality metrics for evaluating candidate materials and agent proposals.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "MATDESIGN quality-metric evaluation",
            "evaluation_method_description": "Provides domain-specific metrics to evaluate generated materials hypotheses (e.g., predicted property satisfaction, synthesizability proxies, computational cost), enabling scalable ranking and triage of materials candidates generated by agents.",
            "evaluation_criteria": "Property-prediction accuracy, synthesizability/feasibility, novelty relative to known materials, and computational/experimental cost.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "materials science",
            "theory_type": "design hypotheses and candidate materials (predictive/engineering proposals)",
            "human_comparison": null,
            "evaluation_results": "Referenced as an emerging task-specific suite; the paper does not report numeric results for MATDESIGN here.",
            "automated_vs_human_evaluation": "Designed for automated scoring with optional human expert curation for high-value candidates (hybrid).",
            "validation_method": "Intended to be validated by downstream simulation/experimental checks and expert comparison in origin work; not detailed here.",
            "limitations_challenges": "Domain-specific proxy metrics may not fully capture real-world synthesizability or long-term performance; requires robust simulation/experimental oracles.",
            "benchmark_dataset": "Domain-specific materials design tasks and metrics (as referenced but not reproduced in this paper).",
            "uuid": "e4450.6",
            "source_info": {
                "paper_title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Logit-Calibrated Prior",
            "name_full": "Logit-based Calibrated Prior Technique for Hypothesis Assessment",
            "brief_description": "A technique using LLM-derived probabilistic priors (from model logits) calibrated into expectations to quantify how surprising or novel a proposed correlation/hypothesis is and rank candidates accordingly.",
            "citation_title": "Exploiting llms for automatic hypothesis assessment via a logit-based calibrated prior",
            "mention_or_use": "mention",
            "evaluation_method_name": "Logit-based Calibrated Prior scoring",
            "evaluation_method_description": "Computes an LLM's predictive prior distribution (using logits) for observed relationships and calibrates these to estimate expectedness; hypotheses are scored by how much observed evidence deviates from this prior (surprise), allowing ranking by novelty and relevance.",
            "evaluation_criteria": "Surprise / novelty relative to LLM prior, evidence support, and calibrated confidence scores.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / empirical-data-driven domains",
            "theory_type": "correlational hypotheses and candidate associations",
            "human_comparison": null,
            "evaluation_results": "Reported in the cited work to enable high-accuracy ranking of hypotheses in real-world data; in this paper, authors summarize that the technique can rank by novelty and relevance with high accuracy without giving numeric measures.",
            "automated_vs_human_evaluation": "Automated metric based on model internal statistics; can be compared/correlated with human judgments (hybrid possible).",
            "validation_method": "Validated (in cited work) by measuring ranking accuracy against held-out real-world datasets and/or expert labels; paper summarizes that the technique performs well in such comparisons.",
            "limitations_challenges": "Depends on the calibration quality and the representativeness of the LLM prior; may reflect biases in pretraining data and requires careful calibration for domain shift.",
            "benchmark_dataset": null,
            "uuid": "e4450.7",
            "source_info": {
                "paper_title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Bayesian Eval",
            "name_full": "Bayesian Frameworks for LLM Evaluation (limited-sample robustness)",
            "brief_description": "Bayesian approaches to evaluate LLM outputs by explicitly modeling uncertainty in estimates and enabling principled inference with limited samples, improving robustness of evaluation metrics in small-N settings.",
            "citation_title": "Confidence in large language model evaluation: A bayesian approach to limited-sample challenges",
            "mention_or_use": "mention",
            "evaluation_method_name": "Bayesian evaluation for LLM outputs",
            "evaluation_method_description": "Treats evaluation as probabilistic hypothesis testing: builds Bayesian models of metric estimates (e.g., accuracy, agreement) to compute posterior distributions and credible intervals, allowing more robust comparisons under limited sample sizes and producing calibrated uncertainty estimates for evaluation metrics.",
            "evaluation_criteria": "Posterior mean/median metric estimates, credible intervals (uncertainty), and Bayes factors when comparing models or treatments.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / evaluation methodology",
            "theory_type": "evaluation of generated theories/explanations (statistical assessment)",
            "human_comparison": null,
            "evaluation_results": "Paper cites Bayesian frameworks as providing probabilistic robustness in limited-sample evaluation scenarios; no numeric evaluation results for hypothesis generation provided here.",
            "automated_vs_human_evaluation": "Automated statistical approach that complements human judgments by quantifying uncertainty (hybrid use recommended).",
            "validation_method": "Validated by showing better-calibrated confidence intervals and improved decision-making in low-sample experiments (as summarized from referenced work).",
            "limitations_challenges": "Requires appropriate priors and careful modeling choices; interpretability of Bayesian outputs to non-experts can be a barrier; computational overhead vs simple frequentist estimates.",
            "benchmark_dataset": null,
            "uuid": "e4450.8",
            "source_info": {
                "paper_title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Agent-Based Validation",
            "name_full": "Agent-Based Validation (Proposal-Critic Multi-Agent Evaluation)",
            "brief_description": "An interactive validation paradigm where proposing agents generate hypotheses and a panel of reviewer/critic agents (or simulators) iteratively challenge, test, and refine proposals, emulating peer review and experimental falsification loops.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Agent-Based Validation",
            "evaluation_method_description": "Uses multiple agents with different roles (proposers, critics, experiment simulators) to iteratively evaluate hypotheses: critics flag logical flaws or missing evidence; simulators/experimental tools run tests; proposer refines the hypothesis based on feedback. This produces a refined set of hypotheses that have survived adversarial critique and (where available) simulated/empirical tests.",
            "evaluation_criteria": "Empirical testability, logical coherence, resistance to adversarial critique, reproducibility of simulated/empirical test outcomes, and documented provenance from evidence sources.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / applied in examples across biomedicine, chemistry, materials",
            "theory_type": "hypotheses and mechanistic explanations",
            "human_comparison": true,
            "evaluation_results": "Cited frameworks (e.g., ResearchAgent, SGA) demonstrate iterative refinement and improved feasibility/novelty of proposals; specific numeric gains vary by domain and are reported in original system papers (this survey cites qualitative improvements).",
            "automated_vs_human_evaluation": "Hybrid: primarily automated multi-agent critique with optional human-in-the-loop at final validation stage.",
            "validation_method": "Validated by closure-of-loop experiments where agent proposals are tested in simulators or real experiments, and by comparing refined outputs to expert assessments (as reported in referenced systems).",
            "limitations_challenges": "Quality of validation depends on fidelity of simulators/tools and the critic agents' models; agent critics can share training biases; risk of collective blind spots absent true human expertise.",
            "benchmark_dataset": null,
            "uuid": "e4450.9",
            "source_info": {
                "paper_title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Evaluation Axes",
            "name_full": "Canonical Evaluation Axes for Hypothesis Assessment",
            "brief_description": "A concise list of commonly used evaluation dimensions for assessing generated scientific hypotheses: plausibility, novelty, feasibility, cost (and related reproducibility/statistical significance).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Canonical Evaluation Axes",
            "evaluation_method_description": "Defines multiple orthogonal criteria used to score hypotheses: plausibility (consistency with known facts), novelty (degree of newness relative to literature), feasibility (practical/experimental implementability), cost (estimated resource/time expense), and associated reproducibility/statistical significance metrics for empirical assessment.",
            "evaluation_criteria": "Plausibility, novelty, feasibility, cost, reproducibility/statistical rigor, traceability to evidence.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general",
            "theory_type": "hypotheses and explanations",
            "human_comparison": null,
            "evaluation_results": "Presented as recommended axes for screening and triage; no quantitative results in this paper.",
            "automated_vs_human_evaluation": "Intended for automated scoring where possible (e.g., literature-similarity for novelty) and human expert scoring for subjective axes (hybrid).",
            "validation_method": "Axes are commonly validated in prior work by correlating axis scores with expert judgments and downstream empirical success; not re-validated empirically in this survey.",
            "limitations_challenges": "Some axes (novelty, plausibility) require comprehensive literature coverage and domain knowledge; interdependence between axes complicates scalar aggregation.",
            "benchmark_dataset": null,
            "uuid": "e4450.10",
            "source_info": {
                "paper_title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
                "publication_date_yy_mm": "2025-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automated hypothesis validation with agentic sequential falsifications",
            "rating": 2,
            "sanitized_title": "automated_hypothesis_validation_with_agentic_sequential_falsifications"
        },
        {
            "paper_title": "Hypobench: Towards systematic and principled benchmarking for hypothesis generation",
            "rating": 2,
            "sanitized_title": "hypobench_towards_systematic_and_principled_benchmarking_for_hypothesis_generation"
        },
        {
            "paper_title": "Exploiting llms for automatic hypothesis assessment via a logit-based calibrated prior",
            "rating": 2,
            "sanitized_title": "exploiting_llms_for_automatic_hypothesis_assessment_via_a_logitbased_calibrated_prior"
        },
        {
            "paper_title": "Confidence in large language model evaluation: A bayesian approach to limited-sample challenges",
            "rating": 2,
            "sanitized_title": "confidence_in_large_language_model_evaluation_a_bayesian_approach_to_limitedsample_challenges"
        },
        {
            "paper_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "rating": 2,
            "sanitized_title": "scimon_scientific_inspiration_machines_optimized_for_novelty"
        },
        {
            "paper_title": "ResearchAgent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        }
    ],
    "cost": 0.025511,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics
10 Oct 2025</p>
<p>Lianhao Zhou 
Department of Computer Science and Engineering
Texas A&amp;M University</p>
<p>Hongyi Ling 
Department of Computer Science and Engineering
Texas A&amp;M University</p>
<p>Cong Fu 
Department of Computer Science and Engineering
Texas A&amp;M University</p>
<p>Yepeng Huang 
Department of Biomedical Informatics
Harvard Medical School</p>
<p>Michael Sun 
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology</p>
<p>Wendi Yu 
Department of Computer Science and Engineering
Texas A&amp;M University</p>
<p>Xiaoxuan Wang 
Department of Computer Science
University of California
Los Angeles</p>
<p>Xiner Li 
Department of Computer Science and Engineering
Texas A&amp;M University</p>
<p>Xingyu Su 
Department of Computer Science and Engineering
Texas A&amp;M University</p>
<p>Junkai Zhang 
Department of Computer Science
University of California
Los Angeles</p>
<p>Xiusi Chen 
Siebel School of Computing and Data Science
University of Illinois Urbana Champaign</p>
<p>Chenxing Liang 
Department of Computer Science and Engineering
Texas A&amp;M University</p>
<p>Xiaofeng Qian 
Department of Materials Science and Engineering
Texas A&amp;M University</p>
<p>Department of Electrical and Computer Engineering
Texas A&amp;M University</p>
<p>Department of Physics and Astronomy
Department of Mechanical Engineering
Texas A&amp;M University
9 J. Mike Walker '66</p>
<p>Texas A&amp;M University</p>
<p>Heng Ji 
Siebel School of Computing and Data Science
University of Illinois Urbana Champaign</p>
<p>Wei Wang 
Department of Computer Science
University of California
Los Angeles</p>
<p>Marinka Zitnik 
Department of Biomedical Informatics
Harvard Medical School</p>
<p>Shuiwang Ji 
Department of Computer Science and Engineering
Texas A&amp;M University</p>
<p>Department of Materials Science and Engineering
Texas A&amp;M University</p>
<p>Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics
10 Oct 2025525032706ACAA9C0F32A66BEDBF7DC90arXiv:2510.09901v1[cs.AI]
Computing has long served as a cornerstone of scientific discovery.Recently, a paradigm shift has emerged with the rise of large language models (LLMs), introducing autonomous systems, referred to as agents, that accelerate discovery across varying levels of autonomy.These language agents provide a flexible and versatile framework that orchestrates interactions with human scientists, natural language, computer language and code, and physics.This paper presents our view and vision of LLM-based scientific agents and their growing role in transforming the scientific discovery lifecycle, from hypothesis discovery, experimental design and execution, to result analysis and refinement.We critically examine current methodologies, emphasizing key innovations, practical achievements, and outstanding limitations.Additionally, we identify open research challenges and outline promising directions for building more robust, generalizable, and adaptive scientific agents.Our analysis highlights the transformative potential of autonomous agents to accelerate scientific discovery across diverse domains.</p>
<p>Introduction</p>
<p>Scientific discovery is fundamental to advancing human knowledge, driving innovations across diverse fields such as biology, chemistry, physics, and materials science, etc. [Zhang et al., 2025a, Wang et al., 2023a].The pivotal role of computing in advancing discoveries across the natural sciences has long been recognized [Dirac, 1929], with impacts ranging from quantum mechanics [Landau andLifshitz, 2013a, Tong, 2025a] to fluid dynamics [Landau andLifshitz, 2013b, Tong, 2025b].For many years, the process of scientific discovery has been predominantly dependent on human intuition, expertise, and iterative experimentation [Xu et al., 2023].Nevertheless, human-driven discovery processes sometimes face challenges, including high costs, substantial time investments, potential cognitive biases, and limitations in exploring vast hypothesis spaces or complex data interactions systematically [Kitano, 2021].Additionally, the increasing complexity and volume of data generated and categorize the applications and achievements of scientific agents by domains (Figure 2).Finally, we identify open challenges and outline future research directions to advance the development of robust, generalizable, and highly adaptive LLM-driven scientific agents.</p>
<p>2 Overview of Autonomous Agents for Scientific Discovery</p>
<p>Overview of Scientific Discovery</p>
<p>To understand how LLM agents transform scientific discovery, we first revisit how scientific processes are typically structured across domains.Scientific discovery is the systematic pursuit of understanding and generating new knowledge or verifying hypotheses within scientific domains.It involves identifying meaningful questions, formulating hypotheses [Wang et al., 2024d], generating experiment procedures [Zhong et al., 2024], conducting experiments, and refining results iteratively to enhance understanding or achieve practical outcomes.While objectives differ across fields, such as identifying stable crystal structures in materials science, elucidating genetic mechanisms in biology, or systematically synthesizing therapeutic compounds in pharmacology, the discovery process shares common core components and phases across disciplines.We conceptualize the scientific discovery processes into three phases.</p>
<p>Hypothesis Discovery.This initial phase is a crucial creative process with the core objective of identifying and forming novel, verifiable scientific hypotheses from vast amounts of data and existing knowledge.It aims to accelerate the pace of discovery by revealing hidden connections that are difficult for human researchers to find on their own.A complete hypothesis discovery workflow is typically composed of three key stages, including Knowledge Extraction, Hypothesis Generation, and Hypothesis Screening and Validation.The goal is to systematically identify meaningful questions and formulate hypotheses that can be tested in subsequent stages.</p>
<p>Domain-Specific Scientific Agents</p>
<p>e.g., SpatialAgent [Wang et al., 2025a], TAIS [Liu et al., 2024a], CRISPR-GPT [Qu et al., 2025], BioDis-coveryAgent [Roohani et al., 2024], PerTurboAgent [Hao et al., 2025], BioAgents [Mehandru et al., 2025], BIA [Xin et al., 2024], scBaseCount [Youngblut et al., 2025], CellAgent [Xiao et al., 2024], CompBioAgent [Zhang et al., 2025b], scAgent [Mao et al., 2025], CASSIA [Xie et al., 2024] e.g., SAMPLE [Rapp et al., 2024], Virtual Lab [Swanson et al., 2024], ProtAgents [Ghafarollahi and Buehler, 2024a], Sparks [Ghafarollahi and Buehler, 2025], VibeGen [Ni and Buehler, 2025], AutoProteinEngine (Au-toPE) [Liu et al., 2024b], ProChat [Huang et al., 2024b] e.g., Biomni [Huang et al., 2025b], AI co-scientist [Gottweis et al., 2025b], TxAgent [Gao et al., 2025b], BioResearcher [Luo et al., 2025b], STELLA [Jin et al., 2025a], CLADD [Lee et al., 2025], DrugAgent [Liu et al., 2024c], PharmAgents [Gao et al., 2025d], LIDDiA [Averly et al., 2025], AgentMD [Jin et al., 2024a], MedAgents [Tang et al., 2023], ClinicalGPT [Wang et al., 2023b], BehaveAgent [Aljovic et al., 2025] e.g., Coscientist [Boiko et al., 2023], ChemCrow [M.Bran et al., 2024], ChemAgents [Song et al., 2025], ChemReasoner [Sprueill et al., 2024], CACTUS [McNaughton et al., 2024], Chemist-X [Chen et al., 2023a], El Agente Q [Zou et al., 2025], LLM-RDF [Ruan et al., 2024], FROGENT [Pan et al., 2025], LARC [Baker et al., 2025], FMG [Sun et al., 2025], MOOSE-Chem [Yang et al., 2024] e.g., A-Lab [Szymanski et al., 2023], ChatMOF [Kang and Kim, 2024], ChatGPT Research Group [Zheng et al., 2023], MDAgent [Shi et al., 2025], HoneyComb [Zhang et al., 2024b], LLMatDesign [Jia et al., 2024], MatAgent [Bazgir et al., 2025a], MatSciAgent [Chaudhari et al., 2025], MatPilot [Ni et al., 2024], Materials Laws Multi-Agent Framework [Hu et al., 2024] e.g., k-agents [Cao et al., 2024], AtomAgents [Ghafarollahi and Buehler, 2024c], Mephisto [Sun et al., 2024], QCopilot [Sha et al., 2025], OpenFOAMGPT [Pandey et al., 2025], MetaOpenFOAM [Chen et al., 2024b], AI-Scientist Framework [Xu et al., 2025a] e.g., Autonomous GIS Agent [Ning et al., 2025], GIS Copilot [Akinboyewa et al., 2025], LP-COMDA [Liu et al., 2024d], ReAct agent for gas turbines [Song et al., 2024]  Experimental Design and Execution.Following hypothesis discovery, experimental design is a critical step that involves creating a structured plan to systematically test ideas and achieve research objectives.This process is a form of workflow generation where high-level scientific goals are translated into concrete, executable protocols.The execution stage serves as the crucial bridge that transforms abstract strategies into concrete actions and empirical evidence needed for generating insights.It involves complex tasks such as orchestrating computational resources, managing large datasets, interfacing with laboratory instruments or simulators, and continuously monitoring the workflow.It's important to clarify that our definition of experiments includes both traditional wetlab experiments and computational simulations.While both are integral to the scientific process, current LLM-based scientific agents predominantly operate in the computational domain, focusing on simulation-based discovery.LLM-based agents accomplish this primarily through two interconnected capabilities, namely tool use and tool creation.</p>
<p>Genomics</p>
<p>Result Analysis and Refinement.This phase begins with the analysis of experimental results, where autonomous agents interpret raw outputs to derive meaningful scientific insights.Since scientific discovery rarely concludes after a single iteration, this stage functions as a crucial iterative mechanism.It involves cycles of reviewing validated results, identifying discrepancies or unexpected findings, and then methodically refining the proposed hypotheses, experimental design, or computational tools to progressively enhance scientific outcomes.</p>
<p>Collectively, these stages encapsulate the iterative and cyclical nature of scientific inquiry, continuously refining and verifying insights to produce credible and innovative scientific knowledge.</p>
<p>Overview of Scientific Agents</p>
<p>A scientific agent is a specialized AI system designed to simulate and autonomously execute aspects of the scientific research process.Unlike general-purpose LLM agents [OpenAI, 2023, Google, 2024, Anthropic, 2024], which are built for a wide array of tasks like conversation and text generation, scientific agents are highly domain-specific and task-oriented.Their primary goal is to accelerate scientific discovery by automating and augmenting the capabilities of human researchers, thereby allowing the navigation of the intricate landscape of modern science with greater speed and efficiency.Table 1 summarizes the key differences between scientific agents and general-purpose LLM agents.To solve specific, complex scientific problems and generate new knowledge.</p>
<p>To understand and respond to a wide range of general user queries and commands.</p>
<p>Knowledge Base</p>
<p>Deeply integrated with specialized scientific databases, literature, and models.</p>
<p>Relies on broad, general knowledge acquired from vast, diverse training data.</p>
<p>Reasoning</p>
<p>Employs rigorous, multi-step logical deduction and inference based on scientific principles.</p>
<p>Utilizes common sense, associative, and heuristic reasoning for broad tasks.</p>
<p>Tool Use</p>
<p>Natively uses highly specialized software (e.g.simulators) and hardware (e.g.robots).</p>
<p>Interacts with general-purpose tools like web search engines and code interpreters.</p>
<p>Memory</p>
<p>Utilizes memory to accumulate domain knowledge and learn from experimental outcomes.</p>
<p>Memory is primarily used for maintaining conversational context and user preferences.</p>
<p>Evaluation Metrics</p>
<p>Success is measured by the accuracy, reproducibility, and novelty of the scientific results.</p>
<p>Success is measured by task completion, user satisfaction, and conversational quality.</p>
<p>Reasoning and Planning.The core cognitive functions of an LLM-based scientific agent are its advanced reasoning and planning abilities, which serve as its intellectual engine.Its reasoning capabilities allow it to sift through vast amount of scientific literature and data to formulate novel hypotheses, identify patterns, and draw logical inferences [Ferrag et al., 2025, Zhang et al., 2024c].This is coupled with a sophisticated planning module that can decompose a high-level research goal into a coherent sequence of actionable steps [Wei et al., 2025b, Huang et al., 2024c].</p>
<p>Tool Use.A defining characteristic of LLM-based scientific agents is their deep integration with a wide array of specialized tools [Jin et al., 2024b, Shen, 2024].This capability serves as the bridge between the agent's cognitive processes and the practical execution of research.These tools are not limited to software, such as domain-specific simulators, data analysis packages, and proprietary knowledge bases, but also extend to physical hardware.Through programmatic interfaces, scientific agents can operate laboratory automation systems, control sensor arrays, and manage other instrumentation, allowing them to directly interact with and conduct experiments in the physical world [Boiko et al., 2023].</p>
<p>Memory.An LLM-based scientific agent's memory mechanism is fundamental to its capacity for sustained research and cumulative learning [Xu et al., 2025b, Wu et al., 2025].Acting as both a working memory for immediate tasks and a persistent knowledge base, it archives all past outcomes.By retaining successful protocols and failed inquiries, this long-term memory enables the agent to learn from experience.This ability to continuously reference its own history allows the agent to refine strategies, avoid repeating errors, and systematically drive the process of scientific discovery forward.</p>
<p>An Information-Theoretic Framework for Autonomous Scientific Discovery</p>
<p>Properties of Information in Autonomous Scientific Discovery</p>
<p>The transition from human-led research to autonomous scientific discovery can be understood through a formal analytical framework.The core of this framework is to examine three key aspects of the information processed by an intelligent agent in the scientific workflow.First, we view scientific discovery as an inherently dissipative process, characterized by uncertainty and the need for extensive trial-and-error exploration.Second, we use the mathematical measure of Information Entropy to quantify the root of this uncertainty.Finally, we examine Verifiability, which measures whether information can be tested against objective standards and is the ultimate goal of the research process.</p>
<p>Information Entropy is a mathematical measure from information theory that quantifies the uncertainty or the size of the hypothesis space of a given problem.A high-entropy state, such as the E n t r o p y V e r i fi a b il it y Human Intent</p>
<p>Natural Language</p>
<p>Computer Language set of all possible scientific hypotheses, presents a vast and unstructured search space for an agent.A core task for the agent is therefore to perform Entropy Reduction-to systematically reduce the uncertainty of the problem through its operations [Yao, 2003].Our notion of reducing information entropy is consistent with the Second Law of Thermodynamics, which states that the total entropy of an isolated system can never decrease [Borgnakke, 2024, Schroeder, 2020].For an agent to achieve Information Entropy Reduction in scientific discovery, the system must be an open system, instead of an isolated one.This systematic decrease in internal informational uncertainty is driven by the necessary exchange of information with the physical world.The agent's internal entropy reduction is fundamentally dependent on interactions with the physical world.The information required to constrain the hypothesis space cannot be generated internally.Instead, it must be obtained from real-world processes (e.g., performing experiments or analyzing raw data that originates from physical processes) to constrain uncertainty.</p>
<p>Physical Information</p>
<p>Verifiability is a property of a specific information object that measures its ability to be objectively tested against a formal, logical, or empirical standard [Patterson, 1978].The ultimate goal of the scientific process is to transform an initial idea with low verifiability into an empirical fact with very high verifiability.The agent's workflow, therefore, is also a process of progressively imparting verifiability to information.</p>
<p>Dissipation refers to the unavoidable computational cost and effort expended on unproductive paths during the exploration of a problem space.This cost encompasses the resources consumed in the formulation, execution, and verification of incorrect hypotheses and experiments that ultimately do not lead to a solution.In thermodynamics, a dissipative process is defined by the irreversible conversion of usable forms of energy (such as mechanical, electrical, or chemical) into unusable thermal energy [Borgnakke, 2024, Schroeder, 2020].Landauer [Landauer, 1961] extended this concept into the Physics of Information.The Landauer's Principle dictates that any irreversible information operation must be accompanied by a minimum energy dissipation.The dissipation defined here is an inherent and unavoidable cost in scientific discovery.The process of navigating a high-entropy space where the agent moves from many uncertain potential solutions to a single determined solution is necessarily dissipative, because this transition constitutes an irreversible information operation.According to Landauer's Principle, the degree of dissipation is directly proportional to the number of non-solution paths that must be explored and subsequently discarded to find a valid one.This trial-and-error cost is a fundamental challenge in scientific discovery.Its interaction with language involves extracting knowledge from literature to formulate verifiable hypotheses and detailed research plans.The agent's interface with code translates high-level plans into executable programs for simulations or instrument control by integrating tool functionalities.Finally, it interacts with physics by using raw data and laws to direct physical or simulated instruments, yielding experimental results.This cycle represents an iterative and autonomous discovery cycle, bridging human intent to empirical evidence.</p>
<p>Using these properties as an analytical lens, we can systematically analyze the transformation of different types of information as they flow through the autonomous discovery process.</p>
<p>A Taxonomy of Information in Autonomous Scientific Discovery</p>
<p>In autonomous scientific discovery, information transforms through a hierarchical process, moving from high-entropy human intent to low-entropy, verifiable physical information.Figure 3 illustrates a series of transitions: from abstract Human Intent to structured Natural Language, then to formal Computer Language, and finally to verifiable Physical Information.Each step reduces entropy by resolving ambiguity and increases verifiability by making the information more precise and testable against reality.4 illustrates the dynamic, closed-loop workflow of an LLM-based scientific agent as the coordinator orchestrating four other key components, including Scientists, Language, Code, and Physics.These four components serve as concrete instantiations of the four corresponding information levels: Human Intent, Natural Language, Computer Language, and Physical Information.</p>
<p>Human Intent serves as the origin point of the scientific process.It exists as an abstract hypothesis or goal within a vast, unstructured conceptual space.This state possesses the highest possible</p>
<p>Very High Entropy</p>
<p>Requires the highest level of linguistic creativity to synthesize disparate concepts and articulate new ideas.</p>
<p>Medium Entropy</p>
<p>Primarily used for auxiliary tasks like knowledge graph construction or data mining, not the core creative act.</p>
<p>High Entropy</p>
<p>Historical data serves as inspiration; the volume is vast, unstructured, and the valuable signals are nonobvious.</p>
<p>High</p>
<p>Requires overcoming high entropy in creative thought, making it difficult to automate.</p>
<p>Experimental Design</p>
<p>High Entropy</p>
<p>The intent is creative and requires navigating a vast space of possibilities (e.g., "design a new experiment").</p>
<p>Medium Entropy</p>
<p>Involves creating a novel, precise, and unambiguous protocol from an abstract idea.</p>
<p>Medium Entropy</p>
<p>Requires selecting, configuring, and integrating from a vast library of existing tools.</p>
<p>Medium Entropy</p>
<p>Must predict and model real-world physical laws and constraints, which are inherently complex and uncertain.</p>
<p>Medium</p>
<p>Connects abstract hypotheses to the physical world, navigating a vast and constrained search space.</p>
<p>Tool Use Low Entropy</p>
<p>The intent is highly specific and direct (e.g., "run this specific analysis tool").</p>
<p>Low Entropy</p>
<p>Typically consists of structured commands or simple instructions with low ambiguity.</p>
<p>Very Low Entropy</p>
<p>Involves calling predefined functions or APIs; the syntax and logic are deterministic.</p>
<p>Low Entropy</p>
<p>Input and output data formats are generally well-defined and structured, reducing uncertainty.</p>
<p>Very Low</p>
<p>The process is deterministic with low entropy, making it the easiest stage for AI to automate reliably.</p>
<p>Tool Creation</p>
<p>Very High Entropy Requires defining and solving a problem for which no solution currently exists.</p>
<p>Very High Entropy</p>
<p>Needs to describe the principles, architecture, and logic of a novel, non-existent entity.</p>
<p>Very High Entropy</p>
<p>Involves designing and implementing new, complex algorithms and systems from scratch.</p>
<p>Very High Entropy</p>
<p>The new tool must reliably and robustly interact with the complex physical or computational world.</p>
<p>Very High</p>
<p>Combines the creative difficulty of hypothesis discovery with the engineering challenge of building a robust, functional system.</p>
<p>Results Analysis &amp; Refinement</p>
<p>Low Entropy</p>
<p>The intent is more open than tool use (e.g., "find meaningful patterns in the data").</p>
<p>Low Entropy</p>
<p>Requires generating interpretations, summaries, and insights, allowing for more linguistic flexibility.</p>
<p>Low Entropy</p>
<p>Involves writing analysis scripts that are more complex than simple API calls but often rely on standard libraries.</p>
<p>Low Entropy</p>
<p>Raw experimental data often contains noise and artifacts, introducing uncertainty into the interpretation process.</p>
<p>Low</p>
<p>The task is constrained by the provided data, but requires sophisticated reasoning to interpret results and suggest refinements.</p>
<p>Information Entropy, representing the profound uncertainty at the outset of any novel inquiry.As a pure, unarticulated concept, it is not a falsifiable proposition about the world; therefore, its Verifiability is almost zero.</p>
<p>Natural Language is the primary medium for structuring this high-entropy intent into a more tractable form.The properties of a natural language object are highly context-dependent.During creative phases like hypothesis generation, its Entropy is extremely high to allow for exploration.During planning or command phases, an agent's task is to generate a low-entropy, unambiguous statement.The Verifiability of a natural language artifact, such as a research plan, is based on its logical consistency and can be assessed against an existing knowledge base.</p>
<p>Computer Language is the medium for formal, unambiguous representation and execution.The computer language discussed in this context is primarily presented as code.Generating a computer language object, such as a query, a simulation script, or a new tool, is a process of significant entropy reduction because it transforms the ambiguity of natural language into a deterministic form.A computer language artifact possesses a high degree of Verifiability; it can be formally tested against a specification to determine if its behavior is correct.</p>
<p>Physical Information is the raw, empirical data gathered directly from the physical world.It may include measurements, signals, and experimental observations, etc.This is different from physics information, which refers to theoretical knowledge and concepts within the discipline of physics itself.Such data plays a dual role in scientific discoveries in the sense that existing datasets serve as an input for generating hypotheses, while new experimental data provides the output for testing them.Although the physical process generating the data may have high entropy due to factors like random noise or complexity, the recorded data itself is a factual observation.Consequently, physical information possesses the highest degree of Verifiability, which is formally determined by statistical significance and reproducibility.This verifiable data constitutes the evidence required to evaluate the original hypothesis.</p>
<p>An Analysis of Information Across Autonomous Scientific Discovery Phases</p>
<p>Using an information theory framework, we evaluate the entropy of different types of information, including human intent, natural language, computer language, and physical information within each phase, linking this to the difficulty of automation (dissipation).As shown in Table 2 and Figure 5 and 6, our findings reveal which phases are the most challenging to automate and which are the easiest.Figure 5: A heatmap representation of information analysis across autonomous scientific discovery phases.</p>
<p>We will delve into the unique challenges and information characteristics of each phase, providing a blueprint for building a fully autonomous scientific discovery system.</p>
<p>Hypothesis Discovery is characterized by high levels of uncertainty, exhibiting very high entropy in Human Intent and Natural Language.The initial intent is abstract and open-ended, requiring significant linguistic creativity to articulate novel concepts by synthesizing disparate ideas.Although the computer language used for auxiliary data mining is less complex, the reliance on vast, unstructured historical data also contributes to high entropy.The immense initial uncertainty and the need to sift through a vast space of possibilities to find a single valuable insight lead to high overall dissipation, making this a fundamentally creative and challenging phase to automate.</p>
<p>Experimental Design serves as a crucial bridge from abstract concepts to concrete actions.The entropy across all types of information is medium to high.The creative intent to design a novel experiment from numerous possibilities and the need to translate that into a precise natural language protocol contribute to this uncertainty.Furthermore, orchestrating existing computational tools for simulation and modeling the inherent uncertainties of real-world physical laws adds to the complexity.This effort to structure an abstract idea into a concrete plan involves navigating a large but constrained search space, leading to medium overall dissipation.</p>
<p>Tool Use represents the most structured and deterministic phase of the scientific workflow.It is defined by low or very low entropy across all information categories.The intent is highly specific, the natural language used is typically a simple command, and the computer language required is often a direct, deterministic API call or function.Because the process is predictable and the information involved is unambiguous, the overall dissipation is very low, making this the most straightforward phase to automate reliably.</p>
<p>Tool Creation stands as the pinnacle of complexity in this framework, exhibiting very high entropy across every information category.The process begins with the highly abstract intent to solve a problem for which no tool currently exists.It requires immense creativity to describe the principles of a novel system in natural language and to design and implement new, complex algorithms in computer language.The resulting tool must also reliably interact with the complex physical or computational world.This combination of high-entropy creative demands and rigorous engineering challenges results in very high overall dissipation, marking it as the most difficult phase to fully automate.Results Analysis and Refinement is a phase that operates under the strong constraints of the data generated from experiments.This grounding in data leads to low entropy across the information types involved.The intent is to find patterns within a defined dataset, and the natural language and computer language required are for interpreting and processing this data, not for open-ended creation.While this phase requires sophisticated reasoning to generate insights and suggest refinements, the task is one of inference within a bounded context rather than unconstrained exploration.This grounding in specific data results in low overall dissipation.</p>
<p>Different Levels of Autonomous Agents for Scientific Discovery</p>
<p>Figure 7 illustrates a five-level framework for scientific agents, detailing a clear progression from human-centric research to full AI autonomy across five key dimensions: Hypothesis Discovery, Experimental Design, Tool Use, Tool Creation, and Analysis and Refinement.</p>
<p>The proposed five-level framework provides a more granular and clearly delineated model of AI autonomy in science compared to existing role-based classifications.Frameworks such as those by Zheng et al. [2025], Gao et al. [2024], Wei et al. [2025a] define levels based on the holistic role or overall capability of the AI system, making the boundaries between stages descriptive rather than precise.</p>
<p>In contrast, this new framework's strength lies in grounding the progression of autonomy in the core information-processing challenges of scientific discovery.The transition through the levels can be understood as an agent's increasing capacity to independently reduce information entropy, minimize dissipation, and systematically generate artifacts with high verifiability.This provides a more structured and measurable methodology for assessing an agent's true capabilities.</p>
<p>Level 1: Human-Led Model.At this foundational level, the agent's role is purely instrumental, confined to the lowest-entropy and lowest-dissipation tasks where the workflow is predictable (e.g., Tool Use).Humans bear the full cognitive load of managing the immense information entropy inherent in creative processes like Hypothesis Discovery and Tool Creation.The human is the sole engine for reducing this initial uncertainty and driving the process toward a verifiable outcome.</p>
<p>Level 2: AI-Augmented Model.The agent evolves into a supportive partner, beginning to assist in higher-entropy domains.While humans still lead, the AI collaborates in Hypothesis Discovery and</p>
<p>More Autonomy</p>
<p>Figure 7: A five-level framework for classifying the autonomy of scientific agents.The framework progresses from a Human-Led Model (Level 1), where the agent is confined to simple, low-entropy tasks, to an AI-Augmented Model (Level 2) and a Full Human-AI Collaboration Model (Level 3), where the agent's role expands to that of a supportive partner.A significant shift occurs in the AI-Led Hybrid Model (Level 4), where the agent takes the lead on most core scientific tasks, with humans collaborating on the most complex challenges.The final stage is Full AI Autonomy (Level 5), where the agent can manage the entire scientific process from abstract ideation to validated discovery, functioning as a truly autonomous researcher.</p>
<p>Experimental Design, helping to explore the large possibility spaces.However, it relies on human guidance to navigate the high dissipation (the cost of exploring unproductive paths) and converge on a viable research direction.</p>
<p>Level 3: Full Human-AI Collaboration Model.This level is characterized by a comprehensive partnership where every task is performed collaboratively.The agent is now capable of functioning effectively across the entire entropy spectrum, from creative ideation to data analysis.It acts as a force multiplier, jointly tackling high-entropy challenges with the human.However, it does not yet possess the autonomy to lead the process or manage the most uncertain, dissipative phases on its own.</p>
<p>Level 4: AI-Led Hybrid Model.A critical threshold is crossed as agents take lead in autonomously reducing entropy for core scientific tasks.It independently drives Hypothesis Discovery, Experimental Design, and Tool Use, demonstrating a sophisticated ability to navigate vast and unstructured information spaces.Human collaboration remains essential for the task with the absolute highest entropy and dissipation (Tool Creation) and for the final step of analysis and refinement, where human judgment ensures the ultimate verifiability of findings.</p>
<p>Level 5: Full AI Autonomy Model.This represents the pinnacle of autonomous science.The agent possesses complete mastery over the entire information lifecycle.A Level 5 agent can independently manage the workflow from start to finish: confronting the maximum information entropy of an abstract scientific goal, navigating the extreme dissipation inherent in creating novel tools from scratch, and producing validated, high verifiability scientific facts.At this stage, the agent operates as a truly autonomous scientific researcher.</p>
<p>By defining the levels based on how an agent handles these fundamental properties of information, this framework moves beyond coarse role descriptions.The boundary between levels is marked by a concrete, observable shift in the agent's ability to process uncertainty and create knowledge, offering a more robust and detailed methodology for measuring progress in the field of autonomous science.</p>
<p>Passive Knowledge Injection</p>
<p>Hypothesis Discovery</p>
<p>Hypothesis Discovery is a crucial creative process in scientific discovery, with the core objective of identifying and forming novel, verifiable scientific hypotheses from vast amounts of data and existing knowledge.This process aims to accelerate the pace of knowledge discovery by revealing hidden connections that are difficult for human researchers to find alone [Swanson, 1986].With the rise of Large Language Models (LLMs), the level of automation and efficiency in hypothesis discovery has been enhanced to an unprecedented degree[He and Chen, 2025, Luo et al., 2025a].A complete hypothesis discovery workflow typically includes the following three key stages: Knowledge Extraction, Hypothesis Generation, and Hypothesis Screening and Validation (Figure 8).</p>
<p>Knowledge Extraction</p>
<p>Knowledge Extraction is the cornerstone of the hypothesis discovery pipeline.Its core task is to automatically identify, extract, and structure key information from large volumes of unstructured data, especially scientific literature, to lay the groundwork for subsequent hypothesis generation and validation.</p>
<p>Traditional knowledge extraction methods involve a labor-intensive, bottom-up pipeline using techniques like Named Entity Recognition (NER) and Relation Extraction (RE) to build static, structured Knowledge Graphs (KGs) from unstructured text [Nickel et al., 2015, Ji et al., 2021].In contrast, modern paradigms leverage the vast implicit knowledge already embedded within Large Language Models (LLMs).This shift moves away from building fixed knowledge bases and instead focuses on dynamically activating and applying the LLM's built-in reasoning capabilities.Crucially, this transition reframes the entire process: multi-step extraction tasks are now often reshaped into a single, end-to-end generative task, where the LLM directly generates the desired structured knowledge.</p>
<p>Scientific Foundation Models.To overcome the limitations of general models in specialized fields, expert models are created by training or fine-tuning them on vast, domain-specific scientific data [Gururangan et al., 2020, Pyzer-Knapp et al., 2025, Zhao et al., 2025, Subramanian et al., 2023, Barman et al., 2025, Narayanan et al., 2025].For instance, models like BioBERT are pre-trained on biomedical texts, enabling them to more accurately extract specific entities like genes, diseases, and chemicals, as well as the relationships between them from research articles [Lee et al., 2020].Similarly, generative models like BioGPT can not only extract but also summarize complex biological information coherently [Luo et al., 2022].On a broader scale, models such as Galactica were designed with the explicit goal of structuring and organizing scientific knowledge from papers, lecture notes, and textbooks, essentially performing knowledge extraction on a massive scale [Taylor et al., 2022].By internalizing the nuances of a specific scientific language, these models serve as far more effective tools for building accurate and comprehensive knowledge bases from technical documents.</p>
<p>Retrieval-Augmented Generation (RAG).Retrieval-Augmented Generation (RAG) is a technique designed to address the critical issues of outdated knowledge and hallucination in LLMs by connecting the model to external knowledge sources [Lewis et al., 2020, Guu et al., 2020, Gao et al., 2023, Izacard et al., 2023, Fan et al., 2024].In the context of knowledge extraction, its primary function is to transform vast, unstructured scientific literature into a verified and structured knowledge base [Garcia et al., 2024, Lopez et al., 2025, Feng et al., 2025a, Krotkov et al., 2025].By retrieving relevant documents and using them as a direct source, RAG generates factually-grounded and traceable outputs, such as precise summaries or extracted entity-relationship pairs, which serve as the reliable foundation for subsequent analysis and discovery [Lee et al., 2025, Xie et al., 2024, Gao et al., 2025a].</p>
<p>Multimodal Knowledge Extraction.With the advent of multimodal models and agents that can invoke analysis tools, knowledge extraction from scientific literature is no longer confined to text [Khalighinejad et al., 2024, Odobesku et al., 2025].For instance, systems like ChemMiner leverage multimodal capabilities to extract chemical information from both text and diagrams within papers [Chen et al., 2024a], while ChartAssistant demonstrates the ability to reverse-engineer scientific charts by converting visual data back into structured tables [Meng et al., 2024].Similarly, the use of Multimodal Models, such as GPT-4o, enables the processing of complex biomedical documents by integrating information across text, tables, and diagrams to answer questions [Hurst et al., 2024].</p>
<p>Hypothesis Generation</p>
<p>Hypothesis generation is the core of hypothesis discovery, aiming to formulate plausible and testable statements that explain observed phenomena or predict outcomes under certain conditions.In scientific practice, it is a creative yet structured step that bridges observation and prediction.A good hypothesis should be grounded in existing knowledge and make predictions that can be empirically tested and potentially proven wrong.Traditionally, scientists generate hypotheses by identifying gaps or contradictions from previous literature, drawing analogies from other domains, and applying domain expertise to reason.Recent advancements are progressing beyond classic literature-based discovery (LBD) [Swanson, 1986] to develop dynamic, agentic systems capable of independently reading, reasoning, and iteratively proposing sophisticated, testable conjectures.This evolution in methodology can be categorized into four key approaches:</p>
<p>Prompt-Based Systems.At its most fundamental level, hypothesis generation using LLMs can be achieved through prompt-based methods.This approach directly leverages the extensive knowledge an LLM acquires during its pre-training phase.By formulating a carefully constructed prompt, a user can query the model to propose hypotheses in a zero-shot fashion [Qi et al., 2023, Zhou et al., 2024].Early evidence shows LLMs can act as zero-shot hypothesis proposers in biomedicine [Qi et al., 2023], relying on their pre-trained knowledge alone to formulate ideas.The efficacy of this method is contingent upon the LLM's internal representation of scientific concepts and relationships.While this allows for the exploration of a broad range of ideas, it also introduces a susceptibility to generating plausible but factually incorrect statements or restating known facts [Xiong et al., 2025].</p>
<p>Knowledge-Grounded Systems.To enhance the factual accuracy and novelty of generated hypotheses, knowledge-grounded methods have been developed.These techniques augment the LLM's internal knowledge with external, authoritative information.One prominent technique is Retrieval-Augmented Generation (RAG), which dynamically retrieves unstructured textual evidence from literature.By synthesizing information from multiple, potentially disparate sources, it enables the model to infer novel connections and propose innovative, evidence-backed hypotheses [Bazgir et al., 2025b].Another powerful technique involves the injection of structured knowledge, often from curated knowledge graphs (KGs).KG-CoI [Xiong et al., 2024], for example, utilizes structured knowledge sources such as knowledge graphs (KGs) to ground its hypotheses, improving the accuracy of its reasoning chains and reducing factual errors.ResearchLink [Borrego et al., 2025], for example, integrates graph embeddings, path-based features, and bibliometric data to generate cross-domain hypotheses with high precision.Moreover, a recent study [Tong et al., 2024] presents a novel framework that combines large language models (LLMs) with causal knowledge graphs to generate psychological hypotheses.By analyzing over 43,000 psychology articles, the approach produced hypotheses on well-being that matched expert-level novelty and significantly outperformed LLM-only methods.</p>
<p>Multi-Agent Systems.To broaden the scope of hypothesis generation, multi-agent frameworks emulate collective brainstorming by leveraging diversity from multiple sources [Wu et al., 2023, Chase, 2022].This approach generates a richer pool of initial ideas by either utilizing different foundation models as distinct agents or by assigning different roles and prompts to instances of the same model.These varied configurations steer the models to generate outputs from different conceptual angles by activating distinct generative distributions.By synthesizing ideas from these diverse agents, the framework effectively uncovers less obvious connections and produces a more comprehensive set of potential hypotheses than a monolithic approach could achieve alone [Chen et al., 2025b, Bazgir et al., 2025b].For example, systems such as AI co-scientist [Gottweis et al., 2025a] employ a proposer-critic dynamic, where some agents generate initial ideas while others rigorously challenge their assumptions to refine them through interaction.This paradigm is implemented in specialized frameworks like ACCELMAT for materials science [Kumbhar et al., 2025a], which uses a structured, iterative loop of proposal and critique among multiple agents to progressively enhance the quality of novel material hypotheses.VIRSCI [Su et al., 2024] extends this by simulating scientific teams using real-world academic data, enabling agents to form collaborative research teams and generate novel ideas through inter-and intra-team discussion mechanisms.Similarly, AstroAgents [Saeedi et al., 2025] deploys domain-specific agents to interpret mass spectrometry data and hypothesize about prebiotic chemical pathways related to the origins of life, with more than 30% of hypotheses validated as scientifically plausible by expert reviewers.</p>
<p>Evolutionary Algorithm-Based Systems.For a dynamic and automated LLM-based approach to discovery, closed-loop systems apply principles from evolutionary algorithms to treat hypothesis generation as an optimization problem.An initial population of hypotheses undergoes iterative refinement using operators such as mutation and recombination, while a fitness function evaluates the quality of the hypothesis to guide the selection.This paradigm is exemplified by systems like MOOSE-Chem [Yang et al., 2025], which uses evolutionary search to guide different ways of associating inspirations and background to generate hypotheses.This combination ensures that generated hypotheses are not only optimized but also validated for novelty against the knowledge baseline established during the LLM's pretraining.Similarly, HypoAgents [Duan et al., 2025] integrate Bayesian updating within an evolutionary loop: agents propose hypotheses, test them via RAG-informed evidence, update probabilities, and refine uncertain hypotheses in a continuous scientific feedback cycle.MolLEO [Wang et al., 2024b] leverages LLMs directly as mutation and recombination operators, prompted with text-based instructions and desirable target properties.LLMbased evolutionary systems show immense promise for navigating vast combinatorial spaces [Liu et al., 2025b] with textual information extraction or rich, semantic objectives, such as discovering new materials [Jia et al., 2024], mechanical structures [Jadhav and Farimani, 2024], molecules [Wang et al., 2024b] or macromolecules [Reinhart and Statt, 2024].They also show potential for multi-objective numerical optimization [Liu et al., 2024e], which includes real-world problems like optimizing nozzle design, heat transfer or wind farm layout [Brahmachary et al., 2025].</p>
<p>Hypothesis Screening and Validation</p>
<p>Rigorous screening and validation of automatically generated hypotheses rely on a clear set of evaluation axes, such as plausibility, novelty, feasibility, and cost [Alkan et al., 2025, Bazgir et al., 2025c].This step is necessary to manage the large volume of generated ideas and requires transparent assessment methods to avoid over-claiming [Beel et al., 2025].Methodologies for this stage can be broadly classified into two approaches: automated evaluation based on defined metrics and dynamic refinement through agent-based critique.</p>
<p>Metric-Based Screening.This approach focuses on developing automated, scalable, and objective criteria to evaluate hypotheses.Some frameworks implement formal statistical controls; for example, POPPER introduces an automated validation framework that achieves human-level accuracy at significantly greater speed by applying rigorous statistical checks [Huang et al., 2025a].Other methods focus on specific metrics like novelty, where systems such as SCIMON explicitly compare new ideas against existing literature and revise them until they no longer resemble prior work [Wang et al., 2024c].To enhance external validity and enable consistent comparisons, standardized benchmarks are emerging.These include task-specific suites like MATDESIGN for materials science, which introduces scalable quality metrics [Kumbhar et al., 2025b], and general platforms like HypoBench, which provides a main benchmark for hypothesis discovery across diverse tasks [Liu et al., 2025a].Beyond these, recent approaches employ LLM-derived priors for automatic hypothesis assessment.In particular, the Logit-based Calibrated Prior technique [Gong and Fernandez, 2025] uses LLMgenerated expectations to quantify how surprising a correlation is, enabling ranking of hypotheses by novelty and relevance with high accuracy in real-world data.Meanwhile, Bayesian frameworks have been deployed for evaluating model capabilities [Xiao et al., 2025], treating performance assessment as a hypothesis testing task under uncertainty-an approach that introduces probabilistic robustness when sample sizes are limited.</p>
<p>Agent-Based Validation.This approach simulates the collaborative and critical processes of scientific inquiry, where agents interact to challenge and improve hypotheses.Frameworks like ResearchAgent facilitate an iterative dialogue between a proposal-generating agent and a panel of reviewer agents.This process mimics peer review, refining the hypothesis until it meets rigorous criteria [Baek et al., 2024].Similarly, to ensure empirical testability, other systems close the loop between theory and evidence.The Scientific Generative Agent (SGA), for instance, combines an LLM acting as a theorist with scientific simulators that function as experimental systems.This allows the agent to immediately test and revise its hypotheses based on simulated experimental outcomes, ensuring ongoing refinement based on feedback [Ma et al., 2024].</p>
<p>Experimental Design and Execution</p>
<p>In this section, we analyze the critical phases of Experimental Design and Execution within the autonomous scientific discovery process.As depicted in Figure 9, we first examine how LLM agents generate robust experimental plans by integrating high-level human guidance, leveraging external knowledge sources via Retrieval-Augmented Generation (RAG), and incorporating feedback from prior experiments.We then detail the Experimental Execution phase, focusing on the various strategies, from highly structured to dynamically collaborative, that agents use to interact with and manage external tools, bridging the gap between abstract plans and physical reality.</p>
<p>Experimental Design</p>
<p>A critical step in scientific discovery, following hypothesis generation, is experimental design.This involves creating a structured plan to systematically test ideas and achieve research objectives.The process is fundamentally a form of workflow generation, where high-level scientific goals are translated into concrete, executable protocols.LLM-based scientific agents are increasingly used for this task, combining natural language reasoning with planning capabilities, often enhanced by retrieval tools and physical knowledge, to generate coherent and detailed experimental plans.</p>
<p>The core challenge for LLM agents in experimental design is navigating a nearly infinite space of possible action sequences to find a solution that is both scientifically sound and practically feasible.To address this, agents employ various strategies to effectively constrain and navigate this complex space.[Huang et al., 2025b, Zhou et al., 2025a, Cao et al., 2024, Yu et al., 2024a] These strategies can be grouped into four main categories: RAG for Grounded Planning, Human High-Level Guidance, Templates and Predefined Actions, Post-Execution Feedback.</p>
<p>Retrieval-Augmented Generation for Grounded Design.This strategy ensures that an agent's plan is scientifically valid from the outset by grounding its reasoning in external, reliable knowledge sources.Instead of relying solely on its internal training, the agent uses a Retrieval-Augmented Generation (RAG) approach to query specialized tools, databases, and scientific literature.This allows the agent to build its experimental plan on a foundation of established facts and methods.</p>
<p>Biomni [Huang et al., 2025b] first creates a comprehensive action space by automatically mining tens of thousands of scientific papers for relevant tools and databases.When given a task, it retrieves the most pertinent tools from this space to construct its plan.TxAgent [Gao et al., 2025b] uses a specialized retrieval model called ToolRAG to select the most appropriate tools from its ToolUniverse of 211 clinical instruments, ensuring its therapeutic reasoning is based on evidence.Chemist-X [Chen et al., 2023a] leverages RAG to search literature and molecular databases to define a promising and constrained search space for reaction condition optimization before any physical experiments are run.CLADD [Lee et al., 2025] has a Planning Team which retrieves the most relevant resources from  previous execution cycles as a critical feedback mechanism for refinement.For the Experimental Execution phase, the agent acts upon the detailed workflow, deciding between using existing instruments (Tool Use) or developing new ones (Tool Creation).Tool use can be implemented through four primary strategies for tool interaction: Embedded, Toolbox-Based, Reflective and Iterative, and Hierarchical.</p>
<p>databases and tools.The Molecule Understanding Team then generate structured reports based on these external sources which are synthesized by a Prediction Agent.</p>
<p>Human High-Level Guidance.For complex or novel scientific challenges, LLMs may struggle to generate a high-quality experimental design on its own.As noted in some studies [Zhou et al., 2025a, Qu et al., 2025], human expertise is often crucial to refining and validating the agent plan.In this collaborative model, the human scientist acts as a supervisor, providing high-level guidance and feedback to guide the agent's decision-making process.</p>
<p>MAPPS [Zhou et al., 2025a] framework includes a Scientific Mediator module.This component allows human scientists to interact with the agent, providing feedback, offering guidance, and making corrections to the experimental plan, ensuring that the agent's autonomous discovery process remains aligned with scientific principles and the researcher's goals.AI co-scientist [Gottweis et al., 2025a] introduces a scientist-in-the-loop paradigm, where human scientists actively collaborate with agents throughout the research cycle.Human scientists interact with the system by specifying a research goal, suggesting their own ideas, providing feedback and reviews.In Virtual Lab [Swanson et al., 2024], The human researcher defines the Principal Investigator (PI) and Scientific Critic agents by specifying their title, expertise, goal, and role, and sets the agenda for each meeting, which guides the agents to further design experiments and solutions.</p>
<p>Human-Provided Templates and Predefined Actions This is a more structured form of human-AI collaboration where humans define the fundamental building blocks of an experiment, and the LLM's role is to assemble them into a coherent workflow.Humans may provide a set of allowed "meta-actions" in an action pool or create high-level workflow templates.The agent then fills in the specific parameters and details.This approach often involves modeling the experiment as a state machine, where the agent's primary task is to establish the rules for transitioning between predefined experimental states.</p>
<p>The k-agents [Cao et al., 2024] framework exemplifies this by having an execution agent that decomposes a human-provided experimental procedure into a state machine.The agent then interacts with other knowledge agents to execute each state and uses the results to determine the next transition, enabling closed-loop control.CRISPR-GPT [Qu et al., 2025] utilizes expert-predefined meta-tasks as high-level templates, while its LLM planner decomposes the user's high-level request and links it to corresponding state machines to assemble a complete, executable workflow.PerTurboAgent [Hao et al., 2025] operates using a predefined Action Pool containing high-level actions like predict, reflect, and refine.The agent designs its experiment by selecting and sequencing these actions to decide which genes to perturb in each round.</p>
<p>Post-Execution Feedback.This strategy does not require a perfect plan from the start.Instead, the agent's design process is an exploratory one, where it learns and adapts by observing the results of its actions.This feedback can be used to make immediate, step-by-step corrections or to inform the design of the next major experimental cycle.</p>
<p>Agents like ChemToolAgent [Yu et al., 2024a] use the ReAct loop to solve chemistry problems by calling a tool, observing the output, and planning the subsequent action based on that result.</p>
<p>BioInformatics Agent [Xin et al., 2024] uses a self-correction loop; if an analysis workflow's initial output doesn't meet user expectations, it modifies the entire workflow and re-executes it.</p>
<p>BioDiscoveryAgent [Roohani et al., 2024] operates in a closed loop, using the results from one round of genetic perturbations to reason about which genes are most promising to test in the subsequent round.OSDA Agent [Hu et al., 2025] adopts an Actor-Evaluator-Reflection loop, where an LLMbased Actor generates candidate OSDA molecules, which are subsequently assessed by an Evaluator through chemical validity checks.A Self-reflector module then integrates this feedback to guide the iterative refinement of the Actor's outputs.</p>
<p>Experimental Execution</p>
<p>The experimental execution stage is a central phase in the scientific discovery process, serving as the crucial bridge that transforms abstract strategies into the concrete actions and empirical evidence needed for generating insights.It involves the systematic implementation of a designed workflow, which extends beyond simply running code to include a range of complex tasks: orchestrating computational resources, managing large datasets, interfacing with laboratory instruments or simulators, and continuously monitoring the workflow to ensure its correctness, efficiency, and reproducibility.LLM-based agents operate at the intersection of natural language instructions, computer language, and physical information, enabling a fluid transition from concept to execution.The execution of planned experiments by LLM agents is primarily achieved through two interconnected capabilities, namely tool use and tool creation.</p>
<p>Tool Use</p>
<p>For LLM-based agents to be effective in science, the ability to use tools is essential.First, generalpurpose tools like search engines allow agents to survey the dynamic landscape of scientific research.This ensures their work is grounded in the most recent findings and established principles.More critically, since LLMs are fundamentally text-based models and lack the ability to directly interact with or measure the physical world, they must rely on domain-specific tools for analysis and exploration.By interfacing with chemical simulation software, data analysis pipelines, or robot lab equipment, agents can bridge the gap between text-based reasoning and empirical science.Over the past few years, several distinct strategies for integrating agents with external tools have emerged, representing a spectrum of autonomy from highly controlled pipelines to dynamic, collaborative systems.[Hu et al., 2025, Swanson et al., 2024, Jin et al., 2024a, Yu et al., 2024a, Huang et al., 2025b] To provide a structured view, we categorize these strategies into four representative models of tool use: Embedded Tool Use, Toolbox-based Tool Use, Reflective &amp; Iterative Tool Use, and Hierarchical Tool Use in Multi-Agent Systems.</p>
<p>Embedded Tool Use.The Embedded Tool Use method represents a structured and highly controlled approach to integrating AI with scientific workflows.In this model, developers pre-define a complete research pipeline where specific tools are hard-coded to execute at particular stages.The Large Language Model (LLM) acts not as a decision-maker for tool selection, but rather as a component within the workflow.Its role is often confined to interpreting the output of a tool, generating code for a specific, predetermined step, or translating human instructions into parameters for the next tool in the sequence.This method ensures reliability and reproducibility, making it well-suited for routine, high-throughput analyses where the scientific process is already well-established.</p>
<p>For example, OSDA Agent [Hu et al., 2025] embeds the LLM into a fixed "Actor-Evaluator-Reflector" loop, where the LLM acts as a molecule generator and reflector, while a set of hard-coded computational chemistry tools serves as the evaluator to validate its output.LLMatDesign [Jia et al., 2024] adopts a pre-set iterative design loop, in which the LLM is responsible for proposing material modification plans and reflecting on the results, but the specific structure relaxation and property prediction are executed by hard-coded Machine Learning Force Field (MLFF) and Machine Learning Property Predictor (MLPP) tools.MatLLMSearch [Gan et al., 2025] embeds the LLM into a fixed evolutionary algorithm framework, where its role is confined to the stage to generate new crystal structure candidates, which are then passed to a preset evaluation pipeline composed of tools like MLFF and DFT.scBaseCount [Youngblut et al., 2025] embeds the LLM in a fixed pipeline for large-scale single-cell data retrieval, preprocessing, normalization and formatting, to create a unified repository for single-cell RNA-seq data.Chemist-X [Chen et al., 2023a] embeds the LLM to interact with pre-packaged API functions including CAD tools and ML models.</p>
<p>Toolbox-Based Tool Use.This method empowers the AI agent with significantly more autonomy.</p>
<p>The agent is provided with a toolbox or a library of available functions.Each tool is accompanied by a natural language description of its purpose, inputs, and outputs.The LLM leverages its reasoning capabilities to interpret a user's high-level goal, decompose it into smaller steps, and select the most appropriate tool from the toolbox to accomplish each step.This approach transforms the LLM from a simple component into a central coordinator, capable of orchestrating complex sequences of operations.</p>
<p>This approach is demonstrated in works such as AgentMD [Jin et al., 2024a] is a clinical agent that curates and applies medical risk calculators by selecting from 2,164 mined clinical tools.Chem-Crow [M.Bran et al., 2024] bridges drug discovery and materials science using 18 expert-designed tools.scAgent [Mao et al., 2025] contains a tool hub offers over 30 plugins that centralizes diverse single-cell analysis methods into a unified interface.TxAgent [Gao et al., 2025b] leverages a broad Tool Universe, spanning drug databases, molecular analysis, and clinical resources, which allows the agent to dynamically select and integrate multiple tools for therapeutic reasoning.GIS Copilot [Akinboyewa et al., 2025] integrates Agent into QGIS to autonomously conduct spatial analysis.Equipped with comprehensive tool documentation and external libraries, it enables the agent to dynamically dynamically select, generate, and execute geospatial analysis workflows.SciToolAgent [Chen et al., 2025a] builds a Scientific Tool Knowledge Graph that maps the intricate relationships, dependencies, and compatibilities among a vast library of scientific tools , which allows an LLM to create an optimal Chain-of-Tools tailored for a specific scientific query.This graph-based toolbox overcomes the limitations of previous agents that struggled with integrating a large and diverse toolset.</p>
<p>Reflective and Iterative Tool Use.This approach moves beyond simple tool selection, enabling the agent to reflect on the results of each tool use and dynamically plan its next steps.This process mimics the human approach to problem-solving: try, observe, reflect, and then decide what to do next.The ReAct [Yao et al., 2023] (Reasoning and Acting) framework is a prime example of this methodology.It operates in a continuous loop of Thought-Action-Observation.Similarly, the CodeAct [Wang et al., 2024e] framework is a specialized extension of ReAct.Here, the agent's actions involve generating and executing code in a sandboxed environment.After execution, the agent observes the code's output or any error messages.If the code fails, the agent reflects on the error, iteratively generates a corrected version, and tries again.</p>
<p>ChemToolAgent [Yu et al., 2024b] is a prime example, using the ReAct framework to orchestrate a suite of 29 different tools, from searching the PubChem database to predicting molecular properties.</p>
<p>It can reason about a user's chemistry question, call the right tool, and then analyze the output to decide the next logical step, like performing another calculation or synthesizing the results into a final answer.Biomni [Huang et al., 2025b] is a general-purpose biomedical AI agent that automates complex data analysis by leveraging the CodeAct framework.This code-based iterative method allows Biomni to navigate vast, interconnected knowledge domains with the flexibility of a virtual biologist, writing, running, observing, and debugging code to accomplish its goals.PerTurboAgent [Hao et al., 2025] is an agent for sequential Perturb-seq.At each round, the agent integrates prior knowledge, perturbation-effect models, and past experimental results to reflect on outcomes, update predictions, and propose the next gene panel.This closed-loop process adaptively prioritizes perturbations with the highest phenotypic impact.</p>
<p>Hierarchical Tool Use in Multi-Agent Systems.As the complexity of scientific inquiry increases, a single agent may struggle to manage an overwhelmingly large and diverse toolbox.The hierarchical delegation method addresses this by employing a team of specialized AI agents.A high-level planner agent first deconstructs a complex problem into sub-tasks.These sub-tasks are then delegated to different expert agents, each equipped with its own smaller, specialized toolbox.This division of labor makes the tool selection process more manageable and robust, mirroring the structure of human scientific teams.</p>
<p>For example, the Team of AI-made Scientists (TAIS) [Liu et al., 2024a] framework simulates a research team by assigning LLM agents specialized roles such as planner, data handler, and analyst.Similarly, MedAgents [Tang et al., 2023] uses a collaborative approach where distinct medical expert agents, each with specialized tools, iterate to reach a consensus diagnosis.In chemistry, ChemAgents [Song et al., 2025] employs a hierarchical system with five specialized agents, including a Literature Reader and Robot Operator.In materials, AtomAgent [Ghafarollahi and Buehler, 2024b] collaborate across knowledge retrieval, multimodal data analysis, and molecular simulations, autonomously orchestrating alloy design workflows and accelerating data-driven discovery of highperformance materials.For protein design, ProtAgents [Ghafarollahi and Buehler, 2024a] deploys multiple agents with distinct skills that dynamically collaborate on engineering tasks.The "ChatGPT Research Group" [Zheng et al., 2023] integrates seven specialized assistants for MOF/COF synthesis, with agents dedicated to planning, literature mining, and robot operation.In antiviral antibody design, Virtual Lab [Swanson et al., 2024] introduces a multi-agent system where a PI agent coordinates scientist agents using the protein language model (PLM) ESM [Lin et al., 2023], the protein folding model AlphaFold-Multimer [Abramson et al., 2024], and the computational biology software Rosetta [Boorla et al., 2023], generating 92 nanobody candidates and experimentally validating two with improved binding to SARS-CoV-2 variants.In physics, the k-agents framework [Cao et al., 2024] enables an autonomous quantum laboratory where agents plan and execute experiments on quantum processors.</p>
<p>Tool Creation</p>
<p>While many AI agents demonstrate a remarkable ability to use existing software libraries and digital tools, a new frontier is emerging where their primary function shifts from application to invention.The most advanced scientific agents are transcending mere tool orchestration to create entirely new scientific tools and algorithms.This paradigm shift marks the evolution of AI from a digital assistant that follows a playbook to a creative partner capable of contributing novel methods and solutions to complex scientific problems.</p>
<p>At the forefront of the creative paradigm are agents that generate code not just to execute a workflow, but to discover new methods.ToolUniverse [Gao et al., 2025c] is designed to democratize the power of AI scientists by enabling LLM agents to not only execute workflows with a comprehensive suite of existing tools, but also autonomously invent, implement, and integrate new, reusable scientific algorithms, transforming agents from mere users to creative toolmakers.MAPPS [Zhou et al., 2025a] represents a powerful hybrid approach that blends the use of existing tools with the creation of new algorithms.Its code generation is twofold: it generates code to use established physics-based foundation models for analysis, but more importantly, it simultaneously invents and implements new algorithms tailored to discover novel materials more efficiently.CodePDE [Li et al., 2025a] iteratively generates, tests, and refines code to produce high-performance solvers for partial differential equations (PDEs).The final output is not just the solution to a single problem, but a robust, reusable solver that constitutes a new and valuable scientific tool.AlphaEvolve [Novikov et al., 2025] takes this concept further by employing an evolutionary framework.It treats a population of algorithms as a gene pool and uses an LLM to "mutate" the code, iteratively evolving better solutions.This process has led to the discovery of entirely new algorithms that are more efficient than those designed by humans, showcasing code generation as a mechanism for pure algorithmic invention [Nagda et al., 2025].</p>
<p>ShinkaEvolve [Lange et al., 2025] similarly evolves programs, improving sample efficiency through exploration-exploitation-balanced parent selection, novelty-based program rejection sampling, and bandit-based, task-dependent LLM prioritization.TOOLMAKER [Wölflein et al., 2025] introduces an autonomous framework for transforming scientific code repositories into LLM-compatible tools, enabling agents to not only employ but also systematically generate new utilities, with cross-domain evaluations demonstrating substantially higher reliability than prior software-engineering agents.</p>
<p>5 Result Analysis and Refinement</p>
<p>Result Analysis</p>
<p>Experimental result analysis in scientific discovery is the critical process where autonomous agents interpret raw outputs-such as numerical data, plots, and images-to derive meaningful scientific insights [Gridach et al., 2025b].This stage presents significant challenges, including the need to handle complex, multimodal data types and ensure the analysis is scientifically robust [Liu et al., 2025c, Zhang et al., 2024d].The approaches to this task are classified into distinct paradigms based on the core mechanism the agent uses to interact with and process the data.</p>
<p>The analysis of experimental results by AI agents can be categorized into three primary paradigms, distinguished by the agent's core mechanism for interacting with and interpreting scientific data.These approaches are not mutually exclusive; rather, the most advanced systems often blend them to tackle complex, multi-faceted research problems.Table 3 presents a concise overview of three distinct approaches to analyzing and interpreting scientific results.</p>
<p>Modality-Driven Analysis.This paradigm is defined by its reliance on the advanced perceptual and interpretive capabilities of Multimodal Large Language Models (MLLMs) [Han et al., 2023, Meng et al., 2024, Hurst et al., 2024, Li et al., 2024a, Fu et al., 2024, Yan et al., 2024].The core mechanism involves the direct processing of non-textual data formats, such as images, charts, videos, and even audio, to extract scientific insights.Instead of relying on structured numerical inputs, the agent "sees" the data as a human researcher would.This approach is critical for disciplines where visual evidence is paramount.Representative systems like The AI Scientist-v2 [Yamada et al., 2025] leverage a Vision-Language Model (VLM) in a feedback loop to critique and refine generated plots, ensuring they are not only accurate but also clearly interpretable.Other specialized work, such as PlotGen [Luo et al., 2024], focuses on deconstructing scientific charts into their semantic components-axes, legends, data points-to perform detailed analysis and verification.</p>
<p>However, this paradigm faces significant limitations.The inherent heterogeneity of scientific data (e.g., diverse chart types, microscopy images, spectral graphs) poses a major processing challenge.Furthermore, the ambiguity of visual information can lead to misinterpretations, and the high computational cost of training and running large-scale MLLMs remains a practical barrier.</p>
<p>Tool-Augmented Analysis.As the most prevalent and mature paradigm, tool-augmented analysis centers on the agent's ability to interact with external, domain-specific resources, including software APIs, databases, and even physical hardware.This approach grounds the LLM's reasoning process in the deterministic and validated outputs of specialized tools, mitigating the risk of factual hallucination and enabling highly technical operations.For instance, ChemCrow [Bran et al., 2023] is equipped with a suite of chemistry tools for tasks like predicting reaction outcomes and searching molecular databases.Coscientist [Boiko et al., 2023] has demonstrated the ability to not only use bioinformatics software but also control liquid handling robots to execute experiments.Similarly, Virtual Lab [Boeck et al., 2023] follows a computational pipeline of specialized tools for nanobody design.</p>
<p>The primary limitations of this paradigm include the reliability of the external tools themselves, the risk of error propagation through a chain of tool calls, and practical challenges like API costs, latency, and the lack of interface standardization across the scientific software ecosystem.</p>
<p>Computation-Native Analysis.This paradigm leverages the agent's intrinsic ability to generate and execute code and perform symbolic reasoning.It treats data analysis as a computational task to be ChartLlama [Han et al., 2023], ChartAssisstant [Meng et al., 2024], The AI Scientist-v2 [Yamada et al., 2025],</p>
<p>PlotGen [Luo et al., 2024] Tool-Augmented Analysis Interaction with external APIs, software, or hardware Domain-specific data formats</p>
<p>ChemCrow [Bran et al., 2023], Coscientist [Boiko et al., 2023], Virtual Lab [Boeck et al., 2023] Computation-Native Analysis</p>
<p>General code generation/execution, symbolic reasoning</p>
<p>Tabular data, text, numerical arrays</p>
<p>Data Interpreter [Hong et al., 2024], LLM-SR [Kamienny et al., 2024], MOBLLM [Binbas et al., 2024] solved from first principles, rather than relying on pre-built tools.This grants the agent maximum flexibility, allowing it to handle a wide array of structured data formats like tabular data, text, and numerical arrays to perform bespoke analyses.For example, Data Interpreter translates natural language queries into Python scripts, using libraries like Pandas and Matplotlib within a secure code interpreter to execute a full data science workflow from cleaning to visualization [Hong et al., 2024].</p>
<p>In the realm of fundamental science, frameworks like LLM-SR and MOBLLM use the agent's code generation capabilities to perform symbolic regression-discovering the underlying mathematical equations that describe experimental data [Kamienny et al., 2024, Binbas et al., 2024].</p>
<p>The key challenges here are technical and philosophical: the risk of "code hallucination" (generating incorrect or inefficient code), the difficulty of scaling logical reasoning for highly complex problems, and the fundamental question of verifying whether an agent is making a novel discovery versus merely recalling a similar solution from its vast training data.</p>
<p>Iterative Result Validation and Refinement</p>
<p>Scientific discovery rarely concludes after a single iteration of experiments.The iterative result refinement and validation phase emerges as a crucial mechanism for enhancing scientific outcomes.This stage involves multiple iterative cycles, each focused on carefully reviewing the results from previous stages, identifying discrepancies or unexpected findings, and methodically refining experimental procedures, computational models, or analytical techniques.Activities such as debugging computational workflows to detect and correct errors and systematically verifying adherence to established experimental protocols are essential to this process.Each iteration provides valuable feedback that informs subsequent improvements, thereby progressively aligning experimental outcomes with original hypotheses and research objectives.Concurrently, validation rigorously compares results against established scientific knowledge, predefined benchmarks, and reproducibility standards, ensuring the reliability, accuracy, and integrity of the scientific conclusions drawn.</p>
<p>Refinement within LLM-based systems typically uses three major strategies: automatic selfcorrection, external evaluation and feedback, and human-in-the-loop approaches (Figure 10).</p>
<p>Automatic Self-correction.Automatic self-correction employs introspective and self-debugging strategies, prompting LLMs to iteratively evaluate and refine their own outputs.This method typically involves the use of specialized prompts that encourage models to critique and revise their responses based on internal reasoning.For example, the Self-Refine framework [Madaan et al., 2023] has shown significant improvements in reasoning tasks by enabling LLMs to critique and iteratively refine their responses.Additionally, frameworks such as those explored by Chen et al. [2023b] highlight how iterative introspection significantly enhances code-generation accuracy.Within scientific LLM agents, ChemAgent uses dynamic memory retrieval to self-correct and refine reasoning based on prior experiences [Tang et al., 2025].SpatialAgent [Wang et al., 2025a] relies on chain-of-thought reasoning and self-reflection to iteratively refine plans to achieve specific goals.Team of AI-made Scientists (TAIS) [Liu et al., 2024a] uses multiple LLMs as distinct agents that provide feedback to one another, collaboratively refining and improving the overall results.BioInformatics Agent [Xin et al., 2024] first summarizes and analyzes the results from the initial run, compares them with the user's expected outcomes, and then modifies the entire workflow to better align with the user's objectives.Recent works incorporate self-verification and self-correction mechanisms into reinforcement learning training to improve the robustness and reasoning capabilities of large language models Zhang et al.</p>
<p>[2025c], Jiang et al. [2025], Liu et al. [2025d].However, self-correction methods alone are often insufficient due to inherent limitations in LLM self-assessment capabilities [Huang et al., 2023].</p>
<p>External Evaluation and Feedback.External evaluation and feedback strategies use tools and automated metrics external to the LLM to objectively validate and refine the model's outputs.These external methods include computational assessments, fact-checking tools, or specialized domainspecific tools to guide iterative improvement.For instance, the CRITIC framework [Gou et al., 2023] systematically validates LLM outputs using external fact-checking, such as a search engine or a knowledge base.In scientific applications, OSDA Agent [Hu et al., 2025] uses computational chemistry tools to evaluate and iteratively refine molecular structure proposals.Similarly, MAPPS [Zhou et al., 2025a] integrates feedback from scientific computational tools and scientist insights to optimize experimental workflows and enhance discovery outcomes.Besides, the AI co-scientist [Gottweis et al., 2025a] employs an external multi-agent critique approach, facilitating peer-review-style evaluations that iteratively improve scientific hypotheses.BioDiscoveryAgent [Roohani et al., 2024] incorporates the phenotypic score from previous rounds in the prompt to help generate a better set of genes to experimentally perturb.SAMPLE [Rapp et al., 2024] connects with gene assembly, protein expression, and biochemical analysis systems, feeding the results back to refine the agent's understanding in an iterative loop.Biomni [Huang et al., 2025b] iteratively refines its reasoning using feedback from tools and code execution results.</p>
<p>Human-in-the-Loop.Human-in-the-loop methods integrate direct human expertise and oversight into the LLM refinement process.These methods often involve structured iterative human evaluations, ensuring that outputs align closely with expert standards and real-world applicability.General research has extensively used reinforcement learning from human feedback (RLHF) to guide models toward behaviors reflecting expert evaluations.Within scientific contexts, AI co-scientist [Gottweis et al., 2025a] explicitly integrates human expertise within its iterative "generate, debate, and evolve" framework, ensuring robust scientific outcomes.ResearchAgent [Baek et al., 2024] similarly uses human-aligned reviewing agents to simulate structured peer review, significantly enhancing the feasibility and novelty of scientific research proposals.MAPPS [Zhou et al., 2025a] relies on human scientists to verify the generated workflow to ensure seamless materials discovery, leading to more robust results.</p>
<p>Domain-Specific Scientific Agents</p>
<p>A new generation of scientific agents is being developed for fields as diverse as biology, chemistry, and materials science.By integrating and using advanced domain-specific tools, these agents are beginning to tackle challenging discovery tasks such as designing novel molecules, predicting protein structures, and discovering new materials with unprecedented speed.</p>
<p>In genomics, tools such as the UCSC Genome Browser [Kuhn et al., 2013], AlphaGenome [Avsec et al., 2025], and Evo2 [Brixi et al., 2025] provide powerful resources for genome annotation, regulatory function prediction, and large-scale genome modeling [Avsec et al., 2021, Nguyen et al., 2024, Su et al., 2025a,b].In single cell biology, Seurat [Hao et al., 2021] and Scanpy [Wolf et al., 2018] analyze large datasets.For proteomics, the AlphaFold family of models [Jumper andet al., 2021, Abramson et al., 2024] and ESM series [Lin et al., 2023, Hayes et al., 2025] predict protein structures and are increasingly applied in protein design workflows, while Rosetta [Leaver-Fay and et al., 2011, Watson et al., 2023, Dauparas et al., 2022] serves as a versatile library for protein modeling and design and has given rise to some of the most powerful deep learning models in the field.In chemistry, RDKit [Landrum, 2006] processes molecular data, and Q-Chem [Shao and et al., 2006] and Psi4 [Smith et al., 2020] perform quantum chemistry calculations.In materials science, ASE [Larsen and et al., 2017], VASP [Kresse and Furthmüller, 1996], and LAMMPS [Plimpton, 1995, Thompson et al., 2022] simulate molecular and materials behavior.Recently, machine learning is advancing molecular and materials simulation through Machine Learning Interatomic Potentials (MLIPs) and newer methods that directly predict quantum Hamiltonian matrices [Yu et al., 2023a[Yu et al., ,b, 2025]].In physics, tools like OpenFOAM [OpenCFD Ltd, 2024] for fluid dynamics and CIGALE [Noll et al., 2009] for galaxy evolution enable complex simulations.This diverse suite of tools highlights how specialized computational resources are driving scientific discovery forward.</p>
<p>Genomics.The biological sciences have seen explosive growth in autonomous agent development.SpatialAgent [Wang et al., 2025a] runs end-to-end spatial biology studies at scale, autonomously processing 2M+ cells across spatial transcriptomics and MERFISH pipelines to annotate tissue niches and map cell-cell interactions, demonstrating parity or gains against automated baselines while handling full projects with minimal human input.Team of AI-made Scientists (TAIS) [Liu et al., 2024a] is a framework that simulates researchers' works for genomics by assigning LLM agents specialized roles, including agents to plan the analysis, handle data selection and preprocessing, conduct biomedical research and analytical studies, analyze data statistics and interpret results, and review the quality of codes.Working together, these agents analyze gene expression datasets to identify disease-predictive genes, streamlining the pipeline of gene discovery (from raw data to insights) without human intervention.CRISPR-GPT [Qu et al., 2025] automates gene-editing experiment design-from system and gRNA selection to delivery and validation protocols-and reports successful real-world use translating agent-generated designs into wet-lab execution.BioDiscoveryAgent [Roohani et al., 2024] chooses genes to perturb to achieve a target phenotype, outperforming trained Bayesian-optimization baselines by +21% on average (and +46% on non-essential genes) across six datasets, and doubling the hit-rate for multi-gene combinations over random.PerTurboAgent [Hao et al., 2025] plans iterative Perturb-seq campaigns, predicting next-round gene panels and improving panel quality over static or heuristic baselines in retrospective/simulation studies, thereby shortening cycles in pooled genetic screens.BioAgents [Mehandru et al., 2025] delivers locally runnable, privacy-preserving bioinformatics workflows using multi-agent SLMs with RAG, achieving near-expert performance on conceptual genomics tasks while enabling on-premises customization with proprietary data.Single-cell genomics has also benefited greatly from multiple agent systems.BioInformatics Agent (BIA) [Xin et al., 2024] runs end-to-end scRNA-seq analyses (dimensionality reduction, clustering, DE, enrichment), producing ready-to-use figures and reports with minimal user intervention.scBaseCount [Youngblut et al., 2025] uses an agentic, hierarchical workflow to continuously mine, standardize, and expand a single-cell compendium now exceeding 230 million cells across 21 organisms and 72 tissues, giving downstream agents a harmonized data backbone.CellAgent [Xiao et al., 2024] automates scRNA-seq (and spatial) pipelines end-to-end via planner-executor-evaluator roles, delivering high-quality analyses without human steps and exportable narratives/plots for downstream interpretation.CompBioAgent [Zhang et al., 2025b] provides natural-language exploration of scRNA-seq cohorts, turning user queries into JSON plans and returning targeted summaries and visualizations that lower the barrier for non-computational users.scAgent [Mao et al., 2025] targets universal cell-type annotation and novel-type discovery; across 160 cell types in 35 tissues it reports state-of-the-art accuracy and generalization, including data-efficient extension to unseen types.CASSIA [Xie et al., 2024] is a multi-agent annotator that produces interpretable cell-type calls with full reports and improves low-quality annotations by retrieving external evidence and consolidating tool outputs into audit-ready documentation.GeneAgent [Wang et al., 2025b] performs gene-set analysis and queries expert-curated biological databases to cross-check its claims, reducing hallucinations and producing auditable rationales for functional descriptions.STAgent [Lin et al., 2025] couples perception, dynamic code generation, tool use, and literature grounding to deliver end-to-end spatial transcriptomics analyses and structured reports with minimal human input.MRAgent [Xu et al., 2025c] automates Mendelian randomization studies end-to-end by triaging literature for exposure-outcome pairs, selecting GWAS, executing MR pipelines, and producing standardized reports.</p>
<p>Protein.The protein engineering domain has witnessed significant advances.SAMPLE [Rapp et al., 2024] delivers a full closed-loop, self-driving protein engineering pipeline that experimentally discovers thermostable enzyme variants, identifying GH1 hydrolases with ≥12 °C higher stability than the starting sequences via autonomous design-build-test cycles in a robotic lab, rather than manual mutagenesis campaigns.The Virtual Lab [Swanson et al., 2024] pushes from computation to wet-lab outcomes by designing 92 novel SARS-CoV-2 nanobodies and experimentally validating expression and binding across variants (with &gt;90% expression in tested constructs), demonstrating that multi-agent pipelines can translate in-silico designs into viable binders.ProtAgents [Ghafarollahi and Buehler, 2024a] demonstrates de novo proteins tailored to mechanical targets, generating sequences that meet target vibrational-frequency profiles and structural specifications validated through structure prediction and physics-aware analyses-showing agentic design can hit property constraints, not just folds.Sparks [Ghafarollahi and Buehler, 2025] shows agentic discovery can move from instances to general principles, autonomously uncovering two protein-design rules: a length-dependent crossover where β-biased peptides surpass α-helical ones in unfolding force beyond ∼80 residues, and a chain-length/secondary-structure stability map with a high-variance "frustration zone" for α/β folds.VibeGen [Ni and Buehler, 2025] extends this to dynamics-aware design, where a designer-predictor duo produces de novo proteins whose all-atom MD trajectories reproduce prescribed normal-mode amplitudes, expanding beyond evolutionary templates toward motion-tuned biomolecules.AutoPro-teinEngine (AutoPE) [Liu et al., 2024b] lowers the barrier for biologists by automating model choice, HPO, and data handling; on two real protein-engineering tasks it shows substantial accuracy gains over zero-shot and manual fine-tuning, turning advanced DL pipelines into practical, conversational workflows.ProtChat [Huang et al., 2024b] further streamlines protein analysis tasks into a multiagent workflow by combining a high-level generalist LLM (e.g.GPT-4o) for task-planning, chatting and visualization, with specialized PLMs (e.g.ESM) for protein understanding.PRIME [Zhou et al., 2025b] interprets high-level goals and dynamically synthesizes computational workflows for protein engineering by reasoning over a curated library of 65+ validated protein tools, constructing customized pipelines end-to-end.</p>
<p>Medicine.</p>
<p>A recent groundbreaking study is Biomni [Huang et al., 2025b], which functions as a general-purpose biomedical agent that actually completes end-to-end analyses-causal gene prioritization, drug repurposing, rare-disease workups, and multi-omics integration-by driving 150+ tools, 105 software packages, and 59 databases, returning ranked candidates with traceable evidence.AI co-scientist [Gottweis et al., 2025b] operationalizes hypothesis-driven research: a Gemini-2.0multiagent system that turns literature + model feedback (e.g., structure prediction) into novel, testable hypotheses and study proposals with iterative review.TxAgent [Gao et al., 2025b] converts patient context into actionable therapy decisions, using 211 tools for up-to-date interaction checks, contraindications, and personalized regimen selection via multi-step tool-augmented reasoning.Beyond these, BioResearcher [Luo et al., 2025b] demonstrates dry-lab automation end-to-end-from goal → literature synthesis → executable analysis/protocol drafts → reviewer-style critiques-showing measurable gains on complex research objectives without manual "glue code."STELLA [Jin et al., 2025a] targets the evidence-curation bottleneck in biomedicine with a self-evolving, multi-agent literature analysis workflow that structures large corpora into machine-navigable knowledge to support hypothesis generation/validation.In the domain of drug discovery, CLADD [Lee et al., 2025] coordinates collaborative agents with RAG to design/dock/triage while ingesting heterogeneous biochemical knowledge without fine-tuning, improving task performance over general LLMs and classical DL baselines.DrugAgent [Liu et al., 2024c] automates end-to-end ML programming for ADMET/repurposing tasks (data acquisition → training → evaluation), reporting strong case-study metrics (e.g., PAMPA absorption F 1 ≈ 0.92).PharmAgents [Gao et al., 2025d] builds a "virtual pharma" where specialist agents span screening → modeling → triage, outputting prioritized leads with assay-ready metadata.LIDDiA [Averly et al., 2025] is an autonomous, language-driven discovery agent that generates molecules meeting pharmaceutical criteria on many targets and surfaces promising EGFR candidates, emphasizing low-cost adaptability.AgentMD [Jin et al., 2024a] automatically selects and executes from 2,164 curated clinical calculators (RiskCalcs) to return risk estimates with formula provenance, substantially outperforming strong prompting baselines on RiskQA.MedAgents [Tang et al., 2023] delivers multi-specialist deliberation in zero-shot settings to reach consensus diagnoses/plans across standard medical QA suites.ClinicalGPT [Wang et al., 2023b] is a domain LLM for clinical tasks; EHR-integrated agentic examples include EHRAgent, which auto-generates/executes code against EHR data to answer complex patient queries and compute scores.BehaveAgent [Aljovic et al., 2025] provides turn-key cross-species behavior analysis from raw video-planning the analysis, tracking/pose estimation, sequence labeling, and report generation-without retraining across paradigms.</p>
<p>Chemistry.Chemistry has emerged as a particularly fertile ground for autonomous agents.For instance, the agent Coscientist [Boiko et al., 2023] tackled the overarching problem of autonomously designing and executing complex physical experiments from high-level prompts, demonstrating its capability by successfully performing Nobel Prize-winning palladium-catalyzed cross-coupling reactions in under 4 minutes.To broaden the utility of LLMs in chemistry, ChemCrow [M.Bran et al., 2024] addressed the challenge of equipping them with specialized knowledge by integrating 18 expert-designed tools, successfully performing autonomous planning and multi-tool analysis across organic synthesis, drug discovery, and materials science.For managing highly complex workflows, ChemAgents [Song et al., 2025] solved the need for distributed expertise by deploying a hierarchical multi-agent system where a manager agent coordinates specialized agents for tasks ranging from literature analysis to robotic operation.Bridging linguistic reasoning with quantum mechanics, ChemReasoner [Sprueill et al., 2024] focused on the difficult task of catalyst discovery by creating a synergistic loop between LLM-driven hypothesis generation and rapid feedback from DFT simulations, thereby accelerating the materials design cycle.To improve accessibility, CACTUS [Mc-Naughton et al., 2024] solved the user-interface problem for complex computational chemistry tools, acting as an intelligent conversational assistant that accurately translates natural language questions into the proper tool calls for property prediction, similarity searches, and toxicity estimation.In the domain of experimental optimization, Chemist-X [Chen et al., 2023a] focused on automating the tedious process of refining reaction conditions, achieving a fully automated, closed-loop system that uses retrieval-augmented generation to propose conditions and then directs a robotic platform for wet-lab validation.In computational chemistry, El Agente Q [Zou et al., 2025] tackled the manual, error-prone steps in computational chemistry, demonstrating how its cognitive architecture could autonomously handle an entire workflow-from file preparation to cluster submission and result parsing-based on a simple natural language prompt.To empower bench chemists, LLM-RDF [Ruan et al., 2024] aimed to provide a fully automated synthesis workflow for users without coding expertise, successfully enabling an end-to-end process from literature search to product purification through its conversational, six-agent framework.In the pharmaceutical space, FROGENT [Pan et al., 2025] addressed the complex, end-to-end process of drug design, showing superior performance on multistep discovery benchmarks by integrating diverse biochemical databases and predictive models into a unified framework.LARC [Baker et al., 2025] solved the critical challenge of applying practical, real-world constraints to retrosynthesis planning, employing an Agent-as-a-Judge mechanism to generate more feasible routes and achieving a near-human-level success rate on diverse tasks.FMG [Sun et al., 2025] demonstrates expert-level molecular design can be achieved by adopting graph representations and rendering them as images, highlighting multiple modalities can enable deeper understanding.Finally, addressing the core of scientific inquiry, MOOSE-Chem [Yang et al., 2024] aimed to automate the creative process of hypothesis generation by mimicking human cognition, successfully rediscovering the core innovations from recent high-impact chemistry papers without any prior knowledge of their content.</p>
<p>Materials.The application of scientific agents in materials science has led to innovative solutions for long-standing research challenges, spanning from discovery to synthesis and analysis.The A-Lab [Szymanski et al., 2023] of Lawrence Berkeley National Laboratory represents a fully autonomous materials synthesis facility.Using three robotic arms, box furnaces, and X-ray diffractometers, it synthesized 41 novel compounds from 58 targets over 17 days of continuous operation, achieving a 71% success rate.ChatMOF [Kang and Kim, 2024] addresses the complex scientific problem of discovering novel Metal-Organic Frameworks (MOFs), a process that requires navigating vast chemical databases, predicting properties of hypothetical structures, and generating new candidates.Its primary scientific contribution is a unified, autonomous workflow that orchestrates these disparate tools, successfully translating high-level goals into a concrete series of actions to accelerate the identification of promising materials for applications like gas storage.Similarly, the "ChatGPT Research Group" [Zheng et al., 2023] tackles the resource-intensive, iterative process of optimizing experimental synthesis conditions for advanced materials.Its achievement is a collaborative multiagent system that mimics a human research team, dividing labor among specialized LLM agents to significantly accelerate the experimental optimization cycle and make the discovery-to-production pipeline more systematic.In the computational domain, MDAgent [Shi et al., 2025] confronts the efficiency bottleneck in Molecular Dynamics (MD) simulations, which are critical for understanding material behavior but require significant user expertise.It automates the entire MD workflow from code generation to execution, achieving a notable scientific result by reducing the total task time for thermodynamic calculations by over 40% and lowering the barrier to entry for performing these complex simulations.To address the limitations of general-purpose LLMs, HoneyComb [Zhang et al., 2024b] solves the problem of their lack of deep, domain-specific knowledge and reliability in scientific computations.It contributes a robust agent framework grounded with a curated materials knowledge base and a hub of validated scientific tools, resulting in significantly higher accuracy in both reasoning and computation.Addressing the grand challenge of inverse design, LLMatDesign [Jia et al., 2024] introduces a framework for autonomous materials discovery, particularly in low-data regimes.Its key scientific achievement is a self-reflection mechanism that allows the agent to learn from computational outcomes and adapt its strategy, enabling the effective discovery of materials with specific, userdefined properties.Meanwhile, MatAgent [Bazgir et al., 2025a] targets the fragmentation and lack of reproducibility in typical materials discovery workflows.It provides an integrated, human-in-theloop multi-agent framework that streamlines the research process, fostering an AI-guided, adaptive laboratory workflow that enhances both the speed and reproducibility of materials research.For managing multi-faceted research projects, MatSciAgent [Chaudhari et al., 2025] solves the challenge of orchestrating diverse computational tasks.It achieves this with a modular architecture where a master agent interprets high-level goals and delegates tasks to specialized sub-agents, creating an extensible and robust system for complex research questions.To better integrate human expertise, MatPilot [Ni et al., 2024] demonstrates a powerful human-machine collaborative framework.It resolves the challenge of seamlessly combining human intuition with AI's computational power by using multi-agent LLMs for hypothesis generation while allowing human experts to guide the overall strategy, enabling a continuous learning loop.Materials Laws Multi-Agent Framework [Hu et al., 2024] tackles the fundamental scientific goal of distilling complex data into simple, interpretable physical laws.Its major scientific result was the successful use of LLM agents to perform symbolic regression, deriving a low-complexity, highly accurate formula for predicting glass-forming ability in metallic glasses and showcasing the potential of AI to automate scientific law discovery.</p>
<p>Physics.Scientific agents are increasingly being applied to solve complex problems in physics and engineering, demonstrating capabilities ranging from controlling physical hardware to automating sophisticated simulations.In quantum computing, the k-agents framework [Cao et al., 2024] addresses the scientific challenge of automating quantum experiment design and execution, a process that typically requires deep human expertise.Its significant scientific achievement was the creation of a fully autonomous laboratory where LLM agents successfully planned and executed experiments on superconducting quantum processors to produce entangled states at a performance level equivalent to human experts.In materials physics, AtomAgents [Ghafarollahi and Buehler, 2024c] tackles the complex, multi-factorial problem of designing new alloys with specific properties.It contributes a physics-aware, multi-agent framework that successfully integrates knowledge retrieval with physicsbased simulations in an iterative loop, enabling the system to autonomously propose, simulate, and refine alloy compositions to meet performance targets.In astrophysics, Mephisto [Sun et al., 2024] solves the complex inverse problem of interpreting multi-band galaxy observations to determine their physical properties.Its key result is a multi-agent framework that automates this by calling the CIGALE astrophysics code as a tool, successfully proposing and testing hypotheses against observational data in an iterative reasoning loop.In quantum instrumentation, QCopilot [Sha et al., 2025] confronts the challenge of designing and diagnosing highly sensitive quantum sensors, a process that involves time-consuming manual parameter tuning.It achieved a remarkable 100-fold speedup over manual procedures by autonomously performing modeling, optimization, and anomaly detection in atom cooling experiments.In computational fluid dynamics (CFD), OpenFOAMGPT [Pandey et al., 2025] solves the problem of the steep learning curve and tedious setup process associated with complex simulation software like OpenFOAM.It successfully automates the entire workflow, from case setup to iterative correction, significantly lowering the barrier to entry for advanced CFD.MetaOpenFOAM [Chen et al., 2024b] further addresses the need for robust and generalized CFD automation by using a multi-agent system to decompose complex natural language instructions, achieving strong performance across a diverse range of flow simulations.Finally, to address the critical need for interpretability in scientific AI, the AI-Scientist Framework [Xu et al., 2025a] introduces a multi-agent system that structures LLM outputs into transparent, executable models.Its main contribution is enhancing systematic validation and human-AI collaboration by making the agent's reasoning process physically grounded and verifiable.</p>
<p>Other Science and Engineering.Beyond the foundational scientific disciplines, scientific agents are driving significant advancements in a range of other scientific and engineering fields.The Autonomous GIS Agent [Ning et al., 2025] and its successor GIS Copilot [Akinboyewa et al., 2025] address the accessibility barrier in geospatial science, where answering spatial questions often requires specialized programming skills.They successfully automated workflows from data retrieval to the generation of maps and statistics by translating natural language requests into executable programs, making advanced geospatial analysis more accessible.For complex engineering systems, the domain-specific ReAct agent for gas turbines [Song et al., 2024] tackles the challenge of modeling and analyzing the thermodynamics of multi-component systems.It successfully integrated expert knowledge with predefined tools to perform iterative gas path analysis, demonstrating a viable path for AI-driven diagnostics in power engineering.LP-COMDA [Liu et al., 2024d] extends agent applications to power electronics, solving the challenge of designing complex and error-prone modulation strategies for power converters.It successfully automated this process with a physics-informed planner, accelerating design time by over 30x while reducing errors by over 60%.</p>
<p>Discussions</p>
<p>From LLM Reasoning to Agentic Reasoning via Reinforcement Learning</p>
<p>The advent of Large Language Models (LLMs) has brought the promise of automating complex tasks, but Reinforcement Learning (RL) plays a crucial role in transitioning from passive text generation to active decision-making and discovery.While Supervised Fine-Tuning (SFT) can enhance domain knowledge and change model behaviors, it is fundamentally a form of imitation learning, limiting a model's ability to generalize to new domains [Chu et al., 2025] and explore more optimal or diverse problem-solving strategies.Reinforcement Learning, particularly from verifiable rewards, provides a fundamental solution [Parashar et al., 2025, Su et al., 2025c, Li et al., 2024b, 2025b] to elicit generalizable reasoning ability from the base policy model.Work exemplified by DeepSeek-R1 [Guo et al., 2025] demonstrates that models can learn complex reasoning behaviors, including self-verification and self-correction, by being rewarded solely from final answer correctness.These emergent capabilities reveal RL's potential for transforming LLMs from passive knowledge repositories into active problem solvers.</p>
<p>The agentic RL paradigm extends model behaviors from pure reasoning to actionable interactions with environments, where agents learn to dynamically interleave reasoning and tool execution.Through RL, agents optimize not just what to think, but when and how to act.Code interpreters enhance mathematical abilities [Feng et al., 2025b, Mai et al., 2025] of agents, with agents learning through RL to optimize reasoning trajectories via multi-turn real-time code execution.Search engines enable agents to acquire external knowledge and up-to-date information, where RL helps agents generate effective search queries as part of their reasoning process [Jin et al., 2025b, Chen et al., 2025c].Beyond augmenting capabilities, agentic RL also enables models to operate in complex, dynamic environments.Web browsing agents [Wei et al., 2025c, Qi et al., 2025] and GUI agents [Qin et al., 2025, Agashe et al., 2024] position models in human-like scenarios, requiring them to interact with environments where states change in response to their actions.Agentic RL foster the model capability in handling continuous perception, decision-making, and action execution in response to environmental feedback.Beyond a single tool, RL also trains agents to coordinate multiple tools concurrently to solve harder tasks like deep research [Geng et al., 2025, Li et al., 2025c].</p>
<p>Applying the successful Agentic RL methods to scientific discovery is a promising way for realizing an AI-driven scientific revolution.However, the unique nature of the scientific domain presents challenges that far exceed those in existing work.</p>
<p>The Environment Problem.The environment for a Science Agent is an unavoidably heterogeneous hybrid.The system parses outputs from diverse digital tools with non-standard interfaces (e.g., command-line software, database APIs, graphical user interfaces) and interacts directly with physical laboratory equipment such as robotic arms and chemical synthesizers.The introduction of the physical world brings with it latency, noise, irreversible actions, and the risk of hardware damage-challenges that purely digital agents have never faced.</p>
<p>The Action Problem.The action space for a Science Agent, however, is enormous and non-standard, as the actions to control a mass spectrometer are entirely different from those to call a data analysis library.More importantly, an advanced Science Agent is expected not only to use tools, but also to create new ones by writing code.This expands the action space from a finite set to the near-infinite space of all possible programs, posing a massive challenge for exploration-based RL.While the Agent RL Scaling Law study reveals that performance scales with the number of interactions, the astronomical number of interactions required to effectively explore the explosive action space of a Science Agent is practically infeasible.</p>
<p>The Observation Problem.A Science Agent needs to process mixed-modality observations, including experimental images, mass spectrometry data, simulation curves, structured data like SMILES chemical formulas, and unstructured text from scientific papers.Furthermore, research projects have long durations, requiring the agent to possess long-term memory capabilities to connect a serendipitous finding from months ago to a current problem.This far exceeds the capacity of existing agents that primarily rely on short-term context for decision-making and places extreme demands on the model's memory and retrieval systems.</p>
<p>The Reward Problem.All successful Agent RL works rely on a clear, sparse, but definitive final reward signal, such as whether a math problem is correct, a web task is completed, or a question is answered correctly [Jin et al., 2025b, Wei et al., 2025c, Chen et al., 2025d].This paradigm almost completely breaks down in scientific discovery.The reward is highly sparse, as a true discovery can take years, and the outcome of a single experiment does not serve as an effective reward signal [Uehara et al., 2025a,b].The objective of success is also ambiguous.Designing a reward function that can measure novelty, impact, or reproducibility is a major, unresolved problem.Although the self-evolving curriculum proposed in WebRL offers a promising direction by generating easier sub-tasks from failures, it is still confined to tasks with clear success criteria.For open-ended scientific exploration, defining and calculating the reward remains the most central obstacle on the path to a general-purpose Science Agent.</p>
<p>The Leap from LLMs to Agents in Autonomous Scientific Discovery</p>
<p>LLMs are primarily seen as powerful reasoning engines, like our human brain.Trained on vast amounts of data, they excel at processing, analyzing, and verifying existing human knowledge.However, even if an LLM could propose a novel, unconventional hypothesis, it lacks the ability to physically interact with the world to verify its own ideas.It is not able to design experiments, conduct tests, or observe real-world results, which are essential steps for turning a hypothesis into a validated discovery.This limitation is visually depicted in the Figure 11, where the LLM is trapped inside the Humanity's Knowledge Closure.It performs efficient reasoning within the existing circle of knowledge but struggles to cross this boundary to reach New Discoveries.</p>
<p>To overcome this challenge, the LLM needs to evolve from a passive reasoning engine into an active scientific agent.As Kambhampati [2025] emphasizes, human can interact with the world and explore new knowledge, and a complete scientific agent should also possess this ability.From an information-theoretic perspective, as detailed in Section 2.3.1, the agent's internal entropy reduction is fundamentally dependent on irreversible interactions with the physical world.The necessary constraining information cannot be generated internally but must be obtained from real-world processes to truly reduce the hypothesis space.</p>
<p>A scientific agent can not only process data but also design and execute experiments, actively exploring unknown domains.They can acquire new data through trial and error, observation, and hands-on actions, much like a human scientist, rather than solely relying on existing information.A scientific agent is no longer a passive recipient of information; it can translate its internal knowledge (explicit facts, procedural models, etc.) into concrete actions in the real world.This action-feedback compared to the potential of a Scientific Agent.An LLM, acting purely as a reasoning engine, is fundamentally confined within Humanity's Knowledge Closure.It excels at processing and making connections within the space of existing and Inferable Knowledge but struggles to generate truly novel insights that lie outside this boundary.In contrast, a Scientific Agent is designed to overcome this limitation.By actively interacting with the world-designing experiments, executing actions, and employing exploration strategies-it can break through the knowledge closure to produce genuinely New Discoveries, thereby expanding the frontier of human understanding.This discussion was motivated by and adapted from Kambhampati [2025], and the figure is expanded and recreated with permission.</p>
<p>loop allows the agent to continuously break through the boundaries of inferable knowledge, gradually expanding the circle of human knowledge.</p>
<p>In this way, an LLM-driven scientific agent will no longer be merely an extension of human thought but can become an independent, exploratory pioneer.It can assist humans in efficiently exploring the frontiers of inferable knowledge and may even, through a chance deviation, help humanity discover entirely new insights and groundbreaking discoveries that truly lie beyond the current knowledge closure.</p>
<p>Role of Serendipity in Discovery</p>
<p>An essential aspect of scientific discovery is the inherent unpredictability and the role that chance and unexpected findings play in significant breakthroughs.Historical milestones, such as the discovery of penicillin, cosmic microwave background radiation, and graphene, highlight how serendipitous findings are pivotal in driving scientific advancement.This phenomenon underscores a crucial gap between human-driven scientific inquiry and current capabilities of LLM-based science agents.</p>
<p>LLMs are trained to maximize likelihood based on extensive historical data, inherently optimizing towards predictable and probable outcomes.Such a training objective inadvertently constrains the models' abilities to venture into the realms of randomness and serendipity that frequently yield novel insights in human science.Major historical scientific breakthroughs, such as the discovery of penicillin [Fleming, 1929], cosmic microwave background radiation [Penzias and Wilson, 1965], and graphene [Novoselov et al., 2004], often originated from chance or unexpected observations that break free from the confines of pure logical deduction.Consequently, purely likelihood-driven agents might overlook unconventional hypotheses or unexpected patterns that do not align closely with existing data distributions.To bridge this gap, specific mechanisms or design principles could be integrated into LLM-based science agents to better accommodate chance and unexpected discoveries.For instance, stochastic exploration strategies could be employed to occasionally deviate from the highest probability outputs, thereby simulating the conditions under which human scientists encounter serendipitous results.</p>
<p>Summary and Outlook</p>
<p>The emergence of LLM-based agents represents a substantial step forward in the automation and acceleration of scientific discovery.By addressing critical limitations inherent to traditional humandriven approaches and reinforcement learning-based computational agents, LLM-driven methods provide a unified and highly flexible framework capable of operating seamlessly across human intent, natural language, computer language, and physical information.This unified capacity significantly enhances the adaptability, scalability, and generalizability of scientific agents, enabling dynamic engagement across all stages of the discovery lifecycle.</p>
<p>As scientific inquiry continues to confront increasingly complex challenges and larger volumes of data, the continued development and refinement of LLM-based scientific agents become essential.Future research should focus on overcoming existing limitations, such as better understanding physical laws, integrating robust mechanisms for using physical tools, and designing sophisticated interactions with experimental environments and physical instrumentation.Such advancements will further strengthen the capabilities of scientific agents, promoting deeper collaboration between humans and computational intelligence.Ultimately, the ongoing evolution of LLM-based agents holds the promise of revolutionizing scientific discovery, enabling unprecedented efficiency, adaptability, and creativity in generating novel, transformative insights across all scientific disciplines.</p>
<p>Figure 1 :
1
Figure 1: An overview of the three-phase workflow for AI-driven scientific discovery.The process begins with Phase 1: Hypothesis Discovery, where a high-level human goal is transformed through knowledge extraction and hypothesis generation into novel, verifiable scientific questions.In Phase 2: Experimental Design &amp; Execution, these hypotheses are translated into detailed workflows which the agent executes by either using existing tools or creating new scientific tools to generate experimental data.Finally, Phase 3: Result Analysis &amp; Refinement involves interpreting the data and entering an iterative refinement loop to progressively improve the process and arrive at validated findings.</p>
<p>Figure 2: A comprehensive overview of the role of LLM-based agents in the scientific discovery lifecycle alongside a diverse collection of domain-specific scientific agents that organize various research systems and papers across fields such as Genomics, Protein, Medicine, Chemistry, Materials, Physics, and Others.</p>
<p>Figure 3 :
3
Figure 3: An information-theoretic framework for autonomous scientific discovery, illustrating the inverse relationship between Information Entropy and Verifiability.</p>
<p>Figure 4 :
4
Figure4: An overview of autonomous agents for scientific discovery in which an agent orchestrates scientists, language, code, and physics.This figure illustrates the dynamic, closed-loop workflow of an LLM-based scientific agent as the coordinator orchestrating four key components, including scientists, language, code, and physics.The agent continuously interacts with human scientists, receiving goals, guidance, and feedback to direct research, and providing summaries and findings.Its interaction with language involves extracting knowledge from literature to formulate verifiable hypotheses and detailed research plans.The agent's interface with code translates high-level plans into executable programs for simulations or instrument control by integrating tool functionalities.Finally, it interacts with physics by using raw data and laws to direct physical or simulated instruments, yielding experimental results.This cycle represents an iterative and autonomous discovery cycle, bridging human intent to empirical evidence.</p>
<p>Figure 6 :
6
Figure 6: A radar chart representation of information analysis across autonomous scientific discovery phases.</p>
<p>Figure 8 :
8
Figure8: The LLM-agent-driven workflow for Automated Hypothesis Discovery, centered around an iterative loop of generation and validation.The agent's reasoning is informed by an external knowledge base through two distinct paradigms: a passive, bottom-up knowledge injection and an active, top-down knowledge acquisition.During the Hypothesis Generation phase, the agent employs one of four primary strategies: Prompt-Based, Knowledge-Grounded, Multi-Agent-Based, or Evolutionary-Algorithm-Based.The resulting ideas are then filtered and refined in the Hypothesis Validation phase, which utilizes either automated Metric-Based Screening or interactive Agent-Based Validation to produce a final set of verified hypotheses.</p>
<p>Figure 9 :
9
Figure9: An LLM-agent-driven workflow of automated Experimental Design and Execution.This workflow illustrates how an LLM agent orchestrates experimental design and execution.In the Experimental Design phase, the central agent generates a detailed workflow by integrating information from multiple inputs.It receives high-level guidance (domain knowledge, feedback) and predefined components (templates, action pools) from Human Scientists, leverages Retrieval-Augmented Generation (RAG) to pull in knowledge from External Sources, and incorporates Experimental Results from previous execution cycles as a critical feedback mechanism for refinement.For the Experimental Execution phase, the agent acts upon the detailed workflow, deciding between using existing instruments (Tool Use) or developing new ones (Tool Creation).Tool use can be implemented through four primary strategies for tool interaction: Embedded, Toolbox-Based, Reflective and Iterative, and Hierarchical.</p>
<p>Figure 10 :
10
Figure10: A diagram of the scientific agent's continuous refinement cycle, which employs a progressive validation strategy.The process escalates through three stages as the difficulty of verifying the results increases.It begins with internal Automatic Self-Correction, moves to validation against tools and data via External Evaluation and Feedback, and for the most challenging cases, incorporates expert judgment through Human-in-the-Loop Approaches.</p>
<p>Figure 11
11
Figure11: A conceptual model illustrating the limitation of a Large Language Model (LLM) compared to the potential of a Scientific Agent.An LLM, acting purely as a reasoning engine, is fundamentally confined within Humanity's Knowledge Closure.It excels at processing and making connections within the space of existing and Inferable Knowledge but struggles to generate truly novel insights that lie outside this boundary.In contrast, a Scientific Agent is designed to overcome this limitation.By actively interacting with the world-designing experiments, executing actions, and employing exploration strategies-it can break through the knowledge closure to produce genuinely New Discoveries, thereby expanding the frontier of human understanding.This discussion was motivated by and adapted fromKambhampati [2025], and the figure is expanded and recreated with permission.</p>
<p>Table 1 :
1
Comparison of Scientific AI Agents and General LLM Agents
FeatureScientific AgentGeneral LLM AgentPrimaryObjective</p>
<p>Table 2 :
2
Information Analysis Across Autonomous Scientific Discovery Phases
Scientific PhaseHuman IntentNatural LanguageComputer Language Physical Information Overall DissipationHypothesis Dis-Very High EntropycoveryThe intent is abstractand completely open-ended (e.g., "discovera novel research gap").</p>
<p>Table 3 :
3
Comparison of Results Analysis Paradigms
ParadigmCore MechanismPrimary Data ModalityRepresentative SystemsModality-DrivenVisual/multimodal perceptionImages, charts,Analysisand interpretationvideo, audio
AcknowledgmentsThe authors thank Ms. Xiaomeng Fu for the professional graphic design support.S.J. gratefully acknowledges support from the National Science Foundation under grants IIS-2243850, MOMS-2331036, and CNS-2328395; the Advanced Research Projects Agency for Health under grant 1AY1AX000053; and the National Institutes of Health under grant U01AG070112.Additional support to S.J. was provided by the Texas A&amp;M Institute of Data Science, the Truchard Family Endowed Chair, the Presidential Impact Fellowship, and the Chancellor EDGES Fellowship at Texas A&amp;M University.W.W. gratefully acknowledges support from the National Science Foundation under grants 2106859, 2119643, 2200274, 2202693, 2312501;  the National Institute of Health under grants U54OD036472, U54HG012517, U24DK097771, OT2OD038003; the United States Department of Agriculture under grant 13434200; as well as support from Amazon, NEC, and Optum AI.X.Q.gratefully acknowledges support from the National Science Foundation under grants CMMI-2226908, DMR-2103842, and DMR-1753054; the Air Force Office of Scientific Research under grant FA9550-24-1-0207; and partial support by the donors of ACS Petroleum Research Fund under grant #65502-ND10.
Artificial intelligence for science in quantum, atomistic, and continuum systems. Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, . . , Shuiwang Ji, Foundations and Trends in Machine Learning. 2025a18</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 62079722023a</p>
<p>Quantum mechanics of many-electron systems. Paul Adrien, Maurice Dirac, Proceedings of the Royal Society of London. Series A. 1237921929</p>
<p>Quantum mechanics: non-relativistic theory. Lev Davidovich, Landau , Evgenii Mikhailovich, Lifshitz , 2013aElsevier3</p>
<p>David Tong, Quantum Mechanics. Lectures on Theoretical Physics. Cambridge University Press2025a9781009594820</p>
<p>Fluid mechanics: course of theoretical physics. Lev Davidovich, Landau , Evgenii Mikhailovich, Lifshitz , 2013bElsevier6</p>
<p>David Tong, Fluid Mechanics. Lectures on Theoretical Physics. Cambridge University Press2025b. ISBN 9781009594691</p>
<p>Artificial intelligence for sciencebridging data to wisdom. Yongjun Xu, Fei Wang, Zhulin An, Qi Wang, Zhao Zhang, 10.1016/j.xinn.2023.10052542023Innovation</p>
<p>Nobel turing challenge: creating the engine for scientific discovery. Hiroaki Kitano, 10.1038/s41540-021-00189-3Systems Biology and Applications. 72021</p>
<p>The fifth era of science: Artificial scientific intelligence. Nina Miolane, 10.1371/journal.pbio.3003230PLoS Biology. 2362025</p>
<p>Targeted materials discovery using bayesian algorithm execution. npj Computational Materials. R Sathya, Akash Chitturi, Yue Ramdas, Willie Wu, Daniel Neiswanger, Ratner, 10.1038/s41524-024-01326-2102024</p>
<p>Agentic ai for scientific discovery: A survey of progress, challenges, and future directions. Mourad Gridach, Jay Nanavati, Khaldoun Zine El Abidine, Lenon Mendes, Christina Mack, arXiv:2503.089792025aarXiv preprint</p>
<p>Ekin Dogus Cubuk, Amil Merchant, et al. An autonomous laboratory for the accelerated synthesis of novel materials. Nathan J Szymanski, Bernardus Rendy, Yuxing Fei, Rishi E Kumar, Tanjin He, David Milsted, Matthew J Mcdermott, Max Gallant, Nature. 62479902023</p>
<p>Protagents: protein discovery via large language model multi-agent collaborations combining physics and machine learning. Alireza Ghafarollahi, Markus J Buehler, Digital Discovery. 372024a</p>
<p>Toward a team of ai-made scientists for scientific discovery from gene expression data. Haoyang Liu, Yijiang Li, Jinglin Jian, Yuxuan Cheng, Jianrong Lu, Shuyi Guo, Jinglei Zhu, Mianchen Zhang, Miantong Zhang, Haohan Wang, arXiv:2402.123912024aarXiv preprint</p>
<p>Chemist-x: large language model-empowered agent for reaction condition recommendation in chemical synthesis. Kexin Chen, Junyou Li, Kunyi Wang, Yuyang Du, Jiahui Yu, Jiamin Lu, Lanqing Li, Jiezhong Qiu, Jianzhang Pan, Yi Huang, arXiv:2311.107762023aarXiv preprint</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, Burak Gokturk, Amin Vahdat, Pushmeet Kohli, Yossi Matias, Andrew Carroll, Kavita Kulkarni, Nenad Tomasev, Yuan Guan, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, R D Tiago, Costa, Gary José R Penadés, Yunhan Peltz, Annalisa Xu, Alan Pawlosky, Vivek Karthikesalingam, Natarajan, 2025a</p>
<p>Chemgymrl: A customizable interactive framework for reinforcement learning for digital chemistry. Chris Beeler, Sriram Ganapathi Subramanian, Kyle Sprague, Mark Baula, Nouha Chatti, Amanuel Dawit, Xinkai Li, Nicholas Paquin, Mitchell Shahen, Zihan Yang, Digital Discovery. 342024</p>
<p>Pankaj Rajak, Aravind Krishnamoorthy, Ankit Mishra, Rajiv Kalia, Aiichiro Nakano, and Priya Vashishta. Autonomous reinforcement learning agent for chemical vapor deposition synthesis of quantum materials. Marco Maurizi, Derek Xu, Yu-Tong Wang, Desheng Yao, David Hahn, Mourad Oudich, Anish Satpati, Mathieu Bauchy, Wei Wang, Yizhou Sun, arXiv:2408.06300npj Computational Materials. 711082024. 2021arXiv preprintInverse designing metamaterials with programmable nonlinear functional responses in graph space</p>
<p>Reinforcement learning for dynamic microfluidic control. J Oliver, Philip D Dressler, Jaebum Howes, Andrew J Choo, Demello, ACS omega. 382018</p>
<p>Adaptive droplet routing in digital microfluidic biochips using deep reinforcement learning. Tung-Che Liang, Zhanwei Zhong, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine Learning2020</p>
<p>Biogpt: Generative pre-trained transformer for biomedical text generation and mining. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu, 2022</p>
<p>Foundational models for scientific discovery. Alexander Dunn, Bernardino Romera-Paredes, Pushmeet Kohli, Nature. 63380312024</p>
<p>Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, arXiv:2401.05561Trustworthiness in large language models. 2024aarXiv preprint</p>
<p>mclm: A function-infused and synthesis-friendly modular chemical language model. Carl Edwards, Chi Han, Gawon Lee, Thao Nguyen, Chetan Kumar Prasad, Sara Szymkuc, Bartosz A Grzybowski, Jin Bowen, Ying Diao, Jiawei Han, Ge Liu, Hao Peng, Martin D Burke, Heng Ji, arxiv2025</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, Jiawei Han, Conference on Empirical Methods in Natural Language Processing. 2024a</p>
<p>Towards scientific discovery with generative ai: Progress, opportunities, and challenges. K Chandan, Parshin Reddy, Shojaee, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 1861863452024a</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.016802024arXiv preprint</p>
<p>Augmented language models: a survey. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Celikyilmaz, arXiv:2302.078422023arXiv preprint</p>
<p>Empowering biomedical discovery with AI agents. Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, Marinka Zitnik, Cell. 187222024</p>
<p>Towards scientific intelligence: A survey of LLM-based scientific agents. Pu Shuo Ren, Zhenjiang Jian, Chunlin Ren, Can Leng, Jiajun Xie, Zhang, arXiv:2503.240472025arXiv preprint</p>
<p>From ai for science to agentic science: A survey on autonomous scientific discovery. Jiaqi Wei, Yuejin Yang, Xiang Zhang, Yuhan Chen, Xiang Zhuang, Zhangyang Gao, Dongzhan Zhou, Guangshuai Wang, Zhiqiang Gao, Juntai Cao, arXiv:2508.141112025aarXiv preprint</p>
<p>Collective intelligence: On the promise and reality of multi-agent systems as a key catalyst for ai-driven scientific discovery. Yongjin Terry Jingchen Zhang, Yinya Yang, Sirui Huang, Bernhard Lu, Zhijing Schölkopf, Jin, </p>
<p>From automation to autonomy: A survey on large language models in scientific discovery. Tianshi Zheng, Zheye Deng, Hong Ting Tsang, Weiqi Wang, Jiaxin Bai, Zihao Wang, Yangqiu Song, arXiv:2505.132592025arXiv preprint</p>
<p>Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, Xinya Du, arXiv:2501.04306Llm4sr: A survey on large language models for scientific research. 2025aarXiv preprint</p>
<p>Biobert: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Bioinformatics. 3642020</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Paster, Oana-Eugen, Peter Ganea, Murali Gray, Kambadur, Galactica: A large language model for science. 2022</p>
<p>James D Siddharth M Narayanan, Ryan-Rhys Braza, Albert Griffiths, Geemi Bou, Mayk Wellawatte, Ludovico Caldas Ramos, Mitchener, Andrew D Samuel G Rodriques, White, arXiv:2506.17238Training a scientific reasoning model for chemistry. 2025arXiv preprint</p>
<p>Developing chemdfm as a large language foundation model for chemistry. Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Yi Xia, Bo Chen, Hongshen Xu, Zichen Zhu, Su Zhu, Cell Reports Physical Science. 642025</p>
<p>Large physics models: towards a collaborative approach with large language models and foundation models. Sascha Kristian G Barman, Emily Caron, Henk W Sullivan, Roberto De Regt, Mieke Ruiz De Austri, Michael Boon, Stefan Färber, Tobias Fröse, Luis G Golling, Lopez, The European Physical Journal C. 85910662025</p>
<p>Rag-enhanced collaborative llm agents for drug discovery. Namkyeong Lee, Edward De Brouwer, Ehsan Hajiramezanali, Tommaso Biancalani, Chanyoung Park, Gabriele Scalia, arXiv:2502.175062025arXiv preprint</p>
<p>Cassia: a multi-agent large language model for reference free, interpretable, and automated cell annotation of single-cell rna-sequencing data. Elliot Xie, Lingxin Cheng, Jack Shireman, Yujia Cai, Jihua Liu, Chitrasen Mohanty, Mahua Dey, Christina Kendziorski, bioRxiv. 2024</p>
<p>Frag: A flexible modular framework for retrieval-augmented generation based on knowledge graphs. Zengyi Gao, Yukun Cao, Hairu Wang, Ao Ke, Yuan Feng, Xike Xie, Kevin Zhou, arXiv:2501.099572025aarXiv preprint</p>
<p>Clinical entity augmented retrieval for clinical information extraction. Ivan Lopez, Akshay Swaminathan, Karthik Vedula, Sanjana Narayanan, Fateme Nateghi Haredasht, Stephen P Ma, April S Liang, Steven Tate, Manoj Maddali, Robert Joseph Gallo, Digital Medicine. 81452025</p>
<p>Kexin Chen, Hanqun Cao, Junyou Li, Yuyang Du, Menghao Guo, Xin Zeng, Lanqing Li, Jiezhong Qiu, arXiv:2402.12993Pheng Ann Heng, and Guangyong Chen. An autonomous large language model agent for chemical literature data mining. 2024aarXiv preprint</p>
<p>Chartassisstant: A universal chart multimodal language model via chart-to-table pre-training and multitask instruction tuning. Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, Yu Qiao, Ping Luo, arXiv:2401.023842024arXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Ghazal Khalighinejad, Sharon Scott, Ollie Liu, Rickard Kelly L Anderson, Aman Stureborg, Bhuwan Tyagi, Dhingra, Matvix, arXiv:2410.20494Multimodal information extraction from visually rich articles. 2024arXiv preprint</p>
<p>Agent-based multimodal information extraction for nanomaterials. Odobesku, Romanova, Mirzaeva, Zagorulko, Sim, Khakimullin, Razlivina, Dmitrenko, Vinogradov, Computational Materials. 1111942025</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, arXiv:2311.059652023arXiv preprint</p>
<p>Toward reliable scientific hypothesis generation: Evaluating truthfulness and hallucination in large language models. Guangzhi Xiong, Eric Xie, Corey Williams, Myles Kim, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, arXiv:2505.145992025arXiv preprint</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, arXiv:2411.023822024arXiv preprint</p>
<p>Research hypothesis generation over scientific knowledge graphs. Knowledge-Based Systems. Agustín Borrego, Danilo Dessì, Daniel Ayala, Inma Hernández, Francesco Osborne, Diego Reforgiato Recupero, Davide Buscaldi, David Ruiz, Enrico Motta, 2025315113280</p>
<p>Hypothesis generation for materials discovery and design using goal-driven and constraint-guided llm agents. Shrinidhi Kumbhar, Venkatesh Mishra, Kevin Coutinho, Divij Handa, Ashif Iquebal, Chitta Baral, 2025a</p>
<p>Many heads are better than one: Improved scientific idea generation by a llm-based multi-agent system. Haoyang Su, Renqi Chen, Shixiang Tang, Zhenfei Yin, Xinzhe Zheng, Jinzhe Li, Biqing Qi, Qi Wu, Hui Li, Wanli Ouyang, arXiv:2410.094032024arXiv preprint</p>
<p>Astroagents: A multi-agent ai for hypothesis generation from mass spectrometry data. Daniel Saeedi, Denise Buckner, Jose C Aponte, Amirali Aghazadeh, arXiv:2503.231702025arXiv preprint</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, 2025</p>
<p>Bayes-entropy collaborative driven agents for research hypotheses generation and optimization. Shiyang Duan, Yuan Tian, Qi Bing, Xiaowei Shao, arXiv:2508.017462025arXiv preprint</p>
<p>Efficient evolutionary search over chemical space with large language models. Haorui Wang, Marta Skreta, Cher-Tian Ser, Wenhao Gao, Lingkai Kong, Felix Strieth-Kalthoff, Chenru Duan, Yuchen Zhuang, Yue Yu, Yanqiao Zhu, arXiv:2406.169762024barXiv preprint</p>
<p>Automated hypothesis validation with agentic sequential falsifications. Kexin Huang, Ying Jin, Ryan Li, Michael Y Li, Emmanuel Candès, Jure Leskovec, 2025a</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics2024c1</p>
<p>Hypothesis generation for materials discovery and design using goal-driven and constraint-guided llm agents. Shrinidhi Kumbhar, Venkatesh Mishra, Kevin Coutinho, Divij Handa, Ashif Iquebal, Chitta Baral, arXiv:2501.132992025barXiv preprint</p>
<p>Hypobench: Towards systematic and principled benchmarking for hypothesis generation. Haokun Liu, Sicong Huang, Jingyu Hu, Yangqiaoyu Zhou, Chenhao Tan, arXiv:2504.115242025aarXiv preprint</p>
<p>Exploiting llms for automatic hypothesis assessment via a logit-based calibrated prior. Yue Gong, Raul Castro Fernandez, arXiv:2506.034442025arXiv preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Pingchuan Ma, Tsun-Hsuan Wang, Minghao Guo, Zhiqing Sun, Joshua B Tenenbaum, Daniela Rus, Chuang Gan, Wojciech Matusik, arXiv:2405.09783Llm and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery. 2024arXiv preprint</p>
<p>Biomni: A general-purpose biomedical ai agent. Kexin Huang, Serena Zhang, Hanchen Wang, Yuanhao Qu, Yingzhou Lu, Yusuf Roohani, Ryan Li, Lin Qiu, Junze Zhang, Yin Di, bioRxiv. 2025b</p>
<p>Txagent: An ai agent for therapeutic reasoning across a universe of tools. Shanghua Gao, Richard Zhu, Zhenglun Kong, Ayush Noori, Xiaorui Su, Curtis Ginder, Theodoros Tsiligkaridis, Marinka Zitnik, arXiv:2503.109702025barXiv preprint</p>
<p>Crispr-gpt for agentic automation of gene-editing experiments. Yuanhao Qu, Kaixuan Huang, Ming Yin, Kanghong Zhan, Dyllan Liu, Di Yin, Henry C Cousins, William A Johnson, Xiaotong Wang, Mihir Shah, Russ B Altman, Denny Zhou, Mengdi Wang, Le Cong, 10.1038/s41551-025-01463-zNature Biomedical Engineering. 2157-846XJuly 2025</p>
<p>The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation. Kyle Swanson, Wesley Wu, L Nash, John E Bulaong, James Pak, Zou, bioRxiv. 2024</p>
<p>Agents for self-driving laboratories applied to quantum computing. Shuxiang Cao, Zijian Zhang, Mohammed Alghadeer, Simone D Fasciati, Michele Piscitelli, Mustafa Bakr, Peter Leek, Alán Aspuru-Guzik, arXiv:2412.079782024arXiv preprint</p>
<p>Perturboagent: A self-planning agent for boosting sequential perturb-seq experiments. Minsheng Hao, Yongju Lee, Hanchen Wang, Gabriele Scalia, Aviv Regev, bioRxiv. 2025</p>
<p>Botao Yu, Frazier N Baker, Ziru Chen, Garrett Herb, Boyu Gou, Daniel Adu-Ampratwum, Xia Ning, Huan Sun, Chemtoolagent, arXiv:2411.07228The impact of tools on language agents for chemistry problem solving. 2024aarXiv preprint</p>
<p>Bioinformatics agent (bia): Unleashing the power of large language models to reshape bioinformatics workflow. Quyu Qi Xin, Hongyi Kong, Yue Ji, Yuqi Shen, Yan Liu, Zhilin Sun, Zhaorong Zhang, Xunlong Li, Bing Xia, Deng, BioRxiv. 2024</p>
<p>Biodiscoveryagent: An ai agent for designing genetic perturbation experiments. Yusuf Roohani, Andrew Lee, Qian Huang, Jian Vora, Zachary Steinhart, Kexin Huang, Alexander Marson, Percy Liang, Jure Leskovec, arXiv:2405.176312024arXiv preprint</p>
<p>OSDA agent: Leveraging large language models for de novo design of organic structure directing agents. Zhaolin Hu, Yixiao Zhou, Zhongan Wang, Xin Li, Weimin Yang, Hehe Fan, Yi Yang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Llmatdesign: Autonomous materials discovery with large language models. Shuyi Jia, Chao Zhang, Victor Fung, 2024</p>
<p>Large language models are innate crystal structure generators. Jingru Gan, Peichen Zhong, Yuanqi Du, Yanqiao Zhu, Chenru Duan, Haorui Wang, Carla P Gomes, Kristin A Persson, Wei Daniel Schwalbe-Koda, Wang, 2025</p>
<p>scbasecount: an ai agent-curated, uniformly processed, and continually expanding single cell data repository. Christopher Nicholas D Youngblut, Jaanak Carpenter, Chiara Prashar, Rajesh Ricci-Tam, Noam Ilango, Silvana Teyssier, Patrick D Konermann, Alexander Hsu, David P Dobin, Burke, bioRxiv. 2025</p>
<p>Agentmd: Empowering language agents for risk prediction with large-scale clinical tool learning. Qiao Jin, Zhizheng Wang, Yifan Yang, Qingqing Zhu, Donald Wright, Thomas Huang, John Wilbur, Zhe He, Andrew Taylor, Qingyu Chen, arXiv:2402.132252024aarXiv preprint</p>
<p>Augmenting large language models with chemistry tools. Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, Nature Machine Intelligence. 652024</p>
<p>scagent: Universal single-cell annotation via a llm agent. Yuren Mao, Yu Mi, Peigen Liu, Mengfei Zhang, Hanqing Liu, Yunjun Gao, arXiv:2504.046982025arXiv preprint</p>
<p>Gis copilot: Towards an autonomous gis agent for spatial analysis. Temitope Akinboyewa, Zhenlong Li, Huan Ning, Lessani Naser, International Journal of Digital Earth. 18124974892025</p>
<p>Scitoolagent: A knowledge graph-driven scientific agent for multi-tool integration. Huajun Chen, Keyan Ding, Jing Yu, Junjie Huang, Yuchen Yang, Qiang Zhang, 2025a</p>
<p>Tooling or not tooling? the impact of tools on language agents for chemistry problem solving. Botao Yu, Frazier N Baker, Ziru Chen, Garrett Herb, Boyu Gou, Daniel Adu-Ampratwum, Xia Ning, Huan Sun, arXiv:2411.072282024barXiv preprint</p>
<p>Medagents: Large language models as collaborators for zero-shot medical reasoning. Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, Mark Gerstein, arXiv:2311.105372023arXiv preprint</p>
<p>Atomagents: Alloy design and discovery through physics-aware multi-modal multi-agent artificial intelligence. Tao Song, Man Luo, Xiaolong Zhang, Linjiang Chen, Yan Huang, Jiaqi Cao, Qing Zhu, Daobin Liu, Baicheng Zhang, Gang Zou, Journal of the American Chemical Society. Alireza Ghafarollahi and Markus J. Buehler147152025. 2024bA multiagent-driven robotic ai chemist enabling autonomous chemical research on demand</p>
<p>Chatgpt research group for optimizing the crystallinity of mofs and cofs. Zhiling Zheng, Oufan Zhang, Nakul Ha L Nguyen, Rampal, Zichao Ali H Alawadhi, Teresa Rong, Christian Head-Gordon, Jennifer T Borgs, Omar M Chayes, Yaghi, ACS Central Science. 9112023</p>
<p>Shanghua Gao, Richard Zhu, Pengwei Sui, Zhenglun Kong, Sufian Aldogom, Yepeng Huang, Ayush Noori, Reza Shamji, Krishna Parvataneni, arXiv:2509.23426Theodoros Tsiligkaridis, et al. Democratizing ai scientists using tooluniverse. 2025carXiv preprint</p>
<p>Toward greater autonomy in materials discovery agents: Unifying planning, physics, and scientists. Lianhao Zhou, Hongyi Ling, Keqiang Yan, Kaiji Zhao, Xiaoning Qian, Raymundo Arróyave, Xiaofeng Qian, Shuiwang Ji, 2025a</p>
<p>Codepde: An inference framework for llm-driven pde solver generation. Shanda Li, Tanya Marwah, Junhong Shen, Weiwei Sun, Andrej Risteski, Yiming Yang, Ameet Talwalkar, 2025a</p>
<p>Alexander Novikov, Ngân Vũ, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, J R Francisco, Ruiz, M Pawan Abbas Mehrabian, Abigail Kumar, Swarat See, George Chaudhuri, Holland, Pushmeet Kohli, and Matej Balog. Alphaevolve: A coding agent for scientific and algorithmic discovery. Alex Davies, Sebastian Nowozin2025</p>
<p>Georg Wölflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelović, Jakob Nikolas Kather, arXiv:2502.11705Llm agents making agent tools. 2025arXiv preprint</p>
<p>Shinkaevolve: Towards open-ended and sample-efficient program evolution. Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin, arXiv:2509.193492025arXiv preprint</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025arXiv preprint</p>
<p>Plot-Gen: A New Era of LLM-Based Multi-Agent Framework for Scientific Visualization Generation. Yancheng Luo, Zihan Wang, Jingwen Li, Ziqi Min, Kexin Xiao, Howard Liu, Zijie J Wang, 2024</p>
<p>Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, Hanwang Zhang, arXiv:2311.16483Chartllama: A multimodal llm for chart understanding and generation. 2023arXiv preprint</p>
<p>Sam Andres M Bran, Oliver Cox, Carlo Schilter, Andrew D Baldassari, Philippe White, Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>Florian Boeck, Mogens Jørgensen, Jesper Jørgensen, Nikolaj , -Others Båk, arXiv:2310.09133A multi-agent llm framework for accelerating materials design. 2023arXiv preprint</p>
<p>Data interpreter: An llm agent for data science. Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Ceyao Zhang, Chenxing Wei, Danyang Li, Jiaqi Chen, Jiayi Zhang, arXiv:2402.186792024arXiv preprint</p>
<p>LLM-SR: A framework for discovering scientific equations with large language models. Pierre-Alexandre Kamienny, D' Stéphane, Guillaume Ascoli, François Lample, Charton, 2024</p>
<p>Autonomous scientific discovery with large language models. Boran Binbas, Meng-Chen Hsieh, Alán Aspuru-Guzik, Florian E Futschik, Mario Krenn, 2024</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202336</p>
<p>Chemagent: Self-updating library in large language models improves chemical reasoning. Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou, Pan Lu, Zhuosheng Zhang, Yilun Zhao, Arman Cohan, Mark Gerstein, 2025</p>
<p>Spatialagent: An autonomous ai agent for spatial biology. Hanchen Wang, Yichun He, Paula P Coelho, Matthew Bucci, Abbas Nazir, Bob Chen, Linh Trinh, Serena Zhang, Kexin Huang, Vineethkrishna Chandrasekar, bioRxiv. 2025a</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023arXiv preprint</p>
<p>Self-driving laboratories to autonomously navigate the protein fitness landscape. Bennett J Jacob T Rapp, Philip A Bremer, Romero, Nature chemical engineering. 112024</p>
<p>Bioagents: Democratizing bioinformatics analysis with multi-agent systems. Nikita Mehandru, Amanda K Hall, Olesya Melnichenko, Yulia Dubinina, Daniel Tsirulnikov, David Bamman, Ahmed Alaa, Scott Saponas, Venkat S Malladi, arXiv:2501.063142025arXiv preprint</p>
<p>Cellagent: An llm-driven multi-agent framework for automated single-cell data analysis. Yihang Xiao, Jinyi Liu, Yan Zheng, Xiaohan Xie, Jianye Hao, Mingzhi Li, Ruitao Wang, Fei Ni, Yuxiao Li, Jintian Luo, arXiv:2407.098112024arXiv preprint</p>
<p>Compbioagent: An llm-powered agent for single-cell rna-seq data exploration. Haotian Zhang, Yu H Sun, Wenxing Hu, Xu Cui, Zhengyu Ouyang, Derrick Cheng, Xinmin Zhang, Baohong Zhang, bioRxiv. 2025b</p>
<p>Alireza Ghafarollahi, Markus J Buehler, arXiv:2504.19017Sparks: Multi-agent artificial intelligence model discovers protein design principles. 2025arXiv preprint</p>
<p>Agentic end-to-end de novo protein design for tailored dynamics using a language diffusion model. Bo Ni, Markus J Buehler, arXiv:2502.101732025arXiv preprint</p>
<p>Autoproteinengine: A large language model driven agent framework for multimodal automl in protein engineering. Yungeng Liu, Zan Chen, Yu Guang Wang, Yiqing Shen, arXiv:2411.044402024barXiv preprint</p>
<p>Protchat: An ai multi-agent for automated protein analysis leveraging gpt-4 and protein language model. Huazhen Huang, Xianguo Shi, Hongyang Lei, Fan Hu, Yunpeng Cai, Journal of Chemical Information and Modeling. 6512024b</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025barXiv preprint</p>
<p>From intention to implementation: automating biomedical research via llms. Yi Luo, Linghang Shi, Yihao Li, Aobo Zhuang, Yeyun Gong, Ling Liu, Chen Lin, Science China Information Sciences. 6872025b</p>
<p>Ruofan Jin, Zaixi Zhang, Mengdi Wang, Le Cong, Stella, arXiv:2507.02004Self-evolving llm agent for biomedical research. 2025aarXiv preprint</p>
<p>Drugagent: Automating ai-aided drug discovery programming through llm multi-agent collaboration. Sizhe Liu, Yizhou Lu, Siyu Chen, Xiyang Hu, Jieyu Zhao, Yingzhou Lu, Yue Zhao, arXiv:2411.156922024carXiv preprint</p>
<p>Bowen Gao, Yanwen Huang, Yiqiao Liu, Wenxuan Xie, Wei-Ying Ma, Ya-Qin Zhang, Yanyan Lan, arXiv:2503.22164Pharmagents: Building a virtual pharma with large language model agents. 2025darXiv preprint</p>
<p>Reza Averly, Frazier N Baker, Ian A Watson, Xia Ning, arXiv:2502.13959Liddia: Language-based intelligent drug discovery agent. 2025arXiv preprint</p>
<p>Clinicalgpt: large language models finetuned with diverse medical data and comprehensive evaluation. Guangyu Wang, Guoxing Yang, Zongxin Du, Longjun Fan, Xiaohu Li, arXiv:2306.099682023barXiv preprint</p>
<p>An autonomous ai agent for universal behavior analysis. Almir Aljovic, Zuwan Lin, Wenbo Wang, Xinhe Zhang, Arnau Marin-Llobet, Ningyue Liang, Bradley Canales, Jaeyong Lee, Jongmin Baek, Ren Liu, bioRxiv. 2025</p>
<p>Chemreasoner: Heuristic search over a large language model's knowledge space using quantum-chemical feedback. Carl Henry W Sprueill, Khushbu Edwards, Agarwal, Udishnu Mariefel V Olarte, Conrad Sanyal, Hongbin Johnston, Heng Liu, Sutanay Ji, Choudhury, arXiv:2402.109802024arXiv preprint</p>
<p>Cactus: Chemistry agent connecting tool usage to science. Gautham Andrew D Mcnaughton, Krishna Sankar, Agustin Ramalaxmi, Kruel, Rohith A Carter R Knutson, Neeraj Varikoti, Kumar, ACS omega. 9462024</p>
<p>El Agente: An autonomous agent for quantum chemistry. Yunheng Zou, Austin H Cheng, Abdulrahman Aldossary, Jiaru Bai, Xuan Shi, Jorge Leong, Arturo Campos-Gonzalez-Angulo, Changhyeok Choi, Cher Tian Ser, Gary Tom, Andrew Wang, Matter. 872025</p>
<p>An automatic end-to-end chemical synthesis development platform powered by large language models. Yixiang Ruan, Chenyin Lu, Ning Xu, Yuchen He, Yixin Chen, Jian Zhang, Jun Xuan, Jianzhang Pan, Qun Fang, Hanyu Gao, Nature communications. 151101602024</p>
<p>Frogent: An end-to-end full-process drug design agent. Qihua Pan, Dong Xu, Jenna Xinyi Yao, Lijia Ma, Zexuan Zhu, Junkai Ji, arXiv:2508.107602025arXiv preprint</p>
<p>Larc: Towards human-level constrained retrosynthesis planning through an agentic framework. Daniel Frazier N Baker, Reza Adu-Ampratwum, Botao Averly, Huan Yu, Xia Sun, Ning, arXiv:2508.118602025arXiv preprint</p>
<p>Foundation molecular grammar: Multi-modal foundation models induce interpretable molecular graph languages. Michael Sun, Weize Yuan, Gang Liu, Wojciech Matusik, Jie Chen, arXiv:2505.229482025arXiv preprint</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, arXiv:2410.070762024arXiv preprint</p>
<p>Chatmof: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models. Yeonghun Kang, Jihan Kim, Nature communications. 15147052024</p>
<p>A fine-tuned large language model based molecular dynamics agent for code generation to obtain material thermodynamic parameters. Zhuofan Shi, Chunxiao Xin, Tong Huo, Yuntao Jiang, Bowen Wu, Xingyue Chen, Wei Qin, Xinjian Ma, Gang Huang, Zhenyu Wang, Scientific Reports. 151102952025</p>
<p>Huan Zhang, Yu Song, Ziyu Hou, Santiago Miret, Bang Liu, arXiv:2409.00135Honeycomb: A flexible llm-based agent system for materials science. 2024barXiv preprint</p>
<p>Matagent: A human-in-the-loop multi-agent llm framework for accelerating the material science discovery cycle. Adib Bazgir, Yuwen Zhang, AI for Accelerated Materials Design-ICLR 2025. 2025a</p>
<p>Modular large language model agents for multi-task computational materials science. Akshat Chaudhari, Janghoon Ock, Amir Barati, Farimani , ChemRxiv. 2025</p>
<p>Matpilot: an llm-enabled ai materials scientist under the framework of human-machine collaboration. Ziqi Ni, Yahao Li, Kaijia Hu, Kunyuan Han, Ming Xu, Xingyu Chen, Fengqi Liu, Yicong Ye, Shuxin Bai, arXiv:2411.080632024arXiv preprint</p>
<p>Bo Hu, Siyu Liu, Beilin Ye, Yun Hao, Tongqi Wen, arXiv:2411.16416A multi-agent framework for materials laws discovery. 2024arXiv preprint</p>
<p>Atomagents: Alloy design and discovery through physics-aware multi-modal multi-agent artificial intelligence. Alireza Ghafarollahi, Markus J Buehler ; Yuan-Sen, Yaobo Ting, Nan Liang, Song Duan, Zheng Huang, Cai, arXiv:2407.10022arXiv:2409.14807Interpreting multi-band galaxy observations with large language model-based agents. 2024c. 2024arXiv preprint</p>
<p>Llm-based multi-agent copilot for quantum sensor. Rong Sha, Binglin Wang, Jun Yang, Xiaoxiao Ma, Chengkun Wu, Liang Yan, Chao Zhou, Jixun Liu, Guochao Wang, Shuhua Yan, arXiv:2508.054212025arXiv preprint</p>
<p>Openfoamgpt: A retrieval-augmented large language model (llm) agent for openfoam-based computational fluid dynamics. Sandeep Pandey, Ran Xu, Wenkang Wang, Xu Chu, Physics of Fluids. 3732025</p>
<p>Metaopenfoam: an llm-based multi-agent framework for cfd. Yuxuan Chen, Xu Zhu, Hua Zhou, Zhuyin Ren, arXiv:2407.213202024barXiv preprint</p>
<p>Advancing ai-scientist understanding: Making llm think like a physicist with interpretable reasoning. Yinggan Xu, Hana Kimlee, Yijia Xiao, Di Luo, arXiv:2504.019112025aarXiv preprint</p>
<p>An autonomous gis agent framework for geospatial data retrieval. Huan Ning, Zhenlong Li, Temitope Akinboyewa, Lessani Naser, International Journal of Digital Earth. 18124586882025</p>
<p>Domain-specific react for physics-integrated iterative modeling: A case study of llm agents for gas path analysis of gas turbines. Junhua Liu, Fanfan Lin, Xinze Li, Kwan Hui Lim, Shuai Zhao ; Tao, Yuwei Song, Chenlong Fan, Keyu Feng, Chao Song, Dongxiang Liu, Jiang, arXiv:2411.14214arXiv:2406.075722024d. 2024arXiv preprintPhysics-informed llm-agent for automated modulation design in power electronics systems</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, Proc. The 62nd Annual Meeting of the Association for Computational Linguistics (ACL2024). The 62nd Annual Meeting of the Association for Computational Linguistics (ACL2024)2024d</p>
<p>Actionie: Action extraction from scientific literature with programming languages. Xianrui Zhong, Yufeng Du, Siru Ouyang, Ming Zhong, Tingfeng Luo, Qirong Ho, Hao Peng, Heng Ji, Jiawei Han, Proc. The 62nd Annual Meeting of the Association for Computational Linguistics (ACL2024), 2024. OpenAI. Chatgpt. The 62nd Annual Meeting of the Association for Computational Linguistics (ACL2024), 2024. OpenAI. Chatgpt2024</p>
<p>Mohamed Amine Ferrag, Norbert Tihanyi, Merouane Debbah, arXiv:2504.19678From llm reasoning to autonomous ai agents: A comprehensive review. 2025arXiv preprint</p>
<p>Llm as a mastermind: A survey of strategic reasoning with large language models. Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian De Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, Furu Wei, arXiv:2404.012302024carXiv preprint</p>
<p>Hui Wei, Zihao Zhang, Shenghua He, Tian Xia, Shijia Pan, Fei Liu, arXiv:2502.11221Plangenllms: A modern survey of llm planning capabilities. 2025barXiv preprint</p>
<p>Understanding the planning of llm agents: A survey. Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, Enhong Chen, arXiv:2402.027162024carXiv preprint</p>
<p>From llms to llmbased agents for software engineering: A survey of current, challenges and future. Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, Huaming Chen, arXiv:2408.024792024barXiv preprint</p>
<p>Zhuocheng Shen, arXiv:2409.18807Llm with tools: A survey. 2024arXiv preprint</p>
<p>A-mem: Agentic memory for llm agents. Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, Yongfeng Zhang, arXiv:2502.121102025barXiv preprint</p>
<p>From human memory to ai memory: A survey on memory mechanisms in the era of llms. Yaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang, Yongyue Zhang, Huifeng Guo, Ruiming Tang, Yong Liu, arXiv:2504.159652025arXiv preprint</p>
<p>Information-theoretic measures for knowledge discovery and data mining. Yao, 2003Entropy measures, maximum entropy principle and emerging applications</p>
<p>Claus Borgnakke, Fundamentals of Thermodynamics. Hoboken, NJWiley202411th edition</p>
<p>An Introduction to Thermal Physics. V Daniel, Schroeder, 2020Oxford University Press</p>
<p>Verifiability in systematics. Colin Patterson, Systematic Zoology. 2721978</p>
<p>Irreversibility and heat generation in the computing process. Rolf Landauer, IBM journal of research and development. 531961</p>
<p>From reasoning to learning: A survey on hypothesis discovery and rule learning with large language models. Don R Swanson, arXiv:2505.21935Undiscovered public knowledge. The Library Quarterly. 1986. 202556arXiv preprintKaiyu He and Zhiyu Chen</p>
<p>A review of relational machine learning for knowledge graphs. Maximilian Nickel, Kevin Murphy, Evgeniy Volker Tresp, Gabrilovich, Proceedings of the IEEE. 10412015</p>
<p>A survey on knowledge graphs: Representation, acquisition, and applications. Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, Philip S Yu, IEEE transactions on neural networks and learning systems. 202133</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A Smith, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Foundation models for materials discovery-current state and future directions. Matteo Edward O Pyzer-Knapp, Peter Manica, Lucas Staar, Patrick Morin, Teodoro Ruch, John R Laino, Alessandro Smith, Curioni, Npj Computational Materials. 111612025</p>
<p>Towards foundation models for scientific machine learning: Characterizing scaling and transfer behavior. Shashank Subramanian, Peter Harrington, Kurt Keutzer, Wahid Bhimji, Dmitriy Morozov, Michael W Mahoney, Amir Gholami, Advances in Neural Information Processing Systems. 362023</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Mingwei Chang, International conference on machine learning. PMLR2020</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, Haofen Wang, arXiv:2312.1099722023arXiv preprint</p>
<p>Tat-Seng Chua, and Qing Li. A survey on rag meeting llms: Towards retrieval-augmented large language models. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave ; Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining. the 30th ACM SIGKDD conference on knowledge discovery and data mining2023. 202424Atlas: Few-shot learning with retrieval augmented language models</p>
<p>Lino Gabriel, Joao Garcia, Renato Ribeiro, Pedro Henrique Manesco, Lucas Paiola, Miranda, arXiv:2412.03531Maria Paola de Salvo, and Joao Paulo Papa. A review on scientific knowledge extraction using large language models in biomedical sciences. 2024arXiv preprint</p>
<p>A retrieval-augmented knowledge mining method with deep thinking llms for biomedical research and clinical support. Yichun Feng, Jiawei Wang, Ruikun He, Lu Zhou, Yixue Li, GigaScience. 141092025a</p>
<p>Nanostructured material design via a retrieval-augmented generation (rag) approach: Bridging laboratory practice and scientific literature. Nikita A Krotkov, Dmitrii A Sbytov, Anna A Chakhoyan, I Polina, Anna A Kornienko, Maxim G Starikova, Stepanov, Timur A Anastasiia O Piven, Tetiana Aliev, Orlova, Mushegh S Rafayelyan, Journal of Chemical Information and Modeling. 2025</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, arXiv:2404.04326Hypothesis generation with large language models. 2024arXiv preprint</p>
<p>Proteinhypothesis: A physics-aware chain of multi-agent rag llm for hypothesis generation in protein science. Adib Bazgir, Yuwen Zhang, Towards Agentic AI for Science: Hypothesis Generation, Comprehension, Quantification, and Validation. 2025b</p>
<p>Automating psychological hypothesis generation with ai: when large language models meet causal graph. Song Tong, Kai Mao, Zhen Huang, Yukun Zhao, Kaiping Peng, Humanities and Social Sciences Communications. 1112024</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation. Qingyun Wu, Gagan Bansal, Jie Zhang, Yiran Wu, Shaokun Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Chi Wang, Song Han, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>. Harrison Chase, Langchain, 10 2022</p>
<p>Beyond brainstorming: What drives high-quality scientific ideas? lessons from multi-agent collaboration. Nuo Chen, Yicheng Tong, Jiaying Wu, Minh Duc Duong, Qian Wang, Qingyun Zou, Bryan Hooi, Bingsheng He, arXiv:2508.045752025barXiv preprint</p>
<p>Large language model for multiobjective evolutionary optimization. Fei Liu, Xi Lin, Shunyu Yao, Zhenkun Wang, Xialiang Tong, Mingxuan Yuan, Qingfu Zhang, International Conference on Evolutionary Multi-Criterion Optimization. Springer2025b</p>
<p>Large language model agent as a mechanical designer. Yayati Jadhav, Amir Barati, Farimani , arXiv:2404.175252024arXiv preprint</p>
<p>Large language models design sequence-defined macromolecules via evolutionary optimization. F Wesley, Antonia Reinhart, Statt, Computational Materials. 1012622024</p>
<p>Ke Tang, and Yew-Soon Ong. Large language models as evolutionary optimizers. Shengcai Liu, Caishun Chen, Xinghua Qu, IEEE Congress on Evolutionary Computation (CEC). 2024. 2024eIEEE</p>
<p>Large language model-based evolutionary optimizer: Reasoning with elitism. Shuvayan Brahmachary, M Subodh, Aniruddha Joshi, Kaushik Panda, Arun Koneripalli, Harshil Kumar Sagotra, Ankush Patel, Ameya D Sharma, Kaushic Jagtap, Kalyanaraman, Neurocomputing. 6221292722025</p>
<p>A survey on hypothesis generation for scientific discovery in the era of large language models. Shashwat Atilla Kaan Alkan, Maja Sourav, Simone Jablonska, Rishabh Astarita, Nikhil Chakrabarty, Pranav Garuda, Maciej Khetarpal, Dimitrios Pióro, Tanoglidis, G Kartheik, Iyer, arXiv:2504.054962025arXiv preprint</p>
<p>Agentichypothesis: A survey on hypothesis generation using llm systems. Adib Bazgir, Yuwen Zhang, Towards Agentic AI for Science: Hypothesis Generation, Comprehension, Quantification, and Validation. 2025c</p>
<p>Evaluating sakana's ai scientist for autonomous research: Wishful thinking or an emerging reality towards' artificial research intelligence. Joeran Beel, Min-Yen Kan, Moritz Baumgart, arXiv:2502.142972025arXiv preprint</p>
<p>Xiao Xiao, Yu Su, Sijing Zhang, Zhang Chen, Yadong Chen, Tian Liu, arXiv:2504.21303Confidence in large language model evaluation: A bayesian approach to limited-sample challenges. 2025arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>Executable code actions elicit better llm agents. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji, Forty-first International Conference on Machine Learning. 2024e</p>
<p>Evolutionary-scale prediction of atomic-level protein structure with a language model. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Science. 37966372023</p>
<p>Accurate structure prediction of biomolecular interactions with alphafold 3. Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, Nature. 63080162024</p>
<p>De novo design and rosetta-based assessment of high-affinity antibody variable regions (fv) against the sars-cov-2 spike receptor binding domain (rbd). Veda Sheersh Boorla, Ratul Chowdhury, Ranjani Ramasubramanian, Brandon Ameglio, Rahel Frick, Jeffrey J Gray, Costas D Maranas, Proteins: Structure, Function, and Bioinformatics. 9122023</p>
<p>Reinforced generation of combinatorial structures: Applications to complexity theory. Ansh Nagda, Prabhakar Raghavan, Abhradeep Thakurta, arXiv:2509.180572025arXiv preprint</p>
<p>Agentic ai for scientific discovery: A survey of progress, challenges, and future directions. M Gridach, L Chen, Y Li, S Wang, X Zhang, arXiv:2503.089792025barXiv preprint</p>
<p>Challenges in ai-driven biomedical multimodal data fusion and analysis. J Liu, X Cen, C Yi, W Huang, H Wang, 10.1093/gpbjnl/qzaf011Proteomics &amp; Bioinformatics. 231112025cGenomics</p>
<p>Robust multimodal perception in autonomous systems: a comprehensive review and enhancement strategies. Q Zhang, L Wang, P Liu, M Zhao, 2024dResearchGate</p>
<p>Geometry informed tokenization of molecules for language model generation. Xiner Li, Limei Wang, Youzhi Luo, Carl Edwards, Shurui Gui, Yuchao Lin, Ji Heng, Shuiwang Ji, arXiv:2408.101202024aarXiv preprint</p>
<p>Fragment and geometry aware tokenization of molecules for structure-based drug design using language models. Cong Fu, Xiner Li, Blake Olson, Heng Ji, Shuiwang Ji, arXiv:2408.097302024arXiv preprint</p>
<p>Invariant Tokenization of Crystalline Materials for Language Model Enabled Generation. Keqiang Yan, Xiner Li, Hongyi Ling, Kenna Ashen, Carl Edwards, Raymundo Arróyave, Marinka Zitnik, Heng Ji, Xiaofeng Qian, Xiaoning Qian, Shuiwang Ji, Advances in Neural Information Processing Systems. 202437</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, arXiv:2304.051282023barXiv preprint</p>
<p>Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chao Yang, Helen Meng, arXiv:2506.03106Critiquegrpo: Advancing llm reasoning with natural language and numerical feedback. 2025carXiv preprint</p>
<p>Pag: Multi-turn reinforced llm self-correction with policy as generative verifier. Yuhua Jiang, Yuwen Xiong, Yufeng Yuan, Chao Xin, Wenyuan Xu, Yu Yue, Qianchuan Zhao, Lin Yan, arXiv:2506.104062025arXiv preprint</p>
<p>Xiaoyuan Liu, Tian Liang, Zhiwei He, Jiahao Xu, Wenxuan Wang, Pinjia He, Zhaopeng Tu, Haitao Mi, Dong Yu, arXiv:2505.13445Trust, but verify: A self-verification approach to reinforcement learning with verifiable rewards. 2025darXiv preprint</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, arXiv:2310.017982023arXiv preprint</p>
<p>The ucsc genome browser and associated tools. David Robert M Kuhn, James Haussler, Kent, Briefings in bioinformatics. 1422013</p>
<p>Alphagenome: advancing regulatory variant effect prediction with a unified dna sequence model. Žiga Avsec, Natasha Latysheva, Jun Cheng, Guido Novati, Tom Kyle R Taylor, Clare Ward, Lauren Bycroft, Eirini Nicolaisen, Joshua Arvaniti, Pan, bioRxiv. 2025</p>
<p>Genome modeling and design across all domains of life with evo 2. Garyk Brixi, Matthew G Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A Gonzalez, Samuel H King, David B Li, Aditi T Merchant, BioRxiv. 2025</p>
<p>Effective gene expression prediction from sequence by integrating long-range interactions. Žiga Avsec, Vikram Agarwal, Daniel Visentin, Agnieszka Joseph R Ledsam, Kyle R Grabska-Barwinska, Yannis Taylor, John Assael, Pushmeet Jumper, David R Kohli, Kelley, Nature methods. 18102021</p>
<p>Sequence modeling and design from molecular to genome scale with evo. Eric Nguyen, Michael Poli, Brian Matthew G Durrant, Dhruva Kang, David B Katrekar, Liam J Li, Armin W Bartie, Samuel H Thomas, Garyk King, Brixi, Science. 386672393362024</p>
<p>Learning to discover regulatory elements for gene expression prediction. Xingyu Su, Haiyang Yu, Degui Zhi, Shuiwang Ji, arXiv:2502.139912025aarXiv preprint</p>
<p>Language models for controllable dna sequence design. Xingyu Su, Xiner Li, Yuchao Lin, Ziqian Xie, Degui Zhi, Shuiwang Ji, arXiv:2507.195232025barXiv preprint</p>
<p>Integrated analysis of multimodal single-cell data. Y Hao, S Hao, J T Andersen, Cell. 184132021</p>
<p>Scanpy: large-scale single-cell gene expression data analysis. F A Wolf, P Angerer, F J Theis, Genome Biology. 191152018</p>
<p>Highly accurate protein structure prediction with AlphaFold. John Jumper, Nature. 59678732021</p>
<p>Simulating 500 million years of evolution with a language model. Thomas Hayes, Roshan Rao, Halil Akin, Nicholas J Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Jonathan Vincent Q Tran, Marius Deaton, Wiggert, Science. 38767362025</p>
<p>Rosetta3: an object-oriented software suite for macromolecular modeling and design. Andrew Leaver, - Fay, Methods in Enzymology. 4872011</p>
<p>De novo design of protein structure and function with rfdiffusion. David Joseph L Watson, Nathaniel R Juergens, Brian L Bennett, Jason Trippe, Helen E Yim, Woody Eisenach, Andrew J Ahern, Robert J Borst, Lukas F Ragotte, Milles, Nature. 62079762023</p>
<p>Robust deep learning-based protein sequence design using proteinmpnn. Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F Milles, Alexis Basile Im Wicky, Rob J De Courbet, Neville Haas, Bethel, Science. 37866152022</p>
<p>RDKit: Open-source cheminformatics. Greg Landrum, Journal of Cheminformatics. 112006</p>
<p>Advances in molecular quantum chemistry contained in the Q-Chem 3.0 program package. Yihan Shao, Physical Chemistry Chemical Physics. 8272006</p>
<p>Psi4: an open-source ab initio electronic structure program. G A Daniel, L A Smith, D A Burns, Sirianni, The Journal of Chemical Physics. 152181841082020</p>
<p>The Atomic Simulation Environment-A Python library for working with atoms. Ask Larsen, Journal of Physics: Condensed Matter. 29272730022017</p>
<p>Efficiency of ab-initio total energy calculations for metals and semiconductors using a plane-wave basis set. Georg Kresse, Jürgen Furthmüller, Computational Materials Science. 611996</p>
<p>Fast parallel algorithms for short-range molecular dynamics. Steve Plimpton, Journal of Computational Physics. 11711995</p>
<p>LAMMPS -a flexible simulation tool for particle-based materials modeling at the atomic, meso, and continuum scales. A P Thompson, H M Aktulga, R Berger, D S Bolintineanu, W M Brown, P S Crozier, P J Veld, A Kohlmeyer, S G Moore, T D Nguyen, R Shan, M J Stevens, J Tranchida, C Trott, S J Plimpton, 10.1016/j.cpc.2021.108171Comp. Phys. Comm. 2712022</p>
<p>Efficient and Equivariant Graph Networks for Predicting Quantum Hamiltonian. Haiyang Yu, Zhao Xu, Xiaofeng Qian, Xiaoning Qian, Shuiwang Ji, International Conference on Machine Learning. PMLR2023a</p>
<p>QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules. Haiyang Yu, Meng Liu, Youzhi Luo, Alex Strasser, Xiaofeng Qian, Xiaoning Qian, Shuiwang Ji, Advances in Neural Information Processing Systems. 362023b</p>
<p>Efficient prediction of SO(3)-equivariant Hamiltonian matrices via SO(2) local frames. Haiyang Yu, Yuchao Lin, Xuan Zhang, Xiaofeng Qian, Shuiwang Ji, arXiv:2506.09398OpenCFD Ltd. OpenFOAM: The Open Source Computational Fluid Dynamics Toolbox. 2025. 2024arXiv preprint</p>
<p>CIGALE, a code for galaxy evolution: a new version with improved features. S Noll, D Burgarella, E Giovannoli, V Buat, D Marcillac, J C Muñoz-Mateos, Astron. Astrophys. 5072009</p>
<p>Geneagent: self-verification language agent for gene-set analysis using domain databases. Zhizheng Wang, Qiao Jin, Chih-Hsuan Wei, Shubo Tian, Po-Ting Lai, Qingqing Zhu, Chi-Ping Day, Christina Ross, Robert Leaman, Zhiyong Lu, 10.1038/s41592-025-02748-6Nature Methods. 1548-7105228August 2025b</p>
<p>Spatial transcriptomics ai agent charts hpsc-pancreas maturation in vivo. Zuwan Lin, Wenbo Wang, Arnau Marin-Llobet, Qiang Li, Samuel D Pollock, Xin Sui, Almir Aljovic, Jaeyong Lee, Jongmin Baek, Ningyue Liang, Xinhe Zhang, Connie Kangni Wang, Jiahao Huang, Mai Liu, Zihan Gao, Hao Sheng, Jin Du, Stephen J Lee, Brandon Wang, Yichun He, Jie Ding, Xiao Wang, Juan R Alvarez-Dominguez, Jia Liu, 10.1101/2025.04.01.646731bioRxiv. 2025</p>
<p>Mragent: an llm-based automated agent for causal knowledge discovery in disease via mendelian randomization. Wei Xu, Gang Luo, Weiyu Meng, Xiaobing Zhai, Keli Zheng, Ji Wu, Yanrong Li, Abao Xing, Junrong Li, Zhifan Li, Ke Zheng, Kefeng Li, 10.1093/bib/bbaf140/8107848Briefings in Bioinformatics. 1467-5463262March 2025c</p>
<p>Prime: A multi-agent environment for orchestrating dynamic computational workflows in protein engineerings. Yuyang Zhou, Jin Su, Jiawei Zhang, Wangyang Hu, Tianli Tao, Guanqi Li, Xibin Zhou, Li Fan, Fajie Yuan, 10.1101/2025.09.22.677756bioRxiv. 2025b</p>
<p>Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Sergey Quoc V Le, Yi Levine, Ma, arXiv:2501.17161Sft memorizes, rl generalizes: A comparative study of foundation model post-training. 2025arXiv preprint</p>
<p>Curriculum reinforcement learning from easy to hard tasks improves llm reasoning. Shubham Parashar, Shurui Gui, Xiner Li, Hongyi Ling, Sushil Vemuri, Blake Olson, Eric Li, Yu Zhang, James Caverlee, Dileep Kalathil, arXiv:2506.066322025arXiv preprint</p>
<p>Iterative distillation for rewardguided fine-tuning of diffusion models in biomolecular design. Xingyu Su, Xiner Li, Masatoshi Uehara, Sunwoo Kim, Yulai Zhao, Gabriele Scalia, Ehsan Hajiramezanali, Tommaso Biancalani, Degui Zhi, Shuiwang Ji, arXiv:2507.004452025carXiv preprint</p>
<p>Derivative-free guidance in continuous and discrete diffusion models with soft value-based decoding. Xiner Li, Yulai Zhao, Chenyu Wang, Gabriele Scalia, Gokcen Eraslan, Surag Nair, Tommaso Biancalani, Shuiwang Ji, Aviv Regev, Sergey Levine, arXiv:2408.082522024barXiv preprint</p>
<p>Dynamic search for inference-time alignment in diffusion models. Xiner Li, Masatoshi Uehara, Xingyu Su, Gabriele Scalia, Tommaso Biancalani, Aviv Regev, Sergey Levine, Shuiwang Ji, arXiv:2503.020392025barXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Retool: Reinforcement learning for strategic tool use in llms. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, Wanjun Zhong, 2025b</p>
<p>Agent rl scaling law: Spontaneous code execution for mathematical problem solving. Xinji Mai, Haotian Xu, Xing Wu, Weinong Wang, Yingying Zhang, Wenqiang Zhang, 2025</p>
<p>Search-r1: Training llms to reason and leverage search engines with reinforcement learning. Hansi Bowen Jin, Zhenrui Zeng, Jinsung Yue, Sercan Ö Yoon, Dong Arık, Hamed Wang, Jiawei Zamani, Han, 2025b</p>
<p>Research: Learning to reason with search for llms via reinforcement learning. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, Weipeng Chen, 2025c</p>
<p>Webagent-r1: Training web agents via endto-end multi-turn reinforcement learning. Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, Lihong Li, 2025c</p>
<p>Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, Yuxiao Dong, 2025</p>
<p>Ui-tars: Pioneering automated gui interaction with native agents. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, arXiv:2501.123262025arXiv preprint</p>
<p>Agent s: An open agentic framework that uses computers like a human. Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, Xin Eric, Wang ; Xinyu Geng, Peng Xia, Zhen Zhang, Xinyu Wang, Qiuchen Wang, Ruixue Ding, Chenxi Wang, Jialong Wu, Yida Zhao, Kuan Li, arXiv:2410.08164arXiv:2508.05748Breaking new frontier of vision-language deep research agent. 2024. 2025arXiv preprint</p>
<p>Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, Zhicheng Dou, arXiv:2504.21776Webthinker: Empowering large reasoning models with deep research capability. 2025carXiv preprint</p>
<p>Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, arXiv:2505.02387Rm-r1: Reward modeling as reasoning. 2025darXiv preprint</p>
<p>Inference-time alignment in diffusion models with reward-guided generation: Tutorial and review. Masatoshi Uehara, Yulai Zhao, Chenyu Wang, Xiner Li, Aviv Regev, Sergey Levine, Tommaso Biancalani, arXiv:2501.096852025aarXiv preprint</p>
<p>Masatoshi Uehara, Xingyu Su, Yulai Zhao, Xiner Li, Aviv Regev, Shuiwang Ji, Sergey Levine, Tommaso Biancalani, arXiv:2502.14944Reward-guided iterative refinement in diffusion models at test-time with applications to protein and dna design. 2025barXiv preprint</p>
<p>Neither LLMs nor LRMs have the ability to go beyond humanity's knowledge closure-which is needed for true discoveries. Subbarao Kambhampati, 2025</p>
<p>On the antibacterial action of cultures of a penicillium, with special reference to their use in the isolation of b. influenzae. Alexander Fleming, British Journal of Experimental Pathology. 101929</p>
<p>A measurement of excess antenna temperature at 4080 mc/s. A A Penzias, R W Wilson, The Astrophysical Journal. 14211965</p>
<p>Electric field effect in atomically thin carbon films. K S Novoselov, A K Geim, S V Morozov, D Jiang, Y Zhang, S V Dubonos, I V Grigorieva, A A Firsov, 10.1126/science.1102896Science. 30656962004</p>            </div>
        </div>

    </div>
</body>
</html>
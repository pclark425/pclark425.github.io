<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2527 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2527</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2527</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-258841365</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.14259v7.pdf" target="_blank">SciMON: Scientific Inspiration Machines Optimized for Novelty</a></p>
                <p><strong>Paper Abstract:</strong> We explore and enhance the ability of neural language models to generate novel scientific directions grounded in literature. Work on literature-based hypothesis generation has traditionally focused on binary link prediction--severely limiting the expressivity of hypotheses. This line of work also does not focus on optimizing novelty. We take a dramatic departure with a novel setting in which models use as input background contexts (e.g., problems, experimental settings, goals), and output natural language ideas grounded in literature. We present SciMON, a modeling framework that uses retrieval of"inspirations"from past scientific papers, and explicitly optimizes for novelty by iteratively comparing to prior papers and updating idea suggestions until sufficient novelty is achieved. Comprehensive evaluations reveal that GPT-4 tends to generate ideas with overall low technical depth and novelty, while our methods partially mitigate this issue. Our work represents a first step toward evaluating and developing language models that generate new ideas derived from the scientific literature</p>
                <p><strong>Cost:</strong> 0.029</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2527.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2527.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCIMON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Inspiration Machines Optimized for Novelty (SCIMON)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented LLM framework that generates natural-language scientific idea suggestions grounded in literature and explicitly optimizes for novelty by iteratively comparing generated ideas to prior work and updating them until they are sufficiently novel.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SCIMON</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>End-to-end framework that (1) constructs training instances by extracting background/problem sentences and idea sentences from scientific papers via IE; (2) augments background context with retrieved 'inspirations' from three sources (semantic neighbors from a training-set similarity graph, KG neighbors from a background scientific knowledge graph, and citation neighbors from the citation graph); (3) generates candidate natural-language ideas using LLMs (few-shot/in‑context or fine-tuned T5/biomedical LLMs); and (4) applies an iterative novelty-boosting retrieve-compare-update loop that measures similarity of candidate idea to a reference corpus and instructs the LLM to revise the idea until similarity is below a threshold. Training-time augmentation includes an in-context contrastive objective (InfoNCE) over decoder hidden states to discourage copying from context.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented, LLM-based (fine-tuning + in-context), contrastive training, iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Primarily NLP / AI research ideas; demonstrated generalization in a biochemical (biomedical) case study</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generative LLMs (GPT-3.5/GPT-4 few-shot; T5 fine-tuned; Meditron-7b for biomedical) produce candidate single-sentence ideas conditioned on: (a) a background context M, (b) an optional seed term v, and (c) retrieved inspirations i1...ik. Retrieval provides relevant example outputs/target entities or neighbor concepts which are concatenated into the LM prompt or fine-tuning input. Fine-tuning uses cross-entropy plus an in-context contrastive loss to reduce copying.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Cosine similarity between generated idea and items in a reference corpus R computed in SentenceBERT embedding space (all-mpnet-base-v2). A novelty penalty γ_nov(I,R) is operationalized by retrieving the k nearest reference ideas (k=20) and comparing each similarity S_i to a threshold μ (μ=0.6); ideas with S_i ≥ μ are considered insufficiently novel and used as negative examples to prompt revision. Iteration continues until all retrieved S_i < μ. Quantified novelty gains are reported as relative % increases from human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Human expert evaluation for relevance, novelty, clarity and scientific reasonableness; automatic metrics (ROUGE-L, BERTScore with SciBERT, BARTScore) are used as proxies for similarity to ground-truth ideas. During iterative updates, model is explicitly instructed to remain relevant to background B while increasing novelty (conditioning on B).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>SCIMON explicitly balances novelty and plausibility by (a) minimizing similarity to prior work (using the μ threshold) while (b) conditioning each update on the original background context to preserve relevance; the iterative loop halts when similarity constraints are met. Additionally, the contrastive loss penalizes copying from input context to keep ideas novel but cross-entropy training maintains fluency and plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Human-rated dimensions: relevance, novelty, clarity, and 'helpfulness' (reasonable scientific suggestion). Automated metrics used: ROUGE-L, BERTScore (SciBERT checkpoint), BARTScore. Quantitative novelty measures: percent increase in human-annotated novelty after iterative boosting (e.g., reported first-iteration novelty deltas like +54.4% for some settings) and counts of new terms added.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Primary validation via multi-study human expert annotation (graduate-level annotators and domain experts) comparing system outputs across model variants and versus ground-truth paper ideas; automated metric comparisons (ROUGE/BERTScore/BARTScore); domain-specific case study with biochemical domain experts for Meditron fine-tuned models. No wet-lab or external experimental validation of generated scientific claims is performed.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Temporal dataset splits (train <2021 / dev 2021 / test 2022), explicit description of IE preprocessing pipelines (PL-Marker, SciCo, ScispaCy, PubTator for biomedical), high-confidence filtering of IE outputs (only retaining high-confidence extractions to reduce noise), specification of retrieval model (SentenceBERT all-mpnet-base-v2), retrieval hyperparameters (k), novelty threshold μ, training hyperparameters for fine-tuning (T5-large lr, batch, beam size). Code and resources are reported as publicly available for research purposes.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Grounding generated ideas via retrieval of literature inspirations (semantic/KG/citation neighbors) to anchor output; high-confidence IE filtering during data construction to reduce noisy training pairs; in-context contrastive loss to discourage copying and trivial rephrasing of context (which reduces spurious claims that come from overfitting input fragments). The paper notes that the system does not perform external fact-checking.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>No explicit automated hallucination detector is described. The paper reports manual memorization checks for GPT-4 on the gold set and relies on human annotators to detect outputs that are generic, copied from context, or factually implausible.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Custom dataset derived from 67,408 ACL Anthology papers (S2ORC), temporally split; a human-vetted gold subset of 194 low-similarity instances; additional biomedical dataset from PubMed (several thousand papers) used for domain generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Automated metrics (Table 9): example numbers — GPT4FS: ROUGE-L 0.143 / BERTScore 0.618 (challenging subset), GPT4FS: ROUGE-L 0.151 / BERTScore 0.624 (gold); T5+SN+CL: ROUGE-L 0.228 / BERTScore 0.671 (challenging), ROUGE-L 0.258 / BERTScore 0.686 (gold). Iterative novelty boosting: first-iteration novelty deltas reported up to +54.4% and second-iteration further improvements (e.g., +57.8% for SN in some runs); human evaluations: GPT4FS and GPT4FS+KG received the largest share of positive human votes across systems (specific percent votes in Table 2 of paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared multiple baselines: GPT-3.5 few-shot variants, GPT-4 few-shot (GPT4FS), fine-tuned T5 variants (with/without semantic neighbor and contrastive losses), Meditron-7b for biochemical domain. Findings: human raters preferred GPT4FS and GPT4FS+KG outputs most often; T5-based fine-tuned models outperform GPT-based models on automatic similarity metrics (likely due to closer style to training targets), while GPT4 outputs are longer and preferred in human judgments. Iterative novelty boosting improves novelty relative to initial outputs (quantified by human annotations).</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Generated ideas are often incremental, lack technical depth, or are superficial recombinations of popular concepts; models frequently copy or rephrase context despite mitigation techniques; novelty boosting often adds generic longer descriptions and common combinations rather than truly deep innovations; IE preprocessing and retrieval quality limit performance; no fact-checking or experimental validation of suggested scientific claims; limited model scales in experiments (up to ~7B or API models), possible irreproducibility due to API changes and model randomness; annotation biases and evaluation challenges (subjective human judgments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2527.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2527.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative Novelty Boosting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Retrieve-Compare-Update Novelty Boosting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure that iteratively refines generated ideas by retrieving similar existing ideas from a reference corpus, measuring similarity, and prompting the LLM to update the idea until it is sufficiently dissimilar (novel) relative to the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Iterative Novelty Boosting (retrieve-compare-update)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Algorithmic loop: start with initial idea I0 from generation module; at each iteration t, use I_t to retrieve top-k nearest ideas {R1..Rk} from reference corpus R using SentenceBERT embeddings (k=20). Compute cosine similarity scores S_i between I_t and each R_i. If any S_i ≥ μ (μ=0.6), include those R_i as negative examples and prompt the LLM with an instruction listing the overlapping literature sentences and asking for a significantly different idea; produce I_{t+1}. Repeat until all S_i < μ or other stopping criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied to NLP idea generation; tested for generalization to biochemical domain</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Refinement-based: initial hypotheses produced by LLMs are updated iteratively using targeted prompts informed by retrieved literature overlaps.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Cosine similarity in SentenceBERT embedding space; threshold μ=0.6 used to decide whether retrieved literature items are too similar; novelty measured by human annotator judgments (percent of ideas judged more novel) and counts of new terms added.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Each updated idea is produced conditioned on the original background context M to preserve relevance; human annotators judge whether update maintains plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>System enforces novelty via similarity threshold while instructing the model to 'make sure the idea is significantly different' but 'conditioned on B', i.e., balancing by explicit prompt instruction rather than formal constrained optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Human-annotated novelty increases (reported as percentage deltas); counts of newly added terms in updated outputs (e.g., ~+23 new terms on first iteration for some methods after filtering), iteration success rates (percentage of updated ideas judged substantially different and more novel).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Human annotation study (Study III) comparing initial and updated ideas over 70 gold instances: reported that for SN retrieval, 88.9% of updated ideas were substantially different and 55.6% judged more novel after first iteration; second iteration further increased novelty for ~57.8% of those continued.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Reported hyperparameters (k=20, μ=0.6), use of SentenceBERT all-mpnet-base-v2 for embeddings, description of the prompt template used to instruct updates.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Indirect: by retrieving and showing similar literature examples that overlap, the mechanism aims to avoid producing outputs that simply restate prior work; not specifically designed to detect factual hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same ACL-derived dataset and gold subset; retrieval reference corpus R built from training-set paper ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-annotated novelty increases: first-iteration novelty deltas reported (e.g., +54.4% for some model configurations); iteration-specific new-term increases (e.g., first iteration ~+23.1 new terms for SN), and second-iteration novelty improvement ~+57.8% for SN in annotated subset.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Applying iterative novelty boosting to initial LLM outputs increases novelty relative to initial outputs (quantified in Study III); results reported across model variants (GPT4FS+SN+CT+KG and T5 variants).</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Although it increases measured novelty, updated ideas often remain superficial recombinations of popular concepts and lack the depth of real scientific contributions; success is limited by retrieval quality and by LLM capacity to invent truly novel, technically detailed proposals; potential for producing longer but not deeper outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2527.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2527.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inspiration Retrieval Module (Semantic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Neighbors Inspiration Retrieval Module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Retrieval component that finds semantically similar training-set input–target pairs and supplies the target terms as inspirations to the generator, using SentenceBERT similarity on concatenated seed+context prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Semantic Neighbors Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a fully connected graph G_S over training instances where nodes = (base input b_i, target term t_i); weights are cosine similarities between base inputs encoded via SentenceBERT (all-mpnet-base-v2). For a query base input b, insert into G_S and retrieve top-k neighbor nodes by edge weight; return the corresponding target terms t_i as semantic inspirations (up to k=20).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>embedding-based retrieval (SentenceBERT), retrieval-augmented input</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP (primary) and biomedical (applied similarly)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Provides salient example target terms from similar contexts that are concatenated to the LM prompt, biasing generation toward literature-grounded ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Grounds generation by providing examples from real paper outputs, increasing plausibility by design.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Acts as a source of plausible inspirations; novelty is later enforced by iterative novelty boosting rather than by this module alone.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Human evaluations show that models using semantic inspirations (e.g., T5+SN+CL) improved performance on automated metrics and human judgments relative to some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Uses a specified SentenceBERT model (all-mpnet-base-v2); retrieval k up to 20; details of graph construction and selection procedure provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By supplying real target terms from similar published work, reduces the tendency to invent unsupported claims (indirect anchoring).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Retrieves from training-set corpus (≈59k papers / ~374k sentences in NLP retrieval set).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>T5+SN+CL (combines semantic neighbors and contrastive loss) reported ROUGE-L 0.228 / BERTScore 0.671 on challenging subset and ROUGE-L 0.258 / BERTScore 0.686 on gold subset, outperforming many baselines on automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Using semantic inspirations improved T5 variants vs T5 baseline and helped reduce copying; human evaluations favored models using semantic inspirations in several comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Dependence on quality and coverage of training-set inspirations; can encourage superficial copying of common literature solutions if not combined with novelty controls; limited by IE extraction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2527.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2527.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG Neighbors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Neighbor Retrieval Module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Retrieves one-hop neighbors from a background scientific knowledge graph (nodes: Task/Method/Material/Metric; edges: used-for relations) to supply conceptually related inspirations grounded in normalized IE outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KG Neighbors Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a global background KG G_B from IE outputs over the corpus prior to a year Y (nodes representing extracted normalized scientific concepts and used-for edges). Given a seed term v, selects adjacent nodes n1... as inspirations and supplies them to the generation module.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>knowledge-graph-based retrieval, retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP (primary) and biomedical (applied via PubTator-based KG)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Supplies KG neighbor concepts to condition LLM generation, providing structured concept-level grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Increases plausibility by anchoring suggestions in normalized relations extracted across the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>KG neighbors provide plausible, literature-backed concept seeds; novelty control applied later via iterative novelty boosting.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Human studies (Study I & II) indicate GPT4FS+KG often increases technical depth and is rated favorably in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>KG constructed from IE extractions (PL-Marker / PubTator) restricted to papers before 2021; one-hop neighbor selection described.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By using normalized KG nodes drawn from many papers, aims to anchor ideas to factual relationships observed in literature (indirect prevention).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Background KG built over the training set (papers before 2021): reported >197k nodes and 261k relations for NLP experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human Study II: GPT4FS+KG found to have higher technical detail in 48% of compared pairs and to be less incremental (more novel) in 45% of pairs vs GPT4FS. Automatic metric differences smaller; T5+KG+CL variants reported in Table 9.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>KG augmentation often improves technical detail relative to LLM-only few-shot variants; complementary to semantic neighbor retrieval and novelty boosting.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>KG coverage and IE extraction quality limit usefulness; KG neighbors alone do not guarantee novelty and may encourage common concept combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2527.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2527.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-Context Contrastive</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Contrastive Objective (InfoNCE over decoder states)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuning augmentation that computes an InfoNCE contrastive loss over decoder hidden states between the positive target and in-context negatives sampled from the input to discourage copying and encourage generation distinct from input context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>In-Context Contrastive Objective</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>During fine-tuning of T5, in addition to cross-entropy, compute an InfoNCE-like loss L_cl = exp(y+/τ) / (exp(y+/τ) + Σ_k exp(y-_k/τ)) where y+ and y-_k are scalar projections (via Avg pooling and learned linear layer + sigmoid) of decoder hidden states for the positive target and in-context negatives respectively. In-context negatives are selected randomly from sentences appearing in the input context; temperature τ and linear projection parameters are learned.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>contrastive loss augmentation during LM fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP (primary) and applied in biomedical fine-tuning experiments</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Used as a training-time regularizer to reduce the model's tendency to copy background context and thus encourage generation of novel suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Indirect; the objective penalizes similarity to in-context negatives drawn from input, encouraging divergence from background phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Cross-entropy loss remains in training to preserve target fidelity and plausibility; contrastive loss is used jointly with cross-entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Joint optimization of cross-entropy (maintain plausibility/target fidelity) and contrastive loss (encourage novelty vs. input) to achieve a trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Models with in-context contrastive loss (T5+CL variants) show improved ROUGE-L and BERTScore vs baseline T5 in Table 9 and better human-rated novelty vs non-contrastive fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Evaluated via automatic metrics and human annotation studies (Study I results show T5+SN+CL performs well).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Described selection of 2 in-context negatives from input, learning hyperparameters, and combined optimization with cross-entropy; specifics listed in Appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By discouraging copying from the context, reduces trivial rephrasing/hallucinated reuse of background fragments (indirect effect).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same ACL-derived dataset and gold subset used for fine-tuning and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>T5+SN+CL reported ROUGE-L 0.228 / BERTScore 0.671 (challenging), 0.258 / 0.686 (gold); these T5+CL variants generally outperform T5 baseline on automatic metrics and are competitive in human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Contrastive-augmented T5 variants outperform vanilla fine-tuned T5 and GPT few-shot variants on automatic similarity metrics and reduce copying behavior according to error analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Contrastive negatives drawn from input may not capture all forms of undesired copying or factual hallucination; does not perform external fact verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2527.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2527.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SentenceBERT Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentence-BERT (all-mpnet-base-v2) Retrieval & Similarity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of SentenceBERT embeddings (all-mpnet-base-v2) for semantic retrieval of inspirations and for similarity-based novelty scoring between generated ideas and literature references.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SentenceBERT-based Retrieval and Similarity Scoring</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>All semantic search and nearest-neighbor retrieval (semantic neighbors, citation neighbor ranking, and retrieving R_i in iterative novelty boosting) use SentenceBERT sentence embeddings (all-mpnet-base-v2) and cosine similarity; these embeddings are central to constructing G_S and ranking candidates for inspirations and novelty checks.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>embedding-based retrieval / semantic similarity model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP and biomedical retrieval tasks within SCIMON</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not a generator itself; used to fetch literature-grounded inspirations which condition LLM generation and to compute similarity scores used for novelty decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Cosine similarity in SentenceBERT embedding space used to measure closeness between candidate idea and reference ideas; used in thresholding (μ=0.6) for novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Embedding-based similarity provides an operational novelty signal that is combined with prompts conditioned on background to preserve plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Retrieval quality and novelty effects are indirectly validated via downstream human evaluation and automatic metrics; quality of retrieved inspirations reported in Table 8 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Explicitly specifies the SentenceBERT checkpoint used (all-mpnet-base-v2) and similarity thresholds and k values for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By grounding with nearest-neighbor literature items, aims to reduce unconstrained invention, though not designed as a hallucination detector.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Retrieval set: ~59k papers and ~374k sentences (NLP); background KG and citation title sets described in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirical retrieval examples and downstream impact on human-evaluated novelty and automated metrics; specific retrieval-augmented models (e.g., GPT4FS+SN, T5+SN+CL) exhibit improved human/automated metrics vs some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Retrieval-augmented inputs (semantic neighbors) improve several T5 variants' performance relative to non-retrieval baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Embedding similarity is an imperfect proxy for conceptual novelty and can miss deep technical overlaps; retrieval quality limits novelty boosting and can introduce noisy inspirations if IE preprocessing is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2527.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2527.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-0314 checkpoint, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model used in few-shot in-context settings to generate candidate scientific idea sentences; outputs were systematically evaluated and augmented with retrieval and novelty-boosting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 (few-shot, in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used in zero-shot and few-shot prompting (GPT4ZS, GPT4FS) and in retrieval-augmented few-shot variants (GPT4FS+SN/GPT4FS+KG). Inputs include background context, seed term, and optionally retrieved inspirations or in-context examples. The authors generated multiple outputs per prompt and selected best outputs for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based (closed API checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP idea generation experiments; compared in biomedical case as well</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Few-shot/in-context generation using prompts constructed from background M, seed v, and optional retrieved inspirations or in-context examples from training set.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Combined with iterative novelty boosting and retrieval-based similarity checks (SentenceBERT) to increase novelty; human annotators evaluate novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Human expert evaluations judged relevance/novelty/clarity/helpfulness; GPT-4 outputs tended to be longer and often preferred by human raters though they lacked deep technical detail.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>When coupled with KG or SN retrieval and iterative novelty boosting, GPT-4 variants improved novelty while human evaluators judged outputs for remaining plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Human rating shares (Table 2) showing GPT4FS and GPT4FS+KG received highest helpful votes; automated metrics in Table 9: GPT4FS ROUGE-L 0.143 / BERT 0.618 (challenging), 0.151 / 0.624 (gold).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Human annotation studies (Study I & II) comparing GPT-4 variants against baselines and ground-truth paper ideas; iterative novelty boosting evaluated with human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Checkpoint specified (gpt-4-0314) and prompts/templates described; authors note API changes and randomness may limit reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Used retrieval grounding and iterative novelty prompts; no external fact-checking; high-confidence IE filtering in data creation reduced noisy training data.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Manual memorization checks for GPT-4 on gold set; human annotators observed copying and generic suggestions as errors, but no systematic automated detector reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Evaluated on ACL-derived dataset and gold subset and biomedical test splits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human evaluations favored GPT4FS/GPT4FS+KG highest among tested systems; automatic metrics lower than T5 variants due to output length and style (see Table 9 values listed above).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperformed GPT-3.5 variants in human preference; underperformed fine-tuned T5 variants on automatic similarity metrics but preferred in subjective human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Tends to produce ideas with low technical depth and generic descriptions of standard workflows; produces longer outputs that may inflate human preference but not scientific novelty; potential memorization risk.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2527.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2527.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5+SN+CL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 fine-tuned with Semantic Neighbors and Contrastive Loss (T5+SN+CL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-large model fine-tuned on the paper-extracted dataset augmented with semantic-neighbor inspirations and trained with an in-context contrastive (InfoNCE) loss to reduce copying and encourage novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>T5+SN+CL (fine-tuned generation model)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>T5-large fine-tuned on the training data combining prompt+context and retrieved inspirations (Semantic Neighbors). Training optimizes cross-entropy plus the in-context contrastive loss L_cl to discourage copying from input; decoding uses beam search with beam size 5 and repetition penalty 1.5.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Fine-tuned sequence-to-sequence transformer with contrastive augmentation and retrieval-conditioned input</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP idea generation; evaluated for biomedical generalization with Meditron variant described separately</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates candidate idea sentence conditioned on prompt P, retrieved inspirations i1..ik, and background M; trained to output target idea sentences extracted from papers.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Indirect via contrastive training (reducing copying) and iterative novelty boosting when applied; novelty evaluated by human annotators and automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Cross-entropy training on ground-truth paper ideas encourages plausible, paper-style outputs; human annotators rate plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Joint loss (cross-entropy + contrastive) is intended to preserve target fidelity while penalizing copying; retrieval provides plausible inspirations while novelty boosting can be applied to force divergence from existing literature.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Automatic metrics: ROUGE-L 0.228 / BERTScore 0.671 (challenging), ROUGE-L 0.258 / BERTScore 0.686 (gold) reported; human evaluations indicate competitive helpfulness though top human preference often went to GPT-4 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Evaluated against several baselines using human annotation studies and automatic metrics; ablation across retrieval/contrastive components reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Detailed hyperparameters (learning rate 6e-6, batch size, epochs, beam settings) and data splits given in appendices; model built with HuggingFace framework.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Contrastive loss reduces copying of input which lowers trivial or unsupported restatements; retrieval grounding supplies real literature signals.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>ACL-derived dataset (training subset prior to 2021) with gold test subset; comparisons reported on challenging and gold subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 9 (see numbers above); T5+SN+CL among top performers on automatic metrics. Human studies show it performs well though GPT4FS variants often preferred by annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms T5 baseline and many GPT3.5 variants on automatic metrics; competes with GPT-4 variants on some human-judged dimensions but often rated below GPT4FS/GPT4FS+KG in Study I.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Tends to produce outputs that resemble training-target style (higher automatic similarity), which may reflect less creative divergence; limited by IE noise and retrieval coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2527.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2527.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Expert Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Expert Annotation and Evaluation Protocols</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-study human evaluation setup employing graduate-level NLP and biomedical experts to judge generated ideas on relevance, novelty, clarity, and scientific reasonableness and to compare system variants and iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Human Expert Evaluation (Study I, II, III)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three human studies: Study I — blind paired rating (helpful/unhelpful) over 50 gold instances across six annotators rating relevance/novelty/clarity/reasonableness; Study II — pairwise ranking between GPT4 variants and ground-truth to assess technical detail and innovation; Study III — focused evaluation of iterative novelty boosting comparing initial and updated ideas across 70 instances with annotators reporting whether regenerated ideas are substantially different and more novel.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>human-in-the-loop expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP (primary) and biomedical (for domain-generalization case study)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Human judgments/rankings asking annotators to judge novelty and incrementality relative to context and to ground-truth ideas; specific instruction to only mark outputs as positive if sufficiently different from input context.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Human judgments of whether suggestions 'make sense' scientifically (reasonable) and whether they have adequate technical detail.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Annotators instructed to consider both novelty and practical reasonableness when rating helpfulness; in Study II, higher bar set by comparing to ground-truth paper ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Binary helpful/unhelpful labels, pairwise rankings of technical detail/novelty, percentages reported (e.g., GPT4FS+KG had higher technical detail in 48% of compared pairs; ground truth judged substantially higher than generated idea in 85% of paper comparisons). Inter-annotator agreement statistics reported in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Direct human judgment is the primary validation mechanism for idea quality and novelty in this work; biochemical domain experts also provided judgments in the domain-generalization case.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Annotation interfaces, instructions, and selection criteria are documented; gold subset curated with explicit filters (low similarity between background and ground truth, manual vetting).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Human annotators detect outputs that are generic, copied from context, or implausible; manual vetting used in dataset creation to prevent trivial overlaps.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Human detection (no automated hallucination detector).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Gold subset of 194 manually curated instances drawn from ACL Anthology-derived dataset used for human evaluation; additional sampled sets for each study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Study I: GPT4FS and GPT4FS+KG receive the largest share of positive votes across systems (percentages in Table 2). Study II: GPT4FS+KG more technically detailed in 48% pairs, less incremental in 45% pairs; ground-truth more novel/technical in 85% of comparisons vs generated outputs. Study III: for SN, 88.9% of updated ideas were substantially different after 1st iteration; 55.6% judged more novel after 1st iteration; second iteration increased novelty further for 57.8% of those continued.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Human evaluation distinguished GPT-4 variants from T5 and GPT-3.5 baselines, showing preference for GPT-4 variants despite automated metrics often favoring T5 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Subjectivity in human judgments; annotator pool mainly graduate students which may bias judgments; difficult to scale; results may not generalize across domains or to non-expert users.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2527.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2527.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Evaluation Metrics (ROUGE, BERTScore, BARTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard NLP automatic metrics used as proxies for similarity between generated idea sentences and ground-truth paper idea sentences: ROUGE-L, BERTScore with SciBERT, and BARTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ROUGE / BERTScore (SciBERT) / BARTScore</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ROUGE-L measures longest common subsequence overlap; BERTScore computes token-level similarity using contextual embeddings (SciBERT checkpoint used here) to better capture semantic similarity in scientific text; BARTScore provides a model-based evaluation of generation quality. These are used to report automatic similarity between generated outputs and ground-truth ideas on challenging and gold subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>automatic evaluation metrics / semantic similarity metrics</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP; applied to scientific idea sentence generation and to biomedical case study</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Not designed to measure novelty—these metrics primarily assess similarity to ground-truth (so lower scores do not necessarily imply higher novelty).</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Proxy: higher similarity to ground-truth implies stylistic/semantic closeness to known plausible ideas, but does not guarantee experimental plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>ROUGE-L (textual overlap), BERTScore (semantic overlap using SciBERT checkpoint 'allenai/scibert_scivocab_uncased'), BARTScore (model-based conditional likelihood proxy).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Used in tandem with human evaluation to compare models; observed that T5-based models achieve higher automatic similarity scores while GPT-4 is often preferred by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Model checkpoints and metric implementations specified (SciBERT checkpoint hash and BERTScore details in Appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Challenging and gold subsets of ACL-derived test set described in paper (gold subset = 194 vetted instances).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 9 shows example automatic metric values: T5+SN+CL ROUGE-L 0.228 / BERT 0.671 (challenging), T5 baseline ROUGE-L 0.223 / BERT 0.672, GPT4FS ROUGE-L 0.143 / BERT 0.618 (challenging).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Automatic metrics favored fine-tuned T5 variants over GPT-based few-shot models, highlighting divergence between automatic metrics and human judgments in this open-ended creative task.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Metrics measure similarity to existing paper ideas, which is a limited proxy for novelty and for real scientific value; long GPT outputs penalized by surface-based metrics despite human preference for them.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Undiscovered public knowledge <em>(Rating: 2)</em></li>
                <li>Literature based discovery: models, methods, and trends <em>(Rating: 2)</em></li>
                <li>Agatha: automatic graph mining and transformer based hypothesis generation approach <em>(Rating: 2)</em></li>
                <li>A computational inflection for scientific discovery <em>(Rating: 2)</em></li>
                <li>PaperRobot: Incremental draft generation of scientific ideas <em>(Rating: 2)</em></li>
                <li>Representation learning with contrastive predictive coding <em>(Rating: 1)</em></li>
                <li>Sentence-BERT: Sentence embeddings using Siamese BERT-networks <em>(Rating: 2)</em></li>
                <li>Meditron-70b: Scaling medical pretraining for large language models <em>(Rating: 1)</em></li>
                <li>Quantifying memorization across neural language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2527",
    "paper_id": "paper-258841365",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "SCIMON",
            "name_full": "Scientific Inspiration Machines Optimized for Novelty (SCIMON)",
            "brief_description": "A retrieval-augmented LLM framework that generates natural-language scientific idea suggestions grounded in literature and explicitly optimizes for novelty by iteratively comparing generated ideas to prior work and updating them until they are sufficiently novel.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SCIMON",
            "system_description": "End-to-end framework that (1) constructs training instances by extracting background/problem sentences and idea sentences from scientific papers via IE; (2) augments background context with retrieved 'inspirations' from three sources (semantic neighbors from a training-set similarity graph, KG neighbors from a background scientific knowledge graph, and citation neighbors from the citation graph); (3) generates candidate natural-language ideas using LLMs (few-shot/in‑context or fine-tuned T5/biomedical LLMs); and (4) applies an iterative novelty-boosting retrieve-compare-update loop that measures similarity of candidate idea to a reference corpus and instructs the LLM to revise the idea until similarity is below a threshold. Training-time augmentation includes an in-context contrastive objective (InfoNCE) over decoder hidden states to discourage copying from context.",
            "system_type": "retrieval-augmented, LLM-based (fine-tuning + in-context), contrastive training, iterative refinement",
            "scientific_domain": "Primarily NLP / AI research ideas; demonstrated generalization in a biochemical (biomedical) case study",
            "hypothesis_generation_method": "Generative LLMs (GPT-3.5/GPT-4 few-shot; T5 fine-tuned; Meditron-7b for biomedical) produce candidate single-sentence ideas conditioned on: (a) a background context M, (b) an optional seed term v, and (c) retrieved inspirations i1...ik. Retrieval provides relevant example outputs/target entities or neighbor concepts which are concatenated into the LM prompt or fine-tuning input. Fine-tuning uses cross-entropy plus an in-context contrastive loss to reduce copying.",
            "novelty_assessment_method": "Cosine similarity between generated idea and items in a reference corpus R computed in SentenceBERT embedding space (all-mpnet-base-v2). A novelty penalty γ_nov(I,R) is operationalized by retrieving the k nearest reference ideas (k=20) and comparing each similarity S_i to a threshold μ (μ=0.6); ideas with S_i ≥ μ are considered insufficiently novel and used as negative examples to prompt revision. Iteration continues until all retrieved S_i &lt; μ. Quantified novelty gains are reported as relative % increases from human annotations.",
            "plausibility_assessment_method": "Human expert evaluation for relevance, novelty, clarity and scientific reasonableness; automatic metrics (ROUGE-L, BERTScore with SciBERT, BARTScore) are used as proxies for similarity to ground-truth ideas. During iterative updates, model is explicitly instructed to remain relevant to background B while increasing novelty (conditioning on B).",
            "novelty_plausibility_balance": "SCIMON explicitly balances novelty and plausibility by (a) minimizing similarity to prior work (using the μ threshold) while (b) conditioning each update on the original background context to preserve relevance; the iterative loop halts when similarity constraints are met. Additionally, the contrastive loss penalizes copying from input context to keep ideas novel but cross-entropy training maintains fluency and plausibility.",
            "hypothesis_quality_metrics": "Human-rated dimensions: relevance, novelty, clarity, and 'helpfulness' (reasonable scientific suggestion). Automated metrics used: ROUGE-L, BERTScore (SciBERT checkpoint), BARTScore. Quantitative novelty measures: percent increase in human-annotated novelty after iterative boosting (e.g., reported first-iteration novelty deltas like +54.4% for some settings) and counts of new terms added.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Primary validation via multi-study human expert annotation (graduate-level annotators and domain experts) comparing system outputs across model variants and versus ground-truth paper ideas; automated metric comparisons (ROUGE/BERTScore/BARTScore); domain-specific case study with biochemical domain experts for Meditron fine-tuned models. No wet-lab or external experimental validation of generated scientific claims is performed.",
            "reproducibility_measures": "Temporal dataset splits (train &lt;2021 / dev 2021 / test 2022), explicit description of IE preprocessing pipelines (PL-Marker, SciCo, ScispaCy, PubTator for biomedical), high-confidence filtering of IE outputs (only retaining high-confidence extractions to reduce noise), specification of retrieval model (SentenceBERT all-mpnet-base-v2), retrieval hyperparameters (k), novelty threshold μ, training hyperparameters for fine-tuning (T5-large lr, batch, beam size). Code and resources are reported as publicly available for research purposes.",
            "hallucination_prevention_method": "Grounding generated ideas via retrieval of literature inspirations (semantic/KG/citation neighbors) to anchor output; high-confidence IE filtering during data construction to reduce noisy training pairs; in-context contrastive loss to discourage copying and trivial rephrasing of context (which reduces spurious claims that come from overfitting input fragments). The paper notes that the system does not perform external fact-checking.",
            "hallucination_detection_method": "No explicit automated hallucination detector is described. The paper reports manual memorization checks for GPT-4 on the gold set and relies on human annotators to detect outputs that are generic, copied from context, or factually implausible.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Custom dataset derived from 67,408 ACL Anthology papers (S2ORC), temporally split; a human-vetted gold subset of 194 low-similarity instances; additional biomedical dataset from PubMed (several thousand papers) used for domain generalization.",
            "performance_metrics": "Automated metrics (Table 9): example numbers — GPT4FS: ROUGE-L 0.143 / BERTScore 0.618 (challenging subset), GPT4FS: ROUGE-L 0.151 / BERTScore 0.624 (gold); T5+SN+CL: ROUGE-L 0.228 / BERTScore 0.671 (challenging), ROUGE-L 0.258 / BERTScore 0.686 (gold). Iterative novelty boosting: first-iteration novelty deltas reported up to +54.4% and second-iteration further improvements (e.g., +57.8% for SN in some runs); human evaluations: GPT4FS and GPT4FS+KG received the largest share of positive human votes across systems (specific percent votes in Table 2 of paper).",
            "comparison_with_baseline": "Compared multiple baselines: GPT-3.5 few-shot variants, GPT-4 few-shot (GPT4FS), fine-tuned T5 variants (with/without semantic neighbor and contrastive losses), Meditron-7b for biochemical domain. Findings: human raters preferred GPT4FS and GPT4FS+KG outputs most often; T5-based fine-tuned models outperform GPT-based models on automatic similarity metrics (likely due to closer style to training targets), while GPT4 outputs are longer and preferred in human judgments. Iterative novelty boosting improves novelty relative to initial outputs (quantified by human annotations).",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Generated ideas are often incremental, lack technical depth, or are superficial recombinations of popular concepts; models frequently copy or rephrase context despite mitigation techniques; novelty boosting often adds generic longer descriptions and common combinations rather than truly deep innovations; IE preprocessing and retrieval quality limit performance; no fact-checking or experimental validation of suggested scientific claims; limited model scales in experiments (up to ~7B or API models), possible irreproducibility due to API changes and model randomness; annotation biases and evaluation challenges (subjective human judgments).",
            "uuid": "e2527.0",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Iterative Novelty Boosting",
            "name_full": "Iterative Retrieve-Compare-Update Novelty Boosting",
            "brief_description": "A procedure that iteratively refines generated ideas by retrieving similar existing ideas from a reference corpus, measuring similarity, and prompting the LLM to update the idea until it is sufficiently dissimilar (novel) relative to the literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Iterative Novelty Boosting (retrieve-compare-update)",
            "system_description": "Algorithmic loop: start with initial idea I0 from generation module; at each iteration t, use I_t to retrieve top-k nearest ideas {R1..Rk} from reference corpus R using SentenceBERT embeddings (k=20). Compute cosine similarity scores S_i between I_t and each R_i. If any S_i ≥ μ (μ=0.6), include those R_i as negative examples and prompt the LLM with an instruction listing the overlapping literature sentences and asking for a significantly different idea; produce I_{t+1}. Repeat until all S_i &lt; μ or other stopping criteria.",
            "system_type": "retrieval-augmented iterative refinement",
            "scientific_domain": "Applied to NLP idea generation; tested for generalization to biochemical domain",
            "hypothesis_generation_method": "Refinement-based: initial hypotheses produced by LLMs are updated iteratively using targeted prompts informed by retrieved literature overlaps.",
            "novelty_assessment_method": "Cosine similarity in SentenceBERT embedding space; threshold μ=0.6 used to decide whether retrieved literature items are too similar; novelty measured by human annotator judgments (percent of ideas judged more novel) and counts of new terms added.",
            "plausibility_assessment_method": "Each updated idea is produced conditioned on the original background context M to preserve relevance; human annotators judge whether update maintains plausibility.",
            "novelty_plausibility_balance": "System enforces novelty via similarity threshold while instructing the model to 'make sure the idea is significantly different' but 'conditioned on B', i.e., balancing by explicit prompt instruction rather than formal constrained optimization.",
            "hypothesis_quality_metrics": "Human-annotated novelty increases (reported as percentage deltas); counts of newly added terms in updated outputs (e.g., ~+23 new terms on first iteration for some methods after filtering), iteration success rates (percentage of updated ideas judged substantially different and more novel).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Human annotation study (Study III) comparing initial and updated ideas over 70 gold instances: reported that for SN retrieval, 88.9% of updated ideas were substantially different and 55.6% judged more novel after first iteration; second iteration further increased novelty for ~57.8% of those continued.",
            "reproducibility_measures": "Reported hyperparameters (k=20, μ=0.6), use of SentenceBERT all-mpnet-base-v2 for embeddings, description of the prompt template used to instruct updates.",
            "hallucination_prevention_method": "Indirect: by retrieving and showing similar literature examples that overlap, the mechanism aims to avoid producing outputs that simply restate prior work; not specifically designed to detect factual hallucinations.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Same ACL-derived dataset and gold subset; retrieval reference corpus R built from training-set paper ideas.",
            "performance_metrics": "Human-annotated novelty increases: first-iteration novelty deltas reported (e.g., +54.4% for some model configurations); iteration-specific new-term increases (e.g., first iteration ~+23.1 new terms for SN), and second-iteration novelty improvement ~+57.8% for SN in annotated subset.",
            "comparison_with_baseline": "Applying iterative novelty boosting to initial LLM outputs increases novelty relative to initial outputs (quantified in Study III); results reported across model variants (GPT4FS+SN+CT+KG and T5 variants).",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Although it increases measured novelty, updated ideas often remain superficial recombinations of popular concepts and lack the depth of real scientific contributions; success is limited by retrieval quality and by LLM capacity to invent truly novel, technically detailed proposals; potential for producing longer but not deeper outputs.",
            "uuid": "e2527.1",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Inspiration Retrieval Module (Semantic)",
            "name_full": "Semantic Neighbors Inspiration Retrieval Module",
            "brief_description": "Retrieval component that finds semantically similar training-set input–target pairs and supplies the target terms as inspirations to the generator, using SentenceBERT similarity on concatenated seed+context prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Semantic Neighbors Retrieval",
            "system_description": "Constructs a fully connected graph G_S over training instances where nodes = (base input b_i, target term t_i); weights are cosine similarities between base inputs encoded via SentenceBERT (all-mpnet-base-v2). For a query base input b, insert into G_S and retrieve top-k neighbor nodes by edge weight; return the corresponding target terms t_i as semantic inspirations (up to k=20).",
            "system_type": "embedding-based retrieval (SentenceBERT), retrieval-augmented input",
            "scientific_domain": "NLP (primary) and biomedical (applied similarly)",
            "hypothesis_generation_method": "Provides salient example target terms from similar contexts that are concatenated to the LM prompt, biasing generation toward literature-grounded ideas.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Grounds generation by providing examples from real paper outputs, increasing plausibility by design.",
            "novelty_plausibility_balance": "Acts as a source of plausible inspirations; novelty is later enforced by iterative novelty boosting rather than by this module alone.",
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Human evaluations show that models using semantic inspirations (e.g., T5+SN+CL) improved performance on automated metrics and human judgments relative to some baselines.",
            "reproducibility_measures": "Uses a specified SentenceBERT model (all-mpnet-base-v2); retrieval k up to 20; details of graph construction and selection procedure provided.",
            "hallucination_prevention_method": "By supplying real target terms from similar published work, reduces the tendency to invent unsupported claims (indirect anchoring).",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Retrieves from training-set corpus (≈59k papers / ~374k sentences in NLP retrieval set).",
            "performance_metrics": "T5+SN+CL (combines semantic neighbors and contrastive loss) reported ROUGE-L 0.228 / BERTScore 0.671 on challenging subset and ROUGE-L 0.258 / BERTScore 0.686 on gold subset, outperforming many baselines on automatic metrics.",
            "comparison_with_baseline": "Using semantic inspirations improved T5 variants vs T5 baseline and helped reduce copying; human evaluations favored models using semantic inspirations in several comparisons.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Dependence on quality and coverage of training-set inspirations; can encourage superficial copying of common literature solutions if not combined with novelty controls; limited by IE extraction accuracy.",
            "uuid": "e2527.2",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "KG Neighbors",
            "name_full": "Knowledge Graph Neighbor Retrieval Module",
            "brief_description": "Retrieves one-hop neighbors from a background scientific knowledge graph (nodes: Task/Method/Material/Metric; edges: used-for relations) to supply conceptually related inspirations grounded in normalized IE outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "KG Neighbors Retrieval",
            "system_description": "Constructs a global background KG G_B from IE outputs over the corpus prior to a year Y (nodes representing extracted normalized scientific concepts and used-for edges). Given a seed term v, selects adjacent nodes n1... as inspirations and supplies them to the generation module.",
            "system_type": "knowledge-graph-based retrieval, retrieval-augmented generation",
            "scientific_domain": "NLP (primary) and biomedical (applied via PubTator-based KG)",
            "hypothesis_generation_method": "Supplies KG neighbor concepts to condition LLM generation, providing structured concept-level grounding.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Increases plausibility by anchoring suggestions in normalized relations extracted across the literature.",
            "novelty_plausibility_balance": "KG neighbors provide plausible, literature-backed concept seeds; novelty control applied later via iterative novelty boosting.",
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Human studies (Study I & II) indicate GPT4FS+KG often increases technical depth and is rated favorably in comparisons.",
            "reproducibility_measures": "KG constructed from IE extractions (PL-Marker / PubTator) restricted to papers before 2021; one-hop neighbor selection described.",
            "hallucination_prevention_method": "By using normalized KG nodes drawn from many papers, aims to anchor ideas to factual relationships observed in literature (indirect prevention).",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Background KG built over the training set (papers before 2021): reported &gt;197k nodes and 261k relations for NLP experiments.",
            "performance_metrics": "Human Study II: GPT4FS+KG found to have higher technical detail in 48% of compared pairs and to be less incremental (more novel) in 45% of pairs vs GPT4FS. Automatic metric differences smaller; T5+KG+CL variants reported in Table 9.",
            "comparison_with_baseline": "KG augmentation often improves technical detail relative to LLM-only few-shot variants; complementary to semantic neighbor retrieval and novelty boosting.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "KG coverage and IE extraction quality limit usefulness; KG neighbors alone do not guarantee novelty and may encourage common concept combinations.",
            "uuid": "e2527.3",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "In-Context Contrastive",
            "name_full": "In-Context Contrastive Objective (InfoNCE over decoder states)",
            "brief_description": "A fine-tuning augmentation that computes an InfoNCE contrastive loss over decoder hidden states between the positive target and in-context negatives sampled from the input to discourage copying and encourage generation distinct from input context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "In-Context Contrastive Objective",
            "system_description": "During fine-tuning of T5, in addition to cross-entropy, compute an InfoNCE-like loss L_cl = exp(y+/τ) / (exp(y+/τ) + Σ_k exp(y-_k/τ)) where y+ and y-_k are scalar projections (via Avg pooling and learned linear layer + sigmoid) of decoder hidden states for the positive target and in-context negatives respectively. In-context negatives are selected randomly from sentences appearing in the input context; temperature τ and linear projection parameters are learned.",
            "system_type": "contrastive loss augmentation during LM fine-tuning",
            "scientific_domain": "NLP (primary) and applied in biomedical fine-tuning experiments",
            "hypothesis_generation_method": "Used as a training-time regularizer to reduce the model's tendency to copy background context and thus encourage generation of novel suggestions.",
            "novelty_assessment_method": "Indirect; the objective penalizes similarity to in-context negatives drawn from input, encouraging divergence from background phrasing.",
            "plausibility_assessment_method": "Cross-entropy loss remains in training to preserve target fidelity and plausibility; contrastive loss is used jointly with cross-entropy.",
            "novelty_plausibility_balance": "Joint optimization of cross-entropy (maintain plausibility/target fidelity) and contrastive loss (encourage novelty vs. input) to achieve a trade-off.",
            "hypothesis_quality_metrics": "Models with in-context contrastive loss (T5+CL variants) show improved ROUGE-L and BERTScore vs baseline T5 in Table 9 and better human-rated novelty vs non-contrastive fine-tuning.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Evaluated via automatic metrics and human annotation studies (Study I results show T5+SN+CL performs well).",
            "reproducibility_measures": "Described selection of 2 in-context negatives from input, learning hyperparameters, and combined optimization with cross-entropy; specifics listed in Appendix.",
            "hallucination_prevention_method": "By discouraging copying from the context, reduces trivial rephrasing/hallucinated reuse of background fragments (indirect effect).",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Same ACL-derived dataset and gold subset used for fine-tuning and evaluation.",
            "performance_metrics": "T5+SN+CL reported ROUGE-L 0.228 / BERTScore 0.671 (challenging), 0.258 / 0.686 (gold); these T5+CL variants generally outperform T5 baseline on automatic metrics and are competitive in human evaluation.",
            "comparison_with_baseline": "Contrastive-augmented T5 variants outperform vanilla fine-tuned T5 and GPT few-shot variants on automatic similarity metrics and reduce copying behavior according to error analysis.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Contrastive negatives drawn from input may not capture all forms of undesired copying or factual hallucination; does not perform external fact verification.",
            "uuid": "e2527.4",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SentenceBERT Retrieval",
            "name_full": "Sentence-BERT (all-mpnet-base-v2) Retrieval & Similarity",
            "brief_description": "Use of SentenceBERT embeddings (all-mpnet-base-v2) for semantic retrieval of inspirations and for similarity-based novelty scoring between generated ideas and literature references.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SentenceBERT-based Retrieval and Similarity Scoring",
            "system_description": "All semantic search and nearest-neighbor retrieval (semantic neighbors, citation neighbor ranking, and retrieving R_i in iterative novelty boosting) use SentenceBERT sentence embeddings (all-mpnet-base-v2) and cosine similarity; these embeddings are central to constructing G_S and ranking candidates for inspirations and novelty checks.",
            "system_type": "embedding-based retrieval / semantic similarity model",
            "scientific_domain": "NLP and biomedical retrieval tasks within SCIMON",
            "hypothesis_generation_method": "Not a generator itself; used to fetch literature-grounded inspirations which condition LLM generation and to compute similarity scores used for novelty decisions.",
            "novelty_assessment_method": "Cosine similarity in SentenceBERT embedding space used to measure closeness between candidate idea and reference ideas; used in thresholding (μ=0.6) for novelty.",
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": "Embedding-based similarity provides an operational novelty signal that is combined with prompts conditioned on background to preserve plausibility.",
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Retrieval quality and novelty effects are indirectly validated via downstream human evaluation and automatic metrics; quality of retrieved inspirations reported in Table 8 examples.",
            "reproducibility_measures": "Explicitly specifies the SentenceBERT checkpoint used (all-mpnet-base-v2) and similarity thresholds and k values for retrieval.",
            "hallucination_prevention_method": "By grounding with nearest-neighbor literature items, aims to reduce unconstrained invention, though not designed as a hallucination detector.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Retrieval set: ~59k papers and ~374k sentences (NLP); background KG and citation title sets described in paper.",
            "performance_metrics": "Empirical retrieval examples and downstream impact on human-evaluated novelty and automated metrics; specific retrieval-augmented models (e.g., GPT4FS+SN, T5+SN+CL) exhibit improved human/automated metrics vs some baselines.",
            "comparison_with_baseline": "Retrieval-augmented inputs (semantic neighbors) improve several T5 variants' performance relative to non-retrieval baselines.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Embedding similarity is an imperfect proxy for conceptual novelty and can miss deep technical overlaps; retrieval quality limits novelty boosting and can introduce noisy inspirations if IE preprocessing is imperfect.",
            "uuid": "e2527.5",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4 (used)",
            "name_full": "GPT-4 (gpt-4-0314 checkpoint, OpenAI)",
            "brief_description": "A state-of-the-art large language model used in few-shot in-context settings to generate candidate scientific idea sentences; outputs were systematically evaluated and augmented with retrieval and novelty-boosting.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4 (few-shot, in-context)",
            "system_description": "Used in zero-shot and few-shot prompting (GPT4ZS, GPT4FS) and in retrieval-augmented few-shot variants (GPT4FS+SN/GPT4FS+KG). Inputs include background context, seed term, and optionally retrieved inspirations or in-context examples. The authors generated multiple outputs per prompt and selected best outputs for evaluation.",
            "system_type": "LLM-based (closed API checkpoint)",
            "scientific_domain": "NLP idea generation experiments; compared in biomedical case as well",
            "hypothesis_generation_method": "Few-shot/in-context generation using prompts constructed from background M, seed v, and optional retrieved inspirations or in-context examples from training set.",
            "novelty_assessment_method": "Combined with iterative novelty boosting and retrieval-based similarity checks (SentenceBERT) to increase novelty; human annotators evaluate novelty.",
            "plausibility_assessment_method": "Human expert evaluations judged relevance/novelty/clarity/helpfulness; GPT-4 outputs tended to be longer and often preferred by human raters though they lacked deep technical detail.",
            "novelty_plausibility_balance": "When coupled with KG or SN retrieval and iterative novelty boosting, GPT-4 variants improved novelty while human evaluators judged outputs for remaining plausibility.",
            "hypothesis_quality_metrics": "Human rating shares (Table 2) showing GPT4FS and GPT4FS+KG received highest helpful votes; automated metrics in Table 9: GPT4FS ROUGE-L 0.143 / BERT 0.618 (challenging), 0.151 / 0.624 (gold).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Human annotation studies (Study I & II) comparing GPT-4 variants against baselines and ground-truth paper ideas; iterative novelty boosting evaluated with human annotators.",
            "reproducibility_measures": "Checkpoint specified (gpt-4-0314) and prompts/templates described; authors note API changes and randomness may limit reproducibility.",
            "hallucination_prevention_method": "Used retrieval grounding and iterative novelty prompts; no external fact-checking; high-confidence IE filtering in data creation reduced noisy training data.",
            "hallucination_detection_method": "Manual memorization checks for GPT-4 on gold set; human annotators observed copying and generic suggestions as errors, but no systematic automated detector reported.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Evaluated on ACL-derived dataset and gold subset and biomedical test splits.",
            "performance_metrics": "Human evaluations favored GPT4FS/GPT4FS+KG highest among tested systems; automatic metrics lower than T5 variants due to output length and style (see Table 9 values listed above).",
            "comparison_with_baseline": "Outperformed GPT-3.5 variants in human preference; underperformed fine-tuned T5 variants on automatic similarity metrics but preferred in subjective human judgments.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Tends to produce ideas with low technical depth and generic descriptions of standard workflows; produces longer outputs that may inflate human preference but not scientific novelty; potential memorization risk.",
            "uuid": "e2527.6",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "T5+SN+CL",
            "name_full": "T5 fine-tuned with Semantic Neighbors and Contrastive Loss (T5+SN+CL)",
            "brief_description": "A T5-large model fine-tuned on the paper-extracted dataset augmented with semantic-neighbor inspirations and trained with an in-context contrastive (InfoNCE) loss to reduce copying and encourage novelty.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "T5+SN+CL (fine-tuned generation model)",
            "system_description": "T5-large fine-tuned on the training data combining prompt+context and retrieved inspirations (Semantic Neighbors). Training optimizes cross-entropy plus the in-context contrastive loss L_cl to discourage copying from input; decoding uses beam search with beam size 5 and repetition penalty 1.5.",
            "system_type": "Fine-tuned sequence-to-sequence transformer with contrastive augmentation and retrieval-conditioned input",
            "scientific_domain": "NLP idea generation; evaluated for biomedical generalization with Meditron variant described separately",
            "hypothesis_generation_method": "Generates candidate idea sentence conditioned on prompt P, retrieved inspirations i1..ik, and background M; trained to output target idea sentences extracted from papers.",
            "novelty_assessment_method": "Indirect via contrastive training (reducing copying) and iterative novelty boosting when applied; novelty evaluated by human annotators and automatic metrics.",
            "plausibility_assessment_method": "Cross-entropy training on ground-truth paper ideas encourages plausible, paper-style outputs; human annotators rate plausibility.",
            "novelty_plausibility_balance": "Joint loss (cross-entropy + contrastive) is intended to preserve target fidelity while penalizing copying; retrieval provides plausible inspirations while novelty boosting can be applied to force divergence from existing literature.",
            "hypothesis_quality_metrics": "Automatic metrics: ROUGE-L 0.228 / BERTScore 0.671 (challenging), ROUGE-L 0.258 / BERTScore 0.686 (gold) reported; human evaluations indicate competitive helpfulness though top human preference often went to GPT-4 variants.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Evaluated against several baselines using human annotation studies and automatic metrics; ablation across retrieval/contrastive components reported.",
            "reproducibility_measures": "Detailed hyperparameters (learning rate 6e-6, batch size, epochs, beam settings) and data splits given in appendices; model built with HuggingFace framework.",
            "hallucination_prevention_method": "Contrastive loss reduces copying of input which lowers trivial or unsupported restatements; retrieval grounding supplies real literature signals.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "ACL-derived dataset (training subset prior to 2021) with gold test subset; comparisons reported on challenging and gold subsets.",
            "performance_metrics": "Reported in Table 9 (see numbers above); T5+SN+CL among top performers on automatic metrics. Human studies show it performs well though GPT4FS variants often preferred by annotators.",
            "comparison_with_baseline": "Outperforms T5 baseline and many GPT3.5 variants on automatic metrics; competes with GPT-4 variants on some human-judged dimensions but often rated below GPT4FS/GPT4FS+KG in Study I.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Tends to produce outputs that resemble training-target style (higher automatic similarity), which may reflect less creative divergence; limited by IE noise and retrieval coverage.",
            "uuid": "e2527.7",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Human Expert Eval",
            "name_full": "Human Expert Annotation and Evaluation Protocols",
            "brief_description": "A multi-study human evaluation setup employing graduate-level NLP and biomedical experts to judge generated ideas on relevance, novelty, clarity, and scientific reasonableness and to compare system variants and iterations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Human Expert Evaluation (Study I, II, III)",
            "system_description": "Three human studies: Study I — blind paired rating (helpful/unhelpful) over 50 gold instances across six annotators rating relevance/novelty/clarity/reasonableness; Study II — pairwise ranking between GPT4 variants and ground-truth to assess technical detail and innovation; Study III — focused evaluation of iterative novelty boosting comparing initial and updated ideas across 70 instances with annotators reporting whether regenerated ideas are substantially different and more novel.",
            "system_type": "human-in-the-loop expert evaluation",
            "scientific_domain": "NLP (primary) and biomedical (for domain-generalization case study)",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": "Human judgments/rankings asking annotators to judge novelty and incrementality relative to context and to ground-truth ideas; specific instruction to only mark outputs as positive if sufficiently different from input context.",
            "plausibility_assessment_method": "Human judgments of whether suggestions 'make sense' scientifically (reasonable) and whether they have adequate technical detail.",
            "novelty_plausibility_balance": "Annotators instructed to consider both novelty and practical reasonableness when rating helpfulness; in Study II, higher bar set by comparing to ground-truth paper ideas.",
            "hypothesis_quality_metrics": "Binary helpful/unhelpful labels, pairwise rankings of technical detail/novelty, percentages reported (e.g., GPT4FS+KG had higher technical detail in 48% of compared pairs; ground truth judged substantially higher than generated idea in 85% of paper comparisons). Inter-annotator agreement statistics reported in appendices.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Direct human judgment is the primary validation mechanism for idea quality and novelty in this work; biochemical domain experts also provided judgments in the domain-generalization case.",
            "reproducibility_measures": "Annotation interfaces, instructions, and selection criteria are documented; gold subset curated with explicit filters (low similarity between background and ground truth, manual vetting).",
            "hallucination_prevention_method": "Human annotators detect outputs that are generic, copied from context, or implausible; manual vetting used in dataset creation to prevent trivial overlaps.",
            "hallucination_detection_method": "Human detection (no automated hallucination detector).",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Gold subset of 194 manually curated instances drawn from ACL Anthology-derived dataset used for human evaluation; additional sampled sets for each study.",
            "performance_metrics": "Study I: GPT4FS and GPT4FS+KG receive the largest share of positive votes across systems (percentages in Table 2). Study II: GPT4FS+KG more technically detailed in 48% pairs, less incremental in 45% pairs; ground-truth more novel/technical in 85% of comparisons vs generated outputs. Study III: for SN, 88.9% of updated ideas were substantially different after 1st iteration; 55.6% judged more novel after 1st iteration; second iteration increased novelty further for 57.8% of those continued.",
            "comparison_with_baseline": "Human evaluation distinguished GPT-4 variants from T5 and GPT-3.5 baselines, showing preference for GPT-4 variants despite automated metrics often favoring T5 variants.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Subjectivity in human judgments; annotator pool mainly graduate students which may bias judgments; difficult to scale; results may not generalize across domains or to non-expert users.",
            "uuid": "e2527.8",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Automated Metrics",
            "name_full": "Automatic Evaluation Metrics (ROUGE, BERTScore, BARTScore)",
            "brief_description": "Standard NLP automatic metrics used as proxies for similarity between generated idea sentences and ground-truth paper idea sentences: ROUGE-L, BERTScore with SciBERT, and BARTScore.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ROUGE / BERTScore (SciBERT) / BARTScore",
            "system_description": "ROUGE-L measures longest common subsequence overlap; BERTScore computes token-level similarity using contextual embeddings (SciBERT checkpoint used here) to better capture semantic similarity in scientific text; BARTScore provides a model-based evaluation of generation quality. These are used to report automatic similarity between generated outputs and ground-truth ideas on challenging and gold subsets.",
            "system_type": "automatic evaluation metrics / semantic similarity metrics",
            "scientific_domain": "NLP; applied to scientific idea sentence generation and to biomedical case study",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": "Not designed to measure novelty—these metrics primarily assess similarity to ground-truth (so lower scores do not necessarily imply higher novelty).",
            "plausibility_assessment_method": "Proxy: higher similarity to ground-truth implies stylistic/semantic closeness to known plausible ideas, but does not guarantee experimental plausibility.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "ROUGE-L (textual overlap), BERTScore (semantic overlap using SciBERT checkpoint 'allenai/scibert_scivocab_uncased'), BARTScore (model-based conditional likelihood proxy).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Used in tandem with human evaluation to compare models; observed that T5-based models achieve higher automatic similarity scores while GPT-4 is often preferred by humans.",
            "reproducibility_measures": "Model checkpoints and metric implementations specified (SciBERT checkpoint hash and BERTScore details in Appendix).",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Challenging and gold subsets of ACL-derived test set described in paper (gold subset = 194 vetted instances).",
            "performance_metrics": "Table 9 shows example automatic metric values: T5+SN+CL ROUGE-L 0.228 / BERT 0.671 (challenging), T5 baseline ROUGE-L 0.223 / BERT 0.672, GPT4FS ROUGE-L 0.143 / BERT 0.618 (challenging).",
            "comparison_with_baseline": "Automatic metrics favored fine-tuned T5 variants over GPT-based few-shot models, highlighting divergence between automatic metrics and human judgments in this open-ended creative task.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Metrics measure similarity to existing paper ideas, which is a limited proxy for novelty and for real scientific value; long GPT outputs penalized by surface-based metrics despite human preference for them.",
            "uuid": "e2527.9",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Undiscovered public knowledge",
            "rating": 2,
            "sanitized_title": "undiscovered_public_knowledge"
        },
        {
            "paper_title": "Literature based discovery: models, methods, and trends",
            "rating": 2,
            "sanitized_title": "literature_based_discovery_models_methods_and_trends"
        },
        {
            "paper_title": "Agatha: automatic graph mining and transformer based hypothesis generation approach",
            "rating": 2,
            "sanitized_title": "agatha_automatic_graph_mining_and_transformer_based_hypothesis_generation_approach"
        },
        {
            "paper_title": "A computational inflection for scientific discovery",
            "rating": 2,
            "sanitized_title": "a_computational_inflection_for_scientific_discovery"
        },
        {
            "paper_title": "PaperRobot: Incremental draft generation of scientific ideas",
            "rating": 2,
            "sanitized_title": "paperrobot_incremental_draft_generation_of_scientific_ideas"
        },
        {
            "paper_title": "Representation learning with contrastive predictive coding",
            "rating": 1,
            "sanitized_title": "representation_learning_with_contrastive_predictive_coding"
        },
        {
            "paper_title": "Sentence-BERT: Sentence embeddings using Siamese BERT-networks",
            "rating": 2,
            "sanitized_title": "sentencebert_sentence_embeddings_using_siamese_bertnetworks"
        },
        {
            "paper_title": "Meditron-70b: Scaling medical pretraining for large language models",
            "rating": 1,
            "sanitized_title": "meditron70b_scaling_medical_pretraining_for_large_language_models"
        },
        {
            "paper_title": "Quantifying memorization across neural language models",
            "rating": 1,
            "sanitized_title": "quantifying_memorization_across_neural_language_models"
        }
    ],
    "cost": 0.02916,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Scientific Inspiration Machines Optimized for Novelty
3 Jun 2024</p>
<p>Qingyun Wang qingyun4@illinois.edu 
University of Illinois at Urbana-Champaign</p>
<p>Doug Downey 
Allen Institute for Artificial Intelligence (AI2)</p>
<p>Heng Ji hengji@illinois.edu 
University of Illinois at Urbana-Champaign</p>
<p>Tom Hope tomh@allenai.org 
Allen Institute for Artificial Intelligence (AI2)</p>
<p>The Hebrew University of Jerusalem</p>
<p>Scientific Inspiration Machines Optimized for Novelty
3 Jun 20248AFCF73D6229DB6E6028F735BDF4E3F3arXiv:2305.14259v7[cs.CL]
We explore and enhance the ability of neural language models to generate novel scientific directions grounded in literature.Work on literature-based hypothesis generation has traditionally focused on binary link predictionseverely limiting the expressivity of hypotheses.This line of work also does not focus on optimizing novelty.We take a dramatic departure with a novel setting in which models use as input background contexts (e.g., problems, experimental settings, goals), and output natural language ideas grounded in literature.We present SCIMON, a modeling framework that uses retrieval of "inspirations" from past scientific papers, and explicitly optimizes for novelty by iteratively comparing to prior papers and updating idea suggestions until sufficient novelty is achieved.Comprehensive evaluations reveal that GPT-4 tends to generate ideas with overall low technical depth and novelty, while our methods partially mitigate this issue.Our work represents a first step toward evaluating and developing language models that generate new ideas derived from the scientific literature 1 .</p>
<p>Introduction</p>
<p>Can machines mine scientific papers and learn to suggest new directions?The idea that information from the literature can be used for automatically generating hypotheses has been around for decades (Swanson, 1986).To date, the focus has been on a specific setting: hypothesizing links between pairs of concepts (often in drug discovery applications (Henry and McInnes, 2017), e.g., new drug-disease links), where concepts are obtained from papers or knowledge bases previously derived from papers (Sybrandt et al., 2020;Nadkarni et al., 2021).</p>
<p>This common setting has fundamental drawbacks.Reducing the "language of scientific ideas" (Hope et al., 2023) to this simplistic form limits the expressivity of the hypotheses we can hope to generate, and does not capture nuanced contexts that scientists consider: target application settings, requirements and constraints, motivations and challenges.In light of the strong progress recently made with large language models (LLMs), in this paper we explore a dramatically different setting: models that take descriptions of problem contextsand return natural language suggestions of novel scientific directions that are grounded in literature.</p>
<p>We develop a framework named SCIMON (Scientific Inspiration Machines with Optimization for Novelty), named after Nobel laureate and AI pioneer Herbert Simon who authored early foundational work on automated scientific discovery (Newell and Simon, 1956;Simon, 1973).We first present an automated data collection methodology that collects examples of past problems and proposed ideas from scientific papers.We then use this data for both fine-tuning and in-context training of LLMs-training them to take problem descriptions and output proposed ideas to address them.We observe that state-of-art LLMs (e.g., GPT-4 (OpenAI, 2023)) struggle with generating novel scientific ideas, and contribute a new modeling framework for generating hypotheses that makes progress in improving the hypothesis generation ability of LLMs (Figure 1).Given a background problem description, models first dynamically retrieve inspirations from past literature in the form of related problems and their solutions along with contexts from a scientific knowledge graph.These retrieved inspirations serve to ground the generated ideas in existing literature.We then endow models with the ability to iteratively boost the novelty of generated ideas.Given an idea I generated by the LLM at step t, the model compares I with existing research in the literature; if it finds strongly overlapping research, the model is tasked with updating its idea to be more novel relative to prior work (much like a good researcher would do).We also introduce an in-context contrastive model which encourages novelty with respect to background context.</p>
<p>We perform the first comprehensive evaluation of language models for generating scientific ideas in our new generative, contextual setting.We focus on AI/NLP ideas to facilitate analysis by AI researchers themselves, and also demonstrate generalization to the biomedical domain.We design extensive evaluation experiments using human annotators with domain expertise to assess relevance, utility, novelty, and technical depth.Our methods substantially improve the ability of LLMs in our task; however, analyses show that ideas still fall far behind scientific papers in terms of novelty, depth and utility-raising fundamental challenges toward building models that generate scientific ideas.</p>
<p>Background and New Setting</p>
<p>We begin with a brief description of related work and background.We then present our novel setting.</p>
<p>Literature-based discovery Nearly four decades have passed since Don Swanson pioneered Literature-Based Discovery (LBD), based on the premise that the literature can be used for generating hypotheses (Swanson, 1986).LBD has been focused on a very specific, narrow type of hypothesis: links between pairs of concepts (often drugs/diseases).The classic formalization of LBD goes back to Swanson (1986) who proposed the "ABC" model where two concepts (terms) A and C are hypothesized as linked if they both co-occur with some intermediate concept B in papers.More recent work has used word vectors (Tshitoyan et al., 2019) or link prediction models (Wang et al., 2019;Sybrandt et al., 2020;Xu et al., 2023) to discover scientific hypotheses as pairwise links between concepts.A tightly related body of research focuses on scientific knowledge graph link prediction (Nadkarni et al., 2021), where predicted links may correspond to new hypotheses, and knowledge bases are reflections of existing scientific knowledge in specific domains, derived from literature.A fundamental gap in this line of work is in the lack of approaches for modeling nuanced contexts (Sosa and Altman, 2022) (e.g., the specific settings in which a drug may be relevant for a disease) for generating ideas in open-ended problem settings with unbounded hypothesis spaces, and for optimizing novelty.Our setting can be viewed as a radical departure addressing the limitations in existing settings.</p>
<p>LLMs for Scientific Innovation Large language models (LLMs) have made remarkable progress in interpreting and producing natural language content and handling knowledge-intensive tasks such as in the medical domain (Nori et al., 2023).Very recent work (Boiko et al., 2023) has explored the use of LLMs in a robotic chemistry lab setting, planning chemical syntheses of known compounds and executing experiments.Robotic lab settings are inherently limited to narrow sub-areas where such experiments are possible and relevant.Other very recent work (Huang et al., 2023) used LLMs to produce code for machine learning tasks such as Kaggle competitions, finding that a GPT-4 agent achieved 0% accuracy on research challenges such as BabyLM (Warstadt et al., 2023).GPT-4 has been anecdotally reported as having "strengths less like those of having a human co-author, and more like a mathematician working with a calculator" (Carlini, 2023).Our goal is to conduct a non-anecdotal evaluation and enhancement of strong LLMs' ability to generate novel open-ended scientific ideas.</p>
<p>SCIMON Problem Setting</p>
<p>We are motivated by imagining an AI-based assistant that suggests ideas in natural language.The assistant takes as input background context B consisting of (1) current problems, motivations, experimental settings and constraints, denoted as M; and optionally (2) a seed term v that should be a focus point of the generated idea I.The seed term is motivated by considering a user-provided cue for the model to limit its hypothesis space.Importantly, generated ideas should not merely paraphrase the background-the output should be novel with respect to B and the broader literature corpus.text that describes problems with "pretrained language models" in the lifelong integration of information sources, including computational costs.The assistant aims to generate an idea for performing "knowledge acquisition" within this context.Given this input, we aim to generate a full sentence describing a novel idea.</p>
<p>Automated Training Data Collection</p>
<p>We obtain training data derived from papers with scientific information extraction (IE) modelsextracting past examples of background sentences and corresponding ideas (e.g., descriptions of methods used for specific problems in the background sentences), along with salient entities as seed terms.This data is used for training in both in-context learning and fine-tuning setups.</p>
<p>We construct a corpus D from 67,408 ACL Anthology papers from S2ORC (Lo et al., 2020) (we later also conduct an experiment with a biomedical corpus §4.1).Given a title and the corresponding abstract from a document d, to select problem/motivation sentences M we first perform sci-entific sentence classification (Cohan et al., 2019) to classify sentences from the abstract into one of {Background, Method, Objective}, selecting sentences with labels of Background and treating the remaining sentences as target sentences T which will serve as desired output examples (Figure 3).</p>
<p>For seed term selection, we apply a state-ofthe-art scientific IE system (Ye et al., 2022) to T to extract entities corresponding to Task, Method, Evaluation Metric, Material, and relations of the form [method,used-for,task]-mentions of methods and the tasks they are used for, materials used for tasks, etc.We treat the head (e.g., method) or tail (e.g., task) entity as the seed term, and name the other entity (tail/head, respectively) as a target term t ∈ T .Continuing our example from Figure 2, Figure 3 shows how the seed and target terms ("knowledge acquisition" and "function preserved model expansion") are extracted from T .During training, each instance contains (B,T ) pairs; during evaluation, target information is removed.</p>
<p>We use SciCo (Cattan et al., 2021) to obtain coreference links for entity normalization, and use ScispaCy (Neumann et al., 2019) to replace abbreviations with a more informative long form.We also collect paper metadata, including the citation network G c .We split our dataset temporally (train/dev/test correspond to papers from years &lt;2021 / 2021 / 2022 respectively).For our experiments, we used model checkpoints trained on data preceding 2022, avoiding the risk of data contamination ( §6).Table 1 shows data statistics. 2uality of IE Preprocessing During preprocessing, we only keep high-confidence outputs from IE models to reduce errors.We observe this removes many of the noisy cases.To validate this, we manually evaluate the precision of each preprocessing step on a random sample of papers and observe that all steps yield high precision (91%-100%) except relation extraction (65%); in total, the rate of instances passing all steps was 79.7%. 3   Gold Test Set We create a high-quality, clean test set.We remove test instances where models can trivially use surface-level background information to infer the ground truth to create a more challenging set, selecting instances with low similarity between background and ground truth sentences.We compute the cosine similarity between each instance's background and corresponding ground truth sentence in the test set and take pairs with similarity ≤ 0.074, which amounts to the tenth percentile of pairs.We further annotate this subset to create a gold subset.We manually exclude instances with trivial overlap between ground truth and background, remove cases with irrelevant background, and retain only instances where the target relation (from which the seed term is taken) is salient to the target sentence.We also remove test pairs that have unexplained terms in the background.We obtain a total of 194 instances.</p>
<p>SCIMON Models</p>
<p>We present a new module to retrieve inspirations as contextual input ( §3.1).Then, we describe another module to generate ideas given the con-text+inspiration ( §3.2).Finally, we introduce a new iterative novelty optimization method to further improve idea quality ( §3.3). 5</p>
<p>3 See Table 6 in Appendix. 4Full annotation details are in Appendix C. 5 Training and hyperparameter details in Appendix B.</p>
<p>Inspiration Retrieval Module</p>
<p>We take broad inspiration from cognitive aspects of innovation (Hope et al., 2023): when researchers generate a new idea, they are grounded in a web of existing concepts and papers bearing on the new idea.We aim to enrich the context of each background by retrieving "inspirations"-pieces of information that can guide hypothesis generation.As illustrated in Figure 2, for a given instance of the SCIMON task, our retrieval augmentation can retrieve from three types of sources.Each source uses a different form of query and output.</p>
<p>Semantic Neighbors For a given problem/motivation as input, ideas proposed for related problems in the training set can serve as a guiding reference for generating a new idea.Given the background context B with a seed term v and problem/motivation M, we construct a base input b: a concatenation of M with a prompt P belonging to one of two templates: "v is used for p" or "v is done by using p", where p is one of Task/Method/Material/Metric.In short, b := P ⊕ context:M.For example, in Figure 2, the concatenation is "Knowledge acquisition is done by using Method; Context:...requires plms to integrate information...lifelong manner...".</p>
<p>We then retrieve inputs from the training set that are semantically related to a new base input b, and obtain target sentences T corresponding to each retrieved training input.We extract the target term t ∈ T matching the seed term in b ( §2.2) as inspiration for input b.Simply put, this means we use as inspiration the salient aspect of the solution proposed in T , which we found empirically to help remove noisy/irrelevant information in T .For example, in Figure 2, we find "informative entities are done by using Method context: in this work, we aim at equipping pre-trained language models with structured knowledge."as similar to the input and use t ="linked knowledge graph" as inspiration.</p>
<p>Technically, we first construct a fully connected graph G S based on the training set where each node is a pair of input text b i and target term t i .We define the weight between two nodes i and j as the cosine similarity between b i and b j based on representations from SentenceBERT (Reimers and Gurevych, 2019) (all-mpnet-base-v2).Given b, we first insert it into G S and compute the weights of its connected edges.We then retrieve neighbors input text {b 1 , . . ., b k } from the training set with the largest edge weight, where k is the number of retrieved instances.We consider the corresponding target terms {t 1 , . . ., t k } as semantic inspirations.</p>
<p>KG Neighbors</p>
<p>We also explore enriching the context by linking it to a background KG with information on related methods and tasks.Using the same IE process used to extract our training examples ( §2.2), we create a global background KG G B which covers all papers in the corpus D Y prior to a given year Y (i.e., the nodes in G B correspond to tasks/methods/materials/metrics, and the edges are used-for relations, extracted and normalized from across the entire corpus as described earlier).Then, given a seed term v at query time, we select adjacent nodes {n 1 , n 2 , ...} from G B as inspirations.</p>
<p>As an example, in Figure 2, the neighbor nodes of "knowledge acquisition" include "collaborative web text annotation editor", "image matching", etc., which we select as inspirations.</p>
<p>Citation Neighbors Another notion of contextual relatedness we explore is via citation graph links.Here, given as input background context B, we assume access to the original source document d from which B was extracted, and consider its cited paper title set C d as potential candidates.This can be seen as a stronger assumption on information available to the model-assuming a researcher using the model provides relevant candidate documents from which ideas could be pooled.Because the training set only contains papers before year Y, we only select papers C dY ⊆ C d prior to year Y.</p>
<p>We then retrieve the top-k titles with the highest cosine similarity to d from C dY based on their Sen-tenceBERT embeddings as earlier.For instance, in Figure 2, the paper ELLE (Qin et al., 2022) cites the paper (de Masson d' Autume et al., 2019).Therefore, we choose the title "episodic memory in lifelong language learning" as inspiration information.</p>
<p>Generation Module</p>
<p>The idea generation module is given retrieved inspirations i 1 , . . ., i k along with context M as input.</p>
<p>In-Context</p>
<p>Learning We experiment with recent state-of-the-art LLMs, GPT3.5 davinci-003 (Ouyang et al., 2022) andGPT4 gpt-4-0314 checkpoint (OpenAI, 2023).We first ask the model to generate sentences based on the seed term and the context in the zero-shot setting without any in-context examples (GPT3.5ZS,GPT4ZS).We then ask the model to generate sentences in a few-shot setting by prompting randomly chosen pairs of input and output from the training set (GPT3.5FS, GPT4FS).Inspired by Liu et al. (2022), we further employ a few-shot setting using semantically similar examples.Instead of random in-context examples, we use the top-k examples from the training set with the highest cosine similarity to the query (GPT3.5Retr).This few-shot retrieval setting differs from the semantic neighbor discussed above, in that we provide both the input and output of each instance rather than solely supplying target entities as additional input.</p>
<p>Fine Tuning We fine-tune T5 (Raffel et al., 2020) (more recent models may be used too; see our biomedical experiment §4.1 fine-tuning an LLM).We observe that the generation models tend to copy phrases from the background context.For example, given the context "...hierarchical tables challenge numerical reasoning ...", the model will generate "hierarchical table reasoning for question answering" as the top prediction.For generating suggestions of novel ideas, we wish to discourage overly copying from the background context.We introduce a new in-context contrastive objective, where negative examples are taken from the text in the input (e.g., in Figure 2, the in-context negatives are plms, pretraining, etc).We compute an InfoNCE loss (Oord et al., 2018) over the hidden states of the decoder, aiming to maximize the probability of the ground truth against those of in-context negatives:
y + = σ(Avg(W y h + + b y )) y − k = σ(Avg(W y h − k + b y )) L cl = exp (y + /τ ) k exp y − k /τ + exp (y + /τ )(1)
where h + and h − k are decoder hidden states from the positive and k-th negative samples, W y and b y are learnable parameters, σ is a sigmoid function, τ is a temperature hyperparameter, and Avg( * ) denotes the average pooling function based on the target sequence length.We optimize with both contrastive loss L cl and the cross-entropy loss.</p>
<p>Iterative Novelty Boosting with Retrieval</p>
<p>We further improve the novelty of generated ideas with a new iterative retrieve-compare-update scheme.Conceptually, we consider a noveltyinducing penalty γ nov (I, R) that penalizes ideas I that are too "close" to existing ideas in literature reference examples R. γ nov (I, R) is included during in-context learning and inference, providing numerical feedback in the form of a score reflecting similarity to existing work.We wish to minimize this score while ensuring I remains relevant to the background context B; we do so iteratively by (1) retrieving related work from R, (2) measuring degree of novelty, (3) instructing the model to update I to be more novel w.r.t R, conditioning on B.</p>
<p>Specifically, in our implementation, we construct a reference corpus R based on all papers in the training set.We then propose an iterative algorithm that compares generated ideas against R. We start with the initial idea I 0 generated by the generation module.At each time step t, we use the generated idea I t as a query to retrieve k nearest ideas from the literature reference corpus R = {R 1 , ..., R k } based on SentenceBERT, with the top-k highest cosine similarity scores to I t (we use k = 20).For each retrieved ground truth literature idea R i , we compare its cosine similarity score S i against a threshold µ (we use 0.6).We provide all the retrieved ground truth ideas R that pass the threshold as additional negative examples for the large language models with the following instruction prompt: "Your idea has similarities with existing research as demonstrated by these j sentences: R Make sure the idea you suggest is significantly different from the existing research mentioned in the above sentences.Let's give it another try."We stop the iteration once all S i are lower than µ. Figure 2 and Table 5 demonstrate novelty iterations.</p>
<p>Experiments</p>
<p>Human Evaluation</p>
<p>We present four human evaluation studies, exploring different facets of our problem and approach.</p>
<p>Study I: Comparing Outputs across Model Variants</p>
<p>We recruit six volunteer NLP experts with graduatelevel education to rate the system.Raters are told to envision an AI assistant that suggests new paper ideas.We randomly select 50 instances (back-ground+seed) from the gold subset.Each annotator receives ten instances, each paired with system outputs from different model variants (Table 2).We ask raters to assess idea quality by considering each output's relevance to the context, novelty, clarity, and whether the idea is reasonable (positive ratings are dubbed "helpful" as shorthand, indicating they pass the multiple considerations).We observe moderately high rater agreement. 6Raters are blind to the condition, and system outputs are randomly shuffled across instances.We instruct annotators to only provide positive ratings to ideas sufficiently different from the input context.In Study I, we ask raters not to anticipate groundbreaking novelty from the system but rather a narrower expectation of quality and utility; in Study II below, we enrich the analysis to examine ranking between top models and also "raise the bar" and compare to actual ideas from papers. 7n a preliminary experiment, we also collected human ratings for GPT4-ZS (zero-shot) vs. GPT4-FS (few-shot) using the same criteria, finding GPT4-FS ranked higher in 65% of cases, with the rest mostly tied; thus, zero-shot GPT-4 was left out of the remainder of study I and subsequent studies to reduce annotation effort and cost.</p>
<p>Results Overall, GPT4FS and GPT4FS+KG outperform other models by a wide margin (Table 2).Apart from GPT4, T5+SN+CL performs best compared to other baselines, given its stronger prior knowledge of useful similar background hypotheses.In general, GPT3.5 models performed worse than fine-tuned T5 and its variants, which echoes results in other work in the scientific NLP domain (Jimenez Gutierrez et al., 2022).GPT4 outputs tended to be longer, which may partially explain higher human preference.Table 2: Percent (%) of total votes each system output receives from human raters.H denotes a helpful output, while U denotes an unhelpful output."3FS" refers to the GPT3.5FS."3Rt" refers to the GPT3.5Retr."4" refers to GPT4FS, and "4+KG" refers to the GPT4FS+KG."T5+SN" refers to the T5+SN+CL.GPT4FS and GPT4FS+KG are rated much higher.While GPT4FS has a slightly higher rating than the KG variant, a further human study reveals that GPT4FS+KG often leads to more technical depth ( §4.1).</p>
<p>Study II: Comparing GPT4 Variants against Real Papers</p>
<p>We conduct a follow-up human study of close competitors GPT4FS and GPT4FS+KG with a subset of the annotators to evaluate the incrementality and novelty of the generated ideas.In this study, model outputs are now ranked, unlike the binary classification of helpful/not in Study I. Suggestions are ranked according to the level of technical detail and innovation in comparison to each other-i.e., ranking which of GPT4FS and GPT4FS+KG had a higher degree of technical detail and novelty, or whether they are roughly the same (tied).Finally, outputs are rated versus the ground truth idea, according to whether or not the suggestions were roughly at the same level of technical detail and innovation as the original paper's idea, or significantly lower.</p>
<p>Results Overall, GPT4FS+KG is found to have higher technical detail in 48% of the compared pairs, and found to be less incremental (more novel) in 45% of the pairs.Among the remaining 52%/55% (respectively), the vast majority are ties, indicating that whenever GPT4FS+KG is not favored, it is of roughly the same quality as GPT4FS, but not vice versa.However, the most crucial aspect is comparing the results against the original ground truth idea on the quality of innovation.Here, we find that in 85% of comparisons, the ground truth is considered to have significantly higher technical level and novelty; and in the remaining 15%, the ground truth was ambiguous or lacking additional context from the paper abstract.This points to a major challenge in obtaining high-quality idea generations using existing state-of-the-art models.</p>
<p>Study III: Evaluation on Iterative Novelty Boosting</p>
<p>We conduct a fine-grained evaluation of our novelty mechanism with qualitative and quantitative evaluation of novelty.Specifically, we ask five annotators to further compare the novelty-enhanced results against the initially generated ideas.We randomly select 70 instances (background+seed) from the sentence generation gold subset.We ask annotators to check whether the new ideas are different than the initial ideas (e.g., adding new information or approaches), and whether they are more novel (i.e., a new idea can be different, but not necessarily more novel).Since GPT4FS+SN outperforms other models, for this model, we further instruct annotators to compare the novelty of the second iteration results against the first iteration results.</p>
<p>Results</p>
<p>For SN, in the first iteration 88.9% of updated ideas are substantially different from initial ideas, and for 55.6% we are able to increase nov- Ideas after novelty iterations are longer than initial ideas.We examine the new terms added after filtering 359 words, including stopwords, as many generic words and terms are often added (e.g., "novel model/method/approach").While our method helps boost novelty, overall the model often tends to suggest combinations between popular concepts ( §4.2).Novelty boosting seemed to often focus on adding dynamic/adaptive modeling, graph models and representations, the fusion of multiple modalities and sources-and sometimes all at once (e.g., "Dynamic Syntax-Aware Graph Fusion Networks (DSAGFN)"), and to explicitly compare against existing ideas from literature (Table 5).Table 4: Human evaluations results of each system output for the idea sentence prediction task on Biomedical Domain."vs.GT" refers to percents which system outputs are better than ground truth ideas.</p>
<p>Domain Generalization Case Study</p>
<p>Our domain-agnostic framework can be applied to other domains by changing the IE system used in the preprocessing procedure.To demonstrate A novel method called Adaptive Speech Unit Boundary Detection (ASUBD) ... a combination of attention mechanisms to focus on relevant acoustic and linguistic features and reinforcement learning to guide the system to make optimal predictions of unit boundaries based on previous decisions... Ground Truth ... an efficient monotonic segmentation module ... accumulate acoustic information incrementally and detect proper speech unit boundaries.</p>
<p>Table 5: Example of iterative novelty iterations.Our novelty iteration method enhances ideas overall; however ideas are often based on superficial recombinations of common concepts, far from the technical depth of scientific papers.</p>
<p>this, we conduct an additional initial experiment in the biochemical domain.We follow a similar data creation procedure as for NLP papers.We collect a dataset from PubMed papers and use PubTator 3 (Islamaj et al., 2021;Wei et al., 2022;Luo et al., 2023;Wei et al., 2023;Lai et al., 2023) as an IE system to extract a KG from paper abstracts.We use a sentence classifier trained on annotated abstracts (Huang et al., 2020) to select background context.We fine-tune a state-of-the-art biomedical large language model (Chen et al., 2023) on our data and evaluate on a test split past its pre-training cutoff date. 8We ask two biochemical domain experts with graduate-level education to evaluate the quality of the results as before, finding them to overall rate 80% of the generated directions positively.Finally, in contrast to NLP-domain experiments, evaluators were more satisfied with the generated outputs than the ground truth regarding technical detail.Detailed results are in Table 4.However, this preliminary experiment was meant mainly to demonstrate the generality of our approach, and a more in-depth exploration of utility and quality is left for future work.</p>
<p>Error Analysis</p>
<p>Models often made generic suggestions, woven together with specific details copied directly from the context (e.g., "NLP with ML algorithms and sentiment analysis" for some problem X, or "data augmentation and transfer learning" for Y, or "BERT or RoBERTa" for Z).Our techniques reduced this behavior but did not fully solve it.GPT4 models, especially, seemed to generate generic descriptions of common steps in NLP workflows (e.g., "Data preprocessing: Clean the text data, remove unnec-8 More data and training details in Appendix A.2, B.2.3.essary characters, perform tokenization...").All models often copied and rephrased directly from the context.In certain cases, models applied simple logical modifications to the context; e.g., when contexts described problems such as "high latency" or "efficiency limitations", the suggestions would include phrases such as "low latency" or "highly efficient".</p>
<p>Automated Evaluation Analysis</p>
<p>In open-ended tasks such as ours, automatic evaluations comparing system output to ground truth texts may be limited.Nonetheless, automated metrics such as ROUGE (Lin, 2004), BERTScore (Zhang* et al., 2020) and BARTScore (Yuan et al., 2021), that check the similarity between ground truth and generated output, may surface interesting findings.We find GPT-based models to be outperformed by T5-based models; GPT4 outputs are much longer than T5, explaining why they underperform in automatic metrics but outperform in human evaluations ( §4.1).Generated sentences often follow certain templates (e.g., "In this paper, we propose a new ... for ..."), which also helps explain why T5 fine-tuned on many examples scores higher superficially.At the same time, our in-context contrastive examples which encourage novelty with respect to background context, helped models perform better than baseline fine-tuning by reducing reliance on copying.See results in Table 9 (Appendix B.4).</p>
<p>Conclusions and Future Directions</p>
<p>We propose a new setting, model and comprehensive evaluation for scientific hypothesis generation with language models that are grounded in literature and optimized for novelty.We present a new framework named SCIMON in which mod-els take background problem contexts and provide suggestions that are novel while based on literature.Models retrieve inspirations from semantic similarity graphs, knowledge graphs, and citation networks.We introduce a new iterative novelty boosting mechanism that helps large language models (LLMs) such as GPT-4 generate more novel ideas by explicitly comparing ideas to prior work and refining them.Our experiments demonstrate that the task of generating natural language scientific hypotheses is highly challenging.While our methods improve upon baseline LLMs, generated ideas tend to be incremental and with insufficient detail.Generating novel and meaningful scientific concepts and their compositions remains a fundamental problem (Hope et al., 2023).Evaluation in this setting is also highly challenging, with a huge space of potentially plausible hypotheses formulated in natural language.One interesting direction is to expand SCIMON with a multimodal analysis of formulas, tables, and figures to provide a more comprehensive background context.</p>
<p>Limitations</p>
<p>We discuss limitations extensively throughout the paper, such as in terms of evaluation challenges and data quality.Here we include additional details on limitations.</p>
<p>Limitations of Data Collection</p>
<p>We crawled papers with Semantic Scholar Academic Graph API from 1952 to June 2022.The number of available papers is limited by the data we crawled from the Semantic Scholar Academic Graph.We also crawled papers from PubMed 1988 to 2024/01.We remove papers that are not English.We also remove papers where abstracts are not correctly parsed from paper PDFs.We will expand our models to papers written in other languages and other domains in the future.</p>
<p>Limitations of System Performance</p>
<p>Our dataset is based on state-of-the-art IE systems, which may be noisy.For instance, the coreference and SciSpacy abbreviation resolution models fail to link A2LCTC to Action-to-Language Connectionist Temporal Classification.The background context detection may also have errors: e.g., the sentence classification component fails to treat "For example, the language models are overall more positive towards the stock market, but there are significant differences in preferences between a pair of industry sectors, or even within a sector." as background context.In our human-vetted gold data subset, we make sure to filter such cases, but they remain in the training data.SentenceBert (Reimers and Gurevych, 2019), and GPT3.5/4 are not finetuned and might be biased towards pretraining datasets.The idea novelty boosting method is limited by the quality of retrieval models.Better retrieval models may be explored in the future.Due to hardware constraints, we mainly investigated models with up to 7 billion parameters.Due to API change and model randomness, our GPT3.5/4results might not be easily reproducible.</p>
<p>Limitations of Evaluation</p>
<p>We recruit annotators from Ph.D. students; their opinions may differ from annotators who have different levels of domain knowledge.Our setting uses a seed term taken from the ground truth as input, to emulate a scenario where a human provides guidance to an assistant model.Future work could explore methods in the setting without a seed term, an even harder task, or evaluate in an interactive setting with user-provided seed terms.In addition, while the seed is sampled from the ground truth, in our human-annotated gold subset, we make sure that in no case does the input context trivially leak the output.et al. (2023) reports that LLMs tend to memorize part of their training data, a well-known concern in evaluating current LLMs.Therefore, we examine the pretraining data of each model:</p>
<p>Memorization Check</p>
<p>Carlini</p>
<p>• T5: Raffel et al. (2020) shows that T5 is pretrained on C4 which was crawled from web prior to April 2019.</p>
<p>• GPT3.5:Based on the documentation,9 GPT-3.5 series is pretrained on a combination of test and code from before Q4 2021.</p>
<p>• GPT4: OpenAI (2023) shows that the GPT-4 checkpoint we used utilizes most pertaining data before September 2021.Despite this, the pretraining and post-training data contain "a small amount" of more recent data.10</p>
<p>Because we evaluate our models on papers published in 2022, the likelihood of test papers appearing in the pretraining corpora for the models is substantially reduced.We additionally performed a manual examination of GPT-4 memorization in our gold set based on 2022 ACL Anthology papers, by seeing if GPT-4 could complete information such as method names or generate text that strongly mimics the ground truth papers, and found no evidence of this occurring.The Meditron-7b (Chen et al., 2023) uses PubMed with a cut-off in August 2023, and our biochemical test set only includes PubMed papers after 2023/08.</p>
<p>A Dataset Collection</p>
<p>A.1 NLP Dataset Collection</p>
<p>We download ACL Anthology papers from 1952 to 2022 using Semantic Scholar Academic Graph API. 11We filter out papers without abstracts and not written in English to obtain 67,408 papers.Our dataset has 58,874 papers before 2021, 5,946 papers from 2021, and 2,588 from 2022.We first use PL-Marker (Ye et al., 2022) pretrained on Sci-ERC (Luan et al., 2018) to extract nodes belonging to six types: Task, Method, Evaluation Metric, Material, Other Scientific Terms, and Generic Terms.The model then predicts relations between nodes belonging to seven relation types: Usedfor, Feature-of, Evaluate-for, Hyponym-of, Partof, Compare, and Conjunction.Because we want to generate new ideas, we focus on used-for relations in papers.Next, we use SciCo (Cattan et al., 2021) with checkpoint from Hugging Face12 to obtain entity coreference to merge identical nodes.Then, we use ScispaCy (Neumann et al., 2019) to perform unsupervised abbreviation detection to replace the abbreviation with a more informative long form.Finally, we perform scientific sentence classification (Cohan et al., 2019) 13 to classify sentences from the abstract into five categories including Background, Method, Objective, Other, and Result.We select sentences with labels of Background and Other as background context.During preprocessing, we only keep high-confidence outputs from IE models.Figure 4 shows an example of the IE systems pipeline.</p>
<p>A.2 Biochemical Dataset Collection</p>
<p>We collect PubMed papers from 1988 to 2024 using Entrez Programming Utilities API14 for the following topics, including Yarrowia, Saccharomyces cerevisiae, Issatchenkia orientalis, and Rhodosporidium toruloides.We use PubTator 3 (Islamaj et al., 2021;Wei et al., 2022;Luo et al., 2023;Wei et al., 2023;Lai et al., 2023).The PubTator 3 performs named entity recognition, relation extraction, entity coreference and linking, and entity normalization for the abstracts in the dataset.Pub-Tator 3 identifies bio entities belonging to seven types: gene, chemical, chromosome, cell line, variant, disease, and speciesl and relations belonging to 13 types: associate, cause, compare, convert, contract, drug interact, inhibit, interact, negative correlate, positive correlate, prevent, stimulate, and treat.Finally, we use a sentence classifier trained on CODA-19 (Huang et al., 2020) to classify sentences in abstracts into background, purpose, method, finding, and other.We select sentences with labels of background as background context and remove sentences with labels of other.We treat the rest sentences that have at least one entity as the target sentence.We only keep samples with low similarity between background context and corresponding ground truth sentences. 15Our final dataset has 4,767 papers before 2023/02, 642 papers from 2023/02 to 2023/08, and 299 papers after 2023/08.</p>
<p>B Finetuning and Automated Evaluation details B.1 Inspiration Retrieval Module</p>
<p>The statistics of each inspiration type are in Table 7.</p>
<p>Table 8 shows sample retrieved inspirations.</p>
<p>B.1.1 Semantic Neighbors</p>
<p>We use all-mpnet-base-v2 from SentenceBert (Reimers and Gurevych, 2019), which performs best in semantic search to retrieve similar nodes from the training set based on query q in §3.1.We retrieve up to 20 relevant semantic neighbors R from the training set for each instance.We treat the target nodes from R as semantic neighbors.</p>
<p>B.1.2 KG Neighbors</p>
<p>We use one-hop connected neighbors from the background KG G B constructed on papers before 2021(i.e., the papers in the training set).Because of the scarcity of KG neighbors, we do not limit the number of KG neighbors.</p>
<p>B.1.3 Citation Neighbors</p>
<p>Similar to semantic neighbors, we use all − mpnet − base − v2 from Sentence-Bert (Reimers and Gurevych, 2019) to retrieve cited paper titles similar to query q.We restrict cited papers only before 2021.We retrieve up to 5 relevant citation neighbors from the papers' citation network.</p>
<p>B.2 Generation Module</p>
<p>Our T5 model and their variants are built based on the Huggingface framework (Wolf et al., 2020). 16e optimize those models by AdamW (Loshchilov and Hutter, 2019) with the linear warmup scheduler.17Those models are finetuned on 4 NVIDIA A6000 48GB GPUs with distributed data parallel. 18he training time for each model is about 10 hours.</p>
<p>Used-for</p>
<p>Used-for</p>
<p>Feature-of</p>
<p>Used-for</p>
<p>Used-for</p>
<p>Coref</p>
<p>Method</p>
<p>Other Scientific Terms Task Impressive milestones have been achieved in text matching by adopting a cross-attention mechanism to capture pertinent semantic connections between two sentence representations.However, regular cross-attention focuses on word-level links between the two input sequences, neglecting the importance of contextual information.</p>
<p>We propose a context-aware interaction network (COIN) to properly align two sequences and infer their semantic relationship.</p>
<p>B.2.1 In-Context Learning</p>
<p>We choose GPT3.5 davinci-003 19 (Brown et al., 2020) as our out-of-the-box causal language modeling baseline.We select 5 instances from the training set as examples for the few-shot setting.We randomly select those examples for GPT3.5FS.For GPT3.5Retr, similar to semantic neighbors, we use all-mpnet-base-v2 from SentenceBert (Reimers and Gurevych, 2019), which performs best in semantic search to retrieve similar instances from the training set based on query q in §3.1.The input length is limited to 2048 tokens due to OpenAI API limits.We choose gpt-4-0314 as our GPT4 model.Our input for GPT4 is similar to GPT3.5.</p>
<p>For each selected example from the training set with forward relation, the template is "Consider the following context: M In that context, which p can be used for v, and why?T ", where M is the background context, p is the target node type, v is the seed term, and T is the target idea sentence; for backward relation, the template is "Consider the following context: M In that context, which p do we use v, and why? s".For selected examples with 19 openai.com/api/additional retrieval inspirations, we concatenate the following additional template to the M: "The retrieval results are: i 1 , . . ., i k ", where i 1 , . . ., i k are retrieved inspirations.For the final prompt, the template is similar to the above example template.However, the target sentence T will not be included.We ask the model to generate 10 outputs.We will select the best output and skip the empty output.</p>
<p>B.2.2 Fine Tuning</p>
<p>Given input without any inspirations, the input combines the prompt P and context M as shown in §3.1 (i.e., P | context: M).Given input with inspirations, the input is P | retrieve: i 1 , . . ., i k | context: M, with i 1 , . . ., i k as retrieved inspirations.The input length is limited to 512 tokens.</p>
<p>For both tasks, we finetune our model based on T5-large with a learning rate of 6 × 10 −6 and ϵ = 1 × 10 −6 .The batch size is 8 for each GPU.</p>
<p>The maximum training epoch for all models is 10 with 4 patience.During decoding, we use beamsearch to generate results with a beam size of 5 and a repetition penalty of 1.5.</p>
<p>In-context Contrastive Augmentation</p>
<p>We randomly select 2 sentences that appeared in the input as in-context negatives.For example, in Figure 1, the in-context negatives could be "knowledge acquisition is done by using Method", "this requires plms to integrate the information from all the sources in a lifelong manner .".</p>
<p>B.2.3 Biochemical Case Study</p>
<p>Our Meditron-7b (Chen et al., 2023) and its variants are built based on the Huggingface framework (Wolf et al., 2020). 20We use its epfl-llm/meditron-7b as the base model.We finetune those models with a learning rate of 2 × 10 −6 and ϵ = 5 × 10 −8 .The maximum training epoch for all models is 5.All models are finetuned on 4 NVIDIA A100 80 GB GPUs with Fully Sharded Data Parallel. 21The training time for each model is about 20 hours.</p>
<p>B.3 The Scale of Retrieval Set</p>
<p>We retrieve from a set of 59k papers with over 374k sentences in the NLP domain, the focus of our experiments.Our background KG built on the training set has more than 197k nodes and 261k relations.Moreover, we collect 87k paper titles from citation networks.This represents a large-scale and diverse domain; retrieving inspirations from this set is expected, in principle, to be more than enough for generating novel ideas.Indeed, NLP papers typically cite each other and build on each other as inspirations to create new ideas -which motivates our inspiration retrieval.</p>
<p>B.4 Automated Evaluation</p>
<p>We use BERTScore (Zhang* et al., 2020) with SciBERT checkpoint for both tasks.</p>
<p>The hash of the checkpoint is allenai/scibert_scivocab_uncased_L8 _no-idf_version=0.3.12(hug_trans=4.19.2).The automated evaluation results are in Table 9.</p>
<p>C Human Annotation and Evaluation Details</p>
<p>Gold Dataset Annotation Details The gold dataset annotation interface is in Figure 5.The quality of the instances in the test set is judged given three criteria: (1) whether the ground truth sentence trivially overlaps with background context;</p>
<p>(2) whether background context contains relevant information for the target relation; (3) whether the target relation (from which the seed term is taken) is a salient aspect of the idea proposed in the target paper.</p>
<p>Study I The instructions for human evaluation can be found in Figure 6, while an example of the 20 github.com/huggingface/transformers 21https://huggingface.co/docs/accelerate/usage _guides/fsdp human evaluation interface is provided in Figure 7 and 8. Human annotators are required to evaluate each system output based on the following criteria: (1) Is the candidate relevant to the context + seed term?(2) Does the candidate copy too much from the context, or is it sufficiently novel/different from the context?(3) Does the candidate's suggestion generally make sense to you scientifically?(4) Is the language sufficiently clear and coherent to understand the suggestion?The input for sample human annotation is in Table 10 and the human labels are in Table 11.The human annotation agreement is in Table 13.</p>
<p>Study III We ask the following questions to human annotators to evaluate the quality of regeneration results: (1) Is the regenerated idea substantially different from the original?(2) Is the regenerated idea more novel and creative than the original idea?(3) Does the second iteration increase novelty?The human annotation agreement is in Table 14.</p>
<p>D Scientific Artifacts</p>
<p>We list the licenses of the scientific artifacts used in this paper: Semantic Scholar Academic Graph API (API license agreement22 ), Huggingface Transformers (Apache License 2.0), SBERT (Apache-2.0license), BERTScore (MIT license), Meditron-7b (Llama2), Entrez Programming Utilities API (Copyright23 ), PubTator 3 (Data use policy24 ), and OpenAI (Terms of use 25 ).</p>
<p>E Ethical Consideration</p>
<p>The SCIMON task and corresponding models we have designed in this paper are limited to the natural language processing (NLP) and biochemical domain, and might not apply to other scenarios.</p>
<p>E.1 Usage Requirement</p>
<p>This paper aims to provide investigative leads for a scientific domain, specifically natural language processing.The final results are not intended to be used without human review.Accordingly, domain experts might use this tool as a research writing assistant to develop ideas.However, our system does not do any fact-checking with external knowledge.In addition, we train our models on the ACL</p>
<p>E.2 Data Collection</p>
<p>We collect 67,408 ACL Anthology papers from 1952 to 2022 using Semantic Scholar Academic Graph API, under API license agreement26 .We ensure our data collection procedure follows the Terms of Use at https://allenai.org/terms.According to the agreement, our dataset can only be used for non-commercial purposes.As mentioned in §4, we perform the human evaluation.All CT is a model with citation inspirations.R-L denotes ROUGE-L.BERT denotes BERTscore with SciBERT as its encoder.† indicates that differences between models are not statistically significant (p ≤ 0.05) when compared to each other but are still significant when compared to the other models on t-test.</p>
<p>Type Content</p>
<p>Seed Term Prompt Irish language learning is done by using Method Context Irish is a minority language which means that l2 learners have limited opportunities for exposure to the language, and additionally, there are also limited (digital) learning resources available.One method that could be used for Irish language learning is computer assisted language learning (CALL) unhelpful</p>
<p>GPT3.5FS+CT</p>
<p>In that context, the most appropriate task for precision grammar would be the development of a part-of-speech tagger for Irish using finite-state morphology and constraint grammar disambiguation.</p>
<p>unhelpful</p>
<p>GPT3.5FS+KG</p>
<p>Cló: An Irish Language Learning Platform -Leveraging Digital Resources and Human Expertise unhelpful</p>
<p>GPT4FS</p>
<p>We propose using a technology-enhanced approach, such as computer-assisted language learning (CALL) or natural language processing (NLP) techniques, to create interactive and engaging Irish language learning resources, as these methods can provide personalized learning experiences and enhance learners' exposure to the language despite limited resources available.</p>
<p>helpful</p>
<p>GPT4FS+KG</p>
<p>Exploiting Social Media for Irish Language Learning: An Analysis of Twitter Data.In this context, we use social media data, particularly from Twitter, as a method for Irish language learning, because it provides a rich source of authentic and diverse language examples that can be used to enhance learning opportunities for L2 learners in a minority language setting.helpful</p>
<p>T5</p>
<p>In this paper, we present a novel machine translation approach to Irish language learning.unhelpful T5+SN+CL</p>
<p>A conversational virtual environment for Irish language learning helpful Table 11: Sample human labels for idea-sentence generation results for seed term prompt "Irish language learning is done by using Method", with context "irish is a minority language ... limited digital learning resources available".</p>
<p>Type Content</p>
<p>Seed Term ARO10 Context</p>
<p>Huangjiu is known for its unique aroma, primarily attributed to its high concentration of bphenylethanol (ranging from 40 to 130 mg/L).</p>
<p>Meditron</p>
<p>We then selected six key genes involved in the phenylalanine metabolism pathway and predicted that their product flux may affect the synthesis of b-phenylethanol.</p>
<p>Meditron + CT</p>
<p>We found that the key amino acid residue that controls the activity of Aro10p was not conserved in wine yeast strains, which may explain the lower b-phenylethanol production in wine fermentation compared with that in Chinese huangjiu.Meditron + KG Both target genes, SSA1 and ARO10, were deleted using the CRISPR-Cas9 genome editing system.Meditron + SN Herein, we report that the key barrier for b-phenylethanol production in Huangjiu is ARO10, the only bi-functional amino acid decarboxylase in Saccharomyces cerevisiae.</p>
<p>Table 12: Input and idea-sentence generation results for seed gene "ARO10" in the biochemical domain.</p>
<p>Annotator Pair 1-2 1-3 1-4 1-5 1-6</p>
<p>Agreement % 68.8 75.0 56.2 43.8 75.0</p>
<p>Table 13: Percent (%) of same labels from overlapped 10 human evaluation instances on each pair of annotators for Study I.</p>
<p>Figure 1: SCIMON takes background context and generates ideas grounded in literature inspirations, optimizing novelty by iteratively comparing to related work.</p>
<p>Figure 2 illustrates the setting, showing a background ...a method that combines continual learning with a dynamic knowledge distillation approach for efficient knowledge acquisition ... 1. Different from previous knowledge distillation methods ... student model learns from teacher model for incremental knowledge extraction ... ...continual learning for knowledge acquisition...This approach is more efficient than exhaustive pre-training on all existing data...</p>
<p>a method that leverages memory-augmented neural networks for knowledge acquisition in a lifelong learning scenario...</p>
<p>Figure 2 :
2
Figure 2: Architecture overview.Our models retrieve inspirations and then pass the background input and retrieved inspirations to an LM-based generation module, which iteratively optimizes novelty.Input from Qin et al. (2022).</p>
<p>Figure 3 :
3
Figure3: We use IE to obtain literature data for our approach: problems/motivations (background) and proposed ideas (target), as well as salient seed terms.</p>
<p>Figure 5 :
5
Figure 5: Gold subset annotation interface</p>
<p>Percent (%) of same labels from overlapped 20 human evaluation instances on each pair of annotators for Study III.(1-3) has 60 shared questions.The rest of the pairs each share 40 questions.</p>
<p>Figure 6 :
6
Figure 6: Human evaluation instructions</p>
<p>4
SplitForwardBackwardTotalTrain55,88458,426114,310Valid7,9388,25716,195Test2,6232,6865,309Table 1: Dataset statistics. Considering a relation of theform [v used-for u], we define [v used-for ?] asforward, and [? used-for u] as backward.</p>
<p>Table 3 :
3
Relative improvements of iterative novelty boosting.Iterations are applied to the ideas for which sufficiently similar related work is detected ( §3.3)."1st Novelty" is % of the 1st iteration ideas that gained novelty over the initial idea, and "2nd Novelty" is the % of gain over the 1st iteration.Our method substantially increases novelty for ideas to which it is applied.To save annotation resources, we only annotate second iteration results for the best-performing method (SN).We report the average number of new terms added, after filtering.elty/creativity (meaning that, e.g., if 100 examples were updated, we would gain 56 examples that are more novel).The 2nd iteration, further increases novelty for 57.8% of the ideas that continued to another iteration.For ideas not considered more novel after applying our method, we do not observe a drop in novelty-the method either increases or maintains novelty.
TypeGPT4FS+SN+CT+KG1st Novelty ∆ (%)+54.4+55.6 +47.8 +46.72nd Novelty ∆(%)-+57.8--1st new terms ∆+23.1+22.8 +22.1 +21.92nd new terms ∆-+21.5--</p>
<p>seed term: speech unit boundaries ; context (abridged): ... generate partial sentence translation given a streaming speech input.existingapproaches... break the acoustic units in speech, as boundaries between acoustic units in speech are not even....Initial ideaA pause prediction model to identify speech unit boundaries ... Iteration 1A method that leverages acoustic and linguistic features to predict speech unit boundaries dynamically, ensuring smooth transitions ... differs from the existing research as it combines both acoustic properties and linguistic context ... adapting to variations in speaker characteristics, speaking styles, and languages.Iteration 2
TypeContentInput (Donget al., 2022)</p>
<p>Table 6 :
6
Human quality evaluation of preprocessing stages(%).Overall pass rate after all steps are applied is 79.7%.
Background Sentence</p>
<p>Table 7 :
7
Average of # of neighbors for each instance, excluding those which do not have any neighbor</p>
<p>an effective solution to data scarcity in low -resource scenarios.however, when applied to token-level tasks such as ner , data augmentation methods often suffer from token-label misalignment, which leads to unsatsifactory performance.
TypeContentSeed Term Promptdata augmentation is used for TaskContext data augmentation is Semantic Neighbors st and automatic speech recognition (asr), low-resource tagging tasks, end-to-endspeech translation, neural online chats response selection, neural machine translation,semi-supervised ner, entity and context learning, semi-supervised setting, dependency pars-ing, low-resource machine translation, slot filling, dialog state tracking, visual questionanswering, visual question answering (vqa), low-resource neural machine translationKG Neighborsnmt-based text normalization, task-oriented dialog systems, task-oriented dialogue system,low-resource languages (lrl), end-to-end speech translation, visual question answering (vqa),multiclass utterance classification, clinical semantic textual similarity, neural online chatsresponse selection, context-aware neural machine translationCitation NeighborsContextual Augmentation: Data Augmentation by Words with Paradigmatic Re-lations,An Analysis of Simple Data Augmentation for Named Entity Recognition,DataAugmentationforLow-ResourceNeuralMachineTranslation,DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks,EDA: Easy Data Augmentation Techniques for Boosting Performance on Text ClassificationTasksGround TruthELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER</p>
<p>Table 8 :
8
Example (from (Zhou et al., 2022)) of retrieved inspirations.Inspirations similar to ground truth are underlined.
context containsRelationrelevantis a partIs theIE is ofinformation forof theoutputsufficienttarget relationmaintrivallyquality(Conservative filterideaoverlap(not-only flag casesproposewith thegeneric,where context isd by theinputcontextentityoutputrelationrel_sentcontextcorrect)highly irrelevant)papertransformer -basedlanguage models usuallytreat texts as linearsequences . however ,most texts also have anWe propose a novel approach toinherent hierarchicalformulate , extract , encode and injectstructure , i.e. , parts of ahierarchical structure informationtext can be identifiedexplicitly into an extractiveusing their position insummarization model based on a pre -this hierarchy . intrained , encoder -only Transformerextractive textaddition , section titleslanguage model ( HiStruct+ model ) ,summarizatiousually indicate thewhich improves SOTA ROUGEs forn is done bycommon topic of theirextractive textextractive summarization on PubMedusing Metricrespective sentences .summarization sota rouges used forand arXiv substantially .</p>
<p>Table 9 :
9
Automatic evaluation results for the challenging and gold subsets.CL is a model with in-context contrastive augmentation.SN is a model with semantic inspirations.KG is a model with KG inspirations.
SubsetChallengingGoldModelR-L↑BERT↑R-L↑BERT↑GPT4ZS0.1200.5810.1300.583GPT4FS0.1430.6180.1510.624T50.2230.672  †0.2460.685GPT4FS+SN0.1440.6200.1490.627GPT4FS+KG0.1430.6190.1520.626GPT4FS+CT0.1440.6170.1490.622T5+CL0.225  †0.671  †0.251  †0.686  †T5+SN+CL0.228  †0.671  †0.258  †0.686  †T5+KG+CL0.223  †0.6690.2480.681  †T5+CT+CL0.225  †0.671  †0.250  †0.686  †</p>
<p>Table 10 :
10
Input for sample human annotation results
ModelOutput
Code, data, and resources are publicly available for research purposes: https://github.com/eaglew/clbd.
More details are in Appendix C.
The agreement scores are in Table13Appendix C.
Full evaluator guidelines are in Appendix C. The sample annotations are in Table 11.
platform.openai.com/docs/model-index-for-res earchers
See footnote 10, page 10 of OpenAI (2023).
huggingface.co/allenai/longformer-scico
github.com/allenai/sequential_sentence_class ification
www.ncbi.nlm.nih.gov/books/NBK25501/
The similarity is calculated with all-mpnet-base-v2.
github.com/huggingface/transformers
huggingface.co/docs/transformers/main_classe s/optimizer_schedules#transformers.get_linear_sc hedule_with_warmup
pytorch.org/tutorials/intermediate/ddp_tutor ial.html
api.semanticscholar.org/license/
www.ncbi.nlm.nih.gov/books/about/copyright/
www.ncbi.nlm.nih.gov/home/about/policies/
openai.com/policies/terms-of-use
https://api.semanticscholar.org/license/ annotators involved in human evaluation are voluntary participants with a fair wage. We further collect 5,708 PubMed papers from 1988 to 2024 using Entrez Programming Utilities
API 27 . We follow their data usage
guidelines 28 . 27 www.ncbi.nlm.nih.gov/books/NBK25501/ 28 www.ncbi.nlm.nih.gov/books/about/copyright/
AcknowledgementsThis work is supported by the Molecule Maker Lab Institute: an AI research institute program supported by NSF under award No. 2019897, by DOE Center for Advanced Bioenergy and Bioproducts Innovation U.S. Department of Energy, Office of Science, Office of Biological and Environmental Research under Award Number DESC0018420, by U.S. the AI Research Institutes program by National Science Foundation and the Institute of Education Sciences, Department of Education through Award No. 2229873 -AI Institute for Transforming Education for Children with Speech and Language Processing Challenges, and by AI Agriculture: the Agriculture and Food Research Initiative (AFRI)grant no.2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture.The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied of, the National Science Foundation, the U.S. Department of Energy, and the U.S. Government.The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.
Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>A llm assisted exploitation of ai-guardian. Nicholas Carlini, arXiv:2307.15008Cryptography and Security Repository. 2023</p>
<p>Quantifying memorization across neural language models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Scico: Hierarchical cross-document coreference for scientific concepts. Arie Cattan, Sophie Johnson, Ido Daniel S Weld, Iz Dagan, Doug Beltagy, Tom Downey, Hope, 20213rd Conference on Automated Knowledge Base Construction</p>
<p>Meditron-70b: Scaling medical pretraining for large language models. Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, arXiv:2311.16079Computation and Language Repository. 2023</p>
<p>Pretrained language models for sequential sentence classification. Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, Dan Weld, 10.18653/v1/D19-1383Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Episodic memory in lifelong language learning. Cyprien De Masson D'autume, Sebastian Ruder, Lingpeng Kong, Dani Yogatama, Advances in Neural Information Processing Systems. Curran Associates, Inc201932</p>
<p>Learning when to translate for streaming speech. Qian Dong, Yaoming Zhu, Mingxuan Wang, Lei Li, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Literature based discovery: models, methods, and trends. Sam Henry, Bridget T Mcinnes, 201774Journal of biomedical informatics</p>
<p>A computational inflection for scientific discovery. Tom Hope, Doug Downey, Oren Etzioni, Eric Daniel S Weld, Horvitz, Communications of the ACM. 2023</p>
<p>Context-aware interaction network for question matching. Zhe Hu, Zuohui Fu, Yu Yin, Gerard De, Melo , 10.18653/v1/2021.emnlp-main.312Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Benchmarking large language models as ai research agents. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, arXiv:2310.03302Machine Learning Repository. 2023</p>
<p>CODA-19: Using a non-expert crowd to annotate research aspects on 10,000+ abstracts in the COVID-19 open research dataset. Ting-Hao Kenneth Huang, Chieh-Yang Huang, Chien-Kuang Cornelia Ding, Yen-Chia Hsu, C Lee Giles, Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020. the 1st Workshop on NLP for COVID-19 at ACL 2020Association for Computational Linguistics2020</p>
<p>Nlm-chem, a new resource for chemical entity recognition in pubmed full text literature. Rezarta Islamaj, Robert Leaman, Sun Kim, Dongseop Kwon, Chih-Hsuan Wei, Donald C Comeau, Yifan Peng, David Cissel, Cathleen Coss, Carol Fisher, Rob Guzman, Preeti Gokal Kochar, Stella Koppel, Dorothy Trinh, Keiko Sekiya, Janice Ward, Deborah Whitman, Susan Schmidt, Zhiyong Lu, 10.1038/s41597-021-00875-1Scientific Data. 81912021</p>
<p>Thinking about GPT-3 in-context learning for biomedical IE? think again. Jimenez Bernal, Nikolas Gutierrez, Clayton Mcneal, You Washington, Lang Chen, Huan Li, Yu Sun, Su, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Biorex: Improving biomedical relation extraction by leveraging heterogeneous datasets. Po-Ting Lai, Chih-Hsuan Wei, Ling Luo, Qingyu Chen, Zhiyong Lu, 10.1016/j.jbi.2023.104487Journal of Biomedical Informatics. 1461044872023</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>What makes good in-context examples for GPT-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.10The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Dublin, Ireland and OnlineAssociation for Computational Linguistics2022. DeeLIO 2022Proceedings of Deep Learning Inside Out</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld, 10.18653/v1/2020.acl-main.447Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, Proceedings of the 7th International Conference on Learning Representations. the 7th International Conference on Learning Representations2019</p>
<p>Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi, 10.18653/v1/D18-1360Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>AIONER: all-in-one scheme-based biomedical named entity recognition using deep learning. Ling Luo, Chih-Hsuan Wei, Po-Ting Lai, Robert Leaman, Qingyu Chen, Zhiyong Lu, 10.1093/bioinformatics/btad310Bioinformatics. 3953102023</p>
<p>Scientific language models for biomedical knowledge base completion: an empirical study. Rahul Nadkarni, David Wadden, Iz Beltagy, Noah A Smith, Hannaneh Hajishirzi, Tom Hope, 2021AKBC</p>
<p>ScispaCy: Fast and robust models for biomedical natural language processing. Mark Neumann, Daniel King, Iz Beltagy, Waleed Ammar, 10.18653/v1/W19-5034Proceedings of the 18th BioNLP Workshop and Shared Task. the 18th BioNLP Workshop and Shared TaskFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>The logic theory machine-a complex information processing system. Allen Newell, Herbert Simon, IRE Transactions on information theory. 231956</p>
<p>Representation learning with contrastive predictive coding. Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz, arXiv:2303.13375arXiv:1807.03748Computation and Language Repository. 2023. 2018Capabilities of GPT-4 on medical challenge problems</p>
<p>arXiv:2303.08774Gpt-4 technical report. Computation and Language Repository. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>ELLE: Efficient lifelong pre-training for emerging data. Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou, 10.18653/v1/2022.findings-acl.220Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211402020</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. Nils Reimers, Iryna Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Does scientific discovery have a logic? Philosophy of science. Simon Herbert, 197340</p>
<p>Contexts and contradictions: a roadmap for computational drug repurposing with knowledge inference. N Daniel, Russ B Sosa, Altman, Briefings in Bioinformatics. 2342682022</p>
<p>Undiscovered public knowledge. Don R Swanson, The Library Quarterly. 5621986</p>
<p>Agatha: automatic graph mining and transformer based hypothesis generation approach. Justin Sybrandt, Ilya Tyagin, Michael Shtutman, Ilya Safro, 10.1145/3340531.3412684Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. the 29th ACM International Conference on Information &amp; Knowledge Management2020</p>
<p>Unsupervised word embeddings capture latent knowledge from materials science literature. John Vahe Tshitoyan, Leigh Dagdelen, Alexander Weston, Ziqin Dunn, Olga Rong, Kristin A Kononova, Gerbrand Persson, Anubhav Ceder, Jain, Nature. 57177632019</p>
<p>PaperRobot: Incremental draft generation of scientific ideas. Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, Yi Luan, 10.18653/v1/P19-1191Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Findings of the babylm challenge: Sample-efficient pretraining on developmentally plausible corpora. Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning2023</p>
<p>Aleksandar Milosavljevic, and Zhiyong Lu. 2022. tmvar 3.0: an improved variant concept recognition and normalization tool. Chih-Hsuan Wei, Alexis Allot, Kevin Riehle, Bioinformatics. 3818</p>
<p>GNorm2: an improved gene name recognition and normalization system. Chih-Hsuan Wei, Ling Luo, Rezarta Islamaj, Po-Ting Lai, Zhiyong Lu, 10.1093/bioinformatics/btad599Bioinformatics. 39105992023</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020</p>
<p>Exploring and verbalizing academic ideas by concept co-occurrence. Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, Chenghu Zhou, 10.18653/v1/2023.acl-long.727Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Packed levitated marker for entity and relation extraction. Deming Ye, Yankai Lin, Peng Li, Maosong Sun, 10.18653/v1/2022.acl-long.337Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>BARTScore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. 2021</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Q Weinberger, Yoav Artzi, Proceedings of the 8th International Conference on Learning Representations. the 8th International Conference on Learning Representations2020</p>
<p>MELM: Data augmentation with masked entity language modeling for low-resource NER. Ran Zhou, Xin Li, Ruidan He, Lidong Bing, Erik Cambria, Luo Si, Chunyan Miao, 10.18653/v1/2022.acl-long.160Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland20221Association for Computational Linguistics</p>            </div>
        </div>

    </div>
</body>
</html>
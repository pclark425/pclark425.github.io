<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3818 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3818</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3818</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-259165244</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.09296v3.pdf" target="_blank">KoLA: Carefully Benchmarking World Knowledge of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For \textbf{ability modeling}, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering $19$ tasks. (2) For \textbf{data}, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For \textbf{evaluation criteria}, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate $28$ open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset and open-participation leaderboard are publicly released at https://kola.xlore.cn and will be continuously updated to provide references for developing LLMs and knowledge-related systems.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3818.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3818.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KoLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-oriented LLM Assessment benchmark (KoLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark introduced in this paper for carefully evaluating LLM world knowledge across a four-level cognitive taxonomy (Knowledge Memorization, Understanding, Applying, Creating) using both known (Wikipedia) and continuously collected evolving data, plus a contrastive evaluation system and season-based updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Ability-level decomposition into four cognitive levels (KM, KU, KA, KC); fairness via known vs evolving data; cross-task comparability via standardized scores; focused evaluation of knowledge-creating faithfulness (distinguishing created knowledge vs hallucinations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Automated metrics per task (EM, relaxed F1/token-match, Rouge, BLEU, accuracy) plus standardized z-score aggregation across models/tasks; a self-contrast method for generation/hallucination evaluation; paired Known vs Evolving comparisons; manual human annotation for validating knowledge-creation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>KoLA benchmark (19 tasks covering memorization, understanding, applying, creating). Data sources: Known = Wikipedia/Wikidata5M (2019 snapshot); Evolving = ~500 recently crawled articles per season (news + fiction) annotated for events and triples; specific tasks include High-Freq/Low-Freq memorization, ETM/ETA/ETC evolving tests, COPEN, FewNERD, DocRED, MAVEN, HotpotQA, 2WikiMulti, MuSiQue, KQA Pro, KoRC, and others.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Task-specific absolute metrics (Exact Match (EM), F1, relaxed/token F1, Accuracy, Rouge-L (F1), BLEU); standardized z-scores (per-task z = (x - mu)/sigma) scaled via Min-Max to [0,100] for cross-task comparability; self-contrast similarity (Rouge-L F1 between T and Tk) and composite KC score x = avg(∂(T,R), ∂(T,Tk), ∂(Tk,R)).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Crowdsourced annotators and internal validators for evolving-data annotation (fact triples, event arguments); human evaluation of KC outputs (4.2K scores collected) with 14 annotators on 1-5 scale for overall quality and faithfulness; multi-round quality checks and validator PhD reviewers; Cohen's Kappa and Fleiss' Kappa reported for annotation reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Coverage limited (19 English datasets, focused on concepts/entities/events) and annotation capacity constrains breadth; automatic KC metric may undervalue genuinely novel yet reasonable model-created knowledge that differs from human-provided references; human evaluation is costly and hard to scale; metric sensitivity differences across tasks necessitate standardization; potential test-data leakage and fairness concerns across models; models ignoring provided K in self-contrast could collapse evaluation; context-length/input-size limitations causing some models to be marked N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>No explicit 'scientific theory' is given; KoLA's KC examples are narrative event continuations. Paper cites an example of a hallucinated causal claim (death of Simon de Montfort causing rebels' loss) used to illustrate inconsistent/hallucinated relations in generated continuations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>KoLA findings: self-contrast metric ∂(T,Tk) correlates with human-judged faithfulness (Spearman 0.61); removing the self-contrast term reduces correlation with human overall quality by ~32%; standardized scoring enables per-task comparability; larger models tend to memorize more knowledge; instruction-tuning/alignment improves higher-level abilities (KA/KC) but may reduce raw memorization (an 'alignment tax'); open-source models generally underperform closed-source commercial models in these knowledge tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KoLA: Carefully Benchmarking World Knowledge of Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3818.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3818.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contrastive Evaluation System</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Evaluation System (standardized overall scoring + self-contrast)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-part evaluation design in KoLA combining standardized cross-task scoring (z-score + Min-Max scaling) and a self-contrast metric for generation that contrasts free completions with knowledge-grounded completions to assess knowledge-creating fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Cross-task comparability (relative performance among evaluated models) and fidelity of created content (consistency between unconstrained and knowledge-grounded generations and alignment with human reference).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>1) Standardized scoring: compute per-task metric x_ij, convert to z_ij = (x_ij - mu_i)/sigma_i, then Min-Max scale z across all tasks/models to [0,100] to produce s_ij; 2) Self-contrast for KC: for same prompt/context C, produce T (free) and Tk (given annotated foreknowledge K), compute similarities ∂(T,R), ∂(T,Tk), ∂(Tk,R) and aggregate by averaging.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used across KoLA's 19 tasks; self-contrast applied specifically to KC tasks: Encyclopedic Knowledge Creation (Wikipedia/MAVEN-based) and Open Knowledge Creation (Evolving news/fiction ETC).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Standardized scores s_ij in [0,100]; component metrics include Rouge-L (F1) for ∂ functions in KC self-contrast; task-native metrics feed into z-score (e.g., F1, EM, Accuracy, BLEU).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-annotated event knowledge K and human references R are required to run self-contrast; human judges used to validate correlation between self-contrast metric and faithfulness/quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Standardization depends on the evaluated model pool (relative metric); Min-Max scaling sensitive to extreme values; self-contrast requires annotated foreknowledge K and may penalize novel-but-valid model outputs not matching K; evaluation collapse possible when models ignore K in Tk prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>No explicit scientific-theory outputs; self-contrast illustrated with narrative continuations where T contains a hallucinated negotiation event while Tk constrained by K does not.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Self-contrast component significantly improves alignment with human judgments: ∂(T,Tk) correlated 0.61 with human faithfulness; inclusion of self-contrast raises correlation with human overall quality by ~32% relative to excluding it.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KoLA: Carefully Benchmarking World Knowledge of Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3818.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3818.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Contrast Metric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Contrast Knowledge-Creating Metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic metric that evaluates knowledge creation by contrasting a model's unconstrained completion (T) with its knowledge-grounded completion (Tk) on the same context and comparing both to the human reference R; focuses on event-level knowledge faithfulness and reduces style-fluency confounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Faithfulness and reasonableness of created knowledge (consistency between free and grounded outputs and agreement with annotated human event knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Generate T from context C alone, generate Tk given C+annotated foreknowledge K, compute text similarities ∂(T,R), ∂(T,Tk), ∂(Tk,R) using Rouge-L (F1), then aggregate x = avg(∂(T,R), ∂(T,Tk), ∂(Tk,R)).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to KoLA's Knowledge Creating tasks: Encyclopedic Knowledge Creation (4-1) and Open Knowledge Creating / ETC (4-2) built from MAVEN/Wikipedia and Evolving data with annotated event triples.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Rouge-L (F1) values for the three pairwise contrasts and the averaged composite KC score; reported correlation with human judgments (Spearman 0.61 for ∂(T,Tk) vs faithfulness).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Requires human annotation of event knowledge K in the reference R; human raters assessed outputs to validate metric (1-5 scale for overall quality and faithfulness).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on human-provided K so novel valid creations differing from K may be underestimated; Rouge-L similarity captures surface overlap and may miss deeper semantic alignment; requires annotated event structures, which is costly to produce at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Used on narrative continuations; the paper cites an example where a model-generated causal relation (Simon de Montfort's death causing rebels to lose) was inconsistent with R and highlighted by self-contrast comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>The self-contrast similarity ∂(T,Tk) shows notable correlation (0.61) with human-judged faithfulness; removing ∂(T,Tk) from composite score causes a ~32% drop in correlation with human-judged overall quality, indicating its strong contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KoLA: Carefully Benchmarking World Knowledge of Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3818.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3818.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standardized Scoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standardized Overall Scoring (z-score + Min-Max scaling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure to make heterogeneous task metrics comparable across tasks and models by converting per-task scores to standardized z-scores relative to evaluated-model distribution and scaling them into a common 0-100 range.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relative standing of a model on each task compared to the pool of evaluated models; cross-task comparability to produce aggregated level and overall scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>For each task d_i and model m_j compute x_ij, then z_ij = (x_ij - mu_i)/sigma_i; apply Min-Max scaling over all z_ij to map to [0,100] producing s_ij; average standardized scores across tasks in a level and then across levels for overall ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used to report KoLA main leaderboard and per-level rankings across all 19 tasks (and paired Known/Evolving task pairs for some standardized computations).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Standardized score s_ij in [0,100]; z-scores; per-task selected canonical metrics before standardization (e.g., F1, EM, Accuracy, Rouge, BLEU).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>No direct human involvement needed for scoring computation; human annotation informs some underlying datasets and reference labels that produce x_ij.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Scores are relative to the specific set of evaluated models (leaderboard-dependent); Min-Max scaling sensitive to outliers; missing/N/A model-task results handled as exclusions or assigned zero in some aggregates, which can bias rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Not applicable (scoring mechanism rather than example content).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Enabled intuitive cross-task comparisons and synthetic overall rankings; helped reveal trends (size correlates with memorization; alignment impacts higher-level abilities); but authors note potential biases from model pool and missing-value handling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KoLA: Carefully Benchmarking World Knowledge of Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3818.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3818.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Annotation & QC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Crowdsourced Annotation and Quality Control for Evolving Data and KC Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The pipeline and workforce used by KoLA to annotate evolving articles for fact triples and fine-grained event arguments and to human-evaluate knowledge-creation outputs, including multi-round QA and inter-annotator agreement metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness of triples (can the fact be known prior to season?), completeness and correctness of event arguments, and quality/faithfulness ratings of model-generated continuations (1-5 scale).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Pre-annotation with NER and relation extraction models (CLEVE, ATLOP variants), manual annotation by a 21-member team with two annotators per triple/event, two rounds of quality checks, validators resolving disagreements; human evaluation of KC outputs by 14 annotators with PhD validators for quality control.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Annotations produced evolving test sets: 2.7K correct triples (459 novel to the season), event argument annotations for 100 articles used in KC tasks; datasets ETM, ETA, ETU, ETC built from these annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Annotation agreement: Cohen's Kappa = 0.71 for triple correctness, Cohen's Kappa = 0.55 for temporal discoverability (b vs c), Fleiss' Kappa = 0.62 for event argument annotation; human KC ratings averaged and reported per model.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Extensive: 21 annotators for data construction, 14 annotators for KC evaluation, three PhD validators, paid crowdsourced workforce under contracts; annotators permitted web searches; work platform supports re-annotation workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Annotation cost constraints limit dataset size and season cadence; disagreements in discoverability annotation (kappa 0.55) show difficulty in provenance judgments; human judgments are time-consuming and costly, limiting scalability of KC reference annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Human raters scored model continuations (1-5) for quality and faithfulness; tables of average human KC scores across models are reported (e.g., GPT-4 and others listed with mean ratings).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>High first-pass annotation QC rates improved with iterative checks (first-pass QC 67%, second 91%); human KC ratings correlated with automated self-contrast metric, supporting the utility of the automated measure as a proxy for faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KoLA: Carefully Benchmarking World Knowledge of Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3818.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3818.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Answer Normalization & Metrics Selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-specific Answer Normalization and Metric Assignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Procedures KoLA uses to normalize open-ended generative outputs into comparable forms per task and to select the most representative metric for each task (e.g., relaxed F1 token-match, Accuracy, Rouge, BLEU).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Best reflection of model's true performance per task type: exactness for memorization, token coverage for extraction, and similarity for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Tokenize predictions and references with GPT2Tokenizer and compute relaxed/token F1 (token match rate) for many tasks; EM used where applicable; accuracy for classification; Rouge/BLEU for generative tasks; task-adaptive post-processing and format extraction (regex/fuzzy matching) to extract answers from noisy model outputs; models failing to produce valid outputs on >90% cases marked N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied across all KoLA tasks; the per-task chosen metric feeds into standardized score computation.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Relaxed F1 (token-match), EM, Accuracy, Rouge-L (F1), BLEU, task-specific F1 for relation/event tasks; frequency of N/A or '-' cases reported where models fail due to format or length limits.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Manual checks performed when models refuse or produce sensitive content; human validators inspect N/A cases to confirm dataset safety and annotate exceptions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Open-ended generation and format non-compliance lead to low EM and many invalid responses; token-based relaxed F1 can be sensitive to tokenization/normalization choices; automated extraction may fail on models that do not follow instruction formatting, which complicates fair scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Not applicable to a specific theory; normalization procedures applied to e.g., KM triple completion and KA multi-hop QA outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Normalization and tailored metric selection made scores more reflective of model capabilities, but some models still produce many invalid outputs producing '-' / 'N/A' entries and affecting standardized score calculations. KoLA documents that handling of missing scores (omission vs zeroing) remains an area for future improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KoLA: Carefully Benchmarking World Knowledge of Large Language Models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Survey of hallucination in natural language generation <em>(Rating: 2)</em></li>
                <li>A note on the evaluation of generative models <em>(Rating: 2)</em></li>
                <li>Language Models as Knowledge Bases? <em>(Rating: 2)</em></li>
                <li>Measuring massive multitask language understanding <em>(Rating: 1)</em></li>
                <li>Gptscore: Evaluate as you desire <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3818",
    "paper_id": "paper-259165244",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "KoLA",
            "name_full": "Knowledge-oriented LLM Assessment benchmark (KoLA)",
            "brief_description": "A benchmark introduced in this paper for carefully evaluating LLM world knowledge across a four-level cognitive taxonomy (Knowledge Memorization, Understanding, Applying, Creating) using both known (Wikipedia) and continuously collected evolving data, plus a contrastive evaluation system and season-based updates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Ability-level decomposition into four cognitive levels (KM, KU, KA, KC); fairness via known vs evolving data; cross-task comparability via standardized scores; focused evaluation of knowledge-creating faithfulness (distinguishing created knowledge vs hallucinations).",
            "evaluation_methods": "Automated metrics per task (EM, relaxed F1/token-match, Rouge, BLEU, accuracy) plus standardized z-score aggregation across models/tasks; a self-contrast method for generation/hallucination evaluation; paired Known vs Evolving comparisons; manual human annotation for validating knowledge-creation metrics.",
            "benchmark_or_dataset": "KoLA benchmark (19 tasks covering memorization, understanding, applying, creating). Data sources: Known = Wikipedia/Wikidata5M (2019 snapshot); Evolving = ~500 recently crawled articles per season (news + fiction) annotated for events and triples; specific tasks include High-Freq/Low-Freq memorization, ETM/ETA/ETC evolving tests, COPEN, FewNERD, DocRED, MAVEN, HotpotQA, 2WikiMulti, MuSiQue, KQA Pro, KoRC, and others.",
            "metrics_reported": "Task-specific absolute metrics (Exact Match (EM), F1, relaxed/token F1, Accuracy, Rouge-L (F1), BLEU); standardized z-scores (per-task z = (x - mu)/sigma) scaled via Min-Max to [0,100] for cross-task comparability; self-contrast similarity (Rouge-L F1 between T and Tk) and composite KC score x = avg(∂(T,R), ∂(T,Tk), ∂(Tk,R)).",
            "human_involvement": "Crowdsourced annotators and internal validators for evolving-data annotation (fact triples, event arguments); human evaluation of KC outputs (4.2K scores collected) with 14 annotators on 1-5 scale for overall quality and faithfulness; multi-round quality checks and validator PhD reviewers; Cohen's Kappa and Fleiss' Kappa reported for annotation reliability.",
            "limitations_or_challenges": "Coverage limited (19 English datasets, focused on concepts/entities/events) and annotation capacity constrains breadth; automatic KC metric may undervalue genuinely novel yet reasonable model-created knowledge that differs from human-provided references; human evaluation is costly and hard to scale; metric sensitivity differences across tasks necessitate standardization; potential test-data leakage and fairness concerns across models; models ignoring provided K in self-contrast could collapse evaluation; context-length/input-size limitations causing some models to be marked N/A.",
            "llm_theory_example": "No explicit 'scientific theory' is given; KoLA's KC examples are narrative event continuations. Paper cites an example of a hallucinated causal claim (death of Simon de Montfort causing rebels' loss) used to illustrate inconsistent/hallucinated relations in generated continuations.",
            "evaluation_results": "KoLA findings: self-contrast metric ∂(T,Tk) correlates with human-judged faithfulness (Spearman 0.61); removing the self-contrast term reduces correlation with human overall quality by ~32%; standardized scoring enables per-task comparability; larger models tend to memorize more knowledge; instruction-tuning/alignment improves higher-level abilities (KA/KC) but may reduce raw memorization (an 'alignment tax'); open-source models generally underperform closed-source commercial models in these knowledge tasks.",
            "uuid": "e3818.0",
            "source_info": {
                "paper_title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Contrastive Evaluation System",
            "name_full": "Contrastive Evaluation System (standardized overall scoring + self-contrast)",
            "brief_description": "A two-part evaluation design in KoLA combining standardized cross-task scoring (z-score + Min-Max scaling) and a self-contrast metric for generation that contrasts free completions with knowledge-grounded completions to assess knowledge-creating fidelity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Cross-task comparability (relative performance among evaluated models) and fidelity of created content (consistency between unconstrained and knowledge-grounded generations and alignment with human reference).",
            "evaluation_methods": "1) Standardized scoring: compute per-task metric x_ij, convert to z_ij = (x_ij - mu_i)/sigma_i, then Min-Max scale z across all tasks/models to [0,100] to produce s_ij; 2) Self-contrast for KC: for same prompt/context C, produce T (free) and Tk (given annotated foreknowledge K), compute similarities ∂(T,R), ∂(T,Tk), ∂(Tk,R) and aggregate by averaging.",
            "benchmark_or_dataset": "Used across KoLA's 19 tasks; self-contrast applied specifically to KC tasks: Encyclopedic Knowledge Creation (Wikipedia/MAVEN-based) and Open Knowledge Creation (Evolving news/fiction ETC).",
            "metrics_reported": "Standardized scores s_ij in [0,100]; component metrics include Rouge-L (F1) for ∂ functions in KC self-contrast; task-native metrics feed into z-score (e.g., F1, EM, Accuracy, BLEU).",
            "human_involvement": "Human-annotated event knowledge K and human references R are required to run self-contrast; human judges used to validate correlation between self-contrast metric and faithfulness/quality.",
            "limitations_or_challenges": "Standardization depends on the evaluated model pool (relative metric); Min-Max scaling sensitive to extreme values; self-contrast requires annotated foreknowledge K and may penalize novel-but-valid model outputs not matching K; evaluation collapse possible when models ignore K in Tk prompt.",
            "llm_theory_example": "No explicit scientific-theory outputs; self-contrast illustrated with narrative continuations where T contains a hallucinated negotiation event while Tk constrained by K does not.",
            "evaluation_results": "Self-contrast component significantly improves alignment with human judgments: ∂(T,Tk) correlated 0.61 with human faithfulness; inclusion of self-contrast raises correlation with human overall quality by ~32% relative to excluding it.",
            "uuid": "e3818.1",
            "source_info": {
                "paper_title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Self-Contrast Metric",
            "name_full": "Self-Contrast Knowledge-Creating Metric",
            "brief_description": "An automatic metric that evaluates knowledge creation by contrasting a model's unconstrained completion (T) with its knowledge-grounded completion (Tk) on the same context and comparing both to the human reference R; focuses on event-level knowledge faithfulness and reduces style-fluency confounds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Faithfulness and reasonableness of created knowledge (consistency between free and grounded outputs and agreement with annotated human event knowledge).",
            "evaluation_methods": "Generate T from context C alone, generate Tk given C+annotated foreknowledge K, compute text similarities ∂(T,R), ∂(T,Tk), ∂(Tk,R) using Rouge-L (F1), then aggregate x = avg(∂(T,R), ∂(T,Tk), ∂(Tk,R)).",
            "benchmark_or_dataset": "Applied to KoLA's Knowledge Creating tasks: Encyclopedic Knowledge Creation (4-1) and Open Knowledge Creating / ETC (4-2) built from MAVEN/Wikipedia and Evolving data with annotated event triples.",
            "metrics_reported": "Rouge-L (F1) values for the three pairwise contrasts and the averaged composite KC score; reported correlation with human judgments (Spearman 0.61 for ∂(T,Tk) vs faithfulness).",
            "human_involvement": "Requires human annotation of event knowledge K in the reference R; human raters assessed outputs to validate metric (1-5 scale for overall quality and faithfulness).",
            "limitations_or_challenges": "Relies on human-provided K so novel valid creations differing from K may be underestimated; Rouge-L similarity captures surface overlap and may miss deeper semantic alignment; requires annotated event structures, which is costly to produce at scale.",
            "llm_theory_example": "Used on narrative continuations; the paper cites an example where a model-generated causal relation (Simon de Montfort's death causing rebels to lose) was inconsistent with R and highlighted by self-contrast comparison.",
            "evaluation_results": "The self-contrast similarity ∂(T,Tk) shows notable correlation (0.61) with human-judged faithfulness; removing ∂(T,Tk) from composite score causes a ~32% drop in correlation with human-judged overall quality, indicating its strong contribution.",
            "uuid": "e3818.2",
            "source_info": {
                "paper_title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Standardized Scoring",
            "name_full": "Standardized Overall Scoring (z-score + Min-Max scaling)",
            "brief_description": "A procedure to make heterogeneous task metrics comparable across tasks and models by converting per-task scores to standardized z-scores relative to evaluated-model distribution and scaling them into a common 0-100 range.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Relative standing of a model on each task compared to the pool of evaluated models; cross-task comparability to produce aggregated level and overall scores.",
            "evaluation_methods": "For each task d_i and model m_j compute x_ij, then z_ij = (x_ij - mu_i)/sigma_i; apply Min-Max scaling over all z_ij to map to [0,100] producing s_ij; average standardized scores across tasks in a level and then across levels for overall ranking.",
            "benchmark_or_dataset": "Used to report KoLA main leaderboard and per-level rankings across all 19 tasks (and paired Known/Evolving task pairs for some standardized computations).",
            "metrics_reported": "Standardized score s_ij in [0,100]; z-scores; per-task selected canonical metrics before standardization (e.g., F1, EM, Accuracy, Rouge, BLEU).",
            "human_involvement": "No direct human involvement needed for scoring computation; human annotation informs some underlying datasets and reference labels that produce x_ij.",
            "limitations_or_challenges": "Scores are relative to the specific set of evaluated models (leaderboard-dependent); Min-Max scaling sensitive to outliers; missing/N/A model-task results handled as exclusions or assigned zero in some aggregates, which can bias rankings.",
            "llm_theory_example": "Not applicable (scoring mechanism rather than example content).",
            "evaluation_results": "Enabled intuitive cross-task comparisons and synthetic overall rankings; helped reveal trends (size correlates with memorization; alignment impacts higher-level abilities); but authors note potential biases from model pool and missing-value handling.",
            "uuid": "e3818.3",
            "source_info": {
                "paper_title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Human Annotation & QC",
            "name_full": "Crowdsourced Annotation and Quality Control for Evolving Data and KC Evaluation",
            "brief_description": "The pipeline and workforce used by KoLA to annotate evolving articles for fact triples and fine-grained event arguments and to human-evaluate knowledge-creation outputs, including multi-round QA and inter-annotator agreement metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Correctness of triples (can the fact be known prior to season?), completeness and correctness of event arguments, and quality/faithfulness ratings of model-generated continuations (1-5 scale).",
            "evaluation_methods": "Pre-annotation with NER and relation extraction models (CLEVE, ATLOP variants), manual annotation by a 21-member team with two annotators per triple/event, two rounds of quality checks, validators resolving disagreements; human evaluation of KC outputs by 14 annotators with PhD validators for quality control.",
            "benchmark_or_dataset": "Annotations produced evolving test sets: 2.7K correct triples (459 novel to the season), event argument annotations for 100 articles used in KC tasks; datasets ETM, ETA, ETU, ETC built from these annotations.",
            "metrics_reported": "Annotation agreement: Cohen's Kappa = 0.71 for triple correctness, Cohen's Kappa = 0.55 for temporal discoverability (b vs c), Fleiss' Kappa = 0.62 for event argument annotation; human KC ratings averaged and reported per model.",
            "human_involvement": "Extensive: 21 annotators for data construction, 14 annotators for KC evaluation, three PhD validators, paid crowdsourced workforce under contracts; annotators permitted web searches; work platform supports re-annotation workflows.",
            "limitations_or_challenges": "Annotation cost constraints limit dataset size and season cadence; disagreements in discoverability annotation (kappa 0.55) show difficulty in provenance judgments; human judgments are time-consuming and costly, limiting scalability of KC reference annotations.",
            "llm_theory_example": "Human raters scored model continuations (1-5) for quality and faithfulness; tables of average human KC scores across models are reported (e.g., GPT-4 and others listed with mean ratings).",
            "evaluation_results": "High first-pass annotation QC rates improved with iterative checks (first-pass QC 67%, second 91%); human KC ratings correlated with automated self-contrast metric, supporting the utility of the automated measure as a proxy for faithfulness.",
            "uuid": "e3818.4",
            "source_info": {
                "paper_title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Answer Normalization & Metrics Selection",
            "name_full": "Task-specific Answer Normalization and Metric Assignment",
            "brief_description": "Procedures KoLA uses to normalize open-ended generative outputs into comparable forms per task and to select the most representative metric for each task (e.g., relaxed F1 token-match, Accuracy, Rouge, BLEU).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Best reflection of model's true performance per task type: exactness for memorization, token coverage for extraction, and similarity for generation.",
            "evaluation_methods": "Tokenize predictions and references with GPT2Tokenizer and compute relaxed/token F1 (token match rate) for many tasks; EM used where applicable; accuracy for classification; Rouge/BLEU for generative tasks; task-adaptive post-processing and format extraction (regex/fuzzy matching) to extract answers from noisy model outputs; models failing to produce valid outputs on &gt;90% cases marked N/A.",
            "benchmark_or_dataset": "Applied across all KoLA tasks; the per-task chosen metric feeds into standardized score computation.",
            "metrics_reported": "Relaxed F1 (token-match), EM, Accuracy, Rouge-L (F1), BLEU, task-specific F1 for relation/event tasks; frequency of N/A or '-' cases reported where models fail due to format or length limits.",
            "human_involvement": "Manual checks performed when models refuse or produce sensitive content; human validators inspect N/A cases to confirm dataset safety and annotate exceptions.",
            "limitations_or_challenges": "Open-ended generation and format non-compliance lead to low EM and many invalid responses; token-based relaxed F1 can be sensitive to tokenization/normalization choices; automated extraction may fail on models that do not follow instruction formatting, which complicates fair scoring.",
            "llm_theory_example": "Not applicable to a specific theory; normalization procedures applied to e.g., KM triple completion and KA multi-hop QA outputs.",
            "evaluation_results": "Normalization and tailored metric selection made scores more reflective of model capabilities, but some models still produce many invalid outputs producing '-' / 'N/A' entries and affecting standardized score calculations. KoLA documents that handling of missing scores (omission vs zeroing) remains an area for future improvement.",
            "uuid": "e3818.5",
            "source_info": {
                "paper_title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Survey of hallucination in natural language generation",
            "rating": 2,
            "sanitized_title": "survey_of_hallucination_in_natural_language_generation"
        },
        {
            "paper_title": "A note on the evaluation of generative models",
            "rating": 2,
            "sanitized_title": "a_note_on_the_evaluation_of_generative_models"
        },
        {
            "paper_title": "Language Models as Knowledge Bases?",
            "rating": 2,
            "sanitized_title": "language_models_as_knowledge_bases"
        },
        {
            "paper_title": "Measuring massive multitask language understanding",
            "rating": 1,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "Gptscore: Evaluate as you desire",
            "rating": 1,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        }
    ],
    "cost": 0.01728275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>KOLA: CAREFULLY BENCHMARKING WORLD KNOWLEDGE OF LARGE LANGUAGE MODELS
1 Jul 2024</p>
<p>Jifan Yu 
Xiaozhi Wang 
Shangqing Tu 
Shulin Cao 
Daniel Zhang-Li 
Xin Lv 
Hao Peng 
Zijun Yao 
Xiaohan Zhang 
Hanming Li 
Chunyang Li 
Zheyuan Zhang 
Yushi Bai 
Yantao Liu 
Amy Xin 
Nianyi Lin 
Kaifeng Yun 
Linlu Gong 
Jianhui Chen 
Zhili Wu 
Yunjia Qi 
Weikai Li 
Yong Guan 
Kaisheng Zeng 
Ji Qi 
Hailong Jin 
Jinxin Liu 
Yu Gu 
Yuan Yao 
Ning Ding 
Lei Hou 
Zhiyuan Liu 
Bin Xu 
Jie Tang 
Juanzi Li 
Contrastive Metrics 
KCCognitive Taxonomy 
Contrastive Generation </p>
<p>Tsinghua University
100084BeijingChina</p>
<p>CALL FOR PART ICIPA TE</p>
<p>Multihop Knowledge Reasoning</p>
<p>1 - 1 1 - 2 1 - 3 2 - 1 2 - 2 2- 3 2-4 2-5 2-6 2-7 2-8 3 - 1 3 - 2 3 - 3 3 - 4 3 - 5 3 - 6 4 - 1 4 - 2</p>
<p>KOLA: CAREFULLY BENCHMARKING WORLD KNOWLEDGE OF LARGE LANGUAGE MODELS
1 Jul 2024FCC74429736984132E8418EB7D1F3242arXiv:2306.09296v3[cs.CL]
The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations.Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations.Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering 19 tasks.(2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge.(3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge-creating ability.We evaluate 28 open-source and commercial LLMs and obtain some intriguing findings.The KoLA dataset will be updated every three months to provide timely references for developing LLMs and knowledge systems.</p>
<p>INTRODUCTION</p>
<p>Recent remarkable breakthroughs achieved by large language models (LLMs) like GPT-4 (OpenAI, 2023) have elicited widespread astonishment.Considering the extensive and profound natural language understanding and generation abilities exhibited by LLMs (Bubeck et al., 2023), the conventional benchmarks (Wang et al., 2018;2019) focusing on relatively narrow and superficial abilities are no longer as helpful for testing them.It has become necessary to construct better benchmarks for effectively comparing LLMs and providing valuable diagnostic results.To this end, various benchmarks are proposed, focusing on extending the evaluation scope to cover broader abilities (Hendrycks et al., 2021;Zhong et al., 2023;Huang et al., 2023) or more challenging tasks (Srivastava et al., 2022;Suzgun et al., 2022).</p>
<p>In addition to broadening the evaluation scope to explore the breadth of LLM abilities, we believe meticulous designs are also necessary to build evaluations that facilitate in-depth insights, maintain impartiality towards different LLMs, and have high applicability for audiences interested in selecting and enhancing LLMs.Designing a benchmark requires careful consideration of three key factors: (1) Ability Modeling.A benchmark should not only define the scope of desired abilities but also model the inherent connections between the evaluated abilities, which allows for diagnostic insights on how to acquire and improve these abilities.(2) Data.Given the extremely broad range of training data for LLMs, which might include annotated data of certain tasks and is often undisclosed, ensuring that differences in training data do not impact the evaluation fairness is critical and challenging.(3) Evaluation Criteria.For high applicability, evaluation metrics should enable audiences to easily understand and gain helpful observations.Moreover, there are many well-known issues (Theis et al., 2016;Sajjadi et al., 2018;Ji et al., 2023) for evaluating tasks with large search spaces like the generative tasks.Evaluations for related abilities still heavily rely on human evaluation, which is time-consuming and not easily reproducible (Belz et al., 2022;2023).</p>
<p>In this paper, we propose a Knowledge-oriented LLM Assessment benchmark (KoLA), which aims at carefully benchmarking the world knowledge of LLMs by undertaking meticulous designs considering the aforementioned three factors:</p>
<p>For ability modeling, we evaluate world knowledge of LLMs and design a four-level cognitive ability taxonomy.We chose world knowledge as our evaluation scope because: (i) World Knowledge is widely recognized as playing a fundamental role in the impressive performance of LLMs (Hendrycks et al., 2021;Zhong et al., 2023;Huang et al., 2023), and a deeper grasp of knowledge enables LLMs to better assist humans; (ii) Recent work has shown that understanding and generating structural world knowledge remain challenging for LLMs.Unlike previous work focusing on expanding the evaluation breadth by covering diver tasks and disciplinary knowledge to test the knowledge boundaries of LLMs (Hendrycks et al., 2021;Zhong et al., 2023;Huang et al., 2023), we focus more on the "depth" of evaluation, i.e., modeling the intrinsic connections between knowledge-related abilities and ensuring reliable evaluation results.Inspired by the human cognitive processes in learning theory, such as Bloom's taxonomy (Krathwohl, 2002), we organize evaluated abilities into four levels: Knowledge Memorization, Knowledge Understanding, Knowledge Applying, and Knowledge Creating.This taxonomy helps to provide more specific and helpful evaluation results, detailing which aspect of knowledge the evaluated models may be deficient in.It also facilitates a preliminary exploration of the similarities and differences between the learning mechanisms of LLMs and humans.To coordinate with our data design considerations introduced later, we selected 19 tasks, primarily focusing on world knowledge about entities, concepts, and events.</p>
<p>For data, we obtain both known and evolving data sources.Some studies adopt unpublished or machine-unreadable data (Zhong et al., 2023;Huang et al., 2023) to reduce the possibility that the test data has been learned by LLMs.However, considering the intense competition between LLMs, those data may also be trained by LLMs in the near future1 .We believe the ideal approach is to do evaluations on newly emerging data and maintain a continuously evolving benchmark, like the attempts that include time-sensitive evolving data (Kasai et al., 2022;Dhingra et al., 2022).In KoLA, we host a new competition season every three months.For each season, we crawl and annotate 500 recently published articles as the evolving data.The evolving data source allows us to (i) evaluate models more fairly, even if some models can rapidly update their knowledge, thereby demonstrating their power, and (ii) better track the model development.Besides evolving data, we also consider the known data of LLMs, which means the data sources that all models have learned.Evaluations on known data enable us to (i) fairly compare the learning efficiency of LLMs by comparing the different knowledge they acquire from the same training data and (ii) assess the generalization ability by comparing LLMs' performance on known data and evolving data.We chose Wikipedia as our known data source due to its common use.Considering the limitations of Wikipedia and our annotation capabilities on the evolving data, we are unable to cover a very wide range of tasks.</p>
<p>For evaluation criteria, we design a contrastive evaluation system, including an overall standard score system and a self-contrast knowledge creating metric.Conventional benchmarks report absolute metrics for different tasks separately.The incomparability of scores across tasks makes it difficult for audiences to intuitively compare the proficiency levels across different abilities.Additionally, the sensitivity of different metrics varies, which may lead less experienced audiences to misinterpret the ability differences represented by numerical differences.In the KoLA main leaderboard, we report standard scores across different tasks, determined by the relative level compared to other evaluated LLMs.This makes KoLA applicable to a broader range of audiences.Experienced audiences can still refer to absolute metrics if desired.Furthermore, evaluating knowledge creation is particularly challenging as it involves distinguishing the correctly created knowledge and knowledge hallucinations (Ji et al., 2023).We design a self-contrast metric for evaluating knowledge hallucination by contrasting freely created completions and knowledge-grounded completions of an LLM given the same beginnings.This metric eliminates the influence of writing styles and focuses on whether the generated completions are consistent with the actually presented knowledge.</p>
<p>In the first two seasons of KoLA, we evaluate 28 widely-used LLMs, including 8 API-access commercial LLMs, such as GPT-4 (OpenAI, 2023) and and 20 open-source LLMs including GLM-130B (Zeng et al., 2022), LLaMa (Touvron et al., 2023), etc. From the experimental results, we obtain some intriguing observations, such as larger base models tend to memorize more knowledge, alignment unleashes the potential of larger models in higher-level abilities but may harm memorization, and open-source models exhibit overall inferiority compared to commercial models.</p>
<p>We welcome the participation of more LLMs in KoLA evaluation and encourage contributions to the new seasons of KoLA.The data, leaderboard, participation information, and supporting tools are publicly available upon acceptance.We hope KoLA can serve as a diagnostic tool to facilitate the development of increasingly knowledgeable LLMs, and also help practitioners select LLMs.</p>
<p>KOLA BENCHMARK</p>
<p>ABILITY MODELING</p>
<p>Within the context of Artificial Intelligence (AI), Knowledge has long been employed to signify information encompassing facts, events, and skills (Feigenbaum, 1977), serving as an indicator for the intelligence level of AI.Hence various knowledge-intensive tasks (Petroni et al., 2019;2021) are proposed to examine language models' knowledge-related abilities.Recently, the impressive performance of LLMs has encouraged the development of more comprehensive benchmarks (Srivastava et al., 2022;Suzgun et al., 2022) with broad human-subject exams (Hendrycks et al., 2021;Zhong et al., 2023;Huang et al., 2023).</p>
<p>Cognitive Ability Taxonomy.Confronted with such a vast array of evaluation datasets, we advocate for considering the stratification and connection of abilities, rather than organizing them discretely (Wang et al., 2019;Hendrycks et al., 2021;Srivastava et al., 2022;Suzgun et al., 2022) or straightforwardly based on disciplines (Zhong et al., 2023) or difficulties (Huang et al., 2023).Such viewpoints have also been upheld by cognitive scientists for several decades, giving rise to a series of cognitive learning theories (Lewis &amp; Smith, 1993).Considering the ongoing debates surrounding high-order thinking (Miri et al., 2007;Collins, 2014), we simplify and select four widely accepted processes in Bloom's taxonomy (Krathwohl, 2002) for organizing the tasks in KoLA benchmark.</p>
<ol>
<li>
<p>Knowledge Memorization (KM) aims to gauge the model's ability in faithfully recalling known facts, exemplified by the previous knowledge probing task (Petroni et al., 2019).</p>
</li>
<li>
<p>Knowledge Understanding (KU) focuses on evaluating the model's ability in understanding the underlying knowledge within texts, instantiated by the conventional information extraction tasks (Yao et al., 2019;Wang et al., 2020;Ding et al., 2021;Peng et al., 2022).</p>
</li>
<li>
<p>Knowledge Applying (KA) reflects the ability of agents in employing knowledge to accomplish reasoning and problem-solving tasks.Consequently, this level is evaluated by various knowledge reasoning tasks (Yang et al., 2018;Trivedi et al., 2022;Cao et al., 2022).</p>
</li>
<li>
<p>Knowledge Creating (KC) denotes the ability to create novel and reasonable knowledge given known facts.This is evaluated by the knowledge coherence and correctness (Chen et al., 2020;Bang et al., 2023) of contents generated by the model.It is worth noting that the evaluation goes beyond merely assessing the generation quality (fluency, etc.).</p>
</li>
</ol>
<p>DATA SOURCE AND SELECTED TASKS</p>
<p>Known &amp; Evolving Data: A common concern in evaluating LLMs is the fairness issue brought by variations in training data and the potential test data leakage risk.To minimize these biases, we propose the design of the following distinctive data sources:</p>
<p>(1) Known Data Source.Wikipedia2 is an acknowledged high-quality corpus containing over 6.6 million English articles, which has been used in pre-training by numerous pre-trained models since BERT (Devlin et al., 2019;Brown et al., 2020;Shuster et al., 2022) and is widely included in open pre-training corpora (Gao et al., 2021).Hence we believe assuming every LLM has been trained on Wikipedia is reasonable and adopt it as our known data source.Considering that many LLMs state they can only provide answers based on "Content before 2021"3 , we select Wikidata5M (Wang et al., 2021a), a high-quality subset of Wikidata, as the basis, which allows linking to the 2019 version of Wikipedia dump, thus enabling the selection or reconstruction of downstream tasks' datasets.</p>
<p>(2) Evolving Data Source.Considering the time required for model training (Zeng et al., 2022), it is less unlikely for newly emerged data to be timely trained by LLMs.Therefore, we have devised an evolving evaluation mechanism that continuously retrieves the web content published in around recent 90 days as the data source and constructs new datasets on them.This approach ensures fair assessment of LLMs' performance on unseen content and whether they "secretly" involve knowledge updating modules like the external search.Each update (we call it a Season of KoLA) requires crawling a minimum of 500 articles to support building test sets.For the first season reported in this paper, we adopt two kinds of data: factual news4 and fictional novels5 .We intend to persist for an additional 4 seasons (approximately 1 year) to promptly integrate the forthcoming top LLMs.We anticipate that the consistently released reports can further support relevant researchers.</p>
<p>Built upon these two data sources, we finally select and construct 19 tasks in KoLA, as shown in Table 1.To ensure both the quality and efficiency of annotations for each season, we randomly select one task at each level to annotate the new evolving evaluation dataset.For the existing datasets, we try to ensure most of the test sets are not public, and this rigorous setting ensures a high level of fairness.</p>
<p>The data collection and task construction details are shown in Appendix C. We briefly introduce the tasks of the four levels below.It is noteworthy that, due to the limitations of data distribution and collection processes, the absolute numerical values of the model on Evolving data are not necessarily destined to be lower than those on Known data.</p>
<p>Knowledge Memorization Tasks: We follow LAMA (Petroni et al., 2019) to evaluate knowledge memorization by probing facts from LLMs but re-construct the datasets on our data sources.Given a triplet in Wikidata5M (Wang et al., 2021a), we transform it into a sentence with a relationspecific template and let LLMs complete its tail entity.Additionally, we want to explore whether the knowledge memorization of LLMs correlates with training frequency.We sort the entities in Wikidata5M according to their frequency of occurrence in Wikipedia (Jin et al., 2019), resulting in the creation of two test sets: (1-1) High-Frequency Knowledge.Randomly selecting 100 entities from the top 2, 000 entities with the highest frequency and construct data with triplets of them; (1-2) Low-Frequency Knowledge.Similarly, we randomly select 100 entities from the lowest-frequency entities and construct a more challenging evaluation set; (1-3) Evolving Test of Memorization (ETM).</p>
<p>From the articles in evolving data sources, we annotate the knowledge triplets shown in them and only preserve 100 triplets that cannot be inferred from previously available corpora.</p>
<p>Knowledge Understanding Tasks: Knowledge understanding is evaluated by whether LLMs can understand various genres of knowledge from texts, including concepts, entities, entity relations, events, and event relations.(2-1/2-2/2-3) Concept Probing employs the three probing tasks (CSJ, CPJ, CiC) of COPEN (Peng et al., 2022) to evaluate the models' understanding of conceptual knowledge.</p>
<p>(2-4) Named Entity Recognition utilizes the FewNERD dataset (Ding et al., 2021), from which we Knowledge Applying Tasks: Knowledge applying ability is evaluated by LLMs' multi-hop reasoning capabilities, specifically over world knowledge.This differs from several recent studies (Lu et al., 2023;Mialon et al., 2023), which cover more general reasoning, such as mathematical reasoning.Therefore, the following progressive Wikipedia-based datasets are included in KoLA: (3-1) Hot-potQA (Yang et al., 2018) is a question-answering dataset that involves a substantial number of natural language questions written by native speakers, examining machine's abilities in comparison, multi-hop reasoning, and more.However, a limitation of HotpotQA is that some questions can be answered through shortcuts.To address this, (3-2) 2WikiMultihopQA (Ho et al., 2020) ensures that questions cannot be solved through shortcuts by manually-designed templates, but their questions lack naturalness in language.Furthermore, the (3-3) MuSiQue (Trivedi et al., 2022) dataset tackles the challenges of shortcuts and naturalness simultaneously.Its questions are composed of simple questions from existing datasets, with up to four-hop complex reasoning.(3-4) KQA Pro (Cao et al., 2022) is a large-scale dataset, whose questions are relatively complex, allowing for more fine-grained evaluation of LLMs' multi-hop reasoning with logical operations and modifiers.(3-5) KoRC (Yao et al., 2023) is a dataset that requires joint reasoning between the text and knowledge base.It differs from the aforementioned four datasets as it requires implicit rather than explicit reasoning.</p>
<p>(3-6) Evolving Test of Applying (ETA) takes the same construction approach as KoRC, producing 49 questions upon 350 annotated knowledge triplets and 40 articles in the evolving data.</p>
<p>Knowledge Creating Tasks: As the highest level of Bloom's Cognitive Taxonomy (Krathwohl, 2002), how to evaluate knowledge creation is a long-standing open and challenging question.The capacity for knowledge creation is evident in open-ended generation tasks.Traditional text generation evaluation metrics (Theis et al., 2016) are based on textual similarities between model-generated content and human-written references, which do not solely focus on knowledge creation ability but cover other skills, such as text style and fluency (Naeem et al., 2020).Ideally, human evaluators should be employed to solely assess whether the content generated by models contains novel and reasonable knowledge (Maher, 2010;Lamb et al., 2018).However, manually evaluating diverse open-domain knowledge is labor-intensive, costly, and lacks scalability.Inspired by the knowledge-grounded text generation tasks (Yu et al., 2022), KoLA proposes a feasible automatic evaluation protocol that specifically contrasts the model-generated knowledge with that in human references.First, we limit the generation scope to narrative texts such as history, news, and fiction.This is because the knowledge creating in generating narrative texts has a clear focus on envisioning plausible subsequent events and articulating them in a reasonable way.As shown in Figure 2, we then conduct human annotation on the reference texts to obtain reference fine-grained event knowledge.The annotated events enable a dedicated self-contrast metric (elaborated below) that emphasizes the quality of event knowledge in generated content.This approach offers an effective assessment of knowledge creation abilities compared to traditional text generation metrics encompassing many other factors (Akter et al., 2022).We conduct annotation on both Wikipedia texts and evolving articles, which constructs two evaluation datasets: (4-1) Encyclopedic Knowledge Creation, which is based on the narrative Wikipedia articles selected by MAVEN (Wang et al., 2020) and (4-2) Open Knowledge Creation, which is based on unseen news and novels, serving as the Evolving Test of Creating (ETC).</p>
<p>Table 1 presents the features and statistics of each selected task.Further details regarding annotation processes and task demonstrations are correspondingly presented in Appendix D.</p>
<p>CONTRASTIVE EVALUATION SYSTEM</p>
<p>Our contrastive evaluation system includes standardized overall scores based on relative model comparisons and a unique self-contrast metric, which can automatically evaluate knowledge hallucination and enhance generation evaluation.</p>
<p>Standardized Overall Scoring.Since the metrics of different KoLA tasks are incomparable and differently sensitive, less experienced audiences cannot easily compare and interpret results, which is also prevalent in recent LLM benchmarks like Big-Bench-Hard (Suzgun et al., 2022) and MMLU (Hendrycks et al., 2021).Therefore, we propose to introduce standardized scores (Dyck et al., 2005) to enhance the applicability of KoLA results.Specifically, given a task set D = {d i }</p>
<p>|D|</p>
<p>i=1 and the evaluated model set M = {m j } |M | j=1 , we first select the most representative metric for each task, allowing us to compute the performance score x ij of model m j on task d i .Then the standardized score z can be calculated as:
zij = xij − µ xi1, ..., x i|M | σ xi1, ..., x i|M | ,(1)
where µ (•) and σ (•) denote the mean and standard deviation.Subsequently, we apply Min-Max scaling (Patro &amp; Sahu, 2015) to adjust all the results to the range of [0, 100], further enhancing the correlation and readability of scores across tasks.The final scores are presented as:
sij = 100 zij − min (z) max (z) − min (z) ,(2)
where the functions max (z) and min (z) correspond to the maximum and minimum of all z ij scores.</p>
<p>Self-contrast Metric.Evaluating knowledge creating is not only about evaluating generation quality (Theis et al., 2016), but more about assessing whether the generated knowledge is faithful and reasonable, i.e., avoiding knowledge hallucination (Ji et al., 2023).We develop a unique self-contrast metric for this, which is defined by contrasting two completions generated by the same model.</p>
<p>As illustrated in Figure 2, C denotes the given preceding context, R denotes the human-written succeeding completion, and K refers to the annotated event knowledge in R. Each model is required to generate two completions: (a) Given only context C, generate a version of completion T , which requires the model to freely imagine the possible events and may have knowledge hallucination like the negotiation event in Figure 2; (b) Given both the context C and the foreknowledge K, generate another completion T k , which only requires the model to reasonably compose the given events.If T and T k demonstrate a strong resemblance, it implies that the model can create highly reasonable events that are consistent with human-provided references and has less knowledge hallucination.The distinct advantage of this self-contrast method is that, since both completions are generated by the same model, factors external to knowledge creation, such as writing style, are highly likely to remain consistent, minimizing their influence on the evaluation.Moreover, to cover more comprehensive aspects of knowledge creation abilities and prevent evaluation collapse caused by the model's disregard for the knowledge K in the prompt of the process (b), the overall knowledge creating score is defined as the mixture of multiple contrasts:
x = avg (∂ (T, R) , ∂ (T, T k ) , ∂ (T k , R)) ,(3)
where avg (•) denotes the average.Function ∂ (•) is to calculate the similarity of the two texts, which we employ the widely-used Rouge-L (F1) (Lin, 2004) in this work.The ∂ (T, R) is the conventional text generation metric.While it captures a broad range of knowledge creation abilities (spanning multiple genres of knowledge beyond events), it also includes undesired factors unrelated to knowledge creation, such as writing styles and text fluency.Hence we add ∂ (T, T k ) and ∂ (T k , R) to emphasize the abilities about creating event-related knowledge, which is important for generating narrative texts.The ∂ (T, T k ) is the newly-proposed self-contrast metric focusing on whether the generated event knowledge is reasonable.The ∂ (T k , R) takes inspiration from the knowledgegrounded generation tasks (Chen et al., 2020;Ghazvininejad et al., 2018).It reflects the ability to create knowledge about the relationships between events, which is required in reasonably composing the given events into a story.For instance, the T k in Figure 2 implies that the death of Simon de Montfort caused the rebels to lose the battle, while this is a hallucinated causal relation inconsistent with the narrative in R.</p>
<p>EXPERIMENT</p>
<p>Evaluated Models.In the first two seasons of KoLA, we evaluate LLMs of two categories: (1) Opensource Model, including GPT-J (6B) (Wang &amp; Komatsuzaki, 2021), GPT-JT (6B) (Computer, 2022), GPT-NeoX (20B) (Black et al., 2022), BLOOM (7B) (Scao et al., 2022), T0++ (11B) (Bang et al., 2023), LLaMa (65B) (Touvron et al., 2023), GLM (130B) (Zeng et al., 2022), UL2 (20B) (Tay et al., 2022), FLAN-T5 (11B) (Chung et al., 2022), FLAN-UL2 (20B) (Research, 2022), Alpaca (7B) (Taori et al., 2023), ChatGLM (6B) (Du et al., 2022), Dolly-v2 (12B) (Conover et al., 2023),RedPajama-Instruct (7B) (Computer, 2023), Tulu (7B) (Wang et al., 2023), Vicuna (13B) (Zheng et al., 2023), Llama2-chat (7B) (Touvron et al., 2023), ChatGLM2-32k (6B) (Zeng et al., 2022), Internlm-chat-8k (7B) (Team, 2023); (2) API service: GPT-3 curie v1 (6.7B)6 and davinci v1 (175B) (Brown et al., 2020), InstructGPT curie v1 (6.7B<em>) 6 and davinci v2 (175B</em>) (Ouyang et al., 2022), ChatGLM (130B) (Zeng et al., 2022), Cohere-command (52.4B)7 , J2-Jumbo-Instruct (178B<em>) (Studio, 2023), GPT3.5-turbo 6 and GPT-4 (OpenAI, 2023).(</em>) indicates the size has not been confirmed.</p>
<p>Overall Performance.We report the standardized scores of all models in Table 2 and 3, where "-" indicates that the result is unavailable due to the input is longer than the model context length.All results are from Season 2nd (Sept.2023), and the comparison with the rankings from Season 1st (June 2023, Appendix F) is shown in the "Rank" column.Despite the overall consistency in rankings across different levels, we can still obtain some intriguing findings from the results:</p>
<p>(1) For models without alignment or instruction tuning (e.g., GPT-J and BLOOM), there is a strong correlation (Spearman's coefficient of 0.79) between the ranking of the Knowledge Memory (KM) and the model size.This suggests that model size has an obvious positive impact on memorizing seen knowledge, which corroborates some of the viewpoints from previous studies (Liang et al., 2022).(2) For models after instruction tuning, there is a significant increase in the correlations between higher-level abilities and model size (exemplified by KA, whose Spearman's coefficient 0.02 to 0.53).This suggests that alignment unleashes the greater potential of LLMs in higher-level capabilities.However, the correlation between size and low-level KM performance exhibits a decline (0.34), potentially demonstrating the widely discussed "alignment tax" (Ouyang et al., 2022).</p>
<p>(3) Compared to the commercial closed-source models like GPT4 and GPT-3.5-turbo,there is still a noticeable gap in the performance of open-source models.Open-source models obtain an average z-score of −0.29, which is below the overall average.Design Analysis.We further discuss several new observations brought by KoLA design factors.</p>
<p>1 0.82 0.79 0.81 0.94 0.86 -0.02 0.38 -0.18 0.28 0.39 0.63 0.74 0.72 0.54 0.59 0.35 0.04 0.12 0.82 1 0.84 0.87 0.77 0.78 -0.03 0.27 0.03 0.42 0.32 0.62 0.65 0.65 0.38 0.62 0.27 0.03 0.1 0.79 0.84 1 0.89 0.8 0.84 -0.04 0.27 0.1 0.43 0.43 0.71 0.66 0.7 0.52 0.5 0.19 0.17 0.29 0.81 0.87 0.89 1 0.82 0.81 -0.06 0.27 -0.05 0.41 0.43 0.68 0.66 0.69 0.49 0.64 0.42 0.15 0.32 0.94 0.77 0.8 0.82 1 0.91 0.06 0.39 -0.02 0.3 0.36 0.59 0.7 0.68 0.54 0.65 0.37 0.11 0.21 0.86 0.78 0.84 0.81 0.91 1 0 0.36 0.11 0.25 0.39 0.67 0.78 0.69 0.62 0.58 0.32 0.13 0.22 -0.02-0.03-0.04-0.060.06 0 1 0.74 0.5 0.44 0.49 -0.04-0.030.19 -0.06 0.35 0.37 0.61 0.55 0.38 0.27 0.27 0.27 0.39 0.36 0.74 1 0.33 0.54 0.63 0.27 0.25 0.46 0.14 0.53 0.53 0.53 0.62 -0.18 0.03 0.1 -0.05-0.020.11 0.5 0.33 1 0.26 0.18 0.17 0.06 0.17 0.02 0.05 -0.01 0.58 0.5 0.28 0.42 0.43 0.41 0.3 0.25 0.44 0.54 0.26 1 0.37 0.22 0.12 0.43 -0.06 0.2 0.26 0.34 0.59 0.39 0.32 0.43 0.43 0.36 0.39 0.49 0.63 0.18 0.37 1 0.35 0.39 0.48 0.21 0.4 0.37 0.57 0.63 0.63 0.62 0.71 0.68 0.59 0.67 -0.04 0.27 0.17 0.22 0.35 1 0.79 0.77 0.82 0.27 0.13 0.21 0.22 0.74 0.65 0.66 0.66 0.7 0.78 -0.03 0.25 0.06 0.12 0.39 0.79 1 0.82 0.81 0.34 0.23 0.15 0.18 0.72 0.65 0.7 0.69 0.68 0.69 0.19 0.46 0.17 0.43 0.48 0.77 0.82 1 0.7 0.37 0.15 0.43 0.37 0.54 0.38 0.52 0.49 0.54 0.62 -0.06 0.14 0.02 -0.06 0.21 0.82 0.81 0.7 1 0.17 0.1 0.15 0.07 0.59 0.62 0.5 0.64 0.65 0.58 0.35 0.53 0.05 0.2 0.4 0.27 0.34 0.37 0.17 1 0.64 0.2 0.21 0.35 0.27 0.19 0.42 0.37 0.32 0.37 0.53 -0.01 0.26 0.37 0.13 0.23 0.15 0.1 0.64 1 0.17 0.44 0.04 0.03 0.17 0.15 0.11 0.13 0.61 0.53 0.58 0.34 0.57 0.21 0.15 0.43 0.15 0.2 0.17 1 0.79 0.12 0.1 0.29 0.32 0.21 0.22 0.55 0.62 0.5 0.59 0.63 0.22 0.18 0.37 0.07 0.21 0.44 0.79 1 1.00  First, there is a high correlation among tasks within each level, indicating that the abilities of LLM indeed possess some inherent hierarchical structure.The knowledge memorization (KM) level shows notable correlations with other levels, especially with the concept tasks in the understanding level (2-1, 2-2, 2-3), as well as with the reasoning tasks (from 3-1 to 3-5) in the applying level, which indicates that these high-level tasks rely heavily on knowledge memory.Moreover, in order to obtain a more dissociated assessment of the LLMs' competence in higher-order cognitive tasks, it is still recommended to design tasks that exhibit substantial disparities from the pre-training corpus to alleviate the potential biases stemming from data.</p>
<p>Second, the results of the models on evolving and non-evolving tasks show an obvious linear correlation, indicating the reliability of our construction of evolving datasets.The performance gap between known and evolving data is more prominent for shallower levels (KM, KU), whereas it is less pronounced in higher-level tasks (KA, KC).The convergence of performance between Independent-Identical-Distribution and Out-of-Distribution evolving settings suggests a potential enhancement in the model's generalization capability and may support the opinion about the model's acquisition of divergent and reasoning abilities that go beyond simple data fitting (Bubeck et al., 2023;Zhong et al., 2023).</p>
<p>Third, we conduct manual annotation (Appendix F.1 for more details about annotation settings and results) on the results in the knowledge creating tasks, where each annotator is required to read the contexts C and foreknowledge K, and then evaluate the model's outputs T in two aspects: overall quality and faithfulness.Ratings are assigned on a scale of 1 (the worst rating) to 5 (the best rating).We calculate Spearman's correlation between the manually annotated results and the metrics introduced in § 2.3.We find that there is a notable correlation (0.61) observed between the self-contrast metric ∂ (T, T k ) and the faithfulness of created content, while removing the self-contrast metric from the overall metric x in Eq. ( 3) brings a significant 32% decrease in the correlation with the human-judged overall quality.We believe this metric can contribute to future explorations on the assessment of generation abilities (Theis et al., 2016;Pillutla et al., 2021;Fu et al., 2023).</p>
<p>ACKNOWLEDGEMENT</p>
<p>This research project is supported by a grant from the Institute for Guo Qiang, Tsinghua University (2019GQB0003).Jie Tang is supported by Technology and Innovation Major Project of the Ministry of Science and Technology of China under Grant 2022ZD0118600, NSFC for Distinguished Young Scholar 61825602 and the New Cornerstone Science Foundation through the XPLORER PRIZE.This project also appreciate the providing of the test set of MusiQue (Trivedi et al., 2022) by Harsh Trivedi, Stony Brook University and the test set of 2Multiwikihop (Ho et al., 2020) by Xanh Ho, National Institute of Informatics, Tokyo, Japan.</p>
<p>ETHICS STATEMENT</p>
<p>In this section, we discuss the ethical considerations regarding our data construction and leave the broader impact to Appendix A.2. (1) Data Risk Control.Regarding the collected evolving data source, we have filtered out content that is inappropriate for presentation to a general audience, and the relevant details are outlined in Appendix C.1.Seven of the authors manual check all the newly constructed evolving test datasets as well as random samples of all the previously released datasets included in KoLA.No instances of personally identifiable information, discriminatory content, explicit, violence, or offensive content were found.(2) Annotator Treatment and Consent.We hire crowdsourced annotators in the annotation of evolving test data and the human evaluation for knowledge creating.The details are introduced in Appendix C.2.We have signed work contracts with all the annotators and provided compensation according to mutually agreed-upon wage standards and working hours.All employment arrangements are in compliance with local regulations.(3) Copyright.Our known data source is Wikipedia, which is licensed under CC BY-SA 3.0 8 and allows for free research use.For all the previously released datasets included in KoLA.Our evolving data source contains public news and fictions.The news data is from The Guardian 9 and we access it strictly following the terms and conditions 10 .The fiction data is from Archive of Our Own (AO3) 11 , a fan-fiction archive site.Although AO3 data has been used in some previous works Cao &amp; Daumé (2020); Yoder et al. (2021); Sun et al. (2022); Krishna et al. (2022), there remains some ambiguity regarding its copyright status.We believe our use of AO3 is appropriate because: (i) AO3 exhibits an open attitude towards data crawling 12 .(ii) We pledge that KoLA will always remain non-commercial and non-profit, and we do not redistribute the crawled data (only samples are provided in our platform).According to the description 13 provided by the Organization for Transformative Works, the operator of AO3, such usage falls under fair use in the context of the U.S. copyright law.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>To promote reproducibility, we provide details about our data collection in Appendix C, all the used task instructions in Appendix D, and experimental details in Appendix E. The evaluation source codes and data samples for all the tasks are submitted as supplementary material.The results of future seasons will be presented at the Github and our platform website.</p>
<p>APPENDICES A BROADER DISCUSSION</p>
<p>A.1 LIMITATION The major limitation of KoLA is that our coverage is not as extensive as some other recent works (Hendrycks et al., 2021;Zhong et al., 2023;Huang et al., 2023).KoLA evaluates LLMs' world knowledge about concepts, entities, and events and only covers 19 English datasets now.While there is no doubt that expanding the evaluation "breadth" to test the boundaries of LLM abilities is valuable, our emphasis lies more on the "depth" of evaluation.Due to careful design considerations regarding data sources and our annotation capacity to host a new competition season every 90 days, it is not easy to significantly broaden our evaluation coverage.The first season results reported in this paper involve 21 LLMs.Although we strive to cover diverse and representative LLMs, it is challenging to cover the ever-emerging and evolving LLMs with solely our own efforts.We sincerely welcome community contributions and participations to introduce new tasks or LLMs.</p>
<p>Another limitation pertains to our evaluation of knowledge creating abilities.To avoid human evaluations, as detailed in Section 2.2 and Section 2.3, we design an automatic evaluation method based on the self-contrast metric.Experiments in Section 3 validate the efficacy of our metric.However, our automatic evaluation still relies on contrasting model-generated content with humanprovided knowledge.If a model produces knowledge that is novel and reasonable but just not aligned with human-provided knowledge, its capabilities might be underestimated.We encourage future work to actively explore this significant avenue and endeavor to develop more effective automatic evaluation methods.</p>
<p>A.2 POTENTIAL IMPACT</p>
<p>As a benchmark, the intended use of KoLA is not to construct applications or train LLMs, but rather to evaluate the foundational abilities about world knowledge of LLMs.Our evaluation tasks do not involve speculating personal sensitive information, making judgments on social issues, or interacting with the real world.Therefore, we believe the likelihood of our benchmark directly leading to negative impacts on safety, security, discrimination, surveillance, deception &amp; harassment, human rights, bias and fairness is very low.However, effective benchmarks will facilitate the development of powerful LLMs, which poses a wide and serious risk of misuse.Although beyond the scope of this paper, we earnestly call for strengthened cooperation from various sectors of society in enhancing the regulation and safety control of LLMs.</p>
<p>Our evaluation needs to do inference with many LLMs on various datasets, which naturally results in carbon emissions and potential environmental issues.The total carbon emissions can be estimated based on the data provided in Appendix E.1.As our evaluation does not involve the pre-training and fine-tuning of LLMs, we believe the impact caused is relatively marginal and controllable.In the participation guidelines on the platform, we also state that we discourage improving evaluation scores through repeated submissions or training specifically on benchmark-related data.This ensures the reliability of the evaluation results while minimizing carbon emissions as much as possible.</p>
<p>B AUTHOR CONTRIBUTION</p>
<p>Data Collection.Xiaozhi Wang, Shulin Cao, Xin Lv, Hao Peng, Zijun Yao collected the Known Data Source from the open-source projects, private academic datasets and Wikidata.Jifan Yu, Daniel Zhang-Li, Nianyi Lin, Linlu Gong and Kaifeng Yun collected and pre-processed the first season's Evovling Data Source.Hailong Jin supported the data from Xlore (Jin et al., 2019).Yuan Yao provided the test set of DocRED (Yao et al., 2019) and Ning Ding helped the construction of FewNERD (Ding et al., 2021) test set.</p>
<p>Data Annotation.Xiaozhi Wang organized the crowdsourcing annotation of fine-grained event arguments for knowledge creating (KC) tasks.Kaisheng Zeng and Yong Guan assisted in the quality control process.Jifan Yu and Daniel Zhang-Li organized the annotation of knowledge triples from the evolving articles, as well as the human evaluation of creating results.</p>
<p>Task Construction.Xin Lv designed the dataset and instruction of the knowledge memorization (KM) tasks, (1-1) and (1-2).Hao Peng organized the knowledge understanding (KU) tasks and constructed instructions for (2-1/2-2/2-3).Zhili Wu, Yunjia Qi, Jianhui Chen and Weikai Li correspondingly constructed instructions for task (2-4) to (2-7).Shulin Cao and Zijun Yao organized the knowledge applying (KA) tasks and constructed instructions for (3-1), (3-3), (3-4) and (3-5).Yantao Liu and Amy Xin constructed task (3-2) and (3-6).Nianyi Lin and Daniel Zhang-Li organized the knowledge creating (KC) tasks (4-1), (4-2) and most of the evolving tasks.Kaifeng Yun and Linlu Gong constructed instructions for task (1-3) and (2-8).</p>
<p>Model Evaluation.Shangqing Tu organized the whole process of model evaluation.Hanming Li deployed and conduct experiments of GPT-J (6B), GPT-JT (6B), GPT-NeoX (20B), BLOOM (7B), T0++ (11B), LLaMa (65B), GLM (130B), UL2 (20B), FLAN-T5 (11B), Alpaca (7B), FLAN-UL2 (20B), ChatGLM (6B).Chunyang Li evaluated GPT-3 curie v1 (6.7B) and davinci v1 (175B), InstructGPT curie v1 (6.7B<em>) and davinci v2 (175B</em>), ChatGLM (130B), and GPT3.5-turbo.Zheyuan Zhang evaluated Cohere-command (52.4B) and J2-Jumbo-Instrcut (178B*), while Yushi Bai evaluated GPT-4.Ji Qi, Daniel Zhang-Li and Jinxin Liu assisted the data analysis and presentation on the inference results.Yu Gu supported some of the candidate APIs.</p>
<p>Platform Development.Xiaohan Zhang's team organized the overall platform development.Jifan Yu and Daniel Zhang-Li respectively contributed to the visualization design and backend development.</p>
<p>Paper Writing.Jifan Yu and Xiaozhi Wang hosted the paper writting.Shangqing Tu, Ji Qi, Daniel Zhang-Li supported the part of experimental analysis.All the authors contributed to their corresponding working details for completing this paper (including Appendix).</p>
<p>Advising.Lei Hou, Zhiyuan Liu, Bin Xu, Jie Tang and Juanzi Li take advisor roles in this project.Juanzi Li is the main advisor, initializing, supporting and organizing this project.This project also express appreciations to other supporters.Special thanks go to Gang Wang for his outstanding product prototype design.The artistic design, including the beautiful illustrations for the paper, was skillfully provided by Shanshan Wang.The platform development and feature implementation were carried out by Zhenfang Lu, Shuai Xie, Shuaiming Wang, Liangliang Cui and Dingxiao Liu.The coordination of crowdsourcing affairs was effectively managed by Jupeng Zhang and Yue Yang.Finally, the long-term assistance and support from Yini Chen to the AIGC research team are greatly acknowledged.</p>
<p>C DETAILS OF DATA COLLECTION</p>
<p>There are two data sources used in KoLA: Known and Evolving, which are correspondingly from Wikipedia and newly crawled corpus.In this section, erview of the collection and maintenance of them, as well as the annotation process involved, including essential statistical information.</p>
<p>C.1 RAW DATA COLLECTION</p>
<p>Known.We collect the corresponding Wikipedia articles for the entities in Wikidata5M (Wang et al., 2021a) from Xlore2 (Jin et al., 2019), a cross-lingual knowledge base in Chinese and English, using its 2019 version, and align them accordingly.This process generates a dataset of 5 million articles.Given that language model training is reliant on textual data, we depart from the conventional graph-based methods typically employed in knowledge graphs to calculate entity frequencies.Instead, we conduct statistical analysis to determine and rank the occurrence frequencies of these entities and their aliases within the text corpus.Subsequently, we establish two sets of high-frequency and low-frequency entities, each containing 2, 000 entities, to fulfill the requirements of the knowledge memorization task 1-1 and 1-2.</p>
<p>Evolving.The data collection process for the Evolving dataset in the first season of KoLA concludes on April 15, 2023.Therefore, we are collecting data from the preceding 90 days (January 15, 2023, to April 14, 2023).In terms of news data, we are experimenting with multiple open-source news scraping interfaces.Our primary focus is on gathering articles that have rich event elements, such as factual news and entertainment news.We have collected a total of 1, 000 such articles, aiming to avoid sensitive news categories like politics.As for the novel data, we randomly selected 1, 000 works from a renowned creative platform14 , striving to achieve a balanced representation of various writing types (fan fiction and original works).Afterward, we employ an open-source tool15 to filter out chapters containing explicit, violent, or other inappropriate content, sorting them based on the number of views.Following this filtering process, we ultimately retain 250 articles each from the news and novel categories as candidates (500 in total) for high-quality texts.</p>
<p>C.2 DATA ANNOTATION</p>
<p>During the construction of the KoLA benchmark, we have two key annotation tasks: (1) Fact Triple Annotation and (2) Event Argument Annotation.The annotated triples from (1) will subsequently be utilized for the construction of the evolving test sets for Task 1-3, 2-8, and 3-6.Meanwhile, the annotated event attributes from (2) will be employed for Task 4-1 and 4-2.Throughout the annotation process, we meticulously provided comprehensive instruction documents and implemented reasonable model pre-annotations, aiming to direct the annotators' focus to the most essential parts, thereby striving to enhance the quality of the primary annotated data.Subsequent to this, we also conducted multiple rounds of quality checks to ensure the reliability of the final constructed test set.Annotation Team.We hire a 21-member annotation team (based on market rates) comprising experienced annotators.</p>
<p>With their permission, we gather some basic information about the annotation team and present it in Table 4.In general, the annotation team consists mostly of individuals with graduate-level qualifications.The validators are three Ph.D. holders from the KoLA team.With the collaboration of this high-quality team, we strive to ensure the efficiency and quality of data annotation.Before initiating the annotation work, we enter into a legally binding contract with the team to protect the rights of the annotators.We also develop a dedicated platform specifically for the annotation process, which facilitates efficient review, publishing, and exporting of the annotation results.The platform allows annotators to have flexibility in choosing their working hours, including the ability to save intermediate results, retrieve relevant resources, and log in or log out at any time.</p>
<p>Annotation Process and Quality Control.For the aforementioned annotation tasks, we carefully employ methods for data pre-processing to facilitate the annotation process and quality control.</p>
<p>For Task (1), the text to be annotated includes only the Evolving data.We take the following steps to obtain the factual knowledge triples and whether they can be infered from the previous corpus.</p>
<ol>
<li>Named Entity Recognition.We first utilize a named entity recognition tool16 to extract entities from the articles.This model exhibits strong performance on four entity types (PER, ORG, LOC, MISC) with a Precision of 90.7%, Recall of 91.9%, and an F1-score of 91.3%.</li>
</ol>
<p>After removing a small percentage of incompletely recognized ones (&lt; 5.3%), these results are used as input for subsequent processes.</p>
<ol>
<li>
<p>Relation Extraction.Subsequently, we replicate a renowned document-level relation extraction model, ATLOP (Zhou et al., 2021), using these entities and documents to extract potential relationships and organized them into triple formats.Due to the task's complexity, the model achieves only an F1 score of 63.4%.Therefore, during replication, we reduce its prediction threshold to 0.4 further enhance the model's Recall (to 84.5%), aiming to minimize the omission of triples during the annotation process.</p>
</li>
<li>
<p>Triple Annotation.After the aforementioned preprocessing, a total of 23.1k candidate triples entered the annotation phase.Each annotator is permitted to use online searches and is instructed to categorize each candidate triple into the following three classes: a) incorrect triples, b) correct triples that can be known prior to the current season, and c) correct triples that can only be known after the current season began.Each triple is guaranteed to be annotated by two annotators.Eventually, the Cohen's Kappa for the correctness of the triple annotations (a vs b+c) is 0.71, and the Cohen's Kappa for whether the triple could be discovered before January 15th (b vs c) is 0.55, indicating a fair good agreement on such issues.In cases of classification discrepancies, the decision is deferred to the quality check lead.</p>
</li>
</ol>
<p>For Task (2), the text to be annotated includes both Known and Evolving data.As for the Evolving data, we also conduct processing as:</p>
<ol>
<li>Event Detection.We also employ the named entity recognition results (as in the annotation task ( 1)) to reproduce the Omni-Event toolkit17 .Specifically, we replace the backbone model with CLEVE (Wang et al., 2021b) for event detection, which achieves a Recall of 81.5% on the ACE2005 dataset and 72.6% on MAVEN dataset.The detected events are subsequently used as candidates for event argument annotation.</li>
</ol>
<p>The Known data portion utilizes articles from the MAVEN dataset (Wang et al., 2020), where the event trigger words and event types have already been annotated, requiring no additional pre-processing.Then, articles with detected events from both known and evolving data are processed into annotations.</p>
<ol>
<li>Event Argument Annotation.Annotators are required to perform the following tasks: a) annotate candidate attributes for each event, and b) correct or delete events if the event trigger words are incorrect.To guarantee the annotation quality, we build an online annotation platform that indexes examples for all 159 event types (with an average of 7 argument roles per type).The annotated results subsequently underwent two rounds of quality checks.Annotations deemed unsatisfactory in each round are returned for re-annotation.The firsttime pass rate for the first round of quality checks is 67%, and 91% for the second round.The overall Fleiss' Kappa for the annotation is 0.62.Ultimately, on average, 57.8 events and 235.3 arguments were annotated per article.</li>
</ol>
<p>C.3 AVAILABILITY</p>
<p>Platform.We develop an online platform to offer a range of services to the community, such as competition news updates, visualizations of evaluation results, and convenient access to submit new models or modify previous submissions.Due to the dynamic nature of the KoLA evolving data source, new results and rankings will be generated in each season, and a selection of results from previous seasons will also be publicly available.(2) Contributor: We maintain a special interest group (invitation link shown at the "About" page of the platform) where volunteers who have ideas for result analysis, model refinement, and benchmark improvement can discuss, propose suggestions, and participate in task construction.Supporting Tools.We release a toolkit to support KoLA-related functions at Github, including:</p>
<p>Participate in</p>
<p>(1) Easy-to-submit.Competitors can employ this function to independently maintain the in-context prompts for each task while providing a single model API, making the submission and modification convenient.</p>
<p>(2) Result Reproduction.We provide the code and developed tools that used in our data visualization and standardization, which support result reproduction and other analyses.(3) Data Acquisition.We also provide a data access API, which assists authorized users in getting the evolving data and results of previous seasons.</p>
<p>C.4 DISTRIBUTION OF THE ANNOTATED RESULTS</p>
<p>For Task (1), we annotate all 500 articles and retained 2.7K correct triples, out of which only 459 triples cannot be found in earlier corpora.Figure 4 illustrates the distribution of relations for all correct triplets, where blue bars represent category b) correct but can be known before the certain season; and red bars are the number of category c) correct and cannot be known before the certain season.It can be observed that even in the Evolving data, the triplets still exhibit a long-tail distribution.Most of the triplets, such as "country of citizen" and "country", do not effectively convey the main content of the articles.This further reinforces our goal of annotating fine-grained event-level knowledge.</p>
<p>For Task (2), we specifically select 100 articles from the MAVEN and Evolving datasets, ensuring they possess extensive knowledge (i.e., a substantial number of entities and event triggers that are not within the lowest 20% frequency range).After completing the annotation process, we examine the number of valid events (events containing at least one argument) and the article lengths, as illustrated in Figure 5 and Figure 6.In general, the distribution of event knowledge between the two datasets is quite similar.The articles in the Evolving dataset are generally longer compared to the Wikipedia articles in MAVEN, resulting in a higher number of valid events.Due to this difference, models may encounter more challenges when performing tasks on the Evolving dataset, but they may also benefit from greater exposure to knowledge.Therefore, the varying performance of models on the tasks upon these two data sources may be influenced by factors such as the model's parameter size or training adequacy.This phenomenon warrants further analysis and discussion.</p>
<p>C.5 ANNOTATION COST</p>
<p>To sustain the project over the long term, we have kept the budget for each season below USD 2,000.Generally, each season's expenditure comprises two main parts: 1) the cost of data annotation, and 2) the expenses for model deployment and API calls.</p>
<p>The data annotation includes labeling knowledge triplets and event arguments.As introduced in Appendix C.2, to reduce the difficulty and cost of annotation, we have pre-labeled the collected text automatically, allowing annotators to focus on the core tasks.Overall, each season requires approximately USD 660-700 for triple annotation and USD 400-420 for event annotation.</p>
<p>Due to the scale of the KoLA test sets, the costs for model deployment and API calls have been kept within an acceptable range.The deployment expenses, estimated from GPU usage time, are approximately USD 200-240, while the API incurs an additional cost of USD 150-180.</p>
<p>Overall, for the two seasons completed thus far, the costs have not exceeded USD 1,600 per season.</p>
<p>With the anticipated increase in the number of models in future seasons, we believe that a budget of USD 2,000 per season should be sufficient to maintain the project.</p>
<p>D.3 KNOWLEDGE UNDERSTANDING TASKS</p>
<p>Knowledge Memorization (KM) level involves various levels of structured information, such as concepts, entities, relationships, and events.It also incorporates multiple documents, even at the multi-document level, which can easily overwhelm the model and distract from the main task.Therefore, we have placed a strong emphasis on standardizing the input and output of this layer to facilitate the model's comprehension of the task objectives.Additionally, we strive to balance the simplicity of the input while ensuring comprehensive understanding.</p>
<p>Specifically, the instructions for each task are as follows:</p>
<p>INSTRUCTION: Conceptual similarity judgment QUERY: Among Tutu Chengcui, The Pierre, Waddesdon Manor, Astro Orbitor, Heian period, 2019 Canadian federal election, Paradiski, Tenughat Dam, Gros Michel banana, Reedy Glacier, Gangotri Glacier, Pinatubo, Interwar period, djon djon, Qiu Shiliang, Caciotta, Firth of Forth, 2011 Rugby World Cup, Cheng Yuanzhen, Pliocene, Sri Maha Bodhi, which one is the most conceptually similar with Botryosphaeria stevensii?Please answer the entity name only.</p>
<p>ANSWER: djon djon</p>
<p>Table 8: The instruction and an example of Task 2-1 COPEN-CSJ, KU.</p>
<p>INSTRUCTION: Conceptual property judgment QUERY: Is the statement "Specieses have a cellulose wall and other polysaccharides."true or false?Please answer true or false.</p>
<p>ANSWER: False</p>
<p>Table 9: The instruction and an example of Task 2-2 COPEN-CPJ, KU.</p>
<p>INSTRUCTION: Conceptualization in contexts</p>
<p>QUERY: Given the context "The next year, he made his stock car racing debut in the American Speed Association, where he won a pole at Winchester Speedway and had four top-tens.",neglect your knowledge about Winchester Speedway and select the most contextually related concept for it from the concept set: Racecourse, Place, ArchitecturalStructure, Infrastructure, RaceTrack, Venue, Road, SportFacility, RouteOfTransportation, Building.</p>
<p>Please answer the concept name only.</p>
<p>ANSWER: Racecourse</p>
<p>Table 10: The instruction and an example of Task 2-3 COPEN-CiC, KU.</p>
<p>INSTRUCTION: Please recognize entities for the given text and classify them into a suitable type.The collection of types is as follows: <set of types> QUERY: Agrippa succeeded in blocking the more manoeuvrable ships of Sextus and, after a long and bloody fight, to defeat his enemy.</p>
<p>ANSWER: Agrippa: person-politician; Sextus: person-politician;   INSTRUCTION: Please classify the relation between two events/"Time" in a given document.There are 10 types of relations: ["before", "overlap", "contains", "simultaneous", "begins-on", "ends-on", "cause", "precondition", "subeven", and "coreference"].In each document, 2 events/"Timex" are marked as "<Event> event name </Event>" or "<Timex> Timex name </Timex>".If there is a relation type or multiple relation types, the answer form is "Answer: [relation type 1, relation type 2, ...]".</p>
<p>QUERY: Document: The Central Park jogger case was a criminal case in the United States based on the assault and <Event> rape </Event> of Trisha Meili, a 28-year-old white woman who was jogging in the park, and attacks on eight other persons, in areas ranging from the North Woods of Manhattan's Central Park to the Reservoir, on the night of April 19, 1989.Three of the victims were black or Latino.Meili was so injured that she was in a coma for 12 days."The New York Times" in 1990 described the attack on her as "one of the most widely publicized crimes of the 1980s".Attacks in Central Park that night were allegedly committed by a loose group of 30 201332 teenagers, and police attempted to apprehend suspects after crimes began to be reported between 9 and 10 p.m.The brutally beaten Meili was not found until 1:30 a.m., after which the police hunt greatly intensified.They took into custody 14 or more other suspects over the next few days, and arrested a total of ten suspects who were ultimately tried for the attacks.Among them were four African American and two Hispanic American teenagers who were indicted on May 10 on charges of assault, robbery, riot, rape, sexual abuse, and attempted murder of Meili and an unrelated man, John Loughlin.The prosecutor planned to try the defendants in two groups, then scheduled the sixth defendant to be tried last.The latter pleaded guilty in January 1991 on lesser charges and received a reduced sentence.Prosecution of the five remaining defendants in the rape and assault case was based primarily on confessions which they had made after police interrogations.None had counsel during this questioning.Within weeks, they each withdrew these confessions, pleaded not guilty, and refused plea deals on the rape and assault charges.None of the suspects ' DNA matched the DNA collected from the crime scene: two semen samples that both belonged to one unidentified man.No substantive physical evidence connected any of the five teenagers to the rape scene, but each was convicted in 1990 of related assault and other charges.Subsequently known as the Central Park Five, they received stiff sentences ranging from 5 to 15 years.Four of the defendants appealed their convictions, but these were affirmed by appellate courts.The four juvenile defendants served 6 20137 years each; the 16-year-old, tried and sentenced as an adult, served 13 years in adult prison.The five other defendants, <Event> indicted </Event> for assaults of other victims, pleaded guilty to reduced charges and received less severe sentences.In 2001, Matias Reyes, a convicted murderer and serial rapist serving life in prison, confessed to officials that he had raped the female jogger.His DNA matched that found at the scene, and he provided other confirmatory evidence.He said he committed the rape alone.Reyes could not be prosecuted for raping Meili, because the statute of limitations had passed.In 2002 Robert Morgenthau, District Attorney for New York County, had his office conduct an investigation and recommended to the state court that the convictions of the five men on all charges be vacated.The court vacated their convictions in 2002, and the state withdrew all charges against the men.In 2003, the five men sued the City of New York for malicious prosecution, racial discrimination, and emotional distress.The city refused to settle the suits for a decade, because its lawyers believed that the city could win a court case.After a change in administration, the city settled in 2014 with the five plaintiffs for $41 million.The five men also filed suit against the State of New York for additional damages; this case was settled in 2016 for a total of $3.9 million.The first event/"Timex": <Event> rape </Event>.The second event/"Timex": <Event> indicted </Event>.</p>
<p>ANSWER: before   Table 20: The instruction and an example of Task 3-5 KoRC, KA.</p>
<p>INSTRUCTION: You are given one document and one anonymized real-world entity with one or more mentions in the passage.Then we will ask your a question about this anonymized entity.The questions cannot be answered solely within the document or the background knowledge.Your task is to leverage world knowledge you have like Wikipedia or Wikidata as background knowledge combined with the given document to answer the question related to the anonymized entity.You must output all answers in the end.</p>
<p>QUERY: Six months after its New Shepard rocket suffered a failure during flight, Blue Origin said Friday its review of the incident pinpointed a problem with its engine nozzle and that it is expecting to return to flight "soon."In September, the rocket lifted off and flew for just over a minute before bright flames flashed from the booster and the capsule's emergency abort system kicked in, propelling it away from the rocket.The mission carried only science experiments; no one was on board, and no one was injured on the ground.In a statement Friday, Blue Origin, the space venture founded by Amazon executive chairman Jeff Bezos, said that it would refly the mission, again carrying scientific payloads.(Bezos owns [daily newspaper].)A flight with people could come later.The vehicle is designed to carry as many as six people to the edge of space and back on suborbital tourist trips that allow passengers to experience weightlessness and view the earth from above.In the statement, Blue Origin said its investigation, which was overseen by the Federal Aviation Administration and included members of the National Transportation Safety Board, concluded that the problem was caused by a failure of the engine nozzle, which experienced "temperatures that exceeded the expected and analyzed values of the nozzle material."Engineers are "implementing corrective actions, including design changes to the combustion chamber and operating parameters," the statement said."Additional design changes to the nozzle have improved structural performance under thermal and dynamic loads."The FAA said in a statement that it is reviewing Blue Origin's mishap report but that the investigation remains open."FAA approval is required to close the investigation and for the New Shepard system to return to flight."It was unclear how long that could take.While the booster was lost, the capsule and the 36 payloads it was carrying landed safely under parachutes and can fly again, Blue Origin said.The booster, which under normal circumstances falls back to Earth and touches down softly on a landing pad so that it can be reused, was a total loss.The company was able to recover all the debris from the rocket within the designated hazard area, it said.Disregarding Instruction: Another factor that significantly affects the model's performance is its potential inability to comprehend the task instructions, resulting in failure to produce outputs in the specified format.This poses challenges for evaluating the generation-based open-ended answers.Therefore, we attempt to extract key information from the answers using techniques such as regular expressions and perform fuzzy matching.Unfortunately, there are still many scenarios where certain models fail to generate correct and valid answers.For models that fail to provide reasonable answers on over 90% of the test cases for a particular task, we mark them as N/A.</p>
<p>Sensitive Result: The models may also trigger or bypass their safety mechanisms in certain tasks, such as refusing to provide answers (due to mistakenly perceiving the given information as containing unsafe content) or generating sensitive content.For cases where the model refuses to provide an answer, we conduct manual checks to ensure that the data in the test set does not contain explicit, violent, discriminatory, or other inappropriate content.In the secure test set, for situations where the model refuses to answer or exhibits similar behavior, we handle them in the same way as mentioned above.For models that cannot provide reasonable answers on over 90% of the test cases for a particular task, we mark them as N/A.</p>
<p>In general, for all tasks, if a model's performance is marked as "-" or "N/A" on a particular task, we do not include that score in the calculation of the standard score.However, when calculating the overall rankings of the models across different levels, we consider these cases as "missing" and assign them a score of 0. There is still room for improvement in this handling approach.Nonetheless, we firmly believe that the model's ability to handle input length, adhere to guidelines, and handle sensitive information is an important foundational skill for dealing with real-world knowledge problems.Besides, for different tasks, we adjust some parameters used in the inference process based on the model's overall performance, as detailed in Table 25.Once we obtain the model inference results, we select various evaluation metrics and answer normalization methods according to the task's characteristics.</p>
<p>Inference Implementation.Following the approach of HELM (Liang et al., 2022), we first designed a unified dataset format template to store instructions, provided examples, test inputsoutputs, and decoding parameters.These parameters, akin to HELM, are adapted based on the specific task, as detailed in Table 25.After model inference, we document the following information: a) the text completed by model inference; b) the duration of the inference; c) the timestamp of the inference; d) each token inferred by the model, and f) the probability of each token after log_softmax, which is used for further analysis and score computation.</p>
<p>Answer Normalization.Since all tasks are set up as open-ended generative question-answering formats, to best reflect the model's true performance, we select different evaluation metrics tailored to the attributes of various task levels for post-processing the answers.Specifically, for the tasks the major evaluation metric, we employ a relaxed F1 (token match rate), i.e., after tokenizing the model's predicted results and the reference answers using the GPT2Tokenizer (Radford et al., 2019), we compute whether each token in the prediction is contained in the corresponding position of the gold standard.For the tasks are formed as classification tasks, we employ Accuracy of the final result as the major evaluation metric, while the generative tasks utilize the widely-accepted Rouge (Lin, 2004).We also make this portion of the code publicly available in our github repository, for introducing more detailed tricks to lift the reproducibility.</p>
<p>Mechanism for Standardized Scores.To make the model scores more comparable, we calculate the standardized scores by pairing each Evolving Task with its corresponding Known Task within the same level, such as 2-5 with 2-8, and 3-5 with 3-6.Notably, the three tasks at the first level are computed together.After determining the standardized scores for each level, we then compute the average standardized score for each level.The final overall score is derived from the average of these average standardized scores across levels.In this manner, we aim to mitigate potential biases caused by the varying number of tasks across different levels.</p>
<p>F MORE EVALUATION RESULT (SEASON 1)</p>
<p>Due to the length constraints of the paper, including the manual annotation process in the experimental section, a series of specific results are not fully presented.In this section, we first introduce the annotation process and results for the knowledge creation level.Then, we provide a detailed list of all absolute performance values for each model in the first season and discuss some notable findings.Annotation Team for Evaluation.We recruit members from the annotation team who are willing to participate in result annotation and further confirm their compensation and bonuses to protect their earnings and rights.Currently, 14 annotators participate in the annotation of knowledge creation results.The composition of these individuals closely resembles that of the overall annotation team, with all members having a bachelor's degree or higher.The annotation results undergo quality checks conducted by three Ph.D. students from KoLA teams, ensuring both efficiency and quality in the annotation process.We use Streamlit18 to build an annotation platform where annotators can simultaneously view the text to be annotated, the context, and the historical annotations.</p>
<p>ANNOTATION OF CREATING TASKS</p>
<p>Annotation Process and Result.For each model, we provide the following information: a) preceding context, b) continuation generated by the model, c) event knowledge content of the actual subsequent text, and d) actual subsequent text.It is important to note that in item b), the model generates the continuation without being informed of the knowledge in the subsequent text.Annotators are required to evaluate two aspects: i) the overall quality of the text, which includes assessing the novelty, coherence, and fluency of the model's creative content; and ii) the plausibility of the generated content in terms of knowledge hallucination, ensuring it does not conflict with real-world situations.The latter aspect is a subtask of the former.The annotation is conducted on a 5-point scale, where 1 represents the worst and 5 represents the best.We collect 4.2K scoring results and calculate the average score for each model.Table 27 presents the results and variances of the two ratings for each model.It is based on these scores and the model's rank that we calculate various correlation coefficients.</p>
<p>Figure 1 :
1
Figure 1: KoLA's careful design on three key factors for LLM evaluation.</p>
<p>Figure 2 :
2
Figure 2: Illustration of the Knowledge Creating (KC) tasks.</p>
<p>Figure 3 :
3
Figure 3: Left: The spearman correlation coefficient.Each cell represents the correlation of model rankings on two tasks.Right: Scatter plots of rolling task vs. corresponding non-rolling task (e.g., 3-5 v.s.3-6).The x-axis and y-axis of each subplot represent the standard scores correspondingly.</p>
<p>KoLA.Researchers can participate in KoLA evaluations in two roles.(1) Competitor: KoLA welcomes open participation for each season by providing the model's APIs or parameters, and provides 5 high-quality examples for each task to help participants debug.It is worth noting that KoLA does not allow the local evaluation to prevent test set leakage and unfairly overfitting datasets.</p>
<p>Figure 4 :
4
Figure 4: The distribution of the top-20 annotated triples in terms of relation type.Blue bars represent the correct triples that can be known prior to the current season, and red bars represent the correct triples that can only be known after the current season begins.</p>
<p>Figure 5 :
5
Figure 5: The distribution of the events and text length in the articles from Wikipedia (MAVEN).Blue bars and red bars correspond to the number of events and the tokens in each article.</p>
<p>Figure 6 :
6
Figure 6: The distribution of the events and text length in the articles from Evolving Data.</p>
<p>Table 1 :
1
The tasks in KoLA(Season 1st and 2nd).Metrics in bold are selected for calculating standardized scores.Exclusive task means their test sets are newly developed or sponsored by the original authors and were not publicly disclosed.Test Set and Pool correspond to the testing instances used in each season and the overall available instances.
Level ID DatasetMetricsExclusiveContext TypeTest SetPoolSourceKM1-1 High-Freq. 1-2 Low-Freq.EM, F1 EM, F1✓ ✓Triple Triple100 10020.6M 20.6MKnown1-3 ETMEM, F1✓Triple1002.7kEvolving2-1 COPEN-CSJAcc.✓Entity, Concept1003.9k2-2 COPEN-CPJAcc.✓Concept1004.7k2-3 COPEN-CiCAcc.✓Concept1002.3kKU2-4 FewNERD 2-5 DocREDF1 F1× ✓Sentence Document, Entity300 100188.2k 12kKnown2-6 MAVENF1✓Document10020.4k2-7 MAVEN-EREF1✓Document(s), Event1991.3M2-8 ETUF1✓Document, Entity1001.6kEvolving3-1 HotpotQAF1×Document(s)1007.4k3-2 2WikiMulti.F1✓Document(s)10012.6kKA3-3 MuSiQue 3-4 KQA ProF1 F1✓ ✓Document(s) KG100 1002.5k 1.2kKnown3-5 KoRCF1✓Document(s), KG1005.2k3-6 ETAF1✓Document(s), KG491.6kEvolvingKC4-1 EncyclopedicBLEU, Rouge✓Document, Event954.5kKnown4-2 ETCBLEU, Rouge✓Document, Event95100Evolvingrandomly select 300 examples in our evaluation. (2-5) Relation Extraction selects the undisclosed testset from the challenging document-level relation extraction dataset, DocRED (Yao et al., 2019). (2-6)Event Detection adopts the undisclosed test set of the finely annotated MAVEN (Wang et al., 2020)dataset. (2-7) Event Relation Extraction involves the undisclosed test set from MAVEN-ERE (Wanget al., 2022), which consists of 113k examples of coreference, temporal, causal, and subevent relationsbetween events. (2-8) Evolving Test of Understanding (ETU). For the articles in evolving data, weconduct the entity recognition and follow the same relation schema of DocRED to annotate a brandnew test set containing 100 relation instances from 50 articles. It is worth noting that apart from theevolving test, the other datasets are all based on Wikipedia texts.</p>
<p>Table 2 :
2
Standardized performance of Knowledge Memorization and Understanding level.
ModelLevel 1: KMLevel 2: KU1-11-21-3Rank2-12-22-32-42-52-62-72-8RankGPT-464.3 68.9 41.51st (-) 69.5 48.6 51.4 66.9 100.0 77.2 78.9 81.31st (-)GPT-3.5-turbo53.4 60.0 38.32nd (↑2) 44.2 49.4 50.2 54.051.3 50.2 56.6 25.52nd (-)InstructGPT davinci v2 (175B<em>) 41.2 48.4 36.17th (↑1) 33.6 48.2 42.4 41.856.7 62.3 40.6 31.33rd (-)Tulu (7B)41.5 48.6 28.48th (↓2) 22.0 25.4 43.0 35.730.9 20.6 22.2 25.8 11th (↑1)Cohere-command (52.4B)59.0 54.5 33.93rd (↓1) 40.0 47.1 46.3 26.638.6 20.6 46.9 25.04th (-)FLAN-UL2 (20B)53.0 42.6 30.76th (↓1) 59.0 47.1 53.0 16.025.0 20.6 22.2 25.06th (-)J2-Jumbo-Instruct (178B</em>)32.6 33.7 19.2 12th (-) 27.3 23.5 31.2 37.132.0 32.7 51.3 25.07th (-)ChatGLM (130B)38.0 56.5 36.15th (↑2) 30.5 47.8 51.9 16.025.0 23.3 30.3 25.08th (-)FLAN-T5 (11B)56.1 51.5 32.94th (↓1) 63.2 47.8 49.1 18.7---25.05th (-)InstructGPT curie v1 (6.7B*)28.1 43.8 28.9 10th (↓1) 29.4 41.2 41.8 22.325.4 22.0 25.8 25.09th (-)LLaMa (65B)24.2 25.6 19.5 15th (↓1) 22.0 18.4 18.3 55.631.4 30.1 25.5 25.0 10th (↑1)ChatGLM2-32k (6B)25.3 22.8 20.1 16th (-) 22.0 40.8 20.0 19.026.5 20.6 22.7 25.4 17th (-)Alpaca (7B)21.5 25.3 18.2 17th (↓2) 22.0 18.4 18.8 25.326.4 31.4 22.2 25.0 20th (-)Llama2-chat (7B)21.6 19.7 17.9 22th (↓3) 25.2 18.4 24.5 37.432.5 20.6 27.2 25.6 14th (-)ChatGLM (6B)31.9 32.9 30.5 11th (-) 23.1 45.5 32.9 16.025.0 22.0 22.8 25.0 13th (-)Vicuna (13B)18.8 19.1 17.4 26th (-) 22.0 18.7 23.3 29.826.0 35.4 30.5 25.0 15th (-)GLM (130B)21.9 25.1 22.9 14th (↑3) 22.0 18.4 18.3 49.633.2 29.7 22.2-12th (↓2)GPT-J (6B)20.8 18.8 18.0 25th (↓1) 22.0 18.4 18.3 22.225.0 31.0-25.0 25th (-)T0++ (11B)41.9 38.4 23.99th (↑1) 30.5 39.2 27.8 16.0---25.0 16th (-)Dolly-v2 (12B)21.1 21.0 18.9 20th (↑2) 22.0 18.4 18.8 28.525.0 20.6 27.1 25.0 23th (-)GPT-JT (6B)19.8 18.9 19.0 24th (↑1) 22.0 18.4 18.3 19.225.0 36.7-25.0 22th (-)Internlm-chat-8k (7B)23.5 20.4 17.2 19th (↑1) 22.0 18.4 18.3 19.027.2 20.6 26.1 25.0 27th (-)UL2 (20B)26.5 28.1 18.9 13th (-) 22.0 18.4 18.3 18.0---25.0 28th (-)GPT-3 davinci v1 (175B)18.1 17.9 16.9 27th (-) 22.0 18.7 18.3 30.225.0 29.6 22.3 25.0 19th (-)GPT-NeoX (20B)19.9 20.7 18.2 23th (-) 22.0 18.4 18.3 25.725.0 32.1-25.0 21th (-)BLOOM (7B)21.0 21.9 18.3 18th (-) 22.0 18.4 18.3 30.128.0 29.2 22.2 25.0 18th (-)GPT-3 curie v1 (6.7B)17.2 17.7 16.8 28th (-) 22.0 18.4 18.3 21.525.0 25.6 23.9 25.0 26th (-)RedPajama-Instruct (7B)21.8 21.2 16.4 21th (-) 22.0 18.4 18.3 29.825.0 20.6 25.4 25.0 24th (-)</p>
<p>Table 3 :
3
Standardized performance of Knowledge Applying, Creating level and all the 4 levels.
ModelLevel 3: KALevel 4: KCOverall (1,2,3,4)3-13-23-33-43-53-6Rank4-14-2RankAvgRankGPT-459.8 60.7 76.8 32.8 60.9 58.11st (-) 48.6 58.92nd (↑1)2.331st (-)GPT-3.5-turbo58.5 42.6 53.9 45.6 30.9 19.65th (↓1) 52.2 46.43rd (↓1)1.342nd (-)InstructGPT davinci v2 (175B<em>) 30.6 39.7 44.2 23.6 50.3 23.47th (↓1) 54.4 56.81st (-)1.123rd (-)Tulu (7B)42.5 45.6 40.7 54.8 42.3 54.83rd (↑2) 32.8 42.59th (↑2)0.644th (↑3)Cohere-command (52.4B)36.2 41.7 45.3 49.3 54.7 44.04th (↓1) 17.1 15.1 25th (↓13)0.545th (↓1)FLAN-UL2 (20B)49.6 47.5 39.4 51.1 43.5 53.32nd (-) 28.3 19.020th (↓3)0.546th (↓1)J2-Jumbo-Instruct (178B</em>)45.2 31.6 31.6 38.3 28.3 25.58th (-) 43.7 53.34th (-)0.477th (↑1)ChatGLM (130B)36.4 34.1 28.0 36.4 36.7 21.29th (↑1) 24.4 28.416th (↑2)0.288th (↑1)FLAN-T5 (11B)45.0 49.0 32.9 51.1 39.7 16.16th (↑1) 20.20.028th (↓6)0.229th (↓3)InstructGPT curie v1 (6.7B*)31.6 37.2 24.3 29.1 31.2 27.2 11th (-) 27.7 29.612th (↑3)0.06 10th (-)LLaMa (65B)16.4 35.4 41.7 25.4 21.7 17.6 16th (↓2) 44.7 37.15th (-)0.01 11th (-)ChatGLM2-32k (6B)35.7 31.1 24.0 40.1 20.7 17.3 13th (↓1) 34.5 41.58th (-) -0.09 12th (↑2)Alpaca (7B)15.1 19.3 20.9 18.1 45.5 50.7 12th (↑5) 35.7 41.07th (↑3) -0.12 13th (↑2)Llama2-chat (7B)17.5 16.8 23.4 14.4 40.5 51.7 14th (↑2) 31.7 38.110th (↓4) -0.18 14th (↓2)ChatGLM (6B)21.2 27.5 22.2 19.9 19.5 28.5 20th (-) 17.7 30.818th (↑6) -0.24 15th (↑5)Vicuna (13B)25.4 10.0 24.7 18.1 21.0 16.4 24th (↓1) 35.0 45.96th (↑1) -0.26 16th (↑1)GLM (130B)23.5 13.0 18.4 21.7 45.0 35.1 17th (↓4) 29.3 19.119th (↓3) -0.33 17th (↓1)GPT-J (6B)38.8 39.4 26.8 49.3 17.5 16.5 10th (↓1) 30.5 24.014th (↑9) -0.33 18th (↑3)T0++ (11B)22.4 23.0 23.8 14.4 39.7 16.1 19th (-) 18.13.727th (↓8) -0.44 19th (↓6)Dolly-v2 (12B)14.1 20.5 18.4 18.1 26.4 25.2 22th (-) 29.5 31.911th (↓2) -0.45 20th (↓1)GPT-JT (6B)31.4 38.2 23.0 32.8 18.5 17.5 15th (-) 21.2 19.521th (↑5) -0.54 21th (↑3)Internlm-chat-8k (7B)19.3 20.8 22.8 14.4 17.1 22.2 23th (↑1) 26.1 27.815th (↑5) -0.56 22th (↑1)UL2 (20B)25.2 28.0 25.9 38.3 16.1 16.1 18th (-) 26.99.023th (↓9) -0.56 23th (↓5)GPT-3 davinci v1 (175B)18.3 13.5 22.8 19.9 21.5 17.0 25th (-) 32.3 22.613th (-) -0.57 24th (↓2)GPT-NeoX (20B)14.0 13.9 17.1 18.1 22.7 16.7 28th (↓1) 32.2 19.217th (↑4) -0.61 25th (-)BLOOM (7B)18.6 22.6 17.1 19.9 27.4 23.3 21th (-) 17.9 21.422th (↑5) -0.61 26th (-)GPT-3 curie v1 (6.7B)22.6 15.2 20.5 18.1 19.1 16.4 26th (-) 23.7 10.124th (↑1) -0.81 27th (-)RedPajama-Instruct (7B)12.6 10.0 17.1 14.4 26.1 23.2 27th (↑1) 13.7 12.326th (↑2) -0.85 28th (-)</p>
<p>Table 4 :
4
Statistics of the annotation team for dataset construction in KoLA.
GenderRateFemale85.7%Male14.3%EducationRateBachelor47.6%Master52.4%</p>
<p>Table 11 :
11
The instruction and an example of Task 2-4 FewNERD, KU.Please follow the above demonstration, and extract relations from the [Question text].Note the relation needs to be in the predefined set of relations.The output format required to is the same as the demonstration, format:(<entity_ID>, relation, <entity_ID>).The predefined set of relations: <set of types>
QUERY: <entity_0> Rickon Stark </entity_0> is a fictional character in the <entity_1> A Song of Ice andFire </entity_1> series of fantasy novels by <entity_2> American </entity_2> author <entity_3> George R.R. Martin </entity_3>, and its television adaptation <entity_4> Game of Thrones </entity_4> . Introduced in<entity_5> 1996 </entity_5> 's <entity_6> A Game of Thrones </entity_6>, <entity_0> Rickon </entity_0>is the youngest child of <entity_7> Eddard Stark </entity_7>, the honorable lord of <entity_8> Winterfell</entity_8>, an ancient fortress in the <entity_9> North </entity_9> of the fictional kingdom of <entity_10>Westeros </entity_10>. He subsequently appeared in <entity_3> Martin </entity_3> 's <entity_11> A Clashof Kings </entity_11> (<entity_12> 1998 </entity_12>) . The <entity_13> Publishers Weekly </entity_13>review of <entity_6> A Game of Thrones </entity_6> noted, Ït is fascinating to watch <entity_3> Martin</entity_3> 's characters mature and grow, particularly <entity_14> Stark </entity_14> 's children, who stand atthe center of the book. <entity_0> Rickon </entity_0> is played by <entity_15> Art Parkinson </entity_15> inthe <entity_16> HBO </entity_16> television adaptation.ANSWER: (<entity_0>, father, <entity_7>); (<entity_0>, present in work, <entity_4>); (<entity_0>, creator,<entity_3>); (<entity_7>, child, <entity_0>); (<entity_7>, present in work, <entity_6>); (<entity_7>, presentin work, <entity_4>); (<entity_6>, publication date, <entity_5>); (<entity_6>, characters, <entity_7>); (<en-tity_6>, author, <entity_3>); (<entity_4>, publication date, <entity_5>); (<entity_4>, characters, <entity_7>);(<entity_4>, has part, <entity_6>); (<entity_4>, author, <entity_3>); (<entity_4>, screenwriter, <entity_3>);(<entity_4>, original network, <entity_16>); (<entity_3>, notable work, <entity_6>); (<entity_10>, present inwork, <entity_4>); (<entity_11>, publication date, <entity_12>); (<entity_11>, series, <entity_6>); (<entity_11>,follows, <entity_6>); (<entity_11>, series, <entity_4>); (<entity_11>, author, <entity_3>); (<entity_14>, presentin work, <entity_4>); (<entity_14>, creator, <entity_3>);
INSTRUCTION:</p>
<p>Table 12 :
12
The instruction and an example of Task 2-5 DocRED, KU.Please identify the events in the text and classify them into appropriate categories; The collection of categories is <set of types> QUERY: The ruling National Command of the Arab Socialist Ba'ath Party were removed from power by a union of the party's Military Committee and the Regional Command, under the leadership of Salah Jadid.</p>
<p>INSTRUCTION:ANSWER: removed:Removing</p>
<p>Table 13 :
13
The instruction and an example of Task 2-6 MAVEN, KU.</p>
<p>Table 14 :
14
The instruction and an example of Task 2-7 MAVEN-ERE, KU.Please follow the above demonstration, extract relations from the [Question text].Note the relation need to be in the predefined set of relations.The output format required to is the same as the demonstration, format:(<entity_ID>, relation, <entity_ID>).The predefined set of relations: <set of types>
QUERY: Text: Less than four months removed from the entity0 World Cup entity0's bright lights , the U.S.men's national soccer team visited an 8,000 -seat bayside stadium on a tiny entity1 Caribbean entity1 islandFriday to face an opponent with players from regional leagues and some of entity2 England entity2's lowestdivisions entity3. <text continued> Relations in the predefined set of relations in the above text:?
INSTRUCTION:ANSWER: (entity4,part of,entity13); (entity6,part of,entity13); (entity13,has part,entity4); (entity15,participant of,entity0); (entity15,country of citizenship,entity9); (entity16,country of citizenship,entity9); (entity17,country of citizenship,entity9); (entity18,country of citizenship,entity9); (entity19,country of citizenship,entity9); (en-tity20,country of citizenship,entity9);</p>
<p>Table 15 :
15
The instruction and an example of Task 2-8 ETU, KU.When was Neville A. Stanton's employer founded?ANSWER: The employer of Neville A. Stanton is University of Southampton.The University of Southampton was founded in 1862.So the answer is: 1862.</p>
<p>INSTRUCTION: Please answer the following question.QUERY:</p>
<p>Table 19 :
19
The instruction and an example of Task 3-4 KQA Pro, KA.You are given one document and one anonymized real-world entity with one or more mentions in the passage.Then we will ask your a question about this anonymized entity.The questions cannot be answered solely within the document or the background knowledge.Your task is to leverage world knowledge you have like Wikipedia or Wikidata as background knowledge combined with the given document to answer the question related to the anonymized entity.You must output all answers in the end.QUERY: Allen is a county in the U.S. state of Ohio.As of the 2010 census, the population was 106,331.The county seat is Lima.The county was created in 1820 and organized in 1831.The county is named for Colonel [a human being], who was killed leading his men at the Battle of Frenchtown, during the War of 1812.It has also been claimed the county was named for Revolutionary War soldier Ethan Allen, but the weight of the evidence in favor of [the human being] led the General Assembly to declare in 1976 that the county was named for him.Allen comprises the Lima, OH Metropolitan Statistical Area, which is also part of the Lima -Van Wert -Wapakoneta, OH Combined Statistical Area.</p>
<p>INSTRUCTION:QUESTION:: Which place was this human being born?ANSWER: Rockbridge County.</p>
<p>Bezos flew on the first flight with people in 2021.It had since flown five other missions with people on board, including one with Star Trek actor William Shatner and television commentator Michael Strahan.It has not flown since the September incident.</p>
<p>QUESTION:: What is the home country of [daily newspaper]?ANSWER: United States</p>
<p>Table 21 :
21
The instruction and an example of Task 3-6 ETA, KA.</p>
<p>Table 25 :
25
(Liang et al., 2022)rameters in KoLA (Season 1st)."MaxTrain" and "Max Eval" correspond to the maximum number of examples and test cases."Output"correspondsto the output format.We both include necessary and additional parameters required to fully specify evaluation (i.e. the number of evaluation instances and runs, which influence the statistical validity and reliability of the results), though they are not strictly part of the adaptation process, as HELM(Liang et al., 2022)presents.
Level ID DatasetTemperature Max Tokens Stop sequence(s) Max Train Max Eval OutputKM1-1 High-Freq. 1-2 Low-Freq.1 164 64None None5 5100 100List1-3 ETM164None5100List2-1 COPEN-CSJ1128None51002-2 COPEN-CPJ1128None51002-3 COPEN-CiC1128None5100KU2-4 FewNERD 2-5 DocRED1 1128 256None \n5 4100 100String2-6 MAVEN0.2200None51002-7 MAVEN-ERE1200None51002-8 ETU1256\n4100String3-1 HotpotQA1128\n51003-2 2WikiMulti.1128\n5100KA3-3 MuSiQue 3-4 KQA Pro1 1128 128\n \n5 5100 100String3-5 KoRC150<stop>51003-6 ETA120<stop>5100StringKC4-1 Encyclopedic0256None5100String4-2 ETC0256None5100StringE.3 ANSWER NORMALIZATION AND SCORING IMPLEMENTATION</p>
<p>Table 26 :
26
Statistics of the annotation team for creating evaluation in KoLA.
GenderRateFemale71.4%Male28.6%EducationRateBachelor57.1%Master42.9%</p>
<p>Table 27 :
27
The human evaluation results of Knowledge Creating (KC).
ModelOverall Knowledge ModelOverall Knowledge ModelOverall KnowledgeFLAN-T52.253.00 LLaMa2.252.75 InstructGPT curie v11.752.75UL22.002.25 Alpaca1.252.00 InstructGPT davinci v22.502.75FLAN-UL23.002.00 J2-Jumbo-Instruct2.503.00 GLM2.753.00GPT-J2.002.25 Cohere-command2.753.00 GPT-42.753.75GPT-NeoX3.253.25 GPT-3.5-turbo2.503.00 ChatGLM2.502.00BLOOM3.252.00 GPT-3 curie v11.753.50 ChatGLM (130B)2.503.00T0++2.252.75 GPT-3 davinci v12.502.50 GPT-JT2.502.25</p>
<p>Table 28 :
28
Season 1's standardized performance of Knowledge Memorization and Understanding level.</p>
<p>Some cases have been noted (https://cevalbenchmark.com/static/leaderboard.html)
https://www.wikipedia.org
https://chat.openai.com
An open source news API at Github. URL: https://github.com/ranahaani/GNews
A well-known open license novel creating community. URL: https://archiveofourown.org
https://platform.openai.com/overview
https://docs.cohere.com/docs/the-command-model
CONCLUSION AND FUTURE WORKThis paper presents KoLA, a carefully designed Knowledge-oriented LLM assessment benchmark. We design a cognitive ability taxonomy for more helpful diagnostic results, adopt both known and evolving data sources for better fairness, and employ contrastive metrics for high applicability. In the first season of KoLA, we evaluate 28 open and commercial LLMs and get some intriguing findings, such as larger models tend to memorize more knowledge, and alignment unleashes the potential of higher-level abilities but may harm the low-level knowledge memorization, etc. In the future, we will continually host more seasons of KoLA to facilitate knowledgeable LLMs, help select backbones for developing knowledge-related applications, and track the development of LLMs with evolving evaluations. KoLA will always welcome open participation and contributions.
https://archiveofourown.org
https://huggingface.co/unitary/toxic-bert
https://huggingface.co/dslim/bert-base-NER
https://github.com/THU-KEG/OmniEvent
https://streamlit.io/
D DETAILS OF TASK INSTRUCTIONAfter completing the data collection, we proceed to construct separate test sets for tasks at different levels, thereby transforming knowledge-related tasks into language tasks driven by instructions, which facilitates the execution by large-scale models.In this section, we first present the design principles for each level of tasks, followed by specific approaches to constructing detailed instructions, and provide corresponding task examples.D.1 CONVERTING TO TASK FORMATThere are 7 tasks' test sets that need to be constructed from scratch ((1-1) High-Freq., (1-2) Low-Freq., (4-1) Encyclopedic Knowledge Creating and 4 Evolving Test tasks).These tasks require the reconstruction and quality control of the data based on our annotations.As for the other 12 tasks, we only focus on designing how they can be transformed into sequence tasks that can be solved by language models, considering the dataset construction methods provided in the original text as references.Overall, we follow two principles during the process of constructing instructions: a) Simplicity: We aim to describe the task objectives using the least amount of text, thus saving the model's in-context length; b) Standardization: We use special markers to identify all structured knowledge, assisting the model in quickly capturing the knowledge objectives.D.2 KNOWLEDGE MEMORIZATION TASKSKnowledge Memorization (KM) level primarily assesses the model's ability to retain knowledge triples.However, this format is not inherently suitable for large models.Therefore, we transform the triple prediction task into a question-answering task that considers 1-to-N relationships.For each type of relationship, we design specific templates to facilitate this transformation.INSTRUCTION: Please give answers to the following questions about knowledge.Note: If there are more than one answer, print them all and separate them with a semicolon (;).Please do not give anything other than the answers.QUESTION: What is the occupation of Wang Guozhen?ANSWER: poetTable5: The instruction and an example of Task 1-1 High-Freq.KM.INSTRUCTION: Please give answers to the following questions about knowledge.Note: If there is more than one answer, print them all and separate them with a semicolon (;).Please do not give anything other than the answers.QUESTION: Which country does White Hall Township belong to?ANSWER: United States of America Table6: The instruction and an example of Task 1-2 Low-Freq.KM.INSTRUCTION: Please give answers to the following questions about knowledge.Note: If there is more than one answer, print them all and separate them with a semicolon (;).Please do not give anything other than the answers.QUESTION: Will Messi still serve in Paris Saint-Germain?ANSWER: NoTable7: The instruction and an example of Task 1-3 ETM KM.D.4 KNOWLEDGE APPLYING TASKSKnowledge Applying (KA) level naturally involves multi-hop reasoning in the form of question answering, which is suitable for large-scale models to perform inference.One major challenge is that the contexts required for these reasoning steps are not consistent.One particular aspect is the KoRC task and ETA task, which assume that the model has access to a corresponding knowledge base.The original dataset for KoRC confirms Wikidata as the knowledge base.However, in the Evolving data, many pieces of knowledge cannot be directly found.Therefore, we construct a virtual knowledge base using the annotated triples and generate questions based on this knowledge base.Finally, we adopt the following task instructions:INSTRUCTION: Please answer the following question.QUERY: Jeremy Theobald and Christopher Nolan share what profession?ANSWER: Jeremy Theobald is an actor and producer.Christopher Nolan is a director, producer, and screenwriter.Therefore, they both share the profession of being a producer.So the answer is: producer.INSTRUCTION: Please answer the following question.QUERY: When did the first large winter carnival take place in the city where CIMI_FM is licensed to broadcast?ANSWER: CIMI_FM is licensed to broadcast in Quebec City.The first large winter carnival in Quebec City took place in 1894.So the answer is: 1894.Table18: The instruction and an example of Task 3-3 MuSiQue, KA.D.5 KNOWLEDGE CREATING TASKSThe Knowledge Creation (KC) level is particularly unique as each task involves two generation processes.Here, we present the process that considers generating subsequent knowledge, which is the most informative.However, for direct generation, the instruction can be replaced with "Complete the following generate" without specifying the "TRIPLETS" item.The example instructions of creating tasks are shown below:INSTRUCTION: Complete the following texts and make sure to contain all the events provided.EVENTS: ## Title: Death of Freddie Gray;### Known Events;#### Event Trigger: charges;##### Event Type: Judgment communication;##### Event Arguments; Agent: Marilyn Mosby; Patient: six police officers; Reason: the medical examiner's report ruled Gray's death a homicide;#### Event Trigger: stated;##### Event Type: Statement;##### Event Arguments; Speaker: The prosecutors; Message: they had probable cause to file criminal charges against the six police officers; Details: who were believed to be...GIVEN CONTEXT: ### TextTo Be Completed; On April 12, 2015, Freddie Carlos Gray, Jr., a 25-year-old black man, was arrested by the Baltimore Police Department for possessing what the police alleged was an illegal knife under Baltimore law.While being transported in a police van, Gray fell into a coma and was taken to a trauma center.Gray died on April 19, 2015 ; his death was ascribed to injuries to his spinal cord.On April 21, 2015, pending an investigation of the incident, six Baltimore police officers were suspended with pay...REFERENCE COMPLETION:On May 1, 2015, the Baltimore City State's Attorney, Marilyn Mosby, announced her office had filed charges against six police officers after the medical examiner's report ruled Gray's death a homicide.The prosecutors stated that they had probable cause to file criminal charges against the six police officers who were believed to be involved in his death...GIVEN CONTEXT: ### TextTo Be Completed; Details of a sordid rift between two prominent U. S. soccer families -one that included allegations of domestic abuse against men' s national team coach Gregg Berhalter and parental complaints about Gio Reyna' s playing time at the 2022 World Cup -continued to spill out Monday when the findings of an independent investigation were released...REFERENCE COMPLETION:Earnie Stewart left the job last month.Anthony Hudson, a World Cup assistant, is the interim coach.The next coach will begin preparing the U.S. team for the 2026 World Cup, which will take place in the United States, Mexico and Canada.Berhalter guided the United States for four years, leading a young squad to two regional championships and a place in the World Cup, where it finished second in group play and lost to the Netherlands in the round of 16...E DETAILS OF RESULT INFERENCEGiven the instructions and test sets for each task, we evalute a total of 21 models in the first season of KoLA.Here, we present some of the deployment environments of the models that participated in our first season, as well as some specific solutions implemented during the evaluations.E.1 DEPLOYMENT ENVIRONMENT AND MODEL INFORMATIONThe participating models in the evaluation include two types: closed-source models that return answers through API calls, and open-source models that are deployed directly for inference (with a temperature set to 0).Here, we primarily introduce the software and hardware environment used for deploying the models.We utilize the widely-used PyTorch and transformers library to load opensource models.The evaluation experiments are conducted on an Ubuntu 20.04.4 server equipped with 112 Intel Xeon(R) Platinum 8336C CPU cores, and graphic cards that contained 8 NVIDIA A100 SXM 80GB GPUs.Besides, The CUDA version is 11.4, the Python version is 3.10.0,the PyTorch version is 2.0.0 and the transformers version is 4.28.1.Table24presents the features of the selected LLMs in the first and second season.For the open-source models, we deploy them using their official versions, with particular emphasis on the HuggingFace versions.As for the closed-source models, we utilize the various model APIs available as of May 15, 2023.We also conduct thorough checks and re-inferencing in case of any network-related errors.E.2 SOLUTION FOR RUN-TIME EXCEPTIONSApart from issues such as user permissions and network environment when invoking the model API, the main challenges we encountered during the model evaluation process were limited input length for some models and output inconsistencies with the required format.Therefore, we have devised the following strategies to handle these exceptional cases during evaluation:Over-length Issue: Due to the length limitations of certain models, performing 5-shot zero-shot inference becomes challenging for tasks with lengthy instructions.Therefore, we have devised the following strategies to enable the models to produce desired outputs: a) Reduce the number of examples until the input-output length requirements are met; b) If reducing the number of examples to one still fails to meet the requirements of all cases, skip the non-compliant cases and treat them as 0; c) If a model skips a substantial number of examples (over 90%) on a particular task, consider it a failure on that task and record it as "-".A key observation is that due to the limited control over generation by many models, the scores obtained using EM are often lower, resulting in numerous cases where a score cannot be assigned.During the analysis of the results, we observe that some models, even without access to external resources, can achieve good performance on evolving task.However, this often requires a substantial scale and instruction tuning.This may be attributed to the fact that certain new knowledge can be inferred from existing knowledge, which also relies on the model's memorization.Table30: Absolute Performance of all metrics on task (1-1), (1-2), and (1-3), KM. 17.0 21.5 12.5 19.3 24.7 27.3 GPT-3.5-turbo9.8 18.7 18.0 22.1 13.8 18.9 GPT-3 curie v1 (6.7B) N/A 0.4 N/A 0.7 N/A N/A GPT-3 davinci v1 (175B) N/A 0.9 N/A 0.8 N/A N/A InstructGPT curie v1 (6.7B<em>)1.3 5.9 10.5 13.9 7.9 13.9 InstructGPT davinci v2 (175B</em>) 5.6 12.6 13.0 16.2 9.6 13.5 GLM (130B) N/A 2.8 N/A 4.4 N/A 0.5 GPT-4 17.1 24.2 20.8 26.5 21.0 26.0 ChatGLM (130B) 7.4 10.9 16.5 20.3 13.8 15.6Knowledge Understanding (KU).The results in this level are unexpected, as a significant amount of knowledge understanding relies on longer texts or generating highly structured content.Therefore, in complex tasks such as document-level relation extraction and event relation extraction, the performance of many models is not satisfactory.This aspect deserves further exploration.Table31: Absolute Performance of accuracy on COPEN (2-1), (2-2), (2-3), KU.Knowledge Applying (KA).In the evaluation of the KA level, a notable phenomenon is that knowledge graph (KG)-based reasoning question answering tasks are almost impossible to complete without the use of KG.This phenomenon is evident in the three tasks (3-4)-(3-6).Furthermore, due to the clear quality progression exhibited by multiple tasks in this layer, the performance of models generally follows a decreasing trend.Table35: Absolute Performance of F1-score on task (3-1), (3-2) and (3-3), KA.Table37: Absolute Performance of all metrics on KoRC (3-5) and ETA (3-6), KA.18.0 23.4 --LLaMa (65B) 2.0 5.6 N/A 8.4 Alpaca (7B) 23.0 29.1 4.1 15.4 J2-Jumbo-Instruct (178B<em>) 6.0 12.1 2.0 4.9 Cohere-command (52.4B) 28.0 38.1 36.7 41.8 GPT-3.5-turbo10.0 14.6 10.2 14.2 GPT-3 curie v1 (6.7B) 2.0 3.0 N/A 0.2 GPT-3 davinci v1 (175B) 3.0 5.3 N/A 1.5 InstructGPT curie v1 (6.7B</em>) 9.0 14.9 8.2 15.7 InstructGPT davinci v2 (175B*) 25.0 33.8 22.4 32.6 GLM (130B)22.0 28.5 22.0 28.5 GPT-4 33.0 44.3 36.7 43.5 ChatGLM (6B) 2.0 3.3 N/A 8.7 ChatGLM (130B) 17.0 20.3 N/A N/A Knowledge Creating (KC).Here, we present the scores of three sub-criteria used to calculate the overall score for each model.If only these scores are considered, it is observed that some well-regarded models such as GPT4 and GPT-3.5-turbodo not necessarily demonstrate superiority.Table40presents the absolute performance for all models on the rolling tasks with season 2's data.We find that most models still fail to get right results on 2-8 as the task require the model's ability on understanding the long context and complex structures.G.2 DETAILED RESULTS OF ALL TASKS FOR NEW MODELSKnowledge Memorization (KM).(2-3), KU.Dolly-v2 (12B) 5.4 14.9 8.0 1.3 5.0 2.0 1.7 10.0 2.9 N/A N/A N/A RedPajama-Instruct (7B) 3.2 6.9 4.4 N/A N/A N/A 1.1 10.0 1.9 1.2 10.0
Revisiting automatic evaluation of extractive summarization task: Can we do better than rouge?. Mousumi Akter, Naman Bansal, Shubhra Kanti, Karmaker , Findings of the Association for Computational Linguistics: ACL 2022. 2022</p>
<p>A multitask, multilingual. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, arxiv:2302.04023chatgpt on reasoning, hallucination, and interactivity. 2023arXiv preprint</p>
<p>Quantified reproducibility assessment of NLP results. Anya Belz, Maja Popovic, Simon Mille, 10.18653/v1/2022.acl-long.2Proceedings of ACL. ACL2022</p>
<p>Missing information, unresponsive authors, experimental flaws: The impossibility of assessing the reproducibility of previous human evaluations in NLP. Anya Belz, Craig Thomson, Ehud Reiter, The Fourth Workshop on Insights from Negative Results in NLP. 2023</p>
<p>GPT-NeoX-20B: An open-source autoregressive language model. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle Mcdonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach, 10.18653/v1/2022.bigscience-1.9Proceedings of BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language Models. BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language Models2022</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Proceedings of NIPS. NIPS2020</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with GPT-4. 2023arXiv preprint</p>
<p>KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base. Shulin Cao, Jiaxin Shi, Liangming Pan, Lunyiu Nie, Yutong Xiang, Lei Hou, Juanzi Li, Bin He, Hanwang Zhang, 10.18653/v1/2022.acl-long.422Proceedings of ACL. ACL2022</p>
<p>Toward gender-inclusive coreference resolution. Yang , Trista Cao, Hal Daumé, Proceedings of ACL. ACL2020</p>
<p>KGPT: Knowledge-grounded pre-training for data-to-text generation. Wenhu Chen, Yu Su, Xifeng Yan, William Yang, Wang , 10.18653/v1/2020.emnlp-main.697Proceedings of EMNLP. EMNLP2020</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Sharan Chowdhery, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, arxiv:2210.114162022arXiv preprint</p>
<p>Skills for the 21st century: teaching higher-order thinking. Robyn Collins, Curriculum &amp; Leadership Journal. 12142014</p>
<p>Together Computer. Releasing GPT-JT powered by open-source AI. 2022</p>
<p>Redpajama: An open source recipe to reproduce llama training dataset. 2023Together Computer</p>
<p>Free dolly: Introducing the world's first truly open instruction-tuned llm. Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, Reynold Xin, 2023</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of NAACL-HLT. NAACL-HLT2019</p>
<p>Time-aware language models as temporal knowledge bases. Bhuwan Dhingra, Jeremy R Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, William W Cohen, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Few-NERD: A few-shot named entity recognition dataset. Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, Zhiyuan Liu, 10.18653/v1/2021.acl-long.248Proceedings of ACL. ACL2021</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>History of standard scoring, notation, and summation of neuromuscular signs. a current survey and recommendation. J Peter, Christopher J Dyck, Donald Boes, Clark Mulder, Anthony J Millikan, P Windebank, Raul James B Dyck, Espinosa, Journal of the Peripheral Nervous System. 1022005</p>
<p>The art of artificial intelligence: Themes and case studies of knowledge engineering. Edward A Feigenbaum, Proceedings of IJCAI. IJCAIBoston19772</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arxiv:2302.041662023arXiv preprint</p>
<p>The pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arxiv:2101.000272021arXiv preprint</p>
<p>A knowledge-grounded neural conversation model. Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-Tau Yih, Michel Galley, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201832</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of ICLR, 2021. ICLR, 2021</p>
<p>Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa, 10.18653/v1/2020.coling-main.580Proceedings of COLING. COLING2020</p>
<p>C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, arxiv:2305.083222023arXiv preprint</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, ACM Computing Surveys. 55122023</p>
<p>XLORE2: large-scale cross-lingual knowledge graph construction and application. Hailong Jin, Chengjiang Li, Jing Zhang, Lei Hou, Juanzi Li, Peng Zhang, Data Intelligence. 112019</p>
<p>Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A Smith, Yejin Choi, Kentaro Inui, arXiv:2207.13332Realtime qa: What's the answer right now?. 2022arXiv preprint</p>
<p>A revision of bloom's taxonomy: An overview. Theory into practice. David R Krathwohl, 200241</p>
<p>RankGen: Improving text generation with large ranking models. Kalpesh Krishna, Yapei Chang, John Wieting, Mohit Iyyer, Proceedings of EMNLP. EMNLP2022</p>
<p>Evaluating computational creativity: An interdisciplinary tutorial. Carolyn Lamb, Daniel G Brown, Charles La Clarke, ACM Computing Surveys (CSUR). 5122018</p>
<p>Defining higher order thinking. Theory into practice. Arthur Lewis, David Smith, 199332</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, arxiv:2211.091102022arXiv preprint</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. 2004</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, arXiv:2304.09842Chameleon: Plug-and-play compositional reasoning with large language models. 2023arXiv preprint</p>
<p>Evaluating creativity in humans, computers, and collectively intelligent systems. Mary Lou, Maher , Proceedings of the 1st DESIRE Network Conference on Creativity and Innovation in Design. the 1st DESIRE Network Conference on Creativity and Innovation in Design2010</p>
<p>Augmented language models: a survey. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Celikyilmaz, arXiv:2302.078422023arXiv preprint</p>
<p>Purposely teaching for the promotion of higher-order thinking skills: A case of critical thinking. Barak Miri, Ben-Chaim David, Zoller Uri, 200737Research in science education</p>
<p>Reliable fidelity and diversity metrics for generative models. Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, Jaejun Yoo, International Conference on Machine Learning. PMLR2020</p>
<p>arxiv:2303.08774Gpt-4 technical report. 2023OpenAIarXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Proceedings of NIPS. NIPS2022</p>
<p>Kishore Sgopal Patro, Sahu Kumar, arXiv:1503.06462Normalization: A preprocessing stage. 2015arXiv preprint</p>
<p>COPEN: Probing conceptual knowledge in pre-trained language models. Hao Peng, Xiaozhi Wang, Shengding Hu, Hailong Jin, Lei Hou, Juanzi Li, Zhiyuan Liu, Qun Liu, Proceedings of EMNLP. EMNLP2022</p>
<p>Language Models as Knowledge Bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of EMNLP. EMNLP2019</p>
<p>KILT: a Benchmark for Knowledge Intensive Language Tasks. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, Sebastian Riedel, 10.18653/v1/2021.naacl-main.200Proceedings of NAACL-HLT. NAACL-HLT2021</p>
<p>MAUVE: measuring the gap between neural text and human text using divergence frontiers. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, Zaïd Harchaoui, Proceedings of NIPS. NIPS2021</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>A new open source flan 20b with ul2. 2022Google Research</p>
<p>Assessing generative models via precision and recall. S M Mehdi, Olivier Sajjadi, Mario Bachem, Olivier Lucic, Sylvain Bousquet, Gelly, Proceedings of NIPS. NIPS</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, arXiv:2211.05100A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage. Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, arxiv:2208.031882022arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arxiv:2206.046152022arXiv preprint</p>
<p>Announcing jurassic-2 and task-specific apis. Studio, 2023</p>
<p>ChapterBreak: A challenge dataset for long-range language models. Simeng Sun, Katherine Thai, Mohit Iyyer, 10.18653/v1/2022.naacl-main.271Proceedings of NAACL-HLT. NAACL-HLT2022</p>
<p>Challenging BIG-Bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arxiv:2210.092612022arXiv preprint</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. 2023</p>
<p>Unifying language learning paradigms. Yi Tay, Mostafa Dehghani, Xavier Vinh Q Tran, Dara Garcia, Tal Bahri, Huaixiu Schuster, Neil Steven Zheng, Donald Houlsby, Metzler, arXiv:2205.051312022arXiv preprint</p>
<p>Internlm: A multilingual language model with progressively enhanced capabilities. Internlm Team, 2023</p>
<p>A note on the evaluation of generative models. Lucas Theis, Aäron Van Den Oord, Matthias Bethge, Proceedings of ICLR. ICLR2016</p>
<p>Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arxiv:2302.139712023arXiv preprint</p>
<p>MuSiQue: Multihop questions via single-hop question composition. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, 10.1162/tacl_a_00475Transactions of the Association for Computational Linguistics. 102022</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Proceedings of EMNLP Workshop BlackboxNLP. EMNLP Workshop BlackboxNLP2018</p>
<p>SuperGLUE: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, Proceedings of NIPS. NIPS</p>
<p>Ben Wang, Aran Komatsuzaki, GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. 2021</p>
<p>MAVEN: A Massive General Domain Event Detection Dataset. Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, Jie Zhou, 10.18653/v1/2020.emnlp-main.129Proceedings of EMNLP. EMNLP2020</p>
<p>KEPLER: A unified model for knowledge embedding and pre-trained language representation. Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, Jian Tang, 10.1162/tacl_a_00360Transactions of the Association for Computational Linguistics. 92021a</p>
<p>MAVEN-ERE: A unified large-scale dataset for event coreference, temporal, causal, and subevent relation extraction. Xiaozhi Wang, Yulin Chen, Ning Ding, Hao Peng, Zimu Wang, Yankai Lin, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu, Peng Li, Jie Zhou, Proceedings of EMNLP. EMNLP2022</p>
<p>How far can camels go? exploring the state of instruction tuning on open resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Raghavi Khyathi, David Chandu, Kelsey Wadden, Noah A Macmillan, Iz Smith, Beltagy, arXiv:2306.047512023arXiv preprint</p>
<p>Cleve: Contrastive pre-training for event extraction. Ziqi Wang, Xiaozhi Wang, Xu Han, Yankai Lin, Lei Hou, Zhiyuan Liu, Peng Li, Juanzi Li, Jie Zhou, Proceedings of ACL, 2021b. ACL, 2021b</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of EMNLP. EMNLP2018</p>
<p>DocRED: A large-scale document-level relation extraction dataset. Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, Maosong Sun, 10.18653/v1/P19-1074Proceedings of ACL. ACL2019</p>
<p>KoRC: Knowledge Oriented Reading Comprehension Benchmark for Deep Text Understanding. Zijun Yao, Yantao Liu, Xin Lv, Shulin Cao, Jifan Yu, Juanzi Li, Lei Hou, Findings of ACL. 2023</p>
<p>FanfictionNLP: A text processing pipeline for fanfiction. Michael Yoder, Sopan Khosla, Qinlan Shen, Aakanksha Naik, Huiming Jin, Hariharan Muralidharan, Carolyn Rosé, 10.18653/v1/2021.nuse-1.2Proceedings of the Third Workshop on Narrative Understanding. the Third Workshop on Narrative Understanding2021</p>
<p>XDAI: A tuning-free framework for exploiting pre-trained language models in knowledge grounded dialogue generation. Jifan Yu, Xiaohan Zhang, Yifan Xu, Xuanyu Lei, Xinyu Guan, Jing Zhang, Lei Hou, Juanzi Li, Jie Tang, Proceedings of KDD. KDD2022</p>
<p>Glm-130b: An open bilingual pre-trained model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arxiv:2210.024142022arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.056852023arXiv preprint</p>
<p>Document-level relation extraction with adaptive thresholding and localized context pooling. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, arxiv:2304.06364Wenxuan Zhou, Kevin Huang, Tengyu Ma, and Jing Huang. 2023. 2021arXiv preprintProceedings of AAAI</p>            </div>
        </div>

    </div>
</body>
</html>
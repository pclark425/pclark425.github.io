<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1872 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1872</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1872</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-36.html">extraction-schema-36</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational or proxy metrics to make predictions or discoveries, and how those predictions compare to experimental or ground-truth validation results.</div>
                <p><strong>Paper ID:</strong> paper-278501824</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.07140v1.pdf" target="_blank">Rigorous Evaluation of Predictive Toxicity Models by Multi-Objective Optimization of Reference Compound Lists Using Genetic Algorithms</a></p>
                <p><strong>Paper Abstract:</strong> In pharmaceutical safety assessments, validation studies are essential for evaluating the predictive performance and reliability of alternative methods prior to regulatory acceptance. Typically, these studies utilize reference compound lists selected to balance multiple critical factors, including chemical structure, physicochemical properties, and toxicity profiles. However, the inherent trade-offs among these criteria complicate the independent optimization of each factor, necessitating a comprehensive multi-objective optimization approach. To address this challenge, we propose a novel multi-objective optimization framework employing a Genetic Algorithm (GA) to simultaneously maximize structural, physicochemical, and toxicity diversity of reference compound lists. Applying this methodology to existing validation study datasets, we demonstrated that GA-optimized compound lists achieved significantly higher overall diversity compared to randomly generated lists. Additionally, toxicity prediction models tested on GA-optimized compound lists exhibited notably lower predictive performance compared to random selections, confirming that these lists provide a rigorous and unbiased assessment environment. These findings emphasize the potential of our GA-based method to enhance the robustness and generalizability of toxicity prediction models. Overall, our approach provides valuable support for developing balanced and rigorous reference compound lists, potentially accelerating the adoption of alternative safety assessment methods by facilitating smoother regulatory validation processes.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1872.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1872.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational or proxy metrics to make predictions or discoveries, and how those predictions compare to experimental or ground-truth validation results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GA-based selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-objective Genetic Algorithm-based Reference Compound List Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DEAP/NSGA-II genetic-algorithm framework that optimizes reference compound lists simultaneously for structural, physicochemical, and toxicity diversity to produce challenging test sets for toxicity-prediction methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-objective GA (DEAP NSGA-II) for reference compound selection</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>toxicology / safety assessment / cheminformatics</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Composite diversity score (structural diversity + physicochemical diversity + toxicity diversity)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven empirical metrics (fingerprint- and descriptor-based diversity metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Structural diversity: sum of pairwise Tanimoto indices computed on ECFP4 fingerprints (lower sum = greater structural diversity). Physicochemical diversity: LogP and TPSA computed per compound, standardized by robust z-score, then summed Euclidean distances across all pairs. Toxicity diversity: for continuous toxicity values, within-list variance; for binary toxicity labels, a log-likelihood-based score rewarding balanced class counts. Composite score for representative-list selection uses weights 10:2:1 (structural:physicochemical:toxicity). Optimization performed with NSGA-II (DEAP v1.4.2) over a population of 100 and 1,000 generations.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric_name</strong></td>
                            <td>Curated toxicity labels and numerical toxicity values from large-scale databases and original validation studies (ICE, TDC, Tox21 and JaCVAM validation study measurements)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_validation_method</strong></td>
                            <td>Comparison against curated toxicity data compiled from public databases (ICE, TDC, Tox21) and values taken from original validation-study documents where database matches were absent; no new wet-lab experiments were performed in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>GA-optimized lists achieved 'significantly higher overall diversity' compared to randomly generated lists and original expert-selected lists (qualitative statement reported; exact numeric diversity scores are shown in figures but not explicitly listed in text). Convergence of diversity metrics observed across 1,000 generations.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Not applicable as a separate numeric performance measure for GA itself; GA outputs were used to create test sets which were then evaluated by ML models against database labels. The paper reports that model predictive metrics (AUC, F1, accuracy) were markedly lower on GA-derived Pareto-front lists versus random lists, but no numeric values for those metrics are provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_gap_measurement</strong></td>
                            <td>The paper reports a qualitative gap: predictive performance (AUC/F1/accuracy distributions) is 'markedly lower' for GA-optimized lists on the Pareto front compared to random lists; no numeric gap (e.g., ΔAUC) is provided in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performed</strong></td>
                            <td>computational only (optimization + retrospective evaluation against curated database labels)</td>
                        </tr>
                        <tr>
                            <td><strong>number_predictions_made</strong></td>
                            <td>Models were evaluated on 1,000 randomly generated lists and multiple GA-derived Pareto-front lists; exact number of compound-level predictions is not specified in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>number_experimentally_validated</strong></td>
                            <td>0 (no new experimental/wet-lab validations performed; ground truth drawn from existing curated databases and validation-study documents)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_novelty</strong></td>
                            <td>novel</td>
                        </tr>
                        <tr>
                            <td><strong>extrapolation_distance</strong></td>
                            <td>Authors compute Euclidean distance in 3D objective space (structural, physicochemical, toxicity diversity) from each list to the Pareto front and examine correlation with prediction difficulty; quantitative distances are used internally but specific numeric extrapolation distances and thresholds are not reported in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not quantified; paper notes experimental validation and compound cost/feasibility were not incorporated due to lack of quantitative data.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>The paper characterizes alternative (in silico) methods as 'gaining prominence' and mentions known optimistic bias in typical ML validation (random split) vs more realistic time-split approaches; computational optimization methods (GA/NSGA-II) are treated as established techniques for multi-objective problems.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_failure_modes</strong></td>
                            <td>GA-optimized lists highlight a failure mode of typical ML evaluation: random-split test sets can be overly optimistic, whereas carefully diversified GA-generated test sets reveal weaker predictive generalization; the paper also notes trade-offs between optimizing one diversity axis and losing diversity in others.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>computational dataset construction/optimization → retrospective computational evaluation (ML models tested against curated labels); no in vitro/in vivo cascade performed in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared to randomly generated lists and original expert-selected validation lists, GA-derived lists show higher diversity and result in lower ML predictive performance; quantitative comparative metrics are presented as distributions in figures but explicit numeric comparisons are not provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Paper cites domain factors such as interspecies variability in animal testing, need for assay reproducibility across labs, and that balancing toxicity potency, chemical structure, and physicochemical properties is critical and involves trade-offs that affect predictive evaluation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1872.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1872.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational or proxy metrics to make predictions or discoveries, and how those predictions compare to experimental or ground-truth validation results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XGBoost model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XGBoost toxicity prediction model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised tree-boosting machine-learning model (XGBoost) trained on curated toxicity datasets (excluding selected test lists) to predict toxicity labels and evaluate prediction difficulty on GA-optimized versus randomly sampled test lists.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>XGBoost (v2.1.4) toxicity classifier</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>toxicology / computational toxicology / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Predicted toxicity labels/scores and standard ML performance metrics (AUC, F1 score, accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML model (gradient-boosted decision trees)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>XGBoost models were trained on datasets comprising all compounds except those in a given test list (either GA-created or randomly sampled). Models produced toxicity predictions evaluated using AUC, F1, and accuracy on each test list. Specific input features and hyperparameters are not detailed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric_name</strong></td>
                            <td>Curated toxicity labels / numerical toxicity endpoints from ICE, TDC, Tox21, and original validation-study documents</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_validation_method</strong></td>
                            <td>Model predictions compared to existing curated toxicity labels from large-scale public databases and validation-study documents; no new experimental validation performed.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Reported qualitatively: predictive performance distributions (AUC, F1, accuracy) are shown in Figure 5; performance is 'markedly lower' when evaluated on GA-optimized Pareto-front lists versus random lists. Exact numeric values (median AUC, etc.) are not provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Not separately reported; ground truth consists of database labels used as evaluation targets. No wet-lab confirmation of model predictions was performed in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_gap_measurement</strong></td>
                            <td>The gap is described qualitatively: models evaluated with GA Pareto-front test sets show significantly reduced AUC/F1/accuracy distributions relative to random test sets; no explicit numeric gap (e.g., ΔAUC) reported in the manuscript text.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performed</strong></td>
                            <td>computational only (retrospective evaluation against curated database labels)</td>
                        </tr>
                        <tr>
                            <td><strong>number_predictions_made</strong></td>
                            <td>Models were evaluated across 1,000 randomly generated test lists and multiple GA-derived Pareto-front test lists; the exact count of per-compound predictions is not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>number_experimentally_validated</strong></td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_novelty</strong></td>
                            <td>not characterized</td>
                        </tr>
                        <tr>
                            <td><strong>extrapolation_distance</strong></td>
                            <td>Authors assessed correlation between a test list's distance from the Pareto front (3D objective space) and prediction difficulty; a relationship is reported qualitatively but quantitative correlation coefficients are not provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not discussed quantitatively; all evaluations were computational and used existing curated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>XGBoost is treated as a standard, well-established ML method for predictive tasks; the paper positions evaluation methodology (data split/selection) as the key determinant of measured performance.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_failure_modes</strong></td>
                            <td>Performance optimistic bias when using naive/random test splits; GA-optimized diversified test sets expose reduced generalization. No additional model failure modes (e.g., calibration, class imbalance) are quantified in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>computational training → computational test against curated labels (no further experimental cascade)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Baseline (randomly sampled test lists) yields higher measured performance; GA-derived Pareto-front test lists yield lower performance, indicating baseline (random split) is overly optimistic. No numeric delta is reported in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Paper emphasizes issues like time-dependent distribution shifts (time-split vs random-split), class imbalance in toxicity labels, and the importance of chemically diverse test sets for realistic performance estimation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1872.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1872.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational or proxy metrics to make predictions or discoveries, and how those predictions compare to experimental or ground-truth validation results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SIMPD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SIMPD (Simulated Medicinal Chemistry Project Data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GA-based algorithm (from related work) that generates dataset splits simulating temporal distinctions between early- and late-stage medicinal chemistry project compounds to provide realistic validation of ML models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SIMPD: an algorithm for generating simulated time splits for validating machine learning approaches</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SIMPD (GA for simulated time-splits)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>cheminformatics / machine learning validation in drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Simulated time-split realism metrics (not fully specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven simulation of dataset splits using GA</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Referenced as a related GA framework that uses multi-objective GA informed by lead-optimization project analyses to produce datasets that emulate temporal distribution shifts seen in prospective drug discovery. Specific metrics and implementation details are in the original SIMPD publication (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric_name</strong></td>
                            <td>Not applicable within this paper (SIMPD is cited as a methodology for better validating ML models using simulated time-splits rather than a direct ground-truth predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_validation_method</strong></td>
                            <td>Mentioned as producing more realistic validation splits compared to random-split; the original SIMPD paper contains the validation details.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Mentioned qualitatively as providing datasets that lead to more realistic (typically lower) estimates of prospective model performance relative to naive random-split validation; no numeric performance values provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explicit_gap_measurement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performed</strong></td>
                            <td>mention only (original SIMPD paper contains validation)</td>
                        </tr>
                        <tr>
                            <td><strong>number_predictions_made</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_experimentally_validated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_novelty</strong></td>
                            <td>not characterized</td>
                        </tr>
                        <tr>
                            <td><strong>extrapolation_distance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Presented as an established approach to simulate realistic time-based dataset shifts for validation purposes in medicinal chemistry ML tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_failure_modes</strong></td>
                            <td>Not discussed here; SIMPD is cited as addressing optimistic bias from random-split validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Referenced as part of improved computational validation methodology (no experimental cascade described here).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Cited as producing more realistic (less optimistic) validation results compared to random-split cross-validation, per the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Designed to reflect temporal changes in compound properties during medicinal chemistry projects which affect ML generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SIMPD: an algorithm for generating simulated time splits for validating machine learning approaches <em>(Rating: 2)</em></li>
                <li>Time-split cross-validation as a method for estimating the goodness of prospective prediction <em>(Rating: 2)</em></li>
                <li>Investigation of a data split strategy involving the time axis in adverse event prediction using machine learning <em>(Rating: 2)</em></li>
                <li>An integrated chemical environment with tools for chemical safety testing <em>(Rating: 1)</em></li>
                <li>The Tox21 10K compound library: Collaborative chemistry advancing toxicology <em>(Rating: 1)</em></li>
                <li>A Scalable Tree Boosting System <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1872",
    "paper_id": "paper-278501824",
    "extraction_schema_id": "extraction-schema-36",
    "extracted_data": [
        {
            "name_short": "GA-based selection",
            "name_full": "Multi-objective Genetic Algorithm-based Reference Compound List Optimization",
            "brief_description": "A DEAP/NSGA-II genetic-algorithm framework that optimizes reference compound lists simultaneously for structural, physicochemical, and toxicity diversity to produce challenging test sets for toxicity-prediction methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Multi-objective GA (DEAP NSGA-II) for reference compound selection",
            "domain": "toxicology / safety assessment / cheminformatics",
            "proxy_metric_name": "Composite diversity score (structural diversity + physicochemical diversity + toxicity diversity)",
            "proxy_metric_type": "data-driven empirical metrics (fingerprint- and descriptor-based diversity metrics)",
            "proxy_metric_description": "Structural diversity: sum of pairwise Tanimoto indices computed on ECFP4 fingerprints (lower sum = greater structural diversity). Physicochemical diversity: LogP and TPSA computed per compound, standardized by robust z-score, then summed Euclidean distances across all pairs. Toxicity diversity: for continuous toxicity values, within-list variance; for binary toxicity labels, a log-likelihood-based score rewarding balanced class counts. Composite score for representative-list selection uses weights 10:2:1 (structural:physicochemical:toxicity). Optimization performed with NSGA-II (DEAP v1.4.2) over a population of 100 and 1,000 generations.",
            "ground_truth_metric_name": "Curated toxicity labels and numerical toxicity values from large-scale databases and original validation studies (ICE, TDC, Tox21 and JaCVAM validation study measurements)",
            "ground_truth_validation_method": "Comparison against curated toxicity data compiled from public databases (ICE, TDC, Tox21) and values taken from original validation-study documents where database matches were absent; no new wet-lab experiments were performed in this study.",
            "proxy_performance": "GA-optimized lists achieved 'significantly higher overall diversity' compared to randomly generated lists and original expert-selected lists (qualitative statement reported; exact numeric diversity scores are shown in figures but not explicitly listed in text). Convergence of diversity metrics observed across 1,000 generations.",
            "ground_truth_performance": "Not applicable as a separate numeric performance measure for GA itself; GA outputs were used to create test sets which were then evaluated by ML models against database labels. The paper reports that model predictive metrics (AUC, F1, accuracy) were markedly lower on GA-derived Pareto-front lists versus random lists, but no numeric values for those metrics are provided in the text.",
            "explicit_gap_measurement": "The paper reports a qualitative gap: predictive performance (AUC/F1/accuracy distributions) is 'markedly lower' for GA-optimized lists on the Pareto front compared to random lists; no numeric gap (e.g., ΔAUC) is provided in the main text.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "has_both_proxy_and_ground_truth": true,
            "validation_performed": "computational only (optimization + retrospective evaluation against curated database labels)",
            "number_predictions_made": "Models were evaluated on 1,000 randomly generated lists and multiple GA-derived Pareto-front lists; exact number of compound-level predictions is not specified in the text.",
            "number_experimentally_validated": "0 (no new experimental/wet-lab validations performed; ground truth drawn from existing curated databases and validation-study documents)",
            "discovery_novelty": "novel",
            "extrapolation_distance": "Authors compute Euclidean distance in 3D objective space (structural, physicochemical, toxicity diversity) from each list to the Pareto front and examine correlation with prediction difficulty; quantitative distances are used internally but specific numeric extrapolation distances and thresholds are not reported in the text.",
            "proxy_bias_correction": false,
            "proxy_bias_correction_method": null,
            "validation_cost_time": "Not quantified; paper notes experimental validation and compound cost/feasibility were not incorporated due to lack of quantitative data.",
            "domain_maturity": "The paper characterizes alternative (in silico) methods as 'gaining prominence' and mentions known optimistic bias in typical ML validation (random split) vs more realistic time-split approaches; computational optimization methods (GA/NSGA-II) are treated as established techniques for multi-objective problems.",
            "proxy_failure_modes": "GA-optimized lists highlight a failure mode of typical ML evaluation: random-split test sets can be overly optimistic, whereas carefully diversified GA-generated test sets reveal weaker predictive generalization; the paper also notes trade-offs between optimizing one diversity axis and losing diversity in others.",
            "uncertainty_quantification": null,
            "uncertainty_calibration": null,
            "multiple_proxy_types": true,
            "validation_cascade": "computational dataset construction/optimization → retrospective computational evaluation (ML models tested against curated labels); no in vitro/in vivo cascade performed in this study.",
            "comparison_to_baseline": "Compared to randomly generated lists and original expert-selected validation lists, GA-derived lists show higher diversity and result in lower ML predictive performance; quantitative comparative metrics are presented as distributions in figures but explicit numeric comparisons are not provided in the text.",
            "domain_specific_factors": "Paper cites domain factors such as interspecies variability in animal testing, need for assay reproducibility across labs, and that balancing toxicity potency, chemical structure, and physicochemical properties is critical and involves trade-offs that affect predictive evaluation.",
            "uuid": "e1872.0"
        },
        {
            "name_short": "XGBoost model",
            "name_full": "XGBoost toxicity prediction model",
            "brief_description": "A supervised tree-boosting machine-learning model (XGBoost) trained on curated toxicity datasets (excluding selected test lists) to predict toxicity labels and evaluate prediction difficulty on GA-optimized versus randomly sampled test lists.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "XGBoost (v2.1.4) toxicity classifier",
            "domain": "toxicology / computational toxicology / machine learning",
            "proxy_metric_name": "Predicted toxicity labels/scores and standard ML performance metrics (AUC, F1 score, accuracy)",
            "proxy_metric_type": "data-driven ML model (gradient-boosted decision trees)",
            "proxy_metric_description": "XGBoost models were trained on datasets comprising all compounds except those in a given test list (either GA-created or randomly sampled). Models produced toxicity predictions evaluated using AUC, F1, and accuracy on each test list. Specific input features and hyperparameters are not detailed in the paper.",
            "ground_truth_metric_name": "Curated toxicity labels / numerical toxicity endpoints from ICE, TDC, Tox21, and original validation-study documents",
            "ground_truth_validation_method": "Model predictions compared to existing curated toxicity labels from large-scale public databases and validation-study documents; no new experimental validation performed.",
            "proxy_performance": "Reported qualitatively: predictive performance distributions (AUC, F1, accuracy) are shown in Figure 5; performance is 'markedly lower' when evaluated on GA-optimized Pareto-front lists versus random lists. Exact numeric values (median AUC, etc.) are not provided in the text.",
            "ground_truth_performance": "Not separately reported; ground truth consists of database labels used as evaluation targets. No wet-lab confirmation of model predictions was performed in this study.",
            "explicit_gap_measurement": "The gap is described qualitatively: models evaluated with GA Pareto-front test sets show significantly reduced AUC/F1/accuracy distributions relative to random test sets; no explicit numeric gap (e.g., ΔAUC) reported in the manuscript text.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "has_both_proxy_and_ground_truth": true,
            "validation_performed": "computational only (retrospective evaluation against curated database labels)",
            "number_predictions_made": "Models were evaluated across 1,000 randomly generated test lists and multiple GA-derived Pareto-front test lists; the exact count of per-compound predictions is not specified.",
            "number_experimentally_validated": "0",
            "discovery_novelty": "not characterized",
            "extrapolation_distance": "Authors assessed correlation between a test list's distance from the Pareto front (3D objective space) and prediction difficulty; a relationship is reported qualitatively but quantitative correlation coefficients are not provided in the text.",
            "proxy_bias_correction": false,
            "proxy_bias_correction_method": null,
            "validation_cost_time": "Not discussed quantitatively; all evaluations were computational and used existing curated datasets.",
            "domain_maturity": "XGBoost is treated as a standard, well-established ML method for predictive tasks; the paper positions evaluation methodology (data split/selection) as the key determinant of measured performance.",
            "proxy_failure_modes": "Performance optimistic bias when using naive/random test splits; GA-optimized diversified test sets expose reduced generalization. No additional model failure modes (e.g., calibration, class imbalance) are quantified in the text.",
            "uncertainty_quantification": false,
            "uncertainty_calibration": null,
            "multiple_proxy_types": false,
            "validation_cascade": "computational training → computational test against curated labels (no further experimental cascade)",
            "comparison_to_baseline": "Baseline (randomly sampled test lists) yields higher measured performance; GA-derived Pareto-front test lists yield lower performance, indicating baseline (random split) is overly optimistic. No numeric delta is reported in the text.",
            "domain_specific_factors": "Paper emphasizes issues like time-dependent distribution shifts (time-split vs random-split), class imbalance in toxicity labels, and the importance of chemically diverse test sets for realistic performance estimation.",
            "uuid": "e1872.1"
        },
        {
            "name_short": "SIMPD",
            "name_full": "SIMPD (Simulated Medicinal Chemistry Project Data)",
            "brief_description": "A GA-based algorithm (from related work) that generates dataset splits simulating temporal distinctions between early- and late-stage medicinal chemistry project compounds to provide realistic validation of ML models.",
            "citation_title": "SIMPD: an algorithm for generating simulated time splits for validating machine learning approaches",
            "mention_or_use": "mention",
            "system_name": "SIMPD (GA for simulated time-splits)",
            "domain": "cheminformatics / machine learning validation in drug discovery",
            "proxy_metric_name": "Simulated time-split realism metrics (not fully specified in this paper)",
            "proxy_metric_type": "data-driven simulation of dataset splits using GA",
            "proxy_metric_description": "Referenced as a related GA framework that uses multi-objective GA informed by lead-optimization project analyses to produce datasets that emulate temporal distribution shifts seen in prospective drug discovery. Specific metrics and implementation details are in the original SIMPD publication (cited).",
            "ground_truth_metric_name": "Not applicable within this paper (SIMPD is cited as a methodology for better validating ML models using simulated time-splits rather than a direct ground-truth predictor)",
            "ground_truth_validation_method": "Mentioned as producing more realistic validation splits compared to random-split; the original SIMPD paper contains the validation details.",
            "proxy_performance": "Mentioned qualitatively as providing datasets that lead to more realistic (typically lower) estimates of prospective model performance relative to naive random-split validation; no numeric performance values provided here.",
            "ground_truth_performance": null,
            "explicit_gap_measurement": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "has_both_proxy_and_ground_truth": false,
            "validation_performed": "mention only (original SIMPD paper contains validation)",
            "number_predictions_made": null,
            "number_experimentally_validated": null,
            "discovery_novelty": "not characterized",
            "extrapolation_distance": null,
            "proxy_bias_correction": null,
            "proxy_bias_correction_method": null,
            "validation_cost_time": null,
            "domain_maturity": "Presented as an established approach to simulate realistic time-based dataset shifts for validation purposes in medicinal chemistry ML tasks.",
            "proxy_failure_modes": "Not discussed here; SIMPD is cited as addressing optimistic bias from random-split validation.",
            "uncertainty_quantification": null,
            "uncertainty_calibration": null,
            "multiple_proxy_types": null,
            "validation_cascade": "Referenced as part of improved computational validation methodology (no experimental cascade described here).",
            "comparison_to_baseline": "Cited as producing more realistic (less optimistic) validation results compared to random-split cross-validation, per the referenced work.",
            "domain_specific_factors": "Designed to reflect temporal changes in compound properties during medicinal chemistry projects which affect ML generalization.",
            "uuid": "e1872.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SIMPD: an algorithm for generating simulated time splits for validating machine learning approaches",
            "rating": 2
        },
        {
            "paper_title": "Time-split cross-validation as a method for estimating the goodness of prospective prediction",
            "rating": 2
        },
        {
            "paper_title": "Investigation of a data split strategy involving the time axis in adverse event prediction using machine learning",
            "rating": 2
        },
        {
            "paper_title": "An integrated chemical environment with tools for chemical safety testing",
            "rating": 1
        },
        {
            "paper_title": "The Tox21 10K compound library: Collaborative chemistry advancing toxicology",
            "rating": 1
        },
        {
            "paper_title": "A Scalable Tree Boosting System",
            "rating": 1
        }
    ],
    "cost": 0.012406499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Rigorous Evaluation of Predictive Toxicity Models by Multi-Objective Optimization of Reference Compound Lists Using Genetic Algorithms</p>
<p>Yohei Ohto 
Graduate School of Pharmaceutical Sciences
Laboratory of Molecular Pharmacokinetics
The University of Tokyo
7-3-1 HongoBunkyo, TokyoJapan</p>
<p>Tadahaya Mizuno tadahaya@gmail.com 
Graduate School of Pharmaceutical Sciences
Laboratory of Molecular Pharmacokinetics
The University of Tokyo
7-3-1 HongoBunkyo, TokyoJapan</p>
<p>Yasuhiro Yoshikai 
Graduate School of Pharmaceutical Sciences
Laboratory of Molecular Pharmacokinetics
The University of Tokyo
7-3-1 HongoBunkyo, TokyoJapan</p>
<p>Hiromi Fujimoto 
Graduate School of Pharmaceutical Sciences
Laboratory of Molecular Pharmacokinetics
The University of Tokyo
7-3-1 HongoBunkyo, TokyoJapan</p>
<p>Hiroyuki Kusuhara 
Graduate School of Pharmaceutical Sciences
Laboratory of Molecular Pharmacokinetics
The University of Tokyo
7-3-1 HongoBunkyo, TokyoJapan</p>
<p>Rigorous Evaluation of Predictive Toxicity Models by Multi-Objective Optimization of Reference Compound Lists Using Genetic Algorithms
ECE5094EB9A40CC99530E58EEB0AF848validation studyoperations researchmulti-objective optimization problemsafety assessmentalternative methods</p>
<p>Introduction</p>
<p>In pharmaceutical development, evaluating the safety of candidate compounds for human use is essential 1 .Animal testing has played a central role in safety assessments while ethical issues, high costs, extended testing periods, and interspecies variability limiting predictive accuracy In pharmaceutical safety assessments, validation studies are essential for evaluating the predictive performance and reliability of alternative methods prior to regulatory acceptance.Typically, these studies utilize reference compound lists selected to balance multiple critical factors, including chemical structure, physicochemical properties, and toxicity profiles.However, the inherent trade-offs among these criteria complicate the independent optimization of each factor, necessitating a comprehensive multi-objective optimization approach.To address this challenge, we propose a novel multi-objective optimization framework employing a Genetic Algorithm (GA) to simultaneously maximize structural, physicochemical, and toxicity diversity of reference compound lists.Applying this methodology to existing validation study datasets, we demonstrated that GA-optimized compound lists achieved significantly higher overall diversity compared to randomly generated lists.Additionally, toxicity prediction models tested on GA-optimized compound lists exhibited notably lower predictive performance compared to random selections, confirming that these lists provide a rigorous and unbiased assessment environment.These findings emphasize the potential of our GA-based method to enhance the robustness and generalizability of toxicity prediction models.Overall, our approach provides valuable support for developing balanced and rigorous reference compound lists, potentially accelerating the adoption of alternative safety assessment methods by facilitating smoother regulatory validation processes.have become major concerns 2,3 .Driven by these issues and the growing emphasis on the 3Rs principle (Replacement, Reduction, and Refinement), alternative safety assessment methodsalso known as new approach methods (NAMs)-including in silico prediction models 4,5 and in vitro assays [6][7][8] have gained prominence.</p>
<p>To implement these methods in regulatory frameworks, validation studies are required to objectively assess their predictive performance and reliability, ensuring their fitness for purpose [9][10][11] .These validation processes typically involve multiple laboratories to confirm method reproducibility, including evaluating inter-laboratory variability using standardized reference compound lists.</p>
<p>Appropriate validation necessitates using balanced and diverse compound lists regarding toxicity potency, chemical structures, and physicochemical properties 16 .Traditionally, reference compounds for validation are selected by experts based on these criteria, often from databases aligned with the Globally Harmonized System of Classification and Labelling of Chemicals (GHS). 13,14However, optimizing for one criterion (e.g., toxicity distribution) frequently introduces trade-offs with other criteria (e.g., structural diversity).Consequently, a comprehensive multi-objective optimization approach is required rather than optimizing individual factors independently.</p>
<p>To address this challenge, this study aims to investigate the effectiveness of formulating the reference compound selection task as a multi-objective optimization problem and solving it using (GA) 15 , a widely used method in operations research.GA applies biological evolutionary principles, such as natural selection, mutation, and crossover, to optimize complex combinatorial problems efficiently.Specifically, we applied GA to optimize reference compound lists for validation studies.comparing these with established reference lists from previous validation studies to evaluate compound diversity across multiple objectives.Additionally, testing in silico toxicity prediction models on GA-optimized lists resulted in lower predictive performance compared to randomly selected lists, suggesting that GA-derived compound lists enable a rigorous assessment of model robustness.We believe that the application of GA supports reference compound selection and contributes to robust validation of alternative safety assessment methods.</p>
<p>Related Works</p>
<p>Genetic Algorithms for Multi-Objective Optimization</p>
<p>GA simulates biological evolution principles-such as natural selection, mutation, and crossover-to efficiently address complex combinatorial optimization problems, even with nonlinear or discontinuous objective functions.A notable strength of GA is its capability to derive Pareto-optimal solutions, where enhancing one objective inherently involves compromising another, thereby effectively navigating trade-offs inherent in multi-objective optimization scenarios 16 .Due to these characteristics, GA has been widely applied in various domains, including combinatorial optimization, machine learning 17 , structural design 18 , and resource allocation. 19</p>
<p>Optimistic Biases and Rigorous Alternatives in Machine Learning for</p>
<p>Pharmaceutical Sciences</p>
<p>When assessing machine learning model performance, particularly in drug property prediction, it is crucial to ensure that evaluations closely mimic real-world scenarios.In practice, predictive models are typically trained on existing data and subsequently applied to newly developed, future compounds.However, the commonly employed random-split cross-validation method often yields overly optimistic performance estimates compared to the more realistic time-split validation approach, which emulates practical application conditions 20,21 .Time-split validation better reflects real-world challenges, accounting for temporal distribution shifts and avoiding information leakage.</p>
<p>In the context of rigorous performance evaluation, particularly in pharmaceutical and medicinal chemistry domains, the SIMPD (Simulated Medicinal Chemistry Project Data) initiative provides a relevant example 22 .SIMPD utilizes GA to generate datasets that realistically represent temporal distinctions between early-stage and late-stage compounds observed in drug discovery projects.The multi-objective GA framework employed by SIMPD, informed by extensive analysis of lead optimization projects at Novartis Institutes for BioMedical Research (NIBR), leverages ChEMBL bioactivity data 23 to create publicly available datasets.These datasets are specifically designed to rigorously evaluate the predictive performance of machine learning models in a manner reflective of actual pharmaceutical development processes.</p>
<p>Methods</p>
<p>Validation Studies of Alternative Methods for Pharmaceutical Safety</p>
<p>Assessment</p>
<p>The present study utilizes reference compound lists employed in validation studies by the Japanese Center for the Validation of Alternative Methods (JaCVAM) 24 as authorized references.The specific validation studies employed in this research are summarized in Figure 1.These are selected primarily due to the availability of large-scale chemical toxicity databases, which are essential for the development and evaluation of such selection methodologies.</p>
<p>Large-Scale Compound Toxicity Databases</p>
<p>We employed data from large-scale toxicity databases ICE 25 , TDC 26 , and Tox21 27 . 25E, developed by National Toxicology Program Interagency Center for the Evaluation of Alternative Toxicological Methods (NICEATM) within the U.S. National Toxicology Program (NTP), provides computational toxicology resources including curated datasets, chemical structures, and toxicity prediction models.26 TDC is an open-source data platform designed to support drug discovery and toxicity prediction.It integrates multiple public resources, including Tox21 27 , SIDER 29 , and ChEMBL, offering benchmark datasets for various biomedical tasks.</p>
<p>ICE (Integrated Chemical Environment)</p>
<p>TDC (Therapeutics Data Commons)</p>
<p>Tox21</p>
<p>Tox21, a collaboration involving the U.S. Environmental Protection Agency (EPA), National Institutes of Health (NIH), and Food and Drug Administration (FDA), conducts high-throughput in vitro screening of chemicals to replace animal testing.</p>
<p>Algorithm</p>
<p>Selection of Toxicity Assay Validation Studies and Large-Scale Compound Toxicity Databases</p>
<p>For each compound listed in the toxicity test validation studies, the CAS Registry Number (CAS-RN) was used to retrieve the corresponding SMILES using the PubChem 30 API .These SMILES were then converted into isomeric SMILES using RDKit 31 .</p>
<p>Compounds from large-scale toxicity databases were matched directly by their isomeric SMILES to extract toxicity data.If unavailable, toxicity profiles were obtained from the original validation study documents using CAS-RN.All processes utilized RDKit module version 2024.09.5.</p>
<p>If the isomeric SMILES of a compound from the validation studies matched an entry in a toxicity database, the corresponding information from the database was used.Otherwise, toxicity data from the original validation studies were adopted.Numerical toxicity values and binary labels (toxic/non-toxic) followed conventions from respective large-scale databases, if available; otherwise, values from the validation studies were used.</p>
<p>Compound List Optimization Using Genetic Algorithm</p>
<p>This study investigates the effectiveness of formulating the selection of reference compounds for validation of alternative methods as a multi-objective optimization problem solved via a genetic algorithm (GA).Optimization aims to maximize diversity across chemical structures, physicochemical properties, and toxicity profiles within a constrained set size, thereby enhancing the applicability and robustness of alternative methods.Specifically, we generated candidate reference compound lists, matching the size of original validation studies, from databases containing relevant toxicity data while excluding compounds previously used.The GA was implemented using the DEAP (Distributed Evolutionary Algorithms in Python) 32 library in Python.The version of DEAP used here is 1.4.2.</p>
<p>Initial Population Generation</p>
<p>Reference compound lists (individuals) were randomly sampled from the available compound pool to form an initial population of the specified size.</p>
<p>Crossover</p>
<p>New compound lists were generated by exchanging a proportion of compounds between two parent lists.</p>
<p>Mutation</p>
<p>Within a given compound list, one or more compounds were randomly replaced with others to introduce variability.</p>
<p>Fitness Evaluation</p>
<p>Each compound list was evaluated using the following objective functions:</p>
<p>Evaluation of Chemical Structures Diversity</p>
<p>For every pair of compounds within a list, the Tanimoto index of their ECFP4 fingerprints 33 was calculated.The sum of these pairwise Tanimoto indices was then used as the structural diversity metric.Lower Tanimoto indices (i.e., less structural similarity) therefore yield lower diversity scores, reflecting higher diversity in terms of structural dissimilarity.</p>
<p>Evaluation of Physicochemical Property Diversity</p>
<p>For each compound, the LogP value 34 (a dimensionless index representing the compound's lipophilicity and hydrophobicity; the n-octanol/water partition coefficient is the most commonly used) and the TPSA value 35 (Topological Polar Surface Area; an estimate of the surface area occupied by polar atoms and their bound hydrogen atoms on the molecular surface) were calculated.</p>
<p>( ( , (
・ ( &gt; → ( , ・ ( &gt; → ( , = ( !" # $ % ( &amp; − !" # $ % ( &amp;( ) + !" # $+ ,-( &amp; − !" # $+ ,-( &amp;( ) ..0
For all pairs of compounds in a given list, these two descriptors were standardized using the Robust z-score method 36 .The sum of the Euclidean distances between all compound pairs in the standardized descriptor space was then computed.A larger total distance indicates greater diversity in physicochemical properties, reflecting lower intra-list similarity and resulting in a higher evaluation score for this metric.</p>
<p>Evaluation of Toxicity Diversity</p>
<p>Toxicity diversity within a compound list was assessed as follows:</p>
<p>When toxicity values were expressed as continuous variables (e.g., LD₅₀ for acute toxicity), the variance of these values within the list was used as the metric.</p>
<p>When toxicity was indicated as a binary variable (e.g., toxic vs. non-toxic), diversity was evaluated using a log-likelihood-based metric designed to reward balanced class distributions.For a list with 123 4 56 toxic and 6784 56 non-toxic compounds (total = 123 4 56 + 6784 56 ), this metric assigns a higher score to lists with class counts ( 123 4 56 , 6784 56 ) closer to ) , reflecting higher diversity in terms of class balance.The diversity score is calculated as:</p>
<p>Higher variance or a more balanced class distribution (in the binary case) led to a higher evaluation score.</p>
<p>Selection</p>
<p>Individuals for the next generation were selected from the current population (or combined parent and offspring population, depending on the NSGA-II 37 (Non-dominated Sorting Genetic Algorithm II) variant used) using the NSGA-II selection mechanism.This process involves nondominated sorting to rank individuals based on their Pareto dominance and crowding distance calculation to maintain diversity within non-dominated fronts.Individuals from lower nondominated ranks were prioritized, and within the same rank, individuals with higher crowding distance were favored, until the population size of 100 was reached.</p>
<p>Termination of Optimization</p>
<p>The evolutionary process, consisting of selection (using NSGA-II), crossover, and mutation (referencing corrected section numbers), was repeated for 1,000 generations.In the final generation's population, the Pareto front (set of non-dominated solutions) was identified and extracted using DEAP's tools.ParetoFront() function.This Pareto front represents the set of solutions achieving the best trade-offs among the three objective functions within the optimized population.For evaluation and comparison, representative compound lists were selected from the final Pareto front obtained from the genetic algorithm optimization.To select a single representative list that balances the three-diversity metrics, a composite score was calculated for each list on the Pareto front using the following weights: representative optimized list for subsequent analyses (e.g., comparison with the original validation list, evaluation of prediction difficulty).</p>
<p>Evaluation of Reference Compound Selection with GA</p>
<p>To assess the effectiveness of the compound lists generated by the optimization algorithm, the following two evaluation approaches were used:</p>
<p>Evaluation Based on Optimization Function Scores</p>
<p>The compound lists generated by the GA were compared with randomly generated compound lists and the original lists used in the validation studies.The comparison was conducted using the same three objective functions employed during optimization: structural diversity, physicochemical property diversity, and toxicity diversity.This allowed for a direct evaluation of how well each compound list performed according to the defined optimization criteria.</p>
<p>Evaluation of Prediction Difficulty of Reference Compounds for Toxicity Assays</p>
<p>To investigate the predictive challenge posed by each compound list, we compared the toxicity prediction difficulty between the genetic algorithm (GA)-generated compound lists and randomly sampled lists.Note that randomly sampled lists often serve as random splits commonly used in evaluating in silico prediction methods.Using XGBoost 38 , models were trained on datasets comprising all compounds excluding those in the specific test lists (GA-generated or random).Prediction performance metrics (e.g., accuracy, AUC) were then compared to evaluate relative difficulty.The version of XGBoost used here is 2.1.4.Furthermore, to investigate the impact of optimization level on prediction difficulty, we examined the correlation between a compound list's distance from the Pareto front and its corresponding toxicity prediction score.The distance from the Pareto front for each compound list was calculated as the three-dimensional Euclidean distance in the objective space (defined by the Structural, Physicochemical, and Toxicity Diversity scores) to the nearest point on the final Pareto front.</p>
<p>Data and Code Availability</p>
<p>The data, code, and results generated in this study are available in the following GitHub repository: https://github.com/mizuno-group/multi-objective-optimization</p>
<p>Results and Discussion</p>
<p>In this section, we present the results and discussion mainly using Validation Assay 09_02, specifically the ER-STTA assay for detecting antagonist activity in endocrine disruption screening. 39Results for the 09_02 agonist assay and the Validation Assay 07_02 (3T3 Neutral Red Uptake Cytotoxicity Assay for Acute Oral Toxicity Testing 40 ), specifically are provided in the Supplementary Figures, while the full results for all other assays are available in the GitHub repository mentioned in Section 3.5.</p>
<p>Analysis of Reference Compound Properties Used in Validation Studies</p>
<p>We first aimed to characterize the diversity of reference compound lists utilized in actual validation studies.These lists, intentionally selected by domain experts, theoretically balance structural, physicochemical, and toxicity diversity to ensure comprehensive assay validation.Figure 1 compares diversity metrics-structural, physicochemical, and toxicity diversity-of the ER-STTA assay reference compound list against distributions from 10,000 randomly generated compound lists derived from a corresponding toxicity dataset.Note that we use the term list like "reference compound list" for ease of understanding, although it is mathematically a set, and each score is derived from a set of compounds.</p>
<p>We observed that reference compounds selected for validation exhibited notably higher internal structural similarity compared to random selections.This intentional similarity likely would aid maintaining consistent experimental conditions and interpretation of biological responses.Similar outlier patterns in random distributions were also observed across other validation studies, reinforcing the notion that expert-selected reference lists prioritize specific attributes to meet assay-specific objectives.</p>
<p>Genetic Algorithm Optimization Process</p>
<p>To verify the effectiveness and convergence of GA optimization, we tracked changes in diversity metrics across generations.Figure 3 illustrates the progression of average diversity scores (structural, physicochemical, and toxicity) for each GA generation.Clear and consistent improvements in all diversity metrics confirm that the GA successfully guided the optimization toward diverse compound sets.To visualize the optimization trajectory, Figure 4 compares initial-generation (randomly generated) and final-generation (GA-optimized) compound lists within the objective space.The figure demonstrates substantial and directional shifts in diversity scores toward the desired optimal region, indicating successful convergence of the algorithm.A representative compound list was then selected from the final Pareto front based on a composite score (using weights of 10:2:1 for structural, physicochemical, and toxicity diversity, respectively, detailed in Section 2.3.5).This representative list was used for subsequent comparative analyses.</p>
<p>Comparative Analysis of GA-Optimized and Original Validation Compound Lists</p>
<p>Next, we compared the optimized compound lists derived from GA optimization against the original expert-selected validation list and the distribution of randomly generated compound lists.Figures 5A, 5B, and 5C clearly demonstrate that the GA-derived lists consistently exhibited higher diversity, effectively distinguishing them from randomly generated and original validation lists.Additionally, visual inspection of the representative GA-optimized list (Figure 4D) revealed diverse compound structures, notably broader in scope than the original validation list, highlighting the advantage of systematic optimization in diversifying compound selection.</p>
<p>Evaluation Rigorousness Using GA-Optimized Compound Lists</p>
<p>Finally, we assessed the impact of compound list diversity on the rigorousness of toxicity prediction evaluations for machine learning models.To this end, we conducted predictive modeling of toxicity using 1,000 randomly generated compound lists and GA-derived compound lists positioned along the Pareto front, each used as test datasets.The remaining compounds served as training datasets.</p>
<p>Our analysis revealed that GA-optimized compound lists from the Pareto front posed significantly greater challenges for predictive modeling.As shown in Figure 5, predictive performance was markedly lower when evaluated using compound lists on the Pareto front compared to those not on the front.These findings highlight that GA-derived lists can be strategically designed to rigorously test predictive models, thereby enhancing the robustness and reliability of toxicity prediction method evaluations.</p>
<p>Conclusion</p>
<p>The key contributions of this study are as follows: 1.We propose a novel approach that formulates the selection of reference compounds as a multi-objective optimization problem solved by a Genetic Algorithm (GA), an innovative perspective in this domain.2. Our GA-based optimization simultaneously maximizes structural, physicochemical, and toxicity diversity, providing balanced and highly diverse compound lists.3. We found that compound lists closer to the Pareto front, indicating higher overall diversity, significantly reduced toxicity prediction accuracy, highlighting their utility for rigorous evaluation of predictive model robustness.In summary, our proposed multi-objective GA-based optimization effectively creates diverse and balanced compound lists suitable for rigorous validation studies.The method enhances the robustness and reliability of predictive toxicity models, offering significant improvements over conventional compound selection approaches.However, practical implementation in regulatory settings must also consider social acceptance, making expert input indispensable.Our approach, in collaboration with domain experts, is expected to support the efficient creation of reference compound lists.Additionally, by enabling rigorous evaluations during the research and development phase, this approach may facilitate smoother transitions into validation studies.We believe that this study aids to alleviate and accelerate the validation process for alternative safety assessment methods.</p>
<p>Limitation</p>
<p>Several parameters were not considered in the current algorithm.</p>
<p>Size of Compound Lists</p>
<p>In this study, the size of the compound lists was fixed to enable direct comparison with the compound list used in the original validation assay.However, allowing for larger compound list sizes could potentially enable the optimization of more complex and challenging problems.</p>
<p>Cost and Experimental Feasibility</p>
<p>Factors such as compound cost and the practicality of conducting experiments implicitly influence the selection of compound lists used in actual validation studies.Moreover, an essential requirement in validation assays is the use of compounds whose measurement values are consistent across multiple laboratories.Therefore, the ease of handling and the stability of compounds during experimentation are also critical considerations.However, due to the lack of sufficient numerical data on compound costs and the absence of quantifiable data on experimental feasibility, these aspects could not be incorporated into the current algorithm.It is conceivable that these factors could be incorporated as objectives or constraints in future algorithm development if reliable quantitative data become available for each compound.</p>
<p>The compound list from the Pareto front with the highest composite score was designated as the</p>
<p>Figure 1 .
1
Figures and Tables</p>
<p>(CFigure 2 .
2
Figure 2. Convergence of the Genetic Algorithm.</p>
<p>(</p>
<p>A-C) Plots showing changes in average population scores across generations for structural diversity (A), toxicity diversity (B), and physicochemical diversity (C).Each diversity metric clearly demonstrates convergence toward its optimization goal.Red dots indicate scores of the representative compound list (selected based on the highest composite score) from the final generation's Pareto front.</p>
<p>Figure 3 .Figure 4 .
34
Figure 3. Evolution of the Pareto Front from Initial to Final Generation.Scatter plot illustrating compound lists with each objective in GA.Triangular points represent the compound lists from the initial generation, while circular points indicate the compound lists from the final generation.The x and y axes denote structure score (lower is better) and physicochemical score (higher is better), respectively.The color bars indicate the toxicity score (higher is better).</p>
<p>(DFigure 5 .
5
Figure 5. Evaluation Rigorousness of Machine Learning Models for Toxicity Prediction Using GA-Optimized Compound Lists.Histograms display prediction scores for two groups of compounds: blue bars represent GA-optimized compounds on the Pareto front, while orange bars correspond to randomly selected compounds.Panel A shows AUC scores, Panel B shows F1 scores, and Panel C shows accuracy scores.Statistical test results comparing the two groups are shown in the top-left corner of each panel.Dashed lines indicate the kernel density estimation (KDE) for each distribution.</p>
<p>Table 1 . List of Validation Studies for Toxicity Testing Used in This Study.
1
in the columns indicates data obtained from JaCVAM's proposal to the Japanese government (unpublished document).<strong>refers to data from Bart et al., Reprod.Toxicol., 201043
JaCVAMJaCVAM TestNumber ofcompoundTest NumberNameCompoundsdataset07_acute toxicity01_Cytotoxicit y Testing72<em> Complete DL file set: page 33ICE Acute Oral Toxicity07_acute toxicity02_Cytotoxicit y Testing56</em> Complete DL file set: page 38ICE Acute Oral Toxicity09_endocrine01_VM7 Luc42 (agonist)<em> Complete DL file set: page 26TDC Use the appropriate tox21 test from among thesedisruptorsER TA assay25 (antagonist)</em> Complete DL file set: page 28TDC Use the appropriate tox21 test from among these09_endocrine02_ER-STTA86 (agonist)<em> Complete DL file set: page 32-34TDC Use the appropriate tox21 test from among thesedisruptorsassay21 (antagonist)</em> Complete DL file set: page 35TDC Use the appropriate tox21 test from among these09_endocrine04_AR-10 (agonist)<em> Complete DL file set: page 29, 31TDC Use the appropriate tox21 test from among thesedisruptorsEcoscreen10 (antagonist)</em> Complete DL file set: page 30, 32TDC Use the appropriate tox21 test from among these09_endocrine05_AR-11 (agonist)</strong> : table 1TDC Use the appropriate tox21 test from among thesedisruptorsCALUX9 (antagonist)<em><em> : table 2TDC Use the appropriate tox21 test from among theseTDC Use the appropriate tox21 test09_endocrine disruptors07_hrER in vitro study36</em> Complete DL file set:from among these TDC Use the appropriate tox21 testfrom among these10. Development al Toxicity Test Prediction01_Embryonic (EST) Stem Cell Technology18</em> Complete DL file set: page 4ICE DART10.Development al Toxicity Prediction02_Hand1-Luc EST16* Complete DL file set: page 56, 69ICE https://ice.ntp.niehs.nih.gov/DATASET DESCRIPTION DARTTest
*</p>
<p>AcknowledgementWe thank all those who contributed to the construction of the datasets employed in the present study such as ICE, TDC, and Tox21.This work was supported by AMED under Grant Number JP22mk0101250h and 23ak0101199h0001.Author ContributionYohei Ohto: Methodology, Software, Investigation, Writing -Original Draft, Visualization.Tadahaya Mizuno: Conceptualization, Resources, Supervision, Project administration, Writing -Original Draft, Writing -Review &amp; Editing, Funding acquisition.Yasuhiro Yoshikai: Methodology, Software.Hiromi Fujimoto: Investigation Hiroyuki Kusuhara: Writing -Review.Conflicts of InterestThe authors declare that they have no conflicts of interest.Supplementary InformationSupplementary Figures and TablesSupplementary
Safety pharmacology in drug discovery and development. B H Morimoto, E Castelloe, A W Fox, Handb. Exp. Pharmacol. 2292015</p>
<p>Limitations of animal studies for predicting toxicity in clinical trials: Is it time to rethink our current approach?. G A Van Norman, JACC Basic Transl. Sci. 42019</p>
<p>The flaws and human harms of animal experimentation. A Akhtar, Camb. Q. Healthc. Ethics. 242015</p>
<p>In silico phototoxicity prediction of drugs and chemicals by using Derek Nexus and QSAR Toolbox. V Ahuja, G Adiga Perdur, Z Aj, M Krishnappa, H Kandarova, Altern. Lab. Anim. 522024</p>
<p>The next generation blueprint of computational toxicology at the U.s. environmental Protection Agency. R S Thomas, Toxicol. Sci. 1692019</p>
<p>Liver microphysiological systems development guidelines for safety risk assessment in the pharmaceutical industry. A Baudy, 10.1039/c9lc00768gLab Chip. 2019</p>
<p>The application of in vitro methods to safety pharmacology. I D Wakefield, C Pollard, W S Redfern, T G Hammond, J.-P Valentin, Fundam. Clin. Pharmacol. 162002</p>
<p>Biology-inspired microphysiological system approaches to solve the prediction dilemma of substance testing. U Marx, ALTEX. 332016</p>
<p>Collaborative study on fifteen compounds in the rat-liver Comet assay integrated into 2-and 4-week repeat-dose studies. A Rothfuss, Mutat. Res. 7022010</p>
<p>The inter-laboratory validation study of EpiSensA for predicting skin sensitization potential. H Mizumachi, J. Appl. Toxicol. 442024</p>
<p>Non-animal photosafety assessment approaches for cosmetics based on the photochemical and photobiochemical properties. S Onoue, Toxicol. In Vitro. 272013</p>
<p>. 小 島 さ ん の 総 説, Preprint at</p>
<p>United Nations: Economic Commission for Europe. Globally Harmonized System of Classification and Labelling of Chemicals (GHS). (United Nations. 2023New York, NY</p>
<p>Predictive performance of the Vitrigel-eye irritancy test method using 118 chemicals: Predictive performance of Vitrigel-eye irritancy test method. H Yamaguchi, H Kojima, T Takezawa, J. Appl. Toxicol. 362016</p>
<p>Adaptation in natural and artificial systems (John H. holland). J R Sampson, SIAM Rev. Soc. Ind. Appl. Math. 181976</p>
<p>Evolutionary Algorithms for Solving Multi-Objective Problems. C A Coello Coello, G B Lamont, D A Van Veldhuizen, 2007SpringerNew York, NY</p>
<p>Hybrid Genetic Algorithm and Hill Climbing optimization for the neural network. K Sarode, S R Javaji, arXiv [cs.NE]2023</p>
<p>Structural topology design optimization using Genetic Algorithms with a bit-array representation. </p>
<p>Resource allocation in cloud computing using genetic algorithm and neural network. M Manavi, Y Zhang, G Chen, arXiv [cs.DC]2023</p>
<p>Time-split cross-validation as a method for estimating the goodness of prospective prediction. R P Sheridan, J. Chem. Inf. Model. 532013</p>
<p>Investigation of a data split strategy involving the time axis in adverse event prediction using machine learning. K Morita, T Mizuno, H Kusuhara, J. Chem. Inf. Model. 622022</p>
<p>SIMPD: an algorithm for generating simulated time splits for validating machine learning approaches. G A Landrum, J. Cheminform. 151192023</p>
<p>The ChEMBL Database in 2023: a drug discovery platform spanning multiple bioactivity data types and time periods. B Zdrazil, Nucleic Acids Res. 522024</p>
<p>Top. </p>
<p>An integrated chemical environment with tools for chemical safety testing. S Bell, Toxicol. In Vitro. 671049162020</p>
<p>Therapeutics Data Commons: Machine learning datasets and tasks for drug discovery and development. K Huang, arXiv [cs.LG]2021</p>
<p>The Tox21 10K compound library: Collaborative chemistry advancing toxicology. A M Richard, Chem. Res. Toxicol. 342021</p>
<p>The SIDER database of drugs and side effects. M Kuhn, I Letunic, L J Jensen, P Bork, Nucleic Acids Res. 442016</p>
<p>PubChem 2025 update. S Kim, Nucleic Acids Res. 532025</p>
<p>Rdkit/Rdkit: 2025_03_2 (Q1 2025) Release. G Landrum, 10.5281/ZENODO.152860102025</p>
<p>DEAP: evolutionary algorithms made easy. F.-A Fortin, F Rainville, M.-A Gardner, M Parizeau, C Gagné, J. Mach. Learn. Res. 132012</p>
<p>Extended-connectivity fingerprints. D Rogers, M Hahn, J. Chem. Inf. Model. 502010</p>
<p>Simple method to calculate octanol-water partition coefficient of organic compounds. E A Tehrany, F Fournier, S Desobry, J. Food Eng. 642004</p>
<p>Fast calculation of molecular polar surface area as a sum of fragment-based contributions and its application to the prediction of drug transport properties. P Ertl, B Rohde, P Selzer, J. Med. Chem. 432000</p>
<p>How to Detect and Handle Outliers. 16Google Books</p>
<p>A fast and elitist multiobjective genetic algorithm: NSGA-II. K Deb, A Pratap, S Agarwal, T Meyarivan, IEEE Trans. Evol. Comput. 62002</p>
<p>T Chen, C Guestrin, Xgboost, 10.1145/2939672.2939785A Scalable Tree Boosting System. 2016</p>
<p>EURL ECVAM Recommendation on the 3T3 Neutral Red Uptake Cytotoxicity Assay for Acute Oral Toxicity Testing. P M D P Prieto, C Griesinger, S P Amcoff, M Whelan, 2013</p>
<p>Optimization and prevalidation of the in vitro AR CALUX method to test androgenic and antiandrogenic activity of compounds. B Van Der Burg, Reprod. Toxicol. 302010</p>            </div>
        </div>

    </div>
</body>
</html>
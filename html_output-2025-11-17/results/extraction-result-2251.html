<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2251 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2251</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2251</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-62.html">extraction-schema-62</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <p><strong>Paper ID:</strong> paper-279464623</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.16565v1.pdf" target="_blank">Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control</a></p>
                <p><strong>Paper Abstract:</strong> World models enable robots to"imagine"future observations given current observations and planned actions, and have been increasingly adopted as generalized dynamics models to facilitate robot learning. Despite their promise, these models remain brittle when encountering novel visual distractors such as objects and background elements rarely seen during training. Specifically, novel distractors can corrupt action outcome predictions, causing downstream failures when robots rely on the world model imaginations for planning or action verification. In this work, we propose Reimagination with Observation Intervention (ReOI), a simple yet effective test-time strategy that enables world models to predict more reliable action outcomes in open-world scenarios where novel and unanticipated visual distractors are inevitable. Given the current robot observation, ReOI first detects visual distractors by identifying which elements of the scene degrade in physically implausible ways during world model prediction. Then, it modifies the current observation to remove these distractors and bring the observation closer to the training distribution. Finally, ReOI"reimagines"future outcomes with the modified observation and reintroduces the distractors post-hoc to preserve visual consistency for downstream planning and verification. We validate our approach on a suite of robotic manipulation tasks in the context of action verification, where the verifier needs to select desired action plans based on predictions from a world model. Our results show that ReOI is robust to both in-distribution and out-of-distribution visual distractors. Notably, it improves task success rates by up to 3x in the presence of novel distractors, significantly outperforming action verification that relies on world model predictions without imagination interventions.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2251.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2251.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReOI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reimagination with Observation Intervention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A test-time, plug-in pipeline that detects visually implausible (novel) distractors from world-model rollouts, inpaints them from the current observation to align inputs with the training distribution, re-rolls the world model in latent feature space, and then reintroduces distractors by depth-aware pixel compositing for downstream visual verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ReOI (test-time observation intervention pipeline with DINO-WM)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent-level prediction (DINOv2 feature space) for dynamics + pixel-level observation intervention and pixel compositing (multi-level/hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>explicit VLM-based identification of degrading regions in rollout (GPT-4o) → segmentation masks (Grounded-SAM2) → explicit masking/inpainting (image inpainting) to remove task-irrelevant/novel distractors</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>real-world robotic manipulation (wrist and third-person visual inputs, pick-and-place in a toy kitchen)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>static novel visual distractors (objects/background elements not seen during training) such as an Amazon box, kettle, high-pressure pot; these are explicitly identified and inpainted</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Improves task success rates by up to 3× in presence of novel distractors relative to using DINO-WM predictions without intervention; also reports improved SSIM and lower LPIPS vs DINO-WM (exact numeric SSIM/LPIPS values not provided in main text); collision rate is reported as low and comparable to conservative TrustRegion baseline</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Dynamics model: DINO-WM trained on single Nvidia A6000 (training hyperparameters reported in paper). ReOI adds test-time compute: VLM reasoning (GPT-4o) on two frames, segmentation (Grounded-SAM2), inpainting model, depth estimation and compositing per predicted frame. Exact FLOPs/params/time/per-frame latency not reported. DINOv2 feature-decoder fine-tuned for one epoch on 80 trajectories (separate adaptation step).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperforms base DINO-WM and a TrustRegion rejection baseline: up to 3× higher task success vs DINO-WM in presence of novel distractors; maintains collision rate comparable to TrustRegion while achieving substantially higher success than DINO-WM (numerical per-method success rates are reported in paper tables but explicit numbers are not shown in the main text descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Decoder adaptation: paper fine-tunes the DINOv2 feature decoder for one epoch on 80 trajectories collected in the testing environment (which included distractors) while freezing dynamics — this improved reconstruction fidelity in the test environment; no broad multi-task transfer experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated — experiments focus on single manipulation task family (pick/place pepper → pan) with varying distractors; ReOI is a plug-in verification strategy rather than a multi-task representation learning evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>May introduce physically unrealistic interactions after reintroducing distractors (e.g., gripper appearing to pass through reinserted objects), though such trajectories are rejected by verifier for safety; residual visual artifacts remain due to limitations in segmentation/inpainting; ReOI does not correct dynamics that depend on unknown object physics (it purposefully avoids modeling novel distractor dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablation that inpaints identified distractors in both ground truth and predicted observations (distractor-free comparison) shows ReOI still improves SSIM/LPIPS relative to vanilla DINO-WM — indicating distractors corrupt dynamics of in-distribution objects; specific numeric ablation values are referenced in Table I but not enumerated in the prose.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>World-model training used 500 robot-environment interaction trajectories (200 from diffusion policy rollouts, 300 random exploration); ReOI requires no dynamics fine-tuning (test-time intervention only) beyond a one-epoch feature-decoder adaptation on 80 trajectories for improved decoding fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Evaluated explicitly on in-distribution and out-of-distribution visual distractors: ReOI demonstrates robust predictions and improved planning performance when novel static distractors are present; DINO-WM alone shows severe degradation under the same OOD distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>ReOI achieves better SSIM (higher) and LPIPS (lower) than DINO-WM on full predicted observations; on distractor-inpainted comparisons (task-relevant objects only) ReOI still outperforms DINO-WM. Exact numeric SSIM/LPIPS values are given in tables in the paper but not quoted in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Explicit analysis: identifies that distractors underrepresented in training tend to degrade in rollouts and corrupt prediction of in-distribution (task-relevant) objects; uses VLM to detect which segmented regions degrade between initial and 5th predicted frame, and removes those to recover more accurate dynamics for task-relevant items (robot, target object).</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No dynamic switching of abstraction levels is reported; the approach is hybrid but static: dynamics operate in latent (DINOv2) space while pixel-level interventions are performed at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Paper argues and demonstrates that pixel-level compositing of removed distractors back into predicted frames preserves visual realism needed by a VLM-based verifier — this is important for downstream verification even though it may produce physically unrealistic interactions; pixel-level fidelity therefore benefits visual plan selection despite being decoupled from latent dynamics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2251.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2251.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DINO-WM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DINO-WM (world model using DINOv2 pre-trained features)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A world model that encodes observations in DINOv2 latent features, predicts future latent trajectories with a forward dynamics model, and decodes predicted latents back to images via a feature decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dino-wm: World models on pre-trained visual features enable zero-shot planning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DINO-WM</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent representation level (DINOv2 features) for dynamics prediction; image reconstruction from predicted features via decoder</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>No explicit task-specific feature selection at deployment; relies on pretrained DINOv2 features (self-supervised visual features) — no reward-weighted or task-privileged feature selection used in the base model as described</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation (real robot pick-and-place trajectories in a kitchen-like environment)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Trained in controlled environment with limited in-distribution variability (green pepper, spoon), and exhibits failure when static novel distractors (box, kettle, pot) are introduced at test time — these produce distortions/erasure in rollouts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline for reported SSIM/LPIPS comparisons: DINO-WM yields lower SSIM and higher LPIPS on predicted observations in presence of novel distractors compared to ReOI; specific numeric values reported in paper tables but not quoted verbatim in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Training hyperparameters provided (Table III): 300 epochs, batch size 64, trained on single Nvidia A6000; other costs (params, FLOPs, wall-clock time) not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Used as the base world model baseline; ReOI applied on top of DINO-WM substantially improves predictive quality and downstream planning success under visual distribution shift.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Paper performs a decoder fine-tuning (one epoch, 80 trajectories) while freezing dynamics to adapt decoding to test environment; DINO-WM dynamics themselves are not fine-tuned for the novel distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Hallucination of future frames when novel/distracting visual elements appear: distractors rapidly distort, disappear, or warp across predicted frames; can erase or mis-predict task-relevant objects and robot motions (e.g., erasing the target object or predicting a successful pick when the true outcome fails).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Paper includes ablation where distractors are inpainted from both predicted and ground-truth observations to measure performance on in-distribution objects; DINO-WM performs worse than ReOI even in this distractor-free comparison, indicating distractors corrupt dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained on 500 trajectories (200 from policy rollouts, 300 random exploration), each 24 steps long; no explicit sample-efficiency curve beyond these counts.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Explicitly evaluated on out-of-distribution static distractors and shown to generalize poorly — degraded rollouts and corrupted dynamics when encountering novel visual elements.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Without distractors DINO-WM produces fine-grained temporally consistent predictions; with distractors reconstruction quality drops severely (artifacts, erasure). SSIM/LPIPS metrics used to quantify quality.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper demonstrates that novel distractors not only degrade pixel quality but also corrupt predicted dynamics of robot and target objects — indicating sensitivity to task-relevant dynamics when irrelevant features are OOD.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No dynamic abstraction mechanism reported.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>DINO-WM reconstructs images from latent features; however, when pixel-level novel content is OOD, fidelity suffers and harms downstream verification.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2251.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2251.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DINOv2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DINOv2 pretrained visual feature backbone / feature decoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained self-supervised visual feature extractor (DINO family) used as the encoding backbone for the world model; a separate DINOv2 feature decoder is fine-tuned to map predicted features back to pixels in the test environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DINOv2</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DINOv2 (pretrained visual representation) + fine-tuned DINOv2 feature decoder</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>semantic/feature-level visual representation (pretrained feature space rather than raw pixels)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>Representation is learned by self-supervised DINO pretraining (not task supervised); no explicit task-relevance selection during encoding in this work</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>used for encoding/decoding observations in robotic manipulation world model</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Decoder fine-tuned on 80 trajectories that intentionally include distractors to improve visual reconstruction in presence of novel distractors; base features are fixed during decoder adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Decoder fine-tuned for one epoch on 80 test-environment trajectories improved reconstruction fidelity in the presence of unfamiliar distractors (qualitative and SSIM/LPIPS improvements reported for full pipeline); no standalone numeric metric for DINOv2 feature quality given.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Decoder fine-tuning: one epoch on 80 trajectories; dynamics frozen. Detailed parameter counts or FLOPs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Using pretrained DINOv2 features enables latent-space dynamics prediction (DINO-WM) and is compared implicitly against pixel-level world models in related work discussions, but no direct pixel-baseline comparison with quantitation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Feature decoder adaptation is a small-scale transfer (one-epoch fine-tune) to adapt visual decoding to a shifted environment with distractors; reported to improve reconstruction without altering dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Pretrained features alone do not prevent dynamics corruption when novel visual elements are present; decoder needs adaptation to test-environment visuals to preserve reconstruction fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Decoder fine-tuning vs frozen-decoder behavior is discussed qualitatively; specifics are limited in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Decoder adaptation uses 80 trajectories (one epoch), indicating modest sample cost for visual adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Decoder adaptation addresses some distributional shift in pixel appearance due to distractors, but dynamics generalization remains primarily dependent on the frozen DINO-WM.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Reported SSIM/LPIPS improvements for the full ReOI pipeline include the decoder-adapted reconstructions; exact numeric decoder-only metrics are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Not directly analyzed for DINOv2 features beyond showing that task-relevant dynamics can be corrupted by OOD visual features unless interventions are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not applicable to DINOv2 itself.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Fine-tuning the decoder increases pixel fidelity in the test environment and reduces visual artifacts when reconstructing predicted latents (used to improve downstream visual verification).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2251.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2251.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TrustRegion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TrustRegion (trust-region rejection baseline using Lipschitz bounds)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A test-time baseline that computes a trust region in input (observation, action) space using an estimated Lipschitz constant of the world-model prediction error and rejects predictions for inputs outside this region to ensure conservative safety.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TrustRegion (Lipschitz-based trust-region rejection)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>operates on world-model latent prediction-error metric (DINOv2 embedding L2 error) — conceptually an input-space/latent-error bounding approach rather than an explicit representational abstraction</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>No explicit feature selection; rejects inputs (observation+action pairs) whose predicted-error bound exceeds a threshold (i.e., outside the trust region)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation verification (same experimental setup as ReOI/DINO-WM comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Designed to detect OOD inputs (including observations with distractors) and reject them conservatively rather than attempt to reimagine — intended for safety when novel distractors exist.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>TrustRegion produced conservative behavior with low collision rates comparable to ReOI; task success lower than ReOI (exact numeric success/collision rates reported in paper tables but not enumerated in text). Estimated Lipschitz constants reported: initial estimate 0.84 (within initial T0), final trust region Lipschitz constant 0.93, error bound 1160 (units: L2 latent-error), r0=0.1, filter threshold 750 for selecting training points.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Implementation steps described: estimation of Lipschitz constant, initializing trust region from low-error training points, progressive radius expansion per Algorithm 2 of cited work; exact compute/time not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>ReOI achieves higher task success than TrustRegion while maintaining a similarly low collision rate — implies rejection-only conservative approach reduces false positives but at cost to success.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not applicable/not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Conservative rejections can reduce task success (i.e., rejecting inputs that might be salvageable via intervention); depends on the quality of Lipschitz constant estimation and availability of representative training points.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Paper compares TrustRegion vs ReOI and DINO-WM but does not provide internal ablations of TrustRegion beyond the description of parameter settings used.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>TrustRegion initialized from training points with prediction error < 750; sensitivity to available training coverage implied but not quantitatively explored.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Designed to detect and reject OOD inputs, but rejection may be overly conservative when the trust-region is small or training coverage is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable — TrustRegion rejects predictions rather than altering reconstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>No explicit mechanism to separate task-relevant vs irrelevant features; it operates on a global latent prediction-error metric.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2251.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2251.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Separated-branch methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Separated network branches for task-relevant vs task-irrelevant modeling (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training-time approaches that allocate separate model branches to represent task-relevant and task-irrelevant components, then use only the task-relevant branch for downstream planning or verification to mitigate visual distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Separated task-relevant / task-irrelevant branch models (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>semantic/object-level separation (explicit splitting of representations into task-relevant vs irrelevant components)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>explicit branch-wise separation driven by task supervision or architectural design; task-relevant branch used downstream</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>discussed generally for robotics/world-model learning (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Typically targeted for in-distribution distractors during training; limited robustness to novel distractors at test time per paper discussion</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not quantified in this paper (referenced in related work). Paper notes that such methods often degrade when the task context shifts at test time or when novel distractors appear.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not reported in this paper (related work mention).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Paper positions ReOI as complementary to and different from these training-time methods, arguing test-time intervention is needed when novel distractors are unpredictable.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Related works may explore transfer — not discussed or quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Related-works vary; paper notes these methods are tightly coupled to training-time task context and can fail when context shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Performance degrades or fails when previously irrelevant distractors become safety-critical or when novel distractors appear at deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not applicable in this paper; cited as prior approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Paper argues these methods are less robust to open-world novel distractors compared to test-time interventions like ReOI.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Varies by method in cited literature; not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>These methods explicitly aim to separate task-relevant representations during training.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Generally static branches; not dynamically switching at test time in the cited approaches (per paper discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Not specifically discussed in relation to separated-branch methods in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2251.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2251.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Privileged-reward methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Privileged reward / task-supervision methods for task-relevant representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training-time approaches that leverage privileged reward signals or task supervision to learn representations that emphasize task-relevant features and suppress distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Privileged reward/task-supervision representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>task-aligned latent-level abstractions informed by reward/supervision</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>reward-weighted learning or supervised objectives that bias representations toward task-relevant features</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>discussed generally in related work for robotics and world-model learning</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Targeted at suppressing in-distribution distractors during training; limited when distractors shift or are novel at deployment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not provided in this paper (referenced as prior approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Paper contrasts these training-time approaches with its test-time intervention approach, arguing training-time solutions may be insufficient for open-world novel distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated here; some privileged-reward methods are task-specific and may not transfer well.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Assumes same task context at deployment; fails when task context changes or when novel distractors arise that become safety-critical.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Paper claims limited generalization to out-of-distribution distractors for these methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>These methods explicitly use task supervision to shape representations toward relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not typically dynamic.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Not the focus of these training-time supervised approaches; their goal is task-relevance rather than pixel fidelity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2251.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2251.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffusion Policy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diffusion Policy (imitation-based generative action sampler)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-based generative policy used to sample multimodal action plans from visual input; used here as the action-plan sampler for sampling candidate trajectories in visual MPC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diffusion policy: Visuomotor policy learning via action diffusion.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Diffusion Policy (action diffusion for visuomotor planning)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>action-level (generative policy outputs sequences of 3D waypoints and gripper signals) — not a visual representation model</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>policy conditioned on current image observations (no explicit distractor suppression reported in this paper for the policy itself)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation (used to propose candidate action plans for the verifier/world model to simulate)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Policy trained from 120 teleoperated demonstrations; no specific distractor-robustness training described in this paper for the policy itself</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as sampler; no standalone performance metrics reported here (policy training dataset size: 120 multimode demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Training details not provided in this paper beyond dataset size.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Not compared as a focus of the paper; used as action proposal mechanism for planning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed in detail; main failure analyses focus on world-model predictions under distractors rather than the policy sampler.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None reported in this paper regarding the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Policy trained with 120 demonstrations (multimode).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Not reported for the policy in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Not applicable to the policy in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2251.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2251.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (vision-language model used for visual reasoning and verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-capable large language model used in two roles: (1) to analyze world-model rollouts and identify segmented regions that degrade (novel distractor detection), and (2) to act as a verifier that evaluates reimagined rollouts and selects/rejects action plans based on safety and task success.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (vision-language model used for distractor identification and plan verification)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>operates on pixel/frame inputs and segmentation overlays (semantic/object-level reasoning from images)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>reasoning-based selection: prompted to compare initial vs 5th rollout frame with overlayed segmentation mask IDs and return mask IDs that disappear/degrade — effectively identifies task-irrelevant/novel regions by observed rollout implausibility</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual reasoning for robotic manipulation plan verification and distractor detection</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Used specifically to detect novel static distractors that degrade in predicted rollouts; critical to ReOI pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Plan-verification selection accuracy and rejection accuracy reported in Appendix B (comparison between VLMs GPT-4o and LLaMA-4 shown visually); precise numeric percentages are provided in paper tables (not quoted in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Uses an off-the-shelf VLM (GPT-4o) at test time for each candidate rollout (analyze two frames per rollout for distractor detection and evaluate concatenated predicted frames for verification); exact latency and compute not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Two off-the-shelf VLMs (GPT-4o and LLaMA-4) evaluated as verifiers; GPT-4o is leveraged for distractor-ID pipeline and verification prompting. The paper provides prompt templates and qualitative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not applicable to the VLMs (used zero-shot, no fine-tuning reported).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated for VLM beyond verification role in single task family.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Accuracy depends on quality of segmentation overlay and on whether distractor degradation is visible by the 5th predicted frame; false positives/negatives in mask selection can propagate to inpainting and affect outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>The paper discusses choice of initial vs 5th frame based on empirical observation (implausible behavior manifests within first four steps), but does not present large-scale ablations of VLM prompting variants in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Zero-shot usage; no fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>VLM reasoning is used as an open-world detector for novel degradations and shown effective in examples/quantitative distractor identification accuracy (Appendix), but dependent on segmentation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>VLM-based comparison of predicted frames is used to identify which segmented regions are behaving implausibly (degrading/disappearing) and hence likely task-irrelevant / novel distractors in the current context.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>VLM verification relies on pixel-level composites of reinserted distractors to make safety judgements; thus pixel fidelity in the final visualizations is important for verifier decisions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dino-wm: World models on pre-trained visual features enable zero-shot planning. <em>(Rating: 2)</em></li>
                <li>DINOv2 <em>(Rating: 2)</em></li>
                <li>Planning with learned dynamics: Probabilistic guarantees on safety and reachability via lipschitz constants <em>(Rating: 2)</em></li>
                <li>Learning task informed abstractions <em>(Rating: 2)</em></li>
                <li>Learning invariant representations for reinforcement learning without reconstruction <em>(Rating: 2)</em></li>
                <li>Semail: eliminating distractors in visual imitation via separated models <em>(Rating: 2)</em></li>
                <li>Diffusion policy: Visuomotor policy learning via action diffusion. <em>(Rating: 2)</em></li>
                <li>Leveraging separated world model for exploration in visually distracted environments <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2251",
    "paper_id": "paper-279464623",
    "extraction_schema_id": "extraction-schema-62",
    "extracted_data": [
        {
            "name_short": "ReOI",
            "name_full": "Reimagination with Observation Intervention",
            "brief_description": "A test-time, plug-in pipeline that detects visually implausible (novel) distractors from world-model rollouts, inpaints them from the current observation to align inputs with the training distribution, re-rolls the world model in latent feature space, and then reintroduces distractors by depth-aware pixel compositing for downstream visual verification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ReOI (test-time observation intervention pipeline with DINO-WM)",
            "abstraction_level": "latent-level prediction (DINOv2 feature space) for dynamics + pixel-level observation intervention and pixel compositing (multi-level/hybrid)",
            "feature_selection_mechanism": "explicit VLM-based identification of degrading regions in rollout (GPT-4o) → segmentation masks (Grounded-SAM2) → explicit masking/inpainting (image inpainting) to remove task-irrelevant/novel distractors",
            "task_domain": "real-world robotic manipulation (wrist and third-person visual inputs, pick-and-place in a toy kitchen)",
            "distractor_presence": "static novel visual distractors (objects/background elements not seen during training) such as an Amazon box, kettle, high-pressure pot; these are explicitly identified and inpainted",
            "performance_metrics": "Improves task success rates by up to 3× in presence of novel distractors relative to using DINO-WM predictions without intervention; also reports improved SSIM and lower LPIPS vs DINO-WM (exact numeric SSIM/LPIPS values not provided in main text); collision rate is reported as low and comparable to conservative TrustRegion baseline",
            "computational_cost_details": "Dynamics model: DINO-WM trained on single Nvidia A6000 (training hyperparameters reported in paper). ReOI adds test-time compute: VLM reasoning (GPT-4o) on two frames, segmentation (Grounded-SAM2), inpainting model, depth estimation and compositing per predicted frame. Exact FLOPs/params/time/per-frame latency not reported. DINOv2 feature-decoder fine-tuned for one epoch on 80 trajectories (separate adaptation step).",
            "comparison_to_baselines": "Outperforms base DINO-WM and a TrustRegion rejection baseline: up to 3× higher task success vs DINO-WM in presence of novel distractors; maintains collision rate comparable to TrustRegion while achieving substantially higher success than DINO-WM (numerical per-method success rates are reported in paper tables but explicit numbers are not shown in the main text descriptions).",
            "transfer_learning_results": "Decoder adaptation: paper fine-tunes the DINOv2 feature decoder for one epoch on 80 trajectories collected in the testing environment (which included distractors) while freezing dynamics — this improved reconstruction fidelity in the test environment; no broad multi-task transfer experiments reported.",
            "multi_task_performance": "Not evaluated — experiments focus on single manipulation task family (pick/place pepper → pan) with varying distractors; ReOI is a plug-in verification strategy rather than a multi-task representation learning evaluation.",
            "failure_modes": "May introduce physically unrealistic interactions after reintroducing distractors (e.g., gripper appearing to pass through reinserted objects), though such trajectories are rejected by verifier for safety; residual visual artifacts remain due to limitations in segmentation/inpainting; ReOI does not correct dynamics that depend on unknown object physics (it purposefully avoids modeling novel distractor dynamics).",
            "ablation_studies": "Ablation that inpaints identified distractors in both ground truth and predicted observations (distractor-free comparison) shows ReOI still improves SSIM/LPIPS relative to vanilla DINO-WM — indicating distractors corrupt dynamics of in-distribution objects; specific numeric ablation values are referenced in Table I but not enumerated in the prose.",
            "sample_efficiency": "World-model training used 500 robot-environment interaction trajectories (200 from diffusion policy rollouts, 300 random exploration); ReOI requires no dynamics fine-tuning (test-time intervention only) beyond a one-epoch feature-decoder adaptation on 80 trajectories for improved decoding fidelity.",
            "generalization_analysis": "Evaluated explicitly on in-distribution and out-of-distribution visual distractors: ReOI demonstrates robust predictions and improved planning performance when novel static distractors are present; DINO-WM alone shows severe degradation under the same OOD distractors.",
            "reconstruction_quality": "ReOI achieves better SSIM (higher) and LPIPS (lower) than DINO-WM on full predicted observations; on distractor-inpainted comparisons (task-relevant objects only) ReOI still outperforms DINO-WM. Exact numeric SSIM/LPIPS values are given in tables in the paper but not quoted in the main text.",
            "task_relevance_analysis": "Explicit analysis: identifies that distractors underrepresented in training tend to degrade in rollouts and corrupt prediction of in-distribution (task-relevant) objects; uses VLM to detect which segmented regions degrade between initial and 5th predicted frame, and removes those to recover more accurate dynamics for task-relevant items (robot, target object).",
            "dynamic_abstraction": "No dynamic switching of abstraction levels is reported; the approach is hybrid but static: dynamics operate in latent (DINOv2) space while pixel-level interventions are performed at test time.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "None reported.",
            "pixel_fidelity_benefits": "Paper argues and demonstrates that pixel-level compositing of removed distractors back into predicted frames preserves visual realism needed by a VLM-based verifier — this is important for downstream verification even though it may produce physically unrealistic interactions; pixel-level fidelity therefore benefits visual plan selection despite being decoupled from latent dynamics.",
            "uuid": "e2251.0"
        },
        {
            "name_short": "DINO-WM",
            "name_full": "DINO-WM (world model using DINOv2 pre-trained features)",
            "brief_description": "A world model that encodes observations in DINOv2 latent features, predicts future latent trajectories with a forward dynamics model, and decodes predicted latents back to images via a feature decoder.",
            "citation_title": "Dino-wm: World models on pre-trained visual features enable zero-shot planning.",
            "mention_or_use": "use",
            "model_name": "DINO-WM",
            "abstraction_level": "latent representation level (DINOv2 features) for dynamics prediction; image reconstruction from predicted features via decoder",
            "feature_selection_mechanism": "No explicit task-specific feature selection at deployment; relies on pretrained DINOv2 features (self-supervised visual features) — no reward-weighted or task-privileged feature selection used in the base model as described",
            "task_domain": "robotic manipulation (real robot pick-and-place trajectories in a kitchen-like environment)",
            "distractor_presence": "Trained in controlled environment with limited in-distribution variability (green pepper, spoon), and exhibits failure when static novel distractors (box, kettle, pot) are introduced at test time — these produce distortions/erasure in rollouts",
            "performance_metrics": "Baseline for reported SSIM/LPIPS comparisons: DINO-WM yields lower SSIM and higher LPIPS on predicted observations in presence of novel distractors compared to ReOI; specific numeric values reported in paper tables but not quoted verbatim in main text.",
            "computational_cost_details": "Training hyperparameters provided (Table III): 300 epochs, batch size 64, trained on single Nvidia A6000; other costs (params, FLOPs, wall-clock time) not reported.",
            "comparison_to_baselines": "Used as the base world model baseline; ReOI applied on top of DINO-WM substantially improves predictive quality and downstream planning success under visual distribution shift.",
            "transfer_learning_results": "Paper performs a decoder fine-tuning (one epoch, 80 trajectories) while freezing dynamics to adapt decoding to test environment; DINO-WM dynamics themselves are not fine-tuned for the novel distractors.",
            "multi_task_performance": "Not evaluated in this work.",
            "failure_modes": "Hallucination of future frames when novel/distracting visual elements appear: distractors rapidly distort, disappear, or warp across predicted frames; can erase or mis-predict task-relevant objects and robot motions (e.g., erasing the target object or predicting a successful pick when the true outcome fails).",
            "ablation_studies": "Paper includes ablation where distractors are inpainted from both predicted and ground-truth observations to measure performance on in-distribution objects; DINO-WM performs worse than ReOI even in this distractor-free comparison, indicating distractors corrupt dynamics.",
            "sample_efficiency": "Trained on 500 trajectories (200 from policy rollouts, 300 random exploration), each 24 steps long; no explicit sample-efficiency curve beyond these counts.",
            "generalization_analysis": "Explicitly evaluated on out-of-distribution static distractors and shown to generalize poorly — degraded rollouts and corrupted dynamics when encountering novel visual elements.",
            "reconstruction_quality": "Without distractors DINO-WM produces fine-grained temporally consistent predictions; with distractors reconstruction quality drops severely (artifacts, erasure). SSIM/LPIPS metrics used to quantify quality.",
            "task_relevance_analysis": "Paper demonstrates that novel distractors not only degrade pixel quality but also corrupt predicted dynamics of robot and target objects — indicating sensitivity to task-relevant dynamics when irrelevant features are OOD.",
            "dynamic_abstraction": "No dynamic abstraction mechanism reported.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "None reported.",
            "pixel_fidelity_benefits": "DINO-WM reconstructs images from latent features; however, when pixel-level novel content is OOD, fidelity suffers and harms downstream verification.",
            "uuid": "e2251.1"
        },
        {
            "name_short": "DINOv2",
            "name_full": "DINOv2 pretrained visual feature backbone / feature decoder",
            "brief_description": "A pretrained self-supervised visual feature extractor (DINO family) used as the encoding backbone for the world model; a separate DINOv2 feature decoder is fine-tuned to map predicted features back to pixels in the test environment.",
            "citation_title": "DINOv2",
            "mention_or_use": "use",
            "model_name": "DINOv2 (pretrained visual representation) + fine-tuned DINOv2 feature decoder",
            "abstraction_level": "semantic/feature-level visual representation (pretrained feature space rather than raw pixels)",
            "feature_selection_mechanism": "Representation is learned by self-supervised DINO pretraining (not task supervised); no explicit task-relevance selection during encoding in this work",
            "task_domain": "used for encoding/decoding observations in robotic manipulation world model",
            "distractor_presence": "Decoder fine-tuned on 80 trajectories that intentionally include distractors to improve visual reconstruction in presence of novel distractors; base features are fixed during decoder adaptation.",
            "performance_metrics": "Decoder fine-tuned for one epoch on 80 test-environment trajectories improved reconstruction fidelity in the presence of unfamiliar distractors (qualitative and SSIM/LPIPS improvements reported for full pipeline); no standalone numeric metric for DINOv2 feature quality given.",
            "computational_cost_details": "Decoder fine-tuning: one epoch on 80 trajectories; dynamics frozen. Detailed parameter counts or FLOPs not reported.",
            "comparison_to_baselines": "Using pretrained DINOv2 features enables latent-space dynamics prediction (DINO-WM) and is compared implicitly against pixel-level world models in related work discussions, but no direct pixel-baseline comparison with quantitation in this paper.",
            "transfer_learning_results": "Feature decoder adaptation is a small-scale transfer (one-epoch fine-tune) to adapt visual decoding to a shifted environment with distractors; reported to improve reconstruction without altering dynamics.",
            "multi_task_performance": "Not evaluated.",
            "failure_modes": "Pretrained features alone do not prevent dynamics corruption when novel visual elements are present; decoder needs adaptation to test-environment visuals to preserve reconstruction fidelity.",
            "ablation_studies": "Decoder fine-tuning vs frozen-decoder behavior is discussed qualitatively; specifics are limited in the main text.",
            "sample_efficiency": "Decoder adaptation uses 80 trajectories (one epoch), indicating modest sample cost for visual adaptation.",
            "generalization_analysis": "Decoder adaptation addresses some distributional shift in pixel appearance due to distractors, but dynamics generalization remains primarily dependent on the frozen DINO-WM.",
            "reconstruction_quality": "Reported SSIM/LPIPS improvements for the full ReOI pipeline include the decoder-adapted reconstructions; exact numeric decoder-only metrics are not provided.",
            "task_relevance_analysis": "Not directly analyzed for DINOv2 features beyond showing that task-relevant dynamics can be corrupted by OOD visual features unless interventions are applied.",
            "dynamic_abstraction": "Not applicable to DINOv2 itself.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "None reported.",
            "pixel_fidelity_benefits": "Fine-tuning the decoder increases pixel fidelity in the test environment and reduces visual artifacts when reconstructing predicted latents (used to improve downstream visual verification).",
            "uuid": "e2251.2"
        },
        {
            "name_short": "TrustRegion",
            "name_full": "TrustRegion (trust-region rejection baseline using Lipschitz bounds)",
            "brief_description": "A test-time baseline that computes a trust region in input (observation, action) space using an estimated Lipschitz constant of the world-model prediction error and rejects predictions for inputs outside this region to ensure conservative safety.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "TrustRegion (Lipschitz-based trust-region rejection)",
            "abstraction_level": "operates on world-model latent prediction-error metric (DINOv2 embedding L2 error) — conceptually an input-space/latent-error bounding approach rather than an explicit representational abstraction",
            "feature_selection_mechanism": "No explicit feature selection; rejects inputs (observation+action pairs) whose predicted-error bound exceeds a threshold (i.e., outside the trust region)",
            "task_domain": "robotic manipulation verification (same experimental setup as ReOI/DINO-WM comparisons)",
            "distractor_presence": "Designed to detect OOD inputs (including observations with distractors) and reject them conservatively rather than attempt to reimagine — intended for safety when novel distractors exist.",
            "performance_metrics": "TrustRegion produced conservative behavior with low collision rates comparable to ReOI; task success lower than ReOI (exact numeric success/collision rates reported in paper tables but not enumerated in text). Estimated Lipschitz constants reported: initial estimate 0.84 (within initial T0), final trust region Lipschitz constant 0.93, error bound 1160 (units: L2 latent-error), r0=0.1, filter threshold 750 for selecting training points.",
            "computational_cost_details": "Implementation steps described: estimation of Lipschitz constant, initializing trust region from low-error training points, progressive radius expansion per Algorithm 2 of cited work; exact compute/time not reported.",
            "comparison_to_baselines": "ReOI achieves higher task success than TrustRegion while maintaining a similarly low collision rate — implies rejection-only conservative approach reduces false positives but at cost to success.",
            "transfer_learning_results": "Not applicable/not reported.",
            "multi_task_performance": "Not evaluated.",
            "failure_modes": "Conservative rejections can reduce task success (i.e., rejecting inputs that might be salvageable via intervention); depends on the quality of Lipschitz constant estimation and availability of representative training points.",
            "ablation_studies": "Paper compares TrustRegion vs ReOI and DINO-WM but does not provide internal ablations of TrustRegion beyond the description of parameter settings used.",
            "sample_efficiency": "TrustRegion initialized from training points with prediction error &lt; 750; sensitivity to available training coverage implied but not quantitatively explored.",
            "generalization_analysis": "Designed to detect and reject OOD inputs, but rejection may be overly conservative when the trust-region is small or training coverage is limited.",
            "reconstruction_quality": "Not applicable — TrustRegion rejects predictions rather than altering reconstructions.",
            "task_relevance_analysis": "No explicit mechanism to separate task-relevant vs irrelevant features; it operates on a global latent prediction-error metric.",
            "dynamic_abstraction": "No.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "None reported.",
            "pixel_fidelity_benefits": "Not applicable.",
            "uuid": "e2251.3"
        },
        {
            "name_short": "Separated-branch methods",
            "name_full": "Separated network branches for task-relevant vs task-irrelevant modeling (related work)",
            "brief_description": "Training-time approaches that allocate separate model branches to represent task-relevant and task-irrelevant components, then use only the task-relevant branch for downstream planning or verification to mitigate visual distractors.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Separated task-relevant / task-irrelevant branch models (related work)",
            "abstraction_level": "semantic/object-level separation (explicit splitting of representations into task-relevant vs irrelevant components)",
            "feature_selection_mechanism": "explicit branch-wise separation driven by task supervision or architectural design; task-relevant branch used downstream",
            "task_domain": "discussed generally for robotics/world-model learning (related work)",
            "distractor_presence": "Typically targeted for in-distribution distractors during training; limited robustness to novel distractors at test time per paper discussion",
            "performance_metrics": "Not quantified in this paper (referenced in related work). Paper notes that such methods often degrade when the task context shifts at test time or when novel distractors appear.",
            "computational_cost_details": "Not reported in this paper (related work mention).",
            "comparison_to_baselines": "Paper positions ReOI as complementary to and different from these training-time methods, arguing test-time intervention is needed when novel distractors are unpredictable.",
            "transfer_learning_results": "Related works may explore transfer — not discussed or quantified in this paper.",
            "multi_task_performance": "Related-works vary; paper notes these methods are tightly coupled to training-time task context and can fail when context shifts.",
            "failure_modes": "Performance degrades or fails when previously irrelevant distractors become safety-critical or when novel distractors appear at deployment.",
            "ablation_studies": "Not applicable in this paper; cited as prior approaches.",
            "sample_efficiency": "Not discussed here.",
            "generalization_analysis": "Paper argues these methods are less robust to open-world novel distractors compared to test-time interventions like ReOI.",
            "reconstruction_quality": "Varies by method in cited literature; not quantified here.",
            "task_relevance_analysis": "These methods explicitly aim to separate task-relevant representations during training.",
            "dynamic_abstraction": "Generally static branches; not dynamically switching at test time in the cited approaches (per paper discussion).",
            "exploration_vs_exploitation": "Not discussed in this paper.",
            "information_theoretic_analysis": "Not discussed here.",
            "pixel_fidelity_benefits": "Not specifically discussed in relation to separated-branch methods in this paper.",
            "uuid": "e2251.4"
        },
        {
            "name_short": "Privileged-reward methods",
            "name_full": "Privileged reward / task-supervision methods for task-relevant representation learning",
            "brief_description": "Training-time approaches that leverage privileged reward signals or task supervision to learn representations that emphasize task-relevant features and suppress distractors.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Privileged reward/task-supervision representation learning",
            "abstraction_level": "task-aligned latent-level abstractions informed by reward/supervision",
            "feature_selection_mechanism": "reward-weighted learning or supervised objectives that bias representations toward task-relevant features",
            "task_domain": "discussed generally in related work for robotics and world-model learning",
            "distractor_presence": "Targeted at suppressing in-distribution distractors during training; limited when distractors shift or are novel at deployment",
            "performance_metrics": "Not provided in this paper (referenced as prior approaches).",
            "computational_cost_details": "Not provided here.",
            "comparison_to_baselines": "Paper contrasts these training-time approaches with its test-time intervention approach, arguing training-time solutions may be insufficient for open-world novel distractors.",
            "transfer_learning_results": "Not evaluated in this paper.",
            "multi_task_performance": "Not evaluated here; some privileged-reward methods are task-specific and may not transfer well.",
            "failure_modes": "Assumes same task context at deployment; fails when task context changes or when novel distractors arise that become safety-critical.",
            "ablation_studies": "Not in this work.",
            "sample_efficiency": "Not reported here.",
            "generalization_analysis": "Paper claims limited generalization to out-of-distribution distractors for these methods.",
            "reconstruction_quality": "Not discussed here.",
            "task_relevance_analysis": "These methods explicitly use task supervision to shape representations toward relevance.",
            "dynamic_abstraction": "Not typically dynamic.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "None reported here.",
            "pixel_fidelity_benefits": "Not the focus of these training-time supervised approaches; their goal is task-relevance rather than pixel fidelity.",
            "uuid": "e2251.5"
        },
        {
            "name_short": "Diffusion Policy",
            "name_full": "Diffusion Policy (imitation-based generative action sampler)",
            "brief_description": "A diffusion-based generative policy used to sample multimodal action plans from visual input; used here as the action-plan sampler for sampling candidate trajectories in visual MPC.",
            "citation_title": "Diffusion policy: Visuomotor policy learning via action diffusion.",
            "mention_or_use": "use",
            "model_name": "Diffusion Policy (action diffusion for visuomotor planning)",
            "abstraction_level": "action-level (generative policy outputs sequences of 3D waypoints and gripper signals) — not a visual representation model",
            "feature_selection_mechanism": "policy conditioned on current image observations (no explicit distractor suppression reported in this paper for the policy itself)",
            "task_domain": "robotic manipulation (used to propose candidate action plans for the verifier/world model to simulate)",
            "distractor_presence": "Policy trained from 120 teleoperated demonstrations; no specific distractor-robustness training described in this paper for the policy itself",
            "performance_metrics": "Used as sampler; no standalone performance metrics reported here (policy training dataset size: 120 multimode demonstrations).",
            "computational_cost_details": "Training details not provided in this paper beyond dataset size.",
            "comparison_to_baselines": "Not compared as a focus of the paper; used as action proposal mechanism for planning experiments.",
            "transfer_learning_results": "Not evaluated here.",
            "multi_task_performance": "Not evaluated here.",
            "failure_modes": "Not discussed in detail; main failure analyses focus on world-model predictions under distractors rather than the policy sampler.",
            "ablation_studies": "None reported in this paper regarding the policy.",
            "sample_efficiency": "Policy trained with 120 demonstrations (multimode).",
            "generalization_analysis": "Not reported for the policy in this paper.",
            "reconstruction_quality": "Not applicable.",
            "task_relevance_analysis": "Not applicable to the policy in this work.",
            "dynamic_abstraction": "Not applicable.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "None reported.",
            "pixel_fidelity_benefits": "Not applicable.",
            "uuid": "e2251.6"
        },
        {
            "name_short": "GPT-4o (VLM)",
            "name_full": "GPT-4o (vision-language model used for visual reasoning and verification)",
            "brief_description": "A vision-capable large language model used in two roles: (1) to analyze world-model rollouts and identify segmented regions that degrade (novel distractor detection), and (2) to act as a verifier that evaluates reimagined rollouts and selects/rejects action plans based on safety and task success.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o (vision-language model used for distractor identification and plan verification)",
            "abstraction_level": "operates on pixel/frame inputs and segmentation overlays (semantic/object-level reasoning from images)",
            "feature_selection_mechanism": "reasoning-based selection: prompted to compare initial vs 5th rollout frame with overlayed segmentation mask IDs and return mask IDs that disappear/degrade — effectively identifies task-irrelevant/novel regions by observed rollout implausibility",
            "task_domain": "visual reasoning for robotic manipulation plan verification and distractor detection",
            "distractor_presence": "Used specifically to detect novel static distractors that degrade in predicted rollouts; critical to ReOI pipeline",
            "performance_metrics": "Plan-verification selection accuracy and rejection accuracy reported in Appendix B (comparison between VLMs GPT-4o and LLaMA-4 shown visually); precise numeric percentages are provided in paper tables (not quoted in main text).",
            "computational_cost_details": "Uses an off-the-shelf VLM (GPT-4o) at test time for each candidate rollout (analyze two frames per rollout for distractor detection and evaluate concatenated predicted frames for verification); exact latency and compute not reported.",
            "comparison_to_baselines": "Two off-the-shelf VLMs (GPT-4o and LLaMA-4) evaluated as verifiers; GPT-4o is leveraged for distractor-ID pipeline and verification prompting. The paper provides prompt templates and qualitative comparisons.",
            "transfer_learning_results": "Not applicable to the VLMs (used zero-shot, no fine-tuning reported).",
            "multi_task_performance": "Not evaluated for VLM beyond verification role in single task family.",
            "failure_modes": "Accuracy depends on quality of segmentation overlay and on whether distractor degradation is visible by the 5th predicted frame; false positives/negatives in mask selection can propagate to inpainting and affect outcome.",
            "ablation_studies": "The paper discusses choice of initial vs 5th frame based on empirical observation (implausible behavior manifests within first four steps), but does not present large-scale ablations of VLM prompting variants in main text.",
            "sample_efficiency": "Zero-shot usage; no fine-tuning reported.",
            "generalization_analysis": "VLM reasoning is used as an open-world detector for novel degradations and shown effective in examples/quantitative distractor identification accuracy (Appendix), but dependent on segmentation quality.",
            "reconstruction_quality": "Not applicable.",
            "task_relevance_analysis": "VLM-based comparison of predicted frames is used to identify which segmented regions are behaving implausibly (degrading/disappearing) and hence likely task-irrelevant / novel distractors in the current context.",
            "dynamic_abstraction": "Not applicable.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "None reported.",
            "pixel_fidelity_benefits": "VLM verification relies on pixel-level composites of reinserted distractors to make safety judgements; thus pixel fidelity in the final visualizations is important for verifier decisions.",
            "uuid": "e2251.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dino-wm: World models on pre-trained visual features enable zero-shot planning.",
            "rating": 2
        },
        {
            "paper_title": "DINOv2",
            "rating": 2
        },
        {
            "paper_title": "Planning with learned dynamics: Probabilistic guarantees on safety and reachability via lipschitz constants",
            "rating": 2
        },
        {
            "paper_title": "Learning task informed abstractions",
            "rating": 2
        },
        {
            "paper_title": "Learning invariant representations for reinforcement learning without reconstruction",
            "rating": 2
        },
        {
            "paper_title": "Semail: eliminating distractors in visual imitation via separated models",
            "rating": 2
        },
        {
            "paper_title": "Diffusion policy: Visuomotor policy learning via action diffusion.",
            "rating": 2
        },
        {
            "paper_title": "Leveraging separated world model for exploration in visually distracted environments",
            "rating": 2
        }
    ],
    "cost": 0.0212225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control
19 Jun 2025</p>
<p>Yuxin Chen 
Denotes equal contribution</p>
<p>University of California
Berkeley</p>
<p>Jianglan Wei 
Denotes equal contribution</p>
<p>University of California
Berkeley</p>
<p>Chenfeng Xu 
University of California
Berkeley</p>
<p>Boyi Li 
University of California
Berkeley</p>
<p>Masayoshi Tomizuka 
University of California
Berkeley</p>
<p>Andrea Bajcsy 
Carnegie Mellon University</p>
<p>Ran Tian 
University of California
Berkeley</p>
<p>Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control
19 Jun 2025EEBDE3FABB77E5DE854CACFBE5CD370CarXiv:2506.16565v1[cs.RO]
World models enable robots to "imagine" future observations given current observations and planned actions, and have been increasingly adopted as generalized dynamics models to facilitate robot learning.Despite their promise, these models remain brittle when encountering novel visual distractors such as objects and background elements rarely seen during training.Specifically, novel distractors can corrupt action outcome predictions, causing downstream failures when robots rely on the world model imaginations for planning or action verification.In this work, we propose Reimagination with Observation Intervention (ReOI), a simple yet effective test-time strategy that enables world models to predict more reliable action outcomes in openworld scenarios where novel and unanticipated visual distractors are inevitable.Given the current robot observation, ReOI first detects visual distractors by identifying which elements of the scene degrade in physically implausible ways during world model prediction.Then, it modifies the current observation to remove these distractors and bring the observation closer to the training distribution.Finally, ReOI "reimagines" future outcomes with the modified observation and reintroduces the distractors posthoc to preserve visual consistency for downstream planning and verification.We validate our approach on a suite of robotic manipulation tasks in the context of action verification, where the verifier needs to select desired action plans based on predictions from a world model.Our results show that ReOI is robust to both in-distribution and out-of-distribution visual distractors.Notably, it improves task success rates by up to 3× in the presence of novel distractors, significantly outperforming action verification that relies on world model predictions without imagination interventions.</p>
<p>I. INTRODUCTION</p>
<p>World models [7,27] enable robots to predict actionconditioned future evolutions of their environments given current observations and planned actions.They have emerged as powerful generalized dynamics models and are increasingly used in model-based policy learning [14,4,21] as well as in deployment-time action plan verification [6,22].However, despite their promising potential, current world models in robotics are brittle against visual distractors such as taskirrelevant objects or background elements rarely seen during training.These novel distractors can corrupt the model, leading to hallucinated imaginations that ultimately compromise downstream action plan verification and selection.Notably, this brittleness persists even when models are trained on large, visually diverse datasets sourced beyond robotics [1,26].</p>
<p>To illustrate this challenge, consider a robot assistant tasked with meal preparation, picking up prepped ingredients and placing them into a cooking pan.The robot uses a world model, trained on demonstrations from similarly structured kitchens, to verify candidate action plans proposed by a pretrained imitation policy.Specifically, the robot uses the world model to imagine the future outcomes of each action plan and evaluates them using a visual reward function to identify the best action plan (i.e., policy verification).However, in open-world deployment, novel visual distractors are inevitable.For example, a user might place an unfamiliar high-pressure pot between the cutting board and the pan, or leave an Amazon package box in the background as illustrated in Figure 1.As the world model rolls out candidate action plans, these unfamiliar distractors often degrade in visually implausible ways, becoming distorted, disappearing, or warping unnaturally across predicted frames.These artifacts cause the model to misrepresent distractors that are critical for ensuring safety and hallucinate incorrect robot behaviors.As a result, the downstream plan verifier failed to detect that a planned motion that would collide with the pot, ultimately leading to deployment-time failures (Figure 1, top).</p>
<p>These failure cases expose a limitation in current world model-based robot policy verification pipelines and motivate the need for strategies that can mitigate the effects of novel distractors.Our key insight is that relying solely on trainingtime solutions is insufficient: the diversity and unpredictability of distractors, especially in open-world settings, make it impractical to fully capture them, even with large-scale datasets.To this end, we propose Reimagination with Observation Intervention, ReOI, a test-time and plug-in strategy that mitigates the impact of novel visual distractors on world model-based robot policy verification.Our key idea is to first identify and inpaint novel distractors from the current observation to bring it closer to the world model's training distribution, then reimagine future action outcomes using the modified input, and finally reintroduce the distractors post-hoc at the pixel level to preserve visual consistency for downstream planning and verification.</p>
<p>We validate our approach on robotic manipulation tasks in the context of action plan verification, where a verifier needs to select action plans based on visual outcome predictions from a world model.Our approach demonstrates strong robustness to both in-distribution and unfamiliar visual distractors.Notably, our method improves task success rates by up to 3x when encountering novel distractors compared to standard policy verification that directly relies on unmodified world model predictions.</p>
<p>Contribution.In this work, we raise and tackle the challenge of mitigating the effects of novel visual distractors on world model-based robot policy verification.We illustrate concrete failure cases induced by such distractors and introduce Reimagination with Observation Intervention (ReOI), a test-time, plug-in strategy that enables world models to produce more reliable action outcome predictions in open-world scenarios where novel and unanticipated distractors are inevitable.To the best of our knowledge, this is the first work that leverages test-time observation intervention to address the problem of novel visual distractors in world model-based robot policy verification and selection.</p>
<p>II. RELATED WORKS Mitigating visual distractors in world model learning.</p>
<p>Learning accurate world models in visually cluttered environments remains a challenge in robotics.Most existing approaches mitigate visual distractors through training-time strategies: either by leveraging privileged reward signals to learn task-relevant representations [24,5,18,28], or by training separate network branches to model task-relevant and task-irrelevant components, using only the task-relevant branch during downstream policy planning or verification [17,19,10].However, these methods primarily target in-distribution visual distractors and are tightly coupled to the training-time task context.When the task context shifts at test time, causing previously irrelevant distractors to become safety-critical or introducing novel distractors, their performance often degrades and even fails entirely [9].Rather than relying on task-specific supervision to suppress distractors during world model training and assuming the same context will hold at deployment, we propose a test-time observation intervention strategy to mitigate novel visual distractors' impact on world model-based robot policy verification.</p>
<p>Observation intervention for improving robot policy robustness.Modifying robot observations by masking out the background or task-irrelevant objects has proven effective in improving the robustness of imitation policies to visual distractors.Prior works have explored using separately trained models [12,16], VLMs [23], or conformal prediction techniques [8] to identify and inpaint visual distractors to improve policy performance.Different from these works that focus on modifying observation to improve model-free imitation policy robustness, our work focuses on test-time observation intervention for improving the reliability of model-based visual planning in the presence of novel visual distractors.</p>
<p>III. PRELIMINARIES</p>
<p>Sampling-based visual model predictive control.We formulate the robot planning problem as a sampling-based visual model predictive control problem where the robot uses a predictive world model to forward-simulate multiple actionconditioned futures and evaluates them using a reward function to select the best plan for execution.Mathematically, this problem is formulated as:
a ⋆ t = argmax at∼π(ot,ℓ) E ot∼f ϕ (ot,at) R o t ; ℓ ,(1)
where f ϕ (o t , a t ) is a visual dynamics model that predicts a sequence of future observations (o t := o t:t+T ) conditioned on an action plan a t and the current observation o t , and R is a reward function that evaluates the predicted outcomes given the task description (ℓ).This framework is particularly appealing because it can leverage pre-trained generative imitationbased policies (π) as action plan samplers and align the robot's behavior with deployment-time task context and preferences without fine-tuning or modifying the base policy.</p>
<p>World model as visual dynamics for action plan verification and selection.World models enable robots to "imagine" future observations given current observations and planned actions.Typically, a world model consists of three key components: an observation encoder model z t = E ϕ (o t ) that maps a visual observation o t into a latent state z t , a forward dynamics model z t+1 = fϕ (z t , a t ) that predicts the next latent state conditioned on the current latent state and action, and an observation decoder model o t = Q ϕ (z t ) that decodes a latent state to visual observation.World models have been increasingly adopted as generalized dynamics models to produce action-conditioned future outcomes for downstream robot policy learning [21,27] or verification under the model predictive control framework described in (1) [6,14,22].</p>
<p>Challenge: novel visual distractors cause hallucinations and compromise planning.While world models enable robots to imagine future visual outcomes and inform policy learning, the accuracy and confidence about their imaginations heavily depend on the consistency between the training environment and the deployment environment.The existing effort on exploring world models for robotics applications mostly focuses on training-testing consistent conditions [6,14,22]; however, real-world deployment inevitably involves openworld environments where novel visual distractors (such as objects and background elements rarely seen during training) are inevitable.Current world models are brittle under such conditions: these novel distractors can corrupt the model, leading to hallucinated imaginations that ultimately compromise downstream action plan verification and selection (refer to the motivating example in Figure 1).</p>
<p>IV. WORLD MODEL REIMAGINATION WITH TEST-TIME OBSERVATION INTERVENTION FOR VISUAL MODEL PREDICTIVE CONTROL</p>
<p>We propose Reimagination with Observation Intervention (ReOI), a test-time and plug-in strategy that mitigates the impact of novel visual distractors on world model-based policy verification and selection, where a verifier needs to select desired action plans based on visual outcome predictions from a world model.</p>
<p>Since mitigating dynamic distractors remains a largely open and underexplored challenge, in this work, we focus on mitigating the impact of static novel visual distractors on world model-based robot planning.We define visual distractors as static objects and background elements that are not directly related to the task from the task specification.Different from the definition in [8], we do not assume that distractors do not affect the robot's motion to reach the goal.Our key idea is to first identify and inpaint problematic novel distractors from the current observation to bring it closer to the training distribution, then reimagine future outcomes using the modified input, and finally reintroduce the distractors post-hoc to preserve visual consistency for downstream planning and verification.</p>
<p>A. Observation Intervention Strategy</p>
<p>Identify novel visual distractors from world model predictions.Since the original training data is typically unavailable at deployment time, we identify novel visual distractors using only the world model's predicted observations.Our key insight is that visual distractors underrepresented in the training distribution tend to exhibit physically implausible behavior in world model rollouts: although initially visible, they quickly become distorted, disappear, or warp unnaturally as the rollout progresses, even when the input actions are in the distribution (shown in Figure 2).This phenomenon arises because the latent dynamics model prioritizes consistent and predictable visual features learned from its training distribution.When a novel distractor appears, the model lacks an accurate latent representation of its dynamics, resulting in initial prediction errors.As predictions unfold autoregressively, these errors compound, causing the distractor's latent representation to degrade progressively.Consequently, pixels corresponding to unfamiliar distractors fail to be reliably propagated through the model's latent dynamics, leading to rapid visual distortion.</p>
<p>Building on this insight, we leverage a VLM's (GPT-4o) visual reasoning ability to identify problematic visual distractors through visual reasoning.Given the current observation, the world model first rolls out an in-distribution "safety-check" action plan to generate a sequence of predicted observations.The VLM is then prompted to analyze this predicted sequence.Leveraging its open-world visual reasoning capabilities, the VLM examines the temporal evolution of objects and flags those that undergo rapid distortion across frames as potential novel visual distractors.The output from the VLM is a string of object proposals deemed as novel visual distractors (shown in Figure 2).</p>
<p>Segmentation and inpainting.Once distractors are identified, we use a segmentation model [15] to localize and segment the corresponding regions.These regions are then passed to an image inpainting model [2], which removes the distractors and fills in the masked areas to produce a modified observation (shown in Figure 2).This intervention yields an input that more closely aligns with the world model's training distribution, alleviating hallucinations caused by artifacts from novel distractors.</p>
<p>Identify visual distractors via unrealistic imagination
Initial Observation Initial Observation Distractor</p>
<p>B. World Model Reimagination with Modified Observation</p>
<p>After intervention, we use the original world model again to predict the action plan outcomes given the modified input observation.While this mitigates hallucinated robot behaviors caused by spurious distractor artifacts, the resulting predictions no longer model the novel visual distractors since they were inpainted.To preserve visual consistency for downstream robot policy verification, we reintroduce the removed distractors into each predicted observation post-hoc.</p>
<p>Specifically, we first decompose each predicted observation into a set of object layers using Grounded-SAM2 [15], separating elements such as the robot, task-relevant objects, and background.For each layer, we estimate its depth from the predicted frame [13].We also extract the inpainted distractor layer from the original observation and estimate its depth based on its original spatial placement.This distractor layer is then added back into the set of layers for every predicted frame.Finally, we reconstruct each predicted frame by compositing the layers in back-to-front depth order, ensuring that all objects are rendered with correct occlusion.This layer-wise rendering approach maintains visual realism for downstream planning, even though the distractors were absent from the rollout itself (illustrated in Figure 3).</p>
<p>We note that this approach may introduce physically unreal-istic interactions, such as the robot gripper may appear to pass through reinserted distractors if they block the robot's motion.However, this does not compromise policy verification.Since the robot must avoid any physical contact with such distractors to ensure safety, any trajectory exhibiting these violations is automatically rejected during the verification process.</p>
<p>C. Action Plan Verification and Selection for Robot Visual Planning</p>
<p>We deploy ReOI as the visual dynamics model in the context of robot action plan verification, where a verifier needs to select action plans based on visual outcome predictions from ReOI.While our framework is agnostic to the choice of verifier, we instantiate it with a VLM that assesses each reimagined rollout in the context of the task instruction ℓ.The VLM is prompted to identify and reject plans that pose safety concerns (e.g., collisions with novel distractors or non-target objects) and to select the outcome that best aligns with the user's intent.If none of the proposed plans are deemed safe or suitable, the VLM can optionally escalate by requesting human intervention.</p>
<p>V. EXPERIMENT SETUP Testing environment.We conduct our evaluations using a Fanuc LR Mate 200iD/7L 6-DoF robot in a real-world robotic manipulation setup.The robot is tasked with picking and placing objects in a toy kitchen environment.</p>
<p>Pre-trained imitation-based generative policy.We use a Diffusion Policy [3] as the imitation-based action plan sampler.The model takes the current image observations from the wrist and third-person cameras as inputs to predict a distribution of the robot's future action plans (each action is a collection of a 3D waypoint and a gripper control signal).We use 120 multimode teleoperated demonstrations to train the policy.</p>
<p>World model training.We use the DINO-WM [27] as our base world model.DINO-WM leverages pre-trained DINOv2 [13] representation to encode visual observations and predict action outcomes directly in the DINOv2 latent representation space.We train the world model using 500 robot-environment interaction trajectories, with 200 trajectories sampled by rolling out the pre-trained diffusion policy and 300 random exploration trajectories.We separately fine-tune a DINOv2 feature decoder using images from the testing environment to map the predicted latent representations back to visual observations.More details can be found in Appendix A.</p>
<p>VI. RESULTS</p>
<p>A. On the Effect of Novel Visual Distractors on World Model Predictions</p>
<p>Qualitative example.In Figure 4, we demonstrate two representative examples illustrating how novel visual distractors (highlighted in red in the first frame of each groundtruth rollout) degrade the predictive performance of the world model.Across these examples, the presence of unfamiliar distractors causes DINO-WM to hallucinate and predict that a failed action plan (one that would not successfully pick up the green pepper) would succeed (left) and mistakenly erase the target object (the green pepper) from the predicted observations (right).In contrast, ReOI produces significantly more accurate and visually consistent predictions aligned with the true task execution by reimagining future observations through test-time observation intervention.</p>
<p>Full obs.eval.</p>
<p>In  Quantitative evaluation: overall prediction quality.We use two standard metrics, SSIM (Structural Similarity Index) [20] and LPIPS (Learned Perceptual Image Patch Similarity) [25], to evaluate the quality of the world model's predicted visual action outcomes.SSIM measures structural similarity based on contrast and spatial consistency, with higher scores indicating closer alignment.LPIPS evaluates perceptual similarity using deep feature comparisons, with lower scores indicating greater visual similarity.As shown in Table I, ReOI achieves better SSIM and LPIPS scores compared to DINO-WM.These improvements indicate that by shifting the input observation closer to the training distribution through testtime intervention and reimagining futures from this intervened input, our approach enables the world model to produce more accurate and robust predictions.</p>
<p>Quantitative evaluation: ReOI effectively mitigates the hallucination of in-distribution objects caused by novel visual distractors.Since ReOI reimagines at test time and explicitly reintroduces the novel visual distractors that DINO-WM struggles to predict, the previous evaluation does not fully disentangle the effect of these distractors on the world model's ability to predict the dynamics of in-distribution components such as the robot and target objects.To ablate this effect, we further inpaint the identified distractors in both the ground truth and predicted observations and then measure SSIM and LPIPS over the inpainted (distractor-free) predictions.The right side of Table I shows that ReOI still achieves better consistency and perceptual similarity compared to DINO-WM.These results indicate that without test-time intervention, novel visual distractors not only degrade the overall visual quality (e.g., distractors distorted across frames) but also corrupt the predicted dynamics of in-distribution objects (robot, target object), leading to hallucinated motions.</p>
<p>B. On the Value of Test-time Observation Intervention for Visual Planning</p>
<p>In this section, we evaluate the system-level performance in the context of action plan verification, where the VLM-based verifier needs to select desired action plans based on visual outcome predictions from the world model.More detailed component-level evaluation (visual distractor identification accuracy and plan verification accuracy) can be found in Appendix B).</p>
<p>Baselines.Our approach mitigates the impact of novel environmental variations on downstream planning by applying testtime observation intervention and reimagination.Alternatively, another intervention strategy is to reject untrustworthy world model action outcome predictions at test time to ensure safety.In addition to comparing against the base DINO-WM, we compare our approach against TrustRegion [11], which finds a region (a union of r-balls about the subset of the world model training data) where the learned visual dynamics model is deemed reliable for downstream planning and rejects world model predictions if the input observation and action plan pair falls outside of the trust region.More details about this baseline can be found in Appendix B.</p>
<p>Metric.We measure the task success rate and collision rate to evaluate the system-level performance of the robot visual planning.For each method, we conduct 10 trials with randomly initialized task configurations and report the average success rate.A trial is considered successful if the robot successfully and safely completes the task.System-level result.We present the system-level quantitative result in Table II.The results show that ReOI achieves a significantly higher task success rate compared to both DINO-WM and TrustRegion.Notably, ReOI maintains a low collision rate comparable to the conservative TrustRegion, while being substantially safer than DINO-WM that directly uses the observations for future prediction.These results suggest</p>
<p>ReOI (ours)</p>
<p>Correctly predict failed action plan outcome.GT Fig. 4: Qualitative examples of prediction quality.The presence of unfamiliar distractor objects causes DINO-WM to hallucinate and predict that a failed action plan would succeed (left side, middle row) and mistakenly erase the target object (the green pepper) from the predicted observations (right side, middle row).ReOI (bottom row) effectively mitigates the effect of the novel visual distractors and produces visual outcomes more aligned with the ground truth (top row).</p>
<p>Visual Planning with DINO-WM + ReOI (Ours) Visual Planning with DINO-WM (Baseline) Initial Observation</p>
<p>Sampled Actions</p>
<p>Pick up the green pepper and place it in the pan</p>
<p>WM</p>
<p>Collide with cooker</p>
<p>Complete the task</p>
<p>Choose Plan 2 as it looks safely without potential collision</p>
<p>Plan 2</p>
<p>Choose Plan 1 because it effectively places the pepper in the pan Plan 1</p>
<p>Plan outcomes re-imagination with intervened observation</p>
<p>Plan outcomes imagination with true observation intervened</p>
<p>Fig. 5: Qualitative example of robot visual planning.The novel distractor object causes DINO-WM to hallucinate and erase a critical obstacle, leading the VLM plan verifier to incorrectly accept an unsafe action plan.In contrast, ReOI generates visual outcome predictions that better aligned with the reality, allowing the plan verifier to recognize the potential collision with the distractor and correctly select a safe action plan for execution.</p>
<p>that ReOI enables more effective and safe visual planning by modifying the input observation and reimagining action outcomes at test time.We show two qualitative examples in Figure 1 and Figure 5 to demonstrate the effectiveness of our approach.</p>
<p>VII. CONCLUSION</p>
<p>World models enable robots to "imagine" future observations given current observations and planned actions, and have been increasingly adopted as generalized dynamics models to facilitate robot learning.Despite their promise, these models remain brittle when encountering novel visual distractors.Specifically, novel distractors can corrupt action outcome predictions, causing downstream failures when robots rely on the world model imaginations for planning or action verification.In this work, we proposed Reimagination with Observation Intervention, a simple yet effective test-time strategy that enables world models to predict more reliable action outcomes in open-world scenarios where novel and unanticipated visual distractors are inevitable.We validated our approach on robotic manipulation tasks in the context of action verification, where the verifier needs to select desired action plans based on predictions from a world model.Results showed that our approach is robust to both in-distribution and unfamiliar visual distractors.Notably, it improved task success rates by up to 3× in the presence of novel distractors, significantly outperforming action verification that relies on world model predictions without imagination interventions.</p>
<p>APPENDIX</p>
<p>Our key insight is that visual distractors underrepresented in the training data often behave implausibly during world model rollouts-quickly distorting, disappearing, or warping unnaturally over time.To detect such cases, we prompt the VLM with two frames from the world model's predicted rollout: the initial frame, which contains the full scene context including potential distractors, and the fifth frame, where empirical analysis shows that distractors are most likely to have degraded or disappeared.This selection is based on our observation that implausible distractor behavior typically manifests within the first four steps of rollout prediction.To enable precise spatial reasoning, we generate segmentation masks for the initial frame using Grounded-SAM2 [15] and overlay unique mask IDs directly onto the segmentation map.The VLM is then presented with the ID-labeled segmentation image alongside the initial and fifth frames, and is prompted to identify which regions (i.e., mask IDs) correspond to objects that have disappeared or changed unnaturally over time.</p>
<p>Prompt for novel distractor identification</p>
<p>You are a helpful assistant for a robotic system that analyzes images to detect changes and disappearing objects.I'll show you three images: 1. img init dis -The initial image 2. img wm -The same scene after some changes 3. mask overlay -The initial image with numbered masks/patches created by a segmentation algorithm Your task is to identify which numbered patches from the mask overlay image exist in img init dis but have disappeared in img wm.Look carefully at each numbered patch and determine if the corresponding object still present in img wm.</p>
<p>Hint: This is a kitchen environment with a robot arm, try only detect patches that the corresponding object are on the white table.</p>
<p>Please provide ONLY a list of the missing patch numbers with no additional explanation.For example, if patches 2, 5, and 7 have disappeared, just respond with: [2,5,7].</p>
<p>Once the VLM produces its response-typically in the form of a list of missing or degraded patch IDs (e.g., Missing patches: [2,8,9])-we use the identified mask indices to extract the corresponding image patches from the initial observation.These patches represent regions that are likely to be visual distractors causing inconsistencies in the rollout.Figure 7 shows an example of these extracted distractor patches.</p>
<p>A. DINO-WM Training</p>
<p>We adopt DINO-WM [27] as our base world model.The hyperparameters used for training are listed in Table III.Training is performed on 500 robot-environment interaction trajectories, comprising 200 rollouts from a pre-trained diffusion policy and 300 from random exploration.Each trajectory spans 24 steps with a 0.75-second interval between steps in the real world.We omit frameskip during training, as the existing time gap is sufficient-predicting further into the future would impair model accuracy.All models are trained on a single Nvidia A6000.All 500 training trajectories are collected in a controlled kitchen environment, where the robot is tasked with transporting a green pepper from a cutting board-randomly positioned on the left side of the table-to a cooking pan placed on an induction stove with varying orientations.To introduce additional variability, a spoon is also randomly placed on the table surface during data collection.As illustrated in Figure 8, DINO-WM is able to accurately predict future states with finegrained visual detail, maintaining both temporal and spatial consistency across frames.It also demonstrates robustness to environmental randomization within the training domain.</p>
<p>However, when we introduce novel visual distractors-such as an Amazon box, a kettle, and a high-pressure pot-randomly positioned within the scene, the predictive performance of DINO-WM significantly deteriorates.In these out-of-distribution scenarios, parts of the predicted images become visibly distorted, disappear altogether, or warp in physically implausible ways across the rollout sequence, as shown in Figure 9.</p>
<p>Figure 10 presents the reimagined rollout results using modified input observations for the same 10 initial states containing visual distractors.While some artifacts remain-primarily due Fig. 6: The VLM is prompted to identify visual distractors by reasoning about missing objects between the initial frame (middle) and the fifth frame (right) of the world model rollout, using mask IDs from the semantic segmentation (left).to limitations in the segmentation and inpainting models-the severe hallucinations observed in the original rollouts have been effectively mitigated.Notably, the differences in row 4 and row 9 between Figure 9 and Figure 10 highlight the improved visual stability and consistency achieved through our intervention.</p>
<p>Mask Distractor Patch</p>
<p>Initial Observation</p>
<p>B. DINOv2 Feature Decoder</p>
<p>To enable accurate reconstruction of visual observations from predicted latent representations in the testing environment, we perform a dedicated fine-tuning of the DINOv2 feature decoder.This process is carried out separately from the original world model training.We first collect a new dataset consisting of 80 trajectories captured within the testing environment, which intentionally includes various visual distractors not present during the initial training phase.These trajectories are used to better adapt the feature decoder to the distributional shift introduced by the novel visual elements.</p>
<p>We then initialize the training pipeline by loading a pretrained DINO-WM checkpoint, which contains the previously trained dynamics model and feature decoder.However, during fine-tuning, we freeze the parameters of the dynamics model entirely to preserve its learned temporal representations and focus solely on updating the parameters of the DINOv2 feature decoder.This ensures that only the visual decoding pathway is adapted to the new environment, without altering the predictive capabilities of the dynamics model itself.</p>
<p>The fine-tuning procedure is conducted over a single epoch using the newly collected dataset.This one-pass update is sufficient to adjust the decoder's mapping from latent space to image space, improving reconstruction fidelity in the presence of unfamiliar distractors while maintaining consistency with the world model's learned representations.</p>
<p>Our approach mitigates the impact of novel environmental variations on downstream planning by applying test-time observation intervention and reimagination.Alternatively, another intervention strategy is to reject untrustworthy world model action outcome predictions at test time to ensure safety.In addition to comparing against the base DINO-WM, we compare our approach against TrustRegion [11], which finds a region where the learned visual dynamics model is deemed reliable for downstream planning and rejects world model predictions if the input observation and action plan pair falls outside of the trust region.</p>
<p>Lipschitz constant of a function quantifies the maximum rate at which the function's output can change with respect to changes in its input.Formally, a function f is Lipschitz continuous if there exists a constant L ≥ 0 such that for all</p>
<p>Fig. 1 :
1
Fig. 1: Reimagination with observation intervention for distractor-robust world model-based robot planning.A robot uses a world model (WM) to verify and select action plans proposed by a pre-trained imitation policy.The world model has never encountered the box, cooker, or teapot during training.Top (baseline): Novel visual distractors become distorted across predicted observations, causing the model to hallucinate incorrect robot behaviors and erase a critical obstacle.This leads the verifier to select an unsafe action plan.Bottom (ours): Test-time observation intervention enables the world model to generate predictions better aligned with reality, allowing the verifier to recognize potential collisions and correctly select a safe action plan.</p>
<p>Fig. 2 :
2
Fig. 2: Novel visual distractor identification and observation intervention.We leverage a VLM to analyze the temporal evolution of objects and flag those that undergo rapid distortion across frames as potential novel visual distractors (left-side; more examples and the full prompt can be found in Appendix A).These problematic distractors are inpainted to bring the observation closer to the world model's training distribution (right-side).</p>
<p>Fig. 3 :
3
Fig.3: Reimagination with modified observation.Predicted future observations from the modified input are post-processed by reintroducing previously inpainted distractors using depthaware compositing.This ensures correct occlusion, e.g., the package box appears behind the gripper.</p>
<p>Critical obstacle erased!Pose safety risks for planning!Novel visual distractor (shaded in red) causes the world model to hallucinate!Critical obstacle restored to ensure the rejection of this action plan in planning.</p>
<p>Fig. 7 :
7
Fig. 7: Distractor patches extracted from the initial observation based on the identified mask.</p>
<p>Fig. 8 :
8
Fig. 8: Trajectories predicted with DINO-WM with 10 different initial observations without visual distractors.</p>
<p>Fig. 9 :
9
Fig. 9: Trajectories predicted with DINO-WM with 10 different initial observations with visual distractors.</p>
<p>Fig. 10 :
10
Fig. 10: Trajectories reimaginated by DINO-WM with 10 modified initial observations.</p>
<p>TABLE I :
I
Quantitative</p>
<p>evaluations of a world model's predicted visual action outcomes.We measure SSIM and LPIPS to evaluate the structural and perceptual similarity of the predicted visual action outcome.The left side shows results on full predicted observations, while the right side focuses on task-relevant, in-distribution objects after inpainting distractors from both the predictions and ground truth.ReOI enables the world model to produce more accurate visual predictions and effectively mitigates the hallucination of in-distribution objects caused by novel visual distractors.</p>
<p>TABLE II :
II
Task Success Rate.ReOI enables more effective and safe visual planning compared to baselines.</p>
<p>TABLE III :
III
Hyperparameters for DINO-WM training.
NameValueseed0epochs300batch size64save every x epoch10reconstruct every x batch500num reconstruct samples6encoder lr1e-6decoder lr2e-5predictor lr5e-4action encoder lr5e-4img size224frameskip1concat dim1saved foldernullnormalize actionTrueaction emb dim10num action repeat1proprio emb dim10num proprio repeat1num hist3num pred1has predictorTruehas decoderTrue
inputs x 1 and x 2 , the following holds: ∥f (x 1 ) − f (x 2 )∥ ≤ L • ∥x − x 2 ∥.Here, is denotes the Lipschitz constant, and it provides an upper bound on the function's sensitivity.TrustRegion leverages an estimated Lipschitz constant (bound how much outputs change with respect to a change in the inputs) of the world model visual outcome prediction error to obtain an error bound for a test-time input observation.By defining a maximum acceptable visual dynamics prediction error threshold, we could find the corresponding trust region such that any input observation and action plan pair within this region produces an error bounded by the threshold.Let T denote a trust region (a union of r-balls about the subset of the world model training data), b T denote the dispersion of the region, and e T be the maximum training error of the trained world model.Then for any input (a pair of an initial observation and an action plan) within the T , its world model prediction error is bound by(Implementation.We estimate the Lipschitz constant of the world model's visual action plan outcome prediction error.In this work, we use the L2 difference between the world model's predicted latent state trajectory and the ground-truth latent state (DinoV2 embedding) trajectory given the same initial observation and the action plan as the prediction error.Following the procedure from[11], we begin by selecting training data points whose prediction error is below a threshold of 750 (an empirically chosen value below which the model's predictions typically highly align well with ground truth).These filtered data points are used to initialize the trust region T 0 , defined as a union of r 0 -balls centered around the selected points.In our experiments, we set r 0 as a unit ball with a radius of 0.1, and the corresponding Lipschitz constant estimated within T 0 is 0.84.To maximize the flexibility of the trust region for planning, we progressively increase r to expand the region, while ensuring that the Lipschitz constant remains within acceptable bounds (&lt; 1).The full procedure for this expansion follows Algorithm 2 in[11].The final trust region has a Lipschitz constant of 0.93 and the error bound is 1160.C. Action Plan Verification and SelectionWe use a VLM as the verifier to evaluate each sampled action plan's visual outcome in the context of the task instruction ℓ.The VLM receives a sequence of predicted observations from the world model (sampled as four concatenated frames) and is prompted to identify and reject plans that pose safety concerns-such as collisions with novel distractors or nontarget objects-and to select the outcome that best aligns with the user's intent.If none of the proposed plans are deemed safe or suitable, the VLM can optionally escalate by requesting human intervention.Plan verification and selection accuracy.We evaluate two off-the-shelf vision-language models (VLMs), GPT-4o and LLaMA 4, as verifiers without any task-specific fine-tuning.(Llama-4) Plan Analysis: Task status: Failure -the pepper is never picked up, let alone placed on the pan.No obvious collisions with the pot, spoon, or scale, but the robot spends time gripping a large box that isn't part of the task, so it's both inefficient and incorrect.the percentage of trials in which the VLM correctly rejects an unsafe or failed action plan.Selection accuracy measures the percentage of trials in which the VLM either correctly selects the most desirable action plan or correctly rejects all options if none are safe.Both metrics are computed based on visual outcome predictions generated by ReOI, with groundtruth labels provided by a human expert.In Figure11, we show response comparisons from the GPT-4o and LLaMa-4.Prompt template.Figure12illustrates the system prompt used during the verification stage, along with example responses from the VLM.You are a helpful assistant aiding a robot in selecting a safe and effective action plan.will be shown multiple image grids, each depicting a future trajectory of the robot.The robot is a yellow arm with 7 joints and a gripper.Your task is to analyze each predicted future and identify the plan that safely and successfully completes the task described below.Wach out for the potential collisions between the robot and other non-target objects.If none of the plans are safe, return None.Otherwise, return the ID of the best plan.Task description: The robot needs to pick up the pepper and place the pepper on the pan.
Cosmos world foundation model platform for physical ai. Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, arXiv:2501.035752025arXiv preprint</p>
<p>Rovi-aug: Robot and viewpoint augmentation for cross-embodiment robot learning. Lawrence Yunliang, Chen , Chenfeng Xu, Karthik Dharmarajan, Muhammad Zubair Irshad, Richard Cheng, Kurt Keutzer, Masayoshi Tomizuka, Quan Vuong, Ken Goldberg, arXiv:2409.034032024arXiv preprint</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, Shuran Song, Proceedings of Robotics: Science and Systems (RSS). Robotics: Science and Systems (RSS)2023</p>
<p>Improving transformer world models for data-efficient rl. Antoine Dedieu, Joseph Ortiz, Xinghua Lou, Carter Wendelken, Wolfgang Lehrach, Miguel Swaroop Guntupalli, Kevin Patrick Lazaro-Gredilla, Murphy, arXiv:2502.015912025arXiv preprint</p>
<p>Learning task informed abstractions. Xiang Fu, Ge Yang, Pulkit Agrawal, Tommi Jaakkola, International Conference on Machine Learning. PMLR2021</p>
<p>Flip: Flow-centric generative planning as general-purpose manipulation world model. Chongkai Gao, Haozhuo Zhang, Zhixuan Xu, Cai Zhehao, Lin Shao, The Thirteenth International Conference on Learning Representations. </p>
<p>Recurrent world models facilitate policy evolution. David Ha, Jürgen Schmidhuber, Advances in neural information processing systems. 201831</p>
<p>Run-time observation interventions make visionlanguage-action models more visually robust. J Asher, Allen Z Hancock, Anirudha Ren, Majumdar, arXiv:2410.019712024arXiv preprint</p>
<p>1x world model: Evaluating bits, not atoms. Daniel Ho, Jack Monas, Juntao Ren, Christina Yu, Supplementary technical progress report. Palo Alto, CAJune 2025Technical report, 1X Technologies</p>
<p>Leveraging separated world model for exploration in visually distracted environments. Kaichen Huang, Shenghua Wan, Minghao Shao, Hai-Hang Sun, Le Gan, Shuai Feng, De-Chuan Zhan, Advances in Neural Information Processing Systems. 202437</p>
<p>Planning with learned dynamics: Probabilistic guarantees on safety and reachability via lipschitz constants. Craig Knuth, Glen Chou, Necmiye Ozay, Dmitry Berenson, IEEE Robotics and Automation Letters. 632021</p>
<p>Roso: Improving robotic policy inference via synthetic observations. Yusuke Miyashita, Dimitris Gahtidis, Colin La, Jeremy Rabinowicz, Jurgen Leitner, arXiv:2311.166802023arXiv preprint</p>
<p>Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, arXiv:2304.07193Learning robust visual features without supervision. 2023arXiv preprint</p>
<p>Strengthening generative robot policies through predictive world modeling. Han Qi, Haocheng Yin, Yilun Du, Heng Yang, arXiv:2502.006222025arXiv preprint</p>
<p>Grounded sam: Assembling openworld models for diverse visual tasks. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, arXiv:2401.141592024arXiv preprint</p>
<p>Less is more-the dispatcher/executor principle for multi-task reinforcement learning. Martin Riedmiller, Tim Hertweck, Roland Hafner, arXiv:2312.091202023arXiv preprint</p>
<p>Semail: eliminating distractors in visual imitation via separated models. Yucen Shenghua Wan, Minghao Wang, Ruying Shao, De-Chuan Chen, Zhan, International Conference on Machine Learning. PMLR2023</p>
<p>Denoised mdps: Learning world models better than the world itself. Tongzhou Wang, Simon S Du, Antonio Torralba, Phillip Isola, Amy Zhang, Yuandong Tian, arXiv:2206.154772022arXiv preprint</p>
<p>Ad3: Implicit action is the key for world models to distinguish the diverse visual distractors. Yucen Wang, Shenghua Wan, Le Gan, Shuai Feng, De-Chuan Zhan, International Conference on Machine Learning. PMLR2024</p>
<p>Image quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, IEEE transactions on image processing. 1342004</p>
<p>Daydreamer: World models for physical robot learning. Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, Ken Goldberg, Conference on robot learning. PMLR2023</p>
<p>Yilin Wu, Ran Tian, Gokul Swamy, Andrea Bajcsy, arXiv:2502.01828From foresight to forethought: Vlm-in-the-loop policy steering via latent alignment. 2025arXiv preprint</p>
<p>Transferring foundation models for generalizable robotic manipulation. Jiange Yang, Wenhui Tan, Chuhao Jin, Keling Yao, Bei Liu, Jianlong Fu, Ruihua Song, Gangshan Wu, Limin Wang, 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE2025</p>
<p>Learning invariant representations for reinforcement learning without reconstruction. Amy Zhang, Rowan Mcallister, Roberto Calandra, Yarin Gal, Sergey Levine, arXiv:2006.107422020arXiv preprint</p>
<p>The unreasonable effectiveness of deep features as a perceptual metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Qiao Haoyu Zhen, Pengxiao Sun, Siyuan Han, Yilun Zhou, Chuang Du, Gan, Learning 4d embodied world models. </p>
<p>Dino-wm: World models on pre-trained visual features enable zero-shot planning. Gaoyue Zhou, Hengkai Pan, Yann Lecun, Lerrel Pinto, arXiv:2411.049832024arXiv preprint</p>
<p>Repo: Resilient model-based reinforcement learning by regularizing posterior predictability. Chuning Zhu, Max Simchowitz, Siri Gadipudi, Abhishek Gupta, Advances in Neural Information Processing Systems. 202336</p>            </div>
        </div>

    </div>
</body>
</html>
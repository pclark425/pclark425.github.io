<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5358 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5358</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5358</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-240cd5f1b47a68c9dcc04b3921e69093c9a55b02</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/240cd5f1b47a68c9dcc04b3921e69093c9a55b02" target="_blank">Random Walks and Neural Network Language Models on Knowledge Bases</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A novel algorithm is presented which encodes the structure of a knowledge base in a continuous vector space, combining random walks and neural net language models in order to produce novel word representations, improving the state of the art in the similarity dataset.</p>
                <p><strong>Paper Abstract:</strong> Random walks over large knowledge bases like WordNet have been successfully used in word similarity, relatedness and disambiguation tasks. Unfortunately, those algorithms are relatively slow for large repositories, with significant memory footprints. In this paper we present a novel algorithm which encodes the structure of a knowledge base in a continuous vector space, combining random walks and neural net language models in order to produce novel word representations. Evaluation in word relatedness and similarity datasets yields equal or better results than those of a random walk algorithm, using a dense representation (300 dimensions instead of 117K). Furthermore, the word representations are complementary to those of the random walk algorithm and to corpus-based continuous representations, improving the stateof-the-art in the similarity dataset. Our technique opens up exciting opportunities to combine distributional and knowledge-based word representations.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5358.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5358.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RWSGRAM / RWCBOW</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Walks + Neural Network Language Model (Skip-gram / CBOW) on Knowledge Base</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text procedure that generates pseudo-sentences by Monte Carlo random walks over a knowledge-base graph (WordNet), emitting words at visited nodes according to a lexicon mapping, and then trains standard NNLMs (Skip-gram or CBOW) on the synthetic corpus to obtain dense 300-d word embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Random-walk pseudo-sentence generation (graph-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Treat the KB as an undirected graph G=(V,E) and an inverse dictionary mapping vertices to lexicalizations with probabilities. Repeatedly (a) pick a starting vertex uniformly at random, (b) run a Monte Carlo random walk: at each step terminate with probability (1-α) or choose a uniformly random neighbor with probability α, (c) when a vertex is visited emit a surface word sampled from the vertex's lexicalization probabilities; when the walk terminates the emitted word sequence is a pseudo-sentence. Collect many pseudo-sentences and train a standard NNLM (Skip-gram or CBOW) on them (vector size 300, window 5, 3 iterations, 5 negative samples in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge base graph (WordNet synset graph with gloss relations)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact dense embeddings (300 dimensions) compared to original concept-space vectors (≈117k dims); encodes KB structural information via local random-walk contexts; captures both similarity and relatedness signals; complementary to both Personalized PageRank vectors and corpus-trained embeddings; depends on a lexicalization dictionary (so coverage limited by dictionary); properties controllable via damping factor (α) and number of generated contexts (convergence observed around 70M contexts).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Semantic relatedness and similarity (word-pair tasks): WS353 (relatedness) and SimLex-999 / SL999 (similarity). Cosine similarity between word vectors compared to human ratings with Spearman rank correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Spearman correlation vs human judgements. Reported results (Table 1): RWSGRAM (Skip-gram trained on random-walk corpus) — SL999: 0.520, WS353: 0.683. RWCBOW — SL999: 0.486, WS353: 0.591. Training details: vector size 300, window=5, negative samples=5, 3 NNLM iterations, damping α=0.85, up to 70M pseudo-sentences (convergence ≈70M).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to: (a) PPV (Personalized PageRank concept-space vectors): RWSGRAM matches or slightly improves on PPV for SL999 and is on-par for WS353 (PPV SL999: 0.493, WS353: 0.683). (b) Corpus-based Skip-gram embeddings (pretrained on large text): Skip-gram on text achieves SL999: 0.442 and WS353: 0.686 — better on WS353 but worse on SL999. Combining RWSGRAM with PPV and corpus Skip-gram (by averaging ranks) yields further improvements and achieves a new best reported result on SL999 in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Coverage limited by dictionary / lexicalization probabilities (relatively small number of embeddings available); requires many generated pseudo-sentences for convergence (≈70M contexts reported); does not by itself scale the vocabulary beyond what KB lexicalizations provide; quality depends on choice of relations included in the KB (they used WordNet gloss relations) and on damping factor α; authors note simple combination strategies were used and more sophisticated fusion could yield better results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5358.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5358.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Monte Carlo PageRank sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo methods for PageRank / random-walk sampling (Avrachenkov et al. approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Monte Carlo sampling algorithm for PageRank-style random walks used to generate random-walk trajectories efficiently (the paper uses this as the generator of pseudo-sentences).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Monte carlo methods in pagerank computation: When one iteration is sufficient</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Monte Carlo random-walk sampling for context generation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use a Monte Carlo PageRank method to simulate random walks on the KB: at each step continue with probability α to a randomly selected neighbor or terminate with probability 1-α; treat each visited node as an emission point for a lexical item, producing sequences of tokens (pseudo-sentences) for NNLM training.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge base graph (WordNet used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Probabilistic sampling of graph neighborhoods yielding local-context sequences; stochastic and scalable via sampling; parameters (α, number of walks/contexts) control the locality vs. breadth of contexts and convergence behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used as internal mechanism to generate corpora for the NNLM evaluation tasks (WS353, SL999).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No independent metric reported for the sampling algorithm itself; its effect is measured via downstream Spearman correlations of embeddings trained on the generated corpora (see RWSGRAM/RWCBOW numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>The Monte Carlo sampling is the mechanism used to produce pseudo-text for NNLM training and is compared indirectly to PPV (which uses Personalized PageRank distributions rather than sampled pseudo-sentences). The sampled pseudo-sentence + NNLM approach yields compact embeddings competitive with PPV and complementary to corpus-based embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Sampling variance and need for large numbers of sampled contexts for stable embeddings (convergence observed ≈70M sentences); choice of termination probability α strongly affects context lengths and hence the semantic signal; requires a reliable inverse-dictionary to emit lexical items from nodes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5358.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5358.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPV (Personalized PageRank)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Personalized PageRank concept-space vectors (UKB implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Personalized PageRank-based method over WordNet that yields a high-dimensional conceptual vector for each target by running personalized random-walk/PPR to convergence and using the steady-state distribution over synsets as features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Personalized PageRank concept vector (no text conversion)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For a target word, run a Personalized PageRank algorithm on the KB graph to obtain a probability distribution over concepts (synsets); represent the word as this high-dimensional vector with one dimension per synset (the feature being the PPR score). This is not converted into pseudo-text; it is a direct graph-to-vector mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge base graph (WordNet synset graph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Very high-dimensional (≈117,522 dims for WordNet used in experiments), interpretable dimensions (one per synset), directly grounded in KB structure, potentially memory-intensive and slower than compact embeddings, captures global graph structure via steady-state probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Word relatedness/similarity: WS353 and SL999 (same evaluation procedure: cosine similarity and Spearman correlation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Spearman correlation (Table 1): PPV — SL999: 0.493, WS353: 0.683.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared directly to random-walk + NNLM embeddings (RWSGRAM/RWCBOW) and corpus Skip-gram. PPV is competitive with RWSGRAM on WS353 (both ≈0.683) but RWSGRAM outperforms PPV on SL999 (0.520 vs 0.493). Combining PPV with RWSGRAM and corpus Skip-gram yields further improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High dimensionality and memory footprint (one dimension per synset); computational cost for large KBs; less compact for downstream tasks compared to NNLM-trained 300-d embeddings; does not produce a low-dimensional embedding directly suitable for many neural methods.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5358.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5358.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Corpus Skip-gram (pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Skip-gram neural language model trained on large text corpora (Mikolov et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard distributional Skip-gram embeddings trained on large raw text corpora (not graph-derived); used as a corpus-based baseline and combined with KB-derived embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient estimation of word representations in vector space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Text-trained Skip-gram embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Train Skip-gram (word2vec) on raw textual corpora (predict context words given target), producing dense vector representations (300 dimensions used in comparisons). Not a graph-to-text method, but serves as a baseline and a complement when combined with KB-derived embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>N/A (text-based / distributional)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact dense vectors (300 dims), capture distributional relatedness; observed tendency to group related words (e.g., thematic relations) more strongly than strict similarity in the paper's qualitative analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>WS353 and SL999 (same evaluation: cosine + Spearman correlation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Spearman correlation (Table 1): Skip-gram (text pretrained) — SL999: 0.442, WS353: 0.686.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Skip-gram on text outperforms KB-only methods on WS353 in this study but underperforms on SL999; combining Skip-gram with KB-derived RWSGRAM and PPV yields improved results over any single method, indicating complementarity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>May conflate relatedness with similarity depending on window size and data; lacks explicit structured KB information; performance depends on corpus size and domain.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5358.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5358.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KB embedding (Wang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge graph and text jointly embedding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned related work: methods that embed KB entities and typed relations into a shared low-dimensional vector space by fitting triple likelihoods, used for relation inference; different from the paper's approach which produces pseudo-text and trains NNLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge graph and text jointly embedding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triple-based KB embedding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent each entity as a k-dimensional vector and model typed relations (e.g., born-in-city) as operations in that vector space; parameters are learned by maximizing triple likelihoods, enabling inference of missing relations. This approach learns embeddings directly from KB triples rather than converting graphs into text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (entities and typed relations, e.g., Freebase / WordNet)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Learns compact entity and relation vectors; focuses on modeling typed relations for inference; generally not producing pseudo-text for NNLM training.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Relation prediction / KB completion (mentioned in related work, not evaluated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Mentioned as contrasting approach: rather than producing pseudo-sentences and training NNLMs, these methods directly fit embeddings to triples. The paper positions its method as complementary — explicitly modeling contexts for words via random walks versus fitting triple likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in depth in this paper (only referenced).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Monte carlo methods in pagerank computation: When one iteration is sufficient <em>(Rating: 2)</em></li>
                <li>Random walks for knowledge-based word sense disambiguation <em>(Rating: 2)</em></li>
                <li>Exploring Knowledge Bases for Similarity <em>(Rating: 2)</em></li>
                <li>Efficient estimation of word representations in vector space <em>(Rating: 2)</em></li>
                <li>Knowledge graph and text jointly embedding <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5358",
    "paper_id": "paper-240cd5f1b47a68c9dcc04b3921e69093c9a55b02",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "RWSGRAM / RWCBOW",
            "name_full": "Random Walks + Neural Network Language Model (Skip-gram / CBOW) on Knowledge Base",
            "brief_description": "A graph-to-text procedure that generates pseudo-sentences by Monte Carlo random walks over a knowledge-base graph (WordNet), emitting words at visited nodes according to a lexicon mapping, and then trains standard NNLMs (Skip-gram or CBOW) on the synthetic corpus to obtain dense 300-d word embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Random-walk pseudo-sentence generation (graph-to-text)",
            "representation_description": "Treat the KB as an undirected graph G=(V,E) and an inverse dictionary mapping vertices to lexicalizations with probabilities. Repeatedly (a) pick a starting vertex uniformly at random, (b) run a Monte Carlo random walk: at each step terminate with probability (1-α) or choose a uniformly random neighbor with probability α, (c) when a vertex is visited emit a surface word sampled from the vertex's lexicalization probabilities; when the walk terminates the emitted word sequence is a pseudo-sentence. Collect many pseudo-sentences and train a standard NNLM (Skip-gram or CBOW) on them (vector size 300, window 5, 3 iterations, 5 negative samples in the paper).",
            "graph_type": "Knowledge base graph (WordNet synset graph with gloss relations)",
            "representation_properties": "Compact dense embeddings (300 dimensions) compared to original concept-space vectors (≈117k dims); encodes KB structural information via local random-walk contexts; captures both similarity and relatedness signals; complementary to both Personalized PageRank vectors and corpus-trained embeddings; depends on a lexicalization dictionary (so coverage limited by dictionary); properties controllable via damping factor (α) and number of generated contexts (convergence observed around 70M contexts).",
            "evaluation_task": "Semantic relatedness and similarity (word-pair tasks): WS353 (relatedness) and SimLex-999 / SL999 (similarity). Cosine similarity between word vectors compared to human ratings with Spearman rank correlation.",
            "performance_metrics": "Spearman correlation vs human judgements. Reported results (Table 1): RWSGRAM (Skip-gram trained on random-walk corpus) — SL999: 0.520, WS353: 0.683. RWCBOW — SL999: 0.486, WS353: 0.591. Training details: vector size 300, window=5, negative samples=5, 3 NNLM iterations, damping α=0.85, up to 70M pseudo-sentences (convergence ≈70M).",
            "comparison_to_other_representations": "Compared to: (a) PPV (Personalized PageRank concept-space vectors): RWSGRAM matches or slightly improves on PPV for SL999 and is on-par for WS353 (PPV SL999: 0.493, WS353: 0.683). (b) Corpus-based Skip-gram embeddings (pretrained on large text): Skip-gram on text achieves SL999: 0.442 and WS353: 0.686 — better on WS353 but worse on SL999. Combining RWSGRAM with PPV and corpus Skip-gram (by averaging ranks) yields further improvements and achieves a new best reported result on SL999 in the paper's experiments.",
            "limitations_or_challenges": "Coverage limited by dictionary / lexicalization probabilities (relatively small number of embeddings available); requires many generated pseudo-sentences for convergence (≈70M contexts reported); does not by itself scale the vocabulary beyond what KB lexicalizations provide; quality depends on choice of relations included in the KB (they used WordNet gloss relations) and on damping factor α; authors note simple combination strategies were used and more sophisticated fusion could yield better results.",
            "uuid": "e5358.0"
        },
        {
            "name_short": "Monte Carlo PageRank sampling",
            "name_full": "Monte Carlo methods for PageRank / random-walk sampling (Avrachenkov et al. approach)",
            "brief_description": "A Monte Carlo sampling algorithm for PageRank-style random walks used to generate random-walk trajectories efficiently (the paper uses this as the generator of pseudo-sentences).",
            "citation_title": "Monte carlo methods in pagerank computation: When one iteration is sufficient",
            "mention_or_use": "use",
            "representation_name": "Monte Carlo random-walk sampling for context generation",
            "representation_description": "Use a Monte Carlo PageRank method to simulate random walks on the KB: at each step continue with probability α to a randomly selected neighbor or terminate with probability 1-α; treat each visited node as an emission point for a lexical item, producing sequences of tokens (pseudo-sentences) for NNLM training.",
            "graph_type": "Knowledge base graph (WordNet used in experiments)",
            "representation_properties": "Probabilistic sampling of graph neighborhoods yielding local-context sequences; stochastic and scalable via sampling; parameters (α, number of walks/contexts) control the locality vs. breadth of contexts and convergence behavior.",
            "evaluation_task": "Used as internal mechanism to generate corpora for the NNLM evaluation tasks (WS353, SL999).",
            "performance_metrics": "No independent metric reported for the sampling algorithm itself; its effect is measured via downstream Spearman correlations of embeddings trained on the generated corpora (see RWSGRAM/RWCBOW numbers).",
            "comparison_to_other_representations": "The Monte Carlo sampling is the mechanism used to produce pseudo-text for NNLM training and is compared indirectly to PPV (which uses Personalized PageRank distributions rather than sampled pseudo-sentences). The sampled pseudo-sentence + NNLM approach yields compact embeddings competitive with PPV and complementary to corpus-based embeddings.",
            "limitations_or_challenges": "Sampling variance and need for large numbers of sampled contexts for stable embeddings (convergence observed ≈70M sentences); choice of termination probability α strongly affects context lengths and hence the semantic signal; requires a reliable inverse-dictionary to emit lexical items from nodes.",
            "uuid": "e5358.1"
        },
        {
            "name_short": "PPV (Personalized PageRank)",
            "name_full": "Personalized PageRank concept-space vectors (UKB implementation)",
            "brief_description": "A Personalized PageRank-based method over WordNet that yields a high-dimensional conceptual vector for each target by running personalized random-walk/PPR to convergence and using the steady-state distribution over synsets as features.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Personalized PageRank concept vector (no text conversion)",
            "representation_description": "For a target word, run a Personalized PageRank algorithm on the KB graph to obtain a probability distribution over concepts (synsets); represent the word as this high-dimensional vector with one dimension per synset (the feature being the PPR score). This is not converted into pseudo-text; it is a direct graph-to-vector mapping.",
            "graph_type": "Knowledge base graph (WordNet synset graph)",
            "representation_properties": "Very high-dimensional (≈117,522 dims for WordNet used in experiments), interpretable dimensions (one per synset), directly grounded in KB structure, potentially memory-intensive and slower than compact embeddings, captures global graph structure via steady-state probabilities.",
            "evaluation_task": "Word relatedness/similarity: WS353 and SL999 (same evaluation procedure: cosine similarity and Spearman correlation).",
            "performance_metrics": "Spearman correlation (Table 1): PPV — SL999: 0.493, WS353: 0.683.",
            "comparison_to_other_representations": "Compared directly to random-walk + NNLM embeddings (RWSGRAM/RWCBOW) and corpus Skip-gram. PPV is competitive with RWSGRAM on WS353 (both ≈0.683) but RWSGRAM outperforms PPV on SL999 (0.520 vs 0.493). Combining PPV with RWSGRAM and corpus Skip-gram yields further improvements.",
            "limitations_or_challenges": "High dimensionality and memory footprint (one dimension per synset); computational cost for large KBs; less compact for downstream tasks compared to NNLM-trained 300-d embeddings; does not produce a low-dimensional embedding directly suitable for many neural methods.",
            "uuid": "e5358.2"
        },
        {
            "name_short": "Corpus Skip-gram (pretrained)",
            "name_full": "Skip-gram neural language model trained on large text corpora (Mikolov et al.)",
            "brief_description": "Standard distributional Skip-gram embeddings trained on large raw text corpora (not graph-derived); used as a corpus-based baseline and combined with KB-derived embeddings.",
            "citation_title": "Efficient estimation of word representations in vector space",
            "mention_or_use": "use",
            "representation_name": "Text-trained Skip-gram embeddings",
            "representation_description": "Train Skip-gram (word2vec) on raw textual corpora (predict context words given target), producing dense vector representations (300 dimensions used in comparisons). Not a graph-to-text method, but serves as a baseline and a complement when combined with KB-derived embeddings.",
            "graph_type": "N/A (text-based / distributional)",
            "representation_properties": "Compact dense vectors (300 dims), capture distributional relatedness; observed tendency to group related words (e.g., thematic relations) more strongly than strict similarity in the paper's qualitative analysis.",
            "evaluation_task": "WS353 and SL999 (same evaluation: cosine + Spearman correlation).",
            "performance_metrics": "Spearman correlation (Table 1): Skip-gram (text pretrained) — SL999: 0.442, WS353: 0.686.",
            "comparison_to_other_representations": "Skip-gram on text outperforms KB-only methods on WS353 in this study but underperforms on SL999; combining Skip-gram with KB-derived RWSGRAM and PPV yields improved results over any single method, indicating complementarity.",
            "limitations_or_challenges": "May conflate relatedness with similarity depending on window size and data; lacks explicit structured KB information; performance depends on corpus size and domain.",
            "uuid": "e5358.3"
        },
        {
            "name_short": "KB embedding (Wang et al.)",
            "name_full": "Knowledge graph and text jointly embedding",
            "brief_description": "Mentioned related work: methods that embed KB entities and typed relations into a shared low-dimensional vector space by fitting triple likelihoods, used for relation inference; different from the paper's approach which produces pseudo-text and trains NNLMs.",
            "citation_title": "Knowledge graph and text jointly embedding",
            "mention_or_use": "mention",
            "representation_name": "Triple-based KB embedding",
            "representation_description": "Represent each entity as a k-dimensional vector and model typed relations (e.g., born-in-city) as operations in that vector space; parameters are learned by maximizing triple likelihoods, enabling inference of missing relations. This approach learns embeddings directly from KB triples rather than converting graphs into text.",
            "graph_type": "Knowledge graph (entities and typed relations, e.g., Freebase / WordNet)",
            "representation_properties": "Learns compact entity and relation vectors; focuses on modeling typed relations for inference; generally not producing pseudo-text for NNLM training.",
            "evaluation_task": "Relation prediction / KB completion (mentioned in related work, not evaluated in this paper).",
            "performance_metrics": null,
            "comparison_to_other_representations": "Mentioned as contrasting approach: rather than producing pseudo-sentences and training NNLMs, these methods directly fit embeddings to triples. The paper positions its method as complementary — explicitly modeling contexts for words via random walks versus fitting triple likelihoods.",
            "limitations_or_challenges": "Not discussed in depth in this paper (only referenced).",
            "uuid": "e5358.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Monte carlo methods in pagerank computation: When one iteration is sufficient",
            "rating": 2
        },
        {
            "paper_title": "Random walks for knowledge-based word sense disambiguation",
            "rating": 2
        },
        {
            "paper_title": "Exploring Knowledge Bases for Similarity",
            "rating": 2
        },
        {
            "paper_title": "Efficient estimation of word representations in vector space",
            "rating": 2
        },
        {
            "paper_title": "Knowledge graph and text jointly embedding",
            "rating": 2
        }
    ],
    "cost": 0.0110295,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Random Walks and Neural Network Language Models on Knowledge Bases</h1>
<p>Josu Goikoetxea, Aitor Soroa and Eneko Agirre<br>IXA NLP Group<br>University of the Basque Country<br>Donostia, Basque Country<br>{josu.goikoetxea,a.soroa,e.agirre}@ehu.eus</p>
<h4>Abstract</h4>
<p>Random walks over large knowledge bases like WordNet have been successfully used in word similarity, relatedness and disambiguation tasks. Unfortunately, those algorithms are relatively slow for large repositories, with significant memory footprints. In this paper we present a novel algorithm which encodes the structure of a knowledge base in a continuous vector space, combining random walks and neural net language models in order to produce novel word representations. Evaluation in word relatedness and similarity datasets yields equal or better results than those of a random walk algorithm, using a dense representation ( 300 dimensions instead of 117 K ). Furthermore, the word representations are complementary to those of the random walk algorithm and to corpus-based continuous representations, improving the state-of-the-art in the similarity dataset. Our technique opens up exciting opportunities to combine distributional and knowledge-based word representations.</p>
<h2>1 Introduction</h2>
<p>Graph-based techniques over Knowledge Bases (KB) like WordNet (Fellbaum, 1998) have been widely used in NLP tasks, including word sense disambiguation (Agirre et al., 2014; Moro et al., 2014), semantic similarity and semantic relatedness between terms (Agirre et al., 2009; Agirre et al., 2010; Pilehvar et al., 2013). For instance, Agirre et al. $(2009 ; 2010)$ apply a random walk algorithm based on Personalized PageRank to WordNet, pre-
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Main architecture for generating KB word embeddings. A random walk algorithm over the KB produces a synthetic corpus, which is fed into a NNLM to produce continuous word representations.
senting the best results to date among WordNetbased methods for the well-known WS353 wordsimilarity dataset (Finkelstein et al., 2001). For each target word, the method performs a personalized random walk on the WordNet graph. At convergence, the target word is represented as a vector in a multi-dimensional conceptual space, with one dimension for each concept in the KB. The good results of the algorithm contrast with the large dimensionality of the vectors that it needs to produce, 117 K dimensions (one per synset) for WordNet.</p>
<p>In recent years a wide variety of Neural Network Language Models (NNLM) have been successfully employed in several tasks, including word similarity (Collobert and Weston, 2008; Socher et al., 2011;</p>
<p>Turian et al., 2010). NNLM extract meaning from unlabeled corpora following the distributional hypothesis (Harris, 1954), where semantic features of a word are related to its co-occurrence patterns. NNLM learn word representations in the form of dense scalar vectors in n-dimensional spaces (e.g. 300 dimensions), in which each dimension is a latent semantic feature. The representations are obtained by optimizing the likelihood of existing unlabeled text. More recently, Mikolov et al. have developed simpler NNLM architectures (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), which drastically reduced computational complexity by deleting the hidden layer, enabling to compute accurate word representations from very large corpora. The representations obtained by these methods are compact, taking 1.5 G for 3 M words on 300-dimensional space, and have been shown to outperform other distributional corpus-based methods on several tasks, including the WS353 word similarity dataset (Baroni et al., 2014).</p>
<p>In this work we propose to encode the meaning of words using the structural information in knowledge bases. That is, instead of modeling the meaning based on the co-occurrences of words in corpora, we model the meaning based on random walks over the knowledge base. Each random walk is seen as a context for words in the vocabulary, and fed into the NNLM architecture, which optimizes the likelihood of those contexts (cf. Fig. 1). The resulting word representations are more compact than those produced by regular random walk algorithms ( 300 vs. tens of thousands), and produce very good results on two well-known benchmarks on word relatedness and similarity: WS353 (Finkelstein et al., 2001) and SL999 (Hill et al., 2014b), respectively. We also show that the obtained representations are complementary to those of random walks alone and to distributional representations obtained by the same NNLM algorithm, improving the results.</p>
<p>Some recent work has explored embedding KBs in low-dimensional continous vector spaces, representing each entity in a k-dimensional vector and characterizing typed relations between entities in the KB (e.g. born-in-city in Freebase or part-of in WordNet) as operations in the k-dimensional space (Wang et al., 2014). The model estimates the parameters which maximize the likelihood of the triples, which
can then be used to infer new typed relations which are missing in the KB. In contrast, we use the relations to explicitly model the context of words, in two complementary approaches to embed information in KBs into continuous spaces.</p>
<h2>2 NNLM</h2>
<p>Neural Network Language Models have become a useful tool in NLP on the last years, specially in semantics. We have used the two models proposed in (Mikolov et al., 2013c) due to their simplicity and effectiveness in word similarity and relatedness tasks (Baroni et al., 2014): Continuous Bag of Words (CBOW) and Skip-gram. The first one is quite similar to the feedforward Neural Network Language Model, but instead of a hidden layer it has a projection layer, and thus all the words are projected in the same position. Word order has thus no influence in the projection. The training criterion is as follows: knowing previous and subsequent words in context, the model maximizes the probability of the predicting the word in the middle. The Skip-gram model uses each current word as an input to a log-linear classifier with a continuous projection layer, and predicts the previous and subsequent words in a context window.</p>
<p>Although the Skip-gram model seems to be more accurate in most of the semantic tasks, we have used both variants in our experiments. We used a publicly available implementation ${ }^{1}$.</p>
<h2>3 Random Walks and NNLM</h2>
<p>Our method performs random walks over KB graphs to create synthetic contexts which are fed into the NNLM architecture, creating novel word representations. The algorithm used for creating the contexts is a Monte Carlo method for computing the PageRank algorithm (Avrachenkov et al., 2007).</p>
<p>We consider a KB as undirected graph $G=$ $(V, E)$, where $V$ is the set of concepts and $E$ represents links among concepts. We also need a dictionary, an association from words to KB concepts. We construct an inverse dictionary that maps graph vertices with the words than can be linked to it.</p>
<p>The inputs of the algorithm are: 1) the graph $G=$ $(V, E), 2)$ the inverse dictionary and 3) the damp-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Spearman results on relatedness (WS353) for different corpus sizes (in sentences).
ing factor $\alpha^{2}$. In our experiments we used WordNet 3.0 with gloss relations ${ }^{3}$, which has 117.522 nodes (synsets) and 525.356 edges (semantic relations). Regarding the dictionary, WordNet already contains links from words to concepts. The dictionary includes the probability of a concept being lexicalized by a specific word, as estimated by the WordNet team from their hand-annotated corpora. Both dictionary and graph are freely available ${ }^{4}$.</p>
<p>The method first chooses a vertex at random from the vertex set $V$, and performs a random walk starting from it. At each step, the random walk might terminate with probability $(1-\alpha)$ or choose a neighbor vertex at random with probability $\alpha$. Each time the random walk reaches a vertex, a word is emitted at random using the probabilities in the inverse dictionary. When the random walk terminates, the sequence of emitted words forms the pseudo sentence which is fed to the NNLM architecture, and the process starts again choosing a vertex at random until a maximum number of pseudo sentences have been generated.</p>
<p>Our method creates pseudo sentences like the following:</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Spearman results on similarity (SL999) for different corpus sizes (in sentences).
(1) amphora wine nebuchadnezzar bear retain long
(2) graphology writer write scribble scrawler heedlessly in haste jot note notebook</p>
<p>These examples give us clues of the kind of the implicit semantic information that is encoded in the generated pseudo-corpus. Example 1 starts with amphora following with wine (with which amphoras are usually filled with), nebuchadnezzar (a particular bottle size) and finishing with words that are related to wine storage, like bear,retain and long. Example 2 shows a similar phenomenom; it starts with graphology, follows with the closely related writer, then writer, finishing with names and adjectives of different variants of writing, such as scribble, scrawler, heedlessly, in haste and jot; finally, the context ends with note and notebook. Note that our method also produces multiword terms like in haste.</p>
<h2>4 Experiments</h2>
<p>We have trained two Neural Network models, CBOW and Skip-gram, with several iterations of random walks over WordNet. We trained both models with default parameters (Mikolov et al., 2013a): vector size 300, 3 iterations, 5 negative samples, and</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">SL999</th>
<th style="text-align: center;">WS353</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Skip-gram</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">$\mathbf{0 . 6 8 6}$</td>
</tr>
<tr>
<td style="text-align: left;">RWSGRAM</td>
<td style="text-align: center;">$\mathbf{0 . 5 2 0}$</td>
<td style="text-align: center;">0.683</td>
</tr>
<tr>
<td style="text-align: left;">RWCBOW</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.591</td>
</tr>
<tr>
<td style="text-align: left;">PPV</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.683</td>
</tr>
</tbody>
</table>
<p>Table 1: Spearman correlation results for our methods (RWSGRAM, RWCBOW) on WordNet random walks, compared to just random walks (PPV), and Skip-gram on text corpora.
window size 5. In order to check how many iterations of the random walk algorithm are needed to learn good word representations, we produced up to $70 \cdot 10^{6}$ contexts. The the damping factor $(\alpha)$ of the random walk algorithm was set to 0.85 , a usual value (Agirre et al., 2010). All parameters were thus set to default, and we only explored different corpus sizes.</p>
<p>The word representations were evaluated on WS353 (Finkelstein et al., 2001) and SL999 (Hill et al., 2014b), two datasets on word relatedness and word similarity, respectively. In order to compute the similarity of two words, it suffices to calculate the cosine between the respective word representations. The evaluation measure computes the rank correlation (Spearman) between the human judgments and the system values.</p>
<p>In order to contrast our results with the two related techniques, we used UKB ${ }^{5}$, a publicly available implementation of Personalized PageRank (Agirre et al., 2014), and ran it over the same graph as our proposed methods. We used it out-of-the-box with a damping value of 0.85 . We also downloaded the embeddings learnt by (Mikolov et al., 2013a) using Skip-gram over a large text corpus ${ }^{6}$. We used the same cosine algorithm to compute similarity with all word representations. To distinguish one word representation from the other, we will call our models RWCBOW and RWSGRAM respectively (RW for random-walk), in contrast to the original Personalized PageRank algorithm (PPV) and the corpusbased embeddings learned using Skip-grams (Skipgram).</p>
<p>Figures 2 and 3 show the learning curves on the WS353 and SL999 datasets relative to the number</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Combinations and best published results: SL999 (Hill et al., 2014a), WS353 (Radinsky et al., 2011).
of contexts produced by the random walks on WordNet. The results show that WordNet representations grow quickly (around 7 million contexts), converging around 70 M , obtaining practically the same results as PPV for WS353, and better results for SL999 ${ }^{7}$.</p>
<p>The results at convergence are shown in Table 1, together with those of PPV and Skip-gram. Regarding SL999, we can see that the best results are obtained with RWSGRAM, improving over PPV and Skip-gram. Regarding WS353, all methods except RWSGRAM obtain similar results. The results show that our methods are able to effectively capture the information in WordNet, performing on par to the original PPV algorithm, and better than the corpusbased Skip-gram on the SL999 dataset. Note that the best published results for WS353 using WordNet are those of (Agirre et al., 2010) using PPV, which report 0.685 .</p>
<p>In order to see if the word representations that we learn are complementary to those of PPV and Skipgram, we combined the scores produced by each word representation. Given the potentially different scales of the similarity values, we assigned to each item the average of the ranks of the pair in each output. The top part of Table 2 repeats the three relevant systems. The ( $\mathrm{a}+\mathrm{b}$ ) row reports an improvement in both datasets, showing that RWSGRAM on WordNet is complementary to PPV in WordNet, and is thus a different representation, even if both use the same knowledge base. The $(\mathrm{a}+\mathrm{b})$ and $(\mathrm{a}+\mathrm{b}+\mathrm{c})$ show that corpus-based Skip-grams are also complemen-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>tary, yielding incremental improvements. In fact, the combination of all three improves over the best published results on SL999, and approaches the best results for WS353, as shown in the last row of the Table. The state of the art on SL999 corresponds to (Hill et al., 2014a), who training a Recurrent Neural Net model on bilingual text. The best results on WS353 correspond to (Radinsky et al., 2011), who combine a Wikipedia-based algorithm with a corpus-based method which uses date-related information from news to learn word representations.</p>
<p>Note that we have only performed some simple combination to show the complementarity of each information source. More sophisticated combinations (e.g. learning a regression model) could further improve results.</p>
<p>We have performed some qualitative analysis, which indicates that there is a slight tendency for corpus embeddings (with the window size used in the experiments) to group related words (e.g. physics - proton), and not so much similar words (e.g. vodka - gin), while our KB embeddings include both. This analysis agrees with the results in Table 1, where all KB results are better than corpusbased Skip-gram for the semantic similarity dataset (SL999). In passing, note that the best published results to date on similarity (Hill et al., 2014a) use embeddings learnt from bilingual text which suggests that bilingual corpora are better suited to learn embeddings capturing semantic similarity.</p>
<h2>5 Conclusions</h2>
<p>We have presented a novel algorithm which encodes the structure of a knowledge base in a continuous vector space, combining random walks and neural net language models to produce new word representations. Our evaluation in word relatedness and similarity datasets has shown that these new word representations attain similar results to those of the original random walk algorithm, using 300 dimensions instead of tens of thousands. Furthermore, the word representations are complementary to those of the random walk algorithm and to corpus-based continuous representations, producing better results when combined, and improving the state-of-the-art in the similarity dataset. Hand inspection reinforces the observation that WordNet-based</p>
<p>A promising direction of this research is to leverage multilingual Wordnets to produce cross-lingual embeddings.</p>
<p>On another direction, one of the main limitations of KB approaches is that they produce a relatively small number of embeddings, limited by the size of the dictionary. In the future we want to overcome this sparsity problem by combining both textual and KB based embeddings into a unified model. In fact, we think that our technique opens up exciting opportunities to combine distributional and knowledgebased word representations.</p>
<p>It would also be interesting to investigate the influence of the different semantic relations in WordNet, either by removing certain relations or by assigning different weights to them. This investigation could give us deeper insights about the way our knowledge-based approach codes meaning in vector spaces.</p>
<h2>Acknowledgements</h2>
<p>This work was partially funded by MINECO (CHIST-ERA READERS project - PCIN-2013-002-C02-01, and SKaTeR project - TIN2012-38584-C06-02), and the European Commission (QTLEAP - FP7-ICT-2013.4.1-610516). The IXA group is funded by the Basque Government (A type Research Group).</p>
<h2>References</h2>
<p>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Paşca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19-27. Association for Computational Linguistics.
Eneko Agirre, Montse Cuadros, German Rigau, and Aitor Soroa. 2010. Exploring Knowledge Bases for Similarity. In LREC.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2014. Random walks for knowledge-based word sense disambiguation. Computational Linguistics, 40(1):57-84.
K. Avrachenkov, N. Litvak, D. Nemirovsky, and N. Osipova. 2007. Monte carlo methods in pagerank computation: When one iteration is sufficient. SIAM J. Numer. Anal., 45(2):890-904.</p>
<p>Marco Baroni, Georgiana Dinu, and Germán Kruszewski. 2014. Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1.
Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160-167. ACM.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, pages 406-414. ACM.
Zellig S Harris. 1954. Distributional structure. Word.
Felix Hill, KyungHyun Cho, Sébastien Jean, Coline Devin, and Yoshua Bengio. 2014a. Not all neural embeddings are born equal. CoRR, abs/1410.0718.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014b. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. arXiv preprint arXiv:1408.3456.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111-3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proceedings of NAACL-HLT, pages 746-751.
Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity linking meets word sense disambiguation: a unied approach. Transactions of the Association of Computational Linguistics, 2:231-244, May.
Mohammad Taher Pilehvar, David Jurgens, and Roberto Navigli. 2013. Align, Disambiguate and Walk: a Unified Approach for Measuring Semantic Similarity. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 13411351, Sofia, Bulgaria.
Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. 2011. A word at a time: computing word relatedness using temporal semantic analysis. In Proceedings of the 20th international conference on World wide web, WWW '11, pages 337-346, New York, NY, USA. ACM.</p>
<p>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semisupervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151-161. Association for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384-394. Association for Computational Linguistics.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph and text jointly embedding. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1591-1601, Doha, Qatar, October. Association for Computational Linguistics.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ http://ixa2.si.ehu.eus
${ }^{6}$ https://code.google.com/p/word2vec/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ We tried larger context sizes, up to 700 M confirming that convergence was around 70 M .&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
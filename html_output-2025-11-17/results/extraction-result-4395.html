<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4395 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4395</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4395</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-225075639</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2010.14235v1.pdf" target="_blank">Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles</a></p>
                <p><strong>Paper Abstract:</strong> Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing the related-work section of a paper based on its abstract and the articles it references. Our work is inspired by extreme summarization, a dataset construction protocol that favours abstractive modeling approaches. Descriptive statistics and empirical results---using several state-of-the-art models trained on the Multi-XScience dataset---reveal that Multi-XScience is well suited for abstractive models.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4395.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4395.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HiMAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HiMAP (Multi-document abstractive model with MMR fusion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-document abstractive summarization approach that adapts a pointer-generator architecture and uses maximal marginal relevance (MMR) to compute weights over multiple input documents (fusion in vector space). Evaluated in this paper on Multi-XScience for generating related-work paragraphs from concatenated abstracts of reference papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HiMAP (as applied in Multi-XScience)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>HiMAP adapts a pointer-generator encoder-decoder model to the multi-document setting by computing MMR-based weights over multiple document inputs to perform a fusion operation in vector space; the weighted fused representation is fed to the generator which can attend to source tokens and (via the pointer mechanism) copy from inputs. In this paper HiMAP is applied to the Multi-XScience task by using the query abstract and the abstracts of reference papers as inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Pointer-generator architecture (neural seq2seq with attention and copy mechanism) adapted with MMR weighting (as in Fabbri et al., 2019).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>MMR-based weighted fusion of multi-document representations; encoder-decoder attention with copy (pointer) mechanism to extract salient content.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Vector-space fusion of multiple documents using MMR weights followed by abstractive generation (fusion-based summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Uses the Multi-XScience inputs: average 4.42 reference abstracts per query (dataset statistic); examples include inputs with ~10–20 references noted in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific articles from arXiv (general scientific domains represented in Multi-XScience).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Abstractive related-work paragraphs (multi-document summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-1, ROUGE-2, ROUGE-L; percentage of novel n-grams; human ranking (pairwise human judgments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On Multi-XScience test set HiMAP (multi-doc) achieved ROUGE-1 = 31.66, ROUGE-2 = 5.91, ROUGE-L = 28.43 (Table 6). Human average rank score for HiMAP was 2.28 (out of 3) in a 25-sample human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against LEAD baseline, extractive oracle (EXT-ORACLE), unsupervised extractive baselines (LexRank, TextRank), and other abstractive models (Pointer-Generator, BART, BertABS, SciBERTAbs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>HiMAP outperforms unsupervised extractive baselines (LexRank/TextRank) and the LEAD baseline on ROUGE; HiMAP's ROUGE-L (28.43) is lower than Pointer-Generator (30.63) but higher than many other abstractive baselines in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MMR-based fusion combined with a pointer-generator yields competitive abstractive summaries on scientific multi-document inputs; fusion helps leverage multiple reference abstracts rather than naive concatenation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on pointer-generator architecture and MMR weighting which may still be sensitive to domain-shift; requires careful weighting and supervised fine-tuning on domain-specific data (paper notes domain shift issues for self-pretrained models).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper does not provide a direct ablation vs. number of input papers for HiMAP; notes that Multi-XScience has variable number of references and that effective multi-document fusion is necessary when number of references increases; also notes transformer-based decoders require large supervised data to scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4395.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4395.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hier-Summ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hier-Summ (Hierarchical summarization with passage ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical transformer-based multi-document abstractive summarization approach that uses a passage ranker to select the most important document/passage as input to a hierarchical transformer generation model; applied here by concatenating selected passages from multiple abstracts to generate related-work paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hierarchical transformers for multi-document summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hier-Summ (as applied in Multi-XScience)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Hier-Summ first ranks passages/documents with a passage ranker to select the most important content across multiple inputs, then feeds selected passages into a hierarchical transformer encoder-decoder generation model to produce abstractive summaries. In this paper it is applied to the Multi-XScience dataset by selecting top passages from concatenated reference abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Hierarchical transformer encoder-decoder (as in Liu and Lapata, 2019a); model details are from the cited Hier-Summ work.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Passage/document ranking to select the most salient input passages from multiple reference abstracts (passage ranker).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Hierarchical transformer-based generation over selected passages (hierarchical summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates on the Multi-XScience inputs (average 4.42 reference abstracts; some examples with more references noted).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific articles (arXiv references in Multi-XScience).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Abstractive related-work paragraphs (multi-document summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-1, ROUGE-2, ROUGE-L; novel n-grams; human evaluation rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported as HIERSUMM (multi) in Table 6 with ROUGE-1 = 30.02, ROUGE-2 = 5.04, ROUGE-L = 27.60 on the Multi-XScience test set.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to LEAD, EXT-ORACLE, LexRank, TextRank, HiMAP, Pointer-Generator, BART, BertABS, SciBERTAbs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Hier-Summ (ROUGE-L 27.60) outperforms unsupervised extractive baselines but is below Pointer-Generator and HiMAP in ROUGE-L on this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Selecting salient passages before generation (hierarchical approach) is a viable strategy for multi-document scientific summarization; passage ranking reduces input length while preserving salient content.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Effectiveness depends on quality of passage ranking; may miss cross-document synthesis if important facts are distributed across many low-ranked passages; scalability to many references not fully evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper does not present a quantitative scaling curve vs. number of input documents; notes dataset contains variable number of references and that passage selection is necessary for computational tractability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4395.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4395.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pointer-Generator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pointer-Generator Network (See et al., 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural encoder-decoder summarization model with attention and a copy (pointer) mechanism that can copy words from the source; in this paper Pointer-Generator is applied to Multi-XScience by concatenating reference abstracts and fine-tuning the model to generate related-work paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Get to the point: Summarization with pointer-generator networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pointer-Generator (as applied in Multi-XScience)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The pointer-generator model is an attention-based seq2seq generator extended with a copy mechanism allowing direct copying from the input; in this paper the model is applied to concatenated multi-document abstracts to produce abstractive related-work paragraphs and is evaluated on ROUGE and human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Pointer-generator encoder-decoder (seq2seq neural model with attention and copy mechanism) as in See et al., 2017.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Concatenation of multiple reference abstracts into a single input sequence; attention and pointer copy mechanism enable extraction/copying of salient spans.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Abstractive generation combining copied tokens and generated tokens from the model's decoder (single-step fusion by concatenation).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to Multi-XScience inputs: average 4.42 references per query; dataset contains examples with more references (noted 10–20 in paper notes).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific articles (arXiv dataset Multi-XScience).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Abstractive related-work paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-1, ROUGE-2, ROUGE-L; novel n-grams; human ranking/quality scores.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Pointer-Generator achieved ROUGE-1 = 34.11, ROUGE-2 = 6.76, ROUGE-L = 30.63 on the Multi-XScience test set; human average rank score = 2.18 in the 25-sample human evaluation (compared to HiMAP 2.28 and ext-oracle 1.54).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against LEAD, EXT-ORACLE, LexRank, TextRank, HiMAP, Hier-Summ, BART, BertABS, SciBERTAbs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Pointer-Generator outperformed other tested abstractive pretrained models (BART, BertABS) and surpassed extractive baselines in ROUGE; highest ROUGE-L among models reported in Table 6 (30.63).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pointer-Generator produced highly abstractive summaries on this dataset and achieved the best ROUGE-L among tested systems; indicates that an explicit copy mechanism plus generation can be highly effective for multi-document scientific summarization when inputs are concatenated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Concatenation of many abstracts can create very long inputs which challenge encoder-decoder models; pointer-generator may still be limited by input length and by lack of explicit cross-document fusion mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper notes transformer-based models require large supervised in-domain data to scale; Pointer-Generator (RNN-based) performed well here without massive pretraining, but systematic scaling with number of papers or model size is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4395.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4395.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BART (Denoising sequence-to-sequence pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained sequence-to-sequence Transformer model trained with a denoising autoencoder objective (Lewis et al., 2019); in this paper BART is fine-tuned on Multi-XScience by concatenating reference abstracts to generate related-work paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BART (applied)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BART is a pretrained encoder-decoder Transformer model trained as a denoising autoencoder; in Multi-XScience it is applied by concatenating the query abstract and reference abstracts as input and fine-tuning the BART model to generate related-work paragraphs using beam search and trigram blocking during decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BART (pretrained seq2seq Transformer as described in Lewis et al., 2019); specific size/configuration used is the default from the original implementation (not further specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Concatenation of multiple reference abstracts into a single input sequence; encoder-decoder attention over concatenated text.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Seq2seq generation from the fused concatenated input; no additional explicit fusion beyond the encoder representation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to Multi-XScience with average 4.42 reference abstracts per query (dataset statistic); paper notes inputs can include up to ~10–20 references in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific articles (arXiv abstracts in Multi-XScience).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Abstractive related-work paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-1, ROUGE-2, ROUGE-L; novel n-grams percentage; human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>BART achieved ROUGE-1 = 32.83, ROUGE-2 = 6.36, ROUGE-L = 26.61 on Multi-XScience (Table 6). BART exhibited the lowest novel n-gram ratio among tested models (most extractive behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Pointer-Generator, HiMAP, Hier-Summ, BertABS, SciBERTAbs, LEAD, EXT-ORACLE, LexRank, TextRank.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BART underperformed Pointer-Generator in ROUGE-L and produced more extractive outputs (lower novel n-gram percentage); paper suggests domain shift of pretraining corpora (Wikipedia, BookCorpus) and large decoder parameter count as possible reasons.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Despite being a strong pretrained seq2seq model, BART was relatively extractive and underperformed some simpler architectures on Multi-XScience, likely due to domain shift and the need for more domain-specific supervised data for large transformer decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Domain shift from BART's pretraining corpora (Wikipedia/BookCorpus) to scientific abstracts; large number of decoder parameters requires massive supervised domain-specific training data; exhibited highly extractive behavior on this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper notes transformer decoders like BART require large supervised in-domain datasets to fully leverage their capacity; no explicit scaling curves vs. number of papers or model size presented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4395.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4395.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BertABS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BertABS (Pretrained BERT encoder with transformer decoder for summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An abstractive summarization model that uses a pretrained BERT encoder and trains a randomly initialized transformer decoder for generation (Liu and Lapata, 2019b); applied here by concatenating multi-document abstracts and fine-tuning for related-work generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text summarization with pretrained encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BertABS (as applied in Multi-XScience)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BertABS uses a pretrained BERT encoder to produce contextualized representations which are fed to a trained transformer decoder to generate summaries; in Multi-XScience the model is applied by concatenating the query and reference abstracts as input and fine-tuning the encoder-decoder for related-work paragraph generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BERT as encoder (pretrained, Devlin et al., 2019) combined with a randomly initialized transformer decoder (per Liu & Lapata, 2019b).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Concatenation of multiple abstracts; encoder representations from pretrained BERT used to identify salient content for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Seq2seq generation from concatenated encoder representations (concatenation-based multi-document synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to Multi-XScience inputs: average 4.42 references per query; variable per example.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific articles (arXiv abstracts).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Abstractive related-work paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-1, ROUGE-2, ROUGE-L; novel n-grams; human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>BertABS achieved ROUGE-1 = 31.56, ROUGE-2 = 5.02, ROUGE-L = 28.05 on the Multi-XScience test set (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to BART, Pointer-Generator, HiMAP, Hier-Summ, SciBERTAbs, LEAD, EXT-ORACLE, LexRank, TextRank.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BertABS performs competitively but below Pointer-Generator and SciBERTAbs in ROUGE-L on this dataset; SciBERTAbs (BertABS with SciBERT encoder) improved ROUGE-L indicating domain-specific encoder helps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a strong pretrained encoder (BERT) helps performance, but domain-specific pretraining (SciBERT) further improves results; decoder capacity and domain adaptation remain important.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Randomly initialized decoders and domain shift limit performance; concatenation of many abstracts can hit input length limits and reduce effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper reports that replacing the encoder with SciBERT improves performance, indicating gains from domain-specific pretraining; no systematic scaling curve by number of papers or model size provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4395.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4395.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciBERTAbs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciBERT-augmented BertABS (SciBERTAbs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of BertABS replacing the encoder with SciBERT (pretrained on scientific text) to better adapt to scientific-domain summarization; applied and evaluated on Multi-XScience showing improved ROUGE-L over non-domain encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scibert: Pretrained contextualized embeddings for scientific text.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciBERTAbs (BertABS with SciBERT encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SciBERTAbs uses SciBERT (a BERT variant pretrained on scientific corpora) as the encoder in the BertABS architecture and trains a transformer decoder to generate abstractive related-work paragraphs from concatenated query and reference abstracts. The adaptation aims to reduce domain shift and improve factual/lexical fit to scientific text.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>SciBERT (pretrained scientific BERT) as encoder + transformer decoder (per BertABS architecture).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Concatenation of multiple abstracts to form encoder input; SciBERT encoder provides contextualized scientific-domain representations to extract salient content.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Seq2seq generation (abstractive) from SciBERT encoder representations using a trained decoder (concatenation-based synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied on Multi-XScience inputs (average 4.42 references per query; some examples contain more references).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature (arXiv abstracts; scientific domains covered in Multi-XScience).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Abstractive related-work paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-1, ROUGE-2, ROUGE-L; novel n-grams; human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>SciBERTAbs achieved ROUGE-1 = 32.12, ROUGE-2 = 5.59, ROUGE-L = 29.01 on the Multi-XScience test set (Table 6), showing an improvement in ROUGE-L compared to BertABS and BART.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against BertABS (non-scientific encoder), BART, Pointer-Generator, HiMAP, Hier-Summ, extractive baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>SciBERTAbs outperformed BART and BertABS in ROUGE-L on this dataset, indicating benefit of domain-specific encoder pretraining for scientific summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain-specific pretraining (SciBERT) for the encoder yields measurable improvements in ROUGE-L and helps mitigate domain shift seen with general-domain pretrained models like BART.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Still constrained by decoder capacity and concatenation strategy; full-text references were approximated by abstracts which may limit available factual detail; no explicit mitigation of hallucination beyond encoder pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper reports improvement when using SciBERT encoder, suggesting gains with domain-specific pretraining; no quantitative scaling curves versus number of input papers or model size provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Get to the point: Summarization with pointer-generator networks. <em>(Rating: 2)</em></li>
                <li>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. <em>(Rating: 2)</em></li>
                <li>Text summarization with pretrained encoders. <em>(Rating: 2)</em></li>
                <li>Scibert: Pretrained contextualized embeddings for scientific text. <em>(Rating: 2)</em></li>
                <li>Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. <em>(Rating: 2)</em></li>
                <li>Hierarchical transformers for multi-document summarization. <em>(Rating: 2)</em></li>
                <li>Adapting the neural encoder-decoder framework from single to multi-document summarization. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4395",
    "paper_id": "paper-225075639",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "HiMAP",
            "name_full": "HiMAP (Multi-document abstractive model with MMR fusion)",
            "brief_description": "A multi-document abstractive summarization approach that adapts a pointer-generator architecture and uses maximal marginal relevance (MMR) to compute weights over multiple input documents (fusion in vector space). Evaluated in this paper on Multi-XScience for generating related-work paragraphs from concatenated abstracts of reference papers.",
            "citation_title": "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model.",
            "mention_or_use": "use",
            "system_name": "HiMAP (as applied in Multi-XScience)",
            "system_description": "HiMAP adapts a pointer-generator encoder-decoder model to the multi-document setting by computing MMR-based weights over multiple document inputs to perform a fusion operation in vector space; the weighted fused representation is fed to the generator which can attend to source tokens and (via the pointer mechanism) copy from inputs. In this paper HiMAP is applied to the Multi-XScience task by using the query abstract and the abstracts of reference papers as inputs.",
            "llm_model_used": "Pointer-generator architecture (neural seq2seq with attention and copy mechanism) adapted with MMR weighting (as in Fabbri et al., 2019).",
            "extraction_technique": "MMR-based weighted fusion of multi-document representations; encoder-decoder attention with copy (pointer) mechanism to extract salient content.",
            "synthesis_technique": "Vector-space fusion of multiple documents using MMR weights followed by abstractive generation (fusion-based summarization).",
            "number_of_papers": "Uses the Multi-XScience inputs: average 4.42 reference abstracts per query (dataset statistic); examples include inputs with ~10–20 references noted in the paper.",
            "domain_or_topic": "Scientific articles from arXiv (general scientific domains represented in Multi-XScience).",
            "output_type": "Abstractive related-work paragraphs (multi-document summaries).",
            "evaluation_metrics": "ROUGE-1, ROUGE-2, ROUGE-L; percentage of novel n-grams; human ranking (pairwise human judgments).",
            "performance_results": "On Multi-XScience test set HiMAP (multi-doc) achieved ROUGE-1 = 31.66, ROUGE-2 = 5.91, ROUGE-L = 28.43 (Table 6). Human average rank score for HiMAP was 2.28 (out of 3) in a 25-sample human evaluation.",
            "comparison_baseline": "Compared against LEAD baseline, extractive oracle (EXT-ORACLE), unsupervised extractive baselines (LexRank, TextRank), and other abstractive models (Pointer-Generator, BART, BertABS, SciBERTAbs).",
            "performance_vs_baseline": "HiMAP outperforms unsupervised extractive baselines (LexRank/TextRank) and the LEAD baseline on ROUGE; HiMAP's ROUGE-L (28.43) is lower than Pointer-Generator (30.63) but higher than many other abstractive baselines in this paper.",
            "key_findings": "MMR-based fusion combined with a pointer-generator yields competitive abstractive summaries on scientific multi-document inputs; fusion helps leverage multiple reference abstracts rather than naive concatenation.",
            "limitations_challenges": "Relies on pointer-generator architecture and MMR weighting which may still be sensitive to domain-shift; requires careful weighting and supervised fine-tuning on domain-specific data (paper notes domain shift issues for self-pretrained models).",
            "scaling_behavior": "Paper does not provide a direct ablation vs. number of input papers for HiMAP; notes that Multi-XScience has variable number of references and that effective multi-document fusion is necessary when number of references increases; also notes transformer-based decoders require large supervised data to scale.",
            "uuid": "e4395.0",
            "source_info": {
                "paper_title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Hier-Summ",
            "name_full": "Hier-Summ (Hierarchical summarization with passage ranking)",
            "brief_description": "A hierarchical transformer-based multi-document abstractive summarization approach that uses a passage ranker to select the most important document/passage as input to a hierarchical transformer generation model; applied here by concatenating selected passages from multiple abstracts to generate related-work paragraphs.",
            "citation_title": "Hierarchical transformers for multi-document summarization.",
            "mention_or_use": "use",
            "system_name": "Hier-Summ (as applied in Multi-XScience)",
            "system_description": "Hier-Summ first ranks passages/documents with a passage ranker to select the most important content across multiple inputs, then feeds selected passages into a hierarchical transformer encoder-decoder generation model to produce abstractive summaries. In this paper it is applied to the Multi-XScience dataset by selecting top passages from concatenated reference abstracts.",
            "llm_model_used": "Hierarchical transformer encoder-decoder (as in Liu and Lapata, 2019a); model details are from the cited Hier-Summ work.",
            "extraction_technique": "Passage/document ranking to select the most salient input passages from multiple reference abstracts (passage ranker).",
            "synthesis_technique": "Hierarchical transformer-based generation over selected passages (hierarchical summarization).",
            "number_of_papers": "Operates on the Multi-XScience inputs (average 4.42 reference abstracts; some examples with more references noted).",
            "domain_or_topic": "Scientific articles (arXiv references in Multi-XScience).",
            "output_type": "Abstractive related-work paragraphs (multi-document summaries).",
            "evaluation_metrics": "ROUGE-1, ROUGE-2, ROUGE-L; novel n-grams; human evaluation rankings.",
            "performance_results": "Reported as HIERSUMM (multi) in Table 6 with ROUGE-1 = 30.02, ROUGE-2 = 5.04, ROUGE-L = 27.60 on the Multi-XScience test set.",
            "comparison_baseline": "Compared to LEAD, EXT-ORACLE, LexRank, TextRank, HiMAP, Pointer-Generator, BART, BertABS, SciBERTAbs.",
            "performance_vs_baseline": "Hier-Summ (ROUGE-L 27.60) outperforms unsupervised extractive baselines but is below Pointer-Generator and HiMAP in ROUGE-L on this dataset.",
            "key_findings": "Selecting salient passages before generation (hierarchical approach) is a viable strategy for multi-document scientific summarization; passage ranking reduces input length while preserving salient content.",
            "limitations_challenges": "Effectiveness depends on quality of passage ranking; may miss cross-document synthesis if important facts are distributed across many low-ranked passages; scalability to many references not fully evaluated in this paper.",
            "scaling_behavior": "Paper does not present a quantitative scaling curve vs. number of input documents; notes dataset contains variable number of references and that passage selection is necessary for computational tractability.",
            "uuid": "e4395.1",
            "source_info": {
                "paper_title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Pointer-Generator",
            "name_full": "Pointer-Generator Network (See et al., 2017)",
            "brief_description": "A neural encoder-decoder summarization model with attention and a copy (pointer) mechanism that can copy words from the source; in this paper Pointer-Generator is applied to Multi-XScience by concatenating reference abstracts and fine-tuning the model to generate related-work paragraphs.",
            "citation_title": "Get to the point: Summarization with pointer-generator networks.",
            "mention_or_use": "use",
            "system_name": "Pointer-Generator (as applied in Multi-XScience)",
            "system_description": "The pointer-generator model is an attention-based seq2seq generator extended with a copy mechanism allowing direct copying from the input; in this paper the model is applied to concatenated multi-document abstracts to produce abstractive related-work paragraphs and is evaluated on ROUGE and human judgments.",
            "llm_model_used": "Pointer-generator encoder-decoder (seq2seq neural model with attention and copy mechanism) as in See et al., 2017.",
            "extraction_technique": "Concatenation of multiple reference abstracts into a single input sequence; attention and pointer copy mechanism enable extraction/copying of salient spans.",
            "synthesis_technique": "Abstractive generation combining copied tokens and generated tokens from the model's decoder (single-step fusion by concatenation).",
            "number_of_papers": "Applied to Multi-XScience inputs: average 4.42 references per query; dataset contains examples with more references (noted 10–20 in paper notes).",
            "domain_or_topic": "Scientific articles (arXiv dataset Multi-XScience).",
            "output_type": "Abstractive related-work paragraphs.",
            "evaluation_metrics": "ROUGE-1, ROUGE-2, ROUGE-L; novel n-grams; human ranking/quality scores.",
            "performance_results": "Pointer-Generator achieved ROUGE-1 = 34.11, ROUGE-2 = 6.76, ROUGE-L = 30.63 on the Multi-XScience test set; human average rank score = 2.18 in the 25-sample human evaluation (compared to HiMAP 2.28 and ext-oracle 1.54).",
            "comparison_baseline": "Compared against LEAD, EXT-ORACLE, LexRank, TextRank, HiMAP, Hier-Summ, BART, BertABS, SciBERTAbs.",
            "performance_vs_baseline": "Pointer-Generator outperformed other tested abstractive pretrained models (BART, BertABS) and surpassed extractive baselines in ROUGE; highest ROUGE-L among models reported in Table 6 (30.63).",
            "key_findings": "Pointer-Generator produced highly abstractive summaries on this dataset and achieved the best ROUGE-L among tested systems; indicates that an explicit copy mechanism plus generation can be highly effective for multi-document scientific summarization when inputs are concatenated.",
            "limitations_challenges": "Concatenation of many abstracts can create very long inputs which challenge encoder-decoder models; pointer-generator may still be limited by input length and by lack of explicit cross-document fusion mechanisms.",
            "scaling_behavior": "Paper notes transformer-based models require large supervised in-domain data to scale; Pointer-Generator (RNN-based) performed well here without massive pretraining, but systematic scaling with number of papers or model size is not provided.",
            "uuid": "e4395.2",
            "source_info": {
                "paper_title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "BART",
            "name_full": "BART (Denoising sequence-to-sequence pretraining)",
            "brief_description": "A pretrained sequence-to-sequence Transformer model trained with a denoising autoencoder objective (Lewis et al., 2019); in this paper BART is fine-tuned on Multi-XScience by concatenating reference abstracts to generate related-work paragraphs.",
            "citation_title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.",
            "mention_or_use": "use",
            "system_name": "BART (applied)",
            "system_description": "BART is a pretrained encoder-decoder Transformer model trained as a denoising autoencoder; in Multi-XScience it is applied by concatenating the query abstract and reference abstracts as input and fine-tuning the BART model to generate related-work paragraphs using beam search and trigram blocking during decoding.",
            "llm_model_used": "BART (pretrained seq2seq Transformer as described in Lewis et al., 2019); specific size/configuration used is the default from the original implementation (not further specified in this paper).",
            "extraction_technique": "Concatenation of multiple reference abstracts into a single input sequence; encoder-decoder attention over concatenated text.",
            "synthesis_technique": "Seq2seq generation from the fused concatenated input; no additional explicit fusion beyond the encoder representation.",
            "number_of_papers": "Applied to Multi-XScience with average 4.42 reference abstracts per query (dataset statistic); paper notes inputs can include up to ~10–20 references in some cases.",
            "domain_or_topic": "Scientific articles (arXiv abstracts in Multi-XScience).",
            "output_type": "Abstractive related-work paragraphs.",
            "evaluation_metrics": "ROUGE-1, ROUGE-2, ROUGE-L; novel n-grams percentage; human evaluation.",
            "performance_results": "BART achieved ROUGE-1 = 32.83, ROUGE-2 = 6.36, ROUGE-L = 26.61 on Multi-XScience (Table 6). BART exhibited the lowest novel n-gram ratio among tested models (most extractive behavior).",
            "comparison_baseline": "Compared to Pointer-Generator, HiMAP, Hier-Summ, BertABS, SciBERTAbs, LEAD, EXT-ORACLE, LexRank, TextRank.",
            "performance_vs_baseline": "BART underperformed Pointer-Generator in ROUGE-L and produced more extractive outputs (lower novel n-gram percentage); paper suggests domain shift of pretraining corpora (Wikipedia, BookCorpus) and large decoder parameter count as possible reasons.",
            "key_findings": "Despite being a strong pretrained seq2seq model, BART was relatively extractive and underperformed some simpler architectures on Multi-XScience, likely due to domain shift and the need for more domain-specific supervised data for large transformer decoders.",
            "limitations_challenges": "Domain shift from BART's pretraining corpora (Wikipedia/BookCorpus) to scientific abstracts; large number of decoder parameters requires massive supervised domain-specific training data; exhibited highly extractive behavior on this dataset.",
            "scaling_behavior": "Paper notes transformer decoders like BART require large supervised in-domain datasets to fully leverage their capacity; no explicit scaling curves vs. number of papers or model size presented.",
            "uuid": "e4395.3",
            "source_info": {
                "paper_title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "BertABS",
            "name_full": "BertABS (Pretrained BERT encoder with transformer decoder for summarization)",
            "brief_description": "An abstractive summarization model that uses a pretrained BERT encoder and trains a randomly initialized transformer decoder for generation (Liu and Lapata, 2019b); applied here by concatenating multi-document abstracts and fine-tuning for related-work generation.",
            "citation_title": "Text summarization with pretrained encoders.",
            "mention_or_use": "use",
            "system_name": "BertABS (as applied in Multi-XScience)",
            "system_description": "BertABS uses a pretrained BERT encoder to produce contextualized representations which are fed to a trained transformer decoder to generate summaries; in Multi-XScience the model is applied by concatenating the query and reference abstracts as input and fine-tuning the encoder-decoder for related-work paragraph generation.",
            "llm_model_used": "BERT as encoder (pretrained, Devlin et al., 2019) combined with a randomly initialized transformer decoder (per Liu & Lapata, 2019b).",
            "extraction_technique": "Concatenation of multiple abstracts; encoder representations from pretrained BERT used to identify salient content for decoding.",
            "synthesis_technique": "Seq2seq generation from concatenated encoder representations (concatenation-based multi-document synthesis).",
            "number_of_papers": "Applied to Multi-XScience inputs: average 4.42 references per query; variable per example.",
            "domain_or_topic": "Scientific articles (arXiv abstracts).",
            "output_type": "Abstractive related-work paragraphs.",
            "evaluation_metrics": "ROUGE-1, ROUGE-2, ROUGE-L; novel n-grams; human evaluation.",
            "performance_results": "BertABS achieved ROUGE-1 = 31.56, ROUGE-2 = 5.02, ROUGE-L = 28.05 on the Multi-XScience test set (Table 6).",
            "comparison_baseline": "Compared to BART, Pointer-Generator, HiMAP, Hier-Summ, SciBERTAbs, LEAD, EXT-ORACLE, LexRank, TextRank.",
            "performance_vs_baseline": "BertABS performs competitively but below Pointer-Generator and SciBERTAbs in ROUGE-L on this dataset; SciBERTAbs (BertABS with SciBERT encoder) improved ROUGE-L indicating domain-specific encoder helps.",
            "key_findings": "Using a strong pretrained encoder (BERT) helps performance, but domain-specific pretraining (SciBERT) further improves results; decoder capacity and domain adaptation remain important.",
            "limitations_challenges": "Randomly initialized decoders and domain shift limit performance; concatenation of many abstracts can hit input length limits and reduce effectiveness.",
            "scaling_behavior": "Paper reports that replacing the encoder with SciBERT improves performance, indicating gains from domain-specific pretraining; no systematic scaling curve by number of papers or model size provided.",
            "uuid": "e4395.4",
            "source_info": {
                "paper_title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "SciBERTAbs",
            "name_full": "SciBERT-augmented BertABS (SciBERTAbs)",
            "brief_description": "A variant of BertABS replacing the encoder with SciBERT (pretrained on scientific text) to better adapt to scientific-domain summarization; applied and evaluated on Multi-XScience showing improved ROUGE-L over non-domain encoders.",
            "citation_title": "Scibert: Pretrained contextualized embeddings for scientific text.",
            "mention_or_use": "use",
            "system_name": "SciBERTAbs (BertABS with SciBERT encoder)",
            "system_description": "SciBERTAbs uses SciBERT (a BERT variant pretrained on scientific corpora) as the encoder in the BertABS architecture and trains a transformer decoder to generate abstractive related-work paragraphs from concatenated query and reference abstracts. The adaptation aims to reduce domain shift and improve factual/lexical fit to scientific text.",
            "llm_model_used": "SciBERT (pretrained scientific BERT) as encoder + transformer decoder (per BertABS architecture).",
            "extraction_technique": "Concatenation of multiple abstracts to form encoder input; SciBERT encoder provides contextualized scientific-domain representations to extract salient content.",
            "synthesis_technique": "Seq2seq generation (abstractive) from SciBERT encoder representations using a trained decoder (concatenation-based synthesis).",
            "number_of_papers": "Applied on Multi-XScience inputs (average 4.42 references per query; some examples contain more references).",
            "domain_or_topic": "Scientific literature (arXiv abstracts; scientific domains covered in Multi-XScience).",
            "output_type": "Abstractive related-work paragraphs.",
            "evaluation_metrics": "ROUGE-1, ROUGE-2, ROUGE-L; novel n-grams; human evaluation.",
            "performance_results": "SciBERTAbs achieved ROUGE-1 = 32.12, ROUGE-2 = 5.59, ROUGE-L = 29.01 on the Multi-XScience test set (Table 6), showing an improvement in ROUGE-L compared to BertABS and BART.",
            "comparison_baseline": "Compared against BertABS (non-scientific encoder), BART, Pointer-Generator, HiMAP, Hier-Summ, extractive baselines.",
            "performance_vs_baseline": "SciBERTAbs outperformed BART and BertABS in ROUGE-L on this dataset, indicating benefit of domain-specific encoder pretraining for scientific summarization.",
            "key_findings": "Domain-specific pretraining (SciBERT) for the encoder yields measurable improvements in ROUGE-L and helps mitigate domain shift seen with general-domain pretrained models like BART.",
            "limitations_challenges": "Still constrained by decoder capacity and concatenation strategy; full-text references were approximated by abstracts which may limit available factual detail; no explicit mitigation of hallucination beyond encoder pretraining.",
            "scaling_behavior": "Paper reports improvement when using SciBERT encoder, suggesting gains with domain-specific pretraining; no quantitative scaling curves versus number of input papers or model size provided.",
            "uuid": "e4395.5",
            "source_info": {
                "paper_title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Get to the point: Summarization with pointer-generator networks.",
            "rating": 2,
            "sanitized_title": "get_to_the_point_summarization_with_pointergenerator_networks"
        },
        {
            "paper_title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.",
            "rating": 2,
            "sanitized_title": "denoising_sequencetosequence_pretraining_for_natural_language_generation_translation_and_comprehension"
        },
        {
            "paper_title": "Text summarization with pretrained encoders.",
            "rating": 2,
            "sanitized_title": "text_summarization_with_pretrained_encoders"
        },
        {
            "paper_title": "Scibert: Pretrained contextualized embeddings for scientific text.",
            "rating": 2,
            "sanitized_title": "scibert_pretrained_contextualized_embeddings_for_scientific_text"
        },
        {
            "paper_title": "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model.",
            "rating": 2,
            "sanitized_title": "multinews_a_largescale_multidocument_summarization_dataset_and_abstractive_hierarchical_model"
        },
        {
            "paper_title": "Hierarchical transformers for multi-document summarization.",
            "rating": 2,
            "sanitized_title": "hierarchical_transformers_for_multidocument_summarization"
        },
        {
            "paper_title": "Adapting the neural encoder-decoder framework from single to multi-document summarization.",
            "rating": 1,
            "sanitized_title": "adapting_the_neural_encoderdecoder_framework_from_single_to_multidocument_summarization"
        }
    ],
    "cost": 0.0165705,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles
27 Oct 2020</p>
<p>Yao Lu lu.yao@ucl.ac.uk 
Mila University of Waterloo</p>
<p>Yue Dong yue.dong2@mail.mcgill.ca 
McGill University
Mila</p>
<p>Laurent Charlin lcharlin@gmail.com 
CIFAR AI Chair
Mila, MontréalHEC, Canada</p>
<p>Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles
27 Oct 202084BD0D90A1957C4D55C0FBD69BA58318arXiv:2010.14235v1[cs.CL]
Multi-document summarization is a challenging task for which there exists little largescale datasets.We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles.Multi-XScience introduces a challenging multidocument summarization task: writing the related-work section of a paper based on its abstract and the articles it references.Our work is inspired by extreme summarization, a dataset construction protocol that favours abstractive modeling approaches.Descriptive statistics and empirical results-using several state-of-the-art models trained on the Multi-XScience dataset-reveal that Multi-XScience is well suited for abstractive models. 1</p>
<p>Introduction</p>
<p>Single document summarization is the focus of most current summarization research thanks to the availability of large-scale single-document summarization datasets spanning multiple fields, including news (CNN/DailyMail (Hermann et al., 2015), NYT (Sandhaus, 2008), Newsroom (Grusky et al., 2018), XSum (Narayan et al., 2018a)), law (BigPatent (Sharma et al., 2019)), and even science (ArXiv and PubMed (Cohan et al., 2018)).These large-scale datasets are a necessity for modern data-hungry neural architectures (e.g.Transformers (Vaswani et al., 2017)) to shine at the summarization task.The versatility of available data has proven helpful in studying different types of summarization strategies as well as both extractive and abstractive models (Narayan et al., 2018a).</p>
<p>In contrast, research on the task of multidocument summarization (MDS) -a more general scenario with many downstream applications 1 Our dataset is available at https://github.com/yaolu/Multi-XScienceSource 1 (Abstract of query paper) ... we present an approach based on ... lexical databases and ... Our approach makes use of WordNet synonymy information to .... Incidentally, WordNet based approach performance is comparable with the training approach one.Source 2 (cite1 abstract) This paper presents a method for the resolution of lexical ambiguity of nouns ... The method relies on the use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts ... Source 3 (cite2 abstract) Word groupings useful for language processing tasks are increasingly available ... This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns ... Disambiguation is performed with respect to WordNet senses ... Source 4 (cite3 abstract) In ... word sense disambiguation... integrates a diverse set of knowledge sources ... including part of speech of neighboring words, morphological form ... Summary (Related work of query paper) Lexical databases have been employed recently in word sense disambiguation.For example, ... [cite1] make use of a semantic distance that takes into account structural factors in WordNet ... Additionally, [cite2] combines the use of WordNet and a text collection for a definition of a distance for disambiguating noun groupings.... [cite3] make use of several sources of information ... (neighborhood, part of speech, morfological form, etc.) ... -has not progressed as much in part due to the lack of large-scale datasets.There are only two available large-scale multi-document summarization datasets: Multi-News (Fabbri et al., 2019) and WikiSum (Liu et al., 2018).While large supervised neural network models already dominate the leadboard associated with these datasets, obtaining better models requires domain-specific, highquality, and large-scale datasets, especially ones for abstractive summarization methods.</p>
<p>We propose Multi-XScience, a large-scale dataset for multi-document summarization using scientific articles.We introduce a challenging multi-document summarization task: write the related work section of a paper using its abstract (source 1 in Tab. 1) and reference papers (additional sources).</p>
<p>Multi-XScience is inspired by the XSum dataset and can be seen as a multi-document version of extreme summarization (Narayan et al., 2018b).Similar to XSum, the "extremeness" makes our dataset more amenable to abstractive summarization strategies.Moreover, Table 4 shows that Multi-XScience contains fewer positional and extractive biases than previous MDS datasets.High positional and extractive biases can undesirably enable models to achieve high summarization scores by copying sentences from certain (fixed) positions, e.g.lead sentences in news summarization (Grenander et al., 2019;Narayan et al., 2018a).Empirical results show that our dataset is challenging and requires models having high-level of text abstractiveness.</p>
<p>Multi-XScience Dataset</p>
<p>We now describe the Multi-XScience dataset, including the data sources, data cleaning, and the processing procedures used to construct it.We also report descriptive statistics and an initial analysis which shows it is amenable to abstractive models.</p>
<p>Data Source</p>
<p>Our dataset is created by combining information from two sources: arXiv.organd the Microsoft Academic Graph (MAG) (Sinha et al., 2015).We first obtain all arXiv papers, and then construct pairs of target summary and multi-reference documents using MAG.2</p>
<p>Dataset Creation</p>
<p>We construct the dataset with care to maximize its usefulness.The construction protocol includes: 1) cleaning the latex source of 1.3 millions arXiv papers, 2) aligning all of these papers and their references in MAG using numerous heuristics, 3) five cleaning iterations of the resulting data records interleaved with rounds of human verification.</p>
<p>Our dataset uses a query document's abstract Q a and the abstracts of articles it references R a 1 , . . ., R a n , where n is the number of reference articles cited by Q in its related-work section.</p>
<p>The target is the query document's related-work section segmented into paragraphs Q rw 1 , . . .Q rw k , where k is the number of paragraphs in the relatedwork section of Q.We discuss these choices below.Table 1 contains an example from our dataset.</p>
<p>Target summary: Q rw i is a paragraph in the related-work section of Q.We only keep articles with an explicit related-work section as query documents.We made the choice of using paragraphs as targets rather than the whole related-work section for the following two reasons: 1) using the whole related work as targets make the dataset difficult to work on, because current techniques struggle with extremely long input and generation targets;3 and 2) paragraphs in the related-work section often refer to (very) different research threads that can be divided into independent topics.Segmenting paragraphs creates a dataset with reasonable input/target length suitable for most existing models and common computational resources.</p>
<p>Source: the source in our dataset is a tuple (Q a , R a 1 , . . ., R a n ).We only use the abstract of the query because the introduction section, for example, often overlaps with the related-work section.Using the introduction would then be closer to single-document-summarization.By only using the query abstract Q a the dataset forces models to focus on leveraging the references.Furthermore, we approximate the reference documents using their abstract, as the full text of reference papers is often not available due to copyright restrictions.4In Table 2 we report the descriptive statistics of current large-scale multi-document summarization (MDS) datasets, including Multi-XScience.Compared to Multi-News, Multi-XScience has 60% more references, making it a better fit for the MDS settings.Despite our dataset being smaller than WikiSum, it is better suited to abstractive summarization as its reference summaries contain more novel n-grams when compared to the source (Table 3).A dataset with a higher novel n-grams score has less extractive bias which should result in better abstraction for summarization models (Narayan et al., 2018a).Multi-XScience has one of the highest novel n-grams scores among existing large-scale datasets.This is expected since writing related works requires condensing complicated ideas into short summary paragraphs.The high level of abstractiveness makes our dataset challenging since models cannot simply copy sentences from the reference articles.Table 4 reports the performance of the lead baseline5 and the extractive oracle6 for several summarization datasets.High ROUGE scores on the lead baseline indicate datasets with strong lead bias, which is typical of news summarization (Grenander et al., 2019).The extractive oracle performance indicates the level of "extractiveness" of each dataset.Highly-extractive datasets force abstractive models to copy input sentences to obtain a high summarization performance.Compared to the existing summarization datasets, Multi-XScience imposes much less position bias and requires a higher level of abstractiveness from models.Both results consolidate that Multi-XScience requires summarization models to "understand" source text (models cannot obtain a high score by learning positional cues) and is suitable for abstractive models (models cannot obtain a high score by copying sentences).</p>
<p>Dataset Statistics and Analysis</p>
<p>Human Evaluation on Dataset Quality</p>
<p>Two human judges evaluated the overlap between the sources and the target on 25 pairs randomly selected from the test set.7They scored each pair using the scale shown in Table 5.The average human-evaluated quality score of Multi-XScience is 2.82±0.4(95% C.I.).There is a large overlap between the reference abstracts and the targets' related work based on this score8 which highlights that the major facts are covered despite using only the abstract.</p>
<p>Experiments &amp; Results</p>
<p>We study the performance of multiple state-of-theart models using the Multi-XScience dataset.Detailed analyses of the generation quality are also provided, including quantitative and qualitative analysis in addition to the abstractiveness study.</p>
<p>Models</p>
<p>In addition to the lead baseline and extractive oracle, we also include two commonly used unsupervised extractive summarization models, LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004), as baselines.</p>
<p>For supervised abstractive models, we test state-of-the-art multi-document summarization models HiMAP (Fabbri et al., 2019) and Both deal with multi-documents using a fusion mechanism, which performs the transformation of the documents in the vector space.</p>
<p>HiMAP adapts a pointer-generator model (See et al., 2017) with maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998;Lebanoff et al., 2018) to compute weights over multi-document inputs.</p>
<p>Hier-Summ (Liu and Lapata, 2019a) uses a passage ranker that selects the most important document as the input to the hierarchical transformer-based generation model.</p>
<p>In addition, we apply existing state-ofthe-art single-document summarization models, including Pointer-Generator (See et al., 2017), BART (Lewis et al., 2019) and BertABS (Liu and Lapata, 2019b), for the task of multidocument summarization by simply concatenating the input references.Pointer-Generator incorporates attention over source texts as a copy mechanism to aid the generation.BART is a sequence-to-sequence model with an encoder that is pre-trained with the denosing autoencoder objective.BertABS uses a pretrained BERT (Devlin et al., 2019) as the encoder and trains a randomly initialized transformer decoder for abstractive summarization.We also report the performance of BertABS with an encoder (SciBert) pretrained on scientific articles (Beltagy et al., 2019).</p>
<p>Implementation Details</p>
<p>All the models used in our paper are based on open-source code released by their authors.For all models, we use the default configuration (model size, optimizer learning rate, etc.) from the original implementation.During the decoding process, we use beam search (beam size=4) and tri-gram blocking as is standard for sequence-to-sequence models.We set the minimal generation length to 110 tokens given the dataset statistics.Similar to the CNN/Dailymail dataset, we adopt the anonymized setting of citation symbols for the evaluation.In our dataset, the target related work contains citation reference to specific papers with special symbols (e.g.cite 2).We replace all of these symbols by a standard symbol (e.g.cite) for evaluation.</p>
<p>Result Analysis</p>
<p>Automatic Evaluation We report ROUGE Scores9 and percentage of novel n-grams for different models on the Multi-XScience dataset in Tables 6 and 7. When comparing abstractive models to extractive ones, we first observe that almost all abstractive models outperform the unsupervised extractive models-TextRank and LexRank-by wide margins.In addition, almost all the abstractive models significantly outperform the extractive oracle in terms of R-L.This further shows the suitability of Multi-XScience for abstractive summarization.</p>
<p>To our surprise, Pointer-Generator outperforms self-pretrained abstractive summarization models, such as BART and BertABS.Our analyses (Table 7) reveal that this model performs highly abstractive summaries on our dataset, indicating that the model chooses to generate rather than copy.BART is highly extractive with the lowest novel n-gram among all approaches.This result may be due to the domain shift of the self pre-training datasets (Wikipedia and BookCorpus) since the performance of SciBertAbs is much higher in terms of ROUGE-L.In addition, the large number of parameters in the transformer-based decoders require massive supervised domain-specific training data.Human Evaluation We conduct human evaluation on ext-oracle, HiMAP, and Pointer-Generator, since each outperforms others in their respective section of Table 6.For evaluation, we randomly select 25 samples and present the system outputs in randomized order to the human judges.Two human judges are asked to rank system outputs from 1 (worst) to 3 (best).Higher rank score means better generation quality.The average score is 1.54, 2.28 and 2.18 for ext-oracle, HiMAP, and Pointer-Generator, respectively.According to the feedback of human evaluators, the overall writing style of abstractive models are much better than extractive models, which provides further evidence of the abstractive nature of Multi-XScience.</p>
<p>In addition, we show some generation examples in Table 8.Since the extractive oracle is copied from the source text, the writing style fails to resemble the related work despite capturing the correct content.In contrast, all generation models can adhere to the related-work writing style and their summaries also the correct content.</p>
<p>Related Work</p>
<p>Scientific document summarization is a challenging task.Multiple models trained on small datasets exist for this task (Hu and Wan, 2014;Jaidka et al., 2013;Hoang and Kan, 2010), as there are no available large-scale datasets (before this paper).Attempts at creating scientific summarization datasets have been emerging, but not to the scale required for training neural-based models.For example, CL-Scisumm (Jaidka et al., 2016) created datasets from the ACL Anthology with 30-50 articles; Yasunaga et al. and  AbuRa'ed et al. 10  proposed human-annotated datasets with at most 1,000 article and summary pairs.We believe that the lack of largescale datasets slowed down development of multi-10 This is concurrent work.</p>
<p>Groundtruth Related Work a study by @cite attempt to address the uncertainty estimation in the domain of crowd counting.this study proposed a scalable neural network framework with quantification of decomposed uncertainty using a bootstrap ensemble ... the proposed uncertainty quantification method provides additional auxiliary insight to the crowd counting model ... Generated Related Work (Oracle) in this work, we focus on uncertainty estimation in the domain of crowd counting.we propose a scalable neural network framework with quantification of decomposed uncertainty using a bootstrap ensemble.we demonstrate that the proposed uncertainty quantification method provides additional insight to the crowd counting problem ... Generated Related Work (HiMAP) in @cite, the authors propose a scalable neural network model based on gaussian filter and brute-force nearest neighbor search algorithm.the uncertainty of the uncertainty is used as a density map for the crowd counting problem. the authors of @cite proposed to use the uncertainty quantification to improve the uncertainty ... Generated Related Work (Pointer-Generator) our work is also related to the work of @cite, where the authors propose a scalable neural network framework for crowd counting.they propose a method for uncertainty estimation in the context of crowd counting, which can be seen as a generalization of the uncertainty ... document summarization methods, and we hope that our proposed dataset will change that.</p>
<p>Extensions of Multi-XScience</p>
<p>We focus on summarization from the text of multiple documents, but our dataset could also be used for other tasks including:</p>
<p>• Graph-based summarization:</p>
<p>Since our dataset is aligned with MAG, we could use its graph information (e.g., the citation graph) in addition to the plain text as input.</p>
<p>• Unsupervised in-domain corpus: Scientificdocument understanding may benefit from using using related work (in addition to other sources such as non-directly related reference manuals).It is worth exploring how to use unsupervised in-domain corpus (e.g., all papers from N-hop subgraph of MAG) for better performance on downstream tasks.</p>
<p>Conclusion</p>
<p>The lack of large-scale dataset has slowed the progress of multi-document summarization (MDS) research.We introduce Multi-XScience, a large-scale dataset for MDS using scientific articles.Multi-XScience is better suited to abstractive summarization than previous MDS datasets, since it requires summarization models to exhibit high text understanding and abstraction capabilities.Experimental results show that our dataset is amenable to abstractive summarization models and is challenging for current models.</p>
<p>Table 1 :
1
An example from our Multi-XScience dataset showing the input documents and the related work of the target paper.Text is colored based on semantic similarity between sources and related work.</p>
<p>Table 2 :
2
Comparison of large-scale multi-document summa-
Dataset# train/val/testdoc. len summ. len # refsMulti-XScience 30,369/5,066/5,093778.08116.444.42Multi-News44,972/5,622/5,622 2,103.49263.662.79WikiSum1, 5m/38k/38k36,802.5139.4525rization datasets. We propose Multi-XScience. Average doc-ument length ("doc. len") is calculated by concatenating allinput sources (multiple reference documents).</p>
<p>Table 3 :
3
The proportion of novel n-grams in the target reference summaries across different summarization datasets.The first and second block compare single-document and multidocument summarization datasets, respectively.
DatasetsR-1LEAD R-2R-LEXT-ORACLE R-1 R-2 R-LCNN-DailyMail 39.58 17.67 36.18 54.67 30.35 50.80NY Times31.85 15.86 23.75 52.08 31.59 46.72XSum16.30 1.61 11.95 29.79 8.81 22.65WikiSum38.22 16.85 26.89 44.40 22.59 41.28Multi-News43.08 14.27 38.97 49.06 21.54 44.27Multi-XScience 27.46 4.57 18.82 38.45 9.93 27.11</p>
<p>Table 4 :
4
ROUGE scores for the LEAD and EXT-ORACLE baselines for different summarization datasets.</p>
<p>Table 5 :
5
Dataset quality evaluation criteria</p>
<p>Table 6 :
6
ROUGE results on Multi-XScience test set.
ModelsROUGE-1 ROUGE-2 ROUGE-LMulti-doc ExtractiveLEAD27.464.5718.82LEXRANK30.195.5326.19TEXTRANK31.515.8326.58EXT-ORACLE38.459.9327.11Multi-doc Abstractive (Fusion)HIERSUMM(MULTI)30.025.0427.60HIMAP(MULTI)31.665.9128.43Multi-doc Abstractive (Concat)BERTABS31.565.0228.05BART32.836.3626.61SCIBERTABS32.125.5929.01POINTER-GENERATOR34.116.7630.63</p>
<p>Table 8 :
8
Generation example of extractive oracle (EXT-ORACLE), HiMAP and Pointer-Generator (PG).</p>
<p>Our dataset is processed based on the October 2019 dump of MAG and arXiv.
10-20 references as input, 2-4 paragraphs as output
Since our dataset relies on MAG for the reference paper as input, some reference papers are not available on arXiv. Our dataset contains all available paper information, including paper ids and corresponding MAG entry.
The lead baseline selects the first-K sentences from the source document as summary.
The EXT-oracle summarizes by greedily selecting the sentences that maximize the ROUGE-L F1 scores as described inNallapati et al. (2017).
We invited two PhD students who have extensive research experiences to conduct the dataset quality assessment on our scientific related-work summarization dataset.
8 This is expected, as it is standard to discuss the key contribution(s) of a paper in its abstract.
The scores are computed with ROUGE-1.5.5 script with option "-c 95 -r
-n 2 -a -m"
AcknowledgmentsThis work is supported by the Canadian Institute For Advanced Research (CIFAR) through its AI chair program and an IVADO fundamental research grant.We thank Daniel Tarlow for the original idea that lead to this work and Compute Canada for providing the computational resources.
A multi-level annotated corpus of scientific papers for scientific document summarization and cross-document relation discovery. Ahmed Abura'ed, Horacio Saggion, Luis Chiruzzo, </p>
<p>Iz Beltagy, Arman Cohan, Kyle Lo, arXiv:1903.10676Scibert: Pretrained contextualized embeddings for scientific text. 2019arXiv preprint</p>
<p>The use of mmr, diversity-based reranking for reordering documents and producing summaries. Jaime Carbonell, Jade Goldstein, Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval. the 21st annual international ACM SIGIR conference on Research and development in information retrieval1998</p>
<p>A discourse-aware attention model for abstractive summarization of long documents. Arman Cohan, Franck Dernoncourt, Soon Doo, Trung Kim, Seokhwan Bui, Walter Kim, Nazli Chang, Goharian, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Short Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20182</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>Lexrank: Graph-based lexical centrality as salience in text summarization. Günes Erkan, Dragomir R Radev, Journal of artificial intelligence research. 222004</p>
<p>Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model. Alexander Richard Fabbri, Irene Li, Tianwei She, Suyi Li, Dragomir Radev, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Countering the effects of lead bias in news summarization via multi-stage training and auxiliary losses. Matt Grenander, Yue Dong, Jackie Chi, Kit Cheung, Annie Louis, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. Max Grusky, Mor Naaman, Yoav Artzi, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLong Papers20181</p>
<p>Teaching machines to read and comprehend. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Advances in neural information processing systems. 2015</p>
<p>Towards automated related work summarization. Cong Duy, Vu Hoang, Min-Yen Kan, Proceedings of the 23rd International Conference on Computational Linguistics: Posters. the 23rd International Conference on Computational Linguistics: Posters2010</p>
<p>Automatic generation of related work sections in scientific papers: an optimization approach. Yue Hu, Xiaojun Wan, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)2014</p>
<p>Overview of the cl-scisumm 2016 shared task. Kokil Jaidka, Muthu Kumar Chandrasekaran, Sajal Rustagi, Min-Yen Kan, Proceedings of the joint workshop on bibliometric-enhanced information retrieval and natural language processing for digital libraries (BIRNDL). the joint workshop on bibliometric-enhanced information retrieval and natural language processing for digital libraries (BIRNDL)2016</p>
<p>Deconstructing human literature reviews-a framework for multi-document summarization. Kokil Jaidka, Christopher Khoo, Jin-Cheon Na, proceedings of the 14th European workshop on natural language generation. the 14th European workshop on natural language generation2013</p>
<p>Adapting the neural encoder-decoder framework from single to multi-document summarization. Logan Lebanoff, Kaiqiang Song, Fei Liu, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.13461Marjan Ghazvininejad,. 2019BartarXiv preprint</p>
<p>Generating wikipedia by summarizing long sequences. J Peter, Mohammad Liu, Etienne Saleh, Ben Pot, Ryan Goodrich, Lukasz Sepassi, Noam Kaiser, Shazeer, International Conference on Learning Representations. 2018</p>
<p>Hierarchical transformers for multi-document summarization. Yang Liu, Mirella Lapata, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019a</p>
<p>Text summarization with pretrained encoders. Yang Liu, Mirella Lapata, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019b</p>
<p>Textrank: Bringing order into text. Rada Mihalcea, Paul Tarau, Proceedings of the 2004 conference on empirical methods in natural language processing. the 2004 conference on empirical methods in natural language processing2004</p>
<p>Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. Ramesh Nallapati, Feifei Zhai, Bowen Zhou, Thirty-First AAAI Conference on Artificial Intelligence. 2017</p>
<p>Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. Shashi Narayan, Shay B Cohen, Mirella Lapata, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018a</p>
<p>Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. Shashi Narayan, Shay B Cohen, Mirella Lapata, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018b</p>
<p>Evan Sandhaus, The new york times annotated corpus. Linguistic Data Consortium. Philadelphia20086e26752</p>
<p>Get to the point: Summarization with pointergenerator networks. Abigail See, Peter J Liu, Christopher D Manning, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational Linguistics2017</p>
<p>Bigpatent: A large-scale dataset for abstractive and coherent summarization. Eva Sharma, Chen Li, Lu Wang, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>An overview of microsoft academic service (mas) and applications. Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, Kuansan Wang, Proceedings of the 24th international conference on world wide web. the 24th international conference on world wide web2015</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 2017</p>
<p>Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R Fabbri, Irene Li, Dan Friedman, Dragomir R Radev, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-418 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-418</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-418</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-227344703</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2012.03225v1.pdf" target="_blank">NaturalCC: A Toolkit to Naturalize the Source Code Corpus</a></p>
                <p><strong>Paper Abstract:</strong> We present NaturalCC, an efficient and extensible toolkit to bridge the gap between natural language and programming language, and facilitate the research on big code analysis. Using NaturalCC, researchers both from natural language or programming language communities can quickly and easily reproduce the state-of-the-art baselines and implement their approach. NaturalCC is built upon Fairseq and PyTorch, providing (1) an efficient computation with multi-GPU and mixed-precision data processing for fast model training, (2) a modular and extensible framework that makes it easy to reproduce or implement an approach for big code analysis, and (3) a command line interface and a graphical user interface to demonstrate each model's performance. Currently, we have included several state-of-the-art baselines across different tasks (e.g., code completion, code comment generation, and code retrieval) for demonstration. The video of this demo is available at https://www.youtube.com/watch?v=q4W5VSI-u3E&t=25s.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e418.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e418.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>reproducibility_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reproducibility and Faithfulness Gap between Natural-Language Experimental Descriptions and Code Implementations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A broadly described class of discrepancies where natural-language descriptions (papers, docs, or specs) omit or differ from concrete code implementations, especially in datasets, preprocessing, hyperparameters, and training details, causing failures or variability in reproducing reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NATURALCC experimental / model training pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An open-source toolkit built on Fairseq and PyTorch for training and evaluating models on software-engineering tasks (code completion, comment generation, code retrieval) with multi-GPU and mixed-precision support, dataset preprocessing, modular model registry, CLI/GUI and baseline implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper descriptions / experimental protocol (methods section and reported baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>released source code and experiment scripts (authors' implementations of baselines and dataset preprocessing pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter and dataset/preprocessing mismatch; incomplete specification of training procedure; implicit implementation details omitted from natural-language descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The paper states that many state-of-the-art DL approaches for code are sensitive to datasets and hyperparameters and suffer replicability issues; specific gaps include differences in dataset cleaning/tokenization (e.g., BPE vs whitespace), omitted preprocessing steps (AST/control/data-flow extraction), unreported hyperparameters (learning rates, batch size, FP16/FP32 settings, gradient accumulation), multi-GPU synchronization details, and training/validation split handling — all of which can make code implementations differ from the natural-language description in the paper and lead to divergent results.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing; model training procedure (hyperparameters, mixed-precision, multi-GPU); model architecture variants and implementation-level modules (tokenizers, feature extractors); evaluation metrics and dataset splits</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual reproducibility checking and comparison against released source code and baseline implementations (the authors 'carefully checked and evaluated' referred baselines by comparing against released code), i.e., manual code review and attempted reproduction</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Not quantitatively measured in the paper as a gap size; toolkit authors used standard task metrics (MRR for code completion/retrieval, BLEU/Rouge for comment generation) to validate implementations, but did not report explicit numeric deltas quantifying the mismatch between natural-language claims and code behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Qualitative impact reported: reduced replicability and reproducibility of state-of-the-art results due to sensitivity to datasets and hyperparameters. The paper does not provide numeric effect sizes; it claims that absent unified tooling and unrevealed implementation details lead to inability to reproduce reported performance reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as common: the paper states 'many state-of-the-art approaches have been suffering from the replicability and reproducibility issues' but provides no frequency statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous or incomplete natural-language descriptions in papers (omitted preprocessing and hyperparameter details), variability in dataset versions/cleaning, lack of standardized experiment pipelines and tooling, and implicit implementation assumptions (multi-GPU/FP16 settings, optimizer/seed defaults).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide a unified, open-source toolkit (NATURALCC) that: (1) supplies cleaned and preprocessed public datasets (CodeSearchNet, Py150, Python), (2) implements and publishes baseline models and training scripts, (3) uses a modular registry for tasks/models to reduce hidden differences, (4) documents and supports multi-GPU and mixed-precision training, and (5) offers CLI/GUI to run and inspect models — thereby reducing ambiguous textual descriptions and aligning code with documented procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantitatively evaluated in the paper; authors claim the toolkit 'can facilitate' reproduction and that baselines were 'carefully checked and evaluated' against released code, but no numerical reproducibility improvement (e.g., success rate or performance deltas) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Machine learning / deep learning for source code (software engineering + NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NaturalCC: A Toolkit to Naturalize the Source Code Corpus', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Codesearchnet challenge: Evaluating the state of semantic code search <em>(Rating: 2)</em></li>
                <li>fairseq: A fast, extensible toolkit for sequence modeling <em>(Rating: 1)</em></li>
                <li>Codebert: A pre-trained model for programming and natural languages <em>(Rating: 1)</em></li>
                <li>Improving automatic source code summarization via deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-418",
    "paper_id": "paper-227344703",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "reproducibility_gap",
            "name_full": "Reproducibility and Faithfulness Gap between Natural-Language Experimental Descriptions and Code Implementations",
            "brief_description": "A broadly described class of discrepancies where natural-language descriptions (papers, docs, or specs) omit or differ from concrete code implementations, especially in datasets, preprocessing, hyperparameters, and training details, causing failures or variability in reproducing reported results.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "NATURALCC experimental / model training pipeline",
            "system_description": "An open-source toolkit built on Fairseq and PyTorch for training and evaluating models on software-engineering tasks (code completion, comment generation, code retrieval) with multi-GPU and mixed-precision support, dataset preprocessing, modular model registry, CLI/GUI and baseline implementations.",
            "nl_description_type": "research paper descriptions / experimental protocol (methods section and reported baselines)",
            "code_implementation_type": "released source code and experiment scripts (authors' implementations of baselines and dataset preprocessing pipelines)",
            "gap_type": "hyperparameter and dataset/preprocessing mismatch; incomplete specification of training procedure; implicit implementation details omitted from natural-language descriptions",
            "gap_description": "The paper states that many state-of-the-art DL approaches for code are sensitive to datasets and hyperparameters and suffer replicability issues; specific gaps include differences in dataset cleaning/tokenization (e.g., BPE vs whitespace), omitted preprocessing steps (AST/control/data-flow extraction), unreported hyperparameters (learning rates, batch size, FP16/FP32 settings, gradient accumulation), multi-GPU synchronization details, and training/validation split handling — all of which can make code implementations differ from the natural-language description in the paper and lead to divergent results.",
            "gap_location": "data preprocessing; model training procedure (hyperparameters, mixed-precision, multi-GPU); model architecture variants and implementation-level modules (tokenizers, feature extractors); evaluation metrics and dataset splits",
            "detection_method": "manual reproducibility checking and comparison against released source code and baseline implementations (the authors 'carefully checked and evaluated' referred baselines by comparing against released code), i.e., manual code review and attempted reproduction",
            "measurement_method": "Not quantitatively measured in the paper as a gap size; toolkit authors used standard task metrics (MRR for code completion/retrieval, BLEU/Rouge for comment generation) to validate implementations, but did not report explicit numeric deltas quantifying the mismatch between natural-language claims and code behavior.",
            "impact_on_results": "Qualitative impact reported: reduced replicability and reproducibility of state-of-the-art results due to sensitivity to datasets and hyperparameters. The paper does not provide numeric effect sizes; it claims that absent unified tooling and unrevealed implementation details lead to inability to reproduce reported performance reliably.",
            "frequency_or_prevalence": "Described as common: the paper states 'many state-of-the-art approaches have been suffering from the replicability and reproducibility issues' but provides no frequency statistics.",
            "root_cause": "Ambiguous or incomplete natural-language descriptions in papers (omitted preprocessing and hyperparameter details), variability in dataset versions/cleaning, lack of standardized experiment pipelines and tooling, and implicit implementation assumptions (multi-GPU/FP16 settings, optimizer/seed defaults).",
            "mitigation_approach": "Provide a unified, open-source toolkit (NATURALCC) that: (1) supplies cleaned and preprocessed public datasets (CodeSearchNet, Py150, Python), (2) implements and publishes baseline models and training scripts, (3) uses a modular registry for tasks/models to reduce hidden differences, (4) documents and supports multi-GPU and mixed-precision training, and (5) offers CLI/GUI to run and inspect models — thereby reducing ambiguous textual descriptions and aligning code with documented procedures.",
            "mitigation_effectiveness": "Not quantitatively evaluated in the paper; authors claim the toolkit 'can facilitate' reproduction and that baselines were 'carefully checked and evaluated' against released code, but no numerical reproducibility improvement (e.g., success rate or performance deltas) is reported.",
            "domain_or_field": "Machine learning / deep learning for source code (software engineering + NLP)",
            "reproducibility_impact": true,
            "uuid": "e418.0",
            "source_info": {
                "paper_title": "NaturalCC: A Toolkit to Naturalize the Source Code Corpus",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Codesearchnet challenge: Evaluating the state of semantic code search",
            "rating": 2,
            "sanitized_title": "codesearchnet_challenge_evaluating_the_state_of_semantic_code_search"
        },
        {
            "paper_title": "fairseq: A fast, extensible toolkit for sequence modeling",
            "rating": 1,
            "sanitized_title": "fairseq_a_fast_extensible_toolkit_for_sequence_modeling"
        },
        {
            "paper_title": "Codebert: A pre-trained model for programming and natural languages",
            "rating": 1,
            "sanitized_title": "codebert_a_pretrained_model_for_programming_and_natural_languages"
        },
        {
            "paper_title": "Improving automatic source code summarization via deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "improving_automatic_source_code_summarization_via_deep_reinforcement_learning"
        }
    ],
    "cost": 0.0061825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>NATURALCC: A Toolkit to Naturalize the Source Code Corpus</p>
<p>Yao Wan wanyao@hust.edu.cn 
School of Computer Science and Technology
Services Computing Technology and System Lab, Cluster and Grid Computing Lab
Huazhong University of Science and Technology
WuhanChina</p>
<p>Yang He 
School of Computer Science
University of Technology Sydney
NSWAustralia</p>
<p>Jian-Guo Zhang 
Department of Computer Science
University of Illinois at Chicago
IllinoisUSA</p>
<p>Yulei Sui yulei.sui@uts.edu.au 
School of Computer Science
University of Technology Sydney
NSWAustralia</p>
<p>Hai Jin hjin@hust.edu.cn 
School of Computer Science and Technology
Services Computing Technology and System Lab, Cluster and Grid Computing Lab
Huazhong University of Science and Technology
WuhanChina</p>
<p>Guandong Xu guandong.xu@uts.edu.au 
School of Computer Science
University of Technology Sydney
NSWAustralia</p>
<p>Caiming Xiong cxiong@salesforce.com 
Salesforce Research
Palo AltoUSA</p>
<p>Philip S Yu psyu@uic.edu 
Department of Computer Science
University of Illinois at Chicago
IllinoisUSA</p>
<p>NATURALCC: A Toolkit to Naturalize the Source Code Corpus
255C47B5B9CAD092855CE808147A1226Natural language processingprogramming language analysisbig codetoolkit
We present NATURALCC, an efficient and extensible toolkit to bridge the gap between natural language and programming language, and facilitate the research on big code analysis.Using NATURALCC, researchers both from natural language or programming language communities can quickly and easily reproduce the state-of-the-art baselines and implement their approach.NATURALCC is built upon Fairseq and PyTorch, providing (1) an efficient computation with multi-GPU and mixedprecision data processing for fast model training, (2) a modular and extensible framework that makes it easy to reproduce or implement an approach for big code analysis, and (3) a command line interface and a graphical user interface to demonstrate each model's performance.Currently, we have included several stateof-the-art baselines across different tasks (e.g., code completion, code comment generation, and code retrieval) for demonstration.The video of this demo is available at https://www.youtube.com/watch?v=q4W5VSI-u3E&amp;t=25s.</p>
<p>I. INTRODUCTION</p>
<p>The rapid growth of machine learning (ML), especially of deep learning (DL) based natural language processing (NLP), brings great opportunities to explore and exploit NLP techniques for various tasks of software engineering (SE), e.g., code documentation [1], [2], code completion [3] and code retrieval [4], [5].The underlying insights for learningbased code analysis is the naturalness hypothesis shared among natural languages and programming languages.Despite the flourishing study, many state-of-the-art approaches have been suffering from the replicability and reproducibility issues.This is due to the fact that the performance of deep learning approaches is sensitive to datasets and hyperparameters, and currently, no unified open-source toolkit is available to our communities.</p>
<p>To fill this gap, this paper introduces NATURALCC, 1 a comprehensive platform to facilitate research NLP-based big code analysis.Both of the researchers from SE community or NLP community can benefit from the toolkit for fast 1 The term NaturalCC also represents natural code comprehension, which is a fundamental task lies in the synergy between the programming language and NLP.training and reproduction.NATURALCC features the following advantages:</p>
<p>• Efficient Data Preprocessing.We have cleaned and preprocessed three public datasets (i.e., CodeSearchNet [6], Py150 [7], and Python [1]) for different tasks.All the data loaders and processes in our model training can be parallelized in multiple GPUs.Besides, our toolkit also supports the mixed-precision for numerical calculation.• Extensibility and Modularity.Based on the registry mechanism implemented in Fairseq [8], our framework is well modularized and can be easily extended to various tasks.In particular, when implementing a new task, we only need to implement the task and models in the corresponding folders and then register them.• Flexible Interface.We provide flexible APIs for developers to easily invoke the trained models for other applications.Additionally, we demonstrate NATURALCC with a command line interface as well as a graphical user interface, using three application tasks, i.e., code completion, code comment generation, and code retrieval.</p>
<p>NATURALCC is an ongoing open-source toolkit maintained by the CodeMind team.We hope NATURALCC can facilitate the research of software engineering with natural language processing.We also encourage researchers to integrate their state-of-the-art approach into NATURALCC, to promote the research in both communities.</p>
<p>All the source code and materials are publicly available via GitHub: http://github.com/CGCL-codes/naturalcc. 2We also build a website for our team and will post the updates in http://xcodemind.github.io.</p>
<p>II. ARCHITECTURE DESIGN</p>
<p>Figure 1 shows a pipeline of our NATURALCC.Given a dataset of code snippets, we first preprocess the data in the data preprocessing stage and then feed each mini-batch of samples into the code representation module, a fundamental component for several downstream tasks.In the code representation module, we have implemented many state-of-the-art encoders (e.g., RNN [9], GNN [10], Transformer [11] and BERT [12]).</p>
<p>Based on the code representation, NATURALCC can also support various downstream tasks, e.g., code documentation, programming language modeling, code retrieval and type inference.The designed Trainer controls model training for each task.</p>
<p>A. Data Preprocessing</p>
<p>In the data preprocessing stage, we first tokenize the source code by a tokenizer (e.g., space tokenizer or BPE [13] tokenizer) and then build a vocabulary for these tokens.In addition, we can also extract some domain-specific features (e.g., AST [1], control-flow graphs [5], or data-flow graphs [14]).The goal of this process is to build a series of mini-batches for training.We put all the data-related processes in the data and dataset folders.</p>
<p>B. Code Representation</p>
<p>Code representation/understanding, which aims to learn an embedding vector, is one of the most critical components for big code analysis.In NATURALCC, we have included most stateof-the-art neural network encoders to represent the source code and their extracted features.For example, we have implemented RNN-based models to represent the sequential tokens or (linearized) AST of code.We implement graph neural networks (GNNs) such as gated graph neural networks (GGNNs) to represent the graph structure features of code (e.g., controlflow and data-flow graphs).We have also included the advanced Transformer networks [11], which serve as the replacement of the RNN network, with its fast computation and ability to handle long-range dependent sequence.In addition, the NATURALCC also supports the masked pre-trained models, e.g., BERT and RoBERTa [15].We put all the code representation networks in the models and modules folders.</p>
<p>C. Applications</p>
<p>Our NATURALCC supports many different downstream tasks.We have currently implemented three tasks, i.e., code completion, code comment generation, and code retrieval, to validate the effectiveness of the proposed framework.All the referred baselines on each task have been carefully checked and evaluated when compared against the released source code from the original papers.The implementations and the tasks in this toolkit will serve as baselines for fair comparison for future research use.We organize all the tasks in the tasks folder.</p>
<p>1) Code Completion: Code completion, which predicts the next code element based on the previously written code, has become an essential tool in many IDEs.It can boost developers' programming productivity.In this task, we have implemented the referred model SeqRNN [16] and TravTrans [3].We train the models in the Py150 dataset and evaluate them using the MRR metrics.</p>
<p>2) Code Comment Generation: Generating comments for code snippets is an effective way for program understanding and facilitate the software development and maintenance.In this task, we have implemented the referred model NeuralTransformer [17].We trained the model in Python and Java datasets and evaluated them using the BLEU and Rouge metrics.</p>
<p>3) Code Retrieval: Searching semantically similar code snippets given a natural language query can provide developers a series of templates as reference for rapid prototyping.In this task, we have implemented four benchmark baselines (i.e., NBOW, 1D-CNN, biRNN and SelfAttn) and evaluated them by using the MRR metrics on CodeSearchNet dataset [6].</p>
<p>Additional tasks with state-of-the-art models are still under development.They will be released soon, including code clone detection [18], type inference [19], vulnerability detection [20], and masked language modeling for code pre-training [21].</p>
<p>D. Trainer</p>
<p>We have designed a trainer (ncc_trainer.py)module to control the whole training process of models.Furthermore, we have designed and implemented a simpler trainer in a universal way (ncc_trainer_simple.py) for those beginners who are not familiar with our framework.</p>
<p>III. IMPLEMENTATION</p>
<p>We have implemented NATURALCC based on the Fairseq and PyTorch.Following the outstanding registry mechanism designed in Fairseq, NATURALCC has good extensibility with the modularized design.We have implemented a register decorator in the entry of building a task, model or module (cf.<strong>init</strong>.py in each folder).Listing 1 shows the workflow of registering a new task.In brief, the registry mechanism is to design a global variable to store each task of model objects for off-the-shelf fetching.This registry mechanism can provide us the ability of extension, as we only need to include this decorator when defining a new task/model/module in the corresponding function.</p>
<p>A. Registry Mechanism</p>
<p>B. Multi-GPU Training</p>
<p>Following Fairseq, we use the NCCL library and torch.distributed to support model training on multiple GPUs.Every GPU stores a copy of model parameters, and the global optimizer functions as synchronous optimization in each GPU.Gradients accumulation is also supported to mitigate multi-GPU computation lagging.</p>
<p>C. Mixed-Precision</p>
<p>NATURALCC can also support both full precision (FP32) and half-precision floating point (FP16) for fast training and inference.From our experience, setting the FP16 option can largely reduce the memory usage, and further save the training time.To preserve model accuracy, the parameters are stored in FP32 while updated by FP16 gradients.</p>
<p>D. An Implementation Example</p>
<p>We take code completion as an example to show the pipeline of how to quickly build a new task in NATURALCC.Note that, in this section, we only describe the main steps in each file, and more details are referred to the corresponding source files.</p>
<p>Building a task.In the first step, we create a CompletionTask in the ncc/tasks/completion.py, with a decorator register_task around.Listing 2 shows the whole processing of building a new task.This class provides a function build_model for building a model according to the arguments defined by users.We define a new class SeqRNNModel in the ncc/models/completion/seqrnn.py, which inherits the NccLanguageModel.In this class, we build a decoder network LSTMDecoder, which is implemented in the modules folder.NATURALCC provides a command line interface that enables researchers and developers to simply explore the included stateof-the-art models.For each code analysis related tasks, users can try this command: 1 $python -m cli.predictor -m <model> -i <input> where -m is the pre-trained model directory, and -i is the corresponding user input (a partial code snippet in the code completion task).NATURALCC will automatically load model parameters, process the user input and return inference information in details.</p>
<p>B. Graphical User Interface</p>
<p>We have also provided a graphical user interface for users to easily access and explore each trained model's results through an online Web browser.The design of our Web is based on the open source demo of AllenNLP [22].We have deployed the graphical demo in the Nginx server and provided flexible APIs via the Flask engine.</p>
<p>As shown in Figure 2, we have integrated three popular software engineering tasks for demonstration, i.e., code completion, code comment generation, and code retrieval.Taking code completion as an example.By default, we have implemented this task based on the programming language modeling.Given Figure 2: A screenshot of our graphical user interface for demonstration a series of written tokens by Python, the predicted tokens with corresponding probabilities generated by our model will appear simultaneously when the user enters the next tokens.In this page, the users can also select the trained model accordingly.</p>
<p>V. CONCLUSION AND FUTURE WORK This paper presents NATURALCC, an efficient and extensible open-source toolkit to bridge the gap between the programming language and natural language.Currently, NATURALCC has implemented several state-of-the-art models across three popular software engineering tasks.We provide a detailed sample as an example to quickly implement a new task.For demonstration, we have provided a command line tool as well as a graphical user interface for other researchers to do quick prototyping.All the materials about this toolkit can be accessed from http://xcodemind.github.io.</p>
<p>We will extend this toolkit to more software engineering tasks in our future work, including code clone detection, vulnerability detection, and masked language modeling.We also encourage more researchers to join our team to promote the development of this toolkit as well as the whole research community.</p>
<p>Figure 1 :
1
Figure 1: A pipeline of NATURALCC</p>
<p>Listing 1 :
1
The registry mechanism in <strong>init</strong>.py</p>
<p>1 @Listing 2 :
12
tasks/completion/completion.py Building a model.Listing 3 shows the process of building an RNN model for code completion.</p>
<p>1 @Listing 3 : 1 # 1 . 3 # 2 . 6 # 3 .
13113263
models/completion/seqrnn.py Model training.Listing 4 shows the construction of a Trainer and the pipeline of train steps.Core parameters are involved in this class such that pre-trained models can be precisely restored during inference or fine-tuning.Setup task, e.g., completion, comment generation, etc. → 2 task = tasks.setup_task(args)Build model and criterion 4 model = task.build_model(args)5 criterion = task.build_criterion(args)Build trainer 7 trainer = Trainer(args, task, model, criterion)</p>
<p>The open-source Fairseq toolkit has inspired us a lot, and our open-source project also follows the MIT license.
ACKNOWLEDGEMENTSThe Fairseq highly inspires the NATURALCC.We appreciate the Fairseq team for their contribution and the high-quality backbone structure of the framework.Code Representation • RNN (LSTM, Tree-LSTM) • GNN (GGNN) • Transformer • BERT … Code Data Preprocess
Improving automatic source code summarization via deep reinforcement learning. Y Wan, Z Zhao, M Yang, G Xu, H Ying, J Wu, P S Yu, Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. the 33rd ACM/IEEE International Conference on Automated Software EngineeringACM2018</p>
<p>U Alon, S Brody, O Levy, E Yahav, arXiv:1808.01400code2seq: Generating sequences from structured representations of code. 2018arXiv preprint</p>
<p>Code prediction by feeding trees to transformers. S Kim, J Zhao, Y Tian, S Chandra, arXiv:2003.138482020arXiv preprint</p>
<p>Deep code search. X Gu, H Zhang, S Kim, 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE2018</p>
<p>Multimodal attention network learning for semantic source code retrieval. Y Wan, J Shu, Y Sui, G Xu, Z Zhao, J Wu, P Yu, 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). 2019</p>
<p>Codesearchnet challenge: Evaluating the state of semantic code search. H Husain, H.-H Wu, T Gazit, M Allamanis, M Brockschmidt, arXiv:1909.094362019arXiv preprint</p>
<p>fairseq: A fast, extensible toolkit for sequence modeling. M Ott, S Edunov, A Baevski, A Fan, S Gross, N Ng, D Grangier, M Auli, Proceedings of NAACL-HLT 2019: Demonstrations. NAACL-HLT 2019: Demonstrations2019</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 981997</p>
<p>A comprehensive survey on graph neural networks. Z Wu, S Pan, F Chen, G Long, C Zhang, S Y Philip, IEEE Transactions on Neural Networks and Learning Systems. 2020</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 2017</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Big code!= big vocabulary: Open-vocabulary models for source code. R.-M Karampatsis, H Babii, R Robbes, C Sutton, A Janes, arXiv:2003.079142020arXiv preprint</p>
<p>Learning to represent programs with graphs. M Allamanis, M Brockschmidt, M Khademi, arXiv:1711.007402017arXiv preprint</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Code completion with statistical language models. V Raychev, M Vechev, E Yahav, Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation. the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation2014</p>
<p>A transformer-based approach for source code summarization. W U Ahmad, S Chakraborty, B Ray, K.-W Chang, arXiv:2005.006532020arXiv preprint</p>
<p>Fcca: Hybrid code representation for functional clone detection using attention networks. W Hua, Y Sui, Y Wan, G Liu, G Xu, IEEE Transactions on Reliability. 2020</p>
<p>Deep learning type inference. V J Hellendoorn, C Bird, E T Barr, M Allamanis, Proceedings of the 2018 26th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering. the 2018 26th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering2018</p>
<p>Vuldeepecker: A deep learning-based system for vulnerability detection. Z Li, D Zou, S Xu, X Ou, H Jin, S Wang, Z Deng, Y Zhong, arXiv:1801.016812018arXiv preprint</p>
<p>Codebert: A pre-trained model for programming and natural languages. Z Feng, D Guo, D Tang, N Duan, X Feng, M Gong, L Shou, B Qin, T Liu, D Jiang, arXiv:2002.081552020arXiv preprint</p>
<p>Allennlp: A deep semantic natural language processing platform. M Gardner, J Grus, M Neumann, O Tafjord, P Dasigi, N F Liu, M Peters, M Schmitz, L S Zettlemoyer, 2017</p>            </div>
        </div>

    </div>
</body>
</html>
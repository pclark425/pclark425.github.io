<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1051 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1051</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1051</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-229297493</p>
                <p><strong>Paper Title:</strong> <a href="https://jair.org/index.php/jair/article/download/13554/26824" target="_blank">Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey</a></p>
                <p><strong>Paper Abstract:</strong> Building autonomous machines that can explore open-ended environments, discover possible interactions and build repertoires of skills is a general objective of artificial intelligence. Developmental approaches argue that this can only be achieved by autotelic agents : intrinsically motivated learning agents that can learn to represent, generate, select and solve their own problems. In recent years, the convergence of developmental approaches with deep reinforcement learning ( rl ) methods has been leading to the emergence of a new field: developmental reinforcement learning . Developmental rl is concerned with the use of deep rl algorithms to tackle a developmental problem â€” the intrinsically motivated acquisition of open-ended repertoires of skills . The self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions. This raises new challenges compared to standard rl algorithms originally designed to tackle pre-defined sets of goals using external reward signals. The present paper introduces developmental rl and proposes a computational framework based on goal-conditioned rl to tackle the intrinsically motivated skills acquisition problem. It proceeds to present a typology of the various goal representations used in the literature, before reviewing existing methods to learn to represent and prioritize goals in autonomous systems. We finally close the paper by discussing some open challenges in the quest of intrinsically motivated skills acquisition.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1051.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1051.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL-IMGEP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning-based Intrinsically Motivated Goal Exploration Process</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual framework introduced in this paper describing autotelic agents that learn compact goal embeddings, goal-conditioned reward functions, goal spaces and goal-sampling policies while training a goal-conditioned policy via RL to acquire a repertoire of skills in open-ended environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>rl-imgep (autotelic agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Conceptual autotelic embodied learning system: a goal-conditioned policy trained with reinforcement learning (deep RL commonly) augmented with modules to learn goal embeddings, goal-conditioned reward functions, support of goal space, and a goal-sampling policy (Algorithm 1 in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>conceptual (could be instantiated as simulated or robotic agents)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>open-ended / reward-free MDPs (survey-level)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Open-ended, reward-free Markov Decision Processes where the agent must autonomously discover possible interactions and build skill repertoires; environments vary in dimensionality (e.g., image observations), sparsity of reward-like signals, presence of distractors, stochasticity and possibly non-stationarity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized qualitatively by state-space dimensionality (e.g., image vs low-dim state), task difficulty (sparse vs dense feedback), number of objects, presence of stochastic transitions, and long-horizon dependencies; no single numeric metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies from low (toy 2D) to high (high-dim visual, long-horizon, stochastic, many objects); paper treats as 'low/medium/high' depending on concrete environment</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Characterized by number of environment instances, procedural generation (procedurally-generated layouts), presence/degree of distractors, non-stationarity and goal space diversity (size/support); often qualitative in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>varies from low (single static instance) to high (procedural generation / many randomized instances); no numeric values given</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proposed evaluation proxies include: exploration measures (state-visit entropy, coverage), success rate on held-out goals, transfer/ downstream task performance, robustness measures, and analyses of internal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey discusses trade-offs: high environment complexity (e.g., high-dimensional observations, sparse rewards, long horizons) amplifies challenges posed by high variation (procedural generation, stochasticity). It argues agents require mechanisms like curriculum learning, goal-sampling adaptation (learning progress, novelty/diversity, empowerment), and hierarchical decomposition to remain sample-efficient and robust; also notes generative goal models typically limit goals to convex hull of seen outcomes, hurting discovery in high-variation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Survey advocates curriculum learning (automatic curriculum via learning progress, intermediate difficulty), novelty/diversity sampling, hierarchical RL, goal-generative models (VAE/GAN), empowerment-based objectives and bandit-based goal selection.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Introducing rl-imgep highlights that agents must learn goal representations and goal-achievement functions in addition to goal-conditioned policies; environment complexity and variation interact such that high variation demands stronger generalization and robustness mechanisms (procedural generation, diversity priors, curriculum), while complexity (sparse/long-horizon tasks) benefits from goal curricula and hierarchical decomposition. Many existing methods assume pre-defined goal spaces, limiting exploration under high variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1051.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1051.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent57</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent57</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL agent architecture which augments exploration via mixing intrinsic and extrinsic rewards and other components to achieve strong performance on Atari benchmarks (mentioned as interpretable as goal-conditioned under the paper's goal definition).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agent57: Outperforming the atari human benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agent57</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deep RL agent for Atari that adaptively mixes intrinsic and extrinsic rewards and controls parameters (e.g., discount) to improve exploration; in this survey reinterpreted as an agent selecting goal-like parameters (a kind of goal embedding).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (Atari environments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Atari game suite (hard-exploration videogames)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>High-dimensional (pixel) observations with sparse and diverse reward structures across games; some games present hard-exploration challenges where rewards are rare and require long sequences of actions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>High observation dimensionality (pixel inputs), sparse rewards, long-horizon credit assignment; no numeric counts provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation across different games (different dynamics and objectives); not cast as procedural variation in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (many different game instances) but per-game static environments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Game score / episode return relative to human baseline (standard Atari metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey uses Agent57 as an example of treating parameter choices as goals; notes that mixing intrinsic/extrinsic objectives can be seen as agents selecting goals that affect behavior, which helps in complex, sparse-reward settings but does not directly address high environment variation like procedural generation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Intrinsic + extrinsic reward mixing, adaptive parameter selection, directed exploration strategies</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Parameterizing exploration strategies (treating them as goals) can improve performance in complex sparse-reward tasks; however, this addresses exploration within static task instances rather than broad environment variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1051.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1051.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NGU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Never Give Up (NGU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL exploration method that promotes directed exploration through intrinsic rewards; referenced as an example of agents selecting objectives (goals) via parameterized mixtures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Never give up: Learning directed exploration strategies</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Never Give Up (NGU)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep RL agent that augments external rewards with intrinsic motivation signals to encourage directed exploration; in the survey used as an example of goal-like selection of objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (Atari / RL benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Hard-exploration videogames / benchmark RL environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments with sparse rewards and long horizons requiring directed exploration to discover reward-bearing states; complexity arises from sparse feedback and high-dimensional observations.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Sparse reward frequency, observation dimensionality, long required action sequences; no numeric values provided.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation across different game instances; not primarily focused on procedural/environmental variation in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low-to-medium (per-game static)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Cumulative episode return / scores</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Used to illustrate that agents can treat mixtures of objectives as goals; intrinsic exploration helps in high-complexity (sparse reward) environments but paper does not quantify interactions with environment variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Intrinsic reward shaping combined with RL</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Directed intrinsic motivation can mitigate exploration difficulties in complex sparse-reward environments; survey highlights reinterpretation as autotelic when parameter selection is internalized.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1051.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1051.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GoalGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goal Generative Adversarial Network (GoalGAN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A goal-sampling approach that trains a GAN to generate goals of intermediate difficulty to form an automatic curriculum for a goal-conditioned learner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic goal generation for reinforcement learning agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GoalGAN (goal-sampling module)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A goal-sampling strategy that models goal difficulty via feasibility scores and trains a GAN to generate goals whose difficulty is intermediate, thereby guiding RL training of goal-conditioned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>module used with simulated RL agents</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Goal-conditioned RL tasks (e.g., navigation/manipulation benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Tasks where target states live in a pre-defined bounded goal space (e.g., positions in a maze or object configurations); environment complexity arises from reachability and sparse success signals.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Goal reachability/feasibility distribution; task difficulty measured via feasibility scores (proportion of successful episodes); no numeric values provided.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high depending on task</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Varies with goal distribution; GoalGAN aims to sample across difficulty but typically within pre-defined goal space (limited variation beyond seen support).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (goal diversity within bounded support)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Goal success / feasibility metrics and downstream task performance (as proxies)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey notes GoalGAN targets intermediate-difficulty goals to maximize learning progress; warns that intermediate-difficulty heuristics can misclassify stochastic/unlearnable goals as learnable, so trade-off exists between focusing on intermediate difficulty and avoiding impossible/stochastic goals.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automatic curriculum via GAN-based goal generation targeted to intermediate difficulty</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automatic curriculum that focuses on intermediate-difficulty goals can accelerate learning in complex tasks, but is sensitive to environmental stochasticity and may need augmentation (e.g., medium-term learning progress) to avoid focusing on unlearnable goals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1051.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1051.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Skew-Fit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Skew-Fit</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-covering, self-supervised RL method that skews the goal-sampling distribution to prioritize rarely seen outcomes and maximize coverage of the goal space (used for image-based goals).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Skew-fit: State-covering self-supervised reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Skew-Fit (goal-sampling + RL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Combines goal-conditioned RL with a generative model of image states (VAE) and skews sampling to favor underrepresented states to improve state-space coverage in visual RL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (image-based control)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Image-based continuous control / manipulation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>High-dimensional visual observations where valid goal images are generated from a VAE trained on past experience; complexity arises from pixel observations and continuous control, variation is limited to the distribution the VAE models.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Observation dimensionality (images), continuous action spaces, sparsity of success signals; no numeric values provided.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (visual + continuous)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Diversity of goal images produced by VAE; limited to convex hull of experienced observations (training distribution), so variation is bounded by past data.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (bounded by VAE prior)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>State coverage, success on reaching sampled visual goals, downstream task success</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey emphasizes that generative-model-based goal sampling (VAE/GAN) tends to limit goals to the training-distribution convex hull, reducing creative exploration in high-variation environments; Skew-Fit aims to maximize coverage within that distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Generative-model-based goal sampling (VAE) with skewed sampling towards rare states; goal-conditioned RL</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Skew-Fit improves state-space coverage for visual goal-conditioned tasks, but its exploration is constrained to the distribution modeled by the generative prior; survey highlights the limitation for discovering out-of-distribution (creative) goals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1051.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1051.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL-RIG / RIG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning with Imagined Goals (RIG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A visual goal-conditioned RL approach that trains a generative model (VAE) of images and samples imagined goals from the learned prior to train goal-conditioned policies in the visual domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual reinforcement learning with imagined goals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RIG (visual goal-conditioned agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Combines goal-conditioned RL with a VAE learned on image observations to provide goal embeddings and goal sampling from the VAE prior, enabling learning of image-goal-reaching behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (robot manipulation with image observations)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Image-based manipulation environments (gripper, block pushing, visual reaching)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>High-dimensional pixel observations, continuous control of manipulators; difficulty derives from partial observability in images, sparse success signals and complicated dynamics of object interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Image observation dimensionality, continuous action space, number of objects; no numeric metrics provided by survey.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Goal variation limited to samples from VAE prior trained on past images; may include different object configurations but constrained to training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (bounded by VAE support)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate on reaching a visual goal, downstream control performance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey notes RIG and similar methods learn visual goal representations but that goal support learned from past data constrains variation (convex-hull limitation), which can hinder discovery in highly variable or out-of-distribution environments.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>VAE-based goal generation (imagined goals) + goal-conditioned RL + hindsight relabelling</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Imagined visual goals via generative models enable goal-conditioned RL in pixel spaces, improving learning in complex visual tasks but limiting creative exploration to the model's support; survey highlights need for richer goal imagination mechanisms to extend variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1051.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1051.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIAYN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diversity Is All You Need (DIAYN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised skill discovery method training policies that maximize mutual information between latent skill variables and states, producing diverse, discriminable behaviors without external rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diversity is all you need: Learning skills without a reward function</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DIAYN (skill-discovery agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Learns a set of latent-conditioned skills by maximizing a mutual-information-based objective (empowerment-like): a discriminator predicts the skill id from states, yielding skill-specific internal rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (control tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>State-space exploration tasks (navigation, locomotion, manipulation benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments where discovering diverse behaviors that lead to distinct state-visitation distributions is the goal; complexity depends on state dimensionality and dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>State-space size and diversity of reachable basins of attraction; no numerical values in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high depending on domain</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation captured by number of discovered distinct skills (latent dimension) and entropy of goal/skill distribution; method seeks high diversity (H(Z)â†—) while being predictable from states (H(Z|S)â†˜).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high by design (encourages many diverse behaviors)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Diversity / discriminability of skills (discriminator accuracy), state coverage</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey connects empowerment/DIAYN-style methods to autotelic goals: maximizing mutual information increases goal distribution entropy (diversity) which can help exploration in complex environments, but these methods often do not expose explicit goal-conditioned reward functions as autotelic frameworks require.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Unsupervised skill discovery via mutual information (empowerment) objectives, discriminator-based internal rewards</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Empowerment-like and skill-discovery methods promote diversity and can be reframed as learning internal goal-conditioned rewards; they are useful for exploring complex state-spaces but require bridging to explicit goal representations for autotelic agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1051.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1051.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMAGINE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IMAGINE (Language-based goal imagination)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that leverages language-based goal representations learned via social supervision and composes them to imagine novel (out-of-distribution) goals, increasing exploration and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language as a cognitive tool to imagine goals in curiosity driven exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>IMAGINE (language-based autotelic agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Autotelic architecture that learns language-conditioned goals (via social supervision) and composes templates in language to generate new, imagined goals beyond the convex hull of past experiences, then uses goal-conditioned RL to pursue them.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (interactive / language-capable environments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Interactive environments with language descriptions and combinatorial object interactions</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments supporting language annotations/commands where goals can be described compositionaly (e.g., object properties, relations); complexity arises from combinatorial compositionality and multi-step interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Combinatorial goal space size due to language compositionality, number of object relations; no numeric values provided.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (combinatorial / structured goals)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>High: supports compositional generation of novel language goals beyond past instances (out-of-distribution imagination via template recombination).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exploration diversity, ability to generalize to novel language-specified goals, downstream competency on human-defined tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper emphasizes that language compositionality enables imagining creative goals increasing variation beyond training distribution, which helps exploration in complex spaces; this contrasts with generative-model-limited approaches whose variation is bounded by training data.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Language-conditioned goal generation (social supervision) + compositional recombination of goal templates + goal-conditioned RL</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Leveraging language as an internal, compositional representation enables imagining out-of-distribution goals, increasing exploration and potential generalization in complex, combinatorial environments; this can alleviate the convex-hull limitations of purely generative-model-based goal sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1051.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1051.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FirstReturnThenExplore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>First Return, Then Explore</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration algorithm that prioritizes returning to previously successful trajectories before exploring further, designed to help hard-exploration problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>First return, then explore</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>First Return, Then Explore (exploration algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Exploration strategy that biases behaviour to re-enter previously rewarding/interesting regions (first return) and then undertake exploratory deviations (explore) to find new rewarding trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>generally applied to simulated RL agents</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Hard-exploration benchmark environments (e.g., sparse-reward tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments where rare successes must be rediscovered consistently before further exploration yields improvements; complexity stems from sparse rewards and long sequences required to rediscover successes.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Rarity of reward-bearing trajectories, sparseness of reward signal; no numeric values provided.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (hard-exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Mostly per-task (low variation across environment instances); approach targets ability to explore within a single instance more effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exploration success, number of times a rare rewarding trajectory is rediscovered, downstream task scores</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey cites this as a method addressing complexity from sparse rewards; emphasizes that strategies optimized for rediscovering rare events help exploration but do not necessarily address environment variation across instances.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Return-to-success + exploratory diversification</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Techniques that prioritize revisiting rare successes can be effective in high-complexity, low-variation hard-exploration tasks but are not a substitute for mechanisms addressing high environmental variation or generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1051.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1051.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLand / OEL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Procedurally-generated multi-player worlds (XLand / Open-Ended Learning environments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Procedurally-generated, large-scale environments used to increase variation via many procedurally-created task instances, enabling studies of generalization and open-ended learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Open-Ended learning leads to generally capable agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agents evaluated in procedurally-generated environments (e.g., XLand / OEL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated RL agents (often goal-conditioned or multi-task) trained and evaluated in procedurally-generated multi-task, multi-player worlds to test robustness and generalization across high variation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agents in procedurally-generated virtual worlds</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Procedurally-generated multi-player worlds (XLand / OEL)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Highly varied environments generated procedurally to create many different tasks/levels; complexity arises from combinatorial map/task variations, differing objectives, and multi-agent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of procedurally-generated instances, diversity of task templates, multi-agent dynamics and combinatorial state-space size; survey does not provide numerical counts.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>High: explicitly designed for large-scale procedural variation across environment instances (many distinct tasks and layouts).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Robustness across instances, generalization to held-out procedurally-generated tasks, task success rates across distributions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey highlights that procedural generation increases variation requiring agents with better generalization and curriculum strategies; notes that training in many varied instances can improve robustness but demands methods that scale sample-efficiency and representation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Training across many procedurally-generated instances (large-scale multi-task / open-ended training), sometimes with curriculum or diversity priors</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Procedural generation is a primary mechanism to create high environmental variation for evaluating robustness and generalization; survey notes that success in such regimes hinges on representation learning, curriculum, and scalable RL methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1051.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1051.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Object Manipulation Envs (manipulation benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Object Manipulation Environments (robotic manipulation benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Class of embodied environments used in survey studies: manipulation tasks involving objects (block positions, door opening, tool use) that vary in complexity from simple target coordinates to multi-object, nested tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Manipulation task agents (goal-conditioned policies)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Robotic or simulated agents trained with goal-conditioned RL or population-based methods to control manipulators to reach target object configurations; often use distance-based reward or image-goal variants with VAEs.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated/robotic agents (manipulators, grippers)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Object manipulation environments (block stacking, tool use, drawer tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments involving multiple objects with possibly nested dependencies (e.g., tool use), varying observation modality (low-dim state vs vision) and requiring precise control and sometimes long-horizon sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of objects, degrees of freedom in the robot, precision/tolerance thresholds, time-horizon for tasks; specific numeric values not in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>range from medium (single object reach) to high (nested tool use, many objects)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation across object types, initial configurations and possibly procedurally-generated object arrangements; in many prior works goal spaces are pre-defined limiting variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low-to-medium when goal space pre-defined; can be increased via procedural generation or compositional language goals</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate on target configurations, task completion, sample-efficiency metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey reports that manipulation tasks benefit from intrinsic motivation and curriculum (learning progress) to handle complexity; when variation is introduced (visual input, many object configurations), representation learning (VAEs, learned reward metrics) becomes critical; generative-model-based goal sampling may constrain exploration under high variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Goal-conditioned RL, generative (VAE) goal sampling, learning progress curricula, imitation where available</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Manipulation tasks illustrate the trade-off: higher sensory complexity (vision) and greater environmental variation require learned goal representations and reward estimators; curricula and intrinsic motivations improve learning in complex manipulation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Agent57: Outperforming the atari human benchmark <em>(Rating: 2)</em></li>
                <li>Never give up: Learning directed exploration strategies <em>(Rating: 2)</em></li>
                <li>Automatic goal generation for reinforcement learning agents <em>(Rating: 2)</em></li>
                <li>Skew-fit: State-covering self-supervised reinforcement learning <em>(Rating: 2)</em></li>
                <li>Visual reinforcement learning with imagined goals <em>(Rating: 2)</em></li>
                <li>Diversity is all you need: Learning skills without a reward function <em>(Rating: 2)</em></li>
                <li>Language as a cognitive tool to imagine goals in curiosity driven exploration <em>(Rating: 2)</em></li>
                <li>First return, then explore <em>(Rating: 2)</em></li>
                <li>Open-Ended learning leads to generally capable agents <em>(Rating: 2)</em></li>
                <li>Automatic curriculum learning for deep RL: A short survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1051",
    "paper_id": "paper-229297493",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "RL-IMGEP",
            "name_full": "Reinforcement Learning-based Intrinsically Motivated Goal Exploration Process",
            "brief_description": "A conceptual framework introduced in this paper describing autotelic agents that learn compact goal embeddings, goal-conditioned reward functions, goal spaces and goal-sampling policies while training a goal-conditioned policy via RL to acquire a repertoire of skills in open-ended environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "rl-imgep (autotelic agent)",
            "agent_description": "Conceptual autotelic embodied learning system: a goal-conditioned policy trained with reinforcement learning (deep RL commonly) augmented with modules to learn goal embeddings, goal-conditioned reward functions, support of goal space, and a goal-sampling policy (Algorithm 1 in the paper).",
            "agent_type": "conceptual (could be instantiated as simulated or robotic agents)",
            "environment_name": "open-ended / reward-free MDPs (survey-level)",
            "environment_description": "Open-ended, reward-free Markov Decision Processes where the agent must autonomously discover possible interactions and build skill repertoires; environments vary in dimensionality (e.g., image observations), sparsity of reward-like signals, presence of distractors, stochasticity and possibly non-stationarity.",
            "complexity_measure": "Characterized qualitatively by state-space dimensionality (e.g., image vs low-dim state), task difficulty (sparse vs dense feedback), number of objects, presence of stochastic transitions, and long-horizon dependencies; no single numeric metric provided.",
            "complexity_level": "varies from low (toy 2D) to high (high-dim visual, long-horizon, stochastic, many objects); paper treats as 'low/medium/high' depending on concrete environment",
            "variation_measure": "Characterized by number of environment instances, procedural generation (procedurally-generated layouts), presence/degree of distractors, non-stationarity and goal space diversity (size/support); often qualitative in survey.",
            "variation_level": "varies from low (single static instance) to high (procedural generation / many randomized instances); no numeric values given",
            "performance_metric": "Proposed evaluation proxies include: exploration measures (state-visit entropy, coverage), success rate on held-out goals, transfer/ downstream task performance, robustness measures, and analyses of internal representations.",
            "performance_value": null,
            "complexity_variation_relationship": "Survey discusses trade-offs: high environment complexity (e.g., high-dimensional observations, sparse rewards, long horizons) amplifies challenges posed by high variation (procedural generation, stochasticity). It argues agents require mechanisms like curriculum learning, goal-sampling adaptation (learning progress, novelty/diversity, empowerment), and hierarchical decomposition to remain sample-efficient and robust; also notes generative goal models typically limit goals to convex hull of seen outcomes, hurting discovery in high-variation settings.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Survey advocates curriculum learning (automatic curriculum via learning progress, intermediate difficulty), novelty/diversity sampling, hierarchical RL, goal-generative models (VAE/GAN), empowerment-based objectives and bandit-based goal selection.",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Introducing rl-imgep highlights that agents must learn goal representations and goal-achievement functions in addition to goal-conditioned policies; environment complexity and variation interact such that high variation demands stronger generalization and robustness mechanisms (procedural generation, diversity priors, curriculum), while complexity (sparse/long-horizon tasks) benefits from goal curricula and hierarchical decomposition. Many existing methods assume pre-defined goal spaces, limiting exploration under high variation.",
            "uuid": "e1051.0",
            "source_info": {
                "paper_title": "Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Agent57",
            "name_full": "Agent57",
            "brief_description": "An RL agent architecture which augments exploration via mixing intrinsic and extrinsic rewards and other components to achieve strong performance on Atari benchmarks (mentioned as interpretable as goal-conditioned under the paper's goal definition).",
            "citation_title": "Agent57: Outperforming the atari human benchmark",
            "mention_or_use": "mention",
            "agent_name": "Agent57",
            "agent_description": "A deep RL agent for Atari that adaptively mixes intrinsic and extrinsic rewards and controls parameters (e.g., discount) to improve exploration; in this survey reinterpreted as an agent selecting goal-like parameters (a kind of goal embedding).",
            "agent_type": "simulated agent (Atari environments)",
            "environment_name": "Atari game suite (hard-exploration videogames)",
            "environment_description": "High-dimensional (pixel) observations with sparse and diverse reward structures across games; some games present hard-exploration challenges where rewards are rare and require long sequences of actions.",
            "complexity_measure": "High observation dimensionality (pixel inputs), sparse rewards, long-horizon credit assignment; no numeric counts provided in survey.",
            "complexity_level": "high",
            "variation_measure": "Variation across different games (different dynamics and objectives); not cast as procedural variation in the survey.",
            "variation_level": "medium (many different game instances) but per-game static environments",
            "performance_metric": "Game score / episode return relative to human baseline (standard Atari metrics).",
            "performance_value": null,
            "complexity_variation_relationship": "Survey uses Agent57 as an example of treating parameter choices as goals; notes that mixing intrinsic/extrinsic objectives can be seen as agents selecting goals that affect behavior, which helps in complex, sparse-reward settings but does not directly address high environment variation like procedural generation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Intrinsic + extrinsic reward mixing, adaptive parameter selection, directed exploration strategies",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Parameterizing exploration strategies (treating them as goals) can improve performance in complex sparse-reward tasks; however, this addresses exploration within static task instances rather than broad environment variation.",
            "uuid": "e1051.1",
            "source_info": {
                "paper_title": "Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "NGU",
            "name_full": "Never Give Up (NGU)",
            "brief_description": "An RL exploration method that promotes directed exploration through intrinsic rewards; referenced as an example of agents selecting objectives (goals) via parameterized mixtures.",
            "citation_title": "Never give up: Learning directed exploration strategies",
            "mention_or_use": "mention",
            "agent_name": "Never Give Up (NGU)",
            "agent_description": "Deep RL agent that augments external rewards with intrinsic motivation signals to encourage directed exploration; in the survey used as an example of goal-like selection of objectives.",
            "agent_type": "simulated agent (Atari / RL benchmarks)",
            "environment_name": "Hard-exploration videogames / benchmark RL environments",
            "environment_description": "Environments with sparse rewards and long horizons requiring directed exploration to discover reward-bearing states; complexity arises from sparse feedback and high-dimensional observations.",
            "complexity_measure": "Sparse reward frequency, observation dimensionality, long required action sequences; no numeric values provided.",
            "complexity_level": "high",
            "variation_measure": "Variation across different game instances; not primarily focused on procedural/environmental variation in survey.",
            "variation_level": "low-to-medium (per-game static)",
            "performance_metric": "Cumulative episode return / scores",
            "performance_value": null,
            "complexity_variation_relationship": "Used to illustrate that agents can treat mixtures of objectives as goals; intrinsic exploration helps in high-complexity (sparse reward) environments but paper does not quantify interactions with environment variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Intrinsic reward shaping combined with RL",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Directed intrinsic motivation can mitigate exploration difficulties in complex sparse-reward environments; survey highlights reinterpretation as autotelic when parameter selection is internalized.",
            "uuid": "e1051.2",
            "source_info": {
                "paper_title": "Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "GoalGAN",
            "name_full": "Goal Generative Adversarial Network (GoalGAN)",
            "brief_description": "A goal-sampling approach that trains a GAN to generate goals of intermediate difficulty to form an automatic curriculum for a goal-conditioned learner.",
            "citation_title": "Automatic goal generation for reinforcement learning agents",
            "mention_or_use": "mention",
            "agent_name": "GoalGAN (goal-sampling module)",
            "agent_description": "A goal-sampling strategy that models goal difficulty via feasibility scores and trains a GAN to generate goals whose difficulty is intermediate, thereby guiding RL training of goal-conditioned policies.",
            "agent_type": "module used with simulated RL agents",
            "environment_name": "Goal-conditioned RL tasks (e.g., navigation/manipulation benchmarks)",
            "environment_description": "Tasks where target states live in a pre-defined bounded goal space (e.g., positions in a maze or object configurations); environment complexity arises from reachability and sparse success signals.",
            "complexity_measure": "Goal reachability/feasibility distribution; task difficulty measured via feasibility scores (proportion of successful episodes); no numeric values provided.",
            "complexity_level": "medium-to-high depending on task",
            "variation_measure": "Varies with goal distribution; GoalGAN aims to sample across difficulty but typically within pre-defined goal space (limited variation beyond seen support).",
            "variation_level": "medium (goal diversity within bounded support)",
            "performance_metric": "Goal success / feasibility metrics and downstream task performance (as proxies)",
            "performance_value": null,
            "complexity_variation_relationship": "Survey notes GoalGAN targets intermediate-difficulty goals to maximize learning progress; warns that intermediate-difficulty heuristics can misclassify stochastic/unlearnable goals as learnable, so trade-off exists between focusing on intermediate difficulty and avoiding impossible/stochastic goals.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Automatic curriculum via GAN-based goal generation targeted to intermediate difficulty",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Automatic curriculum that focuses on intermediate-difficulty goals can accelerate learning in complex tasks, but is sensitive to environmental stochasticity and may need augmentation (e.g., medium-term learning progress) to avoid focusing on unlearnable goals.",
            "uuid": "e1051.3",
            "source_info": {
                "paper_title": "Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Skew-Fit",
            "name_full": "Skew-Fit",
            "brief_description": "A state-covering, self-supervised RL method that skews the goal-sampling distribution to prioritize rarely seen outcomes and maximize coverage of the goal space (used for image-based goals).",
            "citation_title": "Skew-fit: State-covering self-supervised reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Skew-Fit (goal-sampling + RL)",
            "agent_description": "Combines goal-conditioned RL with a generative model of image states (VAE) and skews sampling to favor underrepresented states to improve state-space coverage in visual RL tasks.",
            "agent_type": "simulated agent (image-based control)",
            "environment_name": "Image-based continuous control / manipulation tasks",
            "environment_description": "High-dimensional visual observations where valid goal images are generated from a VAE trained on past experience; complexity arises from pixel observations and continuous control, variation is limited to the distribution the VAE models.",
            "complexity_measure": "Observation dimensionality (images), continuous action spaces, sparsity of success signals; no numeric values provided.",
            "complexity_level": "high (visual + continuous)",
            "variation_measure": "Diversity of goal images produced by VAE; limited to convex hull of experienced observations (training distribution), so variation is bounded by past data.",
            "variation_level": "medium (bounded by VAE prior)",
            "performance_metric": "State coverage, success on reaching sampled visual goals, downstream task success",
            "performance_value": null,
            "complexity_variation_relationship": "Survey emphasizes that generative-model-based goal sampling (VAE/GAN) tends to limit goals to the training-distribution convex hull, reducing creative exploration in high-variation environments; Skew-Fit aims to maximize coverage within that distribution.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Generative-model-based goal sampling (VAE) with skewed sampling towards rare states; goal-conditioned RL",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Skew-Fit improves state-space coverage for visual goal-conditioned tasks, but its exploration is constrained to the distribution modeled by the generative prior; survey highlights the limitation for discovering out-of-distribution (creative) goals.",
            "uuid": "e1051.4",
            "source_info": {
                "paper_title": "Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "RL-RIG / RIG",
            "name_full": "Reinforcement Learning with Imagined Goals (RIG)",
            "brief_description": "A visual goal-conditioned RL approach that trains a generative model (VAE) of images and samples imagined goals from the learned prior to train goal-conditioned policies in the visual domain.",
            "citation_title": "Visual reinforcement learning with imagined goals",
            "mention_or_use": "mention",
            "agent_name": "RIG (visual goal-conditioned agent)",
            "agent_description": "Combines goal-conditioned RL with a VAE learned on image observations to provide goal embeddings and goal sampling from the VAE prior, enabling learning of image-goal-reaching behaviors.",
            "agent_type": "simulated agent (robot manipulation with image observations)",
            "environment_name": "Image-based manipulation environments (gripper, block pushing, visual reaching)",
            "environment_description": "High-dimensional pixel observations, continuous control of manipulators; difficulty derives from partial observability in images, sparse success signals and complicated dynamics of object interactions.",
            "complexity_measure": "Image observation dimensionality, continuous action space, number of objects; no numeric metrics provided by survey.",
            "complexity_level": "high",
            "variation_measure": "Goal variation limited to samples from VAE prior trained on past images; may include different object configurations but constrained to training distribution.",
            "variation_level": "medium (bounded by VAE support)",
            "performance_metric": "Success rate on reaching a visual goal, downstream control performance",
            "performance_value": null,
            "complexity_variation_relationship": "Survey notes RIG and similar methods learn visual goal representations but that goal support learned from past data constrains variation (convex-hull limitation), which can hinder discovery in highly variable or out-of-distribution environments.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "VAE-based goal generation (imagined goals) + goal-conditioned RL + hindsight relabelling",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Imagined visual goals via generative models enable goal-conditioned RL in pixel spaces, improving learning in complex visual tasks but limiting creative exploration to the model's support; survey highlights need for richer goal imagination mechanisms to extend variation.",
            "uuid": "e1051.5",
            "source_info": {
                "paper_title": "Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "DIAYN",
            "name_full": "Diversity Is All You Need (DIAYN)",
            "brief_description": "An unsupervised skill discovery method training policies that maximize mutual information between latent skill variables and states, producing diverse, discriminable behaviors without external rewards.",
            "citation_title": "Diversity is all you need: Learning skills without a reward function",
            "mention_or_use": "mention",
            "agent_name": "DIAYN (skill-discovery agent)",
            "agent_description": "Learns a set of latent-conditioned skills by maximizing a mutual-information-based objective (empowerment-like): a discriminator predicts the skill id from states, yielding skill-specific internal rewards.",
            "agent_type": "simulated agent (control tasks)",
            "environment_name": "State-space exploration tasks (navigation, locomotion, manipulation benchmarks)",
            "environment_description": "Environments where discovering diverse behaviors that lead to distinct state-visitation distributions is the goal; complexity depends on state dimensionality and dynamics.",
            "complexity_measure": "State-space size and diversity of reachable basins of attraction; no numerical values in survey.",
            "complexity_level": "medium-to-high depending on domain",
            "variation_measure": "Variation captured by number of discovered distinct skills (latent dimension) and entropy of goal/skill distribution; method seeks high diversity (H(Z)â†—) while being predictable from states (H(Z|S)â†˜).",
            "variation_level": "high by design (encourages many diverse behaviors)",
            "performance_metric": "Diversity / discriminability of skills (discriminator accuracy), state coverage",
            "performance_value": null,
            "complexity_variation_relationship": "Survey connects empowerment/DIAYN-style methods to autotelic goals: maximizing mutual information increases goal distribution entropy (diversity) which can help exploration in complex environments, but these methods often do not expose explicit goal-conditioned reward functions as autotelic frameworks require.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Unsupervised skill discovery via mutual information (empowerment) objectives, discriminator-based internal rewards",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Empowerment-like and skill-discovery methods promote diversity and can be reframed as learning internal goal-conditioned rewards; they are useful for exploring complex state-spaces but require bridging to explicit goal representations for autotelic agents.",
            "uuid": "e1051.6",
            "source_info": {
                "paper_title": "Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "IMAGINE",
            "name_full": "IMAGINE (Language-based goal imagination)",
            "brief_description": "An approach that leverages language-based goal representations learned via social supervision and composes them to imagine novel (out-of-distribution) goals, increasing exploration and generalization.",
            "citation_title": "Language as a cognitive tool to imagine goals in curiosity driven exploration",
            "mention_or_use": "mention",
            "agent_name": "IMAGINE (language-based autotelic agent)",
            "agent_description": "Autotelic architecture that learns language-conditioned goals (via social supervision) and composes templates in language to generate new, imagined goals beyond the convex hull of past experiences, then uses goal-conditioned RL to pursue them.",
            "agent_type": "simulated agent (interactive / language-capable environments)",
            "environment_name": "Interactive environments with language descriptions and combinatorial object interactions",
            "environment_description": "Environments supporting language annotations/commands where goals can be described compositionaly (e.g., object properties, relations); complexity arises from combinatorial compositionality and multi-step interactions.",
            "complexity_measure": "Combinatorial goal space size due to language compositionality, number of object relations; no numeric values provided.",
            "complexity_level": "high (combinatorial / structured goals)",
            "variation_measure": "High: supports compositional generation of novel language goals beyond past instances (out-of-distribution imagination via template recombination).",
            "variation_level": "high",
            "performance_metric": "Exploration diversity, ability to generalize to novel language-specified goals, downstream competency on human-defined tasks",
            "performance_value": null,
            "complexity_variation_relationship": "Paper emphasizes that language compositionality enables imagining creative goals increasing variation beyond training distribution, which helps exploration in complex spaces; this contrasts with generative-model-limited approaches whose variation is bounded by training data.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Language-conditioned goal generation (social supervision) + compositional recombination of goal templates + goal-conditioned RL",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Leveraging language as an internal, compositional representation enables imagining out-of-distribution goals, increasing exploration and potential generalization in complex, combinatorial environments; this can alleviate the convex-hull limitations of purely generative-model-based goal sampling.",
            "uuid": "e1051.7",
            "source_info": {
                "paper_title": "Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "FirstReturnThenExplore",
            "name_full": "First Return, Then Explore",
            "brief_description": "An exploration algorithm that prioritizes returning to previously successful trajectories before exploring further, designed to help hard-exploration problems.",
            "citation_title": "First return, then explore",
            "mention_or_use": "mention",
            "agent_name": "First Return, Then Explore (exploration algorithm)",
            "agent_description": "Exploration strategy that biases behaviour to re-enter previously rewarding/interesting regions (first return) and then undertake exploratory deviations (explore) to find new rewarding trajectories.",
            "agent_type": "generally applied to simulated RL agents",
            "environment_name": "Hard-exploration benchmark environments (e.g., sparse-reward tasks)",
            "environment_description": "Environments where rare successes must be rediscovered consistently before further exploration yields improvements; complexity stems from sparse rewards and long sequences required to rediscover successes.",
            "complexity_measure": "Rarity of reward-bearing trajectories, sparseness of reward signal; no numeric values provided.",
            "complexity_level": "high (hard-exploration)",
            "variation_measure": "Mostly per-task (low variation across environment instances); approach targets ability to explore within a single instance more effectively.",
            "variation_level": "low",
            "performance_metric": "Exploration success, number of times a rare rewarding trajectory is rediscovered, downstream task scores",
            "performance_value": null,
            "complexity_variation_relationship": "Survey cites this as a method addressing complexity from sparse rewards; emphasizes that strategies optimized for rediscovering rare events help exploration but do not necessarily address environment variation across instances.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Return-to-success + exploratory diversification",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Techniques that prioritize revisiting rare successes can be effective in high-complexity, low-variation hard-exploration tasks but are not a substitute for mechanisms addressing high environmental variation or generalization.",
            "uuid": "e1051.8",
            "source_info": {
                "paper_title": "Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "XLand / OEL",
            "name_full": "Procedurally-generated multi-player worlds (XLand / Open-Ended Learning environments)",
            "brief_description": "Procedurally-generated, large-scale environments used to increase variation via many procedurally-created task instances, enabling studies of generalization and open-ended learning.",
            "citation_title": "Open-Ended learning leads to generally capable agents",
            "mention_or_use": "mention",
            "agent_name": "Agents evaluated in procedurally-generated environments (e.g., XLand / OEL)",
            "agent_description": "Simulated RL agents (often goal-conditioned or multi-task) trained and evaluated in procedurally-generated multi-task, multi-player worlds to test robustness and generalization across high variation.",
            "agent_type": "simulated agents in procedurally-generated virtual worlds",
            "environment_name": "Procedurally-generated multi-player worlds (XLand / OEL)",
            "environment_description": "Highly varied environments generated procedurally to create many different tasks/levels; complexity arises from combinatorial map/task variations, differing objectives, and multi-agent dynamics.",
            "complexity_measure": "Number of procedurally-generated instances, diversity of task templates, multi-agent dynamics and combinatorial state-space size; survey does not provide numerical counts.",
            "complexity_level": "high",
            "variation_measure": "High: explicitly designed for large-scale procedural variation across environment instances (many distinct tasks and layouts).",
            "variation_level": "high",
            "performance_metric": "Robustness across instances, generalization to held-out procedurally-generated tasks, task success rates across distributions",
            "performance_value": null,
            "complexity_variation_relationship": "Survey highlights that procedural generation increases variation requiring agents with better generalization and curriculum strategies; notes that training in many varied instances can improve robustness but demands methods that scale sample-efficiency and representation learning.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Training across many procedurally-generated instances (large-scale multi-task / open-ended training), sometimes with curriculum or diversity priors",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Procedural generation is a primary mechanism to create high environmental variation for evaluating robustness and generalization; survey notes that success in such regimes hinges on representation learning, curriculum, and scalable RL methods.",
            "uuid": "e1051.9",
            "source_info": {
                "paper_title": "Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Object Manipulation Envs (manipulation benchmarks)",
            "name_full": "Object Manipulation Environments (robotic manipulation benchmarks)",
            "brief_description": "Class of embodied environments used in survey studies: manipulation tasks involving objects (block positions, door opening, tool use) that vary in complexity from simple target coordinates to multi-object, nested tool use.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Manipulation task agents (goal-conditioned policies)",
            "agent_description": "Robotic or simulated agents trained with goal-conditioned RL or population-based methods to control manipulators to reach target object configurations; often use distance-based reward or image-goal variants with VAEs.",
            "agent_type": "simulated/robotic agents (manipulators, grippers)",
            "environment_name": "Object manipulation environments (block stacking, tool use, drawer tasks)",
            "environment_description": "Environments involving multiple objects with possibly nested dependencies (e.g., tool use), varying observation modality (low-dim state vs vision) and requiring precise control and sometimes long-horizon sequences.",
            "complexity_measure": "Number of objects, degrees of freedom in the robot, precision/tolerance thresholds, time-horizon for tasks; specific numeric values not in survey.",
            "complexity_level": "range from medium (single object reach) to high (nested tool use, many objects)",
            "variation_measure": "Variation across object types, initial configurations and possibly procedurally-generated object arrangements; in many prior works goal spaces are pre-defined limiting variation.",
            "variation_level": "low-to-medium when goal space pre-defined; can be increased via procedural generation or compositional language goals",
            "performance_metric": "Success rate on target configurations, task completion, sample-efficiency metrics",
            "performance_value": null,
            "complexity_variation_relationship": "Survey reports that manipulation tasks benefit from intrinsic motivation and curriculum (learning progress) to handle complexity; when variation is introduced (visual input, many object configurations), representation learning (VAEs, learned reward metrics) becomes critical; generative-model-based goal sampling may constrain exploration under high variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Goal-conditioned RL, generative (VAE) goal sampling, learning progress curricula, imitation where available",
            "generalization_tested": null,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Manipulation tasks illustrate the trade-off: higher sensory complexity (vision) and greater environmental variation require learned goal representations and reward estimators; curricula and intrinsic motivations improve learning in complex manipulation settings.",
            "uuid": "e1051.10",
            "source_info": {
                "paper_title": "Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Agent57: Outperforming the atari human benchmark",
            "rating": 2,
            "sanitized_title": "agent57_outperforming_the_atari_human_benchmark"
        },
        {
            "paper_title": "Never give up: Learning directed exploration strategies",
            "rating": 2,
            "sanitized_title": "never_give_up_learning_directed_exploration_strategies"
        },
        {
            "paper_title": "Automatic goal generation for reinforcement learning agents",
            "rating": 2,
            "sanitized_title": "automatic_goal_generation_for_reinforcement_learning_agents"
        },
        {
            "paper_title": "Skew-fit: State-covering self-supervised reinforcement learning",
            "rating": 2,
            "sanitized_title": "skewfit_statecovering_selfsupervised_reinforcement_learning"
        },
        {
            "paper_title": "Visual reinforcement learning with imagined goals",
            "rating": 2,
            "sanitized_title": "visual_reinforcement_learning_with_imagined_goals"
        },
        {
            "paper_title": "Diversity is all you need: Learning skills without a reward function",
            "rating": 2,
            "sanitized_title": "diversity_is_all_you_need_learning_skills_without_a_reward_function"
        },
        {
            "paper_title": "Language as a cognitive tool to imagine goals in curiosity driven exploration",
            "rating": 2,
            "sanitized_title": "language_as_a_cognitive_tool_to_imagine_goals_in_curiosity_driven_exploration"
        },
        {
            "paper_title": "First return, then explore",
            "rating": 2,
            "sanitized_title": "first_return_then_explore"
        },
        {
            "paper_title": "Open-Ended learning leads to generally capable agents",
            "rating": 2,
            "sanitized_title": "openended_learning_leads_to_generally_capable_agents"
        },
        {
            "paper_title": "Automatic curriculum learning for deep RL: A short survey",
            "rating": 1,
            "sanitized_title": "automatic_curriculum_learning_for_deep_rl_a_short_survey"
        }
    ],
    "cost": 0.02463075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey
07/2022</p>
<p>Tristan Karch tristan.karch@inria.fr 
Olivier Sigaud olivier.sigaud@upmc.fr 
Pierre-Yves Oudeyer pierre-yves.oudeyer@inria.fr </p>
<p>INRIA and Univ. de Bordeaux
Bordeaux (FR)</p>
<p>INRIA and Univ. de Bordeaux
Bordeaux (FR)</p>
<p>Sorbonne UniversitÃ©
Paris (FR)</p>
<p>INRIA; Bordeaux (FR)
ENSTA Paris Tech
Paris (FR)</p>
<p>Autotelic Agents with Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey
07/2022C8F6739B1192106E209A89A6DCCE6144Submitted 12/2021;
Building autonomous machines that can explore open-ended environments, discover possible interactions and build repertoires of skills is a general objective of artificial intelligence.Developmental approaches argue that this can only be achieved by autotelic agents: intrinsically motivated learning agents that can learn to represent, generate, select and solve their own problems.In recent years, the convergence of developmental approaches with deep reinforcement learning (rl) methods has been leading to the emergence of a new field: developmental reinforcement learning.Developmental rl is concerned with the use of deep rl algorithms to tackle a developmental problem -the intrinsically motivated acquisition of open-ended repertoires of skills.The self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions.This raises new challenges compared to standard rl algorithms originally designed to tackle pre-defined sets of goals using external reward signals.The present paper introduces developmental rl and proposes a computational framework based on goal-conditioned rl to tackle the intrinsically motivated skills acquisition problem.It proceeds to present a typology of the various goal representations used in the literature, before reviewing existing methods to learn to represent and prioritize goals in autonomous systems.We finally close the paper by discussing some open challenges in the quest of intrinsically motivated skills acquisition.</p>
<p>Introduction</p>
<p>Building autonomous machines that can explore large environments, discover interesting interactions and learn open-ended repertoires of skills is a long-standing goal in artificial intelligence.Humans are remarkable examples of this lifelong, open-ended learning.They learn to recognize objects and crawl as infants, then learn to ask questions and interact with peers as children.Across their lives, humans build a large repertoire of diverse skills from a virtually infinite set of possibilities.What is most striking, perhaps, is their ability to invent and pursue their own problems, using internal feedback to assess completion.We would like to build artificial agents able to demonstrate equivalent lifelong learning abilities.</p>
<p>We can think of two approaches to this problem: developmental approaches, in particular developmental robotics, and reinforcement learning (rl).Developmental robotics takes inspirations from artificial intelligence, developmental psychology and neuroscience to model cognitive processes in natural and artificial systems (Asada et al., 2009;Cangelosi &amp; Schlesinger, 2015).Following the idea that intelligence should be embodied, robots are often used to test learning models.Reinforcement learning, on the other hand, is the field interested in problems where agents learn to behave by experiencing the consequences of their actions under the form of rewards and costs.As a result, these agents are not explicitly taught, they need to learn to maximize cumulative rewards over time by trial-and-error (Sutton &amp; Barto, 2018).While developmental robotics is a field oriented towards answering particular questions around sensorimotor, cognitive and social development (e.g.how can we model language acquisition?),reinforcement learning is a field organized around a particular technical framework and set of methods.Now powered by deep learning optimization methods leveraging the computational efficiency of large computational clusters, rl algorithms have recently achieved remarkable results including, but not limited to, learning to solve video games at a super-human level (Mnih et al., 2015), to beat chess and go world players (Silver et al., 2016), or even to control stratospheric balloons in the real world (Bellemare et al., 2020).</p>
<p>Although standard rl problems often involve a single agent learning to solve a unique task, rl researchers extended rl problems to multi-goal rl problems.Instead of pursuing a single goal, agents can now be trained to pursue goal distributions (Kaelbling, 1993;Sutton et al., 2011;Schaul et al., 2015).As the field progresses, new goal representations emerge: from the specific goal states to the high-dimensional goal images or the abstract languagebased goals (Luketina et al., 2019).However, most approaches still fall short of modeling the learning abilities of natural agents because they train them to solve predefined sets of tasks, via external and hand-defined learning signals.</p>
<p>Developmental robotics directly aims to model children learning and, thus, takes inspiration from the mechanisms underlying autonomous behaviors in humans.Most of the time, humans are not motivated by external rewards but spontaneously explore their environment to discover and learn about what is around them.This behavior seems to be driven by intrinsic motivations (ims) a set of brain processes that motivate humans to explore for the mere purpose of experiencing novelty, surprise or learning progress (Berlyne, 1966;Gopnik et al., 1999;Kidd &amp; Hayden, 2015;Oudeyer &amp; Smith, 2016;Gottlieb &amp; Oudeyer, 2018).</p>
<p>The integration of ims into artificial agents thus seems to be a key step towards autonomous learning agents (Schmidhuber, 1991c;Kaplan &amp; Oudeyer, 2007).In developmental robotics, this approach enabled sample efficient learning of high-dimensional motor skills in complex robotic systems (Santucci et al., 2020), including locomotion (Baranes &amp; Oudeyer, 2013;Martius et al., 2013), soft object manipulation (Rolf &amp; Steil, 2013;Nguyen &amp; Oudeyer, 2014), visual skills (Lonini et al., 2013) and nested tool use in real-world robots (Forestier et al., 2017).Most of these approaches rely on population-based optimization algorithms, non-parametric models trained on datasets of (policy, outcome) pairs.Populationbased algorithms cannot leverage automatic differentiation on large computational clusters, often demonstrate limited generalization capabilities and cannot easily handle high-dimension perceptual spaces (e.g.images) without hand-defined input pre-processing.For these reasons, developmental robotics could benefit from new advances in deep rl.</p>
<p>Recently, we have been observing a convergence of these two fields, forming a new domain that we propose to call developmental reinforcement learning, or more broadly developmental artificial intelligence.Indeed, rl researchers now incorporate fundamental ideas from the developmental robotics literature in their own algorithms, and reversely developmental robotics learning architecture are beginning to benefit from the generalization capabilities of deep rl techniques.These convergences can mostly be categorized in two ways depending on the type of intrinsic motivation (ims) being used (Oudeyer &amp; Kaplan, 2007):</p>
<p>â€¢ Knowledge-based IMs are about prediction.They compare the situations experienced by the agent to its current knowledge and expectations, and reward it for experiencing dissonance (or resonance).This family includes ims rewarding prediction errors (Schmidhuber, 1991c;Pathak et al., 2017), novelty (Bellemare et al., 2016;Burda et al., 2019;Raileanu &amp; RocktÃ¤schel, 2020), surprise (Achiam &amp; Sastry, 2017), negative surprise (Berseth et al., 2019), learning progress (Lopes et al., 2012;Kim et al., 2020) or information gains (Houthooft et al., 2016), see a review in Linke et al. (2020).This type of ims is often used as an auxiliary reward to organize the exploration of agents in environments characterized by sparse rewards.It can also be used to facilitate the construction of world models (Lopes et al., 2012;Kim et al., 2020;Sekar et al., 2020).</p>
<p>â€¢ Competence-based IMs, on the other hand, are about control.They reward agents to solve self-generated problems, to achieve self-generated goals.In this category, agents need to represent, select and master self-generated goals.As a result, competence-based ims were often used to organize the acquisition of repertoires of skills in task-agnostic environments (Baranes &amp; Oudeyer, 2010, 2013;Santucci et al., 2016;Forestier &amp; Oudeyer, 2016;Nair et al., 2018b;Warde-Farley et al., 2019;Colas et al., 2019;Blaes et al., 2019;Pong et al., 2020;Colas et al., 2020a).Just like knowledge-based ims, competence-based ims organize the exploration of the world and, thus, might be used to train world models (Baranes &amp; Oudeyer, 2013;Chitnis et al., 2021) or facilitate learning in sparse reward settings (Colas et al., 2018).We propose to use the adjective autotelic, from the Greek auto (self) and telos (end, goal), to characterize agents that are intrinsically motivated to represent, generate, pursue and master their own goals (i.e. that are both intrinsically motivated and goalconditioned).</p>
<p>rl algorithms using knowledge-based ims leverage ideas from developmental robotics to solve standard rl problems.On the other hand, rl algorithms using competence-based ims organize exploration around self-generated goals and can be seen as targeting a developmental robotics problem: the open-ended and self-supervised acquisition of repertoires of diverse skills.</p>
<p>Intrinsically Motivated Goal Exploration Processes (imgep) is the family of autotelic algorithms that bake competence-based ims into learning agents (Forestier et al., 2017).imgep agents generate and pursue their own goals as a way to explore their environment, discover possible interactions and build repertoires of skills.This framework emerged from the field of developmental robotics (Oudeyer &amp; Kaplan, 2007;Baranes &amp; Oudeyer, 2009a, 2010;Rolf et al., 2010) and originally leveraged population-based learning algorithms (popimgep) (Baranes &amp; Oudeyer, 2009b, 2013;Forestier &amp; Oudeyer, 2016;Forestier et al., 2017).</p>
<p>Recently, goal-conditioned rl agents were also endowed with the ability to generate and pursue their own goals and learn to achieve them via self-generated rewards.We call this new set of autotelic methods rl-imgeps.In contrast, one can refer to externally-motivated goal-conditioned rl agents as rl-emgeps.This paper proposes a formalization and a review of the rl-imgep algorithms at the convergence of rl methods and developmental robotics objectives.Figure 1 proposes a visual representation of intrinsic motivations approaches (knowledge-based ims vs competencebased ims or imgeps) and goal-conditioned rl (externally vs intrinsically motivated).Their intersection is the family of autotelic algorithms that train agents to generate and pursue their own goals by training goal-conditioned policies.</p>
<p>We define goals as the combination of a compact goal representation and a goal-achievement function to measure progress.This definition highlights new challenges for autonomous learning agents.While traditional rl agents only need to learn to achieve goals, rl-imgep agents also need to learn to represent them, to generate them and to measure their own progress.After learning, the resulting goal-conditioned policy and its associated goal space form a repertoire of skills, a repertoire of behaviors that the agent can represent and control.We believe organizing past goal-conditioned rl algorithms at the convergence of developmental robotics and rl into a common classification and towards the resolution of a common problem will help organize future research.</p>
<p>Definitions</p>
<p>â€¢ Goal: "a cognitive representation of a future object that the organism is committed to approach (Elliot &amp; Fryer, 2008)."In rl, this takes the form of a (embedding, goal-achievement function) pair, see Section 2.2.</p>
<p>â€¢ Skill: the association of a goal and a policy to reach it, see Section 3.1.</p>
<p>â€¢ Goal-achievement function: a function that measures progress towards a goal (also called goal-conditioned reward function), see Section 2.2.</p>
<p>â€¢ Goal-conditioned policy: a function that generates the next action given the current state and the goal, see Section 3.</p>
<p>â€¢ Autotelic: from the Greek auto (self) and telos (end, goal), characterizes agents that generate their own goals and learning signals.In is equivalent to intrinsically motivated and goal-conditioned.</p>
<p>Scope of the survey.We are interested in algorithms from the rl-imgep family as algorithmic tools to enable agents to acquire repertoires of skills in an open-ended and self-supervised setting.Externally motivated goal-conditioned rl approaches do not enable agents to generate their own goals and thus cannot be considered autotelic (imgeps).However, these approaches can often be converted into autotelic rl-imgeps by integrating the goal generation process within the agent.For this reason, we include some rl-emgeps approaches when they present interesting mechanisms that can directly be leveraged in autotelic agents.</p>
<p>What is not covered.This survey does not discuss some related but distinct approaches such as multi-task rl (Caruana, 1997), rl with auxiliary tasks (Riedmiller et al., 2018;Jaderberg et al., 2017) and rl with knowledge-based ims (Bellemare et al., 2016;Pathak et al., 2017;Burda et al., 2019).None of these approaches do represent goals or see the agent's behavior affected by goals.The subject of intrinsically motivated goal-conditioned rl also relates to transfer learning and curriculum learning.This survey does not cover transfer learning approaches, but interested readers can refer to Taylor and Stone (2009).It discusses automatic curriculum learning approaches that organize the generation of goals according to the agent's abilities in Section 6 but, for a broader picture on the topic, readers can refer to the recent review Portelas et al. (2020a).Finally, this survey does not review policy learning methods but only focuses on goal-related mechanisms.Indeed, the choice of mechanisms to learn to represent and select goals is somewhat orthogonal to the algorithms used to learn to achieve them.Since the policy learning algorithms used in rl-imgep architectures do not differ significantly from standard rl and goal-conditioned rl approaches, this survey focuses on goal-related mechanisms, specific to rl-imgeps.</p>
<p>Survey organization.</p>
<p>We start by presenting some background on the formalization of rl and multi-goal rl problems and the corresponding algorithms to solve them (Section 2).</p>
<p>We then build on these foundations to formalize the intrinsially motivated skills acquisition problem and propose a computational framework to tackle it: rl-based intrinsically motivated goal exploration processes (Section 3).Once this is done, we organize the surveyed literature along three axes: 1) What are the different types of goal representations?(Section 4); 2) How can we learn goal representations?(Section 5) and 3) How can we prioritize goal selection?(Section 6).We finally close the survey on a discussion of open challenges for developmental reinforcement learning (Section 7).</p>
<p>Background: RL, Multi-Goal RL Problems and Their Solutions</p>
<p>This sections presents background information on the rl problem, the multi-goal rl problem and the families of algorithms used to solve them.This will serve as a foundation to define the intrinsically motivated skill acquisition problem and introduce the rl-based intrinsically motivated goal exploration process framework to solve it (rl-imgep, Section 3).</p>
<p>The Reinforcement Learning Problem</p>
<p>In a reinforcement learning (rl) problem, the agent learns to perform sequences of actions in an environment so as to maximize some notion of cumulative reward (Sutton &amp; Barto, 2018).rl problems are commonly framed as Markov Decision Processes (mdps): M = {S, A, T , Ï 0 , R} (Sutton &amp; Barto, 2018).The agent and its environment, as well as their interaction dynamics are defined by the first components {S, A, T , Ï 0 }, where s âˆˆ S describes the current state of the agent-environment interaction and Ï 0 is the distribution over initial states.The agent can interact with the environment through actions a âˆˆ A. Finally, the dynamics are characterized by the transition function T that dictates the distribution of the next state s â€² from the current state and action T (s â€² | s, a).The objective of the agent in this environment is defined by the remaining component of the mdp: R. R is the reward function, it computes a reward for any transition: R(s, a, s â€² ).Note that, in a traditional rl problem, the agent only receives the rewards corresponding to the transitions it experiences but does not have access to the function itself.The objective of the agent is to maximize the cumulative reward computed over complete episodes.When computing the aggregation of rewards, we often introduce discounting and give smaller weights to delayed rewards.
R tot t is then computed as R tot t = âˆž i=t Î³ iâˆ’t R(s iâˆ’1 , a i , s i
) with Î³ being a constant discount factor in ]0, 1].Each instance of an mdp implements an rl problem, also called a task.</p>
<p>Defining Goals for Reinforcement Learning</p>
<p>This section takes inspiration from the notion of goal in psychological research to inform the formalization of goals for reinforcement learning.</p>
<p>Goals in psychological research.Working on the origin of the notion goal and its use in past psychological research, Elliot and Fryer (2008) propose a general definition:</p>
<p>A goal is a cognitive representation of a future object that the organism is committed to approach or avoid (Elliot &amp; Fryer, 2008).</p>
<p>Because goals are cognitive representations, only animate organisms that represent goals qualify as goal-conditioned.Because this representation relates to a future object, goals are cognitive imagination of future possibilities: goal-conditioned behavior is proactive, not reactive.Finally, organisms commit to their goal, their behavior is thus influenced directly by this cognitive representation.</p>
<p>Generalized goals for reinforcement learning.rl algorithms seem to be a good fit to train such goal-conditioned agents.Indeed, rl algorithms train learning agents (organisms) to maximize (approach) a cumulative (future) reward (object).In rl, goals can be seen as a set of constraints on one or several consecutive states that the agent seeks to respect.These constraints can be very strict and characterize a single target point in the state space (e.g.image-based goals) or a specific sub-space of the state space (e.g.target x-y coordinate in a maze, target block positions in manipulation tasks).They can also be more general, when expressed by language for example (e.g.'find a red object or a wooden one').</p>
<p>To represent these goals, rl agents must be able to 1) have a compact representation of them and 2) assess their progress towards it.This is why we propose the following formalization for rl goals: each goal is a g = (z g , R g ) pair where z g is a compact goal parameterization or goal embedding and R g is a goal-achievement function measuring progress towards the goal.The set of goal-achievement function can be represented as a single goalparameterized or goal-conditioned reward function such that R G (â€¢ | z g ) = R g (â€¢).With this definition we can express a diversity of goals, see Section 4 and Table 1.</p>
<p>The goal-achievement function and the goal-conditioned policy both assign meaning to a goal.The former defines what it means to achieve the goal, it describes how the world looks like when it is achieved.The latter characterizes the process by which this goal can be achieved; what the agent needs to do to achieve it.In this search for the meaning of a goal, the goal embedding can be seen as the map: the agent follows this map and via the two functions above, experiences the meaning of the goal.</p>
<p>Generalized definition of the goal construct for RL:</p>
<p>â€¢ Goal: a g = (z g , R g ) pair where z g is a compact goal parameterization or goal embedding and R g is a goal-achievement function.</p>
<p>â€¢ Goal-achievement function: R g
(â€¢) = R G (â€¢ | z g )
where R G is a goal-conditioned reward function.</p>
<p>The Multi-Goal Reinforcement Learning Problem</p>
<p>By replacing the unique reward function R by the space of reward functions R G , rl problems can be extended to handle multiple goals: M = {S, A, T , Ï 0 , R G }.The term goal should not be mistaken for the term task, which refers to a particular mdp instance.As a result, multi-task rl refers to rl algorithms that tackle a set of mdps that can differ by any of their components (e.g.T , R, S 0 , etc.).The multi-goal rl problem can thus be seen as the particular case of the multi-task rl problem where mdps differ by their reward functions.In the standard multi-goal rl problem, the set of goals -and thus the set of reward functions -is pre-defined by engineers.The experimenter sets goals to the agent, and provides the associated reward functions.</p>
<p>Solving the RL Problem with RL Algorithms and Related Approaches</p>
<p>The rl problem can be tackled by several types of optimization methods.In this survey, we focus on rl algorithms, as they currently demonstrate stronger capacities in multi-goal problems (Florensa et al., 2018;Eysenbach et al., 2019;Warde-Farley et al., 2019;Pong et al., 2020;Lynch &amp; Sermanet, 2020;Hill et al., 2020bHill et al., , 2021;;Abramson et al., 2020;Colas et al., 2020a;Stooke et al., 2021).</p>
<p>rl algorithms use transitions collected via interactions between the agent and its environment (s, a, s â€² , R(s, a, s â€² )) to train a policy Ï€: a function generating the next action a based on the current state s so as to maximize a cumulative function of rewards.Deep rl (drl) is the extension of rl algorithms that leverage deep neural networks as function approximators to represent policies, reward and value functions.It has been powering most recent breakthrough in rl (Eysenbach et al., 2019;Warde-Farley et al., 2019;Florensa et al., 2018;Pong et al., 2020;Lynch &amp; Sermanet, 2020;Hill et al., 2020bHill et al., , 2021;;Abramson et al., 2020;Colas et al., 2020a;Stooke et al., 2021).</p>
<p>Other sets of methods can also be used to train policies.Imitation Learning (il) leverages demonstrations, i.e. transitions collected by another entity (e.g.Ho &amp; Ermon, 2016;Hester et al., 2018).Evolutionary Computing (ec) is a group of population-based approaches where populations of policies are trained to maximize cumulative rewards using episodic samples (e.g.Sehnke et al., 2010;Lehman &amp; Stanley, 2011;Wierstra et al., 2014;Mouret &amp; Clune, 2015;Salimans et al., 2017;Forestier et al., 2017;Colas et al., 2020b).Finally, in modelbased rl approaches, agents learn a model of the transition function T .Once learned, this model can be used to perform planning towards reward maximization or train a policy via rl using imagined samples (e.g.Schmidhuber (1990), Dayan et al. (1995), Nguyen-Tuong andPeters (2011), Chua et al. (2018), Charlesworth and Montana (2020), Schrittwieser et al. (2020), see two recent reviews in Hamrick et al. (2021), Moerland (2021)).</p>
<p>This surveys focuses on goal-related mechanisms that are mostly orthogonal to the choice of underlying optimization algorithm.In practice, however, most of the research in that space uses drl methods.</p>
<p>Solving the Multi-Goal RL Problem with Goal-Conditioned RL Algorithms</p>
<p>Goal-conditioned agents see their behavior affected by the goal they pursue.This is formalized via goal-conditioned policies, that is policies which produce actions based on the environment state and the agent's current goal: Î  : S Ã— Z G â†’ A, where Z G is the space of goal embeddings corresponding to the goal space G (Schaul et al., 2015).Note that ensembles of policies can also be formalized this way, via a meta-policy Î  that retrieves the particular policy from a one-hot goal embedding z g (e.g.Kaelbling, 1993;Sutton et al., 2011).</p>
<p>The idea of using a unique rl agent to target multiple goals dates back to Kaelbling (1993).Later, the horde architecture proposed to use interaction experience to update one value function per goal, effectively transferring to all goals the knowledge acquired while aiming at a particular one (Sutton et al., 2011).In these approaches, one policy is trained for each of the goals and the data collected by one can be used to train others.</p>
<p>Building on these early results, Schaul et al. (2015) introduced Universal Value Function Approximators (uvfa).They proposed to learn a unique goal-conditioned value function and goal-conditioned policy to replace the set of value functions learned in horde.Using neural networks as function approximators, they showed that uvfas enable transfer between goals and demonstrate strong generalization to new goals.</p>
<p>The idea of hindsight learning further improves knowledge transfer between goals (Kaelbling, 1993; Andrychowicz et al., 2017).Learning by hindsight, agents can reinterpret a past trajectory collected while pursuing a given goal in the light of a new goal.By asking themselves, what is the goal for which this trajectory is optimal?, they can use the originally failed trajectory as an informative trajectory to learn about another goal, thus making the most out of every trajectory (Eysenbach et al., 2020).This ability dramatically increases the sample efficiency of goal-conditioned algorithms and is arguably an important driver of the recent interest in goal-conditioned rl approaches.</p>
<p>The Intrinsically Motivated Skills Acquisition Problem and the RL-IMGEP Framework</p>
<p>This section builds on the multi-goal rl problem to formalize the intrinsically motivated skills acquisition problem, in which goals are not externally provided to the agents but must be represented and generated by them (Section 3.1).The following section discusses how to evaluate competency in such an open problem (Section 3.2).Finally, we then propose an extension of the goal-conditioned rl framework to tackle this problem: rl-based intrinsically motivated goal exploration process framework (rl-imgep, Section 3.3).</p>
<p>The Intrinsically Motivated Skills Acquisition Problem</p>
<p>In the intrinsically motivated skills acquisition problem, the agent is set in an open-ended environment without any pre-defined goal and needs to acquire a repertoire of skills.Here, a skill is defined as the association of a goal embedding z g and the policy to reach it Î  g .A repertoire of skills is thus defined as the association of a repertoire of goals G with a goal-conditioned policy trained to reach them Î  G .The intrinsically motivated skills acquisition problem can now be modeled by a reward-free mdp M = {S, A, T , Ï 0 } that only characterizes the agent, its environment and their possible interactions.Just like children, agents must be autotelic, i.e. they should learn to represent, generate, pursue and master their own goals.</p>
<p>Evaluating RL-IMGEP Agents</p>
<p>Evaluating agents is often trivial in reinforcement learning.Agents are trained to maximize one or several pre-coded reward functions -the set of possible interactions is known in advance.One can measure generalization abilities by computing the agent's success rate on a held-out set of testing goals.One can measure exploration abilities via several metrics such as the count of task-specific state visitations.</p>
<p>In contrast, autotelic agents evolve in open-ended environments and learn to represent and form their own set of skills.In this context, the space of possible behaviors might quickly become intractable for the experimenter, which is perhaps the most interesting feature of such agents.For these reasons, designing evaluation protocols is not trivial.</p>
<p>The evaluation of such systems raises similar difficulties as the evaluation of task-agnostic content generation systems like Generative Adversarial Networks (gan) (Goodfellow et al., 2014) or self-supervised language models (Devlin et al., 2019;Brown et al., 2020).In both cases, learning is task-agnostic and it is often hard to compare models in terms of their outputs (e.g.comparing the quality of gan output images, or comparing output repertoires of skills in autotelic agents).</p>
<p>One can also draw parallel with the debate on the evaluation of open-ended systems in the field of open-ended evolution (Hintze, 2019;Stanley &amp; Soros, 2016;Stanley, 2019).In both cases, a good system is expected to generate more and more original solutions such that its output cannot be predicted in advance.But what does original mean, precisely?Stanley and Soros (2016) argues that subjectivity has a role to play in the evaluation of open-ended systems.Indeed, the notion of interestingness is tightly coupled with that of open-endedness.What we expect from our open-ended systems, and of our rl-imgep agents in particular, is to generate more and more behaviors that we deem interesting.This is probably why the evaluation of content generators often include human studies.Our end objective is to generate interesting artefacts for us; we thus need to evaluate open-ended processes ourselves, subjectively.</p>
<p>Our end goal would be to interact with trained rl-imgep directly, to set themselves goals and test their abilities.The evaluation would need to adapt to the agent's capabilities.As Einstein said "If you judge a fish by its ability to climb a tree, it will live its whole life believing that it is stupid.".rl-imgep need to be evaluated by humans looking for their area of expertise, assessing the width and depth of their capacities in the world they were trained in.This said, science also requires more objective evaluation metrics to facilitate the comparison of existing methods and enable progress.Let us list some evaluation methods measuring the competency of agents via proxies:</p>
<p>â€¢ Measuring exploration: one can compute task-agnostic exploration proxies such as the entropy of the visited state distribution, or measures of state coverage (e.g.coverage of the high-level x-y state space in mazes) (Florensa et al., 2018).Exploration can also be measured as the number of interactions from a set of interesting interactions defined subjectively by the experimenter (e.g.interactions with objects in Colas et al., 2020a).</p>
<p>â€¢ Measuring generalization: The experimenter can subjectively define a set of relevant target goals and prevent the agent from training on them.Evaluating agents on this held-out set at test time provides a measure of generalization (Ruis et al., 2020), although it is biased towards what the experimenter assesses as relevant goals.</p>
<p>â€¢ Measuring transfer learning: The intrinsically motivated exploration of the environment can be seen as a pre-training phase to bootstrap learning in a subsequent downstream task.In the downstream task, the agent is trained to achieve externallydefined goals.We report its performance and learning speed on these goals.This is akin to the evaluation of self-supervised language models, where the reported metrics evaluate performance in various downstream tasks (e.g. Brown et al., 2020).In this evaluation setup, autotelic agents can be compared to task-specific agents.Ideally, autotelic agents should benefit from their open-ended learning process to outperform task-specific agents on their own tasks.This said, performance on downstream tasks remains an evaluation proxy and should not be seen as the explicit objective of the skill discovery phase.Indeed, in humans, skill discovery processes do not target any specific future task, but emerged from a natural evolutionary process maximizing reproductive success, see a discussion in Singh et al. (2010).</p>
<p>â€¢ Opening the black-box: Investigating internal representations learned during intrinsically motivated exploration is often informative.One can investigate properties of the goal generation system (e.g.does it generate out-of-distribution goals?), investigate properties of the goal embeddings (e.g. are they disentangled?).One can also look at the learning trajectories of the agents across learning, especially when they implement their own curriculum learning (e.g.Florensa et al., 2018;Colas et al., 2019;Blaes et al., 2019;Pong et al., 2020;Akakzia et al., 2021).</p>
<p>â€¢ Measuring robustness: Autonomous learning agents evolving in open-ended environment should be robust to a variety of properties than can be found in the real-world.This includes very large environments, where possible interactions might vary in terms of difficulty (trivial interactions, impossible interactions, interactions whose result is stochastic thus prevent any learning progress).Environments can also include distractors (e.g.non-controllable objects) and various forms of non-stationarity.Evaluating learning algorithms in various environments presenting each of these properties allows to assess their ability to solve the corresponding challenges.</p>
<p>RL-Based Intrinsically Motivated Goal Exploration Processes</p>
<p>Until recently, the imgep family was powered by population-based algorithms (pop-imgep).</p>
<p>The emergence of goal-conditioned rl approaches that generate their own goals gave birth to a new type of imgeps: the rl-based imgeps (rl-imgep).This section builds on traditional rl and goal-conditioned rl algorithms to give a general definition of intrinsically motivated goal-conditioned rl algorithms (rl-imgep).</p>
<p>rl-imgep are intrinsically motivated versions of goal-conditioned rl algorithms.They need to be equipped with mechanisms to represent and generate their own goals in order to solve the intrinsically motivated skills acquisition problem, see Figure 2. Concretely, this means that, in addition to the goal-conditioned policy, they need to learn: 1) to represent goals g by compact embeddings z g ; 2) to represent the support of the goal distribution, also called goal space Z G = {z g } gâˆˆG ; 3) a goal distribution from which targeted goals are sampled D(z g ); 4) a goal-conditioned reward function R G .In practice, only a few architectures tackle the four learning problems above.</p>
<p>In this survey, we call autotelic any architecture where the agent selects its own goals (learning problem 3).Simple autotelic agents assume pre-defined goal represen-tations (1), the support of the goals distribution (2) and goal-conditioned reward functions (4).As autotelic architectures tackle more of the 4 learning problems, they become more and more advanced.As we will see in the following sections, many existing works in goalconditioned rl can be formalized as autotelic agents by including goal sampling mechanisms within the definition of the agent.</p>
<p>With a developmental perspective, one can reinterpret existing work through the autotelic rl framework.Let us take an example.The agent 57 algorithm automatically selects a parameter to balance the intrinsic and extrinsic rewards of the agent at the beginning of each training episode (Badia et al., 2020a).The authors do not mention the concept of goal but instead present this mechanism as a form of reward shaping technique independent from the agent.With a developmental perspective, one can interpret the mixing parameter as a goal embedding.Replacing the sampling mechanism within the boundaries of the agent, agent 57 becomes autotelic.It is intrinsically motivated to sample and target its own goals; i.e. to define its own reward functions (here mixtures of intrinsic and extrinsic reward functions).</p>
<p>Algorithm 1 details the pseudo-code of rl-imgep algorithms.Starting from randomly initialized modules and memory, rl-imgep agents enter a standard rl interaction loop.They first observe the context (initial state), then sample a goal from their goal sampling policy.Then starts the proper interaction.Conditioned on their current goal embedding, they act in the world so as to reach their goal, i.e. to maximize the cumulative rewards generated by the goal-conditioned reward function.After the interaction, the agent can update all its internal models.It learns to represent goals by updating its goal embedding function and goal-conditioned reward function, and improves its behavior towards them by updating its goal-conditioned policy.This surveys focuses on the mechanisms specific to rl-imgep agents, i.e. mechanisms that handle the representation, generation and selection of goals.These mechanisms are mostly orthogonal to the question of how to reach the goals themselves, which often relies on existing goal-conditioned algorithms, but can also be powered by imitation learning, evolutionary algorithms or other control and planning methods.Section 4 first presents a typology of goal representations used in the literature, before Sections 5 and 6 cover existing methods to learn to represent and prioritize goals respectively.Perform Hindsight Relabelling {(s, a, s â€² , z g )} B .</p>
<p>11:</p>
<p>Compute internal rewards r = R G (s, a, s â€² | z g ).</p>
<p>12:</p>
<p>Update policy Î  G via rl on {(s, a, s â€² , z g , r)} B .</p>
<p>13:</p>
<p>Update goal representations Z G .</p>
<p>14:</p>
<p>Update goal-conditioned reward function R G .</p>
<p>15:</p>
<p>Update goal sampling policy GS.
16: return Î  G , R G , Z G</p>
<p>A Typology of Goal Representations in the Literature</p>
<p>Now that we defined the problem of interest and the overall framework to tackle it, we can start reviewing relevant approaches from the literature and how they fit in this framework.This section presents a typology of the different kinds of goal representations found in the literature.Each goal is represented by a pair: 1) a goal embedding and 2) a goalconditioned reward function.Figure 3 also provides visuals of the main environments used by the autotelic approaches presented in this paper.</p>
<p>Goals as Choices Between Multiple Objectives</p>
<p>Goals can be expressed as a list of different objectives the agent can choose from.</p>
<p>Goal embedding.In that case, goal embeddings z g are one-hot encodings of the current objective being pursued among the N objectives available.z i g is the i th one-hot vector:
z i g = (1 j=i ) j=[1..N ]
. This is the case in Oh et al. (2017), Mankowitz et al. (2018), Codevilla et al. (2018).</p>
<p>Reward function. The goal-conditioned reward function is a collection of
N distinct reward functions R G (â€¢) = R i (â€¢) if z g = z i
g .In Mankowitz et al. (2018) and Chan et al. (2019), each reward function gives a positive reward when the agent reaches the corresponding object: reaching guitars and keys in the first case, monsters and torches in the second.</p>
<p>Goals as Target Features of States</p>
<p>Goals can be expressed as target features of the state the agent desires to achieve.</p>
<p>Goal embedding.</p>
<p>In this scenario, a state representation function Ï† maps the state space to an embedding space Z = Ï†(S).Goal embeddings z g are target points in Z that the agent should reach.In manipulation tasks, z g can be target block coordinates (Andrychowicz et al., 2017;Nair et al., 2018a;Plappert et al., 2018;Colas et al., 2019;Fournier et al., 2021;Blaes et al., 2019;Lanier et al., 2019;Ding et al., 2019;Li et al., 2020).In navigation tasks, z g can be target agent positions (e.g. in mazes, Schaul et al., 2015;Florensa et al., 2018).Agent can also target image-based goals.In that case, the state representation function Ï† is usually implemented by a generative model trained on experienced imagebased states and goal embeddings can be sampled from the generative model or encoded from real images (Zhu et al., 2017;Codevilla et al., 2018;Nair et al., 2018b;Pong et al., 2020;Warde-Farley et al., 2019;Florensa et al., 2019;Venkattaramanujam et al., 2019;Lynch et al., 2020;Lynch &amp; Sermanet, 2020;Nair et al., 2020;KovaÄ et al., 2020).</p>
<p>Reward function.</p>
<p>For this type of goals, the reward function R G is based on a distance metric D. One can define a dense reward as inversely proportional to the distance between features of the current state and the target goal embedding: R g = R G (s|z g ) = âˆ’Î± Ã— D(Ï†(s), z g ) (e.g.Nair et al., 2018b).The reward can also be sparse: positive whenever that distance falls below a pre-defined threshold: R G (s|z g ) = 1 if D(Ï†(s), z g ) &lt; Ïµ, 0 otherwise.</p>
<p>Goals as Abstract Binary Problems</p>
<p>Some goals cannot be expressed as target state features but can be represented by binary problems, where each goal expresses as set of constraint on the state (or trajectory) such that these constraints are either verified or not (binary goal achievement).</p>
<p>Goal embeddings.</p>
<p>In binary problems, goal embeddings can be any expression of the set of constraints that the state should respect.Akakzia et al. (2021), Ecoffet et al. (2021) both propose a pre-defined discrete state representation.These representations lie in a finite embedding space so that goal completion can be asserted when the current embedding Ï†(s) equals the goal embedding z g .Another way to express sets of constraints is via languagebased predicates.A sentence describes the constraints expressed by the goal and the state or trajectory either verifies them, or does not (Hermann et al., 2017;Chan et al., 2019;Jiang et al., 2019;Bahdanau et al., 2019aBahdanau et al., , 2019b;;Hill et al., 2020a;Cideron et al., 2020;Colas et al., 2020a;Lynch &amp; Sermanet, 2020), see (Luketina et al., 2019) for a recent review.Language can easily characterize generic goals such as "grow any blue object" (Colas et al., 2020a), relational goals like "sort objects by size" (Jiang et al., 2019), "put the cylinder in the drawer" (Lynch &amp; Sermanet, 2020) or even sequential goals "Open the yellow door after you open a purple door" (Chevalier-Boisvert et al., 2019).When goals can be expressed by language sentences, goal embeddings z g are usually language embeddings learned jointly with either the policy or the reward function.Note that, although rl goals always express constraints on the state, we can imagine time-extended goals where constraints are expressed on the trajectory (see a discussion in Section 7.1).</p>
<p>Reward function.</p>
<p>The reward function of a binary problem can be viewed as a binary classifier that evaluates whether state s (or trajectory Ï„ ) verifies the constraints expressed by the goal semantics (positive reward) or not (null reward).This binary classification setting has directly been implemented as a way to learn language-based goal-conditioned reward functions R g (s | z g ) in Bahdanau et al. (2019a) and Colas et al. (2020a).Alternatively, the setup described in Colas et al. (2020) proposes to turn binary problems expressed by language-based goals into goals as specific target features.To this end, they train a language-conditioned goal generator that produces specific target features verifying constraints expressed by the binary problem.As a result, this setup can use a distance-based metric to evaluate the fulfillment of a binary goal.</p>
<p>Goals as a Multi-Objective Balance</p>
<p>Some goals can be expressed, not as desired regions of the state or trajectory space but as more general objectives that the agent should maximize.In that case, goals can parameterize a particular mixture of multiple objectives that the agent should maximize.</p>
<p>Goal embeddings.</p>
<p>Here, goal embeddings are simply sets of weights balancing the different objectives z g = (Î² i ) i=[1..N ] where Î² i is the weights applied to objective i and N is the number of objectives.Note that, when Î² j = 1 and Î² i = v0, âˆ€i Ì¸ = j, the agent can decide to pursue any of the objective alone.In Never Give Up, for example, rl agents are trained to maximize a mixture of extrinsic and intrinsic rewards (Badia et al., 2020b).The agent can select the mixing parameter Î² that can be viewed as a goal.Building on this approach, agent 57 adds a control of the discount factor, effectively controlling the rate at which rewards are discounted as time goes by (Badia et al., 2020a).</p>
<p>Reward function.</p>
<p>When goals are represented as a balance between multiple objectives, the associated reward function cannot be represented neither as a distance metric, nor as a binary classifier.Instead, the agent needs to maximize a convex combination of the objectives: R g
(s) = N i=1 Î² i g R i (s) where R i is the i th of N objectives and z g = Î² = Î² g i | iâˆˆ[1.
.N ] is the set of weights.</p>
<p>Goal-Conditioning</p>
<p>Now that we described the different types of goal embeddings found in the literature, remains the question of how to condition the agent's behavior -i.e. the policy -on them.Originally, the uvfa framework proposed to concatenate the goal embedding to the state representation to form the policy input.Recently, other mechanisms have emerged.When languagebased goals were introduced, Chaplot et al. ( 2018) proposed the gated-attention mechanism where the state features are linearly scaled by attention coefficients computed from the goal representation Ï†(z g ): input = s âŠ™ Ï†(z g ), where âŠ™ is the Hadamard product.Later, the Feature-wise Linear Modulation (film) approach (Perez et al., 2018) generalized this principle to affine transformations: input = s âŠ™ Ï†(z g ) + Ïˆ(z g ).Alternatively, Andreas et al.</p>
<p>(2016) came up with Neural Module Networks, a mechanism that leverages the linguistic structure of goals to derive a symbolic program that defines how states should be processed (Bahdanau et al., 2019a).</p>
<p>Conclusion</p>
<p>This section presented a diversity of goal representations, corresponding to a diversity of reward functions architectures.However, we believe this represents only a small fraction of the diversity of goal types that humans pursue.Section 7 discusses other goal representations that rl algorithms could target.</p>
<p>How to Learn Goal Representations?</p>
<p>The previous section discussed various types of goal representations.Autotelic agents actually need to learn these goal representations.While individual goals are represented by their embeddings and associated reward functions, representing multiple goals also requires the representation of the support of the goal space, i.e. how to represent the collection of valid goals that the agent can sample from, see Figure 2.This section reviews different approaches from the literature.</p>
<p>Assuming Pre-Defined Goal Representation</p>
<p>Most approaches tackle the multi-goal rl problem, where goal spaces and associated rewards are pre-defined by the engineer and are part of the task definition.Navigation and manipulation tasks, for example, pre-define goal spaces (e.g.target agent position and target block positions respectively) and use the Euclidean distance to compute rewards (Schaul et al., 2015;Andrychowicz et al., 2017;Nair et al., 2018a;Plappert et al., 2018;Florensa et al., 2018;Colas et al., 2019;Blaes et al., 2019;Lanier et al., 2019;Ding et al., 2019;Li et al., 2020).Akakzia et al. (2021), Ecoffet et al. (2021) hand-define abstract state representation and provide positive rewards when these match target goal representations.Finally, Stooke et al. (2021) hand-define a large combinatorial goal space, where goals are Boolean formulas of predicates such as being near, on, seeing, and holding, as well as their negations, with arguments taken as entities such as objects, players, and floors in procedurally-generated multi-player worlds.In all these works, goals can only be sampled from a pre-defined bounded space.This falls short of solving the intrinsically motivated skills acquisition problem.The next sub-section investigates how goal representations can be learned.</p>
<p>Learning Goal Embeddings</p>
<p>Some approaches assume the pre-existence of a goal-conditioned reward function, but learn to represent goals by learning goal embeddings.This is the case of language-based approaches, which receive rewards from the environment (thus are rl-emgep), but learn goal embeddings jointly with the policy during policy learning (Hermann et al., 2017;Chan et al., 2019;Jiang et al., 2019;Bahdanau et al., 2019b;Hill et al., 2020a;Cideron et al., Toy Envs.are used to investigate and visualise goal-as-state coverage over 2D worlds; Hard-Exploration Envs.are used to benchmark goal generation algorithms; Object Manipulation Envs.allow for the study of the diversity of learned goals as well as curriculum learning; Interactive Envs permit to represent goals using language and to model interaction with caregivers; Procedurally Generated Envs.enhance the vastness of potentially reachable goals.
Procedurally
2020; Lynch &amp; Sermanet, 2020).When goals are target images, goal embeddings can be learned via generative models of states, assuming the reward to be a fixed distance metric computed in the embedding space (Nair et al., 2018b;Florensa et al., 2019;Pong et al., 2020;Nair et al., 2020).</p>
<p>Learning the Reward Function</p>
<p>A few approaches go even further and learn their own goal-conditioned reward function.Bahdanau et al. (2019a), Colas et al. (2020a) learn language-conditioned reward functions from an expert dataset or from language descriptions of autonomous exploratory trajectories respectively.However, the agile approach from Bahdanau et al. (2019a) does not generate its own goals.</p>
<p>In the domain of image-based goals, Venkattaramanujam et al. (2019), Hartikainen et al. (2020) learn a distance metric estimating the square root of the number of steps required to move from any state s 1 to any s 2 and generates internal signals to reward agents for getting closer to their target goals.Warde-Farley et al. ( 2019) learn a similarity metric in the space of controllable aspects of the environment that is based on a mutual information objective between the state and the goal state s g .Wu et al. (2019) compute a distance metric representing the ability of the agent to reach one state from another using the Laplacian of the transition dynamics graph, where nodes are states and edges are actions.More precisely, they use the eigenvectors of the Laplacian matrix of the graph given by the states of the environment as basis to compute the L2 distance towards a goal configuration.</p>
<p>Another way to learn reward function and their associated skills is via empowerment methods (Mohamed &amp; Rezende, 2015;Gregor et al., 2016;Achiam et al., 2018;Eysenbach et al., 2019;Dai et al., 2020;Sharma et al., 2020;Choi et al., 2021).Empowerment methods aim at maximizing the mutual information between the agent's actions or goals and its experienced states.Recent methods train agents to develop a set of skills leading to maximally different areas of the state space.Agents are rewarded for experiencing states that are easy to discriminate, while a discriminator is trained to better infer the skill z g from the visited states.This discriminator acts as a skill-specific reward function.</p>
<p>All these methods set their own goals and learn their own goal-conditioned reward function.For these reasons, they can be considered as complete autotelic rl algorithms.</p>
<p>Learning the Support of the Goal Distribution</p>
<p>The previous sections reviewed several approaches to learn goal embeddings and reward functions.To represent collections of goals, one also needs to represent the support of the goal distribution -which embeddings correspond to valid goals and which do not.</p>
<p>Most approaches consider a pre-defined, bounded goal space in which any point is a valid goal (e.g.target positions within the boundaries of a maze, target block positions within the gripper's reach) (Schaul et al., 2015;Andrychowicz et al., 2017;Nair et al., 2018a;Plappert et al., 2018;Colas et al., 2019;Blaes et al., 2019;Lanier et al., 2019;Ding et al., 2019;Li et al., 2020).However, not all approaches assume pre-defined goal spaces.</p>
<p>The option framework (Sutton et al., 1999;Precup, 2000a) proposes to train a high-level policy to compose sequences of behaviors originating from learned low-level policies called options.Each option can be seen as a goal-directed policy where the goal embedding is represented by its index in the set of options.When options are policies aiming at specific states, option discovery methods learn the support of the goal space; they learn which goalstate are most useful to organize higher-level behaviors.Bottleneck states are often targeted as good sub-goals.McGovern and Barto (2001) propose to detect states that are common to multiple successful trajectories.Simsek and Barto (2004) propose to select state with maximal relative novelty, i.e. when the average novelty of following states is higher than the average novelty of previous ones.Simsek and Barto (2008) propose to leverage measures from graph theory.</p>
<p>The option-critic framework then opened the way to a wealth of new approaches (Bacon et al., 2017).Among those, methods based on successor features (Barreto et al., 2017(Barreto et al., , 2020;;Ramesh et al., 2019) propose to learn the option space using reward embeddings.With successor features, the Q-value of a goal can be expressed as a linear combination of learned reward features, efficiently decoupling the rewards from the environmental dynamics.In a multi-goal setting, these methods pair each goal with a reward embedding and use generalized policy improvement to train a set of policies that efficiently share relevant reward features across goals.These methods provide key mechanisms to learn to discover and represent sub-goals.However, they do not belong to the rl-imgep family since high-level goals are externally provided.Some approaches use the set of previously experienced representations to form the support of the goal distribution (Veeriah et al., 2018;Akakzia et al., 2021;Ecoffet et al., 2021).In Florensa et al. (2018), a Generative Adversarial Network (gan) is trained on past representations of states (Ï†(s)) to model a distribution of goals and thus its support.In the same vein, approaches handling image-based goals usually train a generative model of image states based on Variational Auto-Encoders (vae) to model goal distributions and support (Nair et al., 2018b;Pong et al., 2020;Nair et al., 2020).In both cases, valid goals are the one generated by the generative model.</p>
<p>We saw that the support of valid goals can be pre-defined, a simple set of past representations or approximated by a generative model trained on these.In all cases, the agent can only sample goals within the convex hull of previously encountered goals (in representation space).We say that goals are within training distribution.This drastically limits exploration and the discovery of new behaviors.</p>
<p>Children, on the other hand, can imagine creative goals.Pursuing these goals is thought to be the main driver of exploratory play in children (Chu &amp; Schulz, 2020).This is made possible by the compositionality of language, where sentences can easily be combined to generate new ones.The imagine algorithm leverages the creative power of language to generate such out-of-distribution goals (Colas et al., 2020a).The support of valid goals is extended to any combination of language-based goals experienced during training.They show that this mechanism augments the generalization and exploration abilities of learning agents.</p>
<p>In Section 6, we discuss how agents can learn to adapt the goal sampling distribution to maximize the learning progress of the agent.</p>
<p>Conclusion</p>
<p>This section presented how previous approaches tackled the problem of learning goal representations.While most approaches rely on pre-defined goal embeddings and/or reward functions, some approaches proposed to learn internal reward functions and goal embeddings jointly.</p>
<p>How to Prioritize Goal Selection?</p>
<p>Autotelic agents also need to select their own goals.While goals can be generated by uninformed sampling of the goal space, agents can benefit from mechanisms optimizing goal selection.In practice, this boils down to the automatic adaptation of the goal sampling distribution as a function of the agent performance.</p>
<p>Automatic Curriculum Learning for Goal Selection</p>
<p>In real-world scenarios, goal spaces can be too large for the agent to master all goals in its lifetime.Some goals might be trivial, others impossible.Some goals might be reached by chance sometimes, although the agent cannot make any progress on them.Some goals might be reachable only after the agent mastered more basic skills.For all these reasons, it is important to endow autotelic agents learning in open-ended scenarios with the ability to optimize their goal selection mechanism.This ability is a particular case of automatic curriculum learning acl applied for goal selection: mechanisms that organize goal sampling so as to maximize the long-term performance improvement (distal objective).As this objective is usually not directly differentiable, curriculum learning techniques usually rely on a proximal objective.In this section, we look at various proximal objectives used in automatic curriculum learning strategies to organize goal selection.Interested readers can refer to Portelas et al. (2020a), which present a broader review of acl methods for rl.Note that knowledge-based ims can rely on similar proxies but focus on the optimization of the experienced states instead of on the selection of goals (e.g.maximize next-state prediction errors).A recent review of knowledge-based ims approaches can be found in Linke et al. (2020).</p>
<p>Intermediate or uniform difficulty.Intermediate difficulty has been used as a proxy for long-term performance improvement, following the intuition that focusing on goals of intermediate difficulty results in short-term learning progress that will eventually turn into long-term performance increase.goalgan assigns feasibility scores to goals as the proportion of time the agents successfully reaches it (Florensa et al., 2018).Based on this data, a gan is trained to generate goals of intermediate difficulty, whose feasibility scores are contained within an intermediate range.Sukhbaatar et al. (2018) and Campero et al. (2021) train a goal policy with rl to propose challenging goals to the rl agent.The goal policy is rewarded for setting goals that are neither too easy nor impossible.In the same spirit, Stooke et al. (2021) use a mixture of three criteria to filter valid goals: 1) the agent has a low probability of scoring high; 2) the agent has a high probability of scoring higher than a control policy; 3) the control policy performs poorly.Finally, Zhang et al. (2020) select goals that maximize the disagreement in an ensemble of value functions.Value functions agree when the goals are too easy (the agent is always successful) or too hard (the agent always fails) but disagree for goals of intermediate difficulty.RacaniÃ¨re et al. (2019) propose a variant of the goalgan approach and train a goal generator to sample goals of all levels of difficulty, uniformly.This approach seems to lead to better stability and improved performance on more complex tasks compared to goalgan (Florensa et al., 2018).</p>
<p>Note that measures of intermediate difficulty are sensitive to the presence of stochasticity in the environment.Indeed, goals of intermediate difficulty can be detected as such either because the agent has not yet mastered them, or because the environment makes them impossible to achieve sometimes.In the second case, the agent should not focus on them, because it cannot learn anything new.Estimating medium-term learning progress helps overcoming this problem (see below).</p>
<p>Novelty -diversity.Warde-Farley et al. (2019), Pong et al. (2020), Pitis et al. (2020) all bias the selection of goals towards sparse areas of the goal space.For this purpose, they train density models in the goal space.While Warde-Farley et al. ( 2019), Pong et al. (2020) aim at a uniform coverage of the goal space (diversity), Pitis et al. (2020) skew the distribution of selected goals even more, effectively maximizing novelty.KovaÄ et al. (2020) proposed to enhance these methods with a goal sampling prior focusing goal selection towards controllable areas of the goal space.Finally, Fang et al. (2021) use procedural content generation (pcg) to train a task generator that produces diverse environments in which agents can explore customized skills.</p>
<p>These algorithms have strong connections with empowerment methods (Mohamed &amp; Rezende, 2015;Gregor et al., 2016;Achiam et al., 2018;Eysenbach et al., 2019;Campos et al., 2020;Sharma et al., 2020;Choi et al., 2021).Indeed, the mutual information between goals and states that empowerment methods aim to maximize can be rewritten as:
I(Z, S) = H(Z) âˆ’ H(Z | S).
Thus, maximizing empowerment can be seen as maximizing the entropy of the goal distribution while minimizing the entropy of goals given experienced states.Algorithm that both learn to sample diverse goals (H(Z) â†—) and learn to represent goals with variational auto-encoders (H(Z|S) â†˜) can be seen as maximizing empowerment.The recent wealth of empowerment methods, however, rarely discusses the link with autotelic agents: they do not mention the notion of goals or goal-conditioned reward functions and do not discuss the problem of goal representations (Gregor et al., 2016;Achiam et al., 2018;Eysenbach et al., 2019;Campos et al., 2020;Sharma et al., 2020).In a recent paper, Choi et al. (2021) investigated these links and formalized a continuum of methods from empowerment to visual goal-conditioned approaches.</p>
<p>While novelty refers to the originality of a reached outcome, diversity is a term that can only be applied to a collection of these outcomes.An outcome will be said novel if it is semantically different from what exists in the set of known outcomes.A set of outcomes will be said diverse when outcomes are far from each other and cover well the space of possible outcomes.Note that agents can also express diversity in their behavior towards a unique outcome, a skill known as versatility (Hausman et al., 2018;Kumar et al., 2020;Osa et al., 2021;Celik et al., 2021).</p>
<p>Medium-term learning progress.The idea of using learning progress (lp) as a intrinsic motivation for artificial agents dates back to the 1990s (Schmidhuber, 1991a(Schmidhuber, , 1991b;;Kaplan &amp; Oudeyer, 2004;Oudeyer et al., 2007).At that time, however, it was used as a knowledgebased ims and rewarded progress in predictions.From 2007, (Oudeyer &amp; Kaplan, 2007) suggested to use it as a competence-based ims to reward progress in competence instead.In such approaches, agents estimate their lp in different regions of the goal space and bias goal sampling towards areas of high absolute learning progress using bandit algorithms (Baranes &amp; Oudeyer, 2013;Moulin-Frier et al., 2014;Forestier &amp; Oudeyer, 2016;Fournier et al., 2018Fournier et al., , 2021;;Colas et al., 2019;Blaes et al., 2019;Portelas et al., 2020b;Akakzia et al., 2021).Such estimations attempts to disambiguate the incompetency or uncertainty the agent could resolve with more practice (epistemic) from the one it could not (aleatoric).Agents should indeed focus on goals towards which they can make progress and avoid goals that are either too easy, currently too hard, or impossible.Forestier and Oudeyer (2016), Colas et al. (2019), Blaes et al. (2019) and Akakzia et al. (2021) organize goals into modules and compute average lp measures over modules.Fournier et al. (2018) defines goals as a discrete set of precision requirements in a reaching task and computes lp for each requirement value.The use of absolute lp enables agents to focus back on goals for which performance decreases (due to perturbations or forgetting).Akakzia et al. (2021) introduces the success rate in the value optimized by the bandit: v = (1 âˆ’ sr) Ã— lp, so that agents favor goals with high absolute lp and low competence.</p>
<p>Hierarchical Reinforcement Learning for Goal Sequencing.</p>
<p>Hierarchical reinforcement learning (hrl) can be used to guide the sequencing of goals (Dayan &amp; Hinton, 1993;Sutton et al., 1998Sutton et al., , 1999;;Precup, 2000b).In hrl, a high-level policy is trained via rl or planning to generate sequence of goals for a lower level policy so as to maximize a higher-level reward.This allows to decompose tasks with long-term dependencies into simpler sub-tasks.Low-level policies are implemented by traditional goal-conditioned rl algorithms (Levy et al., 2018;RÃ¶der et al., 2020) and can be trained independently from the high-level policy (Kulkarni et al., 2016;Frans et al., 2018) or jointly (Levy et al., 2018;Nachum et al., 2018;RÃ¶der et al., 2020).In the option framework, option can be seen as goal-directed policies that the high-level policy can choose from (Sutton et al., 1999;Precup, 2000a).In that case, goal embeddings are simple indicators.Most approaches consider hand-defined spaces for the sub-goals (e.g.positions in a maze).Recent approaches propose to use the state space directly (Nachum et al., 2018) or to learn the sub-goal space (e.g.Vezhnevets et al. (2017), or with generative model of image states in Nasiriany et al. (2019)).</p>
<p>Open Challenges</p>
<p>This section discusses open challenges in the quest for autotelic agents tackling the intrinsically motivated skills acquisition problem.</p>
<p>Challenge #1: Targeting a Greater Diversity of Goals</p>
<p>Section 4 introduces a typology of goal representations found in the literature.The diversity of goal representations seems however limited, compared to the diversity of goals human target (Ram et al., 1995).Time-extended goals.All rl approaches reviewed in this paper consider time-specific goals, that is, goals whose completion can be assessed from any state s.This is due to the Markov property requirement, where the next state and reward need to be a function of the previous state only.Time-extended goals -i.e.goals whose completion can be judged by observing a sequence of states (e.g.jump twice) -can however be considered by adding time-extended features to the state (e.g. the difference between the current state and the initial state Colas et al., 2020a).To avoid such ad-hoc state representations, one could imagine using reward function architectures that incorporate forms of memory such as Recurrent Neural Network (rnn) architectures (Elman, 1993) or Transformers (Vaswani et al., 2017).Although recurrent policies are often used in the literature (Chevalier-Boisvert et al., 2019;Hill et al., 2020a;Loynd et al., 2020;Goyal et al., 2021), recurrent reward functions have not been much investigated.Some work Sutton and Tanner (2004), Schlegel et al. (2021) investigate the benefit of computing relations between value functions when learning predictive representations.Sutton and Tanner (2004) propose to represent the interrelation of predictions in a TD-network where nodes are predictions computed from states.The network allows to perform predictions that have complex temporal semantics.Schlegel et al. (2021) train a RNN architecture where hidden-states are multi-step predictions.Finally, recent work by Karch et al. (2021) show that agents can derive rewards from linguistic descriptions of time-extended behaviors.Time-extended goals include interactions that span over multiple time steps (e.g.shake the blue ball) and spatio-temporal references to objects (e.g.get the red ball that was on the left of the sofa yesterday).</p>
<p>Learning goals.Goal-driven learning is the idea that humans use learning goals, goals about their own learning abilities as a way to simplify the realization of task goals (Ram et al., 1995).Here, we refer to task goals as goals that express constraints on the physical state of the agent and/or environment.On the other hand, learning goals refer to goals that express constraints on the knowledge of the agent.Although most rl approaches target task goals, one could envision the use of learning goals for rl agents.</p>
<p>In a way, learning-progress-based learning is a form of learning goal: as the agent favors regions of the goal space to sample its task goals, it formulates the goal of learning about this specific goal region (Baranes &amp; Oudeyer, 2013;Fournier et al., 2018Fournier et al., , 2021;;Colas et al., 2019;Blaes et al., 2019;Akakzia et al., 2021).</p>
<p>Embodied Question Answering problems can also be seen as using learning goals.The agent is asked a question (i.e. a learning goal) and needs to explore the environment to answer it (acquire new knowledge) (Das et al., 2018;Yuan et al., 2019).</p>
<p>In the future, one could envision agents that set their own learning targets as sub-goals towards the resolution of harder task or learning goals, e.g.I'm going to learn about knitting so I can knit a pullover to my friend for his birthday.</p>
<p>Goals as optimization under selected constraints.We discussed the representations of goals as a balance between multiple objectives.An extension of this idea is to integrate the selection of constraints on states or trajectories.One might want to maximize a given metric (e.g.walking speed), while setting various constraints (e.g.maintaining the power consumption below a given threshold or controlling only half of the motors).The agent could explore in the space of constraints, setting constraints to itself, building a curriculum on these, etc.This is partially investigated in Colas et al. (2021), where the agent samples constraint-based goals in the optimization of control strategies to mitigate the economic and health costs in simulated epidemics.This approach, however, only considers constraints on minimal values for the objectives and requires the training of an additional Q-function per constraint.</p>
<p>Meta-diversity of goals.Finally, autotelic agents should learn to target all these goals within the same run; to transfer their skills and knowledge between different types of goals.For instance, targeting visual goals could help the agent explore the environment and solve learning goals or linguistic goals.As the density of possible goals increases, agents can organize more interesting curricula.They can select goals in easier representation spaces first (e.g.sensorimotor spaces), then move on to target more difficult goals (e.g. in the visual space), before they can target the more abstract goals (e.g.learning goals, abstract linguistic goals).</p>
<p>This can take the form of goal spaces organized hierarchically at different levels of abstractions.The exploration of such complex goal spaces has been called meta-diversity (Etcheverry et al., 2020).In the outer-loop of the meta-diversity search, one aims at learning a diverse set of outcome/goal representations.In the inner-loop, the exploration mechanism aims at generating a diversity of behaviors in each existing goal space.How to efficiently transfer knowledge and skills between these multi-modal goal spaces and how to efficiently organize goal selection in large multi-modal goal spaces remains an open question.</p>
<p>Challenge #2: Learning to Represent Diverse Goals</p>
<p>This survey mentioned only a handful of complete autotelic architectures.Indeed, most of the surveyed approach assume pre-existing goal embeddings or reward functions.Among the approaches that learn goal representations autonomously, we find that the learned representations are often restricted to very specific domains.Visual goal-conditioned approaches for example, learn reward functions and goal embeddings but restrict them to the visual space (Nair et al., 2018b(Nair et al., , 2020;;Warde-Farley et al., 2019;Venkattaramanujam et al., 2019;Pong et al., 2020;Hartikainen et al., 2020).Empowerment methods, on the other hand, develop skills that maximally cover the state space, often restricted to a few of its dimensions (e.g. the x-y space in navigation tasks Achiam et al., 2018;Eysenbach et al., 2019;Campos et al., 2020;Sharma et al., 2020).</p>
<p>These methods are limited to learn goal representations within a bounded, pre-defined space: the visual space, or the (sub-) state space.How to autonomously learn to represent the wild diversity of goals surveyed in Section 4 and discussed in Challenge #1 remains an open question.</p>
<p>Challenge #3: Imagining Creative Goals</p>
<p>Goal sampling methods surveyed in Section 6 are all bound to sample goals within the distribution of known effects.Indeed, the support of the goals distribution is either pre-defined (e.g.Schaul et al., 2015;Andrychowicz et al., 2017;Colas et al., 2019;Li et al., 2020) or learned using a generative model (Florensa et al., 2018;Nair et al., 2018bNair et al., , 2020;;Pong et al., 2020) trained on previously experienced outcomes.On the other hand, humans can imagine creative goals beyond their past experience which, arguably, powers their exploration of the world.</p>
<p>In this survey, one approach opened a path in this direction.The imagine algorithm uses linguistic goal representation learned via social supervision and leverages the compositionality of language to imagine creative goals beyond its past experience (Colas et al., 2020a).This is implemented by a simple mechanism detecting templates in known goals and recombining them to form new ones.This is in line with a recent line of work in developmental psychology arguing that human play might be about practicing to generate plans to solve imaginary problems (Chu &amp; Schulz, 2020).</p>
<p>Another way to achieve similar outcomes is to compose known goals with Boolean algebras, where new goals can be formed by composing existing atomic goals with negation, conjunction and disjunctions.The logical combinations of atomic goals was investigated in Tasse et al. (2020), Chitnis et al. (2021), andColas et al. (2020), Akakzia et al. (2021).The first approach represents the space of goals as a Boolean algebra, which allows immediate generalization to compositions of goals (and, or, not).The second approach considers using general symbolic and logic languages to express goals, but uses symbolic planning techniques that are not yet fully integrated in the goal-conditioned deep rl framework.The third and fourth train a generative model of goals conditioned on language inputs.Because it generates discrete goals, it can compose language instructions by composing the finite sets of discrete goals associated to each instruction (and is the intersection, or the union etc).However, these works fall short of exploring the richness of goal compositionality and its various potential forms.Tasse et al. (2020) seem to be limited to specific goals as target features, while Akakzia et al. (2021) requires discrete goals.Finally, Barreto et al. (2019) proposes to target new goals that are represented by linear combination of pseudo-rewards called cumulants.They use the option framework and show that an agent that masters a set of options associated with cumulants can generalize to any new behavior induced by a linear combination of those known cumulants.</p>
<p>Challenge #4: Composing Skills for Better Generalization</p>
<p>Although this survey focuses on goal-related mechanisms, autotelic agents also need to learn to achieve their goals.Progress in this direction directly relies on progress in standard rl and goal-conditioned rl.In particular, autotelic agents would considerably benefit from better generalization and skill composition.Indeed, as the set of goals agents can target grows, it becomes more and more crucial that agents can efficiently transfer knowledge between skills, infer new skills from the ones they already master and compose skills to form more complex ones.Although hierarchical rl approach learn to compose skills sequentially, concurrent skill composition remains under-explored.</p>
<p>Challenge #6: Leveraging Socio-Cultural Environments</p>
<p>Decades of research in psychology, philosophy, linguistics and robotics have demonstrated the crucial importance of rich socio-cultural environments in human development (Vygotsky, 1934;Whorf, 1956;Wood et al., 1976;Rumelhart et al., 1986;Berk, 1994;Clark, 1998;Tomasello, 1999Tomasello, , 2009;;Zlatev, 2001;Carruthers, 2002;Dautenhahn et al., 2002;Lindblom &amp; Ziemke, 2003;Mirolli &amp; Parisi, 2011;Lupyan, 2012).However, modern ai may have lost track of these insights.Deep reinforcement learning rarely considers social interactions and, when it does, models them as direct teaching; depriving agents of all autonomy.A recent discussion of this problem and an argument for the need of agents that are both autonomous and teachable can be found in a concurrent work (Sigaud, Caselles-DuprÃ©, Colas, Akakzia, Oudeyer, &amp; Chetouani, 2021).As we embed autotelic agents in richer socio-cultural worlds and let them interact with humans, they might start to learn goal representations that are meaningful for us, in our society.</p>
<p>Discussion &amp; Conclusion</p>
<p>This paper defined the intrinsically motivated skills acquisition problem and proposed to view autotelic rl algorithms or rl-imgep as computational tools to tackle it.These methods belong to the new field of developmental reinforcement learning, the intersection of the developmental robotics and rl fields.We reviewed current goal-conditioned rl approaches under the lens of autotelic agents that learn to represent and generate their own goals in addition of learning to achieve them.</p>
<p>We propose a new general definition of the goal construct: a pair of compact goal representation and an associated goal-achievement function.Interestingly, this viewpoint allowed us to categorize some rl approaches as goal-conditioned, even though the original papers did not explicitly acknowledge it.For instance, we view the Never Give Up (Badia et al., 2020b) and Agent 57 (Badia et al., 2020a) architectures as goal-conditioned, because agents actively select parameters affecting the task at hand (parameter mixing extrinsic and intrinsic objectives, discount factor) and see their behavior affected by this choice (goal-conditioned policies).</p>
<p>This point of view also offers a direction for future research.Autotelic agents need to learn to represent goals and to measure goal achievement.Future research could extend the diversity of considered goal representations, investigate novel reward function architectures and inductive biases to allow time-extended goals, goal composition and to improve generalization.</p>
<p>The general vision we convey in this paper builds on the metaphor of the learning agent as a curious scientist.A scientist that would formulate hypotheses about the world and explore it to find out whether they are true.A scientist that would ask questions, and setup intermediate goals to explore the world and find answers.A scientist that would set challenges to itself to learn about the world, to discover new ways to interact with it and to grow its collection of skills and knowledge.Such a scientist could decide of its own agenda.It would not need to be instructed and could be guided only by its curiosity, by its desire to discover new information and to master new skills.Autotelic agents should nonetheless be immersed in complex socio-cultural environment, just like humans are.In contact with humans, they could learn to represent goals that humans and society care about.</p>
<p>Approach</p>
<p>Goal Type Goal Rep.</p>
<p>Reward Function</p>
<p>Goal sampling strategy RL-IMGEPs that assume goal embeddings and reward functions (Fournier et al., 2018) Target features (+tolerance)</p>
<p>Pre-def Pre-def lp-Based hac (Levy et al., 2018) Target features Pre-def Pre-def hrl hiro (Nachum et al., 2018) Target features Pre-def Pre-def hrl CURIOUS (Colas et al., 2019) Target features Pre-def Pre-def lp-based CLIC (Fournier et al., 2021) Target features Pre-def Pre-def lp-based CWYC (Blaes et al., 2019) Target agents to sample their own goals.The proposed classification groups algorithms depending on their degree of autonomy: 1) rl-imgeps that rely on pre-defined goal representations (embeddings and reward functions); 2) rl-imgeps that rely on pre-defined reward functions but learn goal embeddings and 3) rl-imgeps that learn complete goal representations (embeddings and reward functions).For each algorithm, we report the type of goals being pursued (see Section 4), whether goal embeddings are learned (Section 5), whether reward functions are learned (Section 5.3) and how goals are sampled (Section 6).We mark in bold algorithms that use a developmental approaches and explicitly pursue the intrinsically motivated skills acquisition problem.</p>
<p>Figure 1 :
1
Figure 1: A typology of intrinsically-motivated and/or goal-conditioned rl approaches.pop-imgep, rl-imgep and rl-emgep refer to population-based intrinsically motivated goal exploration processes, rl-based imgep and rl-based externally motivated goal exploration processes respectively.pop-imgep, rl-imgep and rlemgep all represent goals, but knowledge-based ims do not.While imgeps (popimgep and rl-imgep) generate their own goals, rl-emgeps require externallydefined goals.This paper is interested in rl-imgeps, autotelic methods at the intersection of goal-conditioned rl agents and intrinsically motivated processes that train learning agents to generate and pursue their own goals with goalconditioned rl algorithms.</p>
<p>Figure 2 :
2
Figure 2: Representation of the different learning modules in a rl-imgep algorithm.In contrast, externally motivated goal exploration processes (rl-emgeps) only train the goal-conditioned policy and assume external goal generator and goal-conditioned reward function.Learning goal embeddings, goal space support and goalconditioned reward functions are all about learning to represent goals.Learning a sampling distribution is about learning to prioritize their selection.</p>
<p>Algorithm 1
1
Autotelic Agent with RL-IMGEP Require: environment E 1: Initialize empty memory M, 2: goal-conditioned policy Î  G , goal-conditioned reward R G , 3: goal space Z G , goal sampling policy GS.4: loop â–· Observe context 5: Get initial state: s 0 â† E.reset() â–· Sample goal 6:Sample goal embedding z g = GS(s 0 , Z G ).â–· Roll-out goal-conditioned policy7:Execute a roll-out withÎ  g = Î  G (â€¢ | z g ) 8:Store collected transitions Ï„ = (s, a, s â€² ) in M. â–· Update internal models 9:Sample a batch of B transitions: M âˆ¼ {(s, a, s â€² )} B .10:</p>
<p>Figure 3 :
3
Figure 3: Examples of environments in autotelic RL approaches.We organize them by dominant feature but they might share features from other catagories as well.Toy Envs.are used to investigate and visualise goal-as-state coverage over 2D worlds; Hard-Exploration Envs.are used to benchmark goal generation algorithms; Object Manipulation Envs.allow for the study of the diversity of learned goals as well as curriculum learning; Interactive Envs permit to represent goals using language and to model interaction with caregivers; Procedurally Generated Envs.enhance the vastness of potentially reachable goals.</p>
<p>IMGEPs that learn their goal embedding and assume reward functions
featuresPre-defPre-deflp-based +surprisego-explore (Ecoffet et al., 2021)Target featuresPre-defPre-defNoveltyngu (Badia et al., 2020b)ObjectivesPre-defPre-defUniformbalanceagent 57 (Badia et al., 2020a)ObjectivesPre-defPre-defMeta-learnedbalanceDECSTR (Akakzia et al., 2021)Binary problemPre-defPre-deflp-basedslide (Fang et al., 2021)Skill indexPre-defPre-defNovelty (PCG)XLand OEL (Stooke et al., 2021)Binary problemPre-defPre-defIntermediatedifficultyRL-rig (Nair et al., 2018b)Target featuresLearned (vae)Pre-defFrom vae prior(images)goalgan (Florensa et al., 2018)Target featuresPre-def + GANPre-defIntermediatedifficulty(Florensa et al., 2019)Target featuresLearned (vae)Pre-defFrom vae prior(images)skew-fit (Pong et al., 2020)Target featuresLearned (vae)Pre-defDiversity(images)setter-solver (RacaniÃ¨re et al., 2019)Target featuresLearned (Gen.Pre-defUniform difficulty(images)model)mega (Pitis et al., 2020)Target featuresLearned (vae)Pre-defNovelty(images)cc-rig (Nair et al., 2020)Target featuresLearned (vae)Pre-defFrom vae prior(images)amigo (Campero et al., 2021)Target featuresLearned (withPre-defAdversarial(images)policy)GRIMGEP (KovaÄ et al., 2020)Target featuresLearned (withPre-defDiversity and(images)policy)ALPFull RL-IMGEPsdiscern (Warde-Farley et al., 2019)Target featuresLearned (withLearned (sim-Diversity(images)policy)ilarity)diayn (Eysenbach et al., 2019)Discrete skillsLearned (withLearned (dis-Uniformpolicy)criminability)(Hartikainen et al., 2020)Target featuresLearned (withLearned (dis-Intermediate(images)policy)tance)difficulty(Venkattaramanujam et al., 2019)Target featuresLearned (withLearned (dis-Intermediate(images)policy)tance)difficultyIMAGINE (Colas et al., 2020a)Binary problemLearned (withLearnedUniform +(language)reward)Diversityvgcrl (Choi et al., 2021)Target featuresLearnedLearnedEmpowerment</p>
<p>Table 1 :
1
A classification of autotelic RL-IMGEP approaches.Autotelic approaches require</p>
<p>J Abramson, A Ahuja, A Brussee, F Carnevale, M Cassin, S Clark, A Dudzik, P Georgiev, A Guy, T Harley, F Hill, A Hung, Z Kenton, J Landon, T Lillicrap, K Mathewson, A Muldal, A Santoro, N Savinov, V Varma, G Wayne, N Wong, C Yan, R Zhu, Imitating Interactive Intelligence. 2020</p>
<p>Variational option discovery algorithms. J Achiam, H Edwards, D Amodei, P Abbeel, ArXiv -abs/1807.102992018</p>
<p>Surprise-based intrinsic motivation for deep reinforcement learning. J Achiam, S Sastry, ArXiv -abs/1703.017322017</p>
<p>DECSTR: Learning goal-directed abstract behaviors using pre-verbal spatial predicates in intrinsically motivated agents. A Akakzia, C Colas, P.-Y Oudeyer, M Chetouani, O Sigaud, Proc. of ICLR. of ICLR2021</p>
<p>Neural module networks. J Andreas, M Rohrbach, T Darrell, D Klein, 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer Society2016. June 27-30, 2016</p>
<p>Hindsight experience replay. M Andrychowicz, D Crow, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, P Abbeel, W Zaremba, Proc. of NeurIPS. of NeurIPS2017</p>
<p>Cognitive developmental robotics: A survey. M Asada, K Hosoda, Y Kuniyoshi, H Ishiguro, T Inui, Y Yoshikawa, M Ogino, C Yoshida, IEEE transactions on autonomous mental development. 112009</p>
<p>The option-critic architecture. P Bacon, J Harb, D Precup, Proc. of AAAI. of AAAI2017</p>
<p>Agent57: Outperforming the atari human benchmark. A P Badia, B Piot, S Kapturowski, P Sprechmann, A Vitvitskyi, Z D Guo, C Blundell, Proc. of ICML. of ICML2020a119</p>
<p>Never give up: Learning directed exploration strategies. A P Badia, P Sprechmann, A Vitvitskyi, D Guo, B Piot, S Kapturowski, O Tieleman, M Arjovsky, A Pritzel, A Bolt, C Blundell, Proc. of ICLR. of ICLR2020b</p>
<p>Learning to understand goal specifications by modelling reward. D Bahdanau, F Hill, J Leike, E Hughes, S A Hosseini, P Kohli, E Grefenstette, Proc. of ICLR. of ICLR2019a</p>
<p>Systematic generalization: What is required and can it be learned. D Bahdanau, S Murty, M Noukhovitch, T H Nguyen, H De Vries, A C Courville, Proc. of ICLR. of ICLR2019b</p>
<p>Proximo-distal competence based curiosity-driven exploration. A Baranes, P.-Y Oudeyer, Learning, in International Conference on Epigenetic Robotics. 2009a</p>
<p>R-iac: Robust intrinsically motivated exploration and active learning. A Baranes, P.-Y Oudeyer, IEEE Transactions on Autonomous Mental Development. 12009bIEEE</p>
<p>Intrinsically motivated goal exploration for active motor learning in robots: A case study. A Baranes, P.-Y Oudeyer, 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE2010</p>
<p>Active learning of inverse models with intrinsically motivated goal exploration in robots. A Baranes, P.-Y Oudeyer, Robotics and Autonomous Systems. 6112013</p>
<p>The option keyboard: Combining skills in reinforcement learning. A Barreto, D Borsa, S Hou, G Comanici, E AygÃ¼n, P Hamel, D Toyama, J Hunt, S Mourad, D Silver, D Precup, Proc. of NeurIPS. of NeurIPS201932</p>
<p>Successor features for transfer in reinforcement learning. A Barreto, W Dabney, R Munos, J J Hunt, T Schaul, D Silver, H Van Hasselt, Proc. of NeurIPS. of NeurIPS2017</p>
<p>Fast reinforcement learning with generalized policy updates. A Barreto, S Hou, D Borsa, D Silver, D Precup, Proceedings of the National Academy of Sciences. 117482020</p>
<p>Autonomous navigation of stratospheric balloons using reinforcement learning. M G Bellemare, S Candido, P S Castro, J Gong, M C Machado, S Moitra, S S Ponda, Z Wang, Nature. 58878362020</p>
<p>Unifying count-based exploration and intrinsic motivation. M G Bellemare, S Srinivasan, G Ostrovski, T Schaul, D Saxton, R Munos, Proc. of NeurIPS. of NeurIPS2016</p>
<p>Why Children Talk to Themselves. L E Berk, Scientific American. 27151994</p>
<p>Curiosity and exploration. D E Berlyne, Science. 15337311966</p>
<p>Smirl: Surprise minimizing rl in dynamic environments. G Berseth, D Geng, C Devin, C Finn, D Jayaraman, S Levine, ArXiv -abs/1912.055102019</p>
<p>Control what you can: Intrinsically motivated task-planning agent. S Blaes, M V Pogancic, J Zhu, G Martius, Proc. of NeurIPS. of NeurIPS2019</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Proc. of NeurIPS. of NeurIPS2020</p>
<p>Exploration by random network distillation. Y Burda, H Edwards, A J Storkey, O Klimov, Proc. of ICLR. of ICLR2019</p>
<p>Learning with amigo: Adversarially motivated intrinsic goals. A Campero, R Raileanu, H KÃ¼ttler, J B Tenenbaum, T RocktÃ¤schel, E Grefenstette, Proc. of ICLR. of ICLR2021</p>
<p>Explore, discover and learn: Unsupervised discovery of state-covering skills. V Campos, A Trott, C Xiong, R Socher, X GirÃ³-I-Nieto, J Torres, Proc. of ICML. of ICML2020119</p>
<p>Developmental Robotics: From Babies to Robots. A Cangelosi, M Schlesinger, 2015MIT press</p>
<p>Modularity, Language, and the Flexibility of Thought. P Carruthers, Behavioral and Brain Sciences. 2562002</p>
<p>Multitask learning. R Caruana, Machine learning. 2811997</p>
<p>Specializing Versatile Skill Libraries using Local Mixture of Experts. O Celik, D Zhou, G Li, P Becker, G Neumann, 5th Annual Conference on Robot Learning. 2021</p>
<p>Actrce: Augmenting experience via teacher's advice for multi-goal reinforcement learning. H Chan, Y Wu, J Kiros, S Fidler, J Ba, ArXiv -abs/1902.045462019</p>
<p>Gated-attention architectures for task-oriented language grounding. D S Chaplot, K M Sathyendra, R K Pasumarthi, D Rajagopal, R Salakhutdinov, Proc. of AAAI. of AAAI2018</p>
<p>Plangan: Model-based planning with sparse rewards and multiple goals. H Charlesworth, G Montana, Proc. of NeurIPS. of NeurIPS2020</p>
<p>BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop. M Chevalier-Boisvert, D Bahdanau, S Lahlou, L Willems, C Saharia, T H Nguyen, Y Bengio, International Conference on Learning Representations. 2019</p>
<p>Glib: Efficient exploration for relational model-based reinforcement learning via goal-literal babbling. R Chitnis, T Silver, J Tenenbaum, L P Kaelbling, T Lozano-PÃ©rez, AAAI. 2021</p>
<p>Variational Empowerment as Representation Learning for Goal-Based Reinforcement Learning. J Choi, A Sharma, H Lee, S Levine, S S Gu, ArXiv - abs/2106.014042021</p>
<p>Exploratory play, rational action, and efficient search. J Chu, L Schulz, 2020PsyArXiv</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. K Chua, R Calandra, R Mcallister, S Levine, Proc. of NeurIPS. of NeurIPS2018</p>
<p>Higher: Improving instruction following with hindsight generation for experience replay. G Cideron, M Seurin, F Strub, O Pietquin, 2020 IEEE Symposium Series on Computational Intelligence (SSCI). IEEE2020</p>
<p>Being There: Putting Brain, Body, and World Together Again. A Clark, 1998MIT press</p>
<p>End-to-end driving via conditional imitation learning. F Codevilla, M MÃ¼ller, A LÃ³pez, V Koltun, A Dosovitskiy, IEEE International Conference on Robotics and Automation. 2018. 2018IEEE</p>
<p>Languageconditioned goal generation: a new approach to language grounding for rl. C Colas, A Akakzia, P.-Y Oudeyer, M Chetouani, O Sigaud, ArXiv - abs/2006.070432020</p>
<p>Epidemioptim: A toolbox for the optimization of control policies in epidemiological models. C Colas, B Hejblum, S Rouillon, R ThiÃ©baut, P.-Y Oudeyer, C Moulin-Frier, M Prague, Journal of Artificial Intelligence Research. 712021</p>
<p>Language as a cognitive tool to imagine goals in curiosity driven exploration. C Colas, T Karch, N Lair, J Dussoux, C Moulin-Frier, P F Dominey, P Oudeyer, Proc. of NeurIPS. of NeurIPS2020a</p>
<p>Scaling map-elites to deep neuroevolution. C Colas, V Madhavan, J Huizinga, J Clune, Proc. of GECCO. of GECCO2020b</p>
<p>CURIOUS: intrinsically motivated modular multi-goal reinforcement learning. C Colas, P Oudeyer, O Sigaud, P Fournier, M Chetouani, Proc. of ICML. of ICML201997</p>
<p>GEP-PG: decoupling exploration and exploitation in deep reinforcement learning algorithms. C Colas, O Sigaud, P Oudeyer, Proc. of ICML. of ICML201880</p>
<p>An Empowerment-based Solution to Robotic Manipulation Tasks with Sparse Rewards. S Dai, W Xu, A Hofmann, B Williams, ArXiv -abs/2010.079862020</p>
<p>Embodied question answering. A Das, S Datta, G Gkioxari, S Lee, D Parikh, D Batra, 2018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USAIEEE Computer Society2018. 2018. June 18-22, 2018</p>
<p>From Embodied to Socially Embedded Agents -Implications for Interaction-Aware Robots. K Dautenhahn, B Ogden, T Quick, Cognitive Systems Research. 332002</p>
<p>Feudal reinforcement learning. P Dayan, G E Hinton, Advances in neural information processing systems. 1993</p>
<p>The Helmholtz Machine. P Dayan, G E Hinton, R M Neal, R S Zemel, Neural Computation. 751995</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proc. of NAACL-HLT. of NAACL-HLTAssociation for Computational Linguistics2019</p>
<p>Goal-conditioned imitation learning. Y Ding, C Florensa, P Abbeel, M Phielipp, Proc. of NeurIPS. of NeurIPS2019</p>
<p>First return, then explore. A Ecoffet, J Huizinga, J Lehman, K O Stanley, J Clune, Nature. 59078472021</p>
<p>The goal construct in psychology. Handbook of motivation science. A J Elliot, J W Fryer, 200818</p>
<p>Learning and development in neural networks: the importance of starting small. J L Elman, Cognition. 4811993</p>
<p>Hierarchically organized latent modules for exploratory search in morphogenetic systems. M Etcheverry, C Moulin-Frier, P Oudeyer, Proc. of NeurIPS. of NeurIPS2020</p>
<p>Rewriting history with inverse RL: hindsight inference for policy improvement. B Eysenbach, X Geng, S Levine, R R Salakhutdinov, Proc. of NeurIPS. of NeurIPS2020</p>
<p>Diversity is all you need: Learning skills without a reward function. B Eysenbach, A Gupta, J Ibarz, S Levine, Proc. of ICLR. of ICLR2019</p>
<p>Discovering Generalizable Skills via Automated Generation of Diverse Tasks. K Fang, Y Zhu, S Savarese, L Fei-Fei, Proceedings of Robotics: Science and Systems. Robotics: Science and Systems2021</p>
<p>Self-supervised learning of image embedding for continuous control. C Florensa, J Degrave, N Heess, J T Springenberg, M Riedmiller, ArXiv - abs/1901.009432019</p>
<p>Automatic goal generation for reinforcement learning agents. C Florensa, D Held, X Geng, P Abbeel, Proc. of ICML. of ICML201880</p>
<p>Modular active curiosity-driven discovery of tool use. S Forestier, P.-Y Oudeyer, Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on. IEEE2016</p>
<p>Intrinsically motivated goal exploration processes with automatic curriculum learning. S Forestier, R Portelas, Y Mollard, P.-Y Oudeyer, ArXiv -abs/1708.021902017</p>
<p>Clic: Curriculum learning and imitation for object control in nonrewarding environments. P Fournier, C Colas, M Chetouani, O Sigaud, IEEE Transactions on Cognitive and Developmental Systems. 1322021</p>
<p>Accuracy-based curriculum learning in deep reinforcement learning. P Fournier, O Sigaud, M Chetouani, P.-Y Oudeyer, ArXiv -abs/1806.096142018</p>
<p>Meta learning shared hierarchies. K Frans, J Ho, X Chen, P Abbeel, J Schulman, Proc. of ICLR. of ICLR2018</p>
<p>Generative adversarial nets. I J Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A C Courville, Y Bengio, Proc. of NeurIPS. of NeurIPS2014</p>
<p>The scientist in the crib: Minds, brains, and how children learn. A Gopnik, A N Meltzoff, P K Kuhl, 1999William Morrow &amp; Co</p>
<p>Towards a neuroscience of active sampling and curiosity. J Gottlieb, P.-Y Oudeyer, Nature Reviews Neuroscience. 19122018</p>
<p>Recurrent independent mechanisms. A Goyal, A Lamb, J Hoffmann, S Sodhani, S Levine, Y Bengio, B SchÃ¶lkopf, Proc. of ICLR. of ICLR2021</p>
<p>Variational intrinsic control. K Gregor, D J Rezende, D Wierstra, ArXiv - abs/1611.075072016</p>
<p>On the role of planning in model-based deep reinforcement learning. J B Hamrick, A L Friesen, F Behbahani, A Guez, F Viola, S Witherspoon, T Anthony, L H Buesing, P Velickovic, T Weber, Proc. of ICLR. of ICLR2021</p>
<p>Dynamical distance learning for semi-supervised and unsupervised skill discovery. K Hartikainen, X Geng, T Haarnoja, S Levine, Proc. of ICLR. of ICLR2020</p>
<p>Learning an embedding space for transferable robot skills. K Hausman, J T Springenberg, Z Wang, N Heess, M A Riedmiller, Proc. of ICLR. of ICLR2018</p>
<p>K M Hermann, F Hill, S Green, F Wang, R Faulkner, H Soyer, D Szepesvari, W M Czarnecki, M Jaderberg, D Teplyashin, M Wainwright, C Apps, D Hassabis, P Blunsom, ArXiv -abs/1706.06551Grounded Language Learning in a Simulated 3D World. 2017</p>
<p>Deep q-learning from demonstrations. T Hester, M VecerÃ­k, O Pietquin, M Lanctot, T Schaul, B Piot, D Horgan, J Quan, A Sendonaris, I Osband, G Dulac-Arnold, J P Agapiou, J Z Leibo, A Gruslys, Proc. of AAAI. of AAAI2018</p>
<p>Emergent systematic generalization in a situated agent. F Hill, A Lampinen, R Schneider, S Clark, M Botvinick, J L Mcclelland, A Santoro, Proc. of ICLR. of ICLR2020a</p>
<p>Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text. F Hill, S Mokra, N Wong, T Harley, ArXiv -abs/2005.093822020b</p>
<p>Grounded language learning fast and slow. F Hill, O Tieleman, T Von Glehn, N Wong, H Merzic, S Clark, Proc. of ICLR. of ICLR2021</p>
<p>Open-Endedness for the Sake of Open-Endedness. A Hintze, Artificial Life. 2522019</p>
<p>Generative adversarial imitation learning. J Ho, S Ermon, Proc. of NeurIPS. of NeurIPS2016</p>
<p>VIME: variational information maximizing exploration. R Houthooft, X Chen, Y Duan, J Schulman, F D Turck, P Abbeel, Proc. of NeurIPS. of NeurIPS2016</p>
<p>Reinforcement learning with unsupervised auxiliary tasks. M Jaderberg, V Mnih, W M Czarnecki, T Schaul, J Z Leibo, D Silver, K Kavukcuoglu, Proc. of ICLR. of ICLR2017</p>
<p>Language as an abstraction for hierarchical deep reinforcement learning. Y Jiang, S Gu, K Murphy, C Finn, Proc. of NeurIPS. of NeurIPS2019</p>
<p>Learning to achieve goals. L P Kaelbling, IJCAI. Citeseer1993</p>
<p>In search of the neural circuits of intrinsic motivation. F Kaplan, P.-Y Oudeyer, Frontiers in neuroscience. 1172007</p>
<p>Maximizing Learning Progress: An Internal Reward System for Development. F Kaplan, P.-Y Oudeyer, Embodied artificial intelligence. Springer2004</p>
<p>Grounding spatio-temporal language with transformers. T Karch, L Teodorescu, K Hofmann, C Moulin-Frier, P.-Y Oudeyer, Proc. of NeurIPS. of NeurIPS2021</p>
<p>C Kidd, B Y Hayden, The psychology and neuroscience of curiosity. 201588</p>
<p>Active world model learning with progress curiosity. K Kim, M Sano, J D Freitas, N Haber, D Yamins, Proc. of ICML. of ICML2020119</p>
<p>Grimgep: Learning progress for robust goal sampling in visual deep reinforcement learning. G KovaÄ, A Laversanne-Finot, P.-Y Oudeyer, ArXiv -abs/2008.043882020</p>
<p>Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. T D Kulkarni, K Narasimhan, A Saeedi, J Tenenbaum, Proc. of NeurIPS. of NeurIPS2016</p>
<p>One solution is not all you need: Few-shot extrapolation via structured maxent RL. S Kumar, A Kumar, S Levine, C Finn, Proc. of NeurIPS. of NeurIPS2020</p>
<p>Curiosity-driven multi-criteria hindsight experience replay. J B Lanier, S Mcaleer, P Baldi, ArXiv -abs/1906.037102019</p>
<p>Evolving a diversity of virtual creatures through novelty search and local competition. J Lehman, K O Stanley, Proc. of GECCO. of GECCO2011</p>
<p>Hierarchical reinforcement learning with hindsight. A Levy, R Platt, K Saenko, ArXiv -abs/1805.081802018</p>
<p>Towards practical multi-object manipulation using relational reinforcement learning. R Li, A Jabri, T Darrell, P Agrawal, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE2020</p>
<p>Social Situatedness of Natural and Artificial Intelligence: Vygotsky and Beyond. J Lindblom, T Ziemke, Adaptive Behavior. 1122003</p>
<p>Adapting behavior via intrinsic reward: a survey and empirical study. C Linke, N M Ady, M White, T Degris, A White, Journal of Artificial Intelligence Research. 692020</p>
<p>Robust active binocular vision through intrinsically motivated learning. L Lonini, S Forestier, C TeuliÃ¨re, Y Zhao, B E Shi, J Triesch, Frontiers in neurorobotics. 7202013</p>
<p>Exploration in model-based reinforcement learning by empirically estimating learning progress. M Lopes, T Lang, M Toussaint, P Oudeyer, Proc. of NeurIPS. of NeurIPS2012</p>
<p>Working memory graphs. R Loynd, R Fernandez, A Â¸elikyilmaz, A Swaminathan, M J Hausknecht, Proc. of ICML. of ICML2020119</p>
<p>A survey of reinforcement learning informed by natural language. J Luketina, N Nardelli, G Farquhar, J N Foerster, J Andreas, E Grefenstette, S Whiteson, T RocktÃ¤schel, Proc. of IJCAI. of IJCAI2019</p>
<p>What Do Words Do? Toward a Theory of Language-Augmented Thought. G Lupyan, Psychology of Learning and Motivation. 572012Elsevier</p>
<p>Learning latent plans from play. C Lynch, M Khansari, T Xiao, V Kumar, J Tompson, S Levine, P Sermanet, Proceedings of the Conference on Robot Learning. the Conference on Robot Learning2020100</p>
<p>Grounding language in play. C Lynch, P Sermanet, ArXiv -abs/2005.076482020</p>
<p>Unicorn: Continual learning with a universal, off-policy agent. D J Mankowitz, A Å½Ã­dek, A Barreto, D Horgan, M Hessel, J Quan, J Oh, H Van Hasselt, D Silver, T Schaul, ArXiv -abs/1802.082942018</p>
<p>Information driven self-organization of complex robotic behaviors. G Martius, R Der, N Ay, PloS one. 85e634002013</p>
<p>Automatic discovery of subgoals in reinforcement learning using diverse density. A Mcgovern, A G Barto, Proc. of ICML. of ICML2001</p>
<p>Towards a Vygotskyan Cognitive Robotics: The Role of Language as a Cognitive Tool. M Mirolli, D Parisi, New Ideas in Psychology. 2932011</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, nature. 51875402015</p>
<p>The Intersection of Planning and Learning. T M Moerland, 2021NetherlandsDelft University of TechnologyPh.D. thesis</p>
<p>Variational information maximisation for intrinsically motivated reinforcement learning. S Mohamed, D J Rezende, Proc. of NeurIPS. of NeurIPS2015</p>
<p>Self-organization of early vocal development in infants and machines: The role of intrinsic motivation. C Moulin-Frier, S M Nguyen, P.-Y Oudeyer, Frontiers in Psychology (Cognitive Science. 42014. 1006</p>
<p>Illuminating search spaces by mapping elites. J.-B Mouret, J Clune, ArXiv - abs/1504.049092015</p>
<p>Data-efficient hierarchical reinforcement learning. O Nachum, S Gu, H Lee, S Levine, Proc. of NeurIPS. of NeurIPS2018</p>
<p>Contextual imagined goals for self-supervised robotic learning. A Nair, S Bahl, A Khazatsky, V Pong, G Berseth, S Levine, Conference on Robot Learning. 2020</p>
<p>Overcoming exploration in reinforcement learning with demonstrations. A Nair, B Mcgrew, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE international conference on robotics and automation (ICRA). IEEE2018a</p>
<p>Visual reinforcement learning with imagined goals. A Nair, V Pong, M Dalal, S Bahl, S Lin, S Levine, Proc. of NeurIPS. of NeurIPS2018b</p>
<p>Planning with goal-conditioned policies. S Nasiriany, V Pong, S Lin, S Levine, Proc. of NeurIPS. of NeurIPS2019</p>
<p>Socially guided intrinsic motivation for robot learning of motor skills. M Nguyen, P.-Y Oudeyer, Autonomous Robots. 3632014</p>
<p>Model Learning for Robot Control: A Survey. D Nguyen-Tuong, J Peters, Cognitive processing. 1242011</p>
<p>Zero-shot task generalization with multitask deep reinforcement learning. J Oh, S P Singh, H Lee, P Kohli, Proc. of ICML. of ICML201770</p>
<p>Discovering Diverse Solutions in Deep Reinforcement Learning. T Osa, V Tangkaratt, M Sugiyama, ArXiv -abs/2103.070842021</p>
<p>Intrinsic Motivation Systems for Autonomous Mental Development. P.-Y Oudeyer, F Kaplan, V V Hafner, IEEE transactions on evolutionary computation. 1122007</p>
<p>What is intrinsic motivation? a typology of computational approaches. P.-Y Oudeyer, F Kaplan, Frontiers in neurorobotics. 200716</p>
<p>How evolution may work through curiosity-driven developmental process. P.-Y Oudeyer, L B Smith, Topics in Cognitive Science. 822016</p>
<p>Curiosity-driven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, Proc. of ICML. of ICML201770</p>
<p>Film: Visual reasoning with a general conditioning layer. E Perez, F Strub, H De Vries, V Dumoulin, A C Courville, Proc. of AAAI. of AAAI2018</p>
<p>Maximum entropy gain exploration for long horizon multi-goal reinforcement learning. S Pitis, H Chan, S Zhao, B C Stadie, J Ba, Proc. of ICML. of ICML2020119</p>
<p>Multi-goal reinforcement learning: Challenging robotics environments and request for research. M Plappert, M Andrychowicz, A Ray, B Mcgrew, B Baker, G Powell, J Schneider, J Tobin, M Chociej, P Welinder, ArXiv -abs/1802.094642018</p>
<p>Skew-fit: State-covering self-supervised reinforcement learning. V Pong, M Dalal, S Lin, A Nair, S Bahl, S Levine, Proc. of ICML. of ICML2020119</p>
<p>Automatic curriculum learning for deep RL: A short survey. R Portelas, C Colas, L Weng, K Hofmann, P Oudeyer, Proc. of IJCAI. of IJCAI2020a</p>
<p>Teacher Algorithms for Curriculum Learning of Deep Rl in Continuously Parameterized Environments. R Portelas, C Colas, K Hofmann, P.-Y Oudeyer, Proc. of CoRL. of CoRL2020b</p>
<p>Temporal Abstraction in Reinforcement Learning. D Precup, 2000aThe University of MassachussettsPhD Thesis</p>
<p>Temporal abstraction in reinforcement learning. D Precup, 2000bThe University of MassachussettsPh.D. thesis</p>
<p>Automated curricula through setter-solver interactions. S RacaniÃ¨re, A Lampinen, A Santoro, D Reichert, V Firoiu, T Lillicrap, ArXiv -abs/1909.128922019</p>
<p>RIDE: rewarding impact-driven exploration for procedurally-generated environments. R Raileanu, T RocktÃ¤schel, Proc. of ICLR. of ICLR2020</p>
<p>Goal-driven learning. A Ram, D B Leake, D Leake, 1995MIT press</p>
<p>Successor options: An option discovery framework for reinforcement learning. R Ramesh, M Tomar, B Ravindran, Proc. of IJCAI. of IJCAI2019</p>
<p>Learning by playing solving sparse reward tasks from scratch. M A Riedmiller, R Hafner, T Lampe, M Neunert, J Degrave, T V De Wiele, V Mnih, N Heess, J T Springenberg, Proc. of ICML. of ICML201880</p>
<p>Curious hierarchical actor-critic reinforcement learning. F RÃ¶der, M Eppe, P D Nguyen, S Wermter, International Conference on Artificial Neural Networks. Springer2020</p>
<p>Efficient exploratory learning of inverse kinematics on a bionic elephant trunk. M Rolf, J J Steil, IEEE transactions on neural networks and learning systems. 201325</p>
<p>Goal babbling permits direct learning of inverse kinematics. M Rolf, J J Steil, M Gienger, IEEE Transactions on Autonomous Mental Development. 232010</p>
<p>A benchmark for systematic generalization in grounded language understanding. L Ruis, J Andreas, M Baroni, D Bouchacourt, B M Lake, Proc. of NeurIPS. of NeurIPS2020</p>
<p>Sequential Thought Processes in Pdp Models. Parallel distributed processing: explorations in the microstructures of cognition. D E Rumelhart, P Smolensky, J L Mcclelland, G Hinton, 19862</p>
<p>Evolution strategies as a scalable alternative to reinforcement learning. T Salimans, J Ho, X Chen, S Sidor, I Sutskever, ArXiv -abs/1703.038642017</p>
<p>Grail: a goal-discovering robotic architecture for intrinsically-motivated learning. V G Santucci, G Baldassarre, M Mirolli, IEEE Transactions on Cognitive and Developmental Systems. 832016</p>
<p>Intrinsically motivated open-ended learning in autonomous robots. V G Santucci, P.-Y Oudeyer, A Barto, G Baldassarre, Frontiers in Neurorobotics. 202013115</p>
<p>Universal value function approximators. T Schaul, D Horgan, K Gregor, D Silver, Proc. of ICML. of ICML201537</p>
<p>General value function networks. M Schlegel, A Jacobsen, Z Abbas, A Patterson, A White, M White, Journal of Artificial Intelligence Research. 702021</p>
<p>Making the World Differentiable: On Using Self-Supervised Fully Recurrent N eu al Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments. J Schmidhuber, 1990</p>
<p>Curious model-building control systems. J Schmidhuber, Neural Networks, 1991. 1991 IEEE International Joint Conference on. IEEE1991a</p>
<p>Learning to generate sub-goals for action sequences. J Schmidhuber, Artificial neural networks. 1991b</p>
<p>A possibility for implementing curiosity and boredom in modelbuilding neural controllers. J Schmidhuber, Proc. of the international conference on simulation of adaptive behavior: From animals to animats. of the international conference on simulation of adaptive behavior: From animals to animats1991c</p>
<p>Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. J Schrittwieser, I Antonoglou, T Hubert, K Simonyan, L Sifre, S Schmitt, A Guez, E Lockhart, D Hassabis, T Graepel, T Lillicrap, D Silver, Nature. 58878392020</p>
<p>Parameter-exploring policy gradients. F Sehnke, C Osendorfer, T RÃ¼ckstieÃŸ, A Graves, J Peters, J Schmidhuber, Neural Networks. 2342010</p>
<p>Planning to explore via self-supervised world models. R Sekar, O Rybkin, K Daniilidis, P Abbeel, D Hafner, D Pathak, Proc. of ICML. of ICML2020119</p>
<p>Dynamics-aware unsupervised discovery of skills. A Sharma, S Gu, S Levine, V Kumar, K Hausman, Proc. of ICLR. of ICLR2020</p>
<p>O Sigaud, H Caselles-DuprÃ©, C Colas, A Akakzia, P.-Y Oudeyer, M Chetouani, ArXiv -abs/2105.11977Towards teachable autonomous agents. 2021</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, In nature. 5292016Nature Publishing Group</p>
<p>Using relative novelty to identify useful temporal abstractions in reinforcement learning. Ã– Simsek, A G Barto, Proc. of ICML. of ICML200469</p>
<p>Skill characterization based on betweenness. Ã– Simsek, A G Barto, Proc. of NeurIPS. of NeurIPS2008</p>
<p>Intrinsically motivated reinforcement learning: An evolutionary perspective. S Singh, R L Lewis, A G Barto, J Sorg, IEEE Transactions on Autonomous Mental Development. 222010</p>
<p>Why Open-Endedness Matters. K O Stanley, Artificial life. 2532019</p>
<p>The Role of Subjectivity in the Evaluation of Open-Endedness. K O Stanley, L Soros, Presentation delivered in OEE2: The Second Workshop on Open-Ended Evolution. ALIFE2016. 2016</p>
<p>Open-ended learning leads to generally capable agents. A Stooke, A Mahajan, C Barros, C Deck, J Bauer, J Sygnowski, M Trebacz, M Jaderberg, M Mathieu, ArXiv -abs/2107.128082021</p>
<p>Intrinsic motivation and automatic curricula via asymmetric self-play. S Sukhbaatar, Z Lin, I Kostrikov, G Synnaeve, A Szlam, R Fergus, Proc. of ICLR. of ICLR2018</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. R S Sutton, J Modayil, M Delp, T Degris, P M Pilarski, A White, D Precup, The 10th International Conference on Autonomous Agents and Multiagent Systems. 20112</p>
<p>Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning. R S Sutton, D Precup, S Singh, Artificial intelligence. 1121-21999</p>
<p>Intra-option learning about temporally abstract actions. R S Sutton, D Precup, S P Singh, ICML. 199898</p>
<p>Temporal-difference networks. R S Sutton, B Tanner, Advances in Neural Information Processing Systems. L Saul, Y Weiss, L Bottou, MIT Press200417</p>
<p>A boolean task algebra for reinforcement learning. G N Tasse, S D James, B Rosman, Proc. of NeurIPS. of NeurIPS2020</p>
<p>Transfer learning for reinforcement learning domains: A survey. M E Taylor, P Stone, Journal of Machine Learning Research. 7102009</p>
<p>The Cultural Origins of Human Cognition. M Tomasello, Constructing a Language. Harvard university press1999. 2009</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Proc. of NeurIPS. of NeurIPS2017</p>
<p>Many-goals reinforcement learning. V Veeriah, J Oh, S Singh, ArXiv - abs/1806.096052018</p>
<p>Self-supervised learning of distance functions for goal-conditioned reinforcement learning. S Venkattaramanujam, E Crawford, T Doan, D Precup, 2019</p>
<p>Feudal networks for hierarchical reinforcement learning. A S Vezhnevets, S Osindero, T Schaul, N Heess, M Jaderberg, D Silver, K Kavukcuoglu, Proc. of ICML. of ICML201770</p>
<p>Thought and Language. L S Vygotsky, 1934MIT press</p>
<p>Unsupervised control through non-parametric discriminative rewards. Warde-Farley , D De Wiele, T V Kulkarni, T D Ionescu, C Hansen, S Mnih, V , Proc. of ICLR. of ICLR2019</p>
<p>B L Whorf, Language, Thought, and Reality: Selected Writings of Benjamin Lee Whorf. MIT press1956</p>
<p>Natural evolution strategies. D Wierstra, T Schaul, T Glasmachers, Y Sun, J Peters, J Schmidhuber, The Journal of Machine Learning Research. 1512014</p>
<p>The Role of Tutoring in Problem Solving. D Wood, J S Bruner, G Ross, Journal of Child Psychology and Psychiatry. 1721976</p>
<p>The laplacian in RL: learning representations with efficient approximations. Y Wu, G Tucker, O Nachum, Proc. of ICLR. of ICLR2019</p>
<p>Interactive language learning by question answering. X Yuan, M.-A CÃ´tÃ©, J Fu, Z Lin, C Pal, Y Bengio, A Trischler, Proc. of EMNLP. of EMNLPAssociation for Computational Linguistics2019</p>
<p>Automatic curriculum learning through value disagreement. Y Zhang, P Abbeel, L Pinto, Proc. of NeurIPS. of NeurIPS2020</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, L Fei-Fei, A Farhadi, IEEE international conference on robotics and automation. 2017. 2017IEEE</p>
<p>J Zlatev, The Epigenesis of Meaning in Human Beings, and Possibly in Robots. Minds and Machines. 200111</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-346 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-346</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-346</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-265715696</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.17842v2.pdf" target="_blank">Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning</a></p>
                <p><strong>Paper Abstract:</strong> In this study, we are interested in imbuing robots with the capability of physically-grounded task planning. Recent advancements have shown that large language models (LLMs) possess extensive knowledge useful in robotic tasks, especially in reasoning and planning. However, LLMs are constrained by their lack of world grounding and dependence on external affordance models to perceive environmental information, which cannot jointly reason with LLMs. We argue that a task planner should be an inherently grounded, unified multimodal system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a novel approach for long-horizon robotic planning that leverages vision-language models (VLMs) to generate a sequence of actionable steps. ViLa directly integrates perceptual data into its reasoning and planning process, enabling a profound understanding of commonsense knowledge in the visual world, including spatial layouts and object attributes. It also supports flexible multimodal goal specification and naturally incorporates visual feedback. Our extensive evaluation, conducted in both real-robot and simulated environments, demonstrates ViLa's superiority over existing LLM-based planners, highlighting its effectiveness in a wide array of open-world manipulation tasks.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e346.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e346.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VILA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robotic Vision-Language Planning (VILA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language planning method that prompts a VLM (GPT-4V) with the current image and a high-level instruction to generate step-by-step textual skills, selecting the first step, executing a primitive, updating the image, and repeating for closed-loop embodied planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V(ision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses a pretrained multimodal foundation model (GPT-4V) that fuses image encodings with a large language model; leverages the VLM's visually grounded world knowledge and chain-of-thought style reasoning to produce natural-language action sequences (primitive skill tokens). Operates zero-shot (no in-context examples in many real-world prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Real-world long-horizon manipulation benchmark (16 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A suite of 16 tabletop, everyday manipulation tasks grouped into (i) comprehension of commonsense visual knowledge (8 tasks), (ii) multimodal goal specification (4 tasks), and (iii) visual-feedback / closed-loop tasks (4 tasks). Tasks require discovering occluded objects, rearranging, stacking, classifying object attributes in context, opening containers, and using multimodal goal images.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning; object manipulation; instruction following; household tasks</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational (joint): spatial layouts and occlusion reasoning, sequential procedural decomposition into primitive skills, and context-dependent object attribute/affordance reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training of GPT-4V on large-scale image-text corpora (internet-scale), plus zero-shot prompting with current visual observation and high-level language goal (no added embodied fine-tuning in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting with image + language inputs; iterative closed-loop prompting; chain-of-thought style visual reasoning included in VLM responses</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit multimodal knowledge encoded in model weights grounded by image embeddings at inference time; outputs natural-language action sequences (textual primitive-step plans) that serve as an explicit procedural representation; scene understanding arises from joint vision-language internal representations rather than an external symbolic map.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task success rate (per-task percent success across multiple environment variations / episodes)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Real-world commonsense tasks (8 tasks): VILA average success 80% (zero-shot across 8 tasks). Multimodal-goal tasks (Table II): Arrange Sushi 80%, Arrange Jigsaw Pieces 100%, Pick Vegetables 100%, Tidy Up Study Desk 60%. In simulation (RAVENS-derived 16 tasks) VILA substantially outperforms LLM+affordance baselines (no single numeric average reported in-text for all simulated tasks, but described as 'marked enhancements').</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Successfully infers spatial relations (detects blocking objects and sequences removal before target retrieval), deduces hidden/occluded object presence (e.g., object likely inside a closed container -> open container first), composes multi-step procedures (decomposes high-level goals into primitive skills), and disambiguates object attributes in context (e.g., keep scissors for art class vs. remove hazardous tools based on scene cues). Handles multimodal goals (goal images, pointing fingers) and uses visual feedback to replan (e.g., search other drawers if stapler not found).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Occasional 'response structure errors' (VLM outputs outside the primitive-skill template), sporadic perception misses in VLM chain-of-thought (failing to recognize some objects in images), and limits caused by assuming availability of robust primitive skills; no systematic catastrophic failures reported but some difficult occlusions/perception cases remain.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against SayCan and Grounded Decoding (LLM-based planners grounded by external affordance models): VILA 80% vs SayCan 13% and GD 20% on the 8 commonsense visual tasks; in simulation VILA outperforms Grounded Decoding which itself outperforms other LLM-only approaches and CLIPort variants.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Closed-loop (visual-feedback) VILA substantially outperforms open-loop VILA on dynamic tasks requiring replanning; removing closed-loop visual input (open-loop) degrades performance in tasks with disturbances (exact numeric ablation values not provided in text). No in-context examples were used in many real-world prompts (strict zero-shot) — using in-context examples is suggested as a means to reduce response-structure errors but was not part of primary evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Directly integrating visual observations into the language model's reasoning enables grounded commonsense spatial, procedural, and object-relational reasoning that language-only planners miss; VLMs can infer hidden objects, judge spatial constraints and object attributes in context, decompose tasks into primitive actions, and leverage visual feedback for closed-loop replanning — all in a largely zero-shot manner.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e346.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e346.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V(ision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large vision-language model used to jointly reason over images and text, producing chain-of-thought style visual reasoning and natural-language action sequences for embodied planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pretrained multimodal foundation model that accepts images and text and performs integrated visual-language reasoning; trained on large-scale image-text data (details external to this paper); used here without additional task-specific finetuning and prompted zero-shot to generate step-by-step plans and to act as a success detector.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used as planner for the paper's real-world and simulated manipulation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same 16-task real-world benchmark and 16 simulated RAVENS-based tasks described in the VILA method; GPT-4V is the backbone VLM producing plans and visual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning; object manipulation; instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large-scale image-text corpora (internet-scale); no additional grounded finetuning used in presented experiments</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot image+text prompting; chain-of-thought style visual reasoning; iterative prompting for closed-loop behavior</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Visually grounded implicit representations in model weights and multimodal embeddings; explicit output as natural-language sequences of primitive actions (procedural plans); internal scene understanding used to judge affordances and success conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task success rate on real and simulated tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When used within VILA, GPT-4V achieves the reported VILA success rates (e.g., 80% average on 8 commonsense visual tasks; multimodal goal tasks per Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Effectively leverages image inputs to represent spatial layouts and object attributes and to produce feasible procedural plans in zero-shot contexts; interprets goal images and pointing cues to ground tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Sometimes produces outputs incompatible with predefined primitive-skill templates (response structure errors) and occasionally misses objects in its chain-of-thought perception reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>GPT-4V (in VILA) outperforms LLM-only planners (Llama 2, GPT-4 without vision) augmented with affordance detectors in the evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>The paper notes benefit from closed-loop iterative prompting with updated images; no parameter ablations of GPT-4V itself were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A multimodal VLM like GPT-4V encodes visually grounded commonsense and can directly utilize image input to improve spatial, procedural, and object-relational planning for embodied agents, reducing failures that stem from purely language-based grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e346.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e346.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based planners (SayCan / Grounded Decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-only planners grounded by external affordance models (e.g., SayCan, Grounded Decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that use large language models (LLMs) to generate plans but rely on separate affordance/perception modules to ground language to the environment, operating effectively without direct integrated image input to the language model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 (70B) / GPT-4 in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained large language models encoding broad world knowledge as implicit weights; in these planners the LLM generates candidate actions while external affordance models (detectors or value functions) score/ground those actions. In the experiments Llama 2 70B was used as a proxy where model token probabilities were required; GPT-4 was also evaluated as an LLM-only planner in simulation comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same real-world manipulation tasks and simulated RAVENS rearrangement tasks (used as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>LLMs were used to produce plans for tabletop manipulation tasks; because LLMs do not accept raw images in these baselines, perceptual grounding is provided by separate affordance modules (OWL-ViT, CLIPort logits or groundtruth affordances in sim).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning; object manipulation; instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>primarily procedural + object-relational, with limited spatial reasoning when reliant only on language; overall described as 'language-only' grounding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora and any in-context examples (some baselines used few-shot prompts or training data, e.g., CLIPort variants), plus separate perception/affordance modules providing visual signals</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot or few-shot prompting of the LLM; combination with external affordance models (e.g., SayCan uses value functions, Grounded Decoding uses detector logits) to select grounded actions</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit semantic/world knowledge encoded in weights; planning output as natural-language action sequences; perceptual info represented separately as affordance scores or detection outputs rather than integrated multimodal internal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>On the 8 commonsense visual tasks: SayCan achieved 13% average success, Grounded Decoding achieved 20% average success; on particularly spatially and attribute-demanding tasks (e.g., Take Out Marvel Model, contextual classification tasks), their success rates were close to zero.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Can name and detect objects (when afforded by perception module) and produce plausible high-level language plans in text; in some cases can sequence reasonable actions when affordance scores are informative.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Fails to capture fine-grained spatial relationships (occlusion, object blocking), misjudges context-dependent object attributes (e.g., whether scissors are allowed), and cannot jointly reason with scene perception leading to infeasible or unsafe plans (e.g., attempting to pick up an occupied plate). Dominant error type labeled 'understanding error' in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly to VILA: 13% (SayCan) and 20% (GD) vs VILA 80% on the 8 commonsense tasks; perceptual errors from separate detectors further contributed to failures.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No explicit ablation of the LLM-only architecture reported here, but analysis highlights that the unidirectional separation (perception -> LLM) is the core limitation; using more informative affordance signals helps (GD > naive LLM-only) but still underperforms integrated VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language-only planners, even with external affordance models, suffer when they cannot jointly reason with raw visual input: the unidirectional, independent affordance modules fail to convey task-dependent, relational spatial information and contextual object attributes needed for feasible embodied plans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e346.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e346.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OWL-ViT / CLIPort affordances</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-vocabulary detector (OWL-ViT) and CLIPort-derived affordance signals (affordance models used as external perceptual grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Perception/affordance modules used to provide grounding signals to language-only planners; OWL-ViT is an open-vocabulary detector used in real-world baselines, and CLIPort logits / simulation groundtruth affordances were used in simulated baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OWL-ViT (detector) / CLIPort (policy logits used as affordance proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Separate perceptual modules that output detection labels, bounding boxes or affordance logits and provide numerical scores to guide LLM-based planners; they operate as stand-alone vision modules, not jointly integrated into the language model's reasoning loop.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used to ground LLM-based planners (SayCan, Grounded Decoding) on the real-world and simulated manipulation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide object presence and/or affordance scores (e.g., probability that executing primitive skill will succeed) which LLM-based planners use to choose among candidate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation; multi-step planning (as perception grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (object identity and some attributes) and limited spatial (detection/localization), but not rich relational spatial layouts or contextual affordances</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>vision-only pretraining/fine-tuning of detection models (external to LLMs); simulation groundtruth affordances where available</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>detection and affordance scoring; outputs fed as inputs to LLMs or used in grounded decoding scoring</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Explicit perceptual outputs such as detection labels, bounding boxes, and scalar affordance scores; not represented as integrated multimodal embeddings inside the planner LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>contribution to overall planner success; perception error counted as failure mode</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Baselines using OWL-ViT and CLIPort-derived affordances showed substantial perception errors contributing to overall low performance; specific failure counts labeled 'perception error' in the analysis (dominant error type for baselines). Exact numeric error breakdown per detector not tabulated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Can detect visible objects and provide affordance cues when objects are directly visible and detection generalizes to the domain.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Fails when target objects are occluded/inside containers (predicting zero affordance), cannot convey task-dependent contextual attributes, and cannot jointly reason about scene geometry and goal-directed constraints — leading to infeasible plans from the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used in SayCan and Grounded Decoding baselines; these pipelines underperformed relative to VILA which removes the separate affordance module and instead queries a VLM directly.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No explicit ablation of detectors presented, but paper emphasizes their limitations and that replacing them with an integrated VLM (GPT-4V) yields large improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Standalone affordance detectors provide unidirectional, limited perceptual signals that lack task-dependent relational and contextual information; this is a core reason LLM+affordance pipelines fail on spatially and attribute-complex tasks compared to integrated vision-language reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e346.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e346.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Closed-loop VILA (visual feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Closed-loop Vision-Language Planning with visual feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The iterative variant of VILA which re-prompts the VLM with updated visual observations after each primitive execution to detect success/failure and replan if necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V(ision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same VLM backbone as VILA but used in an explicit closed-loop fashion: select first plan step, execute primitive, capture new image x_{t+1}, include executed step in prompt context and re-query the VLM until 'done'.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Dynamic/feedback-critical tasks (e.g., Stack Blocks with noise, Pack Chip Bags with human interference, Find Stapler with variable drawer locations, Human-Robot Interaction waiting for hand)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require continuous replanning or success-detection from visual input due to stochastic primitive execution, human intervention, or variable object placement.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning; closed-loop manipulation; success detection and replanning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+spatial+object-relational with temporal feedback (procedural sequences updated by visual feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>VLM pretraining + on-line visual observations at each step (no retraining); uses the VLM as both planner and success detector</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>iterative zero-shot prompting with updated images; VLM reasons about images to detect success and plan next steps</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Procedural plans as natural-language steps updated iteratively; scene state implicitly encoded in VLM's multimodal internal representation derived from the current image input.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task success rate under disturbances / replanning scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Closed-loop VILA substantially outperforms open-loop VILA on dynamic tasks (exact numeric deltas not provided in-text), and succeeds in example scenarios such as continuing to search other drawers when stapler not found in top drawer.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Recovers from external disturbances (human removing placed items), retries failed primitive executions due to injected noise (stacking with noisy controller), and adapts search strategies based on visual feedback (checking alternative drawers).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Open-loop variant (no visual feedback) struggles; closed-loop still limited by perception misses or malformed plan outputs, and by the assumption that primitive skills themselves are robust.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared internally to an open-loop VILA variant; closed-loop variant shows clear advantage in dynamic/feedback tasks. Baselines (SayCan, GD) lacked natural incorporation of raw visual feedback into LLM reasoning and thus performed worse in these settings.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Comparing open-loop vs closed-loop shows visual feedback is a critical component; removing feedback (open-loop) degrades performance significantly on dynamic tasks (no precise numeric ablation values reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Direct visual feedback integrated into the language model's reasoning loop enables robust success detection and online replanning, which is essential for embodied tasks with stochasticity or human interaction; this contrasts with language-only planners that would need expensive visual-to-text translation to approximate feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do as I can, not as I say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>Grounded Decoding: Guiding text generation with grounded models for robot control <em>(Rating: 2)</em></li>
                <li>Language models are zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>PaLM-E: An embodied multimodal language model for robotic control <em>(Rating: 1)</em></li>
                <li>RT-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 1)</em></li>
                <li>Vision-language models as success detectors <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-346",
    "paper_id": "paper-265715696",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "VILA",
            "name_full": "Robotic Vision-Language Planning (VILA)",
            "brief_description": "A vision-language planning method that prompts a VLM (GPT-4V) with the current image and a high-level instruction to generate step-by-step textual skills, selecting the first step, executing a primitive, updating the image, and repeating for closed-loop embodied planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4V(ision)",
            "model_size": null,
            "model_description": "Uses a pretrained multimodal foundation model (GPT-4V) that fuses image encodings with a large language model; leverages the VLM's visually grounded world knowledge and chain-of-thought style reasoning to produce natural-language action sequences (primitive skill tokens). Operates zero-shot (no in-context examples in many real-world prompts).",
            "task_name": "Real-world long-horizon manipulation benchmark (16 tasks)",
            "task_description": "A suite of 16 tabletop, everyday manipulation tasks grouped into (i) comprehension of commonsense visual knowledge (8 tasks), (ii) multimodal goal specification (4 tasks), and (iii) visual-feedback / closed-loop tasks (4 tasks). Tasks require discovering occluded objects, rearranging, stacking, classifying object attributes in context, opening containers, and using multimodal goal images.",
            "task_type": "multi-step planning; object manipulation; instruction following; household tasks",
            "knowledge_type": "spatial+procedural+object-relational (joint): spatial layouts and occlusion reasoning, sequential procedural decomposition into primitive skills, and context-dependent object attribute/affordance reasoning",
            "knowledge_source": "pre-training of GPT-4V on large-scale image-text corpora (internet-scale), plus zero-shot prompting with current visual observation and high-level language goal (no added embodied fine-tuning in experiments)",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot prompting with image + language inputs; iterative closed-loop prompting; chain-of-thought style visual reasoning included in VLM responses",
            "knowledge_representation": "Implicit multimodal knowledge encoded in model weights grounded by image embeddings at inference time; outputs natural-language action sequences (textual primitive-step plans) that serve as an explicit procedural representation; scene understanding arises from joint vision-language internal representations rather than an external symbolic map.",
            "performance_metric": "task success rate (per-task percent success across multiple environment variations / episodes)",
            "performance_result": "Real-world commonsense tasks (8 tasks): VILA average success 80% (zero-shot across 8 tasks). Multimodal-goal tasks (Table II): Arrange Sushi 80%, Arrange Jigsaw Pieces 100%, Pick Vegetables 100%, Tidy Up Study Desk 60%. In simulation (RAVENS-derived 16 tasks) VILA substantially outperforms LLM+affordance baselines (no single numeric average reported in-text for all simulated tasks, but described as 'marked enhancements').",
            "success_patterns": "Successfully infers spatial relations (detects blocking objects and sequences removal before target retrieval), deduces hidden/occluded object presence (e.g., object likely inside a closed container -&gt; open container first), composes multi-step procedures (decomposes high-level goals into primitive skills), and disambiguates object attributes in context (e.g., keep scissors for art class vs. remove hazardous tools based on scene cues). Handles multimodal goals (goal images, pointing fingers) and uses visual feedback to replan (e.g., search other drawers if stapler not found).",
            "failure_patterns": "Occasional 'response structure errors' (VLM outputs outside the primitive-skill template), sporadic perception misses in VLM chain-of-thought (failing to recognize some objects in images), and limits caused by assuming availability of robust primitive skills; no systematic catastrophic failures reported but some difficult occlusions/perception cases remain.",
            "baseline_comparison": "Compared against SayCan and Grounded Decoding (LLM-based planners grounded by external affordance models): VILA 80% vs SayCan 13% and GD 20% on the 8 commonsense visual tasks; in simulation VILA outperforms Grounded Decoding which itself outperforms other LLM-only approaches and CLIPort variants.",
            "ablation_results": "Closed-loop (visual-feedback) VILA substantially outperforms open-loop VILA on dynamic tasks requiring replanning; removing closed-loop visual input (open-loop) degrades performance in tasks with disturbances (exact numeric ablation values not provided in text). No in-context examples were used in many real-world prompts (strict zero-shot) — using in-context examples is suggested as a means to reduce response-structure errors but was not part of primary evaluation.",
            "key_findings": "Directly integrating visual observations into the language model's reasoning enables grounded commonsense spatial, procedural, and object-relational reasoning that language-only planners miss; VLMs can infer hidden objects, judge spatial constraints and object attributes in context, decompose tasks into primitive actions, and leverage visual feedback for closed-loop replanning — all in a largely zero-shot manner.",
            "uuid": "e346.0",
            "source_info": {
                "paper_title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V(ision)",
            "brief_description": "A large vision-language model used to jointly reason over images and text, producing chain-of-thought style visual reasoning and natural-language action sequences for embodied planning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4V",
            "model_size": null,
            "model_description": "A pretrained multimodal foundation model that accepts images and text and performs integrated visual-language reasoning; trained on large-scale image-text data (details external to this paper); used here without additional task-specific finetuning and prompted zero-shot to generate step-by-step plans and to act as a success detector.",
            "task_name": "Used as planner for the paper's real-world and simulated manipulation tasks",
            "task_description": "Same 16-task real-world benchmark and 16 simulated RAVENS-based tasks described in the VILA method; GPT-4V is the backbone VLM producing plans and visual reasoning.",
            "task_type": "multi-step planning; object manipulation; instruction following",
            "knowledge_type": "spatial+procedural+object-relational (multimodal)",
            "knowledge_source": "pre-training on large-scale image-text corpora (internet-scale); no additional grounded finetuning used in presented experiments",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot image+text prompting; chain-of-thought style visual reasoning; iterative prompting for closed-loop behavior",
            "knowledge_representation": "Visually grounded implicit representations in model weights and multimodal embeddings; explicit output as natural-language sequences of primitive actions (procedural plans); internal scene understanding used to judge affordances and success conditions.",
            "performance_metric": "task success rate on real and simulated tasks",
            "performance_result": "When used within VILA, GPT-4V achieves the reported VILA success rates (e.g., 80% average on 8 commonsense visual tasks; multimodal goal tasks per Table II).",
            "success_patterns": "Effectively leverages image inputs to represent spatial layouts and object attributes and to produce feasible procedural plans in zero-shot contexts; interprets goal images and pointing cues to ground tasks.",
            "failure_patterns": "Sometimes produces outputs incompatible with predefined primitive-skill templates (response structure errors) and occasionally misses objects in its chain-of-thought perception reasoning.",
            "baseline_comparison": "GPT-4V (in VILA) outperforms LLM-only planners (Llama 2, GPT-4 without vision) augmented with affordance detectors in the evaluated tasks.",
            "ablation_results": "The paper notes benefit from closed-loop iterative prompting with updated images; no parameter ablations of GPT-4V itself were reported.",
            "key_findings": "A multimodal VLM like GPT-4V encodes visually grounded commonsense and can directly utilize image input to improve spatial, procedural, and object-relational planning for embodied agents, reducing failures that stem from purely language-based grounding.",
            "uuid": "e346.1",
            "source_info": {
                "paper_title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLM-based planners (SayCan / Grounded Decoding)",
            "name_full": "Language-only planners grounded by external affordance models (e.g., SayCan, Grounded Decoding)",
            "brief_description": "Approaches that use large language models (LLMs) to generate plans but rely on separate affordance/perception modules to ground language to the environment, operating effectively without direct integrated image input to the language model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 2 (70B) / GPT-4 in comparisons",
            "model_size": "70B",
            "model_description": "Pretrained large language models encoding broad world knowledge as implicit weights; in these planners the LLM generates candidate actions while external affordance models (detectors or value functions) score/ground those actions. In the experiments Llama 2 70B was used as a proxy where model token probabilities were required; GPT-4 was also evaluated as an LLM-only planner in simulation comparisons.",
            "task_name": "Same real-world manipulation tasks and simulated RAVENS rearrangement tasks (used as baselines)",
            "task_description": "LLMs were used to produce plans for tabletop manipulation tasks; because LLMs do not accept raw images in these baselines, perceptual grounding is provided by separate affordance modules (OWL-ViT, CLIPort logits or groundtruth affordances in sim).",
            "task_type": "multi-step planning; object manipulation; instruction following",
            "knowledge_type": "primarily procedural + object-relational, with limited spatial reasoning when reliant only on language; overall described as 'language-only' grounding",
            "knowledge_source": "pre-training on text corpora and any in-context examples (some baselines used few-shot prompts or training data, e.g., CLIPort variants), plus separate perception/affordance modules providing visual signals",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot or few-shot prompting of the LLM; combination with external affordance models (e.g., SayCan uses value functions, Grounded Decoding uses detector logits) to select grounded actions",
            "knowledge_representation": "Implicit semantic/world knowledge encoded in weights; planning output as natural-language action sequences; perceptual info represented separately as affordance scores or detection outputs rather than integrated multimodal internal representations.",
            "performance_metric": "task success rate",
            "performance_result": "On the 8 commonsense visual tasks: SayCan achieved 13% average success, Grounded Decoding achieved 20% average success; on particularly spatially and attribute-demanding tasks (e.g., Take Out Marvel Model, contextual classification tasks), their success rates were close to zero.",
            "success_patterns": "Can name and detect objects (when afforded by perception module) and produce plausible high-level language plans in text; in some cases can sequence reasonable actions when affordance scores are informative.",
            "failure_patterns": "Fails to capture fine-grained spatial relationships (occlusion, object blocking), misjudges context-dependent object attributes (e.g., whether scissors are allowed), and cannot jointly reason with scene perception leading to infeasible or unsafe plans (e.g., attempting to pick up an occupied plate). Dominant error type labeled 'understanding error' in experiments.",
            "baseline_comparison": "Compared directly to VILA: 13% (SayCan) and 20% (GD) vs VILA 80% on the 8 commonsense tasks; perceptual errors from separate detectors further contributed to failures.",
            "ablation_results": "No explicit ablation of the LLM-only architecture reported here, but analysis highlights that the unidirectional separation (perception -&gt; LLM) is the core limitation; using more informative affordance signals helps (GD &gt; naive LLM-only) but still underperforms integrated VLM.",
            "key_findings": "Language-only planners, even with external affordance models, suffer when they cannot jointly reason with raw visual input: the unidirectional, independent affordance modules fail to convey task-dependent, relational spatial information and contextual object attributes needed for feasible embodied plans.",
            "uuid": "e346.2",
            "source_info": {
                "paper_title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "OWL-ViT / CLIPort affordances",
            "name_full": "Open-vocabulary detector (OWL-ViT) and CLIPort-derived affordance signals (affordance models used as external perceptual grounding)",
            "brief_description": "Perception/affordance modules used to provide grounding signals to language-only planners; OWL-ViT is an open-vocabulary detector used in real-world baselines, and CLIPort logits / simulation groundtruth affordances were used in simulated baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OWL-ViT (detector) / CLIPort (policy logits used as affordance proxy)",
            "model_size": null,
            "model_description": "Separate perceptual modules that output detection labels, bounding boxes or affordance logits and provide numerical scores to guide LLM-based planners; they operate as stand-alone vision modules, not jointly integrated into the language model's reasoning loop.",
            "task_name": "Used to ground LLM-based planners (SayCan, Grounded Decoding) on the real-world and simulated manipulation tasks",
            "task_description": "Provide object presence and/or affordance scores (e.g., probability that executing primitive skill will succeed) which LLM-based planners use to choose among candidate steps.",
            "task_type": "object manipulation; multi-step planning (as perception grounding)",
            "knowledge_type": "object-relational (object identity and some attributes) and limited spatial (detection/localization), but not rich relational spatial layouts or contextual affordances",
            "knowledge_source": "vision-only pretraining/fine-tuning of detection models (external to LLMs); simulation groundtruth affordances where available",
            "has_direct_sensory_input": true,
            "elicitation_method": "detection and affordance scoring; outputs fed as inputs to LLMs or used in grounded decoding scoring",
            "knowledge_representation": "Explicit perceptual outputs such as detection labels, bounding boxes, and scalar affordance scores; not represented as integrated multimodal embeddings inside the planner LLM.",
            "performance_metric": "contribution to overall planner success; perception error counted as failure mode",
            "performance_result": "Baselines using OWL-ViT and CLIPort-derived affordances showed substantial perception errors contributing to overall low performance; specific failure counts labeled 'perception error' in the analysis (dominant error type for baselines). Exact numeric error breakdown per detector not tabulated in main text.",
            "success_patterns": "Can detect visible objects and provide affordance cues when objects are directly visible and detection generalizes to the domain.",
            "failure_patterns": "Fails when target objects are occluded/inside containers (predicting zero affordance), cannot convey task-dependent contextual attributes, and cannot jointly reason about scene geometry and goal-directed constraints — leading to infeasible plans from the LLM.",
            "baseline_comparison": "Used in SayCan and Grounded Decoding baselines; these pipelines underperformed relative to VILA which removes the separate affordance module and instead queries a VLM directly.",
            "ablation_results": "No explicit ablation of detectors presented, but paper emphasizes their limitations and that replacing them with an integrated VLM (GPT-4V) yields large improvements.",
            "key_findings": "Standalone affordance detectors provide unidirectional, limited perceptual signals that lack task-dependent relational and contextual information; this is a core reason LLM+affordance pipelines fail on spatially and attribute-complex tasks compared to integrated vision-language reasoning.",
            "uuid": "e346.3",
            "source_info": {
                "paper_title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Closed-loop VILA (visual feedback)",
            "name_full": "Closed-loop Vision-Language Planning with visual feedback",
            "brief_description": "The iterative variant of VILA which re-prompts the VLM with updated visual observations after each primitive execution to detect success/failure and replan if necessary.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4V(ision)",
            "model_size": null,
            "model_description": "Same VLM backbone as VILA but used in an explicit closed-loop fashion: select first plan step, execute primitive, capture new image x_{t+1}, include executed step in prompt context and re-query the VLM until 'done'.",
            "task_name": "Dynamic/feedback-critical tasks (e.g., Stack Blocks with noise, Pack Chip Bags with human interference, Find Stapler with variable drawer locations, Human-Robot Interaction waiting for hand)",
            "task_description": "Tasks that require continuous replanning or success-detection from visual input due to stochastic primitive execution, human intervention, or variable object placement.",
            "task_type": "multi-step planning; closed-loop manipulation; success detection and replanning",
            "knowledge_type": "procedural+spatial+object-relational with temporal feedback (procedural sequences updated by visual feedback)",
            "knowledge_source": "VLM pretraining + on-line visual observations at each step (no retraining); uses the VLM as both planner and success detector",
            "has_direct_sensory_input": true,
            "elicitation_method": "iterative zero-shot prompting with updated images; VLM reasons about images to detect success and plan next steps",
            "knowledge_representation": "Procedural plans as natural-language steps updated iteratively; scene state implicitly encoded in VLM's multimodal internal representation derived from the current image input.",
            "performance_metric": "task success rate under disturbances / replanning scenarios",
            "performance_result": "Closed-loop VILA substantially outperforms open-loop VILA on dynamic tasks (exact numeric deltas not provided in-text), and succeeds in example scenarios such as continuing to search other drawers when stapler not found in top drawer.",
            "success_patterns": "Recovers from external disturbances (human removing placed items), retries failed primitive executions due to injected noise (stacking with noisy controller), and adapts search strategies based on visual feedback (checking alternative drawers).",
            "failure_patterns": "Open-loop variant (no visual feedback) struggles; closed-loop still limited by perception misses or malformed plan outputs, and by the assumption that primitive skills themselves are robust.",
            "baseline_comparison": "Compared internally to an open-loop VILA variant; closed-loop variant shows clear advantage in dynamic/feedback tasks. Baselines (SayCan, GD) lacked natural incorporation of raw visual feedback into LLM reasoning and thus performed worse in these settings.",
            "ablation_results": "Comparing open-loop vs closed-loop shows visual feedback is a critical component; removing feedback (open-loop) degrades performance significantly on dynamic tasks (no precise numeric ablation values reported).",
            "key_findings": "Direct visual feedback integrated into the language model's reasoning loop enables robust success detection and online replanning, which is essential for embodied tasks with stochasticity or human interaction; this contrasts with language-only planners that would need expensive visual-to-text translation to approximate feedback.",
            "uuid": "e346.4",
            "source_info": {
                "paper_title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do as I can, not as I say: Grounding language in robotic affordances",
            "rating": 2,
            "sanitized_title": "do_as_i_can_not_as_i_say_grounding_language_in_robotic_affordances"
        },
        {
            "paper_title": "Grounded Decoding: Guiding text generation with grounded models for robot control",
            "rating": 2,
            "sanitized_title": "grounded_decoding_guiding_text_generation_with_grounded_models_for_robot_control"
        },
        {
            "paper_title": "Language models are zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2,
            "sanitized_title": "language_models_are_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        },
        {
            "paper_title": "PaLM-E: An embodied multimodal language model for robotic control",
            "rating": 1,
            "sanitized_title": "palme_an_embodied_multimodal_language_model_for_robotic_control"
        },
        {
            "paper_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 1,
            "sanitized_title": "rt2_visionlanguageaction_models_transfer_web_knowledge_to_robotic_control"
        },
        {
            "paper_title": "Vision-language models as success detectors",
            "rating": 1,
            "sanitized_title": "visionlanguage_models_as_success_detectors"
        }
    ],
    "cost": 0.018158,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning
24 Dec 2023</p>
<p>Yingdong Hu 
Tsinghua University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Shanghai Qi Zhi Institute</p>
<p>Fanqi Lin 
Tsinghua University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Shanghai Qi Zhi Institute</p>
<p>Tong Zhang zhangton20@mails.tsinghua.edu.cn 
Tsinghua University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Shanghai Qi Zhi Institute</p>
<p>Li Yi 
Yang Gao 
Tsinghua University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Shanghai Qi Zhi Institute</p>
<p>Tsinghua University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Shanghai Qi Zhi Institute</p>
<p>Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning
24 Dec 202378727F83F97FDC312F8FB3D8395D5530arXiv:2311.17842v2[cs.RO]
including spatial layouts and object attributes.It also supports flexible multimodal goal specification and naturally incorporates visual feedback.Our extensive evaluation, conducted in both real-robot and simulated environments, demonstrates VILA's superiority over existing LLM-based planners, highlighting its effectiveness in a wide array of open-world manipulation tasks.</p>
<p>Fig. 1: We present VILA, a simple and effective method for long-horizon robotic task planning.By integrating vision directly into the reasoning process, VILA can leverage the wealth of commonsense knowledge grounded in the visual world.This results in remarkable performance in tasks that demand an understanding of spatial layouts (top row), object attributes (middle row), and tasks with multimodal goals (bottom row).</p>
<p>Abstract-In this study, we are interested in imbuing robots with the capability of physically-grounded task planning.Recent advancements have shown that large language models (LLMs) possess extensive knowledge useful in robotic tasks, especially in reasoning and planning.However, LLMs are constrained by their lack of world grounding and dependence on external affordance models to perceive environmental information, which cannot jointly reason with LLMs.We argue that a task planner should be an inherently grounded, unified multimodal system.To this end, we introduce Robotic Vision-Language Planning (VILA), a novel approach for long-horizon robotic planning that leverages vision-language models (VLMs) to generate a sequence of actionable steps.VILA directly integrates perceptual data into its reasoning and planning process, enabling a profound understanding of commonsense knowledge in the visual world,</p>
<p>I. INTRODUCTION</p>
<p>Scene-aware task planning is a pivotal facet of human intelligence [83,75].When presented with a simple language instruction, humans demonstrate a spectrum of complex behaviors depending on the context.Take the instruction "get a can of coke," for example.If a coke can is visible, a person will immediately pick it up.If not, they will search locations like the refrigerator or storage cabinets.This adaptability reflects humans' deep understanding of the scene and exten-sive common sense, enabling them to interpret instructions contextually.In this paper, we explore how we can create an embodied agent, such as a robot, that emulates this human-like adaptability and exhibits long-horizon task planning in varying scenes.</p>
<p>In recent years, large language models (LLMs) [9,62,13,6] have showcased their remarkable capabilities in encoding extensive semantic knowledge about the world [65,42,29].This has sparked a growing interest in leveraging LLMs for generating step-by-step plans for complex, long-horizon tasks [2,37,38].However, a critical limitation of LLMs is their lack of world grounding -they cannot perceive and reason about the physical state of robots and their environments, including object shapes, physical properties, and real-world constraints.</p>
<p>To overcome this challenge, a prevalent approach involves employing external affordance models [27], such as openvocab detectors [57] and value functions [2], to provide realworld grounding for LLMs [2,40].However, these modules often fail to convey the truly necessary task-dependent information in complex environments, as they serve as onedirectional channels transmitting perceptual information to LLMs.In this scenario, the LLM is like a blind person, while the affordance model serves as a sighted guide.On the one hand, the blind person relies solely on their imagination and the guide's limited narrative to comprehend the world; on the other hand, the sighted guide may not accurately comprehend the blind person's purpose.This combination often leads to unfeasible or unsafe action plans in the absence of precise, task-relevant visual information.For instance, a robot tasked with taking out a Marvel model from a shelf (see Figure 1 (d)) may overlook obstacles like the paper cup and coke can, leading to collisions.Consider another example of preparing art class (see Figure 1 (h)), scissors can be perceived as sharp and hazardous objects, or as essential tools for handicrafts.This distinction is challenging for the vision module due to the lack of specific task information.These examples highlight the limitations of LLM-based planners in capturing intricate spatial layouts and fine-grained object attributes, underscoring the necessity for active joint reasoning between vision and language.</p>
<p>The recent advancements in vision-language models (VLMs), exemplified by GPT-4V(ision) [61,88], have significantly broadened the horizons of research.VLMs synergize perception and language processing into a unified system, enabling direct incorporation of perceptual information into the language model's reasoning [53,14,5,97].Building upon these developments, we introduce Robotic Vision-Language Planning (VILA) -a simple, effective, and scalable method for long-horizon robotic planning.VILA distinguishes itself from previous LLM-based planning methods by eschewing independent affordance models and instead directly prompting VLMs to generate a sequence of actionable steps based on visual observations of the environment and high-level language instructions.VILA exhibits the following key properties absent in LLM-based planning methods:</p>
<p>• Profound Understanding of Commonsense Knowledge Grounded in the Visual World.VILA excels in complex tasks that demand an understanding of spatial layouts (e.g., Take Out Marvel Model) or object attributes (e.g., Stack Plates Steadily).This kind of commonsense knowledge pervades nearly every task of interest in robotics, but previous LLM-based planners consistently fall short in this regard.• Versatile Goal Specifiaction.VILA supports flexible multimodal goal specification approaches.It is capable of utilizing not just language instructions but also diverse forms of goal images, and even a blend of both language and images, to define objectives effectively.• Visual Feedback.VILA effectively utilizes visual feedback in an intuitive and natural way, enabling robust closed-loop planning in dynamic environments.We conduct a systematic evaluation of VILA across 16 realworld, everyday manipulation tasks, which involve a diverse range of open-set instructions and objects.VILA consistently outperforms LLM-based planners, such as SayCan [2] and Grounded Decoding [40], by a significant margin.To facilitate a more exhaustive and rigorous comparison, we extend our evaluation to include 16 simulated tasks based on the RAVENS environment [93], wherein VILA continues to show marked enhancements.All these outcomes provide compelling evidence that VILA possesses the potential to serve as a universal task planning method for general-purpose robotic systems.</p>
<p>II. RELATED WORK</p>
<p>Vision-Language Models.The striking advancements made by scaling up large language models (LLMs) [9,62,13,79,31] have sparked a surge of interest in similarly expanding large vision-language models (VLMs) [23,61,88].The prevalent approach to construct VLMs involves employing a crossmodal connector to align the features of pre-trained visual encoders with the input embedding space of the LLMs [3,53,52,10,47,36,90,97,5,81].The ability of VLMs to understand both images and text renders them highly adaptable for a range of applications, including visual question answering [4,92], image captioning [1,34], and optical character recognition [48].In contrast to these uses, our study takes a different path.We concentrate on harnessing the rich world knowledge and the visually grounded attribute of VLMs to address complex long-horizon planning challenges in robotics.Pre-Trained Foundation Models For Robotics.Recent advancements in applying large pre-trained foundation models to robotics can be classified into three categories:</p>
<p>(1) Pre-Trained Vision Models: A wealth of prior approaches employ vision models pre-trained on large-scale image datasets [28,15] to generate visual representations for visuomotor control tasks [64,85,67,59,55,95].Nonetheless, a robotic system encompasses more than just a perception module; it includes a control policy as well.Relying solely on visual representations that capture high-level semantics may not ensure the control policy's generalizability or the system's overall effectiveness [35,91,30].</p>
<p>(2) Pre-Trained Language Models: Another research avenue explores the use of large language models (LLMs) for robotic tasks, particularly in reasoning and planning [37,2,73,84,74,51,80,50,16,68].However, to ground these language models in physical environments, auxiliary modules such as affordance models [40], perception APIs [49], and textual scene descriptions [38,94] are essential.In contrast, our work emphasizes generating plans without depending on these auxiliary models for grounding.This approach allows for the seamless integration of perceptual information directly into the reasoning and planning process.</p>
<p>(3) Pre-Trained Vision-Language Models: Numerous studies have explored the application of vision-language models (VLMs) in robotics [39,19,70,96,24].Notably, RT-2 [8] demonstrates the integration of VLMs in low-level robotic control.In contrast, our research is primarily centered on high-level robotic planning.Although PaLM-E [18] shares similarities with our approach, it necessitates training on a substantial mixture of robotics and general visual-language data [11,54].This approach implies that introducing a robot to a new environment necessitates the collection of additional data and subsequent retraining of the model.In stark contrast, our VILA stands out as an open-world, zero-shot model.It is capable of performing a broad spectrum of everyday manipulation tasks without additional training data and incontext examples in the prompt.Task and Motion Planning.Task and Motion Planning (TAMP) [43,26] stands as a critical framework in solving long-horizon planning tasks, integrating low-level continuous motion planning [46] with high-level discrete task planning [22,69,60].While traditional research in this domain has predominantly centered on symbolic planning [22,60] or optimization-based methods [77,78], the advent of machine learning [87,20,25,41] and LLMs [16,12,72,86] is revolutionizing this arena.In our work, we leverage VLMs to comprehend the robot environment and interpret high-level instructions.By incorporating commonsense knowledge that is intrinsically grounded in the visual world, our approach excels in handling complex tasks beyond the reach of previous LLMbased planning methods.</p>
<p>III. METHOD</p>
<p>We first provide the formulation of the planning problem in Sec.III-A.Subsequently, we present how VILA utilize vision-language models as robot planners (Sec.III-B).Finally, we describe unique properties of VILA that contribute to its advantages (Sec.III-C).</p>
<p>A. Problem Statement</p>
<p>Our robotic system takes a visual observation x t of the environment and a high-level language instruction L (e.g."stack these containers of different colors steadily") that describes a manipulation task.We assume that the visual observation x t serves as an accurate representation of world state.The language instruction L can be arbitrarily long-horizon or under-specified (i.e., requires contextual understanding).The Algorithm 1 VILA Require: Initial visual observation x 1 , a high level instruction L and a set of skills Π.
1: t = 1, ℓ 1 = ∅ 2: while ℓ t−1 ̸ = "done" do 3: p 1:N = VLM(x t , L, ℓ 1 , ..., ℓ t−1 )
▷ Get plan steps 4:
ℓ t = p 1 ▷ Select the first step 5:
Execute skill π ℓt (x t ), updating observation x t+1 6:
t = t + 1 7: end while central problem investigated in this work is to generate a sequence of text actions, represented as ℓ 1 , ℓ 2 , • • • , ℓ T .
Each text action ℓ t is a short-horizon language instruction (e.g."pick up blue container") that specifies a sub-task/primitive skill π ℓt ∈ Π.Note that our contributions do not focus on the acquisition of these skills Π; rather, we assume that all the necessary skills are already available.These skills can take the form of predefined script policies or may have been acquired through various learning methods, including reinforcement learning (RL) [76] and behavior cloning (BC) [66].</p>
<p>B. Vision-Language Models as Robot Planners</p>
<p>To generate feasible plans, high-level robot planning must be grounded in the physical world.While LLMs possess a wealth of structured world knowledge, their exclusive reliance on language input necessitates external components, such as affordance models, to complete the grounding process.However, these external affordance models (e.g., value functions of RL policies [2,44], object detection models [57], and action detection models [71]) are manually designed as independent channels, operating separately from LLMs, rather than being integrated into an end-to-end system.Moreover, their role is solely transmitting high-dimensional visual perceptual information to LLMs, lacking the capability for joint reasoning.This separation of vision and language modalities results in the vision module's inability to provide comprehensive, taskrelevant visual information, thereby hindering the LLM from planning based on accurate task-related visual insights.</p>
<p>Recent advances in vision-language models (VLMs) offer a solution.VLMs demonstrate unprecedented ability in understanding and reasoning across both images and language [53,14,5,97].Crucially, the extensive world knowledge encapsulated in VLMs is inherently grounded in the visual data they process.Therefore, we advocate for directly employing VLMs that synergizes vision and language capabilities to decompose a high-level instruction into a sequence of lowlevel skills.</p>
<p>We refer to our method as Robotic Vision-Language Planning (VILA).Concretely, given current visual observation x t of environment and a high-level language goal L, VILA operates by prompting the VLMs to yield a step-by-step plan p 1:N .We enable closed-loop execution by selecting the first step as the text action ℓ t = p 1 .Once the text action ℓ t is selected, the corresponding policy π ℓt is executed by the robot  Fig. 2: Overview of VILA.Given a language instruction and current visual observation, we leverage a VLM to comprehend the environment scene through chain-of-though reasoning, subsequently generating a step-by-step plan.The first step of this plan is then executed by a primitive policy.Finally, the step that has been executed is added to the finished plan, enabling a closed-loop planning method in dynamic environments.and the VLM query is amended to include ℓ t and the process is run again until a termination token (e.g., "done") is reached.The entire process is shown in Figure 2 and described in Algorithm 1.</p>
<p>In our study, we utilizes GPT-4V(ision) [61,88] as the VLM.GPT-4V, trained on vast internet-scale data, exhibits exceptional versatilities and extremely strong generalization capabilities.These attributes make it particularly adept at handling open-world scenarios presented in our paper.Furthermore, we find that VILA, powered by GPT-4V, is capable of solving a variety of challenging planning problems, even when operating in a zero-shot mode (i.e., without requiring any in-context examples).This significantly reduces the prompt engineering efforts required in previous approaches [2,37,40].</p>
<p>C. Intriguing Properties of VILA</p>
<p>In this section, we delve deeper into VILA, shedding light on its advantages and differentiations from previous planning methods.</p>
<p>Comprehension of Common Sense in the Visual World.</p>
<p>Previous studies primarily focus on leveraging the knowledge of LLMs for high-level planning [2,37], centering on language while often overlooking the crucial role of vision.Images and languages, as distinct types of signals, offer unique nature: languages are human-generated and semantically rich, yet they are limited in their ability to represent comprehensive information.In contrast, images are natural signals imbued with low-level fine-grained features, a single image can capture the entirety of a scene's information.This disparity is especially pertinent when the complex environment is challenging to encapsulate in simple language.Directly integrating images into the reasoning and planning process, such as in the case of VILA, allows for a more intuitive understanding of commonsense knowledge grounded in the physical world.Specifically, this understanding manifests in two key aspects:</p>
<p>1) Spatial Layout Understanding: Describing complex geometric configurations, particularly spatial localization, object relationships, and environmental constraints, can be challenging with just simple language.Consider a cluttered scene where object A obscures object B. To reach object B, one must first reposition object A. Relying solely on verbal language descriptions to convey these nuanced relationships between objects is inadequate.Moreover, consider a situation where the desired object is inside a container (like a cabinet or refrigerator).In that case, if an external affordance model (like object detection model) is utilized, since the desired object is not visible, the affordance model would predict a zero probability of successful retrieval, leading to task failure.However, by directly incorporating vision into the reasoning process, VILA can deduce that the sought object, hidden from view, is likely inside the container.This realization necessitates opening the container as a preliminary step to accomplish the task.</p>
<p>2) Object Attribute Understanding: An object is defined by multiple attributes, including its shape, color, material, function, etc.However, the expressive capacity of natural language is limited, making it a somewhat cumbersome medium for conveying these attributes comprehensively.Furthermore, note that an object's attributes is intricately tied to the specific tasks at hand.For example, scissors might be deemed hazardous for children, but they become essential tools during a paper-cutting art class.Previous approaches employ a standalone affordance model to identify object attributes, but this method can only convey a limited subset of attributes in a unidirectional manner.Therefore, active joint reasoning between image and language emerges as a crucial necessity when our tasks demand a thorough understanding of an object's attributes.</p>
<p>Versatile Goal Specification.In many complex, long-term tasks, using a goal image to represent the desired outcome is Please pass me the blue empty plate.We are having an art class, please prepare an area for the children.Fig. 3: Illustration of the execution of VILA (left) and the decision-making process of SayCan (right).In the Bring Empty Plate task, the robot must first relocate the apple and banana from the blue plate.However, SayCan's initial step is to directly pick up the blue plate.In the Prepare Art Class task, while the scissor is supposed to remain on the table, SayCan erroneously picks up the scissor and places it in a box.</p>
<p>often more effective than relying solely on verbal instructions.For example, to direct a robot to tidy a desk, providing a photo of the desk arranged as desired can be more efficient.Likewise, for food plating tasks, a robot can replicate the arrangement from an image.Such tasks, previously unattainable with LLMbased planning methods, are now remarkably straightforward with VILA.Specifically, VILA can not only accepts current visual observation x n and language instructions L as inputs but also incorporates a goal image x g .This feature sets it apart from many existing goal-conditioned RL/IL algorithms [58,21,17], as it does not require the goal and visual observation images to originate from the same domain.The goal image merely needs to convey the essential elements of the task, offering flexibility in its form -it could range from an internet photo to a child's drawing, or even an image showing a target location indicated by a pointing finger.This versatility greatly enhances the system's practicality.Additionally, the ability to combine images and language in describing task goals introduces an additional layer of flexibility and diversity in our goal specification approach.</p>
<p>Visual Feedback.The embodied environments are inherently dynamic, making closed-loop feedback essential for robots.In an effort to incorporate environment feedback into planning methods that rely solely on LLMs, Huang et al. [38] investigate converting all feedback to natural language.However, this approach proves to be cumbersome and ineffective because most of the feedback is initially observed visually.Converting visual feedback into language not only adds complexity to the system but also risks losing valuable information.We believe that providing visual feedback directly is a more intuitive and natural approach, as demonstrated in VILA.Within VILA, the VLM serves both as a scene descriptor to recognize object states and as a success detector to determine if the environment satisfies the success conditions defined by the instructions.By reasoning over visual feedback, VILA enables robots to make corrections or replan in response to changes in the environment or when a skill fails.</p>
<p>IV. EXPERIMENTS AND ANALYSIS</p>
<p>In this section, we first carry out extensive experiments in a real-world system to evaluate VILA's capability in planning everyday manipulation tasks (Sec.IV-A).Subsequently, we conduct a detailed quantitative comparison of VILA against baseline methods within a simulated tabletop environment (Sec.IV-B).</p>
<p>A. Real-World Manipulation Tasks</p>
<p>Experimental Setup.</p>
<p>1) Hardware: We set up a real-world tabletop environment.We use a Franka Emika Panda robot (a 7-DoF arm) and a 1-DoF parallel jaw gripper.For perception, we use a Logitech Brio color camera mounted on a tripod, at an angle, pointing towards the tabletop.To ensure consistency in our experiments, we maintain a fixed camera view for all tasks, but for visual aesthetics, we record video demos at different views.2) Tasks and Evaluation: We design 16 long-horizon manipulation tasks to assess VILA's performance in three domains: comprehension of commonsense knowledge in the visual world (8 tasks), flexibility in goal specification (4 tasks), and utilization of visual feedback (4 tasks).Figure 1 illustrates a selection of 12 tasks, drawn from the first two domains.For each task, we evaluate all methods across the 10 different variations of the environment, including changes in scene configuration and lighting conditions, etc.For comprehensive details of each task, please see Appendix A2.</p>
<p>3) VLM and Prompting: We use GPT-4V from OpenAI API as our VLM.Unlike previous approaches [2,40], we do not include any in-context examples in the prompt, but only use high-level language instructions and some simple constraints that the robot needs to meet (i.e., strict zero-shot).The full prompt is shown in Appendix A3.</p>
<p>4) Primitive Skills: We use five categories of primitive skills that lend themselves to complex behaviors through composition and planning.These include "pick up object", "place object in/on object", "open object", "close object", and "pour object into/onto object".We concentrate on high-level, temporally extended planning rather than acquiring low-level primitive skills, which is orthogonal to our study.Therefore, we employ script policies as the primitive skills for both the baselines and VILA.Additional details of low-level primitive skills are in Appendix A4.</p>
<p>5) Baselines:</p>
<p>We compare with SayCan [2] and Grounded Decoding (GD) [40], which both ground LLMs with external affordance models.Implementing these baselines necessitates accessing output token probabilities from LLMs.However, since OpenAI API currently does not return these probabilities, we employ the open-source Llama 2 70B [79] as an alternative.For the affordance models, we utilize the openvocabulary detector OWL-ViT [57,56], following Huang et al [40].VILA can understand commonsense knowledge in the visual world.In Table I, we compare the planning success rates on tasks that require understanding of spatial layouts  and object attributes (see Figure 1 (a-h) for illustrations of the tasks).VILA stands out with an average success rate of 80% across 8 tasks, significantly surpassing the performances of SayCan and GD, which achieve success rates of only 13% and 20%, respectively.Particularly in intricate and challenging tasks such as Take Out Marvel Model (it's crucial to avoid the cup and coke can) and Righteous Characters1 , SayCan and GD's success rates are close to zero.These tasks all necessitate the integration of images into the reasoning and planning processes and a deep understanding of commonsense knowledge in the visual world.Furthermore, the tasks outlined in Table I are representative of typical realworld scenarios and are not specifically tailored for VILA.The across-the-board exceptional performance of VILA not only highlights its superior generalizability but also underscores its potential as a universal planner for open-world tasks.</p>
<p>Figure 3 shows two environment rollouts comparing VILA with SayCan.In the first Bring Empty Plate task, VILA identifies the need to relocate the apple and banana from the blue plate before picking it up.In contrast, SayCan recognizes the items (apple, banana, blue plate) but lacks awareness of their spatial relationship, leading it to attempt picking up the blue plate directly.This highlights the significance of comprehending complex geometric configurations and environmental constraints visually.In another scenario involving the preparation of a safe area for a children's art class (Prepare Art Class), VILA discerns that only the screwdriver and fruit knife are hazardous, sparing the scissors necessary for the class, based on the contextual clue of paper cuttings on the table.However, SayCan misclassifies the scissors as dangerous, showing that a comprehensive, global visual understanding is crucial to accurately assess object attributes.The videos of experiment rollouts can be found on the project website: robot-vila.github.io.</p>
<p>In Figure 4, we present a failure breakdown analysis."Response structure error" here refers to errors of LLMs and VLMs in generating plan steps that fall outside our predefined Goal Image Initial Obs.</p>
<p>Final Obs.</p>
<p>Arrange the sushi similar to the one in the first picture.</p>
<p>Task Plan I need to put the two vegetables in picture 2 onto the plate pointed by the finger in picture 1. Fig. 5: Illustration of the execution of VILA on image goal-conditioned tasks.In the Arrange Sushi task, VILA generates a plan to arrange sushi based on a reference image.In the Pick Vegetables task, the scenario involves a table set with a pink plate, a black sushi plate, a pizza plate, and a green snack plate.Here, VILA deduces from pointing finger in the goal image that the vegetables should be placed on the pink plate.set of primitive skills.In the case of baselines, "perception error" denotes failures within the open-vocab detector [56].While VLMs lack a separate perception module, their output, as observed in the chain-of-thought process [82], occasionally fails to recognize some objects.The dominant error in baseline models is "understanding error", which involves errors in understanding the complex spatial layouts and object attributes in the physical world, such as occlusions and context-specific attributes.VILA significantly reduces the "understanding error" by seamlessly integrating vision and language reasoning, thereby resulting in the lowest overall error.Furthermore, we suggest that careful prompt engineering (i.e., providing examples in the prompt) [9,63] could steer VLM outputs towards admissible primitive skills, thereby reducing "response structure error".</p>
<p>VILA supports flexible multimodal goal specification.We introduce a suite of 4 tasks, each with distinct goal types, as illustrated in Figure 1 (i-l).The quantitative results are shown in Table II, where VILA demonstrates strong capabilities across all tasks.Utilizing the internet-scale knowledge imbued in GPT-4V, VILA exhibits the remarkable ability to understand a variety of goal images.This includes interpreting vibrant children's drawings for puzzle completion, preparing a sushi platter by referencing a photograph of the dish (illustrated in Figure 5 top row), and even accurately identifying the intended arrangement of vegetables as indicated by a human finger (refer to Figure 5 bottom row).Additionally, we explore goal specification through a combination of image and language instructions.For instance, in the Tidy Up Study Desk task, we not only provide an image of a neatly organized desk as the target but also verbally direct the swapping of two specific objects on the desk.Leveraging its dual-capacity in vision and language reasoning, VILA consistently achieves success in this task as well.</p>
<p>VILA can leverage visual feedback naturally.We design 4 tasks that require real-time visual feedback for successful execution.In the Stack Blocks task, we inject Gaussian noise into the joint position controller, which increases the likelihood of failure in the primitive policy.For the Pack Chip Bags task, task progress is reverted by an experimenter who takes out previously packed chip bags from the box.In the Find Stapler task, the stapler's location varies among three potential places: the top drawer, the bottom drawer, or the      III, reveal that the open-loop variant struggles with these dynamic tasks that demand continuous replanning, while the closedloop VILA significantly outperforms it.VILA is not only able to effectively recover from external disturbances but can also adapt its strategy based on real-time visual observations.A case in point, depicted in Figure 6, is when VILA, not finding the stapler in the top drawer, proceeds to check the bottom drawer, successfully locates the stapler, and completes the task.See Appendix B5 for a detailed breakdown.VILA consistently outperforms baselines across seen and unseen tasks.</p>
<p>B. Simulated Tabletop Rearrangement</p>
<p>Experimental Setup.We conduct experiments on simulated tabletop rearrangement tasks to provide a more rigorous and fair comparison with baseline methods.Following the setting in Grounded Decoding [40], we develop 16 tasks based on the RAVENS environment [93].These tasks are categorized into two groups: a seen group, consisting of 6 tasks used for few-shot prompting or as training for supervised baselines, and an unseen group of 10 tasks.Each task requires a UR5 robot to rearrange the objects on the table in some desired configuration, specified by high-level language instructions.The tasks are further classified into two types (see Figure 7): (i) Blocks &amp; Bowls (8 tasks), which focus on rearranging or combining blocks and bowls (e.g., "put all the blocks in the bowls with matching colors").(ii) Letters (8 tasks), which involve rearranging alphabetical letters (e.g., "put the letters on the tables in alphabetical order").More details about the environmental setup are in Appendix B Our comparison encompasses three baseline categories: (i) CLIPort [71], a language-conditioned imitation learning agent that directly take in the high-level language instructions without a planner.We consider two variants: "Short", trained on single-step pick-and-place instructions, and "Long", trained on high-level instructions.(ii) An LLM-based planner that does not relay on any grounding/affordance model.We evaluate Llama 2 and GPT-4.(iii) Grounded Decoding (GD), which integrates an LLM with an affordance model for enhanced planning.Here, Llama 2 is used as the LLM.For tasks in Blocks &amp; Bowls, affordances are derived from CLIPort's predicted logits, while for tasks in Letters, we use groundtruth affordance values obtained from simulation.We use script policies as the primitive skills for LLM-based planner, GD and our VILA.Analysis.The results are presented in Table IV, where each method is evaluated over 20 episodes per task within each category.We observe that CLIPort-based methods have a limited capacity for generalizing to novel, unseen tasks.Given that GD requires access to the output token probabilities of LLMs, we employ Llama 2 instead of GPT-4 for GD.As depicted in Table IV, both Llama 2 and GPT-4 exhibit comparable performances across all tasks, ensuring a fair comparison between GD and VILA (utilizing GPT-4V).While GD surpasses other LLM-based planning methods by leveraging an external affordance model, it significantly lags behind VILA.This finding further highlights the benefits of synergistic reasoning between vision and language for high-level robotic planning.</p>
<p>V. CONCLUSION, LIMITATIONS, &amp; FUTURE WORKS</p>
<p>In this work, we present VILA, a novel approach for robotic planning that utilizes VLMs to decompose a highlevel language instruction into a sequence of actionable steps.VILA integrates perceptual information into the reasoning and planning process, enabling the understanding of commonsense knowledge in the visual world (e.g., spatial layouts and object attributes).It also supports flexible multimodal goal specification and naturally integrates visual feedback.Our extensive evaluation, conducted in both real-world and simulated settings, demonstrate VILA's effectiveness in addressing a variety of complex, long-horizon tasks.</p>
<p>VILA has several limitations that future work can improve.First, we presuppose the existence of all single-step primitive skills.While obtaining robust low-level control policies remains a challenging problem, recent advancements in transferring web knowledge to robotic control [7,8] holds promise for enabling the cultivation of a repertoire of generalizable skills.Secondly, our dependence on a black-box VLM hampers steerability and complicates the explanation of certain errors.Future developments could leverage parameter-efficient finetuning methods [32,33] to customize VLMs [24].Finally, our current approach excludes in-context examples within prompts, leading to a more versatile output format.Methods developed for prompting [45,89] can also be used to refine output consistency.It's esential to stick to the template of primitive skills.Constrains are as follows:</p>
<p>1.For each task, you need to first select a letter and invoke the first skill, where for the sort letters task, the first letter should be placed on the bottom side, and for other tasks (e.g.put letters in order or word spelling tasks), the letter should be placed in the bottom left corner 2. Each subsequent step in the plan requires selecting other letters and placing them to the right of the previous letters 3.In the plan, there should be no parentheses, square brackets, annotations, or explanations (e.g.skip this step) 4.You need to use the symbols "START PLAN" and "FINISH PLAN" to indicate the beginning and end of your plan Explaining the relevant concepts in the task instruction may help your planning.Please first itemize all letters in each image and then detail the plan With These Letters.Do not include any letters that are not in the image in your plan.</p>
<p>[Initial Environment Image] [Task Instruction]</p>
<p>My child wants to play with a Marvel model, please take one out for him.UserTask-Related Objects and Locations:1.Marvel Model (item to be retrieved) 2. Pepsi Can (blocking object)3.Shelf (storage location)Chain-of-Thought</p>
<p>Fig. 4 :
4
Fig. 4: Error breakdown of VILA and baselines.By leveraging commonsense knowledge grounded in the visual world, VILA significantly reduces understanding error.</p>
<p>Fig. 6 :
6
Fig. 6: Illustration of the execution of VILA on the Find Stapler task.By incorporating visual feedback and replanning at every step, VILA is able to continue exploring the bottom drawer when it does not find the stapler in the top drawer, thereby successfully locating the stapler.</p>
<p>Fig. 7 :
7
Fig. 7: Simulated environment based on RAVENS.We design 16 distinct tasks, which are grouped into two categories: Blocks &amp; Bowls (left) and Letters (right).</p>
<p>Fig. B. 1 :
1
Fig. B.1: Prompt for Blocks &amp; Bowls in simulated environment.</p>
<p>Fig. B. 2 :
2
Fig. B.2: Prompt for Letters in simulated environment.</p>
<p>1 .
1
Pick up paper cup 2. Place paper cup on table
Current Observation3. Pick up pepsi can4. Place pepsi can onFinished Plan</p>
<p>table 5 .
5
Pick up Marval model 6.Place Marvel model on</p>
<p>table 7 .
7
Done
Task PlanAdd to Finished Plan</p>
<p>table 3 .
3
Pick up banana 4. Place banana on
1. Pick up apple2. Place apple on</p>
<p>table 5 .
5
Pick up blue plate 6. Place blue plate in human hand</p>
<p>1 .
1
Pick up shrimp sushi 2. Place shrimp sushi on the left side of the plate 3. Pick up salmon sushi 4. Place salmon sushi in the center of the plate 5. Pick up tuna sushi 6. Place tuna sushi on the right side of the plate
7. DoneTask Plan1. Pick up carrot2. Place carrot on pink plate3. Pick up tomato4. Place tomato on pink plate5. DoneGoal ImageInitial Obs.Final Obs.</p>
<p>TABLE II :
II
Quantitative evaluation results of VILA in tasks featuring multimodal goals.
TaskGoal TypeSucc. %Arrange SushiReal Image80%Arrange Gigsaw PiecesDrawing100%Pick VegetablesPointing Finger100%Tidy Up Study DeskImage + Language60%</p>
<p>Put the stapler on the table.
1. Open top drawer2. Close top drawer 3. Open bottom drawer4. Pick up stapler 5. Close bottom drawer 6. Place stapler on tableInitial Obs.No StaplerFind Stapler</p>
<p>TABLE III :
III
Open
Blocks &amp; BowlsLetters
-loop VILA vs. closed-loop VILA.By leveraging visual feedback, closed-loop VILA substantially outperforms the open-loop variant.</p>
<p>TABLE IV :
IV
Average success rate in simulated environment.</p>
<p>TABLE B . 1 :
B1
Full experimental results in simulation on seen tasks and unseen tasks.Each entry represents success rate averaged across 20 episodes.</p>
<p>You excel at counting and identifying letters in images, as well as planning for robotic table tasks by simplifying complex tasks into primitive skills.The primitive skills that robot arm can execute is: 1. pick up the letter [A capital English letter] and place it on the bottom left corner / bottom side 2. pick up the letter [A capital English letter] and place it on the right of [previous letter]</p>
<p>Choose righteous characters from three Marvel models, while referring to the model only by its color. Details of this task are in Appendix A2.
ACKNOWLEDGMENTSThis work is supported by the Ministry of Science and Technology of the People's Republic of China, the 2030 Innovation Megaprojects "Program on New Generation Artificial Intelligence" (Grant No. 2021AAA0150000).This work is also supported by the National Key R&amp;D Program of China (2022ZD0161700).APPENDIXA. Real-World Environment 1) Hardware Setup: We use a Franka Emika Panda robot (a 7-DoF arm) and a 1DoF parallel jaw gripper.The robot is operated using the joint controller from Deoxys[98].For perception, we use a Logitech Brio color camera mounted on a tripod, at an angle, pointing towards the tabletop.This camera offers high-resolution images at 1920 × 1080, ensuring maximum detail retention.2) Tasks and Evaluation: We design 16 long-horizon tasks, categorized into three domains: (i) understanding commonsense knowledge in the visual world (8 tasks, detailed in TableA.1); (ii) flexibility in goal specification (4 tasks, detailed in TableA.2); and (iii) utilization of visual feedback (4 tasks, detailed in TableA.3).For each task, we perform 10 evaluations under different variations of the environment, accounting for changes in scene configuration, lighting conditions, etc.3) Prompts: We do not include any in-context examples in the prompt, but only use high-level language instructions and some simple constraints that the robot needs to meet (i.e., strict zero-shot).The full prompt is shown in Figure A.1.You are highly skilled in robotic task planning, breaking down intricate and long-term tasks into distinct primitive actions.If the object is in sight, you need to directly manipulate it.If the object is not in sight, you need to use primitive skills to find the object first.If the target object is blocked by other objects, you need to remove all the blocking objects before picking up the target object.At the same time, you need to ignore distracters that are not related to the task.And remember your last step plan needs to be "done".Consider the following skills a robotic arm can perform.In the descriptions below, think of [sth] as an object: You are only allowed to use the provided skills.It's essential to stick to the format of these basic skills.When creating a plan, replace these placeholders with specific items or positions without using square brackets or parentheses.You can first itemize the task-related objects to help you plan.[Initial Environment Image] [Task Instruction] [Reply from GPT-4V] [Environment Image after Executing Some Steps]This image displays a scenario after you have executed some steps from the plan generated earlier.When interacting with people, sometimes the robotic arm needs to wait for the person's action.If you do not find the target object in the current image, you need to continue searching elsewhere.[Reply from GPT-4V]4) Primitive Skills:We use five categories of primitive skills that lend themselves to complex behaviors through composition and planning.These include "pick up object", "place object in/on object", "open object", "close object", and "pour object into/onto object".We concentrate on high-level, temporally extended planning rather than acquiring low-level primitive skills, which is orthogonal to our study.Therefore, we employ script policies as the primitive skills.For simple tasks like "pick up object", we teleoperate the robots by operating a 3D SpaceMouse.For more intricate, contact-rich tasks such as "open drawer", kinematic teaching is employed.These skills are tailored to the tasks considered in our study, developing a generalizable and robust set of primitive skills is an important area for future exploration and research.Pour ChipsInstruction: "My child is hungry, please pour him a plate of chips."Description: In five of the evaluation episodes, the chips are stored inside a cabinet, requiring the robot arm to first open the cabinet in order to locate the chips.For the remaining five episodes, the chips are directly visible, the robot arm should immediately pick up the chip bag.Bring Pepsi CanInstructionRighteous CharactersInstruction: "I want to pick some righteous character models for my child, but I am not familiar with these characters.Which color toys should I put in the box?" Description: There are three Marvel character models: Iron Man (righteous), Captain America (righteous), and Thanos (unrighteous).Due to the constraints in the instruction stating, "I am not familiar with these characters" and "which color toys should I ...", the robot plan must not explicitly mention the names of the characters (such as Iron Man), but is limited to referencing models by their color (like red model).Pick Fresh FruitsInstruction: "I want to buy some fruits.Help me pick the fresh fruits from this pile of fruits and put them into the orange box."Description: There are some rotten fruits and some incomplete fruits (such as a half-peeled orange).The robot arm needs to disregard these distracting fruits and accurately select the fresh fruits.Stack Plates SteadilyInstruction: "Steadily stack these containers of different colors."Description: There are several containers of varying sizes and colors.The robot arm must accurately discern the relative sizes of these containers and stack them steadily in order of size.Prepare Art ClassInstruction: "We are having an art class, please prepare an area for the children.Please put any inappropriate items on the table into the box."Description: Certain objects are unsuitable for an art class setting (such as screwdrivers and fruit knives), while others (like glue and colored paper) are appropriate.Classifying scissors is challenging as they can be viewed as either hazardous or a craft tool for cutting paper.In this specific context, with paper cuttings present, scissors should be retained for this task.This task requires the task planner to ground objects within the specific scene to determine their attributes.TABLE A.1:A list of 8 tasks requiring understanding commonsense knowledge in the visual world.The first four tasks are centered on comprehending spatial layouts, while the subsequent four are dedicated to understanding object attributes.For every task, we provide the instruction as used in our experiments and a detailed description of the task.Arrange SushiInstruction: "In the second picture, arrange the sushi on a specific side of the plate similar to the one in the first picture."Goal Type: Real Image Description: The planner needs to identify the types of sushi and their arrangement in the goal image, and then, based on the observed image from the experiment, place the sushi onto a specific location on the sushi plate.Arrange Jigsaw PiecesInstruction: "The first picture is my child's drawing.In the second picture, arrange the jigsaw pieces on the corners of the whiteboard similar to the landscape image shown in the first picture."Goal Type: Drawing Description: The planner needs to identify the positions of elements in the goal image, and then, based on the observed image from the experiment, place the jigsaw pieces in a specific corner of the whiteboard.Pick VegetablesInstruction: "I need to put the two vegetables in picture 2 onto the plate pointed by the finger in picture 1." Goal Type: Pointing Finger Description: In the goal image, there are multiple plates of different colors.The planner is required to identify the plate being pointed at by a finger, and based on the image observed in the experiment, place the vegetables from the scene onto the indicated plate.Tidy Up Study DeskInstruction: "Study the arrangement in the first picture.Replicate it in the second picture, yet switching the cup and pen holder's positions this time."Goal Type: Image + Language Description: The planner must precisely identify the arrangement of objects in the goal image, while also considering the instruction to switch the positions of the cup and the pen holder.TABLE A.2:A list of 4 tasks featuring multimodal goals.For every task, we provide the instruction as used in our experiments, the goal type, and a detailed description of the task.Stack BlocksInstruction: "Stack all the blocks."Description: In this task, we inject noise during the execution of the primitive skill "Place a block on another block".In 4 out of 10 evaluation episodes, the primitive skill fails when stacking blocks for the first time.In another 4 episodes, the failure occurs during the second stacking attempt.For the remaining 2 episodes, the primitive skill does not fail.Pack Chip BagsInstruction: "Put the chip bag on the table in the gift box."Description: This task involves human intervention, where a person removes the chip bag placed in the gift box by a robot arm.In five of the evaluation episodes, the chip bag that is placed in the gift box for the first time is removed and placed on the table by human.In the other five episodes, the chip bag that is placed in the gift box for the second time is removed and placed on the table.Find StaplerInstruction: "Put the stapler on the table."Description: In this task, the target object (stapler) may be placed in the top drawer, bottom drawer, or cabinet.The planner is required to locate the target object based on visual feedback.In three evaluation episodes, the target object is placed in the top drawer; in four episodes, it is placed in the bottom drawer; and in the remaining three episodes, it is placed inside the cabinet.Human-Robot InteractionInstruction: "Pass me a can of cola."Description: In this task, the robot arm can only execute "Place can of cola in human hand" after detecting a human hand.Before that, the robot repeatedly waits and checks every five seconds for the hand's appearance.In two of the evaluation episodes, the human hand appears directly in the observation; in the remaining eight episodes, the human hand appears in the observation several seconds after the robot arm picks up the can of cola.
Nocaps: Novel object captioning at scale. Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in Neural Information Processing Systems. 202235</p>
<p>Vqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu ; Dhruv, Lawrence Batra, Devi Zitnick, Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionMargaret Mitchell,. 2015</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.12966Qwen-vl: A frontier large vision-language model with versatile abilities. 2023arXiv preprint</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Bosselut, arXiv:2108.07258Emma Brunskill, et al. On the opportunities and risks of foundation models. 2021arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Shikra: Unleashing multimodal llm's referential dialogue magic. Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, Rui Zhao, arXiv:2306.151952023arXiv preprint</p>
<p>Xi Chen, Xiao Wang, Soravit Changpinyo, Piotr Piergiovanni, Daniel Padlewski, Sebastian Salz, Adam Goodman, Basil Grycner, Lucas Mustafa, Beyer, arXiv:2209.06794A jointly-scaled multilingual language-image model. 2022arXiv preprint</p>
<p>Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, Chuchu Fan, arXiv:2306.06531Autotamp: Autoregressive task and motion planning with llms as translators and checkers. 2023arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, 2023</p>
<p>Imagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee2009</p>
<p>Task and motion planning with large language models for object rearrangement. Yan Ding, Xiaohan Zhang, Chris Paxton, Shiqi Zhang, arXiv:2303.062472023arXiv preprint</p>
<p>Pieter Abbeel, and Mariano Phielipp. Goal-conditioned imitation learning. Advances in neural information processing systems. Yiming Ding, Carlos Florensa, 201932</p>
<p>Palme: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.033782023arXiv preprint</p>
<p>Vision-language models as success detectors. Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando De Freitas, Serkan Cabi, arXiv:2303.072802023arXiv preprint</p>
<p>Search on the replay buffer: Bridging planning and reinforcement learning. Ben Eysenbach, Russ R Salakhutdinov, Sergey Levine, Advances in Neural Information Processing Systems. 201932</p>
<p>Contrastive learning as goalconditioned reinforcement learning. Benjamin Eysenbach, Tianjun Zhang, Sergey Levine, Russ R Salakhutdinov, Advances in Neural Information Processing Systems. 202235</p>
<p>Strips: A new approach to the application of theorem proving to problem solving. E Richard, Nils J Fikes, Nilsson, Artificial intelligence. 23-41971</p>
<p>Vision-language pre-training: Basics, recent advances, and future trends. Foundations and Trends® in Computer Graphics and Vision. Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, 202214</p>
<p>Physically grounded vision-language models for robotic manipulation. Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh, arXiv:2309.025612023arXiv preprint</p>
<p>Online replanning in belief space for partially observable task and motion problems. Caelan Reed Garrett, Chris Paxton, Tomás Lozano-Pérez, Leslie Pack Kaelbling, Dieter Fox, 2020 IEEE International Conference on Robotics and Automation (ICRA). </p>
<p>Caelan Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim, Tom Silver, Leslie Pack Kaelbling, Tomás Lozano-Pérez, Integrated task and motion planning. Annual review of control, robotics, and autonomous systems. 20214</p>
<p>The theory of affordances. James J Gibson, 19771Hilldale, USA</p>
<p>Ego4d: Around the world in 3,000 hours of egocentric video. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Language models represent space and time. Wes Gurnee, Max Tegmark, arXiv:2310.022072023arXiv preprint</p>
<p>On pre-training for visuo-motor control: Revisiting a learning-from-scratch baseline. Nicklas Hansen, Zhecheng Yuan, Yanjie Ze, Tongzhou Mu, Aravind Rajeswaran, Hao Su, Huazhe Xu, Xiaolong Wang, arXiv:2212.057492022arXiv preprint</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.155562022arXiv preprint</p>
<p>Parameterefficient transfer learning for nlp. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, International Conference on Machine Learning. PMLR2019</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Scaling up vision-language pre-training for image captioning. Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, Lijuan Wang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>For pre-trained vision models in motor control, not all policy learning methods are created equal. Yingdong Hu, Renhao Wang, Li Erran Li, Yang Gao, arXiv:2304.045912023arXiv preprint</p>
<p>Language is not all you need: Aligning perception with language models. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, arXiv:2302.140452023arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.056082022arXiv preprint</p>
<p>Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023arXiv preprint</p>
<p>Grounded decoding: Guiding text generation with grounded models for robot control. Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman, arXiv:2303.008552023arXiv preprint</p>
<p>Broadlyexploring, local-policy trees for long-horizon task planning. Brian Ichter, Pierre Sermanet, Corey Lynch, arXiv:2010.064912020arXiv preprint</p>
<p>How can we know what language models know?. Zhengbao Jiang, Frank F Xu, Jun Araki, Graham Neubig, Transactions of the Association for Computational Linguistics. 82020</p>
<p>Hierarchical task and motion planning in the now. Leslie Pack, Kaelbling , Tomás Lozano-Pérez, 2011 IEEE International Conference on Robotics and Automation. IEEE2011</p>
<p>Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski, Chelsea Finn, Sergey Levine, Karol Hausman, arXiv:2104.08212Mt-opt: Continuous multi-task robotic reinforcement learning at scale. 2021arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Planning algorithms. Steven M Lavalle, 2006Cambridge university press</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.125972023arXiv preprint</p>
<p>Trocr: Transformer-based optical character recognition with pre-trained models. Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). </p>
<p>Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, Jeannette Bohg, arXiv:2303.12153Text2motion: From natural language instructions to feasible plans. 2023arXiv preprint</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, arXiv:2310.03744Improved baselines with visual instruction tuning. 2023arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. 2023arXiv preprint</p>
<p>Interactive language: Talking to robots in real time. Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, Pete Florence, IEEE Robotics and Automation Letters. 2023</p>
<p>Vip: Towards universal visual reward and representation via value-implicit pre-training. Jason Yecheng, Shagun Ma, Dinesh Sodhani, Osbert Jayaraman, Vikash Bastani, Amy Kumar, Zhang, arXiv:2210.000302022arXiv preprint</p>
<p>Scaling open-vocabulary object detection. Neil Houlsby, Matthias Minderer, Alexey Gritsenko, NeurIPS. 2023</p>
<p>Simple open-vocabulary object detection with vision transformers. Minderer, Gritsenko, Stone, Neumann, Weissenborn, Dosovitskiy, Mahendran, Arnab, Dehghani, Shen, arXiv:2205.062302022arXiv preprint</p>
<p>Visual reinforcement learning with imagined goals. Vitchyr Ashvin V Nair, Murtaza Pong, Shikhar Dalal, Steven Bahl, Sergey Lin, Levine, Advances in neural information processing systems. 312018</p>
<p>R3m: A universal visual representation for robot manipulation. Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta, arXiv:2203.126012022arXiv preprint</p>
<p>Shop: Simple hierarchical ordered planner. Dana Nau, Yue Cao, Amnon Lotem, Hector Munoz-Avila, Proceedings of the 16th international joint conference on Artificial intelligence. the 16th international joint conference on Artificial intelligence19992</p>
<p>Gpt-4v(ision) system card. 2023OpenAI</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Senthil Purushwalkam, and Abhinav Gupta. The unsurprising effectiveness of pre-trained vision models for control. Simone Parisi, Aravind Rajeswaran, International Conference on Machine Learning. PMLR2022</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, Sebastian Riedel, arXiv:1909.01066Language models as knowledge bases?. 2019arXiv preprint</p>
<p>Alvinn: An autonomous land vehicle in a neural network. A Dean, Pomerleau, Advances in neural information processing systems. 11988</p>
<p>Real-world robot learning with masked visual pre-training. Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, Trevor Darrell, Conference on Robot Learning. PMLR2023</p>
<p>Anushri Allen Z Ren, Alexandra Dixit, Sumeet Bodrova, Stephen Singh, Noah Tu, Peng Brown, Leila Xu, Fei Takayama, Jake Xia, Varley, arXiv:2307.01928Robots that ask for help: Uncertainty alignment for large language model planners. 2023arXiv preprint</p>
<p>A structure for plans and behavior. D Earl, Sacerdoti, 1975Department of Computer Science, Stanford UniversityPhD thesis</p>
<p>Lmnav: Robotic navigation with large pre-trained models of language, vision, and action. Dhruv Shah, Błażej Osiński, Sergey Levine, Conference on Robot Learning. PMLR2023</p>
<p>Cliport: What and where pathways for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, Proceedings of the 5th Conference on Robot Learning (CoRL). the 5th Conference on Robot Learning (CoRL)2021</p>
<p>Generalized planning in pddl domains with pretrained large language models. Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B Tenenbaum, Leslie Pack Kaelbling, Michael Katz, arXiv:2305.110142023arXiv preprint</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Plans and situated actions: The problem of human-machine communication. Lucille Alice, Suchman , 1987Cambridge university press</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, 2018MIT press</p>
<p>Logic-geometric programming: An optimization-based approach to combined task and motion planning. Marc Toussaint, IJCAI. 2015</p>
<p>Differentiable physics and stable modes for tool-use and manipulation planning. Kelsey Marc A Toussaint, Kevin A Rebecca Allen, Joshua B Smith, Tenenbaum, 2018</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Chatgpt for robotics: Design principles and model abilities. Sai Vemprala, Rogerio Bonatti, Arthur Bucker, Ashish Kapoor, Microsoft Auton. Syst. Robot. Res. 2202023</p>
<p>Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, arXiv:2311.03079Visual expert for pretrained language models. 2023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Planning and understanding: A computational approach to human reasoning. Robert Wilensky, 1983</p>
<p>Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, Thomas Funkhouser, arXiv:2305.05658Tidybot: Personalized robot assistance with large language models. 2023arXiv preprint</p>
<p>Masked visual pre-training for motor control. Tete Xiao, Ilija Radosavovic, Trevor Darrell, Jitendra Malik, arXiv:2203.061732022arXiv preprint</p>
<p>Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, Harold Soh, arXiv:2302.05128Translating natural language to planning goals with large-language models. 2023arXiv preprint</p>
<p>Neural task programming: Learning to generalize across hierarchical tasks. Danfei Xu, Suraj Nair, Yuke Zhu, Julian Gao, Animesh Garg, Li Fei-Fei, Silvio Savarese, IEEE International Conference on Robotics and Automation (ICRA). 2018</p>
<p>The dawn of lmms: Preliminary explorations with gpt-4v (ision). Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, arXiv:2309.1742120239arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, arXiv:2304.14178mplug-owl: Modularization empowers large language models with multimodality. 2023arXiv preprint</p>
<p>Rl-vigen: A reinforcement learning benchmark for visual generalization. Zhecheng Yuan, Sizhe Yang, Pu Hua, Can Chang, Kaizhe Hu, Xiaolong Wang, Huazhe Xu, arXiv:2307.102242023arXiv preprint</p>
<p>Merlot: Multimodal neural script knowledge models. Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, Yejin Choi, Advances in Neural Information Processing Systems. 202134</p>
<p>Transporter networks: Rearranging the visual world for robotic manipulation. Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, Conference on Robot Learning. PMLR2021</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, arXiv:2204.005982022arXiv preprint</p>
<p>A universal semantic-geometric representation for robotic manipulation. Tong Zhang, Yingdong Hu, Hanchen Cui, Hang Zhao, Yang Gao, arXiv:2306.104742023arXiv preprint</p>
<p>Xiaohan Zhang, Yan Ding, Saeid Amiri, Hao Yang, Andy Kaminski, Chad Esselink, Shiqi Zhang, arXiv:2304.08587Grounding classical task planners via vision-language models. 2023arXiv preprint</p>
<p>Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.105922023arXiv preprint</p>
<p>Viola: Imitation learning for vision-based manipulation with object proposal priors. Yifeng Zhu, Abhishek Joshi, Peter Stone, Yuke Zhu, arXiv:2210.113392022arXiv preprint</p>
<p>Each task requires a UR5 robot to rearrange the objects on the table in some desired configuration, specified by high-level language instructions. A camera is employed to capture a top-down view for task planning purposes. The tasks are categorized into two categories: (i) Blocks &amp; Bowls (8 tasks), which focus on rearranging or combining blocks and bowls. (ii) Letters (8 tasks), which involve rearranging alphabetical letters. Upon each reset, taskrelevant objects, along with some distractor objects, are randomly distributed across the workspace. details of which are provided below: • corner/side: top left corner, top side, top right corner, left side, right side, bottom right corner, bottom side. Prompts: The prompt for Blocks &amp; Bowls is shown in Figure B.1. We incorporate three in-context examples (seen tasks) into the prompt. This approach addresses the substantial domain gap between simulated images of blocks and bowls and their real-world counterparts. Due to this gap, GPT-4V is unable to recognize and comprehend these objects in a zero-shot setting. Conversely, for Letters, we omit in-context examples from the prompt (see Figure B.2), as GPT-4V demonstrates proficient recognition and understanding of all letters</p>
<p>Primitive Skills: In prior work, such as Grounded Decoding, CLIPort policies are utilized as low-level primitive skills. </p>
<p>However, our findings suggest that this approach does not accurately represent the capabilities of the high-level planner. We observe that, in many tests, CLIPort policies correctly interact with objects even when the planner generate an incorrect step. To address this, we shift to employing script policies as our primitive skills. These policies can directly access the ground-truth states of objects within the simulator. more accurate measure of the planner's success rate</p>
<p>Short", trained on single-step pick-and-place instructions, and "Long", trained on high-level instructions. During evaluation, both CLIPort (Short) and CLIPort (Long) receive only the high-level instructions. These baselines aim to evaluate whether solitary language-conditioned policies can perform well on long-horizon tasks and generalize to new task instructions. Our implementation adheres closely to the description in the Grounded Decoding paper [40]; for more details, please refer to this paper. (ii) An LLM-based planner that does not relay on any grounding/affordance model. We evaluate Llama 2 70B and GPT-4. Our evaluation includes Llama 2 and GPT-4. The inclusion of Llama 2 stems from its use in our reimplementation of Grounded Decoding. Grounded Decoding requires access to the output token probabilities from LLMs. However, with the OpenAI API not providing these probabilities, we are constrained to using the open-source Llama 2. (iii) Grounded Decoding (GD), which integrates an LLM with an affordance model for enhanced planning. Here, Llama 2 is used as the LLM. For the Blocks &amp; Bowls scenario, affordances are deduced from CLIPort's predicted logits. Baselines, We consider two variants. as outlined in the GD paper. For Letters, we resort to ground-truth affordance values from simulation due to the limited generalization capability of CLIPort's predicted logits on unseen letters. We employ the beam search variant of GD</p>
<p>Full Results on Simulated Environments: In Table B.1, we show the full list of tasks in simulated environment, alongside their corresponding experimental results. The tasks are categorized by background color: those with a blue background are 'seen' tasks, while those with an orange background are 'unseen' tasks. 'Seen' tasks are used for training for supervised baselines (CLIPort), or included in prompts for high-level planners. However. in the case of the VILA' prompt within the Letters category, we do not include any 'seen' tasks</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5729 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5729</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5729</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-773c31a64ebbc22cc7b61faae4811ef8f046043b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/773c31a64ebbc22cc7b61faae4811ef8f046043b" target="_blank">On Improving Deep Learning Trace Analysis with System Call Arguments</a></p>
                <p><strong>Paper Venue:</strong> IEEE Working Conference on Mining Software Repositories</p>
                <p><strong>Paper TL;DR:</strong> A general approach to learning a representation of the event names along with their arguments using both embedding and encoding is introduced, which was able to increase the performance of two widely-used neural networks by up to 11.3% on two unsupervised language modelling tasks.</p>
                <p><strong>Paper Abstract:</strong> Kernel traces are sequences of low-level events comprising a name and multiple arguments, including a timestamp, a process id, and a return value, depending on the event. Their analysis helps uncover intrusions, identify bugs, and find latency causes. However, their effectiveness is hindered by omitting the event arguments. To remedy this limitation, we introduce a general approach to learning a representation of the event names along with their arguments using both embedding and encoding. The proposed method is readily applicable to most neural networks and is task-agnostic. The benefit is quantified by conducting an ablation study on three groups of arguments: call-related, process-related, and time-related. Experiments were conducted on a novel web request dataset and validated on a second dataset collected on pre-production servers by Ciena, our partnering company. By leveraging additional information, we were able to increase the performance of two widely-used neural networks, an LSTM and a Transformer, by up to 11.3% on two unsupervised language modelling tasks. Such tasks may be used to detect anomalies, pre-train neural networks to improve their performance, and extract a contextual representation of the events.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5729.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5729.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Short-Term Memory (deep unidirectional LSTM used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent neural network architecture used here as a left-to-right language model on sequences of kernel trace events (system calls); implemented as a deep unidirectional LSTM with two hidden layers of 96 units each and trained on event representations with and without arguments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM (deep unidirectional, 2 layers, 96 units)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deep unidirectional Long Short-Term Memory network with two hidden layers of 96 units each. Used as a next-event (left-to-right) language model predicting the next system call token given previous events; trained with cross-entropy on sequences of length 256. No total parameter count reported.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Left-to-right language modelling scoring: compute conditional likelihood (sequence probability via chain rule) from the trained LM and use likelihood (or surprisal) to flag anomalous/low-likelihood sequences; models also explored MLM pretraining (masked LM) with zero-shot evaluation on LM.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Structured sequences: system-call/kernel trace event sequences (events with name + structured arguments/fields).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Behavioral anomalies in system-call sequences (outlier or unlikely sequences indicative of intrusions, bugs, or latency-causing behavior); detection is via low likelihood / high surprisal of sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Web Requests dataset (proposed in paper) and Ciena pre-production traces (partner dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cross-entropy and top-1 accuracy on next-event prediction (used as proxies for modelling quality relevant to anomaly detection). Examples (LM objective): Web Requests LSTM: none (no args) 0.528 cross-entropy / 83.1% accuracy; all arguments 0.423 / 86.4%. Ciena LSTM: none 0.294 / 91.8%; all 0.264 / 92.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against (a) 'none' baseline using only system-call name, and (b) 'none cmp.' where sysname embedding dimension was increased to match total input dimension. Models that include all arguments consistently outperformed both baselines (lower cross-entropy and higher top-1 accuracy), indicating gains are due to argument information not only model-size increases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper does not evaluate LSTM models in a deployed anomaly detection setting with labeled anomalies — only LM predictive metrics are reported. Timestamp argument had negligible impact for LSTM (time-related info not useful for left-to-right LM in LSTM). Dataset is unlabeled and relatively simple; results may not directly translate to supervised anomaly detection without further work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Improving Deep Learning Trace Analysis with System Call Arguments', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5729.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5729.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer (Vaswani et al. style, vanilla transformer used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-attention based sequence model (vanilla Transformer) employed as a language model (LM) and masked language model (MLM) over sequences of system-call events enriched with argument embeddings/encodings; architecture: 6 layers, 8 attention heads, feedforward size 128.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (6 layers, 8 heads, FFN 128)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vanilla Transformer encoder/decoder-style architecture as used in Vaswani et al.; in experiments the model had 6 layers, 8 attention heads per layer, and feedforward sublayer size 128. Positional information was injected by concatenating positional encodings to event embeddings (concatenation was empirically better than addition in these experiments). No total parameter count reported.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Left-to-right LM scoring (and MLM pretraining then zero-shot LM evaluation): compute model likelihoods / token surprisals to detect anomalous sequences; MLM used for pretraining and evaluated zero-shot on LM.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Structured sequences: sequences of system-call/kernel trace events (with structured arguments embedded/encoded into event vectors).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalies as unlikely or out-of-distribution event sequences indicating intrusions, faults, or latency causes; model capacity targets long-range dependencies in sequences to detect anomalous patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Web Requests dataset (this paper) and Ciena pre-production traces (partner dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cross-entropy and top-1 accuracy on next-event prediction and masked LM proxy metrics. Representative numbers (LM): Web Requests Transformer LM: none 0.609 / 80.3%, all args 0.380 / 87.3%; Ciena Transformer LM: none 0.323 / 90.4%, all 0.238 / 92.8%. Representative MLM (pretraining) effect: Web Requests MLM Transformer: none 0.535 / 81.7% vs all 0.182 / 94.1%; Ciena MLM: none 0.285 / 90.8% vs all 0.125 / 96.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to same-model baselines with no arguments ('none') and an enlarged no-argument model ('none cmp.'). Transformers with full argument-enriched event representation systematically outperform baselines. Positional encoding (explicit position vector) often provided as much or more benefit than including timestamp encodings — position dim = 16 gave best gains in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No end-to-end labeled anomaly detection experiments were performed; reported metrics are LM/MLM predictive metrics as proxies for anomaly detection ability. Transformer quadratic complexity limits sequence length (they trained on fixed-length segments of 256 events), so whole-request anomaly scoring (requests can be thousands of events) was not evaluated. Timestamp is redundant with explicit position; timestamp helps Transformer modestly but positional encoding is easier to exploit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Improving Deep Learning Trace Analysis with System Call Arguments', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5729.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5729.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Arg-Enriched Event Representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Argument-enriched event representation for system-call events (embeddings + encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to represent each kernel trace event as a dense vector by (1) embedding inherently meaningful categorical args (sysname, procname, entry, ret) and (2) encoding contextual numeric args (pid, tid, timestamp) with sinusoidal positional-style encodings, combining via addition for some small embeddings and concatenation for others; used as input to LSTM and Transformer language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Argument-enriched event representation (used with LSTM and Transformer LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Event vector construction: sysname, entry, and ret embeddings (each 32-d) are added; procname embedding (16-d) is concatenated with pid and tid encodings (4-d each); timestamp encoded with sinusoidal encoding (8-d). Final event vector dimension = 64 (positional encoding dimension matched to timestamp when used). Encodings for numeric arguments use Vaswani-style sin/cos positional encodings. Addition used to model relation shifts (e.g., entry/exit) in same embedding space; concatenation used to avoid bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Used as input to LM scoring (left-to-right LM) and MLM pretraining; improvements in predictive LM/MLM metrics are used as evidence that argument enrichment improves the model's ability to characterize normal behavior, which in turn is relevant for anomaly detection based on likelihood scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Structured/tabular-like data embedded into sequence tokens: system-call events each with a set of structured arguments (categorical and numeric).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalies detectable include anomalous sequences of events, contextually unlikely return values / procnames / pid/tid patterns, and temporal irregularities that may indicate intrusions, bugs, or latency issues.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Web Requests dataset (generated by authors) and Ciena pre-production dataset (collected by industry partner)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Measured via LM/MLM cross-entropy and top-1 accuracy. Including all arguments yielded consistent gains: e.g., Web Requests LM (Transformer) accuracy improved from 80.3% (no args) to 87.3% (all args); MLM Transformer accuracy on Web Requests improved from 81.7% to 94.1% when using all arguments. Authors report up to an 11.3% improvement (abstract) on two unsupervised LM tasks by leveraging arguments.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to (a) using only system-call name, and (b) compensated larger sysname-only embedding. Argument-enriched representation outperformed both, showing gains are from added argument information not merely larger input dimensionality. Computational overhead minimal (per-epoch time increased only slightly; see reported epoch timings).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Evaluations limited to unsupervised LM and MLM predictive metrics; no labeled anomaly detection experiments. Dataset unlabeled, relatively simple generated web workload; generalization to diverse real-world workloads and supervised anomaly detection tasks remains to be validated. Timestamp encoding provided limited benefit beyond explicit positional encodings in some cases; some numeric args (e.g., pid) are contextual rather than inherently meaningful and require encoding choices that can induce collisions if not tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Improving Deep Learning Trace Analysis with System Call Arguments', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LSTM-Based System-Call Language Modeling and Robust Ensemble Method for Designing Host-Based Intrusion Detection Systems <em>(Rating: 2)</em></li>
                <li>EXAD: A System for Explainable Anomaly Detection on Big Data Traces <em>(Rating: 2)</em></li>
                <li>Anomaly detection from system tracing data using multimodal deep learning <em>(Rating: 1)</em></li>
                <li>On The Learning Of System Call Attributes For Host-based Anomaly Detection <em>(Rating: 2)</em></li>
                <li>Learning Useful System Call Attributes for Anomaly Detection <em>(Rating: 2)</em></li>
                <li>Recurrent neural network attention mechanisms for interpretable system log anomaly detection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5729",
    "paper_id": "paper-773c31a64ebbc22cc7b61faae4811ef8f046043b",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "LSTM",
            "name_full": "Long Short-Term Memory (deep unidirectional LSTM used in this paper)",
            "brief_description": "A recurrent neural network architecture used here as a left-to-right language model on sequences of kernel trace events (system calls); implemented as a deep unidirectional LSTM with two hidden layers of 96 units each and trained on event representations with and without arguments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LSTM (deep unidirectional, 2 layers, 96 units)",
            "model_description": "Deep unidirectional Long Short-Term Memory network with two hidden layers of 96 units each. Used as a next-event (left-to-right) language model predicting the next system call token given previous events; trained with cross-entropy on sequences of length 256. No total parameter count reported.",
            "model_size": null,
            "anomaly_detection_method": "Left-to-right language modelling scoring: compute conditional likelihood (sequence probability via chain rule) from the trained LM and use likelihood (or surprisal) to flag anomalous/low-likelihood sequences; models also explored MLM pretraining (masked LM) with zero-shot evaluation on LM.",
            "data_type": "Structured sequences: system-call/kernel trace event sequences (events with name + structured arguments/fields).",
            "anomaly_type": "Behavioral anomalies in system-call sequences (outlier or unlikely sequences indicative of intrusions, bugs, or latency-causing behavior); detection is via low likelihood / high surprisal of sequences.",
            "dataset_name": "Web Requests dataset (proposed in paper) and Ciena pre-production traces (partner dataset)",
            "performance_metrics": "Cross-entropy and top-1 accuracy on next-event prediction (used as proxies for modelling quality relevant to anomaly detection). Examples (LM objective): Web Requests LSTM: none (no args) 0.528 cross-entropy / 83.1% accuracy; all arguments 0.423 / 86.4%. Ciena LSTM: none 0.294 / 91.8%; all 0.264 / 92.4%.",
            "baseline_comparison": "Compared against (a) 'none' baseline using only system-call name, and (b) 'none cmp.' where sysname embedding dimension was increased to match total input dimension. Models that include all arguments consistently outperformed both baselines (lower cross-entropy and higher top-1 accuracy), indicating gains are due to argument information not only model-size increases.",
            "limitations_or_failure_cases": "Paper does not evaluate LSTM models in a deployed anomaly detection setting with labeled anomalies — only LM predictive metrics are reported. Timestamp argument had negligible impact for LSTM (time-related info not useful for left-to-right LM in LSTM). Dataset is unlabeled and relatively simple; results may not directly translate to supervised anomaly detection without further work.",
            "uuid": "e5729.0",
            "source_info": {
                "paper_title": "On Improving Deep Learning Trace Analysis with System Call Arguments",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Transformer",
            "name_full": "Transformer (Vaswani et al. style, vanilla transformer used in this paper)",
            "brief_description": "A self-attention based sequence model (vanilla Transformer) employed as a language model (LM) and masked language model (MLM) over sequences of system-call events enriched with argument embeddings/encodings; architecture: 6 layers, 8 attention heads, feedforward size 128.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Transformer (6 layers, 8 heads, FFN 128)",
            "model_description": "Vanilla Transformer encoder/decoder-style architecture as used in Vaswani et al.; in experiments the model had 6 layers, 8 attention heads per layer, and feedforward sublayer size 128. Positional information was injected by concatenating positional encodings to event embeddings (concatenation was empirically better than addition in these experiments). No total parameter count reported.",
            "model_size": null,
            "anomaly_detection_method": "Left-to-right LM scoring (and MLM pretraining then zero-shot LM evaluation): compute model likelihoods / token surprisals to detect anomalous sequences; MLM used for pretraining and evaluated zero-shot on LM.",
            "data_type": "Structured sequences: sequences of system-call/kernel trace events (with structured arguments embedded/encoded into event vectors).",
            "anomaly_type": "Anomalies as unlikely or out-of-distribution event sequences indicating intrusions, faults, or latency causes; model capacity targets long-range dependencies in sequences to detect anomalous patterns.",
            "dataset_name": "Web Requests dataset (this paper) and Ciena pre-production traces (partner dataset)",
            "performance_metrics": "Cross-entropy and top-1 accuracy on next-event prediction and masked LM proxy metrics. Representative numbers (LM): Web Requests Transformer LM: none 0.609 / 80.3%, all args 0.380 / 87.3%; Ciena Transformer LM: none 0.323 / 90.4%, all 0.238 / 92.8%. Representative MLM (pretraining) effect: Web Requests MLM Transformer: none 0.535 / 81.7% vs all 0.182 / 94.1%; Ciena MLM: none 0.285 / 90.8% vs all 0.125 / 96.2%.",
            "baseline_comparison": "Compared to same-model baselines with no arguments ('none') and an enlarged no-argument model ('none cmp.'). Transformers with full argument-enriched event representation systematically outperform baselines. Positional encoding (explicit position vector) often provided as much or more benefit than including timestamp encodings — position dim = 16 gave best gains in some settings.",
            "limitations_or_failure_cases": "No end-to-end labeled anomaly detection experiments were performed; reported metrics are LM/MLM predictive metrics as proxies for anomaly detection ability. Transformer quadratic complexity limits sequence length (they trained on fixed-length segments of 256 events), so whole-request anomaly scoring (requests can be thousands of events) was not evaluated. Timestamp is redundant with explicit position; timestamp helps Transformer modestly but positional encoding is easier to exploit.",
            "uuid": "e5729.1",
            "source_info": {
                "paper_title": "On Improving Deep Learning Trace Analysis with System Call Arguments",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Arg-Enriched Event Representation",
            "name_full": "Argument-enriched event representation for system-call events (embeddings + encodings)",
            "brief_description": "A method to represent each kernel trace event as a dense vector by (1) embedding inherently meaningful categorical args (sysname, procname, entry, ret) and (2) encoding contextual numeric args (pid, tid, timestamp) with sinusoidal positional-style encodings, combining via addition for some small embeddings and concatenation for others; used as input to LSTM and Transformer language models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Argument-enriched event representation (used with LSTM and Transformer LMs)",
            "model_description": "Event vector construction: sysname, entry, and ret embeddings (each 32-d) are added; procname embedding (16-d) is concatenated with pid and tid encodings (4-d each); timestamp encoded with sinusoidal encoding (8-d). Final event vector dimension = 64 (positional encoding dimension matched to timestamp when used). Encodings for numeric arguments use Vaswani-style sin/cos positional encodings. Addition used to model relation shifts (e.g., entry/exit) in same embedding space; concatenation used to avoid bottleneck.",
            "model_size": null,
            "anomaly_detection_method": "Used as input to LM scoring (left-to-right LM) and MLM pretraining; improvements in predictive LM/MLM metrics are used as evidence that argument enrichment improves the model's ability to characterize normal behavior, which in turn is relevant for anomaly detection based on likelihood scoring.",
            "data_type": "Structured/tabular-like data embedded into sequence tokens: system-call events each with a set of structured arguments (categorical and numeric).",
            "anomaly_type": "Anomalies detectable include anomalous sequences of events, contextually unlikely return values / procnames / pid/tid patterns, and temporal irregularities that may indicate intrusions, bugs, or latency issues.",
            "dataset_name": "Web Requests dataset (generated by authors) and Ciena pre-production dataset (collected by industry partner)",
            "performance_metrics": "Measured via LM/MLM cross-entropy and top-1 accuracy. Including all arguments yielded consistent gains: e.g., Web Requests LM (Transformer) accuracy improved from 80.3% (no args) to 87.3% (all args); MLM Transformer accuracy on Web Requests improved from 81.7% to 94.1% when using all arguments. Authors report up to an 11.3% improvement (abstract) on two unsupervised LM tasks by leveraging arguments.",
            "baseline_comparison": "Compared to (a) using only system-call name, and (b) compensated larger sysname-only embedding. Argument-enriched representation outperformed both, showing gains are from added argument information not merely larger input dimensionality. Computational overhead minimal (per-epoch time increased only slightly; see reported epoch timings).",
            "limitations_or_failure_cases": "Evaluations limited to unsupervised LM and MLM predictive metrics; no labeled anomaly detection experiments. Dataset unlabeled, relatively simple generated web workload; generalization to diverse real-world workloads and supervised anomaly detection tasks remains to be validated. Timestamp encoding provided limited benefit beyond explicit positional encodings in some cases; some numeric args (e.g., pid) are contextual rather than inherently meaningful and require encoding choices that can induce collisions if not tuned.",
            "uuid": "e5729.2",
            "source_info": {
                "paper_title": "On Improving Deep Learning Trace Analysis with System Call Arguments",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LSTM-Based System-Call Language Modeling and Robust Ensemble Method for Designing Host-Based Intrusion Detection Systems",
            "rating": 2
        },
        {
            "paper_title": "EXAD: A System for Explainable Anomaly Detection on Big Data Traces",
            "rating": 2
        },
        {
            "paper_title": "Anomaly detection from system tracing data using multimodal deep learning",
            "rating": 1
        },
        {
            "paper_title": "On The Learning Of System Call Attributes For Host-based Anomaly Detection",
            "rating": 2
        },
        {
            "paper_title": "Learning Useful System Call Attributes for Anomaly Detection",
            "rating": 2
        },
        {
            "paper_title": "Recurrent neural network attention mechanisms for interpretable system log anomaly detection",
            "rating": 1
        }
    ],
    "cost": 0.01125175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>On Improving Deep Learning Trace Analysis with System Call Arguments</h1>
<p>Quentin Fournier<br>Polytechnique Montréal Quebec H3T 1J4<br>quentin.fournier@polymtl.ca</p>
<p>Daniel Aloise
Polytechnique Montréal
Quebec H3T 1J4
daniel.aloise@polymtl.ca</p>
<p>Seyed Vahid Azhari
Ciena
Ottawa K2K 0L1
vazhari@ciena.com</p>
<p>François Tetreault
Ciena
Ottawa K2K 0L1
ftetreau@ciena.com</p>
<p>Abstract-Kernel traces are sequences of low-level events comprising a name and multiple arguments, including a timestamp, a process id, and a return value, depending on the event. Their analysis helps uncover intrusions, identify bugs, and find latency causes. However, their effectiveness is hindered by omitting the event arguments. To remedy this limitation, we introduce a general approach to learning a representation of the event names along with their arguments using both embedding and encoding. The proposed method is readily applicable to most neural networks and is task-agnostic. The benefit is quantified by conducting an ablation study on three groups of arguments: call-related, process-related, and time-related. Experiments were conducted on a novel web request dataset and validated on a second dataset collected on pre-production servers by Ciena, our partnering company. By leveraging additional information, we were able to increase the performance of two widely-used neural networks, an LSTM and a Transformer, by up to $\mathbf{1 1 . 3 \%}$ on two unsupervised language modelling tasks. Such tasks may be used to detect anomalies, pre-train neural networks to improve their performance, and extract a contextual representation of the events.</p>
<p>Index Terms-Tracing, Machine Learning, Deep Learning.</p>
<h2>I. INTRODUCTION</h2>
<p>In recent years, deep learning has been successfully applied to an ever-growing range of supervised and unsupervised tasks. This trend has been enabled by the ever-increasing computational resources and the novel techniques introduced to take advantage of these resources. As of today, the largest model for natural language processing (NLP) comprises 175 billion parameters and has been trained on half a terabyte of curated text [1]. The authors showed that the model performance scales consistently with the number of parameters and the amount of available data.</p>
<p>A technique that surely generates a large amount of data is tracing. Tracing is the act of collecting a trace which is a sequence of low-level events. Such events are produced whenever a specific instruction called tracepoint is encountered at runtime and comprises a name, a precise timestamp, and possibly many arguments. Figure 1 depicts three trace events.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Traces provide insights on the execution of a piece of code and have been extensively used to detect intrusions, identify bugs, and find the root cause of latency issues. The main advantages of tracers are (1) they do not require to stop the execution contrary to debuggers, and (2) they do not aggregate events or metrics contrary to loggers.</p>
<p>In this paper, we consider the events generated by the operating system also known as kernel events. The benefit of such events is two-fold: (1) tracepoints are already implemented in the Linux kernel, which allows tracing virtually any Linux system without having to modify the source code, and (2) the behaviour of the whole system is visible from the kernel. In this paper, we focus on a subset of the kernel events called system calls. System calls are the only way for an application to communicate with the operating system.</p>
<p>Although the manual inspection of traces may reveal insights that are virtually impossible to extract automatically, the amount of human labour required is often prohibitive. Indeed, the operating system produces thousands of events every second, most of which may be collected. The sheer size of traces is the primary reason why automatic analysis is required. Traces are used to detect unknown intrusions, to identify unknown bugs, or to locate the unknown root cause of anomalies, making their analysis often challenging to specify in practice. Therefore, machine learning techniques, that is, techniques that learn how to solve a task from examples, are well suited to analyse traces.</p>
<p>Most machine learning methods take a vector of numerical features as input. Hand-crafted features of traces have been proposed, but no representation seems to work universally well or to encapsulate the true underlying explanatory factors [2, 3, 4]. Instead of relying on hand-crafted features, neural networks learn how to extract meaningful features for the task. By finding a relevant input representation for the task, neural networks reduce the need for an expert, and the model performance is improved in most cases.</p>
<p>Although a wide range of deep learning techniques has been applied on traces by previous works, only a small fraction of the accessible information has been considered. The event arguments and, in certain cases, the event ordering inside the trace, have been left out in the literature. Section II discusses in more detail the related works and their limitations. We argue that the increase in resources and the improvement of deep</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Trace events as displayed by Babeltrace<sup>2</sup>. The arguments are all the values except the event name. In this example, the arguments are from left to right: the timestamp, the hostname, the CPU id, the process name, the process id, the thread id, the file descriptor, and the return value.</p>
<p>learning techniques allow fully exploiting traces.</p>
<p>A trace is a sequence of discrete values arguably comprising a syntax and a semantic. Due to their resemblance to natural language, the most common approach is to apply deep learning techniques from natural language processing. Our methodology follows the previous works by considering the widely used Long Short-Term Memory (LSTM) [5]. A recent alternative to LSTM for processing variable-length sequences called the Transformer [6] is also evaluated. Although this model is omnipresent in NLP, it has not yet been applied on traces. The two models were evaluated on two unsupervised objectives: (1) left-2-right language model (LM) that allows computing the likelihood of a sequence, and therefore, detecting anomalies, and (2) masked language model (MLM) that is used for pre-training [7].</p>
<p>This paper's first contribution is the introduction in Section III of a novel method to learn a single representation of the system call names with their arguments. Results are detailed in Section V and an ablation study is conducted to investigate the impact of three groups of arguments: call-related, process-related, and time-related.</p>
<p>The second contribution of this paper is the introduction of a novel dataset comprising around 250,000 web requests. The actual dataset is provided, but most importantly, the data generation methodology is explained in Section IV. One may argue that our dataset is too simple or that it inaccurately represents actual web servers. Therefore, every experiment is validated on a second dataset collected on pre-production servers by Ciena, the partnering company of this research.</p>
<p>Finally, Section VI discusses the possible threats to validity, and Section VII answers interesting questions about the pertinence of the proposed approach and future works.</p>
<h2>II. RELATED WORK</h2>
<p>Over the last two decades, a wide range of machine learning techniques has been applied to analyze traces, including naive Bayes [8], random forest [9], and hidden Markov models [10]. Recently, the trend has shifted toward more flexible approaches, and especially toward deep learning methods. Model flexibility relates to the space of functions that the model is able to learn and increases with the number of parameters. Therefore, highly flexible methods, such as large neural networks, are able to learn complex solutions that typically perform better than less flexible ones. This section provides an overview of the main neural networks that have been studied in the tracing literature as well as their limitations.</p>
<p>Recurrent neural networks (RNNs) allow processing variable-length sequences with a fixed number of parameters. Such a network produces an output at every time step and is depicted in figure 2. The Long Short-Term Memory (LSTM) [5] is a recurrent neural network specifically designed to learn dependencies across a large number of time steps. This network has been extensively and successfully used across many fields. Tracing is no exception, and LSTM is by far the most popular neural network to analyze traces [9, 11, 12, 13, 14].</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. The unrolled computational graph of a recurrent neural network. The input and output sequences are depicted in blue and red, respectively. The time step is indicated in exponent and between parenthesis. Note that the network parameters <em>W</em>, <em>U</em>, and <em>V</em>, are replicated at every time step. Therefore, the network can process variable-length input sequences.</p>
<p>Dymshits et al. [11] trained a unidirectional and a bidirectional LSTM on <em>sequences of system call count vectors</em>. Such vectors are bag-of-words, that is to say, the normalized counts of system call names, from a fixed-duration window. This aggregation is a trade-off between computational efficiency and performance, and is controlled by the window size. The authors also trained an Inception-like net consisting of multiple LSTMs with tied weights. They found that simpler LSTMs perform on par with the more complex ones.</p>
<p>Kim et al. [12] trained an ensemble of LSTMs on sequences of system call names. Ensemble techniques improve the performance, although not significantly, and the robustness of the chosen method. While ensemble techniques may be necessary for industry products, this paper will not leverage them as the main objective is to show the relative impact of the arguments rather than the approach's absolute performance.</p>
<p>Song et al. [9] compared an LSTM with less flexible machine learning techniques to detect and explain anomalies from streams of traces. They did not, however, explicitly say which events were considered or describe their preprocessing.</p>
<p>Recurrent neural networks output a vector at every time step, so the output sequence must have the same length as the input sequence (see Figure 2). This property of RNNs may become a constraint depending on the task. To overcome this</p>
<p><sup>2</sup>https://babeltrace.org</p>
<p>limitation, Sutskever et al. [15] introduced the sequence-tosequence framework where a first network (encoder) encodes the input sequence into a fixed-size context. A second network (decoder) then generates the output sequences based on this context. This framework allows outputting a variable-length sequence independently of the input sequence length and is illustrated in Figure 3.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Sequence-to-sequence framework. A first network (encoder) encodes the input sequence into a fixed-size context <strong>b</strong><sup>(n</sup><sub>i</sub> shown in red, then a second network (decoder) generates the output sequences based on this context.</p>
<p>Lv et al. [16] used a gated recurrent unit<sup>3</sup> (GRU) [17] in a sequence-to-sequence fashion to extend sequences of system calls names and increase the accuracy of intrusion detection.</p>
<p>Recurrent networks, including LSTMs and GRUs, suffer from an issue related to memory compression [18]. As the input sequence gets processed, information must be stored in the fixed-size hidden representation <em>h</em>. Either <em>h</em> is too large and computational resources are wasted, or <em>h</em> is too small and information is lost. In the latter case, the model performance might be significantly impacted. Bahdanau et al. [19] introduced an alignment mechanism called <em>inter-attention</em> to mitigate the effect of memory compression. This mechanism computes a different representation of the input for each output step, effectively allowing the decoder to "look at" the relevant part(s) of the input for each output step. Figure 4 illustrates the inter-attention mechanism.</p>
<p>Brown et al. [20] augmented an LSTM with the dot-product inter-attention and explored different ways of computing the attention: fixed, position-based (syntax attention), and context-based (semantic attention). For the task of system log anomaly detection, every attention yielded comparable results.</p>
<p>Finally, due to their sequential nature, recurrent networks do not scale efficiently to longer sequences [6]. Dai et al. [21] introduced the relative effective context length (RECL), the largest context length that leads to a substantial relative gain over the best model. Simply put, increasing the context length over the RECL yields a negligible increase in performance; thus, RECL indicates the maximum dependency length that the model is able to learn. They showed that the RECL of LSTM is limited to around 400 time-steps. This is problematic for trace analysis since hundreds of events may be generated every second.</p>
<p>To overcome this limitation, Vaswani et al. [6] introduced the Transformer, a sequence-to-sequence model based solely on the inter-attention and self-attention mechanisms. The self-attention allows relating any two positions in a sequence</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Inter-attention mechanism. The attention weight <em>α</em><sup>(t)</sup><sub>i</sub> corresponds to the strength with which the <em>i</em>-th encoder hidden representation <em>h</em><sup>(i)</sup> contributes to the context of the <em>t</em>-th decoder step.</p>
<p>regardless of their distance thus allowing for a significant increase in performance in most natural language processing tasks at the cost of a quadratic complexity with respect to the sequence length. To the best of our knowledge, this model has not been applied on traces but has been included considering its ubiquity in NLP.</p>
<p>None of the aforementioned works considered the system call arguments. Arguably, the main reason is that the community does "<em>[…] not have a compact fixed-dimensional representation for system call arguments suitable for large-volume training and classification.</em>" Dymshits et al. [11].</p>
<p>Nonetheless, Nedelkoski et al. [13] used a bimodal LSTM that is the concatenation of two LSTM hidden representations trained on the real-valued duration and one-hot-encoded texts, respectively. Albeit their work considered logs rather than traces, one may view their method as leveraging a temporal argument. The neural network proposed by Ezeme et al. [14] is the closest to actually considering multiple arguments values. The authors trained an LSTM using the system call name, the CPU cycles count, and the distribution of characters in the arguments' values.</p>
<p>As far as we know, only two works by Tandon and Chan [22, 23] considered the actual values of multiple system call arguments. The authors trained a conditional rule-learning algorithm called LERAD, which, contrary to neural networks, does not require to learn a fixed-size representation of the arguments.</p>
<h2>III. PROPOSED APPROACH</h2>
<p>Before introducing the proposed approach, let us clarify the different categories of system call arguments. One may group them depending on whether they are part of the stream context, the event context, or the event fields (see Figure 1). In this work, the arguments are grouped based on their semantic. The</p>
<p><sup>3</sup>GRU is similar to LSTM but requires fewer parameters.</p>
<p>first category comprises all call-related arguments such as the return value, the file descriptor, the type of futex operation, and the number of bytes to write - depending on the event. The second category consists of all process-related arguments such as the process name, the thread id, and the process id. Note that this category corresponds exactly to the event context. Finally, the third group consists of time-related arguments such as the timestamp and the timeout duration.</p>
<p>The scope of this work is limited to the arguments that are common to virtually all system calls. Namely, the return value (ret), whether the event corresponds to the start or end of a system call execution (entry), the process name (procname), the thread id (tid), the process id (pid), and the timestamp (timestamp). As explained later, extending this work to other arguments is simple but may require a substantially larger dataset. Table I recapitulates the considered arguments.</p>
<p>TABLE I
THE STUDIED SYSTEM CALL ARGUMENTS.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Argument</th>
<th style="text-align: left;">Notation</th>
<th style="text-align: left;">Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">call-related</td>
<td style="text-align: left;">return value</td>
<td style="text-align: left;">ret</td>
<td style="text-align: left;">integer</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">start/end of execution</td>
<td style="text-align: left;">entry</td>
<td style="text-align: left;">boolean</td>
</tr>
<tr>
<td style="text-align: left;">process-related</td>
<td style="text-align: left;">process name</td>
<td style="text-align: left;">procname</td>
<td style="text-align: left;">string</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">process id</td>
<td style="text-align: left;">pid</td>
<td style="text-align: left;">integer</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">thread id</td>
<td style="text-align: left;">tid</td>
<td style="text-align: left;">integer</td>
</tr>
<tr>
<td style="text-align: left;">time-related</td>
<td style="text-align: left;">timestamp</td>
<td style="text-align: left;">timestamp</td>
<td style="text-align: left;">integer</td>
</tr>
</tbody>
</table>
<p>In order to determine how to represent the arguments, one must identify the intrinsically meaningful ones. In other words, one has to assess whether the argument values convey meaning in themselves - without any context. As an example, let us consider the process name "apache". This value means that an Apache web server has generated the system call, hence procname is inherently meaningful. On the contrary, the process id "12523" is only meaningful in the context of the trace. Indeed, the pid allows relating events that have been generated by the same process; the value "12523", however, may well be associated with two distinct processes at different points in time.</p>
<p>The procname, the return value, and the entry are intrinsically meaningful arguments, and hence, an embedding will be learned for them. On the contrary, the pid, the tid, and the timestamp are not inherently meaningful and an encoding will be applied.</p>
<h2>A. Embedding</h2>
<p>One way to represent textual words is through a sparse binary vector called one-hot-encoding. The $i$-th word of the vocabulary is mapped to a row vector $\boldsymbol{e}<em i="i">{w</em>$ whose dimension is equal to the size of the vocabulary. Such vector is filled with 0 except for the $i$-th position which is equal to 1 . Given a toy vocabulary of three system call names {open, close, timer}, their one-hot-encoding would be $[1,0,0],[0,1,0]$, and $[0,0,1]$, respectively.}</p>
<p>One-hot-encoding has two major drawbacks: (1) the vector dimension is equal to the vocabulary size which may be large, and (2) the encoding of any two distinct words are perpendicular, meaning that words are equidistant. For instance, one would expect $\operatorname{dist}\left(\boldsymbol{e}<em _close="{close" _text="\text">{\text {open }}, \boldsymbol{e}</em>}}\right)&lt;\operatorname{dist}\left(\boldsymbol{e<em _text="\text" _timer="{timer">{\text {open }}, \boldsymbol{e}</em>\right)$ as open is semantically closer to close than to timer.}</p>
<p>A better representation is expected to be more compact and to encapsulate semantic knowledge about the word. Such representation is called an embedding. Note that in the natural language processing community, an embedding refers to both the general mapping from a textual space to a semantic vector space and the actual dense vectorial representation of a word.</p>
<p>Formally, an embedding is defined by a dense matrix $\boldsymbol{W} \in \mathbb{R}^{d_{v} \times d_{e}}$ with $d_{v}$ the size of the vocabulary and $d_{e}$ the dimension of the embedding such as $d_{e} \ll d_{v}$. The embedding $\boldsymbol{x}<em i="i">{w</em>}}$ of the word $w_{i}$ is computed by multiplying its one-hotencoding $\boldsymbol{e<em i="i">{w</em>$ which effectively acts as a lookup table (see example below). The embedding matrix is typically treated as any other model parameter in that it is randomly initialized and learned with gradient descent.
$\underbrace{\left[\begin{array}{lllllll}0 &amp; 0 &amp; 1 &amp; 0\end{array}\right]}}}$ with the embedding matrix $\boldsymbol{W<em _3="{3" _text="\text">{\text {One-hot vector }} \times \underbrace{\left[\begin{array}{cccccccc}5 &amp; 6 &amp; 2 &amp; 1 &amp; 4 \ 0 &amp; 1 &amp; 7 &amp; 3 &amp; 1\end{array}\right]}</em>}}=\underbrace{\left[\begin{array}{cccccccc}4 &amp; 8 &amp; 1 &amp; 6 &amp; 9\end{array}\right]<em w__i="w_{i">{\text {Word embedding } \boldsymbol{x}</em>$}}</p>
<h2>B. Encoding</h2>
<p>It would be ill-advised to learn an embedding of a value that is not inherently meaningful - whose interpretation depends entirely on the context. Instead, one should use a deterministic transformation without any parameter that is called an encoding.</p>
<p>Once more, let us consider the process id. Neural networks take as input a vector of numerical values. Therefore one may provide the actual pid as input. It is, however, a best practice to normalize the input vector to mitigate numerical instabilities, help training, and improve the model performance. Since the pid is not inherently meaningful in general ${ }^{4}$, any bijection from the argument space to a small interval such as $[0,1]$ or $[-1,1]$ works well. The simplest solution would be to map the pid uniformly to real values between $[0,1]$. In practice, the number of distinct pid within a trace varies and is often unknown beforehand.</p>
<p>A practical way to encode a numerical value is to apply the cosine function. Indeed, the codomain is $[-1,1]$, and the function requires no knowledge about the distribution or the extremum of the input variable. The cosine function is not, however, a bijection. As a result, collisions may occur: two different values assigned to the same encoding. Consider $x=$ 1 and $x^{\prime}=1+4 \pi$ :</p>
<p>$$
\cos (1)=\cos (1+4 \pi)
$$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The number of collisions may be reduced by dividing $x$ by an appropriately large number which effectively controls the period of the cosine function. Note that if the denominator is too small, collisions may still occur.</p>
<p>$\cos(1/2)=\cos((1+4\pi)/2)$</p>
<p>If the denominator is too large, the encodings will be extremely close, hence difficult for a model to distinguish.</p>
<p>$\cos(1/1000)$ $\approx 0.9999995$
$\cos((1+4\pi)/1000)$ $\approx 0.99991$</p>
<p>Instead, the denominator should be equal to the estimated maximum value that $x$ can take.</p>
<p>The number of collisions may be further reduced by applying multiple cosine functions with different periods. In that case, the encoding is a vector comprising the output of each cosine function. In other words, the output of every cosine function is concatenated into a vector which is the encoding.</p>
<p>Our approach relies on the encoding proposed by Vaswani et al. [6] which leverages an alternation of cosine and sine functions with an increasing denominator. More formally, the encoding of a numerical value $x$ is a vector $\boldsymbol{pe}_{x}$ of dimension $d$ whose $j$-th value is given either by equation 1 or 2 depending on whether $j$ is even ($j=2i$) or odd ($j=2i+1$), respectively.</p>
<p>$pe_{x,2i}$ $=\sin(x/10000^{2i/d})$
$pe_{x,2i+1}$ $=\cos(x/10000^{2i/d})$ (1)</p>
<p>As the authors underlined, there exists a linear relation between the $\boldsymbol{pe}<em x_k="x+k">{x}$ and $\boldsymbol{pe}</em>$, which they hypothesized should facilitate learning. Figure 5 illustrates the encoding.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Encoding of the value $x=80$ using Vaswani et al. [6] formula and a dimension $d=4$.</p>
<h3>C. Addition or Concatenation</h3>
<p>Let us now investigate how to combine the arguments embedding and encoding into a single event representation. The two most common approaches are the addition and the concatenation.</p>
<p>One may describe the addition of two vectors $\boldsymbol{x}$ and $\boldsymbol{y}$ as the translation of a point $\boldsymbol{x}$ by a vector $\boldsymbol{y}$ – or equally a point $\boldsymbol{y}$ by a vector $\boldsymbol{x}$. Let us consider the system call name and the argument entry which has two possible values, “entry” and “exit”. Furthermore, let us consider the system call name embedding as a point and the entry embedding as a vector. The addition effectively shifts the system call name embedding depending on whether the event corresponds to the start or the end of the system call execution. As illustrated by figure 6, the relation has been explicitly modelled in the same space as the embedding of the system call name, which simplifies their visualization and interpretation.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. The embedding of the system call names “open” and “close”. The green and red dashed lines represent the explicitly modelled “entry” and “exit” relations, respectively. The blue dotted line represents the implicitly modelled relation between the two system call names.</p>
<p>The addition preserves the dimension, which may be too small to store all the information, thus creating a bottleneck. Instead, the concatenation allows combining vectors without such a bottleneck. Indeed, the dimension of the resulting vector is the sum of the dimensions of the concatenated vectors. That may, however, be a drawback if the model size scales with the input dimension as larger models are computationally expensive to train and prone to overfitting. One may mitigate the overfitting by collecting a sufficiently large dataset.</p>
<p>Although the embedding visualization is outside of the scope of this work, we believe interesting to model the system call name, the argument entry, and the argument ret in the same space. One may gain insights into the system by investigating the relations between those vectors. Therefore, only those values will be added, and the remaining arguments will be concatenated. Note that it would be ill-advised to add an encoding to an embedding since the former is not inherently meaningful.</p>
<h3>D. Event Representation</h3>
<p>Figure 7 illustrates the computational graph of the event representation. For the call-related arguments, the embedding of the sysname, the entry, and the ret are added. Note that the return value is simplified to either “success” if the numerical value is greater or equal to zero, or “failure” otherwise. For the process-related arguments, the procname embedding is concatenated with the pid and tid encodings. For the time-related argument, the timestamp is converted from nanoseconds to microseconds and is encoded. Finally, the representation of each category of arguments is concatenated.</p>
<p>Neural networks take numerical values as input that may be arranged as vectors, matrices, or, more generally, tensors. In the case of traces, the network’s input is typically a sequence of vectors corresponding to the events. Such vectors may be the one-hot encoding of system call names, or better, their embedding. The proposed approach outputs a vectorial representation of the event with its arguments; therefore, it applies to most deep learning models.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7. Computational graph of the event representation. Blue rounded rectangles represent the arguments. Green rectangles indicate that the transformation is learned (embedding), and the parametrization is noted next to the incoming arrow. White rectangle indicates that the transformation is not learned (encoding, addition, or concatenation).</p>
<p>The proposed event representation is non-contextual: a system call with its arguments will have the same representation regardless of the other trace events. Some tasks greatly benefit from a contextual representation which may be obtained with a Transformer trained on the masked language model objective [7]. Although such a model has been evaluated, contextual representations are outside the scope of this paper.</p>
<h2>IV. DATA COLLECTION</h2>
<p>Over the years, many tracing datasets have been explored; however, most of them are not publicly available. Consequently, the now-obsolete UNM [24] and KDD98 [25] datasets are still widely used [2]. Those datasets were collected more than two decades ago and are clearly not representative of modern systems anymore. Therefore, they should not be used to evaluate recent approaches. In 2013, Murtaza et al. [2] and Creech and Hu [26] addressed this issue by introducing two new datasets: FirefoxDS and ADFA-LD, respectively. Unfortunately, the former is unavailable, and the system call arguments were omitted from the latter.</p>
<p>As indicated by Brown et al. [1], increasing the size of language models greatly improves their performance regardless of the task. As larger models require more data to be properly trained, the dataset must not only be modern but also massive. To the best of our knowledge, no massive and modern datasets comprising the system call arguments are publicly available. To that extent, we propose to generate such a dataset using requests. A request is a task delimited by specific start and end events. Examples include database queries, micro-services, and application functions. Notably, web requests have been extensively studied in the literature as they are ubiquitous. We introduce a methodology to generate a massive dataset of web request traces using a simple client-server framework (see Figure 8). The source code and the dataset are publicly available on GitHub and Zenodo, respectively.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8. Client-server framework. The client and the server are two distinct physical machines that communicate over the network. The server may be executing other software while handling a request which is considered to be noise from a request point of view.</p>
<h3>A. Methodology</h3>
<p>On the client-side, a benchmark tool is used to send many concurrent requests to the server via the hypertext transfer protocol (HTTP). We chose wrk2, an open-source multithreaded equivalent of the Apache benchmark, as it guarantees a constant throughput load with an accuracy up to 99.9999% for sufficiently long runs. Moreover, wrk2 yields a latency summary which allows extracting statistics about the dataset without processing it.</p>
<p>On the server-side, a web server handles the client requests and communicates with a database to retrieve the necessary information. For the web server, we chose Apache2 for its omnipresence and its modularity. Indeed, Apache2 is the most</p>
<p><sup>5</sup>https://github.com/qfournier/syscall_args</p>
<p><sup>6</sup>https://zenodo.org/record/4091287#.X4hhGNjpNQI</p>
<p><sup>7</sup>https://github.com/giltene/wrk2</p>
<p>popular web server since 1996, and its vast community has developed many optional modules, including app servers and database connection managers. For the database, we chose MySQL for its ease of use and performance. MySQL is filled with the Sakila Sample Database which includes an author table comprising ids, first names, and last names. Finally, PHP was installed as an Apache module to query the database.</p>
<p>One may be interested in simulating different behaviours such as slow or abnormal requests. In order to increase the likelihood of such requests, the server must be overloaded, which is done by restricting the amount or speed of the resources (CPU, memory, network, and disk). Consequently, Apache2 is deployed in a virtual machine using Virtual Box.</p>
<p>Physical servers often execute multiple tasks simultaneously. Since our server was dedicated, Firefox was automatically and randomly called from the console to take screenshots of random Wikipedia pages. The monitoring tools htop and bmon were also running in separate terminals. This allows creating a load on the CPU, the disk, and the network, as well as generating random events in the trace which adds variability.</p>
<p>In this work, we focus on the server-side since it is the source of most delays. A single trace is collected during the entire benchmark, therefore containing many individual requests. Depending on the task at hand, one may consider the whole trace as a single sequence or individual requests as separate sequences. Several tracers are available; however, the Linux Tracing Toolkit: next generation (LTTng) [27] is often the prefered choice given its lightweight and rapidity. Although only some system calls arguments are considered in this work, all arguments have been collected in order to have a complete view of the system.</p>
<h2>B. Dataset Analysis</h2>
<p>The server was deployed on a virtual machine with two cores from an Intel Core i7-8700 (up to 4.6 GHz), 1 Gb of DDR4 RAM, and an NVME SSD. The operating system was Ubuntu 18.04. Different throughputs were used to simulate different usages: idle, low, medium, and high. High usage means that the server is barely able to handle requests in real-time and that some end up with a timeout. Note that the training set and the test set were collected separately using different throughputs to avoid any overlap.</p>
<p>We collected around 250,000 requests which amount to almost 150 million system calls. One would likely have to collect a larger dataset in order to consider additional arguments such as the file descriptor without overfitting.</p>
<p>Figure 9 depicts the distribution of process names. As expected, the three most frequent processes are those that handle requests, namely the web server, its workers, and the database. Note that Firefox is responsible for issuing 13% of the system calls.</p>
<p>Figure 10 depicts the distribution of system call names. The two most frequent system calls are futex and poll which provide a method for waiting until a condition becomes true</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9. Distribution of process names
and until a file descriptor becomes available to perform IO operations, respectively. This behaviour is to be expected in networked multicore systems, especially when many remote requests are being handled concurrently.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 10. Distribution of system calls
For an equivalent analysis of Ciena's dataset, we refer the reader to the GitHub repository.</p>
<h2>V. COMPUTATIONAL EXPERIMENTS</h2>
<p>This section introduces the neural networks and objectives on which the system call arguments' impact was evaluated. The source code, hyperparameters, and trained models are publicly available on GitHub.</p>
<h2>A. Networks</h2>
<p>The first model evaluated is a deep unidirectional Long Short-Term Memory (LSTM) network with two hidden layers comprising 96 units. The vast majority of existing works to analyze traces apply an LSTM on system call names only [9, 11, 12, 13, 14]; therefore, those methods would require almost no modification to leverage the arguments with the proposed approach.</p>
<p>The second model evaluated is a Transformer. Transformers are highly parallelizable and are able to learn dependencies across an unlimited number of steps at the price of quadratic complexity. Many works address this limitation; however, since this paper aims to demonstrate the usefulness of the system call arguments, we settled for the vanilla Transformer</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>introduced by Vaswani et al. [6]. In particular, the network consists of six layers, each comprising 8 attention heads and a feedforward network with 128 units.</p>
<p>Contrary to LSTMs, Transformers are agnostic to the event position in the sequence. To solve this shortcoming, Vaswani et al. [6] injected positional knowledge by summing a positional encoding with the embedding. In our experiments, the model achieved better results when the positional encoding was concatenated to the event embedding.</p>
<p>The dimensions of the arguments embedding and encoding have a significant impact on the model performance; thus, various configurations were evaluated. The following dimensions performed well in all experiments: 32 for the sysname, entry, and ret, 16 for the procname, 4 for the pid and tid, and 8 for the timestamp. Consequently, the dimension of the whole event representation is 64. Note that the dimension of the positional encoding was equal to that of the timestamp.</p>
<h2>B. Objectives</h2>
<p>The first objective is the left-to-right language model (LM), which predicts the conditional probability of the next system call name given the previous system calls. The chain rule allows computing the joint probability of the whole sequence, that is, its likelihood, and therefore may be used to detect changes in the system behaviour, intrusions, and anomalies. Notably, Kim et al. [12] used language modelling for hostbased intrusion detection.</p>
<p>The second objective is the masked language model (MLM), which independently estimates the probability of masked words given the rest of the sequence. The more events are masked, the less context is available, and the more difficult is the training. In practice, MLM is often used to pre-train neural networks, and it has been shown to improve the model performance on downstream tasks, that is, the tasks of interest. Therefore, we evaluated the pre-trained model on LM in a zero-shot manner and determined that masking 25% of the events performed reasonably well on both datasets (see Table V). In particular, we followed the methodology of Devlin et al. [7] by randomly selecting 25% of the events, of which 80% were entirely masked, 10% were replaced by a random system call name with the same argument values, and 10% were left unchanged. Randomly replacing the selected events generates noise which increases the robustness of the model. The proportion of random events is identical to Devlin et al. [7] as their ablation study showed it worked well for pre-training. Note that masked LMs are technically not language models as they are not trained to maximize the joint probability of sentences. Figure 11 illustrates the masked language model.</p>
<h2>C. Data</h2>
<p>Due to memory constraints on the graphics processing unit (GPU), the models must be trained on small sequences. Therefore, the traces were split into non-overlapping sequences of 256 events. Note that those sequences do not correspond to requests. One would need to implement the proposed approach</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Fig. 11. Masked language model. Events in green have been randomly selected, and system call names in red are the predictions independently considered.</p>
<p>with a lower computational complexity model in order to process whole requests as they usually contain thousands of events.</p>
<p>The first dataset studied has been introduced in Section IV and comprises 318,674 training sequences and 258,190 test sequences. The second dataset has been collected by Ciena on pre-production servers executing proprietary software and comprises 190,924 training sequences and 64,628 test sequences. Although smaller, this dataset is designed to be representative of a real-world use case.</p>
<p>A quarter of each test set was randomly selected to create a validation set on which the hyperparameters were fine-tuned, and the model was evaluated at train time for early stopping.</p>
<h2>D. Results</h2>
<p>For each combination of datasets, objectives, and neural networks, two event representations have been compared: the system call name without any argument (none) and with every argument as described in Figure 7 (all).</p>
<p>Arguments may affect the performance differently; however, the computational cost of evaluating the impact of each argument, or worse, each combination of arguments, is prohibitive in practice. Instead, we evaluated the global impact of three groups of arguments: call-related (entry and ret), process-related (procname, pid, and tid), and time-related (timestamp).</p>
<p>Because the arguments embedding and encoding are concatenated, considering additional arguments increases the event representation dimension, which also increases the model size. On one side, the additional information allows the network to be larger without overfitting; hence one may see the increase in size as a byproduct of the arguments. On the other side, one may argue that a larger model only considering the system call name would perform better. In order to test these hypotheses, a compensated model considering no argument is evaluated (none cmp.). The dimension of the sysname embedding is increased from 32 to 64, which is the event representation's dimension when all the arguments are considered.</p>
<p>The model performance was measured in terms of cross-entropy (lower is better) and top-1 accuracy (higher is better). The cross-entropy is a measure of the difference between two distributions, in our case, the model output and the label. In the usual case of one-hot labels, the cross-entropy is defined as the</p>
<p>negative logarithm of the correct event's predicted probability. The top-1 accuracy is the percentage of correct predictions, where a prediction is the system call name with the highest predicted probability. Results are detailed in Table II.</p>
<p>In every experiment, the models that consider all the arguments achieved the lowest cross-entropy and the highest accuracy. The compensated models perform on par or better than their smaller counterpart; however, they are systematically outperformed by the models considering all the arguments. These results indicate that the increase in performance is not only due to the increase in model size but also to the additional arguments. Therefore, the arguments must contain useful information for language modelling tasks. Interestingly, the masked language model objective benefits more from callrelated arguments than process-related ones.</p>
<p>The time-related argument has a negligible impact on the LSTMs; consequently, the temporality must be of little use for the left-to-right language model objective. Nonetheless, Transformers appear to benefit from the timestamp and, as a result, an ablation study of the timestamp and the position was conducted to quantify their impact. The results shown in table III reveal that timestamp does increase the performance over a model without any arguments, although not as much as the position, which indicates that Transformers are able to leverage the redundancy of the positional information embedded in the timestamp. However, with an equal number of parameters, a model considering only the position performs on par or better than one considering both values. Such behaviour is to be expected since the positional information in the timestamp is harder to extract. One may be tempted to dismiss the timestamp; however, it should be noted that some downstream tasks, including latency detection, may greatly benefit from the timestamp.</p>
<p>As shown in Table IV, the computational overhead imposed by the additional arguments was negligible compared to the overall training cost, making the proposed approach suitable for real-world applications. This is to be expected as the embedding is simply a matrix multiplication, and the encoding is only a small number of cosine and sine functions.</p>
<h2>VI. Threats to Validity</h2>
<p>The main threat to validity is the limited scope of the evaluation. Indeed, the approach has only been evaluated on two unsupervised language modelling tasks due to the lack of a publicly available dataset comprising the system call arguments. To mitigate this limitation, we provide the source code as well as the trained models for researchers and practitioners to evaluate our approach to their task.</p>
<p>The second threat to validity is the simplicity of the environment on which our dataset was collected. Consequently, the dataset may not represent real-world use cases and may not reflect the approach's actual benefit. This limitation is addressed by evaluating the two objectives on a second dataset collected by Ciena on pre-production servers. Additionally, our dataset is unlabelled. Consequently, it is challenging to use for supervised tasks such as anomaly detection. To alleviate this
shortcoming, we provide a tutorial and the scripts required to generate the dataset such that users can produce their own labels.</p>
<p>Finally, although the proposed approach's computational overhead is negligible, neural networks still require powerful GPUs to be trained. The models' average training time described in Table II was less than 2 hours, with the slowest model taking about 5 hours on a single NVIDIA RTX2080Ti and two Intel Xeon Bronze 3104 1.7Ghz. Therefore, the experiments are easily reproducible with modest computational resources.</p>
<h2>VII. CONCLUDING REMARKS</h2>
<p>In practice, it is often difficult to determine whether a specific deep learning approach is beneficial for the task at hand. In this section, we answer two general questions to help researchers and practitioners decide whether to adopt the proposed method.</p>
<p>Do the arguments invariably increase the model performance? We argue that the performance either improves or remains the same, provided two conditions. Firstly, the model must be flexible enough to be able to extract relevant information from the arguments. Such a model would be able to leverage the additional information in order to make more informed predictions, hence more accurate. If the arguments only contain irrelevant information to the task, the performance cannot increase. It may, however, decrease. Indeed, larger inputs translate into larger embeddings, which increase the model size, hence its flexibility. As the model flexibility increases, it becomes prone to overfitting, that is, to learn peculiarities from the dataset that do not reflect real explanatory factors. It is well-known that the difference between training and generalization errors grows with the model flexibility and shrinks with the number of training examples [28]. Therefore, the second condition is that enough samples must be available to prevent the model from overfitting. Large datasets of traces are typically easy to obtain, so the amount of data is not a limiting factor. Notably, this work introduced a methodology to generate a massive dataset of requests. Furthermore, many techniques such as dropout [29], batch normalization [30], and early stopping [31] allow mitigating the overfitting that may occur. Nonetheless, the arguments should be omitted if one knows beforehand that the information is irrelevant to the task. For instance, if a single thread is recorded, the tid is constant and may be safely omitted.</p>
<p>In practice, how does one know when to consider additional arguments? It seems that one would need to estimate a priori (1) if the model is complex enough, (2) if the dataset is large enough, and (3) if the arguments could be relevant to the task at hand. Fortunately, in the case of neural networks, the models are generally more flexible than necessary - they contain many more parameters than there are samples in the dataset [31]. As explained above, collecting large datasets of traces is often trivial, and the risk of overfitting may be significantly reduced. When possible, we recommend considering the arguments and comparing the model with a baseline that does not.</p>
<p>TABLE II
IMPACT OF THREE CATEGORIES OF SYSTEM CALL ARGUMENTS (CROSS-ENTROPY/ACCURACY).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">none</th>
<th style="text-align: center;">none cmp.</th>
<th style="text-align: center;">time</th>
<th style="text-align: center;">call</th>
<th style="text-align: center;">process</th>
<th style="text-align: center;">all</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Web <br> Requests</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">$0.528 / 83.1$</td>
<td style="text-align: center;">$0.529 / 83.1$</td>
<td style="text-align: center;">$0.526 / 83.2$</td>
<td style="text-align: center;">$0.451 / 85.6$</td>
<td style="text-align: center;">$0.443 / 85.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">$0.609 / 80.3$</td>
<td style="text-align: center;">$0.506 / 83.3$</td>
<td style="text-align: center;">$0.599 / 80.6$</td>
<td style="text-align: center;">$0.489 / 84.3$</td>
<td style="text-align: center;">$0.452 / 85.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MLM</td>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">$0.535 / 81.7$</td>
<td style="text-align: center;">$0.485 / 82.8$</td>
<td style="text-align: center;">$0.524 / 81.8$</td>
<td style="text-align: center;">$0.400 / 87.2$</td>
<td style="text-align: center;">$0.423 / 85.0$</td>
</tr>
<tr>
<td style="text-align: center;">Ciena</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">$0.294 / 91.8$</td>
<td style="text-align: center;">$0.301 / 91.5$</td>
<td style="text-align: center;">$0.301 / 91.6$</td>
<td style="text-align: center;">$0.277 / 92.2$</td>
<td style="text-align: center;">$0.283 / 91.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">$0.323 / 90.4$</td>
<td style="text-align: center;">$0.292 / 91.3$</td>
<td style="text-align: center;">$0.310 / 90.8$</td>
<td style="text-align: center;">$0.290 / 91.5$</td>
<td style="text-align: center;">$0.271 / 91.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MLM</td>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">$0.285 / 90.8$</td>
<td style="text-align: center;">$0.264 / 91.3$</td>
<td style="text-align: center;">$0.270 / 91.2$</td>
<td style="text-align: center;">$0.202 / 94.0$</td>
<td style="text-align: center;">$0.245 / 91.8$</td>
</tr>
</tbody>
</table>
<p>TABLE III
IMPACT OF THE EVENT'S POSITION AND TIMESTAMP ENCODING DIMENSIONS ON THE TRANSFORMER WITHOUT ARGUMENTS (CROSS-ENTROPY/ACCURACY). A DIMENSION OF ZERO IS EQUIVALENT TO OMITTING THE ARGUMENT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">timestamp</th>
<th style="text-align: center;">position</th>
<th style="text-align: center;">Web Requests</th>
<th style="text-align: center;">Ciena</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.730 / 76.9$</td>
<td style="text-align: center;">$0.444 / 86.9$</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.661 / 78.4$</td>
<td style="text-align: center;">$0.337 / 89.8$</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$0.609 / 80.3$</td>
<td style="text-align: center;">$0.323 / 90.4$</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$0.599 / 80.6$</td>
<td style="text-align: center;">$\mathbf{0 . 3 1 0 / 9 0 . 8}$</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 7 / 8 0 . 9}$</td>
<td style="text-align: center;">$0.313 / \mathbf{9 0 . 8}$</td>
</tr>
</tbody>
</table>
<p>TABLE IV
AVERAGE EPOCHS TIME ( $\pm$ STD) IN MILLISECONDS OF THE TRANSFORMERS TRAINED ON THE WEB REQUESTS.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">LM</th>
<th style="text-align: center;">MLM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">none</td>
<td style="text-align: center;">$99.3( \pm 2.0)$</td>
<td style="text-align: center;">$232.2( \pm 8.2)$</td>
</tr>
<tr>
<td style="text-align: left;">none cmp.</td>
<td style="text-align: center;">$102.2( \pm 1.6)$</td>
<td style="text-align: center;">$232.8( \pm 5.3)$</td>
</tr>
<tr>
<td style="text-align: left;">time</td>
<td style="text-align: center;">$104.1( \pm 2.6)$</td>
<td style="text-align: center;">$228.5( \pm 3.8)$</td>
</tr>
<tr>
<td style="text-align: left;">call</td>
<td style="text-align: center;">$102.4( \pm 2.1)$</td>
<td style="text-align: center;">$227.0( \pm 6.6)$</td>
</tr>
<tr>
<td style="text-align: left;">process</td>
<td style="text-align: center;">$103.6( \pm 1.7)$</td>
<td style="text-align: center;">$234.3( \pm 6.9)$</td>
</tr>
<tr>
<td style="text-align: left;">all</td>
<td style="text-align: center;">$106.0( \pm 2.4)$</td>
<td style="text-align: center;">$238.5( \pm 4.5)$</td>
</tr>
</tbody>
</table>
<p>TABLE V
IMPACT OF THE PERCENTAGE OF SELECTED EVENTS FOR PRE-TRAINING THE TRANSFORMER WITH ALL ARGUMENTS AS EVALUATED ON LM (CROSS-ENTROPY/ACCURACY).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$p_{\text {mask }}$</th>
<th style="text-align: center;">Web Requests</th>
<th style="text-align: center;">Ciena</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">$3.826 / 54.6$</td>
<td style="text-align: center;">$1.738 / 80.1$</td>
</tr>
<tr>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">$3.881 / 55.7$</td>
<td style="text-align: center;">$1.641 / 80.2$</td>
</tr>
<tr>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">$\mathbf{3 . 3 1 4 / 5 6 . 7}$</td>
<td style="text-align: center;">$1.639 / 80.2$</td>
</tr>
<tr>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">$3.543 / 56.1$</td>
<td style="text-align: center;">$1.617 / \mathbf{8 0 . 4}$</td>
</tr>
<tr>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">$3.334 / 56.1$</td>
<td style="text-align: center;">$1.647 / \mathbf{8 0 . 4}$</td>
</tr>
<tr>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">$3.387 / 56.3$</td>
<td style="text-align: center;">$\mathbf{1 . 5 4 8 / 8 0 . 2}$</td>
</tr>
</tbody>
</table>
<p>In this work, we introduced a massive dataset of web requests and a general approach to learning a representation of the system call names along with their arguments. By leveraging the left-out information, we were able to systematically increase the performance of two neural networks on two language-modelling tasks at a negligible computational cost. Possible future works include extending the embedding to userspace events, applying the models to downstream tasks such as anomaly detection, and applying the embedding to the many previous works that rely on LSTMs.</p>
<h2>VIII. ACKNOWLEDGMENT</h2>
<p>We would like to gratefully acknowledge the Natural Sciences and Engineering Research Council of Canada (NSERC), Prompt, Ericsson, Ciena, and EffciOS for funding this project.</p>
<h2>REFERENCES</h2>
<p>[1] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCand lish, A. Radford, I. Sutskever, and D. Amodei, "Language Models are Few-Shot Learners," arXiv e-prints, p. arXiv:2005.14165, May 2020.
[2] S. S. Murtaza, W. Khreich, A. Hamou-Lhadj, and M. Couture, "A host-based anomaly detection approach by representing system calls as states of kernel modules," in 2013 IEEE 24th International Symposium on Software Reliability Engineering, ISSRE 2013, 2013.
[3] H. Nemati, S. V. Azhari, and M. R. Dagenais, "Host hypervisor trace mining for virtual machine workload characterization," in 2019 IEEE International Conference on Cloud Engineering (IC2E), 2019, pp. 102-112.
[4] Q. Fournier, N. Ezzati-jivan, D. Aloise, and M. R. Dagenais, "Automatic cause detection of performance problems in web applications," in 2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW), 2019, pp. 398-405.
[5] S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural Comput., vol. 9, no. 8, pp. 1735-</p>
<p>1780, Nov. 1997, http://www.bioinf.jku.at/publications/ older/2604.pdf.
[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is All you Need," in Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. Curran Associates, Inc., 2017, pp. 5998-6008.
[7] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding," CoRR, vol. abs/1810.04805, 2018.
[8] K. Asmitha and P. Vinod, "A machine learning approach for linux malware detection," Proceedings of the 2014 International Conference on Issues and Challenges in Intelligent Computing Techniques, ICICT 2014, pp. 825830, 2014.
[9] F. Song, A. Stiegler, Y. Diao, J. Read, and A. Bifet, "EXAD: A System for Explainable Anomaly Detection on Big Data Traces," in ICDMW 2018 - IEEE International Conference on Data Mining Workshops, Singapore, Singapore, Nov. 2018.
[10] Z. Xu, X. Yu, Y. Feng, J. Hu, Z. Tari, and F. Han, "A multi-module anomaly detection scheme based on system call prediction," in 2013 IEEE 8th Conference on Industrial Electronics and Applications (ICIEA), 2013, pp. 1376-1381.
[11] M. Dymshits, B. Myara, and D. Tolpin, "Process Monitoring on Sequences of System Call Count Vectors," CoRR, vol. abs/1707.0, 2017.
[12] G. Kim, H. Yi, J. Lee, Y. Paek, and S. Yoon, "LSTMBased System-Call Language Modeling and Robust Ensemble Method for Designing Host-Based Intrusion Detection Systems," CoRR, vol. abs/1611.0, 2016.
[13] S. Nedelkoski, J. Cardoso, and O. Kao, "Anomaly detection from system tracing data using multimodal deep learning," in 2019 IEEE 12th International Conference on Cloud Computing (CLOUD), July 2019, pp. 179-186.
[14] O. M. Ezeme, Q. Mahmoud, and A. Azim, "A Framework for Anomaly Detection in Time-Driven and EventDriven Processes using Kernel Traces," IEEE Transactions on Knowledge and Data Engineering, p. 1, 2020.
[15] I. Sutskever, O. Vinyals, and Q. V. Le, "Sequence to sequence learning with neural networks," in Advances in Neural Information Processing Systems 27, Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2014, pp. 3104-3112.
[16] S. Lv, J. Wang, Y. Yang, and J. Liu, "Intrusion prediction with system-call sequence-to-sequence model," CoRR, vol. abs/1808.01717, 2018.
[17] K. Cho, B. van Merrienboer, Ç. Gülçehre, F. Bougares, H. Schwenk, and Y. Bengio, "Learning phrase representations using RNN encoder-decoder for statistical machine translation," CoRR, vol. abs/1406.1078, 2014.
[18] J. Cheng, L. Dong, and M. Lapata, "Long short-term memory-networks for machine reading," CoRR, vol. abs/1601.06733, 2016.
[19] D. Bahdanau, K. Cho, and Y. Bengio, "Neural Machine Translation by Jointly Learning to Align and Translate," CoRR, vol. abs/1409.0473, 2014.
[20] A. Brown, A. Tuor, B. Hutchinson, and N. Nichols, "Recurrent neural network attention mechanisms for interpretable system log anomaly detection," CoRR, vol. abs/1803.04967, 2018.
[21] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le, and R. Salakhutdinov, "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context," CoRR, vol. abs/1901.0, 2019.
[22] G. Tandon and P. K. Chan, "On The Learning Of System Call Attributes For Host-based Anomaly Detection," International Journal on Artificial Intelligence Tools, vol. 15, no. 06, pp. 875-892, 2006.
[23] ——, "Learning Useful System Call Attributes for Anomaly Detection," in Proceedings of the Eighteenth International Florida Artificial Intelligence Research Society Conference, Clearwater Beach, Florida, {USA}, 2005, pp. 405-411.
[24] S. Forrest, S. A. Hofmeyr, A. Somayaji, and T. A. Longstaff, "A sense of self for unix processes," in Proceedings 1996 IEEE Symposium on Security and Privacy, 1996, pp. 120-128.
[25] D. Fried, I. Graf, J. Haines, K. Kendall, D. Mcclung, D. Weber, S. Webster, D. Wyschogrod, R. Cunningham, and M. Zissman, "Evaluating intrusion detection systems: The 1998 darpa off-line intrusion detection evaluation," vol. 2, 052000.
[26] G. Creech and J. Hu, "Generation of a new ids test dataset: Time to retire the kdd collection." in WCNC. IEEE, 2013, pp. 4487-4492.
[27] M. Desnoyers and M. R. Dagenais, "The lttng tracer: A low impact performance and behavior monitor for gnu/linux," in OLS (Ottawa Linux Symposium) 2006, 2006, pp. 209-224.
[28] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016, http://www.deeplearningbook.org.
[29] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, "Dropout: a simple way to prevent neural networks from overfitting." Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929-1958, 2014.
[30] S. Ioffe and C. Szegedy, "Batch normalization: Accelerating deep network training by reducing internal covariate shift," 2015.
[31] R. Caruana, S. Lawrence, and C. L. Giles, "Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping," in NIPS, 2000.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ https://github.com/qfournier/syscall_args&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
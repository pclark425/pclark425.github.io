<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-722 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-722</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-722</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1" target="_blank">Understanding Black-box Predictions via Influence Functions</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This paper uses influence functions — a classic technique from robust statistics — to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction.</p>
                <p><strong>Paper Abstract:</strong> How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e722.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e722.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>theory-vs-implementation (convexity/differentiability)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between theoretical assumptions (twice-differentiable, strictly convex loss) and practical implementations on non-convex/non-differentiable models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies a systematic gap between the assumptions used to derive influence-function formulas (twice-differentiability and strict convexity of the empirical risk) and real-world model training (non-convex objectives, non-convergence, and non-PD Hessians), and describes how this gap is detected, quantified, and mitigated in implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>influence-function-based model-diagnosis pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A pipeline that computes influence quantities (I_up,loss, I_up,params, I_pert,loss) to trace test predictions back to training points by differentiating through training via Hessian-vector products and H^{-1} approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>theoretical assumptions in paper (methods/derivation)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>model training code and influence estimation implementation (TensorFlow/Theano auto-grad + HVP/CG or stochastic estimator)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>assumption vs implementation mismatch (non-convexity and non-differentiability)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Influence functions are derived assuming the empirical risk minimizer is a global minimizer and that the loss is twice-differentiable and strictly convex so H is PD. In practice the paper applies influence computations to neural networks trained with SGD (non-convex, possibly non-converged) where H may have negative eigenvalues and gradients at the reported parameter vector are nonzero. This is a mismatch between the theory described in natural language and the actual implementation context.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model objective / training procedure (assumptions underlying influence computation) and numerical stability when computing H^{-1}</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical validation: compare influence-based predictions to leave-one-out retraining (retraining experiments) and inspect Hessian eigenvalues; check correlation between predicted and actual change in test loss</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Quantitative comparison of predicted influence (-1/n * I_up,loss) vs actual leave-one-out change in test loss; Pearson's correlation coefficients reported (e.g., Pearson's R = 0.86 for a large non-converged CNN experiment). Plots of predicted vs actual loss differences were used.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Despite the mismatch, influence estimates remained meaningful and correlated with retraining results after mitigation (damping, smoothing). For a non-converged CNN with damping (λ=0.01) the predicted vs actual changes had Pearson's R = 0.86; without mitigation results would be unreliable. Thus the gap can degrade fidelity of influence estimates but does not make them unusable if mitigated.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across multiple experiments in the paper (logistic regression, CNN, SVM); the problem is typical when applying theoretical derivations to deep models and early-stopped training.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Theory requires strict convexity and smoothness; practical deep-learning objectives violate these assumptions and optimization often yields non-converged local points with non-PD Hessians.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Form a convex quadratic approximation around current parameters by adding damping (λ I) to the empirical Hessian (equivalently L2 regularization) and compute influence using the damped Hessian; when non-differentiable components exist, replace them with smooth approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective in experiments: damping (λ=0.01) produced high correlation (Pearson's R = 0.86) between predicted and actual changes for a non-converged CNN; smoothing non-differentiable losses (see separate entry) also recovered accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Black-box Predictions via Influence Functions', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e722.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e722.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>nondiff-loss-gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between non-differentiable loss descriptions and differentiable code needed for influence computation (hinge/ReLU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that influence-function formulas relying on second derivatives fail on non-differentiable losses (e.g., hinge loss), and shows that computing influence on smoothed approximations recovers accurate predictions of retraining behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>influence estimation on models with non-differentiable losses</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedure for computing influence measures for models whose losses are not twice-differentiable, by substituting smoothed loss approximations and computing HVPs via auto-grad.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>mathematical description of loss differentiability in methods/assumptions</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>model code (SVM objective) and influence computation code using smoothed loss in implementation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing differentiability / smoothing required</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The natural-language/theoretical derivation assumes availability of ∇θL and ∇^2_θL. For piecewise-linear losses (hinge) or models with ReLUs, second derivatives are zero or undefined at kinks, so the quadratic (second-order) approximation used by influence overestimates influence and gives inaccurate predictions when computed naively on the non-differentiable loss.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>loss function specification / mathematical assumptions vs implemented gradient/Hessian computations</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical mismatch: compare predicted influence computed from raw hinge derivatives with actual leave-one-out retraining outcomes (plots showed large overestimation).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Pearson correlation between predicted and actual leave-one-out loss changes before and after smoothing; example: using SmoothHinge(s, 0.001) gave Pearson's R = 0.95 versus poor correlation with raw hinge.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Naive influence computed on non-differentiable hinge substantially overestimated influence and failed to predict retraining effects; after smoothing, predictions matched retraining closely (R up to 0.95), restoring diagnostic utility.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Relevant whenever non-differentiable components (hinge, ReLU) are present; demonstrated specifically on linear SVM (hinge) experiments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Use of non-differentiable loss components in models contradicts the differentiability assumptions used by the influence derivation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Replace non-differentiable losses with smooth approximations (e.g., SmoothHinge(s, t) = t log(1 + exp((1-s)/t))) with small t for influence calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Highly effective in the experiments: SmoothHinge with t=0.001 yielded Pearson's R = 0.95 between predicted and actual leave-one-out changes; correlation remained high across a range of t values until smoothing became too large.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning (SVMs, neural nets with ReLU-like components)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Black-box Predictions via Influence Functions', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e722.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e722.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>hessian-inversion-gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Computational gap: exact Hessian inversion vs scalable H^{-1} approximations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Theoretical influence formulas require H^{-1}, which is computationally infeasible for large models; the paper deploys and validates two scalable approximations (conjugate gradients with HVPs and a stochastic Taylor-based estimator) and quantifies their accuracy and computational tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>influence computation implementation (H^{-1} estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Implementation of influence estimation relying on Hessian-vector products computed via auto-differentiation and approximate inversion via conjugate gradients or a stochastic Taylor-expansion estimator that samples individual training-Hessians.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>algorithmic specification (compute H^{-1} v) in methods section</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Python implementations using auto-grad frameworks (TensorFlow/Theano) with HVPs, CG solver, and stochastic recursive estimator</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>computational approximation / scalability-driven implementation choice</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The closed-form influence expression requires forming and inverting a p×p Hessian (O(np^2 + p^3)), infeasible for deep models. Implementations therefore use implicit Hessian-vector products and iterative solvers (CG) or a stochastic Taylor-recursive estimator that samples per-example Hessians, producing approximations to H^{-1} v with much lower cost but introducing estimator variance and approximation error.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>numerical linear algebra / solver used to compute H^{-1} v (influence inner loop)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Complexity analysis and empirical validation: compare influence values produced by exact inversion (when feasible) vs CG and stochastic estimators; measure how well approximations predict leave-one-out retraining outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Empirical runtime complexity statements and accuracy measured by correlation/plots between approximate-influence predictions and actual retraining results; examples: stochastic estimator with r=10, t=5000 iterations produced accurate estimates (plots in paper); r=1 gave noisier but still useful results; reported cost O(n p + r t p).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Using approximations made influence computation tractable on large models and datasets while retaining predictive fidelity; with appropriate settings (e.g., r=10, t=5000) stochastic estimator matched full CG results and accurately identified the most influential points. Approximation errors can increase noise but did not prevent useful diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High: required in all large-scale experiments (MNIST logistic regression with p≈7,840, CNNs, ImageNet-derived tasks); stochastic estimator was used often because CG that touches all n can be slow.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>High dimensionality (large p) and large datasets (large n) make direct Hessian formation/inversion computationally infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use implicit HVPs (Pearlmutter) with iterative solvers: (1) Conjugate gradient to solve H x = v using HVPs; (2) stochastic Taylor-series-based recursive estimator that draws single-example Hessians to build unbiased estimates of H^{-1} v and averages multiple repeats to reduce variance.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Empirically effective: stochastic estimator (r=10, t=5000) gave results comparable to exact CG in experiments; r=1 still identified most influential points though noisier. The paper reports that choosing r t = O(n) gives accurate results empirically.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / numerical optimization</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Black-box Predictions via Influence Functions', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e722.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e722.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>euclidean-vs-influence-misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between naive Euclidean-nearest-neighbors explanations and influence-based explanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that natural-language heuristics like 'nearest neighbors in Euclidean/pixel space' do not align with the actual training points that most influence a model's prediction; influence functions reveal harmful/supporting examples that Euclidean distance misses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>model explanation heuristics vs influence-based explanation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Comparison between simple nearest-neighbor (Euclidean) heuristics used as natural-language explanations of 'which training points matter' and influence-function-derived rankings of influential training points for given test predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>common heuristic explanation in literature (nearest-neighbors / Euclidean similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>analysis scripts computing Euclidean distances and influence scores from model parameters</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different algorithmic variant / incomplete heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>A commonly stated natural-language explanation method is to show a test point's nearest neighbors in input (pixel) space as the training points that explain the prediction. The paper shows this heuristic is misaligned with the model's learned decision boundary: points that are close in pixel space can be harmful or irrelevant, while influence functions (which include train loss and H^{-1} weighting) identify non-obvious training examples that actually increase or decrease test loss.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>explainability / debugging stage (selection of exemplar training points to explain predictions)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical comparison: scatter plots of I_up,loss vs Euclidean inner product and visual inspection of examples where the methods disagree (including showing a harmful training image with the same label that Euclidean distance would rank as similar).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative and quantitative comparison via scatter plots and identification of mismatches; demonstration that influence picks up harmful influential examples that Euclidean ranking misses (plots in Fig.1 and Fig.4).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Relying on Euclidean nearest neighbors for explanations can mislead debugging and interpretation (e.g., misidentifying harmful examples), while influence-based explanations provide more accurate signals for tasks like debugging, dataset cleaning, and understanding model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in image classification experiments (MNIST logistic regression and Inception/SVM comparisons); implies the heuristic is commonly unreliable for complex models.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Euclidean similarity does not account for label, model loss curvature, or the parameter-space resistance to changes (H^{-1}); it is an incomplete specification of influence.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use influence functions which incorporate gradients and the inverse empirical Hessian to rank training points by true effect on test loss.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective in experiments—influence functions successfully highlighted harmful or helpful training points that Euclidean heuristics failed to identify; used successfully in debugging and mislabeled-example detection tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / model interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Black-box Predictions via Influence Functions', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e722.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e722.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>infinitesimal-approximation-gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Approximation gap between infinitesimal upweighting derivation and finite leave-one-out retraining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The derived influence quantities assume infinitesimal upweighting (ε→0); the paper assesses how well this linear approximation predicts finite changes such as removing a point (ε = -1/n) by comparing to actual leave-one-out retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>influence-based leave-one-out approximation workflow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedure to approximate the effect of removing a training point on test loss by computing dθ/dε at ε=0 (influence) and scaling by -1/n, then validating by retraining without the point.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>mathematical counterfactual derivation ('what if we remove this training point?')</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>retraining experiments (leave-one-out) and influence-computation code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>linearization approximation (infinitesimal -> finite)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Influence functions compute the derivative at ε=0 and use a linear approximation to predict finite parameter changes corresponding to removing a point (ε = -1/n). The paper identifies and measures discrepancies between the linear prediction and actual retraining results.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>counterfactual estimation step (approximation error when moving from local derivative to finite dataset change)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Direct empirical comparison: compute -1/n * I_up,loss for many training points and compare to the actual change in test loss after leave-one-out retraining (plots in Fig.2 and Fig.3).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Scatter plots and correlations between predicted and actual change in test loss for the most influential points; logistic regression on MNIST showed close matching, CNN experiments and smoothed SVM experiments quantified with Pearson's R (examples: strong correlations reported; SVM needed smoothing to match).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>When assumptions hold (or with mitigations), the linear approximation accurately predicts leave-one-out effects for influential points; when violated (e.g., non-differentiable hinge without smoothing), the approximation overestimates effects. Thus the approximation is usable but must be validated per setting.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Validated across multiple experiments (logistic regression, CNN, SVM) — accurate in many practical settings after mitigation (smoothing/damping).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Using a first-order linearization (derivative at ε=0) to predict finite (non-infinitesimal) changes introduces approximation error; the magnitude depends on curvature and nonlinearity of the loss landscape.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Empirically validate influence predictions by comparing against leave-one-out retraining for a sample of points; use smoothing/damping to improve linear approximation validity.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Empirical validation showed good effectiveness: predictions closely matched retraining for logistic regression; smoothing improved SVM performance to Pearson's R = 0.95; CNN with damping achieved R = 0.86.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / diagnostics and reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Black-box Predictions via Influence Functions', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Second order stochastic optimization in linear time <em>(Rating: 2)</em></li>
                <li>Fast exact multiplication by the Hessian <em>(Rating: 2)</em></li>
                <li>"Influence sketching": Finding influential samples in large-scale regressions <em>(Rating: 2)</em></li>
                <li>Poisoning attacks against support vector machines <em>(Rating: 2)</em></li>
                <li>Model selection in kernel based regression using the influence function <em>(Rating: 1)</em></li>
                <li>Debugging machine learning models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-722",
    "paper_id": "paper-08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "theory-vs-implementation (convexity/differentiability)",
            "name_full": "Mismatch between theoretical assumptions (twice-differentiable, strictly convex loss) and practical implementations on non-convex/non-differentiable models",
            "brief_description": "The paper identifies a systematic gap between the assumptions used to derive influence-function formulas (twice-differentiability and strict convexity of the empirical risk) and real-world model training (non-convex objectives, non-convergence, and non-PD Hessians), and describes how this gap is detected, quantified, and mitigated in implementations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "influence-function-based model-diagnosis pipeline",
            "system_description": "A pipeline that computes influence quantities (I_up,loss, I_up,params, I_pert,loss) to trace test predictions back to training points by differentiating through training via Hessian-vector products and H^{-1} approximations.",
            "nl_description_type": "theoretical assumptions in paper (methods/derivation)",
            "code_implementation_type": "model training code and influence estimation implementation (TensorFlow/Theano auto-grad + HVP/CG or stochastic estimator)",
            "gap_type": "assumption vs implementation mismatch (non-convexity and non-differentiability)",
            "gap_description": "Influence functions are derived assuming the empirical risk minimizer is a global minimizer and that the loss is twice-differentiable and strictly convex so H is PD. In practice the paper applies influence computations to neural networks trained with SGD (non-convex, possibly non-converged) where H may have negative eigenvalues and gradients at the reported parameter vector are nonzero. This is a mismatch between the theory described in natural language and the actual implementation context.",
            "gap_location": "model objective / training procedure (assumptions underlying influence computation) and numerical stability when computing H^{-1}",
            "detection_method": "empirical validation: compare influence-based predictions to leave-one-out retraining (retraining experiments) and inspect Hessian eigenvalues; check correlation between predicted and actual change in test loss",
            "measurement_method": "Quantitative comparison of predicted influence (-1/n * I_up,loss) vs actual leave-one-out change in test loss; Pearson's correlation coefficients reported (e.g., Pearson's R = 0.86 for a large non-converged CNN experiment). Plots of predicted vs actual loss differences were used.",
            "impact_on_results": "Despite the mismatch, influence estimates remained meaningful and correlated with retraining results after mitigation (damping, smoothing). For a non-converged CNN with damping (λ=0.01) the predicted vs actual changes had Pearson's R = 0.86; without mitigation results would be unreliable. Thus the gap can degrade fidelity of influence estimates but does not make them unusable if mitigated.",
            "frequency_or_prevalence": "Observed across multiple experiments in the paper (logistic regression, CNN, SVM); the problem is typical when applying theoretical derivations to deep models and early-stopped training.",
            "root_cause": "Theory requires strict convexity and smoothness; practical deep-learning objectives violate these assumptions and optimization often yields non-converged local points with non-PD Hessians.",
            "mitigation_approach": "Form a convex quadratic approximation around current parameters by adding damping (λ I) to the empirical Hessian (equivalently L2 regularization) and compute influence using the damped Hessian; when non-differentiable components exist, replace them with smooth approximations.",
            "mitigation_effectiveness": "Effective in experiments: damping (λ=0.01) produced high correlation (Pearson's R = 0.86) between predicted and actual changes for a non-converged CNN; smoothing non-differentiable losses (see separate entry) also recovered accuracy.",
            "domain_or_field": "machine learning / deep learning",
            "reproducibility_impact": true,
            "uuid": "e722.0",
            "source_info": {
                "paper_title": "Understanding Black-box Predictions via Influence Functions",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "nondiff-loss-gap",
            "name_full": "Mismatch between non-differentiable loss descriptions and differentiable code needed for influence computation (hinge/ReLU)",
            "brief_description": "The paper documents that influence-function formulas relying on second derivatives fail on non-differentiable losses (e.g., hinge loss), and shows that computing influence on smoothed approximations recovers accurate predictions of retraining behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "influence estimation on models with non-differentiable losses",
            "system_description": "Procedure for computing influence measures for models whose losses are not twice-differentiable, by substituting smoothed loss approximations and computing HVPs via auto-grad.",
            "nl_description_type": "mathematical description of loss differentiability in methods/assumptions",
            "code_implementation_type": "model code (SVM objective) and influence computation code using smoothed loss in implementation",
            "gap_type": "missing differentiability / smoothing required",
            "gap_description": "The natural-language/theoretical derivation assumes availability of ∇θL and ∇^2_θL. For piecewise-linear losses (hinge) or models with ReLUs, second derivatives are zero or undefined at kinks, so the quadratic (second-order) approximation used by influence overestimates influence and gives inaccurate predictions when computed naively on the non-differentiable loss.",
            "gap_location": "loss function specification / mathematical assumptions vs implemented gradient/Hessian computations",
            "detection_method": "Empirical mismatch: compare predicted influence computed from raw hinge derivatives with actual leave-one-out retraining outcomes (plots showed large overestimation).",
            "measurement_method": "Pearson correlation between predicted and actual leave-one-out loss changes before and after smoothing; example: using SmoothHinge(s, 0.001) gave Pearson's R = 0.95 versus poor correlation with raw hinge.",
            "impact_on_results": "Naive influence computed on non-differentiable hinge substantially overestimated influence and failed to predict retraining effects; after smoothing, predictions matched retraining closely (R up to 0.95), restoring diagnostic utility.",
            "frequency_or_prevalence": "Relevant whenever non-differentiable components (hinge, ReLU) are present; demonstrated specifically on linear SVM (hinge) experiments in the paper.",
            "root_cause": "Use of non-differentiable loss components in models contradicts the differentiability assumptions used by the influence derivation.",
            "mitigation_approach": "Replace non-differentiable losses with smooth approximations (e.g., SmoothHinge(s, t) = t log(1 + exp((1-s)/t))) with small t for influence calculations.",
            "mitigation_effectiveness": "Highly effective in the experiments: SmoothHinge with t=0.001 yielded Pearson's R = 0.95 between predicted and actual leave-one-out changes; correlation remained high across a range of t values until smoothing became too large.",
            "domain_or_field": "machine learning (SVMs, neural nets with ReLU-like components)",
            "reproducibility_impact": true,
            "uuid": "e722.1",
            "source_info": {
                "paper_title": "Understanding Black-box Predictions via Influence Functions",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "hessian-inversion-gap",
            "name_full": "Computational gap: exact Hessian inversion vs scalable H^{-1} approximations",
            "brief_description": "Theoretical influence formulas require H^{-1}, which is computationally infeasible for large models; the paper deploys and validates two scalable approximations (conjugate gradients with HVPs and a stochastic Taylor-based estimator) and quantifies their accuracy and computational tradeoffs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "influence computation implementation (H^{-1} estimation)",
            "system_description": "Implementation of influence estimation relying on Hessian-vector products computed via auto-differentiation and approximate inversion via conjugate gradients or a stochastic Taylor-expansion estimator that samples individual training-Hessians.",
            "nl_description_type": "algorithmic specification (compute H^{-1} v) in methods section",
            "code_implementation_type": "Python implementations using auto-grad frameworks (TensorFlow/Theano) with HVPs, CG solver, and stochastic recursive estimator",
            "gap_type": "computational approximation / scalability-driven implementation choice",
            "gap_description": "The closed-form influence expression requires forming and inverting a p×p Hessian (O(np^2 + p^3)), infeasible for deep models. Implementations therefore use implicit Hessian-vector products and iterative solvers (CG) or a stochastic Taylor-recursive estimator that samples per-example Hessians, producing approximations to H^{-1} v with much lower cost but introducing estimator variance and approximation error.",
            "gap_location": "numerical linear algebra / solver used to compute H^{-1} v (influence inner loop)",
            "detection_method": "Complexity analysis and empirical validation: compare influence values produced by exact inversion (when feasible) vs CG and stochastic estimators; measure how well approximations predict leave-one-out retraining outcomes.",
            "measurement_method": "Empirical runtime complexity statements and accuracy measured by correlation/plots between approximate-influence predictions and actual retraining results; examples: stochastic estimator with r=10, t=5000 iterations produced accurate estimates (plots in paper); r=1 gave noisier but still useful results; reported cost O(n p + r t p).",
            "impact_on_results": "Using approximations made influence computation tractable on large models and datasets while retaining predictive fidelity; with appropriate settings (e.g., r=10, t=5000) stochastic estimator matched full CG results and accurately identified the most influential points. Approximation errors can increase noise but did not prevent useful diagnostics.",
            "frequency_or_prevalence": "High: required in all large-scale experiments (MNIST logistic regression with p≈7,840, CNNs, ImageNet-derived tasks); stochastic estimator was used often because CG that touches all n can be slow.",
            "root_cause": "High dimensionality (large p) and large datasets (large n) make direct Hessian formation/inversion computationally infeasible.",
            "mitigation_approach": "Use implicit HVPs (Pearlmutter) with iterative solvers: (1) Conjugate gradient to solve H x = v using HVPs; (2) stochastic Taylor-series-based recursive estimator that draws single-example Hessians to build unbiased estimates of H^{-1} v and averages multiple repeats to reduce variance.",
            "mitigation_effectiveness": "Empirically effective: stochastic estimator (r=10, t=5000) gave results comparable to exact CG in experiments; r=1 still identified most influential points though noisier. The paper reports that choosing r t = O(n) gives accurate results empirically.",
            "domain_or_field": "machine learning / numerical optimization",
            "reproducibility_impact": true,
            "uuid": "e722.2",
            "source_info": {
                "paper_title": "Understanding Black-box Predictions via Influence Functions",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "euclidean-vs-influence-misalignment",
            "name_full": "Mismatch between naive Euclidean-nearest-neighbors explanations and influence-based explanations",
            "brief_description": "The paper documents that natural-language heuristics like 'nearest neighbors in Euclidean/pixel space' do not align with the actual training points that most influence a model's prediction; influence functions reveal harmful/supporting examples that Euclidean distance misses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "model explanation heuristics vs influence-based explanation",
            "system_description": "Comparison between simple nearest-neighbor (Euclidean) heuristics used as natural-language explanations of 'which training points matter' and influence-function-derived rankings of influential training points for given test predictions.",
            "nl_description_type": "common heuristic explanation in literature (nearest-neighbors / Euclidean similarity)",
            "code_implementation_type": "analysis scripts computing Euclidean distances and influence scores from model parameters",
            "gap_type": "different algorithmic variant / incomplete heuristic",
            "gap_description": "A commonly stated natural-language explanation method is to show a test point's nearest neighbors in input (pixel) space as the training points that explain the prediction. The paper shows this heuristic is misaligned with the model's learned decision boundary: points that are close in pixel space can be harmful or irrelevant, while influence functions (which include train loss and H^{-1} weighting) identify non-obvious training examples that actually increase or decrease test loss.",
            "gap_location": "explainability / debugging stage (selection of exemplar training points to explain predictions)",
            "detection_method": "Empirical comparison: scatter plots of I_up,loss vs Euclidean inner product and visual inspection of examples where the methods disagree (including showing a harmful training image with the same label that Euclidean distance would rank as similar).",
            "measurement_method": "Qualitative and quantitative comparison via scatter plots and identification of mismatches; demonstration that influence picks up harmful influential examples that Euclidean ranking misses (plots in Fig.1 and Fig.4).",
            "impact_on_results": "Relying on Euclidean nearest neighbors for explanations can mislead debugging and interpretation (e.g., misidentifying harmful examples), while influence-based explanations provide more accurate signals for tasks like debugging, dataset cleaning, and understanding model behavior.",
            "frequency_or_prevalence": "Observed in image classification experiments (MNIST logistic regression and Inception/SVM comparisons); implies the heuristic is commonly unreliable for complex models.",
            "root_cause": "Euclidean similarity does not account for label, model loss curvature, or the parameter-space resistance to changes (H^{-1}); it is an incomplete specification of influence.",
            "mitigation_approach": "Use influence functions which incorporate gradients and the inverse empirical Hessian to rank training points by true effect on test loss.",
            "mitigation_effectiveness": "Effective in experiments—influence functions successfully highlighted harmful or helpful training points that Euclidean heuristics failed to identify; used successfully in debugging and mislabeled-example detection tasks.",
            "domain_or_field": "machine learning / model interpretability",
            "reproducibility_impact": true,
            "uuid": "e722.3",
            "source_info": {
                "paper_title": "Understanding Black-box Predictions via Influence Functions",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "infinitesimal-approximation-gap",
            "name_full": "Approximation gap between infinitesimal upweighting derivation and finite leave-one-out retraining",
            "brief_description": "The derived influence quantities assume infinitesimal upweighting (ε→0); the paper assesses how well this linear approximation predicts finite changes such as removing a point (ε = -1/n) by comparing to actual leave-one-out retraining.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "influence-based leave-one-out approximation workflow",
            "system_description": "Procedure to approximate the effect of removing a training point on test loss by computing dθ/dε at ε=0 (influence) and scaling by -1/n, then validating by retraining without the point.",
            "nl_description_type": "mathematical counterfactual derivation ('what if we remove this training point?')",
            "code_implementation_type": "retraining experiments (leave-one-out) and influence-computation code",
            "gap_type": "linearization approximation (infinitesimal -&gt; finite)",
            "gap_description": "Influence functions compute the derivative at ε=0 and use a linear approximation to predict finite parameter changes corresponding to removing a point (ε = -1/n). The paper identifies and measures discrepancies between the linear prediction and actual retraining results.",
            "gap_location": "counterfactual estimation step (approximation error when moving from local derivative to finite dataset change)",
            "detection_method": "Direct empirical comparison: compute -1/n * I_up,loss for many training points and compare to the actual change in test loss after leave-one-out retraining (plots in Fig.2 and Fig.3).",
            "measurement_method": "Scatter plots and correlations between predicted and actual change in test loss for the most influential points; logistic regression on MNIST showed close matching, CNN experiments and smoothed SVM experiments quantified with Pearson's R (examples: strong correlations reported; SVM needed smoothing to match).",
            "impact_on_results": "When assumptions hold (or with mitigations), the linear approximation accurately predicts leave-one-out effects for influential points; when violated (e.g., non-differentiable hinge without smoothing), the approximation overestimates effects. Thus the approximation is usable but must be validated per setting.",
            "frequency_or_prevalence": "Validated across multiple experiments (logistic regression, CNN, SVM) — accurate in many practical settings after mitigation (smoothing/damping).",
            "root_cause": "Using a first-order linearization (derivative at ε=0) to predict finite (non-infinitesimal) changes introduces approximation error; the magnitude depends on curvature and nonlinearity of the loss landscape.",
            "mitigation_approach": "Empirically validate influence predictions by comparing against leave-one-out retraining for a sample of points; use smoothing/damping to improve linear approximation validity.",
            "mitigation_effectiveness": "Empirical validation showed good effectiveness: predictions closely matched retraining for logistic regression; smoothing improved SVM performance to Pearson's R = 0.95; CNN with damping achieved R = 0.86.",
            "domain_or_field": "machine learning / diagnostics and reproducibility",
            "reproducibility_impact": true,
            "uuid": "e722.4",
            "source_info": {
                "paper_title": "Understanding Black-box Predictions via Influence Functions",
                "publication_date_yy_mm": "2017-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Second order stochastic optimization in linear time",
            "rating": 2
        },
        {
            "paper_title": "Fast exact multiplication by the Hessian",
            "rating": 2
        },
        {
            "paper_title": "\"Influence sketching\": Finding influential samples in large-scale regressions",
            "rating": 2
        },
        {
            "paper_title": "Poisoning attacks against support vector machines",
            "rating": 2
        },
        {
            "paper_title": "Model selection in kernel based regression using the influence function",
            "rating": 1
        },
        {
            "paper_title": "Debugging machine learning models",
            "rating": 1
        }
    ],
    "cost": 0.016412999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Understanding Black-box Predictions via Influence Functions</h1>
<p>Pang Wei Koh ${ }^{1}$ Percy Liang ${ }^{1}$</p>
<h4>Abstract</h4>
<p>How can we explain the predictions of a blackbox model? In this paper, we use influence functions - a classic technique from robust statistics - to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visuallyindistinguishable training-set attacks.</p>
<h2>1. Introduction</h2>
<p>A key question often asked of machine learning systems is "Why did the system make this prediction?" We want models that are not just high-performing but also explainable. By understanding why a model does what it does, we can hope to improve the model (Amershi et al., 2015), discover new science (Shrikumar et al., 2017), and provide end-users with explanations of actions that impact them (Goodman \&amp; Flaxman, 2016).</p>
<p>However, the best-performing models in many domains e.g., deep neural networks for image and speech recognition (Krizhevsky et al., 2012) - are complicated, blackbox models whose predictions seem hard to explain. Work on interpreting these black-box models has focused on understanding how a fixed model leads to particular predictions, e.g., by locally fitting a simpler model around the test</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>point (Ribeiro et al., 2016) or by perturbing the test point to see how the prediction changes (Simonyan et al., 2013; Li et al., 2016b; Datta et al., 2016; Adler et al., 2016). These works explain the predictions in terms of the model, but how can we explain where the model came from?</p>
<p>In this paper, we tackle this question by tracing a model's predictions through its learning algorithm and back to the training data, where the model parameters ultimately derive from. To formalize the impact of a training point on a prediction, we ask the counterfactual: what would happen if we did not have this training point, or if the values of this training point were changed slightly?</p>
<p>Answering this question by perturbing the data and retraining the model can be prohibitively expensive. To overcome this problem, we use influence functions, a classic technique from robust statistics (Hampel, 1974) that tells us how the model parameters change as we upweight a training point by an infinitesimal amount. This allows us to "differentiate through the training" to estimate in closed-form the effect of a variety of training perturbations.</p>
<p>Despite their rich history in statistics, influence functions have not seen widespread use in machine learning; to the best of our knowledge, the work closest to ours is Wojnowicz et al. (2016), which introduced a method for approximating a quantity related to influence in generalized linear models. One obstacle to adoption is that influence functions require expensive second derivative calculations and assume model differentiability and convexity, which limits their applicability in modern contexts where models are often non-differentiable, non-convex, and highdimensional. We address these challenges by showing that we can efficiently approximate influence functions using second-order optimization techniques (Pearlmutter, 1994; Martens, 2010; Agarwal et al., 2016), and that they remain accurate even as the underlying assumptions of differentiability and convexity degrade.</p>
<p>Influence functions capture the core idea of studying models through the lens of their training data. We show that they are a versatile tool that can be applied to a wide variety of seemingly disparate tasks: understanding model behavior, debugging models, detecting dataset errors, and creating visually-indistinguishable adversarial training examples that can flip neural network test predictions, the training set analogue of Goodfellow et al. (2015).</p>
<h2>2. Approach</h2>
<p>Consider a prediction problem from some input space $\mathcal{X}$ (e.g., images) to an output space $\mathcal{Y}$ (e.g., labels). We are given training points $z_{1}, \ldots, z_{n}$, where $z_{i}=\left(x_{i}, y_{i}\right) \in$ $\mathcal{X} \times \mathcal{Y}$. For a point $z$ and parameters $\theta \in \Theta$, let $L(z, \theta)$ be the loss, and let $\frac{1}{n} \sum_{i=1}^{n} L\left(z_{i}, \theta\right)$ be the empirical risk. The empirical risk minimizer is given by $\hat{\theta} \stackrel{\text { def }}{=} \arg \min <em i="1">{\theta \in \Theta} \frac{1}{n} \sum</em>$ Assume that the empirical risk is twice-differentiable and strictly convex in $\theta$; in Section 4 we explore relaxing these assumptions.}^{n} L\left(z_{i}, \theta\right) .{ }^{1</p>
<h3>2.1. Upweighting a training point</h3>
<p>Our goal is to understand the effect of training points on a model's predictions. We formalize this goal by asking the counterfactual: how would the model's predictions change if we did not have this training point?</p>
<p>Let us begin by studying the change in model parameters due to removing a point $z$ from the training set. Formally, this change is $\hat{\theta}<em -z="-z">{-z}-\hat{\theta}$, where $\hat{\theta}</em> \arg \min } \stackrel{\text { def }}{=<em z__i="z_{i">{\theta \in \Theta} \sum</em>, \theta\right)$. However, retraining the model for each removed $z$ is prohibitively slow.} \neq z} L\left(z_{i</p>
<p>Fortunately, influence functions give us an efficient approximation. The idea is to compute the parameter change if $z$ were upweighted by some small $\epsilon$, giving us new parameters $\hat{\theta}<em _Theta="\Theta" _in="\in" _theta="\theta">{\epsilon, z} \stackrel{\text { def }}{=} \arg \min </em>$ is given by} \frac{1}{n} \sum_{i=1}^{n} L\left(z_{i}, \theta\right)+\epsilon L(z, \theta)$. A classic result (Cook \&amp; Weisberg, 1982) tells us that the influence of upweighting $z$ on the parameters $\hat{\theta</p>
<p>$$
\mathcal{I}<em _epsilon_="\epsilon," z="z">{\text {up,params }}(z) \stackrel{\text { def }}{=} \frac{d \hat{\theta}</em>)\right.
$$}}{d \epsilon} \left\lvert\, \epsilon=0=-H_{\hat{\theta}}^{-1} \nabla_{\theta} L(z, \hat{\theta</p>
<p>where $H_{\hat{\theta}} \stackrel{\text { def }}{=} \frac{1}{n} \sum_{i=1}^{n} \nabla_{\theta}^{2} L\left(z_{i}, \hat{\theta}\right)$ is the Hessian and is positive definite (PD) by assumption. In essence, we are forming a quadratic approximation to the empirical risk around $\hat{\theta}$ and take a single Newton step; see appendix A for a derivation. Since removing a point $z$ is the same as upweighting it by $\epsilon=-\frac{1}{n}$, we can linearly approximate the parameter change due to removing $z$ without retraining the model by computing $\hat{\theta}<em _text="\text" _up_params="{up,params">{-z}-\hat{\theta} \approx-\frac{1}{n} \mathcal{I}</em>(z)$.
Next, we apply the chain rule to measure how upweighting $z$ changes functions of $\hat{\theta}$. In particular, the influence of upweighting $z$ on the loss at a test point $z_{\text {test }}$ again has a closed-form expression:}</p>
<p>$$
\begin{aligned}
\mathcal{I}<em _test="{test" _text="\text">{\text {up,loss }}\left(z, z</em>}}\right) &amp; \left.\stackrel{\text { def }}{=} \frac{d L\left(z_{\text {test }}, \hat{\theta<em _epsilon="0">{\epsilon, z}\right)}{d \epsilon}\right|</em> \
&amp; =\left.\nabla_{\theta} L\left(z_{\text {test }}, \hat{\theta}\right)^{\top} \frac{d \hat{\theta}<em _epsilon="0">{\epsilon, z}}{d \epsilon}\right|</em> \
&amp; =-\nabla_{\theta} L\left(z_{\text {test }}, \hat{\theta}\right)^{\top} H_{\hat{\theta}}^{-1} \nabla_{\theta} L(z, \hat{\theta})
\end{aligned}
$$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>2.2. Perturbing a training input</h3>
<p>Let us develop a finer-grained notion of influence by studying a different counterfactual: how would the model's predictions change if a training input were modified?</p>
<p>For a training point $z=(x, y)$, define $z_{\delta} \stackrel{\text { def }}{=}(x+\delta, y)$. Consider the perturbation $z \mapsto z_{\delta}$, and let $\hat{\theta}<em _delta="\delta">{z</em>},-z}$ be the empirical risk minimizer on the training points with $z_{\delta}$ in place of $z$. To approximate its effects, define the parameters resulting from moving $\epsilon$ mass from $z$ onto $z_{\delta}: \hat{\theta<em _delta="\delta">{\epsilon, z</em>$ $\arg \min },-z} \stackrel{\text { def }}{=<em i="1">{\theta \in \Theta} \frac{1}{n} \sum</em>, \theta\right)-\epsilon L(z, \theta)$. An analogous calculation to (1) yields:}^{n} L\left(z_{i}, \theta\right)+\epsilon L\left(z_{\delta</p>
<p>$$
\begin{aligned}
\left.\frac{d \hat{\theta}<em _delta="\delta">{\epsilon, z</em>\right|},-z}}{d \epsilon<em _text="\text" _up_params="{up,params">{\epsilon=0} &amp; =\mathcal{I}</em>}}\left(z_{\delta}\right)-\mathcal{I<em _hat_theta="\hat{\theta">{\text {up,params }}(z) \
&amp; =-H</em>)\right)
\end{aligned}
$$}}^{-1}\left(\nabla_{\theta} L\left(z_{\delta}, \hat{\theta}\right)-\nabla_{\theta} L(z, \hat{\theta</p>
<p>As before, we can make the linear approximation $\hat{\theta}<em _delta="\delta">{z</em>},-z}-$ $\hat{\theta} \approx \frac{1}{n}\left(\mathcal{I<em _delta="\delta">{\text {up,params }}\left(z</em>}\right)-\mathcal{I<em _delta="\delta">{\text {up,params }}(z)\right)$, giving us a closedform estimate of the effect of $z \mapsto z</em>$. This is particularly useful for working with discrete data (e.g., in NLP) or with discrete label changes.
If $x$ is continuous and $\delta$ is small, we can further approximate (3). Assume that the input domain $\mathcal{X} \subseteq \mathbb{R}^{d}$, the parameters $\Theta \subseteq \mathbb{R}^{p}$, and $L$ is differentiable in $\theta$ and $x$. As $|\delta| \rightarrow 0, \nabla_{\theta} L\left(z_{\delta}, \hat{\theta}\right)-\nabla_{\theta} L(z, \hat{\theta}) \approx\left[\nabla_{x} \nabla_{\theta} L(z, \hat{\theta})\right] \delta$, where $\nabla_{x} \nabla_{\theta} L(z, \hat{\theta}) \in \mathbb{R}^{p \times d}$. Substituting into (3),}$ on the model. Analogous equations also apply for changes in $y$. While influence functions might appear to only work for infinitesimal (therefore continuous) perturbations, it is important to note that this approximation holds for arbitrary $\delta$ : the $\epsilon$ upweighting scheme allows us to smoothly interpolate between $z$ and $z_{\delta</p>
<p>$$
\left.\frac{d \hat{\theta}<em _delta="\delta">{\epsilon, z</em>\right|},-z}}{d \epsilon<em _hat_theta="\hat{\theta">{\epsilon=0} \approx-H</em>)\right] \delta
$$}}^{-1}\left[\nabla_{x} \nabla_{\theta} L(z, \hat{\theta</p>
<p>We thus have $\hat{\theta}<em _delta="\delta">{z</em>)\right] \delta$. Differentiating w.r.t. $\delta$ and applying the chain rule gives us},-z}-\hat{\theta} \approx-\frac{1}{n} H_{\hat{\theta}}^{-1}\left[\nabla_{x} \nabla_{\theta} L(z, \hat{\theta</p>
<p>$$
\begin{aligned}
\mathcal{I}<em _test="{test" _text="\text">{\text {pert,loss }}\left(z, z</em>}}\right) &amp; \stackrel{\text { def }}{=}\left.\nabla_{\delta} L\left(z_{\text {test }}, \hat{\theta<em _delta="\delta">{z</em>\right)\right|},-z<em _theta="\theta">{\delta=0} \
&amp; =-\nabla</em>)
\end{aligned}
$$} L\left(z_{\text {test }}, \hat{\theta}\right)^{\top} H_{\hat{\theta}}^{-1} \nabla_{x} \nabla_{\theta} L(z, \hat{\theta</p>
<p>$\left[\mathcal{I}<em _test="{test" _text="\text">{\text {pert,loss }}\left(z, z</em>}}\right)\right] \delta$ tells us the approximate effect that $z \mapsto$ $z+\delta$ has on the loss at $z_{\text {test }}$. By setting $\delta$ in the direction of $\mathcal{I<em _test="{test" _text="\text">{\text {pert,loss }}\left(z, z</em>}}\right)^{\top}$, we can construct local perturbations of $z$ that maximally increase the loss at $z_{\text {test }}$. In Section 5.2, we will use this to construct training-set attacks. Finally, we note that $\mathcal{I<em _test="{test" _text="\text">{\text {pert,loss }}\left(z, z</em>$.}}\right)$ can help us identify the features of $z$ that are most responsible for the prediction on $z_{\text {test }</p>
<h3>2.3. Relation to Euclidean distance</h3>
<p>To find the training points most relevant to a test point, it is common to look at its nearest neighbors in Euclidean</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Components of influence. (a) What is the effect of the training loss and $H_{\hat{\theta}}^{-1}$ terms in $\mathcal{I}<em _text="\text" _up_loss="{up,loss">{\text {up,loss }}$ ? Here, we plot $\mathcal{I}</em>}}$ against variants that are missing these terms and show that they are necessary for picking up the truly influential training points. For these calculations, we use logistic regression to distinguish 1's from 7's in MNIST (LeCun et al., 1998), picking an arbitrary test point $z_{\text {test }}$; similar trends hold across other test points. Green dots are train images of the same label as the test image (7) while red dots are 1's. Left: Without the train loss term, we overestimate the influence of many training points: the points near the horizontal axis should have $\mathcal{I<em _hat_theta="\hat{\theta">{\text {up,loss }}$ close to 0 , but instead have high influence when we remove the train loss term). Mid: Without $H</em> x$, which fails to accurately capture influence; the scatter plot deviates quite far from the diagonal. (b) The test image and a harmful training image with the same label. To the model, they look very different, so the presence of the training image makes the model think that the test image is less likely to be a 7. The Euclidean inner product does not pick up on these less intuitive, but important, harmful influences.
space (e.g., Ribeiro et al. (2016)); if all points have the same norm, this is equivalent to choosing $x$ with the largest $x \cdot x_{\text {test }}$. For intuition, we compare this to $\mathcal{I}}}^{-1}$, all green training points are helpful (removing each point increases test loss) and all red points are harmful (removing each point decreases test loss). This is because $\forall x, x \succeq 0$ (all pixel values are positive), so $x \cdot x_{\text {test }} \geq 0$, but it is incorrect: many harmful training points actually share the same label as $z_{\text {test }}$. See panel (b). Right: Without training loss or $H_{\hat{\theta}}^{-1}$, what is left is the scaled Euclidean inner product $y_{\text {test }} y \cdot \sigma\left(-y_{\text {test }} \theta^{\top} \cdot x_{\text {test }}\right) x_{\text {loss }}^{\top<em _test="{test" _text="\text">{\text {up,loss }}\left(z, z</em>\right)$ on a logistic regression model and show that influence is much more accurate at accounting for the effect of training.
Let $p(y \mid x)=\sigma\left(y \theta^{\top} x\right)$, with $y \in{-1,1}$ and $\sigma(t)=$ $\frac{1}{1+\exp (-t)}$. We seek to maximize the probability of the training set. For a training point $z=(x, y), L(z, \theta)=$ $\log \left(1+\exp \left(-y \theta^{\top} x\right)\right), \nabla_{\theta} L(z, \theta)=-\sigma\left(-y \theta^{\top} x\right) y x$, and $H_{\theta}=\frac{1}{n} \sum_{i=1}^{n} \sigma\left(\theta^{\top} x_{i}\right) \sigma\left(-\theta^{\top} x_{i}\right) x_{i} x_{i}^{\top}$. From (2), $\mathcal{I}}<em _test="{test" _text="\text">{\text {up,loss }}\left(z, z</em>\right)$ is:}</p>
<p>$$
-y_{\text {test }} y \cdot \sigma\left(-y_{\text {test }} \theta^{\top} x_{\text {test }}\right) \cdot \sigma\left(-y \theta^{\top} x\right) \cdot x_{\text {test }}^{\top} H_{\hat{\theta}}^{-1} x
$$</p>
<p>We highlight two key differences from $x \cdot x_{\text {test }}$. First, $\sigma\left(-y \theta^{\top} x\right)$ gives points with high training loss more influence, revealing that outliers can dominate the model parameters. Second, the weighted covariance matrix $H_{\hat{\theta}}^{-1}$ measures the "resistance" of the other training points to the removal of $z$; if $\nabla_{\theta} L(z, \hat{\theta})$ points in a direction of little variation, its influence will be higher since moving in that direction will not significantly increase the loss on other training points. As we show in Fig 1, these differences mean that influence functions capture the effect of model training much more accurately than nearest neighbors.</p>
<h2>3. Efficiently calculating influence</h2>
<p>There are two challenges to efficiently computing $\mathcal{I}<em _test="{test" _text="\text">{\text {up,loss }}\left(z, z</em>\right)$ operations, which
is too expensive for models like deep neural networks with millions of parameters. Second, we need to calculate $\mathcal{I}}}\right)=-\nabla_{\theta} L\left(z_{\text {test }}, \hat{\theta}\right)^{\top} H_{\hat{\theta}}^{-1} \nabla_{\theta} L(z, \hat{\theta})$. First, it requires forming and inverting $H_{\hat{\theta}}=\frac{1}{n} \sum_{i=1}^{n} \nabla_{\theta}^{2} L\left(z_{i}, \hat{\theta}\right)$, the Hessian of the empirical risk. With $n$ training points and $\theta \in \mathbb{R}^{p}$, this requires $O\left(n p^{2}+p^{3<em i="i">{\text {up,loss }}\left(z</em>$.}, z_{\text {test }}\right)$ across all training points $z_{i</p>
<p>The first problem is well-studied in second-order optimization. The idea is to avoid explicitly computing $H_{\hat{\theta}}^{-1}$; instead, we use implicit Hessian-vector products (HVPs) to efficiently approximate $s_{\text {test }} \stackrel{\text { def }}{=} H_{\hat{\theta}}^{-1} \nabla_{\theta} L\left(z_{\text {test }}, \hat{\theta}\right)$ and then compute $\mathcal{I}<em _test="{test" _text="\text">{\text {up,loss }}\left(z, z</em>$.}}\right)=-s_{\text {test }} \cdot \nabla_{\theta} L(z, \hat{\theta})$. This also solves the second problem: for each test point of interest, we can precompute $s_{\text {test }}$ and then efficiently compute $-s_{\text {test }} \cdot \nabla_{\theta} L\left(z_{i}, \hat{\theta}\right)$ for each training point $z_{i</p>
<p>We discuss two techniques for approximating $s_{\text {test }}$, both relying on the fact that the HVP of a single term in $H_{\hat{\theta}}$, $\left[\nabla_{\theta}^{2} L\left(z_{i}, \hat{\theta}\right)\right] v$, can be computed for arbitrary $v$ in the same time that $\nabla_{\theta} L\left(z_{i}, \hat{\theta}\right)$ would take, which is typically $O(p)$ (Pearlmutter, 1994).</p>
<p>Conjugate gradients (CG). The first technique is a standard transformation of matrix inversion into an optimization problem. Since $H_{\hat{\theta}} \succ 0$ by assumption, $H_{\hat{\theta}}^{-1} v \equiv$ $\arg \min <em _hat_theta="\hat{\theta">{t}\left{t^{\top} H</em>$. While an exact solution takes $p$ CG iterations, in practice we can get a good approximation with fewer iterations; see Martens (2010) for more details.}} t-v^{\top} t\right}$. We can solve this with CG approaches that only require the evaluation of $H_{\hat{\theta}} t$, which takes $O(n p)$ time, without explicitly forming $H_{\hat{\theta}</p>
<p>Stochastic estimation. With large datasets, standard CG can be slow; each iteration still goes through all $n$ training points. We use a method developed by Agarwal et al. (2016) to get an estimator that only samples a single point per iteration, which results in significant speedups.</p>
<p>Dropping the $\hat{\theta}$ subscript for clarity, let $H_{j}^{-1} \stackrel{\text { def }}{=} \sum_{i=0}^{j}(I-$ $H)^{i}$, the first $j$ terms in the Taylor expansion of $H^{-1}$. Rewrite this recursively as $H_{j}^{-1}=I+(I-H) H_{j-1}^{-1}$. From the validity of the Taylor expansion, $H_{j}^{-1} \rightarrow H^{-1}$ as $j \rightarrow \infty .^{2}$ The key is that at each iteration, we can substitute the full $H$ with a draw from any unbiased (and faster-to-compute) estimator of $H$ to form $\hat{H}<em j="j">{j}$. Since $\mathbb{E}\left[\hat{H}</em>$.}^{-1}\right]=$ $H_{j}^{-1}$, we still have $\mathbb{E}\left[\hat{H}_{j}^{-1}\right] \rightarrow H^{-1</p>
<p>In particular, we can use $\nabla_{\hat{\theta}}^{2} L\left(z_{i}, \hat{\theta}\right)$, for any $z_{i}$, as an unbiased estimator of $H$. This gives us the following procedure: uniformly sample $t$ points $z_{s_{1}}, \ldots, z_{s_{t}}$ from the training data; define $\hat{H}<em j="j">{0}^{-1} v=v$; and recursively compute $\hat{H}</em>}^{-1} v=v+\left(I-\nabla_{\hat{\theta}}^{2} L\left(z_{s_{j}}, \hat{\theta}\right)\right) \hat{H<em t="t">{j-1}^{-1} v$, taking $\hat{H}</em>$ stabilizes, and to reduce variance we repeat this procedure $r$ times and average results. Empirically, we found this significantly faster than CG.}^{-1} v$ as our final unbiased estimate of $H^{-1} v$. We pick $t$ to be large enough such that $\hat{H}_{t</p>
<p>We note that the original method of Agarwal et al. (2016) dealt only with generalized linear models, for which $\left[\nabla_{\hat{\theta}}^{2} L\left(z_{i}, \hat{\theta}\right)\right] v$ can be efficiently computed in $O(p)$ time. In our case, we rely on Pearlmutter (1994)'s more general algorithm for fast HVPs, described above, to achieve the same time complexity. ${ }^{3}$
With these techniques, we can compute $\mathcal{I}<em i="i">{\text {up,loss }}\left(z</em>}, z_{\text {test }}\right)$ on all training points $z_{i}$ in $O(n p+r t p)$ time; we show in Section 4.1 that empirically, choosing $r t=$ $O(n)$ gives accurate results. Similarly, we can compute $\mathcal{I<em i="i">{\text {pert,loss }}\left(z</em>\right)$ with the same HVP trick. These computations are easy to implement in auto-grad systems like TensorFlow (Abadi et al., 2015) and Theano (Theano D. Team, 2016), as users need only specify the loss; the rest is automatically handled.}, z_{\text {test }}\right)=-\frac{1}{n} \nabla_{\theta} L\left(z_{\text {test }}, \hat{\theta}\right)^{\top} H_{\hat{\theta}}^{-1} \nabla_{x} \nabla_{\theta} L\left(z_{i}, \hat{\theta}\right)$ with two matrix-vector products: we first compute $s_{\text {test }}$, then find $s_{\text {test }}^{\top} \nabla_{x} \nabla_{\theta} L\left(z_{i}, \hat{\theta</p>
<h2>4. Validation and extensions</h2>
<p>Recall that influence functions are asymptotic approximations of leave-one-out retraining under the assumptions that (i) the model parameters $\hat{\theta}$ minimize the empirical risk, and that (ii) the empirical risk is twice-differentiable and strictly convex. Here, we empirically show that influence functions are accurate approximations (Section 4.1) that</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Influence matches leave-one-out retraining. We arbitrarily picked a wrongly-classified test point $z_{\text {test }}$, but this trend held more broadly. These results are from 10-class MNIST. Left: For each of the 500 training points $z$ with largest $\left|\mathcal{I}<em _test="{test" _text="\text">{\text {up,loss }}\left(z, z</em>}}\right)\right|$, we plotted $-\frac{1}{n} \cdot \mathcal{I<em _test="{test" _text="\text">{\text {up,loss }}\left(z, z</em>$ for 30k steps.
provide useful information even when these assumptions are violated (Sections 4.2, 4.3).}}\right)$ against the actual change in test loss after removing that point and retraining. The inverse HVP was solved exactly with CG. Mid: Same, but with the stochastic approximation. Right: The same plot for a CNN, computed on the 100 most influential points with CG. For the actual difference in loss, we removed each point and retrained from $\hat{\theta</p>
<h3>4.1. Influence functions vs. leave-one-out retraining</h3>
<p>Influence functions assume that the weight on a training point is changed by an infinitesimally small $\epsilon$. To investigate the accuracy of using influence functions to approximate the effect of removing a training point and retraining, we compared $-\frac{1}{n} \mathcal{I}<em _test="{test" _text="\text">{\text {up,loss }}\left(z, z</em>}}\right)$ with $L\left(z_{\text {test }}, \hat{\theta<em _test="{test" _text="\text">{-z}\right)-$ $L\left(z</em>$ the predicted and actual changes matched closely (Fig 2-Left).
The stochastic approximation from Agarwal et al. (2016) was also accurate with $r=10$ repeats and $t=5,000$ iterations (Fig 2-Mid). Since each iteration only requires one HVP $\left[\nabla_{\hat{\theta}}^{2} L\left(z_{i}, \hat{\theta}\right)\right] v$, this runs quickly: in fact, we accurately estimated $H^{-1} v$ without even looking at every data point, since $n=55,000&gt;r t$. Surprisingly, even $r=1$ worked; while results were noisier, it was still able to identify the most influential points.}}, \hat{\theta}\right)$ (i.e., actually doing leave-one-out retraining). With a logistic regression model on 10-class MNIST, ${ }^{4</p>
<h3>4.2. Non-convexity and non-convergence</h3>
<p>In Section 2, we took $\hat{\theta}$ as the global minimum. In practice, if we obtain our parameters $\hat{\theta}$ by running SGD with early stopping or on non-convex objectives, $\hat{\theta} \neq \hat{\theta}$. As a result, $H_{\hat{\theta}}$ could have negative eigenvalues. We show that influence functions on $\hat{\theta}$ still give meaningful results in practice.
Our approach is to form a convex quadratic approximation of the loss around $\hat{\theta}$, i.e., $\hat{L}(z, \theta)=L(z, \hat{\theta})+$ $\nabla L(z, \hat{\theta})^{\top}(\theta-\hat{\theta})+\frac{1}{2}(\theta-\hat{\theta})^{\top}\left(H_{\hat{\theta}}+\lambda I\right)(\theta-\hat{\theta})$. Here, $\lambda$ is a damping term that we add if $H_{\hat{\theta}}$ has negative eigenvalues;</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Smooth approximations to the hinge loss. (a) By varying $t$, we can approximate the hinge loss with arbitrary accuracy: the green and blue lines are overlaid on top of each other. (b) Using a random, wrongly-classified test point, we compared the predicted vs. actual differences in loss after leave-one-out retraining on the 100 most influential training points. A similar trend held for other test points. The SVM objective is to minimize $0.005|w|<em i="i">{2}^{2}+\frac{1}{n} \sum</em>=0.91$.} \operatorname{Hinge}\left(y_{i} w^{\top} x_{i}\right)$. Left: Influence functions were unable to accurately predict the change, overestimating its magnitude considerably. Mid: Using SmoothHinge $(\cdot, 0.001)$ let us accurately predict the change in the hinge loss after retraining. Right: Correlation remained high over a wide range of $t$, though it degrades when $t$ is too large. When $t=0.001$, Pearson's $\mathrm{R}=0.95$; when $t=0.1$, Pearson's $\mathrm{R</p>
<p>This corresponds to adding $L_{2}$ regularization on the parameters. We then calculate $\mathcal{I}_{\text {up,loss }}$ using $\tilde{L}$. If $\tilde{\theta}$ is close to a local minimum, this is correlated with the result of taking a Newton step from $\tilde{\theta}$ after removing $\epsilon$ weight from $z$ (see appendix B).</p>
<p>We checked the behavior of $\mathcal{I}<em _tilde_theta="\tilde{\theta">{\text {up,loss }}$ in a non-convergent, non-convex setting by training a convolutional neural network for 500k iterations. ${ }^{5}$ The model had not converged and $H</em>$ was not PD, so we added a damping term with $\lambda=0.01$. Even in this difficult setting, the predicted and actual changes in loss were highly correlated (Pearson's R $=0.86$, Fig 2-Right).}</p>
<h3>4.3. Non-differentiable losses</h3>
<p>What happens when the derivatives of the loss, $\nabla_{\theta} L$ and $\nabla_{\theta}^{2} L$, do not exist? In this section, we show that influence functions computed on smooth approximations to non-differentiable losses can predict the behavior of the original, non-differentiable loss under leave-one-out retraining. The robustness of this approximation suggests that we can train non-differentiable models and swap out non-differentiable components for smoothed versions for the purposes of calculating influence.</p>
<p>To see this, we trained a linear SVM on the same 1s vs. 7s MNIST task in Section 2.3. This involves minimizing $\operatorname{Hinge}(s)=\max(0,1-s)$; this simple piece-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>wise linear function is similar to ReLUs, which cause nondifferentiability in neural networks. We set the derivatives at the hinge to 0 and calculated $\mathcal{I}<em _text="\text" _up_loss="{up,loss">{\text {up,loss }}$. As one might expect, this was inaccurate (Fig 3b-Left): the second derivative carries no information about how close a support vector $z$ is to the hinge, so the quadratic approximation of $L(z, \tilde{\theta})$ is linear, which leads to $\mathcal{I}</em>\right)$ overestimating the influence of $z$.}}\left(z, z_{\text {test }</p>
<p>For the purposes of calculating influence, we approximated $\operatorname{Hinge}(s)$ with $\operatorname{SmoothHinge}(s, t)=t \log \left(1+\exp \left(\frac{1-s}{t}\right)\right)$, which approaches the hinge loss as $t \rightarrow 0$ (Fig 3a). Using the same SVM weights as before, we found that calculating $\mathcal{I}_{\text {up,loss }}$ using $\operatorname{SmoothHinge}(s, 0.001)$ closely matched the actual change due to retraining in the original $\operatorname{Hinge(s)}$ (Pearson's $\mathrm{R}=0.95$; Fig 3b-Mid) and remained accurate over a wide range of $t$ (Fig 3b-Right).</p>
<h2>5. Use cases of influence functions</h2>
<h3>5.1. Understanding model behavior</h3>
<p>By telling us the training points "responsible" for a given prediction, influence functions reveal insights about how models rely on and extrapolate from the training data. In this section, we show that two models can make the same correct predictions but get there in very different ways.</p>
<p>We compared (a) the state-of-the-art Inception v3 network (Szegedy et al., 2016) with all but the top layer frozen ${ }^{6}$ and (b) an SVM with an RBF kernel on a dog vs. fish image classification dataset we extracted from ImageNet (Russakovsky et al., 2015), with 900 training examples for each class. Freezing neural networks in this way is not uncommon in computer vision and is equivalent to training a</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>logistic regression model on the bottleneck features (Donahue et al., 2014). We picked a test image both models got correct (Fig 4-Top) and used $\operatorname{SmoothHinge}(\cdot, 0.001)$ to compute the influence for the SVM.</p>
<p>As expected, $\mathcal{I}<em _text="\text" _up_loss="{up,loss">{\text {up,loss }}$ in the RBF SVM varied inversely with raw pixel distance, with training images far from the test image in pixel space having almost no influence; the Inception influences were much less correlated with distance in pixel space (Fig 4-Left). Looking at the two most helpful images (most positive $-\mathcal{I}</em>$ ) for each model in Fig 4 -Right, we see that the Inception network picked on the distinctive characteristics of clownfish, whereas the RBF SVM pattern-matched training images superficially.}</p>
<p>Moreover, in the RBF SVM, fish (green points) close to the test image were mostly helpful, while dogs (red) were mostly harmful, with the RBF acting as a soft nearest neighbor function (Fig 4-Left). In contrast, in the Inception network, fish and dogs could be helpful or harmful for correctly classifying the test image as a fish; in fact, the 5th most helpful training image was a dog that, to the model, looked very different from the test fish (Fig 4-Top).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Inception vs. RBF SVM. Bottom left: $-\mathcal{I}<em _test="{test" _text="\text">{\text {up,loss }}\left(z, z</em>$. Green dots are fish and red dots are dogs. Bottom right: The two most helpful training images, for each model, on the test. Top right: An image of a dog in the training set that helped the Inception model correctly classify the test image as a fish.}}\right)$ vs. $\left|z-z_{\text {test }}\right|_{2}^{2</p>
<h3>5.2. Adversarial training examples</h3>
<p>In this section, we show that models that place a lot of influence on a small number of points can be vulnerable to training input perturbations, posing a serious security risk in real-world ML systems where attackers can influence the training data (Huang et al., 2011). Recent work has generated adversarial test images that are visually indistinguish-
able from real test images but completely fool a classifier (Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2016). We demonstrate that influence functions can be used to craft adversarial training images that are similarly visuallyindistinguishable and can flip a model's prediction on a separate test image. To the best of our knowledge, this is the first proof-of-concept that visually-indistinguishable training attacks can be executed on otherwise highly-accurate neural networks.</p>
<p>The key idea is that $\mathcal{I}<em _test="{test" _text="\text">{\text {pert,loss }}\left(z, z</em>}}\right)$ tells us how to modify training point $z$ to most increase the loss on $z_{\text {test }}$. Concretely, for a target test image $z_{\text {test }}$, we can construct $\hat{z<em i="i">{i}$, an adversarial version of a training image $z</em>}$, by initializing $\hat{z<em i="i">{i}:=z</em>}$ and then iterating $\hat{z<em i="i">{i}:=\Pi\left(\hat{z}</em>}+\right.$ $\left.\alpha \operatorname{sign}\left(\mathcal{I<em i="i">{\text {pert,loss }}\left(\hat{z}</em>$. After each iteration, we retrain the model. This is an iterated, training-set analogue of the methods used by, e.g., Goodfellow et al. (2015); MoosaviDezfooli et al. (2016) for test-set attacks.}, z_{\text {test }}\right)\right)\right)$, where $\alpha$ is a step size and $\Pi$ projects onto the set of valid images that share the same 8bit representation with $z_{i</p>
<p>We tested these adversarial training perturbations on the same Inception network on dogs vs. fish from Section 5.1, choosing this pair of animals to provide a stark contrast between the classes. We set $\alpha=0.02$ and ran the attack for 100 iterations on each test image. As before, we froze all but the top layer for training; note that computing $\mathcal{I}_{\text {pert,loss }}$ still involves differentiating through the entire network. Originally, the model correctly classified 591 / 600 test images. For each of these 591 test images, considered separately, we tried to find a visually-indistinguishable perturbation (i.e., same 8-bit representation) to a single training image, out of 1,800 total training images, that would flip the model's prediction. We were able to do this on 335 (57\%) of the 591 test images. If we perturbed 2 training images for each test image, we could flip predictions on $77 \%$ of the 591 test images; and if we perturbed 10 training images, we could flip all but 1 of the 591 . The above results are from attacking each test image separately, i.e., we use a different training set to attack each test image. We next tried to attack multiple test images simultaneously by increasing their average test loss, and found that single training image perturbations could simultaneously flip multiple test predictions as well (Fig 5).</p>
<p>We make three observations about these attacks. First, though the change in pixel values is small, the change in the final Inception feature layer is significantly larger: in pixel space and using $L_{2}$ distance, the training values change by less than $1 \%$ of the mean distance of a training point to the class centroid, whereas in Inception feature space, the change is on the same order as the mean distance. Second, the attack tries to perturb the training example in a direction of low variance, causing the model to overfit in that</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Training-set attacks. We targeted a set of 30 test images featuring the first author's dog in a variety of poses and backgrounds. By maximizing the average loss over these 30 images, we found a visuallyimperceptible change to the particular training image (shown on top) that flipped predictions on 16 test images.
direction and consequently incorrectly classify the test images; we expect the attack to become harder as the number of training examples grows. Third, ambiguous or mislabeled training images are effective points to attack, since the model has low confidence and thus high loss on them, making them highly influential (recall Section 2.3). For example, the image in Fig 5 contains both a dog and a fish and is highly ambiguous; as a result, it is the training example that the model is least confident on (with a confidence of $77 \%$, compared to the next lowest confidence of $90 \%$ ).</p>
<p>This attack is mathematically equivalent to the gradientbased training set attacks explored by Biggio et al. (2012); Mei \&amp; Zhu (2015b) and others in the context of different models.Biggio et al. (2012) constructed a dataset poisoning attack against a linear SVM on a two-class MNIST task, but had to modify the training points in an obviously distinguishable way to be effective. Measuring the magnitude of $\mathcal{I}_{\text {pert,loss }}$ gives model developers a way of quantifying how vulnerable their models are to training-set attacks.</p>
<h3>5.3. Debugging domain mismatch</h3>
<p>Domain mismatch - where the training distribution does not match the test distribution - can cause models with high training accuracy to do poorly on test data (Ben-David et al., 2010). We show that influence functions can identify the training examples most responsible for the errors, helping model developers identify domain mismatch.</p>
<p>As a case study, we predicted whether a patient would be readmitted to a hospital. Domain mismatches are common in biomedical data; for example, different hospitals can serve very different populations, and readmission models trained on one population can do poorly on another (Kansagara et al., 2011). We used logistic regression to predict readmission with a balanced training dataset of 20 K diabetic patients from 100+ US hospitals, each represented by 127 features (Strack et al., 2014). ${ }^{7}$</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>3 out of the 24 children under age 10 in this dataset were re-admitted. To induce a domain mismatch, we filtered out 20 children who were not re-admitted, leaving 3 out of 4 readmitted. This caused the model to wrongly classify many children in the test set. Our aim is to identify the 4 children in the training set as being "responsible" for these errors.</p>
<p>As a baseline, we tried the common practice of looking at the learned parameters $\hat{\theta}$ to see if the indicator variable for being a child was obviously different. However, this did not work: 14/127 features had a larger coefficient.</p>
<p>Picking a random child $z_{\text {test }}$ that the model got wrong, we calculated $-\mathcal{I}<em i="i">{\text {up,loss }}\left(z</em>}, z_{\text {test }}\right)$ for each training point $z_{i}$. This clearly highlighted the 4 training children, each of whom were 30-40 times as influential as the next most influential examples. The 1 child in the training set who was not readmitted had a very positive influence, while the other 3 had very negative influences. Calculating $\mathcal{I<em _text="\text" _up_loss="{up,loss">{\text {pert,loss }}$ on these 4 children showed that a change in the 'child' indicator variable had by far the largest effect on $\mathcal{I}</em>$.}</p>
<h3>5.4. Fixing mislabeled examples</h3>
<p>Labels in the real world are often noisy, especially if crowdsourced (Frénay \&amp; Verleysen, 2014), and can even be adversarially corrupted, as in Section 5.2. Even if a human expert could recognize wrongly labeled examples, it is impossible in many applications to manually review all of the training data. We show that influence functions can help human experts prioritize their attention, allowing them to inspect only the examples that actually matter.</p>
<p>The key idea is to flag the training points that exert the most influence on the model. Because we do not have access to the test set, we measure the influence of $z_{i}$ with $\mathcal{I}<em i="i">{\text {up,loss }}\left(z</em>$ from the training set.}, z_{i}\right)$, which approximates the error incurred on $z_{i}$ if we remove $z_{i</p>
<p>Our case study is email spam classification, which relies</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>on user-provided labels and is also vulnerable to adversarial attack (Biggio et al., 2011). We flipped the labels of a random 10% of the training data and then simulated manually inspecting a fraction of the training points, correcting them if they had been flipped. Using influence functions to prioritize the training points to inspect allowed us to repair the dataset (Fig 6, blue) without checking too many points, outperforming the baselines of checking points with the highest train loss (Fig 6, green) or at random (Fig 6, red). No method had access to the test data.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Fixing mislabeled examples. Plots of how test accuracy (left) and the fraction of flipped data detected (right) change with the fraction of train data checked, using different algorithms for picking points to check. Error bars show the std. dev. across 40 repeats of this experiment, with a different subset of labels flipped in each; error bars on the right are too small to be seen. These results are on the Enron1 spam dataset (Metsis et al., 2006), with 4,147 training and 1,035 test examples; we trained logistic regression on a bag-of-words representation of the emails.</p>
<h2>6. Related work</h2>
<p>The use of influence-based diagnostics originated in statistics in the 70s, driven by the seminal papers of Hampel (1974) and Jaeckel (1972) (where it was called the infinitesimal jackknife). It was further developed in the book by Hampel et al. (1986) and many other contemporary papers (Cook, 1977; Cook &amp; Weisberg, 1980; Pregibon et al., 1981; Cook &amp; Weisberg, 1982). Earlier work focused on removing training points from linear models, with later work extending this to more general models and a wider variety of perturbations (Hampel et al., 1986; Cook, 1986; Thomas &amp; Cook, 1990; Chatterjee &amp; Hadi, 1986; Wei et al., 1998). Prior work mostly focused on experiments with small datasets, e.g., $n=24$ and $p=10$ in Cook &amp; Weisberg (1980), and thus paid special attention to exact solutions, or if not possible, characterizations of the error terms.</p>
<p>Influence functions have not been used much in the ML literature, with some exceptions. Christmann &amp; Steinwart (2004); Debruyne et al. (2008); Liu et al. (2014) use influence functions to study model robustness and to do fast cross-validation in kernel methods. Wojnowicz et al. (2016) use matrix sketching to estimate Cook's distance, which is closely related to influence; they focus on prioritizing training points for human attention and derive methods specific to generalized linear models. Kabra et al. (2015) define a different notion of influence that is specialized to finite hypothesis classes.</p>
<p>As noted in Section 5.2, our training-set attack is mathematically equivalent to an approach first explored by Biggio et al. (2012) in the context of SVMs, with follow-up work extending the framework and applying it to linear and logistic regression (Mei &amp; Zhu, 2015b), topic modeling (Mei &amp; Zhu, 2015a), and collaborative filtering (Li et al., 2016a). These papers derived the attack directly from the KKT conditions without considering influence, though for continuous data, the end result is equivalent. Influence functions additionally let us consider attacks on discrete data (Section 2.2), but we have not tested this empirically. Our work connects the literature on trainingset attacks with work on "adversarial examples" (Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2016), visually-imperceptible perturbations on test inputs.</p>
<p>In contrast to training-set attacks, Cadamuro et al. (2016) consider the task of taking an incorrect test prediction and finding a small subset of training data such that changing the labels on this subset makes the prediction correct. They provide a solution for OLS and Gaussian process models when the labels are continuous. Our work with influence functions allow us to solve this problem in a much larger range of models and in datasets with discrete labels.</p>
<h2>7. Discussion</h2>
<p>We have discussed a variety of applications, from creating training-set attacks to debugging models and fixing datasets. Underlying each of these applications is a common tool, influence functions, which are based on a simple idea—we can better understand model behavior by looking at how it was derived from its training data.</p>
<p>At their core, influence functions measure the effect of local changes: what happens when we upweight a point by an infinitesimally-small $\epsilon$ ? This locality allows us to derive efficient closed-form estimates, and as we show, they can be surprisingly effective. However, we might want to ask about more global changes, e.g., how does a subpopulation of patients from this hospital affect the model? Since influence functions depend on the model not changing too much, how to tackle this is an open question.</p>
<p>It seems inevitable that high-performing, complex, black-box models will become increasingly prevalent and important. We hope that the approach presented here—of looking at the model through the lens of the training data—will become a standard part of the toolkit of developing, understanding, and diagnosing machine learning.</p>
<h2>Reproducibility</h2>
<p>The code and data for replicating our experiments is available on GitHub http://bit.ly/gt-influence and Codalab http://bit.ly/cl-influence.</p>
<h2>Acknowledgements</h2>
<p>We thank Jacob Steinhardt, Zhenghao Chen, and Hongseok Namkoong for helpful discussions and comments. We are also grateful to Doug Martin, Swee Keat Lim, and Teresa Yeo for finding typos and omissions in a previous version of the manuscript. This work was supported by a Future of Life Research Award and a Microsoft Research Faculty Fellowship.</p>
<h2>A. Deriving the influence function $\mathcal{I}_{\text {up,params }}$</h2>
<p>For completeness, we provide a standard derivation of the influence function $\mathcal{I}_{\text {up,params }}$ in the context of loss minimization (M-estimation). This derivation is based on asymptotic arguments and is not fully rigorous; see van der Vaart (1998) and other statistics textbooks for a more thorough treatment.</p>
<p>Recall that $\hat{\theta}$ minimizes the empirical risk:</p>
<p>$$
R(\theta) \stackrel{\text { def }}{=} \frac{1}{n} \sum_{i=1}^{n} L\left(z_{i}, \theta\right)
$$</p>
<p>We further assume that $R$ is twice-differentiable and strongly convex in $\theta$, i.e.,</p>
<p>$$
H_{\hat{\theta}} \stackrel{\text { def }}{=} \nabla^{2} R(\hat{\theta})=\frac{1}{n} \sum_{i=1}^{n} \nabla_{\theta}^{2} L\left(z_{i}, \hat{\theta}\right)
$$</p>
<p>exists and is positive definite. This guarantees the existence of $H_{\hat{\theta}}^{-1}$, which we will use in the subsequent derivation.</p>
<p>The perturbed parameters $\hat{\theta}_{\epsilon, z}$ can be written as</p>
<p>$$
\hat{\theta}<em _Theta="\Theta" _in="\in" _theta="\theta">{\epsilon, z}=\arg \min </em>{R(\theta)+\epsilon L(z, \theta)}
$$</p>
<p>Define the parameter change $\Delta_{\epsilon}=\hat{\theta}_{\epsilon, z}-\hat{\theta}$, and note that, as $\hat{\theta}$ doesn't depend on $\epsilon$, the quantity we seek to compute can be written in terms of it:</p>
<p>$$
\frac{d \hat{\theta}<em _epsilon="\epsilon">{\epsilon, z}}{d \epsilon}=\frac{d \Delta</em>
$$}}{d \epsilon</p>
<p>Since $\hat{\theta}_{\epsilon, z}$ is a minimizer of (8), let us examine its firstorder optimality conditions:</p>
<p>$$
0=\nabla R\left(\hat{\theta}<em _epsilon_="\epsilon," z="z">{\epsilon, z}\right)+\epsilon \nabla L\left(z, \hat{\theta}</em>\right)
$$</p>
<p>Next, since $\hat{\theta}_{\epsilon, z} \rightarrow \hat{\theta}$ as $\epsilon \rightarrow 0$, we perform a Taylor expansion of the right-hand side:</p>
<p>$$
\begin{aligned}
0 \approx &amp; {\left[\nabla R(\hat{\theta})+\epsilon \nabla L(z, \hat{\theta})\right]+} \
&amp; {\left[\nabla^{2} R(\hat{\theta})+\epsilon \nabla^{2} L(z, \hat{\theta})\right] \Delta_{\epsilon}
\end{aligned}
$$</p>
<p>where we have dropped $o\left(\left|\Delta_{\epsilon}\right|\right)$ terms.
Solving for $\Delta_{\epsilon}$, we get:</p>
<p>$$
\begin{aligned}
\Delta_{\epsilon} \approx &amp; -\left[\nabla^{2} R(\hat{\theta})+\epsilon \nabla^{2} L(z, \hat{\theta})\right]^{-1} \
&amp; {\left[\nabla R(\hat{\theta})+\epsilon \nabla L(z, \hat{\theta})\right] }
\end{aligned}
$$</p>
<p>Since $\hat{\theta}$ minimizes $R$, we have $\nabla R(\hat{\theta})=0$. Dropping $o(\epsilon)$ terms, we have</p>
<p>$$
\Delta_{\epsilon} \approx-\nabla^{2} R(\hat{\theta})^{-1} \nabla L(z, \hat{\theta}) \epsilon
$$</p>
<p>Combining with (7) and (9), we conclude that:</p>
<p>$$
\begin{aligned}
\left.\frac{d \hat{\theta}<em _epsilon="0">{\epsilon, z}}{d \epsilon}\right|</em>) \
&amp; \stackrel{\text { def }}{=} \mathcal{I}_{\text {up,params }}(z)
\end{aligned}
$$} &amp; =-H_{\hat{\theta}}^{-1} \nabla L(z, \hat{\theta</p>
<h2>B. Influence at non-convergence</h2>
<p>Consider a training point $z$. When the model parameters $\hat{\theta}$ are close to but not at a local minimum, $\mathcal{I}<em _text="\text" _up_params="{up,params">{\text {up,params }}(z)$ is approximately equal to a constant (which does not depend on $z$ ) plus the change in parameters after upweighting $z$ and then taking a single Newton step from $\hat{\theta}$. The high-level idea is that even though the gradient of the empirical risk at $\hat{\theta}$ is not 0 , the Newton step from $\hat{\theta}$ can be decomposed into a component following the existing gradient (which does not depend on the choice of $z$ ) and a second component responding to the upweighted $z$ (which $\mathcal{I}</em>(z)$ tracks).
Let $g \stackrel{\text { def }}{=} \frac{1}{n} \sum_{i=1}^{n} \nabla_{\theta} L\left(z_{i}, \tilde{\theta}\right)$ be the gradient of the empirical risk at $\hat{\theta}$; since $\hat{\theta}$ is not a local minimum, $g \neq 0$. After upweighting $z$ by $\epsilon$, the gradient at $\tilde{\theta}$ goes from $g \mapsto g+\epsilon \nabla_{\theta} L(z, \tilde{\theta})$, and the empirical Hessian goes from $H_{\tilde{\theta}} \mapsto H_{\tilde{\theta}}+\epsilon \nabla_{\theta}^{2} L(z, \tilde{\theta})$. A Newton step from $\hat{\theta}$ therefore changes the parameters by:}</p>
<p>$$
N_{\epsilon, z} \stackrel{\text { def }}{=}-\left[H_{\tilde{\theta}}+\epsilon \nabla_{\theta}^{2} L(z, \tilde{\theta})\right]^{-1}\left[g+\epsilon \nabla_{\theta} L(z, \tilde{\theta})\right]
$$</p>
<p>Ignoring terms in $\epsilon g, \epsilon^{2}$, and higher, we get $N_{\epsilon, z} \approx$ $-H_{\tilde{\theta}}^{-1}\left(g+\epsilon \nabla_{\theta} L(z, \tilde{\theta})\right)$. Therefore, the actual change due to a Newton step $N_{\epsilon, z}$ is equal to a constant $-H_{\tilde{\theta}}^{-1} g$ (that doesn't depend on $z$ ) plus $\epsilon$ times $\mathcal{I}<em _tilde_theta="\tilde{\theta">{\text {up,params }}(z)=$ $-H</em>)$ (which captures the contribution of $z$ ).}}^{-1} \nabla_{\theta} L(z, \tilde{\theta</p>
<h2>References</h2>
<p>Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I. J., Harp, A., Irving, G., Isard, M., Jia, Y., Józefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mané, D., Monga, R., Moore, S., Murray, D. G., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P. A., Vanhoucke, V., Vasudevan, V., Viégas, F. B., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2015.</p>
<p>Adler, P., Falk, C., Friedler, S. A., Rybeck, G., Scheidegger, C., Smith, B., and Venkatasubramanian, S. Auditing black-box models for indirect influence. arXiv preprint arXiv:1602.07043, 2016.</p>
<p>Agarwal, N., Bullins, B., and Hazan, E. Second order stochastic optimization in linear time. arXiv preprint arXiv:1602.03943, 2016.</p>
<p>Amershi, S., Chickering, M., Drucker, S. M., Lee, B., Simard, P., and Suh, J. Modeltracker: Redesigning performance analysis tools for machine learning. In Conference on Human Factors in Computing Systems (CHI), pp. 337-346, 2015.</p>
<p>Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan, J. W. A theory of learning from different domains. Machine Learning, 79(1):151175, 2010.</p>
<p>Biggio, B., Nelson, B., and Laskov, P. Support vector machines under adversarial label noise. ACML, 20:97-112, 2011.</p>
<p>Biggio, B., Nelson, B., and Laskov, P. Poisoning attacks against support vector machines. In International Conference on Machine Learning (ICML), pp. 1467-1474, 2012.</p>
<p>Cadamuro, G., Gilad-Bachrach, R., and Zhu, X. Debugging machine learning models. In ICML Workshop on Reliable Machine Learning in the Wild, 2016.</p>
<p>Chatterjee, S. and Hadi, A. S. Influential observations, high leverage points, and outliers in linear regression. Statistical Science, pp. 379-393, 1986.</p>
<p>Chollet, F. Keras, 2015.
Christmann, A. and Steinwart, I. On robustness properties of convex risk minimization methods for pattern recognition. Journal of Machine Learning Research (JMLR), 5(0):1007-1034, 2004.</p>
<p>Cook, R. D. Detection of influential observation in linear regression. Technometrics, 19:15-18, 1977.</p>
<p>Cook, R. D. Assessment of local influence. Journal of the Royal Statistical Society. Series B (Methodological), pp. 133-169, 1986.</p>
<p>Cook, R. D. and Weisberg, S. Characterizations of an empirical influence function for detecting influential cases in regression. Technometrics, 22:495-508, 1980.</p>
<p>Cook, R. D. and Weisberg, S. Residuals and influence in regression. New York: Chapman and Hall, 1982.</p>
<p>Datta, A., Sen, S., and Zick, Y. Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In Security and Privacy (SP), 2016 IEEE Symposium on, pp. 598-617, 2016.</p>
<p>Debruyne, M., Hubert, M., and Suykens, J. A. Model selection in kernel based regression using the influence function. Journal of Machine Learning Research (JMLR), 9 (0):2377-2400, 2008.</p>
<p>Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on Machine Learning (ICML), volume 32, pp. 647-655, 2014.</p>
<p>Frénay, B. and Verleysen, M. Classification in the presence of label noise: a survey. IEEE Transactions on Neural Networks and Learning Systems, 25:845-869, 2014.</p>
<p>Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. In International Conference on Learning Representations (ICLR), 2015.</p>
<p>Goodman, B. and Flaxman, S. European union regulations on algorithmic decision-making and a "right to explanation". arXiv preprint arXiv:1606.08813, 2016.</p>
<p>Hampel, F. R. The influence curve and its role in robust estimation. Journal of the American Statistical Association, 69(346):383-393, 1974.</p>
<p>Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., and Stahel, W. A. Robust Statistics: The Approach Based on Influence Functions. Wiley, 1986.</p>
<p>Huang, L., Joseph, A. D., Nelson, B., Rubinstein, B. I., and Tygar, J. Adversarial machine learning. In Proceedings of the 4th ACM workshop on Security and artificial intelligence, pp. 43-58, 2011.</p>
<p>Jaeckel, L. A. The infinitesimal jackknife. Unpublished memorandum, Bell Telephone Laboratories, Murray Hill, NJ, 1972.</p>
<p>Kabra, M., Robie, A., and Branson, K. Understanding classifier errors by examining influential neighbors. In Computer Vision and Pattern Recognition (CVPR), pp. 39173925, 2015.</p>
<p>Kansagara, D., Englander, H., Salanitro, A., Kagen, D., Theobald, C., Freeman, M., and Kripalani, S. Risk prediction models for hospital readmission: a systematic review. JAMA, 306(15):1688-1698, 2011.</p>
<p>Kingma, D. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.</p>
<p>Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS), pp. 1097-1105, 2012.</p>
<p>LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.</p>
<p>Li, B., Wang, Y., Singh, A., and Vorobeychik, Y. Data poisoning attacks on factorization-based collaborative filtering. In Advances in Neural Information Processing Systems (NeurIPS), 2016a.</p>
<p>Li, J., Monroe, W., and Jurafsky, D. Understanding neural networks through representation erasure. arXiv preprint arXiv:1612.08220, 2016b.</p>
<p>Liu, D. C. and Nocedal, J. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(1):503-528, 1989.</p>
<p>Liu, Y., Jiang, S., and Liao, S. Efficient approximation of cross-validation for kernel methods using Bouligand influence function. In International Conference on Machine Learning (ICML), pp. 324-332, 2014.</p>
<p>Martens, J. Deep learning via hessian-free optimization. In International Conference on Machine Learning (ICML), pp. 735-742, 2010.</p>
<p>Mei, S. and Zhu, X. The security of latent Dirichlet allocation. In Artificial Intelligence and Statistics (AISTATS), 2015a.</p>
<p>Mei, S. and Zhu, X. Using machine teaching to identify optimal training-set attacks on machine learners. In Association for the Advancement of Artificial Intelligence (AAAI), 2015b.</p>
<p>Metsis, V., Androutsopoulos, I., and Paliouras, G. Spam filtering with naive Bayes - which naive Bayes? In CEAS, volume 17, pp. 28-69, 2006.</p>
<p>Moosavi-Dezfooli, S., Fawzi, A., and Frossard, P. Deepfool: a simple and accurate method to fool deep neural networks. In Computer Vision and Pattern Recognition (CVPR), pp. 2574-2582, 2016.</p>
<p>Pearlmutter, B. A. Fast exact multiplication by the Hessian. Neural Computation, 6(1):147-160, 1994.</p>
<p>Pregibon, D. et al. Logistic regression diagnostics. Annals of Statistics, 9(4):705-724, 1981.</p>
<p>Ribeiro, M. T., Singh, S., and Guestrin, C. "Why Should I Trust You?": Explaining the predictions of any classifier. In International Conference on Knowledge Discovery and Data Mining (KDD), 2016.</p>
<p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 115(3): 211-252, 2015.</p>
<p>Shrikumar, A., Greenside, P., and Kundaje, A. Learning important features through propagating activation differences. In International Conference on Machine Learning (ICML), 2017.</p>
<p>Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.</p>
<p>Springenberg, J. T., Dosovitskiy, A., Brox, T., and Riedmiller, M. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.</p>
<p>Strack, B., DeShazo, J. P., Gennings, C., Olmo, J. L., Ventura, S., Cios, K. J., and Clore, J. N. Impact of HbA1c measurement on hospital readmission rates: Analysis of 70,000 clinical database patient records. BioMed Research International, 2014, 2014.</p>
<p>Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the Inception architecture for computer vision. In Computer Vision and Pattern Recognition (CVPR), pp. 2818-2826, 2016.</p>
<p>Theano D. Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv preprint arXiv:1605.02688, 2016.</p>
<p>Thomas, W. and Cook, R. D. Assessing influence on predictions from generalized linear models. Technometrics, 32(1):59-65, 1990.
van der Vaart, A. W. Asymptotic statistics. Cambridge University Press, 1998.</p>
<p>Wei, B., Hu, Y., and Fung, W. Generalized leverage and its applications. Scandinavian Journal of Statistics, 25: $25-37,1998$.</p>
<p>Wojnowicz, M., Cruz, B., Zhao, X., Wallace, B., Wolff, M., Luan, J., and Crable, C. "Influence sketching": Finding influential samples in large-scale regressions. arXiv preprint arXiv:1611.05923, 2016.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ Hospital readmission was defined as whether a patient would be readmitted within the next 30 days. Features were demo-&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>graphic (e.g., age, race, gender), administrative (e.g., length of hospital stay), or medical (e.g., test results).&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3716 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3716</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3716</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3" target="_blank">Causal Reasoning and Large Language Models: Opening a New Frontier for Causality</a></p>
                <p><strong>Paper Venue:</strong> Trans. Mach. Learn. Res.</p>
                <p><strong>Paper TL;DR:</strong> Across a wide range of tasks, it is found that LLMs can generate text corresponding to correct causal arguments with high probability, surpassing the best-performing existing methods.</p>
                <p><strong>Paper Abstract:</strong> The causal capabilities of large language models (LLMs) are a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We conduct a"behavorial"study of LLMs to benchmark their capability in generating causal arguments. Across a wide range of tasks, we find that LLMs can generate text corresponding to correct causal arguments with high probability, surpassing the best-performing existing methods. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain) and event causality (86% accuracy in determining necessary and sufficient causes in vignettes). We perform robustness checks across tasks and show that the capabilities cannot be explained by dataset memorization alone, especially since LLMs generalize to novel datasets that were created after the training cutoff date. That said, LLMs exhibit unpredictable failure modes, and we discuss the kinds of errors that may be improved and what are the fundamental limits of LLM-based answers. Overall, by operating on the text metadata, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. As a result, LLMs may be used by human domain experts to save effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. Given that LLMs ignore the actual data, our results also point to a fruitful research direction of developing algorithms that combine LLMs with existing causal techniques. Code and datasets are available at https://github.com/py-why/pywhy-llm.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3716",
    "paper_id": "paper-10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00638925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Causal Reasoning and Large Language Models: Opening a New Frontier for Causality</h1>
<p>Emre Kıciman * emrek@microsoft.com<br>Microsoft Research<br>Robert Osazuwa Ness<br>mbertness@microsoft.com<br>Microsoft Research<br>Amit Sharma<br>amshar@microsoft.com<br>Microsoft Research<br>Chenhao Tan<br>chenhao@uchicago.edu</p>
<h2>Abstract</h2>
<p>The causal capabilities of large language models (LLMs) are a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We conduct a "behavorial" study of LLMs to benchmark their capability in generating causal arguments. Across a wide range of tasks, we find that LLMs can generate text corresponding to correct causal arguments with high probability, surpassing the best-performing existing methods. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task ( $97 \%, 13$ points gain), counterfactual reasoning task ( $92 \%, 20$ points gain) and event causality ( $86 \%$ accuracy in determining necessary and sufficient causes in vignettes). We perform robustness checks across tasks and show that the capabilities cannot be explained by dataset memorization alone, especially since LLMs generalize to novel datasets that were created after the training cutoff date.
That said, LLMs exhibit unpredictable failure modes and we discuss the kinds of errors that may be improved and what are the fundamental limits of LLM-based answers. Overall, by operating on the text metadata, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. As a result, LLMs may be used by human domain experts to save effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. Given that LLMs ignore the actual data, our results also point to a fruitful research direction of developing algorithms that combine LLMs with existing causal techniques. Code and datasets are available at https://github.com/py-why/pywhy-llm.</p>
<h2>1 Introduction</h2>
<p>Recent advances in scaling large language models (LLMs) have led to breakthroughs in AI capabilities. As language models increase in number of parameters and are trained on larger datasets, they gain complex, emergent behaviors, such as abilities to write code in programming languages, generate stories, poems, essays, and other texts, and demonstrate strong performance in certain reasoning tasks (Chen et al., 2021; Nguyen \&amp; Nadi, 2022; Bubeck et al., 2023; Katz</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: When tackling real-world causal tasks, people strategically alternate between logical- and covariance-based causal inferences as they formulate (sub-)questions, iterate, and verify their premises and implications. Now, LLMs may have the capability to automate or assist with every step of this process and seamlessly transition between covarianceand logic-based causality.
et al., 2023; Wei et al., 2022a). Impressively, when asked to explain their outputs, update their conclusions given new evidence, and even generate counterfactuals, LLMs can create plausible responses (Nori et al., 2023; Lee et al., 2023a;b). This apparent capacity for both implicit and explicit consideration of causal factors has created excitement towards understanding their reasoning capabilities (Hobbhahn et al., 2022; Kosoy et al., 2022; Willig et al., 2022; Liu et al., 2023; Zhang et al., 2023). Figure 2(a) shows an example of such generation of causal arguments ${ }^{1}$.</p>
<p>At the same time, LLMs are imperfect: they can make absurd claims and are often observed to make basic errors of logic and mathematics, much less complex reasoning (Bubeck et al., 2023; Zhong et al., 2023; Ghazal et al., 2017). Figure 2(b) shows an example of an LLM making basic causal mistakes. This has incited a debate about whether LLMs truly perform causal reasoning, or are simply unreliable mimics, regenerating memorized responses in the context of critical analysis and decision-making tasks across many domains, including health, medicine, legal reasoning, education, policy making and business strategy (Bender et al., 2021; Marcus, 2022). Complicating the situation is the variety of distinct formal and informal methods that people ascribe to 'causal reasoning', and the breadth of purposes for which people apply causal reasoning in some form.</p>
<p>In short, what kinds of causal arguments can LLMs generate, how valid are these arguments, and what causal reasoning workflows can this generation support or automate? This paper clarifies this debate, advancing our understanding of LLM models and their causal implications, and proposes a framework for future research at the intersection of LLMs and causality.</p>
<p>We begin with a recognition of the distinct kinds of causal knowledge and reasoning implicated in this debate, including prior knowledge of general and domain-specific causal mechanisms, intuitive and logical methods for causal and counterfactual reasoning, and covariance-based causality, such as building causal DAGs and inferring causal effects (see Figure 1 for an illustration). We present intriguing results in each of areas and then proceed with deeper analyses to better understand the drivers of LLM behavior and the reliability of their causal capabilities in two areas-causal DAG generation and token causality. Our results indicate that LLMs bring significant new capabilities which are complementary to existing causal methods. They do so by capturing the human domain knowledge relevant to the task, which forms an essential part of any causal analysis. As a result, LLMs have the capability of transforming how causal</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>SYSTEM: You are a business advisor and give clear, wellreasoned but brief (5-15 line) advice in response to questions about what people should do.
USER: A toy shop owner in the western USA wants to decide whether the ad they bought in early December is really better than their previous ads. Here is their sales data:
October: $\$ 10,200$; November: $\$ 10,000$; December: $\$ 13,000$; January: $\$ 10,100$
They now want to decide which ad to show in February. Can you help them decide whether the increase in sales in December was due to the ads? Note that the new ad costs $\$ 1000$ more to produce, so the toy maker is interested in maximizing their profit.
GPT-4: Based on the available data, it is difficult to definitively attribute the increase in December sales to the new ad, as holiday season and other external factors may have influenced sales. To make a more informed decision, I recommend conducting an A/B test in February by dividing the targeted audience into two groups, showing one group the old ad and the other group the new ad. Compare the sales performance of both groups and calculate the net profit (considering the $\$ 1000$ additional cost for the new ad) to determine which ad is more effective in driving sales and maximizing profit. This will provide a clearer understanding of the ad's impact on sales.
(a) A GPT-4 powered advisor gives sound causal advice</p>
<p>USER: Thanks. In February, the toy shop owner split their audience into two groups to show them two different ads. They showed the old ad to people on the newspaper; and showed the new ad to people online. When people came into the store, the owner would ask them which ad they saw. In February, they found out that they sold $\$ 6000$ of toys to people who saw the new online ad, and $\$ 4500$ to people who saw the old newspaper ad. What should the toymaker do in March?
GPT-4: Based on the February results, the new online ad generated $\$ 1,500$ more in sales than the old newspaper ad. Considering the new ad costs $\$ 1,000$ more to produce, the net profit increase is $\$ 500$. In this case, it seems that the new online ad is more effective in driving sales. For March, the toy shop owner should continue using the new online ad, but also consider monitoring the long-term performance of the ad, as well as any potential changes in the target audience's preferences or external factors that may influence sales. Additionally, the owner may want to explore optimizing the new ad further to maximize its cost-effectiveness and reach.
(b) Continuing the conversation, GPT-4 gives a wrong causal answer</p>
<p>Figure 2: Probing LLM-ability to build causal arguments. Two example outputs from an LLM (GPT-4). In the first dialog, the LLM discusses causal issues, such as a potential confounder and recommends an A/B experiment to correctly characterize effects and drive the requested decision-making. The second example continues the conversation and requires arguably the same kind of causal awareness of potential confounders (e.g., the population characteristics and even population sizes of the online and newspaper audience are unknown) but the LLM proceeds regardless and provides an incorrect answer.
analysis is done, with the potential to automate or assist in each step of a causal reasoning process, shown by the arrows in Figure 1.</p>
<p>Summary of results. Given a set of variables, causal DAG generation is the task of specifying the causal graph characterizing their data-generating process. Starting with a pairwise causal task, we find that LLM-based methods using variable names as input substantially outperform existing algorithms for building causal DAGs from statistical evidence in data (causal discovery algorithms (Peters et al., 2017)), achieving $97 \%$ accuracy on the Tübingen benchmark compared to the previous best of $83 \%$. The benchmark includes over hundred causal relationships from a variety of domains, including physics, biology, zoology, cognitive science, epidemiology, and soil science. We repeat the experiment on a more specialized, real-world task related to medical pain diagnosis and obtain similarly high accuracy for GPT 3.5 and 4 models. On the more challenging task of discovering the full graph, we find that LLMs obtain similar accuracy to recent deep learning-based methods. We extend these experiments to pairwise and full graph datasets that were made public after the LLMs' training cutoff date and find similar high accuracies. The errors made by the two kinds of methods-LLMs using variable names and discovery algorithms using the observed data-are not the same, highlighting the potential of combining these methods for greater accuracy. That said, LLMs do make unexpected mistakes; the surprising part is how few such mistakes are, since LLMs only consider the metadata (i.e., variable names) rather than analyzing the data values. To characterize our measurement constructs, we probe whether LLM has been</p>
<p>trained on and memorized the datasets underlying our benchmarks and lay out the implications for interpreting causal benchmarks.</p>
<p>The second major causal task we consider is token causality Halpern, 2016. Unlike building causal DAGs, which deals with variables and their effect on each other, token causality considers individual events and aims to find what caused them. Inferring causes of an event involves simulation of different counterfactual scenarios, but it also involves human judgment to understand the background context and determine which causes need to be considered. For counterfactual reasoning tasks, we find that LLMs like GPT-4 are capable of answering natural language questions. On a benchmark of counterfactual queries spanning basic physics, logic, and common sense, gpt-4 obtains $92 \%$ accuracy, 20 points higher than previously reported accuracy. These counterfactual capabilities also help LLMs to isolate the necessary and sufficient causes given any event, one of the key building blocks of token causality methods. GPT-4 obtains over $86 \%$ accuracy in identifying the necessary and sufficient causes on a commonly used benchmark of 15 vignettes, as well as a novel benchmark that avoids any memorization concerns. That said, tasks that heavily depend on an understanding of human factors in deciding the cause of an event, e.g., assessing the normality of a particular action, remain challenging for LLMs: GPT-4 obtains $70 \%$ on our benchmark task for assessing normality.</p>
<p>Implications for causality research. Irrespective of whether LLMs can perform true causal reasoning (as opposed to merely generate well-formed causal arguments that support such reasoning), their empirically observed ability to perform certain causal tasks is strong enough to provide a useful augmentation for aspects of causal reasoning where we currently rely on humans alone. For example, conventional causal DAG generation and causal effect inference rely strongly on prior domain knowledge of potential causal mechanisms in a system. Current best practice is to rely on human domain experts to provide this knowledge, yet correctly capturing domain knowledge in a formal representation suitable for analysis remains a challenge and is often a primary point of weakness for the validity of causal analyses. LLM capabilities now open the possibility of programmatic access to an array of (memorized or inferred) causal mechanisms, capturing general and domain-specific knowledge, and may augment human domain experts by aiding in bootstrapping, critiquing, etc. Other areas where LLMs provide significant benefit include the ability to understand and formalize causal scenarios, generate relevant formal premises based on background knowledge about the world; and ability to identify and correctly frame challenging causal constraints, validations and refutations, such as negative and positive controls, monotonicity relationships, etc. These are all tasks where previously we relied on human experts alone, and now can be partially or entirely automated with human supervision.</p>
<p>That said, LLMs do have unexpected failure modes. In each of the tasks that we studied, LLMs achieve high average accuracies but also make simple, unpredictable mistakes on certain inputs. Further, their accuracy (and consequently robustness) depends substantially on the prompt used, as observed by Long et al. (2023). We provide some basic empirical tests to probe their robustness to specific prompt language and understand the relative contribution of data memorization. However, more research is needed to understand when LLM outputs can be trusted and increase their robustness, either through external tools or other instances of LLMs themselves.</p>
<p>We close with an updated landscape for causal analysis research and practice. Fully characterizing LLMs inherent capacity to build causal arguments and understanding its underlying mechanisms, requires significant research effortsand until this is accomplished, it is not prudent to trust LLMs alone in critical decision-making tasks and other causal applications. However, current capabilities are sufficiently advanced to be useful in conjunction with existing methods for formal causal reasoning, discovery, and effect inference. We discuss these opportunities, including the possibility that LLMs may provide opportunities for building tighter automated integration between logical and covariance-based approaches to causality.</p>
<p>Outline of the paper. Section 2 reviews background in causality and LLMs. Section 3 provides our key results of causal DAG generation on causal discovery benchmarks and the associated implications for causality effect inference research. Section 4 discusses token causality and the performance of LLMs in generating counterfactual arguments, determining necessary and sufficient causes, and causal judgment tasks. Finally, Section 5 discusses how these LLM capabilities provide a way to augment existing causal analyses with domain knowledge and natural language-based reasoning, while also making it possible to combine different kinds of causal methods in a single analysis.</p>
<h1>2 Background and Preliminaries</h1>
<p>Working with natural language, LLMs bring capabilities to causal questions that are complementary to existing approaches. These capabilities span across hitherto disparate fields of causal enquiry, from specifying models of general causal mechanisms to token causality over events, and from statistical effect inference to logic-based reasoning. As a result, we believe that LLMs offer an opportunity to bring together different approaches to causality and build unified pipelines that seamlessly transition between language-based and data-based analyses. Below we review these different approaches or subfields under which causality is studied. We also provide an introduction to LLMs and summarize methods for probing their capabilities.</p>
<h3>2.1 Many Kinds of Causality</h3>
<p>The science of causality is the study of cause and effect relationships, and is a fundamental tool for understanding and reasoning about the world and how it works. Correct causal reasoning is crucial to making correct decisions, building robust systems, and scientific discovery itself. While advances in causal modeling have formalized core concepts of causality, the varying tasks and problems across domains have led different fields to use related but distinct conceptualizations and tools.</p>
<p>Here we describe three orthogonal categories of causal approaches and tasks. The first axis (covariance- and logic-based causality) distinguishes methodological approaches that emphasize data analysis from those that emphasize logical reasoning. The second axis (type vs. token causality) ${ }^{2}$ focuses on the setting for a causal question: Are we asking about causal relationships in general, such as for a population, or asking questions about specific causes for a specific event. The third axis, presented in Section 2.2 focus on the causal question or task itself: are we interested in inferring new relationships, characterizing the strength of specific known or hypothesized relationships, attributing blame or reward for some outcome, etc.</p>
<p>Covariance- and Logic-based Causality: Many fields, including statistics, biostatistics, and econometrics, place primary emphasis on covariance-based causal analysis. This family of methods uses statistical approaches to infer and estimate the strengths of causal relationships from data (Imbens \&amp; Rubin, 2015; Hernán \&amp; Robins, 2010). Applications include the evaluation of drug efficacy, understanding effects of novel economic policies, and optimizing business decisions. A causal analysis typically starts with a question whose answer is converted to a statistical estimand and then estimated using statistical estimators that work with available data (Pearl, 2009a).</p>
<p>Other domains such as law, forensic investigations, and fault diagnosis, often emphasize logic-based causality, which uses logical reasoning and domain knowledge to reason about causal relationships in systems (Hellner, 2000). For example, some notions of legal liability involve establishing proximate cause for an event based on reasoning about counterfactuals and plausible scenarios (Knobe \&amp; Shapiro, 2021).</p>
<p>Type and Token Causality: Type causality encompasses inference on causal relationships between variables, such as causal effect estimation (Peters et al., 2017). In contrast, token causality (also called specific causality) refers to inference of the degree to which specific events cause other events (Halpern, 2016; Hausman, 2005). That is, token causality is concerned with reasoning about specific events and their causes whereas type causality focuses on variables and their average effects. For example, questions concerning medical science such as "does smoking causes lung cancer" or "what are causes of lung cancer" or "how much does smoking increase the risk of lung cancer" are examples of type-causal inference. Scientists are often interested in type causality questions, because these questions help develop theory and make predictions. For example, average causal effect of an intervention tells us something about its effect on a general population and can be used to inform policy.</p>
<p>In contrast, questions under the umbrella of token causality include "Was Fred's smoking habit responsible for his lung cancer? Or was it his exposure to asbestos?", or "What was the reason this machine failed?". These are questions that concern decision-making in specific situations and their answers need not generalize to other situations. Still, the answers are important to inform a specific decision or conclusion that needs to be made, e.g., deciding a legal case or</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>fixing a machine in a factory. In practice, beyond simple logical reasoning, to determine token causality, people use mental "forward simulations" of processes to predict the potential outcomes of an event, or attribute the underlying events led to an observed outcome (Hegarty, 2004; Jeannerod, 2001).</p>
<h1>2.2 Different Causal Tasks and their Connection to Kinds of Causality</h1>
<p>Causal tasks or questions seek to reason about cause-and-effect relationships in a system, often with the goal of understanding and improving a desired outcome, as other mechanisms and environment change. This broad umbrella encompasses a large array of distinct causal tasks, with its own developed methodologies. Notably, while some of these tasks intersect-they may share common abstractions (e.g., causal graphs) or the result of one task may be fed as an input of another-an ability to solve one task does not imply an ability to solve the others.</p>
<p>Specifying the causal graph is the type-causal task of specifying a graph that represents the underlying causal mechanisms that govern a system. Causal discovery is an approach to causal graph specification that attempts to reverse engineer the causal graph from covariance in data. (Peters et al., 2017; Glymour et al., 2019).</p>
<p>Effect inference is the task of characterizing the strength and shape of a known or hypothesized causal relationship. While it is most commonly characterized as relying on covariance-based methods, effect inference does rely on logical reasoning approaches for determining validity of causal assumptions (Sharma et al., 2021) and identifying approaches to validation and sensitivity analyses (e.g., negative controls (Lipsitch et al., 2010)). Effect inference is primarily focused in type causality scenarios. However, individual treatment effect estimation and counterfactual estimation straddle the realm of token causality.</p>
<p>Attribution is the task of determining the cause or causes of a change. Depending on the application domain, approaches to attribution include both covariance-based and logical-based approaches, and are set in both type and token causality settings. For example, determining the likely cause of performance slowdowns in a large-scale Internet service is usually a covariance-based analysis in a type causality setting (Budhathoki et al., 2021); determining the root cause of a specific error in a system execution can be a covariance-based analysis in an token causality setting (Sharma et al., 2022); whereas determining the cause of a fire in an arson investigation may rely purely on logical-reasoning in an token causality setting (Halpern, 2016). Judgement tasks (Gerstenberg et al., 2014) extend attribution tasks to questions of reward or blame assignment for outcomes. They usually incorporate additional considerations of morality, normality, and intent of agents (Sloman \&amp; Lagnado, 2015).</p>
<p>These are only some of the many causal tasks. Others include policy optimization, decision-making, explanation, scientific discovery, and others.</p>
<h3>2.3 LLMs and Causality</h3>
<p>A large language model is a particular kind of machine learning model, built using transformers, a class of deep neural network architectures (Devlin et al., 2019). The primary task of LLMs is next-word completion. Initially, next-word prediction is based primarily on word distribution probabilities. Further training applies additional human feedback to shape the reward function to take into account factors beyond word distribution probability, such as instruction following and safety (Ouyang et al., 2022).</p>
<p>Recent work has explored causal capabilities of LLMs (Willig et al., 2022). For instance, Long et al. (2023) consider simple graphs of 3-4 nodes and test whether LLMs can recover the correct structure. They consider each variable pair $(\mathrm{A}, \mathrm{B})$ and ask an LLM to score two competing statements, one implying that A causes B and the other that B causes A. Tu et al. (2023) consider a harder task using a dataset on medical pain diagnosis and find that LLM-based graph inference obtains poor accuracy. On the question of inferring causality from natural language, Hobbhahn et al. (2022) study whether LLMs can understand causal implications embedded in natural language, i.e., given two sentences, whether (the event in) one sentence is the cause of another. In this paper, we extend this line of work and make two contributions. First, we investigate causal graph generation capabilities of LLMs, as an analog to covariance-based causal discovery, over a broad set of complex real-world datasets and explore the robustness of LLM-based graph generation. As one example, in Section 3.1, we revisit the case study on LLM-based graph graph generation from Tu</p>
<p>et al. (2023) and show that with an appropriate prompt, LLMs can achieve substantially higher accuracy (increasing the F1 score for retrieved edges from 0.21 to 0.68). Second, we probe the ability of LLMs to do counterfactual reasoning and infer necessary or sufficient cause based on natural language description of events. We also provide a unifying framework that shows how LLMs can be used to transfer knowledge between covariance-based and logic-based causal methods for a given real-world problem.</p>
<h1>2.4 Probing LLM behaviors</h1>
<p>The primary input-output mechanism exposed by an LLM is a textual prompt as input and a textual response as output. Given black-box access to an LLM, therefore, the main way to understand its causal capabilities is to probe it with different inputs and observe how its output changes. This probing paradigm of building causal arguments, however, has the classic limitation of construct validity for the measurements (Smith, 2005). That is, we may be tempted to ascribe a particular causal capability to an LLM if it answers well on a set of questions related to the capability, but the answers may not necessarily be due to the capability; they may be due to other factors such as exploiting some structure in the questions, or in the case of LLMs, memorizing similar questions that it encountered in its web-scale training set (Carlini et al., 2022). Hence we use multiple probing strategies to understand the causal capabilities of LLMs. In addition to the standard practice of testing LLM performance on existing benchmarks, we conduct memorization tests to estimate the extent to which a benchmark dataset may have been memorized, construct novel datasets to avoid memorization concerns, and conduct semantically-equivalent perturbation and other interventions on the input prompts to understand what the LLM pays attention to.</p>
<p>Benchmark Tests and Question-Answer Evaluation: The standard approach to evaluating LLM behavior is to ask it questions and evaluate its answers. Our question is the prompt for the LLM; and the LLM's completion of the text is the answer. Most large scale benchmarks and evaluations follow this approach, using purposefully created benchmarks and/or adapting standardized exams written to assess human capability (Ghazal et al., 2017; Nori et al., 2023; Zhong et al., 2023).</p>
<p>Our evaluation approach to characterizing the causal capabilities of LLMs begins with such question-answering evaluations. For each evaluation, we ask a series of questions or causal challenges and score the correctness of the resulting answer. Then, we probe deeper, to better understand the threats to the validity of the question-answer evaluations, and the likely robustness of LLM capabilities.</p>
<p>Memorization Test ${ }^{3}$ : The primary threat to the validity of benchmark or question-answer style assessments is that the LLM may have directly memorized the benchmark answers. In this case, our questions are likely not testing the LLM's inherent capability to complete a task (unless memorization is the capability we are testing!)</p>
<p>To test whether the LLM has memorized a particular dataset or benchmark, we give the LLM a partial row of data from the dataset and ask it to autocomplete the remainder of the row. For non-tabular datasets, we present a snippet of the file and ask it to auto-complete from that starting point. To encourage the LLM to succeed, we prepend details about the dataset, such as its name, URL, and description, and also provide few-shot examples. The final measurement from the memorization test is the percentage of rows the LLM was able to regenerate correctly. Supplementary E. 1 provides additional details on the procedure.</p>
<p>Redaction Test: When we see an LLM generate a correct answer to a question, we are still sometimes unsure why. Is the LLM attending to the appropriate aspects of the question, or has it learned to repeat unreliable patterns that may lead to errors in the future. To better understand what aspects of the prompt or question an LLM is attending to, we use redaction and perturbation tests, motivated from explainable AI methods for NLP Sinha et al. (2021); Danilevsky et al. (2020). First, we redact words in the prompt, one by one, and see how the answer changes each time. Changes in the answer indicate the LLM is attending to the redacted word.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3 Generating causal graphs using LLMs</h1>
<p>The task of building a causal graph is a foundational step in a causal inference workflow. Typically, the goal is to obtain a directed graph where the edges denote the presence and direction of causal effect. Such a graph characterizes the underlying data-generating process (DGP) for a dataset and specifies how a change in one variable may (or may not) affect the others. This graph is then used as a base on which downstream analysis relevant to a task is conducted, such as for effect inference, prediction or attribution. Having the correct graph that encodes causal assumptions is critical for ensuring the correctness of any downstream analysis.</p>
<p>Causal discovery attempts to address the challenge of specifying a causal graph by searching for a graph consistent with observed evidence of covariance between the variables in data Peters et al. (2017). The allure of causal discovery is relying on empiricism and avoiding subjective model misspecification errors that could bias the downstream workflow. The challenge, however, is that it is generally not possible to learn the correct graph for a given dataset, given only observational data. The reason is that multiple graph structures are equally likely given the same data distribution, a set of graphs known as the Markov equivalence class (Pearl, 2009c). In the last two decades, two main approaches have been proposed to overcome this limitation. The first is to restrict data-generating process to specific functional forms under which identification of a single graph is possible (Glymour et al., 2019). In some specific settings, such as adding non-gaussian noise to linear data-generating process (Shimizu et al., 2006) or assuming that all functions are non-linear with additive noise (Zhang \&amp; Chan, 2006; Zhang \&amp; Hyvärinen, 2009), identification is possible. However, there still exist simple settings that are unidentifiable, e.g., a dataset with linear equations and gaussian noise. The second approach is to utilize the power of deep learning to model covariances of all variables jointly and hope that it improves the quality of learnt graphs. However, the identification issue is still not resolved. As a result, recent evaluations of state-of-the-art causal discovery methods on real-world datasets present a sobering picture of their effectiveness (Kaiser \&amp; Sipos, 2022; Tu et al., 2019; Huang et al., 2021).</p>
<p>LLMs offer a fresh perspective on the use of statistical algorithms for constructing a causal graph. This perspective focuses on the metadata associated with variables in a dataset, rather than their data values. Typically, such metadatabased reasoning is done by human domain experts when they construct causal graphs. By looking at the names of variables and the problem context, for example, people can construct a causal structure based on their knowledge of physics, common sense, or specialized domain knowledge. However, this is a challenging process, hindering the widespread use of such metadata-based reasoning.</p>
<p>We find that LLMs can bridge this gap by filling in the domain knowledge that earlier only humans could provide. In contrast to causal discovery algorithms that use data values of variables, LLMs can infer causal structure by reasoning on metadata associated with the variables, for example, the name of the variable and the problem context expressed in natural language. In other words, similar to how domain experts formalize their knowledge in a graph, LLMs can use the knowledge from their training data to infer the edges of the causal graph. To differentiate from the existing covariance-based causal discovery, we call the LLMs capability as knowledge-based causal graph generation.</p>
<p>Remarkably, LLMs practicing knowledge-based causal graph generation outperform state-of-the-art covariance-based algorithms on causal discovery benchmarks. Below we present experiments on inferring causal structure using LLMs. We first start with pairwise causal discovery: a simple task involving two variables where the goal is to decide the direction of causal effect, across a range of domains. Next, we will study the problem of full graph discovery in two datasets: one in medicine and one in climate science. While the first task includes many "simple" relationships that an average person is expected to answer correctly, the other two tasks require specialized knowledge (in neuropathic pain and arctic atmosphere science respectively) in order to check whether LLM's graph generation capabilities extend to complex domains.</p>
<h3>3.1 Pairwise causal edge inference: Inferring causal direction among variable pairs</h3>
<p>We start with the pairwise causal edge inference task (Hoyer et al., 2008). Typically, these tasks are set up as discovering causal relations between two variables based on observed data. In this work, instead of relying on observed data, we directly ask large language models whether a variable causes another variable.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Variable A</th>
<th style="text-align: left;">Variable B</th>
<th style="text-align: left;">Domain</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Age of Abalone</td>
<td style="text-align: left;">Shell weight</td>
<td style="text-align: left;">Zoology</td>
</tr>
<tr>
<td style="text-align: left;">Cement</td>
<td style="text-align: left;">Compressive strength of concrete</td>
<td style="text-align: left;">Engineering</td>
</tr>
<tr>
<td style="text-align: left;">Alcohol</td>
<td style="text-align: left;">Mean corpuscular volume</td>
<td style="text-align: left;">Biology</td>
</tr>
<tr>
<td style="text-align: left;">Organic carbon in soil</td>
<td style="text-align: left;">Clay content in soil</td>
<td style="text-align: left;">Pedology</td>
</tr>
<tr>
<td style="text-align: left;">PPFD (Photosynthetic Photon Flux Density)</td>
<td style="text-align: left;">Net Ecosystem productivity</td>
<td style="text-align: left;">Physics</td>
</tr>
<tr>
<td style="text-align: left;">Drinking water access</td>
<td style="text-align: left;">Infant mortality</td>
<td style="text-align: left;">Epidemiology</td>
</tr>
<tr>
<td style="text-align: left;">Ozone concentration</td>
<td style="text-align: left;">Radiation</td>
<td style="text-align: left;">Atmospheric Science</td>
</tr>
<tr>
<td style="text-align: left;">Contrast of tilted Gabor patches</td>
<td style="text-align: left;">Accuracy of detection by participants</td>
<td style="text-align: left;">Cognitive Science</td>
</tr>
<tr>
<td style="text-align: left;">Time for 1/6 rotation of a Stirling engine</td>
<td style="text-align: left;">Heat bath temperature</td>
<td style="text-align: left;">Engineering</td>
</tr>
<tr>
<td style="text-align: left;">Time for passing first segment of a ball track</td>
<td style="text-align: left;">Time for passing second segment</td>
<td style="text-align: left;">Basic Physics</td>
</tr>
</tbody>
</table>
<p>Table 1: Example cause-effect pairs from the Tübingen benchmark. The task is to determine whether Variable A causes Variable B, or vice-versa.</p>
<h1>3.1.1 Tübingen cause-effect pairs dataset</h1>
<p>This dataset (Mooij et al., 2016) consists of data for 108 different cause-effect pairs selected from 37 datasets from various domains, including meteorology, biology, medicine, engineering, and economics (see examples in Table 1). As directionality of an edge is a fundamental building block for learning the full graph, it is a widely used dataset for benchmarking causal discovery algorithms. However, the Markov equivalence class of graphs admissible for a two-node graph $(\mathrm{A}, \mathrm{B})$ contains both $A \rightarrow B$ and $B \rightarrow A$. Therefore, the dataset remains a challenging one for causal discovery algorithms, with many recent methods achieving less than $80 \%$ accuracy, as shown in Table 2. The best known accuracy is $83 \%$, achieved by the Mosaic algorithm (Wu \&amp; Fukumizu, 2020).</p>
<p>We now apply LLMs to the pairwise edge inference problem. We extract the names of each variable from the benchmark and use them to construct prompts for the LLMs. Example prompts are shown in Suppl. A. 1 (Table 14). We start with a simple prompt that asks, "Does changing A cause a change in B?", where A and B are the variable names. For a given pair $(A, B)$, we ask the question in both directions and then take the mean accuracy. We choose the mean accuracy since it maps correctly to the accuracy of any method choosing between $A \rightarrow B$ or $B \rightarrow A$. Specifically, the LLM-based method obtains an accuracy of 0 if it answers both questions incorrectly. If it answers one of the questions correctly (indicating that none cause each other or both cause each other), then it effectively chooses the output at random $(A \rightarrow B$ or $B \rightarrow A)$. We also report weighted accuracy, as recommended by Mooij et al. (2016) to avoid the effect of overcounting some similar pairs.</p>
<p>Results. The second half of Table 2 shows the performance of different LLMs on the task. Similar to prior studies on capabilities of LLMs (Wei et al., 2022a), we observe the emergent behavior that only text-davinci-002 and above achieve a non-trivial accuracy than random chance. With GPT3.5 class models (text-davinci-003 and gpt-3.5-turbo), accuracy of LLM-based method reaches $83 \%$ and is competitive to the covariance-based causal discovery algorithms. This suggests that the ability of inferring causal direction emerges as the size of the model is increased. It may also depend on the specific training data and the use of reinforcement learning from human feedback (RLHF) for instruction finetuning.</p>
<p>Following recent work showing the value of the prompt tuning for obtaining high-quality results (Wei et al., 2022b; Long et al., 2023), we employ two different kinds of prompts. In the first, we prepend the prompt with the message, "You are a helpful assistant for causal reasoning", with the intention of steering the LLM's output space towards more causally consistent answers. This simple message of asking gpt-turbo-3.5 (i.e., ChatGPT) to be an agent of causal reasoning increases the (weighted) accuracy by almost $5 \%$ to $86.9 \%$, consistent with the impact of prompt engineering observed by (Long et al., 2023). The accuracy is higher than previously reported LLM-based accuracy on the task from Choi et al. (2022) that used the davinci-instruct-beta model ( $83 \%$ unweighted accuracy). Moreover, by changing the prompt to instead ask a single question that asks the LLM to output the more likely causal direction between $A \rightarrow B$ or $A \leftarrow B$ while explaining its reasoning in a "step-by-step" manner (see Suppl. A.1), the weighted accuracy improves to</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Acc.</th>
<th style="text-align: center;">Wt. Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Slope (Marx \&amp; Vreeken, 2017)</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: left;">bQCD (Tagasovska et al., 2020)</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: left;">PNL-MLP (Zhang \&amp; Hyvarinen, 2012)</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: left;">Mosaic (Wu \&amp; Fukumizu, 2020)</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.82</td>
</tr>
<tr>
<td style="text-align: left;">ada</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">text-ada-001</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">babbage</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">text-babbage-001</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">curie</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.52</td>
</tr>
<tr>
<td style="text-align: left;">text-curie-001</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">davinci</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.47</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-001</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-002</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.79</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: left;">LMPrior (Choi et al., 2022)</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo (causal agent)</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo (single prompt)</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.92</td>
</tr>
<tr>
<td style="text-align: left;">gpt-4 (single prompt)</td>
<td style="text-align: center;">$\mathbf{0 . 9 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 7}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy of different versions of GPT on the Tübingen cause-effect pairs dataset. The best LLM performance outperforms the current state-of-the-art covariance-based approaches that rely on observational data of the two variables. Weighted accuracy weights individual pairs to account for overcounting due to some pairs sharing the same source dataset. The causal agent is gpt-3.5-turbo with system message set as "You are a helpful assistant for causal reasoning.". LMPrior uses davinci-instruct-beta.
$92.1 \%$. Finally, using gpt-4 with this prompt, the weighted accuracy shoots up to $97 \%$. This accuracy is substantially higher than that of the best-known covariance-based causal discovery method.</p>
<p>Understanding LLM output. The combined single prompt allows us to inspect the LLM output, since it provides an explanation in addition to its final answer. For the examples below, we show the output from gpt-3.5-turbo. Let us look at the example, (age of abalone, length of abalone) where the LLM correctly outputs that changing the age of an abalone is more likely to change its length. Figure 3(a) shows the LLM's output. Its explanation gives the impression that it can reason about the problem using its knowledge to arrive at the correct answer:</p>
<p>However, on another pair that arguably should involve similar reasoning, (age of abalone, diameter of abalone), the LLM outputs an incorrect answer and its explanation does not make much sense (Figure 3(b)).</p>
<p>The fact that the LLM can make such errors is not surprising, since it is not explicitly optimizing for being consistent with prior knowledge or causal mechanisms. However, it is remarkable that such errors occur infrequently: The LLM made only 12 errors in the dataset of 108 pairs. On some other pairs where the LLM's output did not match the ground-truth, it reveals possible ambiguity in the variable names, that can be fixed by adding more context to the question. Consider the example of (ozone concentration, radiation) below where the LLM initially provides an incorrect answer.</p>
<p>LLM: 1. Ozone is a gas in Earth's atmosphere that absorbs most of the Sun's ultraviolet (UV) radiation.
2. Chlorofluorocarbons (CFCs), pollutants that were once widely used in refrigerants and aerosol sprays, have been found to destroy ozone molecules in the atmosphere.
3. The destruction of the ozone layer in the upper atmosphere causes more UV radiation to reach the surface of Earth.</p>
<p>SYSTEM: You are a helpful assistant for causal reasoning.
USER: Which cause-and-effect relationship is more likely?
A. changing the age of an abalone causes a change in its length.
B. changing the length of an abalone causes a change in its age.
Let's work this out in a step by step way to be sure that we have the right answer. Then provide your final answer within the tags <Answer>A/B</Answer>.
GPT-3.5-TURBO: 1. Cause: Changing the age of an abalone
Effect: a change in its length.
2. Cause: Changing the length of an abalone
Effect: a change in its age.
Based on the biological knowledge about abalones, it is more likely that changing the age of an abalone causes a change in its length. This is because the length of an abalone is known to increase as it ages, and not the other way around. Therefore, the correct answer is $&lt;$ Answer $&gt;$ A $&lt;$ Answer $&gt;$.</p>
<p>SYSTEM: You are a helpful assistant for causal reasoning. USER: Which cause-and-effect relationship is more likely? A. changing the age of an abalone causes a change in its diameter.
B. changing the diameter of an abalone causes a change in its age.
Let's work this out in a step by step way to be sure that we have the right answer. Then provide your final answer within the tags <Answer>A/B</Answer>.
GPT-3.5-TURBO: Step 1: Determine which variable is the cause and which is the effect.
In Option A, "age" is the cause and "diameter" is the effect.
In Option B, "diameter" is the cause and "age" is the effect.
Step 2: Consider which variable is more likely to cause a change in the other.
Abalone growth is primarily determined by environmental factors such as water temperature, food availability, and water quality. It is unlikely that changing the age of an abalone would cause its diameter to change significantly. On the other hand, changes in the environment could potentially impact the size of the abalone, and therefore, its diameter could change. Therefore, Option B is more likely: changing the diameter of an abalone causes a change in its age. $&lt;$ Answer $&gt;$ B $&lt;$ Answer $&gt;$
(a) Correct answer
(b) Incorrect answer</p>
<p>Figure 3: Probing reasoning of LLMs for deciding causal direction. Two example outputs from a LLM (gpt-3.5turbo). The first gives the impression of the LLM performing causal reasoning to arrive at the correct answer-that age may cause the length of abalone to increase. The second example asks a question that requires arguably the same causal knowledge but the LLM proceeds with an incoherent argument and provides an incorrect answer.
4. Increased UV radiation can lead to various negative effects such as skin cancer, cataracts, and damage to crops.</p>
<p>From these facts, we can determine that a decrease in ozone concentration would lead to an increase in radiation. Therefore, Option A is more likely:
$&lt;$ Answer $&gt;$ A $&lt;$ Answer $&gt;$</p>
<p>The correct answer according to the benchmark is that radiation causes ozone concentration. However, the LLM interpreted the question as about the ozone concentration in the stratospheric ozone layer, whereas the question was about ground-level ozone concentration measured in a city. After adding this context, the LLM's answer changes,</p>
<p>USER: Wait, this question is not about stratospheric ozone concentration. Rather, I'm asking about the ground-level ozone concentration measured in a city and the corresponding ground-level radiation due to sunlight.</p>
<p>GPT-3.5-TURBO:: I see, thank you for clarifying. In that case, the answer is likely option B: changing radiation causes a change in ground-level ozone concentration. This is because ground-level ozone is formed through a chemical reaction between nitrogen oxides (NOx) and volatile organic compounds (VOCs) in the presence of sunlight. This reaction is known as photochemical smog and occurs more frequently on hot, sunny days when there is more radiation. Therefore, an increase in radiation would likely lead to an increase in ground-level ozone concentration.</p>
<p>Finally, in some cases, the direction of the cause-effect relationship is not absolute and depends on other supporting information. For the pair (stock return of Hang Seng bank, stock return of HSBC Hldgs.), the LLM correctly determines that Hang Seng is a subsidiary of HSBC Hldgs. but its answer does not match the benchmark's label. HSBC Hldgs. owns a majority stake in Hang Seng bank, so the benchmark answer indicates that any change in Hang Seng's stock return should also affect HSBC Hldgs. stock return. At the same time, since stock markets do not follow any fixed pattern, it is possible that a major change in HSBC Hldgs. stock return causes people to invest more or less in Hang Seng Bank and hence affect its stock return, as the LLM output argues.</p>
<p>LLM: To determine which cause-and-effect relationship is more likely, we need to consider the nature of the two companies and their relationship with each other.</p>
<p>Hang Seng Bank and HSBC Holdings are two separate companies but they are both major players in the Hong Kong financial market. Hang Seng Bank is a subsidiary of HSBC Holdings and is listed on the Hong Kong Stock Exchange. HSBC Holdings is a multinational banking and financial services company that operates in over 60 countries worldwide.</p>
<p>Given that relationship and the fact that Hang Seng Bank is a subsidiary of HSBC Holdings, it is more likely that:
<Answer>B</Answer>changing the stock return of HSBC Hldgs causes a change in the stock return of Hang Seng Bank.</p>
<p>Creating a novel Tubingen dataset. It is possible that LLMs achieve high accuracy in predicting causal direction because Tubingen pairs is a popular dataset which may have been included in LLMs' training data. To test generalization of LLMs to causal pairs that were not a part of LLMs' training data, we build a new dataset. Since the training cutoff date for the models we use (GPT-3.5-turbo and GPT-4) is September 2021, we aim to collect cause-effect pairs that were published after the cutoff date. To do so, we use Google Book search to search for cause-effect pairs in books published on or after Oct 2021. We first categorize pairs from the original dataset into domains such as biology, engineering, medicine, and so on. For each of these categories, we search for appropriate books that describe a causal relationship in their text. Based on this procedure, we obtain a dataset with 67 variable pairs. For instance, the variable pair, "battery capacity", "ambient temperature" was derived from the book, Battery Management System and Its Applications, published in 2022. For a sample list of variables, see Appendix A.3. The full dataset is available at https://github.com/py-why/pywhy-llm.</p>
<p>On this dataset, we applied the exact same (single) prompt from the original benchmark. GPT-3.5-turbo and GPT-4 obtain $80.3 \%$ and $98.5 \%$ accuracy respectively, indicating that the capability to identify causal direction generalizes to variable pairs outside of popular datasets.</p>
<h1>3.1.2 Neuropathic pain dataset</h1>
<p>We now consider a ground-truth causal graph constructed by medical experts on neuropathic pain diagnosis (Tu et al., 2019). It contains relationships between different nerves and the associated symptoms that patients express. Table 3 shows example of edges in the graph. Unlike the Tübingen benchmark which contains some commonly known variables, understanding the names of variables in this graph requires specialized medical knowledge.</p>
<p>We evaluate LLMs on a pairwise causality task similar to the Tubingen benchmark. We do so by creating a pair for each edge in the graph and setup the task to determine the direction of the edge. That is, for all the edges present in the ground-truth graph, we ask an LLM to infer the correct direction of causality. We obtain a dataset of 475 edges.</p>
<p>Table 4 shows the accuracy of different LLMs in determining the direction of edges. Similar to the Tübingen benchmark, smaller language models are unable to obtain accuracy substantially higher than random chance (50\%). In addition, even text-davinci-003, the largest text completion model in our comparison set, is unable to obtain a high accuracy using the two-sided prompt. However, chat-based models, trained using human feedback, are able to distinguish causal</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Variable A</th>
<th style="text-align: left;">Variable B</th>
<th style="text-align: left;">Dir.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Right L1 Radiculopathy</td>
<td style="text-align: left;">Right adductor tendonitis</td>
<td style="text-align: left;">$\rightarrow$</td>
</tr>
<tr>
<td style="text-align: left;">Pharyngeal discomfort</td>
<td style="text-align: left;">Right C3 Radiculopahty</td>
<td style="text-align: left;">$\leftarrow$</td>
</tr>
<tr>
<td style="text-align: left;">Right L5 Radiculopathy</td>
<td style="text-align: left;">Lumbago</td>
<td style="text-align: left;">$\rightarrow$</td>
</tr>
<tr>
<td style="text-align: left;">Left PTA</td>
<td style="text-align: left;">Left L4 Radiculopahty</td>
<td style="text-align: left;">$\leftarrow$</td>
</tr>
<tr>
<td style="text-align: left;">Left T3 Radiculopahty</td>
<td style="text-align: left;">Toracal dysfunction</td>
<td style="text-align: left;">$\rightarrow$</td>
</tr>
<tr>
<td style="text-align: left;">DLS L5-S1</td>
<td style="text-align: left;">Right S1 Radiculopathy</td>
<td style="text-align: left;">$\rightarrow$</td>
</tr>
<tr>
<td style="text-align: left;">Left C3 Radiculopathy</td>
<td style="text-align: left;">DLS C2-C3</td>
<td style="text-align: left;">$\leftarrow$</td>
</tr>
<tr>
<td style="text-align: left;">Left C7 Radiculopathy</td>
<td style="text-align: left;">Left medial elbow problem</td>
<td style="text-align: left;">$\rightarrow$</td>
</tr>
<tr>
<td style="text-align: left;">Right Ischias</td>
<td style="text-align: left;">Right L5 Radiculopathy</td>
<td style="text-align: left;">$\leftarrow$</td>
</tr>
<tr>
<td style="text-align: left;">Right Morton trouble</td>
<td style="text-align: left;">Right L5 Radiculopathy</td>
<td style="text-align: left;">$\leftarrow$</td>
</tr>
</tbody>
</table>
<p>Table 3: Example cause-effect pairs from the Neuropathic pain diagnosis benchmark. 'Dir.' refers to the ground-truth causal direction between the variables.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ada</td>
<td style="text-align: center;">40.1</td>
</tr>
<tr>
<td style="text-align: left;">text-ada-001</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">babbage</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">text-babbage-001</td>
<td style="text-align: center;">50.9</td>
</tr>
<tr>
<td style="text-align: left;">curie</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">text-curie-001</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">davinci</td>
<td style="text-align: center;">38.4</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-001</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-002</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: center;">55.1</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo</td>
<td style="text-align: center;">71.1</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo (neuropathic pain expert)</td>
<td style="text-align: center;">75.1</td>
</tr>
<tr>
<td style="text-align: left;">gpt-4</td>
<td style="text-align: center;">78.4</td>
</tr>
<tr>
<td style="text-align: left;">gpt-4 (neuropathic pain expert)</td>
<td style="text-align: center;">84.3</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003 (single prompt)</td>
<td style="text-align: center;">86.0</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo (single prompt)</td>
<td style="text-align: center;">85.5</td>
</tr>
<tr>
<td style="text-align: left;">gpt-4 (single prompt)</td>
<td style="text-align: center;">$\mathbf{9 6 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy of different versions of GPT on the inferring the edge directions of the Neuropathic pain diagnosis graph. As with the Tübingen dataset, LLMs like gpt-3.5-turbo obtain more than $85 \%$ accuracy on determining the direction of edges. The causal agent is gpt-3.5-turbo with a system message set as "You are a helpful assistant for causal reasoning."
direction accurately. With the two-sided prompt, gpt-3.5-turbo achieves the highest accuracy of $75 \%$. The choice of prompt also has a big impact. As with the Tübingen dataset, using single prompt per variable pair leads to a significant increase in accuracy: text-davinci-003 now matches gpt-3.5-turbo and both models obtain more than $85 \%$ accuracy. While the performance of these models is impressive, gpt-4 obtains an even higher accuracy ( $96 \%$ ), yielding more than 457 correct responses out of 475 .</p>
<p>Understanding LLM output. We use the output of gpt-3.5-turbo (single prompt) to understand how LLMs obtain such high accuracies. On manual inspection of the output by gpt-3.5-turbo, we find that it correctly understand the medical terms in almost all cases. In the example below, the LLM provides the correct definitions of DLS T5-T6 and Left T6 Radiculopathy (without access to any such information in the prompt) and provides the correct causal direction. Suppl. A. 2 provides the LLM output for this pair. However, even as it provides the correct definitions, it can provide erroneous output in unpredictable ways. Here it went from the correct definition of the same concept (DLS) to confusing it with the "T5-T6 region" associated with DLS T5-T6 (see Suppl. A. 2 for the full output).</p>
<p>Moreover, the example in Table 5 adds evidence that LLMs may not be capable of consistent and coherent reasoning, even as they output correct answers for a majority of the causal questions. In the left panel, the LLM provides the correct reasoning for most of its answer but ultimately chooses a final answer inconsistent with its own reasoning (A instead of B). Unlike reasoning errors, fortunately self-consistency is an easier problem to verify using the LLM itself. We find that gpt3.5-turbo and earlier class of models are unable to check consistency of an output, but GPT-4 is able to. Therefore, it may be possible to pass all LLM outputs through a GPT-4 verifier to ensure self-consistency of the outputs. The right panel of Table 5 shows the verifier prompt and output from GPT-4 (the question and answer from gpt-3.5-turbo are not fully shown for brevity).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Initial response</th>
<th style="text-align: center;">Critiquing the response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SYSTEM: You are an expert on neuropathic pain diagnosis. <br> USER: You are a helpful assistant to a neuropathic pain diagnosis expert. Which cause-and-effect relationship is more likely? <br> A. Right C7 Radiculopathy causes DLS C6C7. <br> B. DLS C6-C7 causes Right C7 Radiculopathy. <br> Let's work this out in a step by step way to be sure that we have the right answer. Then provide your final answer within the tags $&lt;$ Answer $&gt;\mathrm{A} / \mathrm{B}&lt;$ /Answer $&gt;$. <br> GPT-3.5-TURBO: First, we need to understand what each term means. DLS stands for degenerative lumbar spondylosis, which is a condition where the spinal discs in the lower back degenerate and cause pain. C6-C7 refers to the sixth and seventh vertebrae in the cervical (neck) spine. Radiculopathy is a condition where a nerve root is compressed or damaged, causing pain, numbness, or weakness in the corresponding area of the body. <br> Based on this information, it is more likely that option $A$ is the correct cause-and-effect relationship. This is because radiculopathy is often caused by compression or damage to a nerve root, which could be the result of degenerative changes in the spine (such as DLS). In this case, the degeneration in the lower back (DLS) could be causing compression or damage to the nerve root at the C7 level, resulting in right C7 radiculopathy. <br> Therefore, the answer is $&lt;$ Answer $&gt;A&lt;/$ Answer $&gt;$.</td>
<td style="text-align: center;">USER: Analyze the output from an AI assistant. Is the final answer (A) consistent with the reasoning provided by the assistant? <br> Question: <br> {{USERPROMPT-FROM-BEFORE}} <br> AI assistant: {{ANSWER-FROMBEFORE}} <br> GPT-4-WEB: The final answer (A) is not consistent with the reasoning provided by the assistant. The reasoning suggests that DLS C6-C7 could cause Right C7 Radiculopathy, which corresponds to option B. The correct answer should be $&lt;$ Answer $&gt;$ B $&lt;$ /Answer $&gt;$.</td>
</tr>
</tbody>
</table>
<p>Table 5: Critiquing LLM output using another LLM instance. To increase robustness of an LLM's response, we can use GPT-4 as a critic. The left panel shows an incorrect reply from gpt-3.5-turbo wherein the reasoning is correct but the LLM outputs the incorrect option (A). We create a special "critique" prompt that asks gpt-4 to evaluate the response from an AI assistant for self-consistency. gpt-4 finds the logical inconsistency and provides the correct answer.</p>
<p>In addition to self-consistency, when LLM outputs do not match the ground-truth, they can sometimes help critique the ground-truth itself. For instance, the ground-truth graph contains an edge from Left L5 Radiculopathy to Obesity. However, the LLM output provides the opposite direction: obesity can cause L5 Radiculopathy, which is corroborated by research that claims obesity as a risk factor for radiculopathy (Atchison \&amp; Vincent, 2012; Tomić et al., 2009).</p>
<p>LLM: Step 1: Understand the terms</p>
<ul>
<li>Neuropathic pain: pain caused by damage or disease affecting the nervous system</li>
<li>L5 Radiculopathy: a condition where the nerve root that exits the spine at the L5 level is compressed or irritated, causing pain, numbness, or weakness in the leg</li>
<li>Obesity: a medical condition characterized by excessive body fat that increases the risk of health problems</li>
</ul>
<p>Step 2: Analyze the cause-and-effect relationship</p>
<ul>
<li>Option A: Left L5 Radiculopathy causes Left Obesity. This relationship does not make sense because L5 radiculopathy is a nerve condition that affects the leg, while obesity is a whole-body condition related to excessive body fat. It is unlikely that a leg nerve condition would cause obesity.</li>
<li>Option B: Left Obesity causes Left L5 Radiculopathy. This relationship is more plausible because obesity can put extra pressure on the spine and nerves, leading to nerve compression or irritation. It is possible that obesity could cause L5 radiculopathy.</li>
</ul>
<p>Step 3: Choose the more likely cause-and-effect relationship</p>
<ul>
<li>Based on the analysis, Option B is more likely: Left Obesity causes Left L5 Radiculopathy.
$&lt;$ Answer $&gt;$ B $&lt;$ Answer $&gt;$</li>
</ul>
<h1>3.1.3 Other properties of pair-wise relationships</h1>
<p>In addition to asking the LLM about the existence and direction of a causal relationship between a pair of edges, we can also ask many other questions as well. For example, we can ask if a relationship may be confounded by other variables, or what variables may mediate or moderate the relationship. We can ask whether a causal relationship is monotonic, whether its effects are homogeneous or not with respect to other properties. Other useful properties to identify include the time-lag of a causal relationship, whether it is stable, and whether it is subject to spillover effects. Designing prompts for these questions should be a straightforward extension of the pairwise edge existence prompts.</p>
<h3>3.2 Full graph generation</h3>
<p>We now extend the pairwise analysis to generating the full graph. Given a set of variables, a straightforward extension is to construct a list of all possible variable pairs and repeat the pairwise test for each pair. However, full graph generation provides additional challenges. First, the pairwise analysis assumed the existence of an edge and our goal was simply to determine its direction. In the graph generation problem, there are three possible options for each variable pair: $A \rightarrow B$, $A \leftarrow B$, or no edge exists. Second, graph generation requires the ability to distinguish between direct and indirect causes, given the other nodes in the graph. For example, if the true relationship is $A \rightarrow B \rightarrow C$, then it may be correct to output both $A \rightarrow B$ and $A \rightarrow C$ in a pairwise task, but for graph generation, outputting $A \rightarrow C$ is an incorrect answer. Moreover, the decision depends on which variables are present in the input variable set for the graph generation task. For example, if $B$ is not one of the variables in the input variable set, then $A \rightarrow C$ is a valid edge and should be included.</p>
<p>Due to these reasons, it is unclear how well a simple extension of the pairwise task using LLMs will transfer to the full graph generation problem. For simple graphs over 3-4 variables, Long et al. (2023) show that LLMs can obtain promising accuracy in inferring the edges. Below we report the results of applying the pairwise LLM prompts on more complex, real-world datasets involving specialized knowledge: on pain diagnosis and on artic sea ice coverage.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Precision</th>
<th>Recall</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random</td>
<td>0.25</td>
<td>0.5</td>
<td>0.33</td>
</tr>
<tr>
<td>chatGPT (Tu et al., 2023)</td>
<td>1</td>
<td>0.12</td>
<td>0.21</td>
</tr>
<tr>
<td>text-davinci-003</td>
<td>0.59</td>
<td>0.68</td>
<td>0.63</td>
</tr>
<tr>
<td>gpt-3.5-turbo (single prompt)</td>
<td>0.66</td>
<td>$\mathbf{0 . 7 1}$</td>
<td>$\mathbf{0 . 6 8}$</td>
</tr>
<tr>
<td>gpt-4 (single prompt)</td>
<td>$\mathbf{0 . 7 4}$</td>
<td>0.58</td>
<td>0.65</td>
</tr>
</tbody>
</table>
<p>Table 6: Accuracy of different versions of GPT on the Neuropathic pain dataset.</p>
<h1>3.2.1 Neuropathic pain dataset</h1>
<p>We continue our exploration with the neuropathic pain dataset. It has 221 variables, so the total number of possible variable pairs are $C(221,2)=24310$. Before conducting full graph generation, we utilize a 100-pair dataset provided by the authors of the dataset to evaluate LLM's capability to infer causal relationships (Tu et al., 2023). It has 50 variable pairs that form edges in the graph and 50 pairs that do not form an edge. We notice that many of the variable names are in Swedish, so we employ an LLM (gpt-4-web) to translate the names to English as a preprocessing step.</p>
<p>Results. Table 6 shows the results of different LLM-based methods. As a baseline, using an algorithm that return either direction at random for each pair, we would obtain 0.5 recall and 0.25 precision on recovering the true edges, leading to an F1 score of 0.33 . Using a simple prompt, " $A$ causes $B . R$ and $L$ refer to the right and left sides of the body respectively. Answer with true or false.", Tu et al. (2023) use chatgpt-3.5-web to obtain an F1 score of 0.21 . This LLM performance is worse than the random baseline which may indicate that LLMs are not useful for full graph generation. However, with a different prompt, we find that the same model can provide over 3X increase in the F1 score. We provide the LLMs with an identical prompt to the "single" prompt from the pairwise task, with one addition: we add a third option, "C: No causal relationship exists". With this prompt, gpt-3.5-turbo (API analogue of chatgpt-3.5-web) obtains 0.68 F 1 score. This score is double the F1 score expected from a random guess baseline, indicating that the output from LLMs provide a non-trivial utility for inferring causal edges.</p>
<p>Using a novel medical dataset. It is possible that the Neuropathic dataset may be a part of the training set for the LLMs. Therefore, we consider a causal DAG proposed by Mascaro et al. (2023) that was published online in February $2022^{4}$, a few months after the training cutoff date of the LLMs considered. The DAG captures expert-provided relationships concering COVID-19 disease processes. We consider a subset of the full DAG containing 11 nodes, as reported by Mascaro et al. in their paper. Variables correspond to concepts in respiratory medicine, such as "Alveolar Epithelial infection" and "Viremia".</p>
<p>We apply the same prompt that we used for the Neuropathic dataset. GPT-4 obtains an F1 score of 0.73 which is comparable to the best-reported F1 on the Neuropathic dataset ( $\mathrm{F} 1=0.68$ ).</p>
<h3>3.2.2 Arctic sea ice dataset</h3>
<p>To evaluate LLM's ability to generate causal graphs, we now consider a dataset from a different field of science: arctic sea ice and atmospheric science. This dataset is on the drivers of arctic sea ice thickness (or coverage): what causes the arctic sea coverage to increase or decrease? In a recent publication, Huang et al. (2021) provide a domain knowledge-based graph with 12 variables and 48 edges. Variables in the graph include total cloud water path, sea level pressure, geopotential height, meridional and zonal wind at 10 m , net shortwave and longwave flux at the surface. Importantly, the graph contains some double-sided edges.</p>
<p>Since the number of variables is low (12), we are able to conduct an experiment to generate the full graph using LLMs. Huang et al. (2021) evaluate the accuracy of three recent discovery algorithms: temporal causality discovery framework (Nauta et al., 2019), NOTEARS (Zheng et al., 2018), and DAG-GNN (Yu et al., 2019). They evaluate the algorithms on the normalized Hamming distance (NHD) between the predicted graph $G^{\prime}$ and the ground-truth graph $G$.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>NHD</th>
<th>No. of predicted edges</th>
<th>Baseline NHD</th>
<th>Ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCDF</td>
<td>0.33</td>
<td>9</td>
<td>0.39</td>
<td>0.84</td>
</tr>
<tr>
<td>NOTEARS (Static)</td>
<td>0.33</td>
<td>15</td>
<td>0.44</td>
<td>0.75</td>
</tr>
<tr>
<td>NOTEARS (Temporal)</td>
<td>0.35</td>
<td>7</td>
<td>0.38</td>
<td>0.92</td>
</tr>
<tr>
<td>DAG-GNN (Static)</td>
<td>0.32</td>
<td>23</td>
<td>0.49</td>
<td>0.65</td>
</tr>
<tr>
<td>DAG-GNN (Temporal)</td>
<td>0.34</td>
<td>16</td>
<td>0.44</td>
<td>0.77</td>
</tr>
<tr>
<td>gpt-3.5-turbo</td>
<td>0.33</td>
<td>62</td>
<td>0.76</td>
<td>0.43</td>
</tr>
<tr>
<td>gpt-4</td>
<td>$\mathbf{0 . 2 2}$</td>
<td>46</td>
<td>0.65</td>
<td>$\mathbf{0 . 3 4}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Normalized hamming distance (NHD) for different causal discovery algorithms. Since NHD depends on the number of predicted edges, we compare the ratio of NHD and baseline NHD across algorithms. A lower NHD ratio is better. LLM-based graph generation (gpt-3.5-turbo) obtains comparable NHD and the lowest NHD ratio compared to recent covariance-based discovery algorithms.</p>
<p>For a graph with $m$ nodes, the distance is given by $\sum_{i, j=1}^{m} \frac{1}{m^{2}} 1_{G_{i j} \neq G_{i j}^{\prime}}$, the number of edges that are present in one graph but not the other, divided by the total number of all possible edges. The distance is zero if $G^{\prime}$ outputs exactly the same edges as $G$. For reference, since the number of nodes and edges in our ground-truth graph are 12 and 48 respectively, if $G^{\prime}$ outputs 48 completely different edges compared to the 48 edges of $G$, the NHD between $G$ and $G^{\prime}$ will be 0.66 . However, if we output no edges at all, the NHD will be better than the algorithm, 0.33 . Since the NHD depends on the number of edges returned by a discovery algorithm, we compare the ratio of NHD of the discovery algorithm and a "floor" baseline that outputs the same number of edges but all of them are incorrect. A lower ratio implies the multiple by which the graph discovery algorithm is better than the worst baseline returning the same number of edges.</p>
<p>Table 7 shows the NHD metric for the three covariance-based causal discovery algorithms and knowledge-based graph generation with LLMs. For each algorithm, we also show the number of predicted edges and the NHD for a worst case algorithm that predicts the same number of edges. NHD for the three covariance-based algorithms is similar. However, the ratio of NHD for the algorithm and the baseline algorithm provides the relative improvement that the algorithm provides compared to the baseline. Using the ratio metric, we see that DAG-GNN (static) performs the best and has the lowest Ratio distance.</p>
<p>For LLM-based graph generation, we use the same prompt as in the neuropathic pain dataset. Using gpt-3.5-turbo, we obtain a normalized hamming distance of 0.33 , comparable to the three recently proposed covariance-based causal discovery algorithms. However, the LLM returns 62 edges, so its NHD ratio (0.43) is substantially better than covariance-based algorithms. Finally, gpt-4 obtains the lowest NHD among all algorithms and significantly outperforms state-of-the-art covariance-based discovery algorithms. It outputs almost the same edges as the ground-truth (46) with an NHD of 0.22 , one-third less than other algorithms. Its NHD ratio is also the lowest. gpt-4 recovers over half (29) of the edges correctly with a precision of 0.63 (F1 score=0.57), indicating the competitiveness of LLM-based algorithms for causal generation.</p>
<p>Measuring LLM's NHD metric on a novel dataset. In addition to the Arctic sea ice dataset, we consider a dataset that was created in 2023 by consultation with medical experts on Alzheimer's disease (Abdulaal et al., 2023). Since the causal graph was created after the training cutoff date of September 2021, it cannot be present in the training data for LLMs. Abdulaal et al. (2023) present an expert-validated causal graph of 11 nodes. Variables in the graph include age, ventricular volume, brain MRI, etc. They use the same prompt that we use for the Arctic sea ice dataset and report NHD of the obtained graphs using different LLMs. Table 16 in Appendix A. 3 reports their results on the dataset. The results show that the graph generated by gpt-4 obtains the lowest NHD (0.14) compared to existing existing data-based methods such as NOTEARS (0.22) and DAG-GNN (0.37). Its NHD ratio is also the lowest (0.28). These results, similar to those obtained for the Arctic sea dataset, indicate the generalizability of LLMs like GPT-4 to for inferring full graph over novel datasets. Note that novel datasets may still contain concepts that existed before the LLM's training cutoff date. See Section 5.1 for a discussion on utility of LLMs with respect to novel scenarios.</p>
<p>Which cause-and-effect relationship is more likely? A: changing SLOT1 causes a change in SLOT2. B: changing SLOT3 causes a change in SLOT4. Let's work this out in a step by step way to be sure that we have the right answer. Then provide your final answer within the tags <Answer>A/B</Answer></p>
<p>Figure 4: We probe the importance of individual words for getting a correct answer by redacting a random word from a question. We highlight words based on their importance for getting a correct result. A white background indicates that redacting the word did not reduce accuracy. A dark blue highlight indicates that redacting the word reduced accuracy the most.</p>
<h1>3.3 Probing LLM behavior further</h1>
<p>To better characterize the validity and robustness of our experiments at the intersection of LLMs and building causal graphs, we report the results of our LLM probing experiments, described in Sec 2.4.</p>
<p>Memorization Test Results: We provide the LLM with the first 3 columns of the dataset (the row ID and 2 variable names), and ask the model to complete the remaining 2 columns of the row (the source dataset name and the ground truth causal direction. We find that GPT-3.5 is able to correctly recall $58 \%$ of the remaining cells in the dataset, and recall $19 \%$ of the rows without error. Our memorization test with GPT-4 is able to recall $61 \%$ of remaining cells in the dataset and $25 \%$ of rows without error. These results indicate that the Tübingen dataset is certainly in GPT's training set and that GPT has had an opportunity to memorize the dataset. However, the gap between percentage of inputs memorized and LLMs accuracy is still significant and more work is needed to understand the role of memorization for the inputs that are not captured by our memorization test.</p>
<p>As the neuropathic pain and the arctic sea ice coverage datasets are not available online in a tabular file format, we adapted our memorization test to ask for completions of randomized samples of JSON and code files ${ }^{5}$. We provide the LLM with a contiguous section of text from a randomly chosen location within a file, and ask the model to complete the file from that point. Counting the number of characters that are correctly reproduced by the LLM, provides an indication of whether the dataset has been memorized by the model. Our results, detailed in Section E.1, indicate that the neuropathic dataset has been at least partially memorized by GPT-3.5 Turbo, but not GPT-4, and the arctic sea ice dataset has not been memorized by either GPT-3-5 Turbo or GPT-4.</p>
<p>More generally, we expect knowledge-based causal graph generation to be founded partly on memorized information, but also dependent on the LLM's ability to process and transform seen causal relationships for use in multiple contexts. We can conceptually model the LLM's end-to-end efficacy $P(Y \mid D) P(D)$, where $P(D)$ represents the likelihood of a causal relationship being memorized by the LLM, and $P(Y \mid D)$ the likelihood that the relationships can be correctly transformed. Knowing that our benchmark dataset is memorized implies that our experiments can only measuring $P(Y \mid D)$, but not measure $P(D)$. That is, when evaluating an LLM on an existing benchmark, its performance on the benchmark is a test of how well the LLM's ability to process and transform its knowledge into the necessary causal relationship assuming that it has been trained using some representation of the underlying information.</p>
<p>Redaction Test: To characterize whether or not the LLM is attending to the correct words and concepts in a question when answering our pair-wise causal edge inference questions, we use a redaction test. Our redaction test shows what words are important for correctly labeling a causal relationship between two features. Figure 4. shows our results for experiments in gpt-3.5-turbo, averaged across 357 random redaction probes over our Tübingen experiment. If the LLM is correctly reading the question, we expect that it will pay a lot of attention to key instructions and to the options themselves. Here we see that some of the key instruction words that indicate we are asking about causal relationships, such as 'changing', and 'causes' as well as instructions for how to format the final answer in tags are</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>critically important, each affecting the accuracy of the result in the most redactions. The SLOT phrases are important but redundant (SLOT1 and SLOT4 are identical and SLOT2 and SLOT3 are identical). Therefore the LLM seems more robust to their individual redactions in this test, each individual redaction affecting the accuracy of the result, but less often. While most unimportant words do not affect the accuracy of the result much, we do see the redaction of number of 'minor' words having a surprisingly strong effect, possibly indicating a general sensitivity of the LLM to grammatical correctness.</p>
<h1>4 LLMs for Token Causality and Causal Judgments</h1>
<p>In the previous section, we saw how LLM's ability to capture common sense and domain knowledge allows for a knowledge-based approach to building a causal graph. In this section, we study how LLMs' can bring these capabilities to enable a systematized approach to token causality challenges. Token causality is motivated by problems of attribution and assigning responsibility in real world applications, such as legal reasoning, machine failure debugging, and rootcause analysis for system regressions. For example in tort law, the core problem of deciding how much blame for a plaintiff's injury is attributed to a defendant's action relative to other potentially mitigating events is fundamentally an token causal inference task.</p>
<p>Causal inference researchers have attempted to use formal causal models to define token causality in a way that is consistent with how humans naturally attribute cause and related concepts of responsibility, blame, and explanations to events and their outcomes (Halpern, 2016; Kueffner, 2021). This task has proven exceedingly difficult, because human judgments of causality depend on elements of background context that are difficult to formalize in an SCM. Types of background context that are fundamental to human causal judgments but difficult to formalize in an SCM include:</p>
<ul>
<li>Causal frame: The set of candidate causal events that are relevant to a particular outcome event. SCMs require relevant causes be included as endogenous variables, but in practice humans rely on domain knowledge and common sense to set the causal frame after the outcome occurs (Icard \&amp; Goodman, 2015). For example, there are a multitude of potential causes of forest fires, but in the case of a particular forest fire, upon learning lightning struck and that a hiker was carelessly smoking a cigarette during fire season, we know these are relevant causal events. We would also know whether to ignore "drought" or "lack of controlled burn" if they are not relevant in the case of this particular fire.</li>
<li>Necessary causality: Whether a candidate cause needed to happen for the outcome to occur.</li>
<li>Sufficient causality: Whether a candidate cause's occurance would have led to the outcome event if other causal events had occurred differently.</li>
<li>Normality: The degree to which causal events align with statistical norms or prescriptive norms (social, moral, or legal norms) (Phillips et al., 2015; Kominsky et al., 2015). When agents violate norms, they are typically judged to be more of a cause of resulting outcomes (Phillips et al., 2015; Kominsky et al., 2015). Human causal judgments depend highly on whether candidate cause is a norm violation, or whether it is more or less normal than other causal events.</li>
<li>Other human factors: Other human factors include bias towards action, handling intention and epistemic state, and how bad outcomes are interpreted. When the candidate cause is an agent's behavior, humans tend to ascribe more causality actions (e.g., tossing a cigarette) than to lack of actions (e.g, not doing a controlled burn) (Henne et al., 2017). In addition, when the candidate cause is an agent's behavior, whether the agent acted with intention and knew what they were doing (Nadelhoffer, 2006; Knobe, 2003) matters. Finally, human causal judgments also depend on whether the outcome is undesirable (e.g., causing a forest fire vs. causing a reforestation initiative) (Kominsky et al., 2015).</li>
</ul>
<p>Sidestepping the challenge of formalizing these concepts into a causal model, LLMs offer an opportunity to capture the necessary and relevant background context for an event directly from natural language description of an event.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Premise</th>
<th style="text-align: center;">Counterfactual Question</th>
<th style="text-align: center;">Multiple-choices answers</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">A woman does not order Chinese food.</td>
<td style="text-align: center;">What would have happened if she had ordered Chinese food?</td>
<td style="text-align: center;">The woman would have become less hungry.;The woman would have become very hungry.;That is not possible.</td>
</tr>
<tr>
<td style="text-align: center;">A woman sees a fire.</td>
<td style="text-align: center;">What would have happened if the woman had touched the fire?</td>
<td style="text-align: center;">She would have been burned.;She would not have been burned.;That is not possible.;She would have seen fire.</td>
</tr>
<tr>
<td style="text-align: center;">A bird lands in a forest.</td>
<td style="text-align: center;">What would have happened if a plane had landed in the forest?</td>
<td style="text-align: center;">The plane would have crashed.;Everything would have been fine.;The plane would have landed safe and sound.;In a forest you will find lots of planes.</td>
</tr>
<tr>
<td style="text-align: center;">A plant grows in a planter.</td>
<td style="text-align: center;">What would have happened if the planter grows in the plant?</td>
<td style="text-align: center;">That is not possible.;It would have grown quicker.;The plant would have suffered.;The planter would have cultivated the plant.</td>
</tr>
<tr>
<td style="text-align: center;">A mortician prepares a corpse.</td>
<td style="text-align: center;">What would have happened if the mortician had prepared a dinner?</td>
<td style="text-align: center;">He would have had a delicious dish.;Morticians cannot prepare dinners.;The dinner would have been buried.;The mortician would have killed the corpse.</td>
</tr>
<tr>
<td style="text-align: center;">An oil tanker sails across an ocean.</td>
<td style="text-align: center;">What would have happened if the oil tanker had broken up in an ocean?</td>
<td style="text-align: center;">There would have been environmental pollution. That is not possible.;The oil tanker would have continued to carry oil.;The oil tanker would have been saved.;</td>
</tr>
<tr>
<td style="text-align: center;">A car crashes into a tree.</td>
<td style="text-align: center;">What would have happened if the car had parked beneath the tree?</td>
<td style="text-align: center;">Nothing special would have happe ned.;The car would have been hit by the tree.;That is not possible.;I think it would have crashed into the tree.</td>
</tr>
<tr>
<td style="text-align: center;">A child draws a picture.</td>
<td style="text-align: center;">What would have happened if the child had erased the picture?</td>
<td style="text-align: center;">The picture would not have been visible.;The picture would have been visible.;That is not possible.</td>
</tr>
<tr>
<td style="text-align: center;">A craftsman builds a house.</td>
<td style="text-align: center;">What would have happened if the house had built a craftsman?</td>
<td style="text-align: center;">That is not possible.;The house would have been built faster.;Everything would have been fine.;The craftsman would have hands.</td>
</tr>
<tr>
<td style="text-align: center;">A doctor washes their hands at work.</td>
<td style="text-align: center;">What would have happened if the doctor hadn't washed their hands?</td>
<td style="text-align: center;">The patients could get an infection.;The patients could get better.;That is not possible.</td>
</tr>
</tbody>
</table>
<p>Table 8: Example scenarios from the CRASS counterfactual reasoning benchmark. The task is to select the best answer choice for the counterfactual question, given a premise.</p>
<p>Given that an LLM is trained on text narratives written by humans, the subjective elements of causal judgments may be encoded as part of the LLM's internal representations.</p>
<p>In this section, we first review the concept of counterfactual reasoning and show that LLMs can answer counterfactual questions about an event with accuracy close to that of humans on a benchmark task. We then present an example scenario to illustrate the challenges of modeling real-world events using token causality and introduce the two key counterfactual concepts used in formal models of token causality: necessity and sufficiency. Using 15 different vignettes commonly used in token causality literature, we provide evidence that LLM can capture the necessary background context and reason about necessity and sufficiency of token causes. Finally, we show how this capability transfers to a causal judgment benchmark task from Big Bench (Suzgun et al., 2022), specifically designed to test LLMs on their ability to infer the normality of a candidate causes.</p>
<h1>4.1 Building block of token causality: Counterfactual reasoning</h1>
<p>Counterfactual reasoning is the process of considering hypothetical situations or alternate realities by altering specific elements or conditions of an actual event or situation (Kahneman \&amp; Miller, 1986; Byrne, 2005). Such reasoning is a key element of token causality. To determine the causes of an event, it is important to simulate alternative worlds where an event may not have happened and reason about the consequences. For example, a naive approach to token causality is to use a definition based on counterfactual dependence: an event A is a cause of another event B if B would not have happened without A.</p>
<p>Independent of token causality, counterfactual argument making is a desirable skill for a LLM, as it would assist users in decision-making, planning, and offer insights that may not explicitly apparent from the original context. With this motivation, Frohberg \&amp; Binder (2022) provide a benchmark called CRASS (Counterfactual Reasoning Assessment) for evaluating the ability of LLMs to answer counterfactual conditional questions, a subset of which is included in the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3 (Frohberg \&amp; Binder, 2022)</td>
<td style="text-align: center;">58.39</td>
</tr>
<tr>
<td style="text-align: left;">T0pp (Sanh et al., 2021)</td>
<td style="text-align: center;">72.63</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: center;">83.94</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo</td>
<td style="text-align: center;">87.95</td>
</tr>
<tr>
<td style="text-align: left;">gpt-4</td>
<td style="text-align: center;">$\mathbf{9 2 . 4 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Human annotators</td>
<td style="text-align: center;">98.18</td>
</tr>
</tbody>
</table>
<p>Table 9: Accuracy of different LLMs on the CRASS counterfactual reasoning benchmark. gpt-4 achieves $92 \%$ accuracy, significantly higher than the previous reported accuracy on this benchmark and within six percentage points of human annotators' accuracy.</p>
<p>BIGBench collection of datasets (Srivastava et al., 2022). ${ }^{6}$ The benchmark contains 275 instances, where each instance has the LLM select from multiple choice answers to a counterfactual conditional question such as "A woman opens a treasure chest. What would have happened if the woman had not opened the treasure chest?". Table 8 lists some instances from the dataset. Our memorization test indicates the CRASS dataset has not been memorized (see Section E).</p>
<p>The authors report GPT test-davinci-003 as the best performing (zero-shot) LLM on this task, with a top-1 accuracy of $58.39 \%$. Another language model (T0pp) (Sanh et al., 2021), which was finetuned on a multi-task setup of classification tasks, obtains $72 \%$ accuracy. This is contrasted with $98.18 \%$ accuracy human baseline, indicating that counterfactual predictions remain a challenging task for LLMs.</p>
<p>We revisit the claim using LLMs released after the benchmark's publication. As shown in Table 8, each problem instance has a premise, counterfactual question and a list of answer options. We construct a prompt for the LLM by concatenating the premise and the counterfactual question, and then presenting the answer options as A, B, C, D (for an example prompt, see Suppl. B.1). For gpt-3.5-turbo and gpt-4, we additionally provided a system message, "You are a helpful assistant for counterfactual reasoning."</p>
<p>Table 9 shows the accuracy of different LMs. Remarkably, GPT 3.5 version models show substantial improvement in accuracy over GPT 3. gpt-3.5-turbo obtains an accuracy of 87.95 . gpt-4 improves it further to $92.44 \%$, which is 20 percentage points higher than the previous best accuracy. Comparing to the human accuracy on this task, gpt-4 is within six percentage points of average human accuracy. The results indicate that large language models of GPT-3.5 and 4 series represent a substantial jump in LLM ability to generate counterfactual arguments.</p>
<p>Understanding LLM output. We now try to analyze some outputs from gpt-4 to understand its counterfactual argument-making capabilities. For full LLM outputs of the prompt discussed here, see Suppl. B.1.As the high accuracy indicates, gpt-4 is able to simulate different scenarios based on the text prompt and answer what-if scenarios. In many cases, when gpt-4's answer does not match the benchmark, it reveals ambiguity in the text used to describe the scenario. In one of the cases, the LLM is asked what will happen if a man catches a water balloon, given that he has caught a ball. The benchmark answer is that he will get wet whereas the LLM output correctly argues that he may get wet or not, depending on whether the balloon bursts as he catches it. Similarly, another question asks what will happen if a man gets very nervous. The benchmark answer indicates that he will pass out, but the LLM output correctly mentions that, "Passing out due to extreme nervousness is possible, but not guaranteed. and concludes that the man would not pass out. We suspect that given enough clarifying context, the LLM should be able to correctly answer such questions.</p>
<p>That said, there are a few instances where the LLM does not capture context that people may easily determine through common sense. For instance, one of the questions asks what would happen if a man walks on a bed instead of walking on the street. "Walking on a bed" conveys that the man is likely indoors, but the LLM incorrectly imagines a scenario where the man is still walking to the same destination, but now he is walking over a bed, which makes it incorrectly</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ The full dataset was released on Github on August 2021, so it is after the training cutoff for text-davinci-003 model. But it is a month before the cutoff for gpt-3.5-turbo. A smaller subset of inputs was released as a part of BIG-Bench in May 2021 with a canary string for LLMs to avoid including it in their training data.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
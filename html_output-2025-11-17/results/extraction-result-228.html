<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-228 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-228</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-228</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-12.html">extraction-schema-12</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the brain allocates neural resources when learning multiple tasks or skills that involve overlapping or shared neural circuits, including evidence of interference, transfer, resource competition, and representational changes.</div>
                <p><strong>Paper ID:</strong> paper-d97ce247705f816e493e063f73c35a5275d26208</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d97ce247705f816e493e063f73c35a5275d26208" target="_blank">Continual task learning in natural and artificial agents</a></p>
                <p><strong>Paper Venue:</strong> Trends in Neurosciences</p>
                <p><strong>Paper TL;DR:</strong> Combinations of supervised and unsupervised learning mechanisms may help partition task knowledge and avoid catastrophic interference (i.e., overwriting existing knowledge) in natural and artificial agents.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e228.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e228.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the brain allocates neural resources when learning multiple tasks or skills that involve overlapping or shared neural circuits, including evidence of interference, transfer, resource competition, and representational changes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Orthogonal task representations (human fMRI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Orthogonal task-specific representations in posterior parietal and prefrontal cortex</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human fMRI evidence that context-dependent tasks are represented on separate, approximately orthogonal low-dimensional neural planes, with task-irrelevant dimensions compressed and task-relevant dimensions preserved.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Orthogonal representations for robust context-dependent task performance in brains and neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_1_name</strong></td>
                            <td>Garden A: decisions based on leafiness (plant/don't plant)</td>
                        </tr>
                        <tr>
                            <td><strong>task_2_name</strong></td>
                            <td>Garden B: decisions based on branchiness (plant/don't plant)</td>
                        </tr>
                        <tr>
                            <td><strong>additional_tasks</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_similarity</strong></td>
                            <td>overlapping visual stimuli that differ only in which stimulus dimension (leafiness vs branchiness) is task-relevant</td>
                        </tr>
                        <tr>
                            <td><strong>brain_regions</strong></td>
                            <td>posterior parietal cortex (PPC), dorsomedial/dorsolateral prefrontal cortex (dmPFC/DLPFC), early visual cortex (EVC)</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_type</strong></td>
                            <td>partial overlap in same regions but segregated into orthogonal subspaces (task-specific planes)</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_measurement</strong></td>
                            <td>fMRI multivariate pattern analysis (representational dissimilarity matrices, neural geometry visualisations)</td>
                        </tr>
                        <tr>
                            <td><strong>learning_schedule</strong></td>
                            <td>blocked training in human participants (blocked practice of each task/context)</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_interference</strong></td>
                            <td>Humans trained under interleaved schedules performed worse in test than blocked-trained subjects; blocked training reduced interference by supporting independent application of category boundaries (no numeric percent given)</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_transfer</strong></td>
                            <td>Blocked training improved ability to apply separate category boundaries; unsupervised pretraining on latent features (β-VAE or similarity arrangement) facilitated subsequent supervised learning and transfer</td>
                        </tr>
                        <tr>
                            <td><strong>neural_allocation_mechanism</strong></td>
                            <td>representational separation into orthogonal low-dimensional subspaces and compression of task-irrelevant dimensions (subspace partitioning)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_changes</strong></td>
                            <td>compression along irrelevant axes and rotation into orthogonal task-specific neural planes; task-relevant axes preserved, producing reduced overlap in representational geometry</td>
                        </tr>
                        <tr>
                            <td><strong>capacity_limits</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plasticity_mechanisms</strong></td>
                            <td>unspecified neural plasticity underlying reconfiguration (paper links effect to Hebbian/unsupervised mechanisms and supervised fine-tuning in models)</td>
                        </tr>
                        <tr>
                            <td><strong>species</strong></td>
                            <td>human</td>
                        </tr>
                        <tr>
                            <td><strong>methods</strong></td>
                            <td>behavioural training, fMRI, multivariate representational analyses, computational neural network modelling</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Blocked learning produces orthogonal, low-dimensional task representations in parietal and prefrontal cortex that partition neural resources across tasks by compressing irrelevant dimensions and placing each task on an independent neural plane.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Continual task learning in natural and artificial agents', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e228.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e228.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the brain allocates neural resources when learning multiple tasks or skills that involve overlapping or shared neural circuits, including evidence of interference, transfer, resource competition, and representational changes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixed selectivity / high-dimensional coding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixed selectivity in prefrontal cortex enabling high-dimensional task coding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Single neurons multiplex multiple task variables (observations, actions, rules), producing high-dimensional population codes that can represent many task combinations and support flexible readout.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The importance of mixed selectivity in complex cognitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_1_name</strong></td>
                            <td>Generic complex cognitive tasks with multiple stimulus/response variables</td>
                        </tr>
                        <tr>
                            <td><strong>task_2_name</strong></td>
                            <td>Other task conditions requiring different input-output mappings (various)</td>
                        </tr>
                        <tr>
                            <td><strong>additional_tasks</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_similarity</strong></td>
                            <td>multiple variables combined within the same stimuli/responses (overlapping features)</td>
                        </tr>
                        <tr>
                            <td><strong>brain_regions</strong></td>
                            <td>prefrontal cortex (PFC)</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_type</strong></td>
                            <td>substantial overlap at the single-cell level (neurons multiplex variables and task signals)</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_measurement</strong></td>
                            <td>single-unit recordings, population analyses of dimensionality</td>
                        </tr>
                        <tr>
                            <td><strong>learning_schedule</strong></td>
                            <td>not specified (studies measure coding during task performance/training)</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_interference</strong></td>
                            <td>Not directly reported in this paper; mixed selectivity increases representational capacity that can reduce the need for interference-prone dedicated representations but does not by itself eliminate interference</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_transfer</strong></td>
                            <td>High-dimensional mixed codes enable linear readout of many task mappings, supporting positive transfer/rapid learning of new readouts</td>
                        </tr>
                        <tr>
                            <td><strong>neural_allocation_mechanism</strong></td>
                            <td>neural expansion into high-dimensional mixed-selectivity representations (multiplexing of variables)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_changes</strong></td>
                            <td>emergence of conjunctive/mixed-selectivity tuning across neurons leading to high population dimensionality</td>
                        </tr>
                        <tr>
                            <td><strong>capacity_limits</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plasticity_mechanisms</strong></td>
                            <td>synaptic plasticity implied (learning produces mixed selectivity), not pinned to a single rule</td>
                        </tr>
                        <tr>
                            <td><strong>species</strong></td>
                            <td>monkey (primarily), general neurophysiology</td>
                        </tr>
                        <tr>
                            <td><strong>methods</strong></td>
                            <td>single-unit electrophysiology, population dimensionality analyses, modeling</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Neurons with mixed selectivity generate high-dimensional population codes that can represent many task combinations and support flexible context-dependent behaviour, trading off separability against overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Continual task learning in natural and artificial agents', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e228.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e228.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the brain allocates neural resources when learning multiple tasks or skills that involve overlapping or shared neural circuits, including evidence of interference, transfer, resource competition, and representational changes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task-selective PFC neurons (monkey)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prefrontal cortex activity during flexible categorization showing task-selective neurons</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Single-cell recordings in macaque PFC show many neurons active in one categorisation scheme and largely silent in the other, indicating partitioning of task representations across neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prefrontal cortex activity during flexible categorization.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_1_name</strong></td>
                            <td>Categorisation scheme 1 of morphed animal images</td>
                        </tr>
                        <tr>
                            <td><strong>task_2_name</strong></td>
                            <td>Categorisation scheme 2 (independent) of same morphed images</td>
                        </tr>
                        <tr>
                            <td><strong>additional_tasks</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_similarity</strong></td>
                            <td>same visual stimuli but different categorisation rules (overlapping stimuli, different rules)</td>
                        </tr>
                        <tr>
                            <td><strong>brain_regions</strong></td>
                            <td>prefrontal cortex (PFC)</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_type</strong></td>
                            <td>partial overlap in same region; many neurons are scheme-specific, small fraction active in both</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_measurement</strong></td>
                            <td>single-unit recordings (spike activity during two classification schemes)</td>
                        </tr>
                        <tr>
                            <td><strong>learning_schedule</strong></td>
                            <td>tasks learned/experienced by animals (details in original study); contexts alternated during experiments</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_interference</strong></td>
                            <td>Not quantified here; observed neural partitioning suggests a mechanism to reduce interference between the two categorisations</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_transfer</strong></td>
                            <td>Not directly reported in this citation within the review</td>
                        </tr>
                        <tr>
                            <td><strong>neural_allocation_mechanism</strong></td>
                            <td>partitioning of tasks across largely non-overlapping subsets of neurons in PFC</td>
                        </tr>
                        <tr>
                            <td><strong>representational_changes</strong></td>
                            <td>neurons become selective for one task but not the other (task-specific recruitment)</td>
                        </tr>
                        <tr>
                            <td><strong>capacity_limits</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plasticity_mechanisms</strong></td>
                            <td>changes in single-neuron task tuning (implied synaptic/functional plasticity)</td>
                        </tr>
                        <tr>
                            <td><strong>species</strong></td>
                            <td>macaque (monkey)</td>
                        </tr>
                        <tr>
                            <td><strong>methods</strong></td>
                            <td>single-unit electrophysiology during categorisation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Many PFC neurons become active selectively for one classification scheme and not the other, indicating partitioning of task knowledge across neuronal subpopulations to reduce interference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Continual task learning in natural and artificial agents', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e228.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e228.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the brain allocates neural resources when learning multiple tasks or skills that involve overlapping or shared neural circuits, including evidence of interference, transfer, resource competition, and representational changes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task specificity in mouse parietal cortex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task specificity in mouse parietal cortex recorded with two-photon imaging</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two-photon imaging in mouse parietal cortex shows that over half of recorded neurons are active in at least one of two tasks but only a small fraction are active in both, demonstrating task-specific recruitment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Task specificity in mouse parietal cortex.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_1_name</strong></td>
                            <td>Grating discrimination task</td>
                        </tr>
                        <tr>
                            <td><strong>task_2_name</strong></td>
                            <td>T-maze navigation task</td>
                        </tr>
                        <tr>
                            <td><strong>additional_tasks</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_similarity</strong></td>
                            <td>distinct behaviours using overlapping parietal circuitry but differing sensory/motor demands (partially overlapping stimulus/behavioral features)</td>
                        </tr>
                        <tr>
                            <td><strong>brain_regions</strong></td>
                            <td>parietal cortex (mouse)</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_type</strong></td>
                            <td>partial overlap in same region but largely segregated neuron populations across tasks</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_measurement</strong></td>
                            <td>two-photon calcium imaging of population activity</td>
                        </tr>
                        <tr>
                            <td><strong>learning_schedule</strong></td>
                            <td>tasks trained separately; recordings during performance of each task</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_interference</strong></td>
                            <td>Not quantified in review; neural partitioning suggests limited interference</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_transfer</strong></td>
                            <td>Not reported in the cited summary</td>
                        </tr>
                        <tr>
                            <td><strong>neural_allocation_mechanism</strong></td>
                            <td>task-specific recruitment of largely non-overlapping neuronal ensembles</td>
                        </tr>
                        <tr>
                            <td><strong>representational_changes</strong></td>
                            <td>many neurons active in at least one task, few active across both tasks (sparse overlap)</td>
                        </tr>
                        <tr>
                            <td><strong>capacity_limits</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plasticity_mechanisms</strong></td>
                            <td>experience-dependent changes in which neurons are recruited (implied functional plasticity)</td>
                        </tr>
                        <tr>
                            <td><strong>species</strong></td>
                            <td>mouse</td>
                        </tr>
                        <tr>
                            <td><strong>methods</strong></td>
                            <td>two-photon calcium imaging, behavioural training</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Parietal cortex neurons are largely task-specific across two distinct tasks, with substantial partitioning of neural resources and only modest overlap between task-activated ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Continual task learning in natural and artificial agents', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e228.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e228.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the brain allocates neural resources when learning multiple tasks or skills that involve overlapping or shared neural circuits, including evidence of interference, transfer, resource competition, and representational changes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stable dendritic spines (structural protection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stably maintained dendritic spines associated with lifelong memories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two-photon tracking of dendritic spines in mice shows that spines formed during learning of distinct tasks are maintained long-term and tend to occupy different dendritic branches, suggesting structural allocation and protection of memory traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Stably maintained dendritic spines are associated with lifelong memories.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_1_name</strong></td>
                            <td>Running forward (example motor task used in study)</td>
                        </tr>
                        <tr>
                            <td><strong>task_2_name</strong></td>
                            <td>Running backwards (another motor task used in study)</td>
                        </tr>
                        <tr>
                            <td><strong>additional_tasks</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_similarity</strong></td>
                            <td>distinct motor tasks (different behavioural outputs but same animal)</td>
                        </tr>
                        <tr>
                            <td><strong>brain_regions</strong></td>
                            <td>cortex (dendritic spines observed on apical tuft branches of cortical neurons)</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_type</strong></td>
                            <td>differential structural allocation across dendritic branches (reduced overlap at synaptic structural level)</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_measurement</strong></td>
                            <td>two-photon microscopy tracking of dendritic spine formation and maintenance</td>
                        </tr>
                        <tr>
                            <td><strong>learning_schedule</strong></td>
                            <td>sequential training on unrelated tasks (in original study)</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_interference</strong></td>
                            <td>Not directly quantified; structural maintenance suggests preservation of earlier task memories against later learning</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_transfer</strong></td>
                            <td>Not reported here</td>
                        </tr>
                        <tr>
                            <td><strong>neural_allocation_mechanism</strong></td>
                            <td>structural allocation: formation and long-term maintenance of task-associated dendritic spines on separate branches, effectively protecting synapses from later modification</td>
                        </tr>
                        <tr>
                            <td><strong>representational_changes</strong></td>
                            <td>formation of new, stable spines correlated with learned tasks; long-term retention of task-specific synaptic structures</td>
                        </tr>
                        <tr>
                            <td><strong>capacity_limits</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plasticity_mechanisms</strong></td>
                            <td>structural synaptic plasticity (dendritic spine formation and stabilization)</td>
                        </tr>
                        <tr>
                            <td><strong>species</strong></td>
                            <td>mouse</td>
                        </tr>
                        <tr>
                            <td><strong>methods</strong></td>
                            <td>two-photon longitudinal imaging of dendritic spines during and after task learning</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>New spines formed during learning of different tasks are allocated across distinct dendritic branches and remain stable long-term, suggesting a synaptic structural mechanism to preserve old memories and reduce interference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Continual task learning in natural and artificial agents', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e228.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e228.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the brain allocates neural resources when learning multiple tasks or skills that involve overlapping or shared neural circuits, including evidence of interference, transfer, resource competition, and representational changes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complementary Learning Systems / Replay</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complementary Learning Systems (hippocampus fast learning + neocortex slow consolidation) and replay-driven consolidation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Theory and empirical evidence that fast hippocampal encoding plus slow neocortical consolidation (including replay) reduces interference between sequentially learned tasks by internally interleaving experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_1_name</strong></td>
                            <td>Generic previously learned task A (various domains)</td>
                        </tr>
                        <tr>
                            <td><strong>task_2_name</strong></td>
                            <td>New task B learned subsequently</td>
                        </tr>
                        <tr>
                            <td><strong>additional_tasks</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_similarity</strong></td>
                            <td>can be similar or different; CLS addresses sequentially acquired tasks generally</td>
                        </tr>
                        <tr>
                            <td><strong>brain_regions</strong></td>
                            <td>hippocampus (fast encoding, replay), neocortex (slow consolidation and storage)</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_type</strong></td>
                            <td>overlapping representational substrates over time (hippocampus → neocortex transfer), but replay reduces destructive overwriting</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_measurement</strong></td>
                            <td>rodent hippocampal replay electrophysiology, human intracranial recordings, computational simulations</td>
                        </tr>
                        <tr>
                            <td><strong>learning_schedule</strong></td>
                            <td>sequential learning in naturalistic settings; consolidation via replay interleaves tasks internally</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_interference</strong></td>
                            <td>Replay/consolidation reduces catastrophic interference and supports retention of earlier tasks during acquisition of new tasks (demonstrated in models and supported by replay evidence in animals/humans)</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_transfer</strong></td>
                            <td>Consolidated neocortical knowledge can be reused to facilitate future learning (positive transfer) depending on overlap</td>
                        </tr>
                        <tr>
                            <td><strong>neural_allocation_mechanism</strong></td>
                            <td>time-separated storage with hippocampal replay driving gradual neocortical integration, effectively allocating memory traces across systems</td>
                        </tr>
                        <tr>
                            <td><strong>representational_changes</strong></td>
                            <td>gradual consolidation produces stable neocortical representations of prior tasks while hippocampus retains rapid, labile codes</td>
                        </tr>
                        <tr>
                            <td><strong>capacity_limits</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plasticity_mechanisms</strong></td>
                            <td>fast hippocampal synaptic plasticity and slower neocortical plasticity; replay-driven synaptic changes</td>
                        </tr>
                        <tr>
                            <td><strong>species</strong></td>
                            <td>rodent (empirical replay), human (evidence for replay), general theoretical framework</td>
                        </tr>
                        <tr>
                            <td><strong>methods</strong></td>
                            <td>computational modelling (CLS), electrophysiological recordings of hippocampal replay, human intracranial/fMRI studies</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Fast hippocampal encoding plus replay-driven slow consolidation to neocortex allows internal interleaving of experiences, preventing catastrophic forgetting by allocating memories across complementary systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Continual task learning in natural and artificial agents', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e228.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e228.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the brain allocates neural resources when learning multiple tasks or skills that involve overlapping or shared neural circuits, including evidence of interference, transfer, resource competition, and representational changes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Blocked vs interleaved continual learning (behavioural)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparing continual task learning in minds and machines (trees task behavioural and modeling study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human behavioural study showing that blocked training improves later interleaved test performance on two context-dependent categorisation rules, contrasting with artificial networks which suffer catastrophic forgetting under blocked training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Comparing continual task learning in minds and machines.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_1_name</strong></td>
                            <td>Tree categorisation in Garden A (leafiness relevant)</td>
                        </tr>
                        <tr>
                            <td><strong>task_2_name</strong></td>
                            <td>Tree categorisation in Garden B (branchiness relevant)</td>
                        </tr>
                        <tr>
                            <td><strong>additional_tasks</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_similarity</strong></td>
                            <td>same stimuli but different decision rules (overlapping stimuli, different task rules)</td>
                        </tr>
                        <tr>
                            <td><strong>brain_regions</strong></td>
                            <td>behavioural study (no direct brain recordings in this paper); linked to later fMRI work showing parietal/prefrontal representations</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_type</strong></td>
                            <td>overlapping stimulus representations but participants learned to partition rules behaviorally</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_measurement</strong></td>
                            <td>behavioural performance comparisons; computational model simulations</td>
                        </tr>
                        <tr>
                            <td><strong>learning_schedule</strong></td>
                            <td>blocked versus interleaved training schedules compared experimentally</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_interference</strong></td>
                            <td>Interleaved human learners performed worse on subsequent interleaved test than blocked learners; artificial neural networks show catastrophic forgetting under blocked training (large performance drop on earlier task after training on second task)</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_transfer</strong></td>
                            <td>Blocked training led to better independent learning of category boundaries (facilitated application of task-specific rules)</td>
                        </tr>
                        <tr>
                            <td><strong>neural_allocation_mechanism</strong></td>
                            <td>behavioural evidence consistent with participants partitioning task knowledge across contexts (implying subspace separation), models showed that partitioning reduces interference</td>
                        </tr>
                        <tr>
                            <td><strong>representational_changes</strong></td>
                            <td>blocked learning promoted representations that support independent application of category boundaries (inferred from behavioural patterns and later fMRI results)</td>
                        </tr>
                        <tr>
                            <td><strong>capacity_limits</strong></td>
                            <td>demonstrates catastrophic forgetting in standard ANNs under blocked schedules (evidence of capacity/optimization limitations in models)</td>
                        </tr>
                        <tr>
                            <td><strong>plasticity_mechanisms</strong></td>
                            <td>behavioural; modelling work links observed behaviour to Hebbian gating and context signals</td>
                        </tr>
                        <tr>
                            <td><strong>species</strong></td>
                            <td>human (behavioural) and artificial agents (models)</td>
                        </tr>
                        <tr>
                            <td><strong>methods</strong></td>
                            <td>behavioural experiments (blocked vs interleaved training), computational neural network simulations</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Humans learn context-dependent rules better under blocked training by effectively partitioning knowledge, while standard artificial neural networks suffer catastrophic forgetting under the same blocked schedule, highlighting different resource allocation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Continual task learning in natural and artificial agents', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e228.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e228.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the brain allocates neural resources when learning multiple tasks or skills that involve overlapping or shared neural circuits, including evidence of interference, transfer, resource competition, and representational changes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hebbian context gating model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hebbian context gating with exponentially decaying task signals for continual learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational model combining Hebbian (Oja-like) unsupervised updates with sluggish/contextual task signals that produces anticorrelated task input weights and ReLU-driven partitioning of hidden units, thereby reducing interference in sequential task learning and reproducing human blocked-learning benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Modelling continual learning in humans with Hebbian context gating and exponentially decaying task signals.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_1_name</strong></td>
                            <td>Context A (task A) in context-dependent categorisation tasks (e.g., leafiness)</td>
                        </tr>
                        <tr>
                            <td><strong>task_2_name</strong></td>
                            <td>Context B (task B) in context-dependent categorisation tasks (e.g., branchiness)</td>
                        </tr>
                        <tr>
                            <td><strong>additional_tasks</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_similarity</strong></td>
                            <td>overlapping stimuli with different context-specific relevant dimensions</td>
                        </tr>
                        <tr>
                            <td><strong>brain_regions</strong></td>
                            <td>modeling study (maps conceptually to cortical regions implementing context gating and to hippocampus for replay in complementary frameworks)</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_type</strong></td>
                            <td>model produces partitioning across hidden units (reduced representational overlap between contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_measurement</strong></td>
                            <td>computational analyses of network weight vectors and hidden-layer representations (simulated RDMs and geometry)</td>
                        </tr>
                        <tr>
                            <td><strong>learning_schedule</strong></td>
                            <td>sequential/blocked and interleaved schedules simulated; model accounts for advantage of blocked schedules when context signals are temporally autocorrelated</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_interference</strong></td>
                            <td>Model reproduces human pattern: interleaved training reduces partitioning and leads to worse test performance; standard error-driven networks show catastrophic interference under blocked schedules</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_transfer</strong></td>
                            <td>Model permits compositional transfer when Hebbian step learns independent factors; unsupervised structure learning (pretraining) also facilitates transfer</td>
                        </tr>
                        <tr>
                            <td><strong>neural_allocation_mechanism</strong></td>
                            <td>Oja/Hebbian-driven orthogonalisation of task-input weights and context-gated recruitment of separate hidden units (representational separation via Hebbian context gating)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_changes</strong></td>
                            <td>hidden units become anticorrelated with respect to task inputs and selective for one context due to ReLU rectification, producing distinct task representations and reduced overlap</td>
                        </tr>
                        <tr>
                            <td><strong>capacity_limits</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plasticity_mechanisms</strong></td>
                            <td>Hebbian synaptic updates (Oja's rule), combined with supervised gradient updates and exponentially decaying (sluggish) context signals</td>
                        </tr>
                        <tr>
                            <td><strong>species</strong></td>
                            <td>modeling of human behaviour (simulations aligned to human data)</td>
                        </tr>
                        <tr>
                            <td><strong>methods</strong></td>
                            <td>computational neural network modelling, representational analyses, comparison to human behavioural data</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Combining Hebbian unsupervised updates with sluggish context signals produces anticorrelated task inputs and context-gated hidden units, enabling partitioning of representations that prevents interference and reproduces human blocked-learning advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Continual task learning in natural and artificial agents', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e228.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e228.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the brain allocates neural resources when learning multiple tasks or skills that involve overlapping or shared neural circuits, including evidence of interference, transfer, resource competition, and representational changes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oja's rule (Hebbian PCA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oja's rule: simplified neuron model as a principal component analyzer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Hebbian unsupervised learning rule that strengthens connections between covarying units and converges to principal components, thereby grouping commonly coactivated inputs and tending to orthogonalize different input sources over time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Simplified neuron model as a principal component analyzer.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_1_name</strong></td>
                            <td>generic task-context input A</td>
                        </tr>
                        <tr>
                            <td><strong>task_2_name</strong></td>
                            <td>generic task-context input B</td>
                        </tr>
                        <tr>
                            <td><strong>additional_tasks</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_similarity</strong></td>
                            <td>can operate on inputs that are independent or share features (overlapping sensory contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>brain_regions</strong></td>
                            <td>theoretical/mechanistic rule applicable to cortical circuits (no specific region assigned)</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_type</strong></td>
                            <td>mechanism tends to orthogonalize weight vectors associated with different inputs, reducing overlap</td>
                        </tr>
                        <tr>
                            <td><strong>overlap_measurement</strong></td>
                            <td>theoretical analysis and computational simulation (convergence to principal components)</td>
                        </tr>
                        <tr>
                            <td><strong>learning_schedule</strong></td>
                            <td>operates on ongoing, temporally correlated inputs; temporal autocorrelation influences outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_interference</strong></td>
                            <td>By orthogonalising inputs, Oja-like Hebbian learning can reduce interference between contexts when inputs are temporally structured; not an empirical behavioural report here</td>
                        </tr>
                        <tr>
                            <td><strong>behavioral_transfer</strong></td>
                            <td>Can group shared features and thereby support reuse of common components across tasks (potential positive transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>neural_allocation_mechanism</strong></td>
                            <td>unsupervised Hebbian-driven orthogonalisation / PCA-like grouping of inputs leading to partitioning of downstream units</td>
                        </tr>
                        <tr>
                            <td><strong>representational_changes</strong></td>
                            <td>weights converge to principal components of inputs; commonly coactivated inputs are grouped while independent inputs become orthogonal</td>
                        </tr>
                        <tr>
                            <td><strong>capacity_limits</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plasticity_mechanisms</strong></td>
                            <td>unsupervised Hebbian synaptic updates (Oja's rule)</td>
                        </tr>
                        <tr>
                            <td><strong>species</strong></td>
                            <td>theoretical / applicable across species</td>
                        </tr>
                        <tr>
                            <td><strong>methods</strong></td>
                            <td>mathematical analysis, computational simulation; cited as a candidate mechanism for biological partitioning</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Oja's Hebbian rule provides a biologically plausible mechanism to group coactive contextual inputs and orthogonalize independent task signals, supporting partitioning of neural resources across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Continual task learning in natural and artificial agents', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The importance of mixed selectivity in complex cognitive tasks. <em>(Rating: 2)</em></li>
                <li>Prefrontal cortex activity during flexible categorization. <em>(Rating: 2)</em></li>
                <li>Task specificity in mouse parietal cortex. <em>(Rating: 2)</em></li>
                <li>Stably maintained dendritic spines are associated with lifelong memories. <em>(Rating: 2)</em></li>
                <li>Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. <em>(Rating: 2)</em></li>
                <li>Comparing continual task learning in minds and machines. <em>(Rating: 2)</em></li>
                <li>Modelling continual learning in humans with Hebbian context gating and exponentially decaying task signals. <em>(Rating: 2)</em></li>
                <li>Alleviating catastrophic forgetting using contextdependent gating and synaptic stabilization. <em>(Rating: 2)</em></li>
                <li>Rotational dynamics reduce interference between sensory and memory representations. <em>(Rating: 2)</em></li>
                <li>Learning orthogonalizes visual cortical population codes. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-228",
    "paper_id": "paper-d97ce247705f816e493e063f73c35a5275d26208",
    "extraction_schema_id": "extraction-schema-12",
    "extracted_data": [
        {
            "name_short": "Orthogonal task representations (human fMRI)",
            "name_full": "Orthogonal task-specific representations in posterior parietal and prefrontal cortex",
            "brief_description": "Human fMRI evidence that context-dependent tasks are represented on separate, approximately orthogonal low-dimensional neural planes, with task-irrelevant dimensions compressed and task-relevant dimensions preserved.",
            "citation_title": "Orthogonal representations for robust context-dependent task performance in brains and neural networks.",
            "mention_or_use": "use",
            "task_1_name": "Garden A: decisions based on leafiness (plant/don't plant)",
            "task_2_name": "Garden B: decisions based on branchiness (plant/don't plant)",
            "additional_tasks": null,
            "task_similarity": "overlapping visual stimuli that differ only in which stimulus dimension (leafiness vs branchiness) is task-relevant",
            "brain_regions": "posterior parietal cortex (PPC), dorsomedial/dorsolateral prefrontal cortex (dmPFC/DLPFC), early visual cortex (EVC)",
            "overlap_type": "partial overlap in same regions but segregated into orthogonal subspaces (task-specific planes)",
            "overlap_measurement": "fMRI multivariate pattern analysis (representational dissimilarity matrices, neural geometry visualisations)",
            "learning_schedule": "blocked training in human participants (blocked practice of each task/context)",
            "behavioral_interference": "Humans trained under interleaved schedules performed worse in test than blocked-trained subjects; blocked training reduced interference by supporting independent application of category boundaries (no numeric percent given)",
            "behavioral_transfer": "Blocked training improved ability to apply separate category boundaries; unsupervised pretraining on latent features (β-VAE or similarity arrangement) facilitated subsequent supervised learning and transfer",
            "neural_allocation_mechanism": "representational separation into orthogonal low-dimensional subspaces and compression of task-irrelevant dimensions (subspace partitioning)",
            "representational_changes": "compression along irrelevant axes and rotation into orthogonal task-specific neural planes; task-relevant axes preserved, producing reduced overlap in representational geometry",
            "capacity_limits": null,
            "plasticity_mechanisms": "unspecified neural plasticity underlying reconfiguration (paper links effect to Hebbian/unsupervised mechanisms and supervised fine-tuning in models)",
            "species": "human",
            "methods": "behavioural training, fMRI, multivariate representational analyses, computational neural network modelling",
            "key_finding": "Blocked learning produces orthogonal, low-dimensional task representations in parietal and prefrontal cortex that partition neural resources across tasks by compressing irrelevant dimensions and placing each task on an independent neural plane.",
            "uuid": "e228.0",
            "source_info": {
                "paper_title": "Continual task learning in natural and artificial agents",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Mixed selectivity / high-dimensional coding",
            "name_full": "Mixed selectivity in prefrontal cortex enabling high-dimensional task coding",
            "brief_description": "Single neurons multiplex multiple task variables (observations, actions, rules), producing high-dimensional population codes that can represent many task combinations and support flexible readout.",
            "citation_title": "The importance of mixed selectivity in complex cognitive tasks.",
            "mention_or_use": "mention",
            "task_1_name": "Generic complex cognitive tasks with multiple stimulus/response variables",
            "task_2_name": "Other task conditions requiring different input-output mappings (various)",
            "additional_tasks": null,
            "task_similarity": "multiple variables combined within the same stimuli/responses (overlapping features)",
            "brain_regions": "prefrontal cortex (PFC)",
            "overlap_type": "substantial overlap at the single-cell level (neurons multiplex variables and task signals)",
            "overlap_measurement": "single-unit recordings, population analyses of dimensionality",
            "learning_schedule": "not specified (studies measure coding during task performance/training)",
            "behavioral_interference": "Not directly reported in this paper; mixed selectivity increases representational capacity that can reduce the need for interference-prone dedicated representations but does not by itself eliminate interference",
            "behavioral_transfer": "High-dimensional mixed codes enable linear readout of many task mappings, supporting positive transfer/rapid learning of new readouts",
            "neural_allocation_mechanism": "neural expansion into high-dimensional mixed-selectivity representations (multiplexing of variables)",
            "representational_changes": "emergence of conjunctive/mixed-selectivity tuning across neurons leading to high population dimensionality",
            "capacity_limits": null,
            "plasticity_mechanisms": "synaptic plasticity implied (learning produces mixed selectivity), not pinned to a single rule",
            "species": "monkey (primarily), general neurophysiology",
            "methods": "single-unit electrophysiology, population dimensionality analyses, modeling",
            "key_finding": "Neurons with mixed selectivity generate high-dimensional population codes that can represent many task combinations and support flexible context-dependent behaviour, trading off separability against overlap.",
            "uuid": "e228.1",
            "source_info": {
                "paper_title": "Continual task learning in natural and artificial agents",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Task-selective PFC neurons (monkey)",
            "name_full": "Prefrontal cortex activity during flexible categorization showing task-selective neurons",
            "brief_description": "Single-cell recordings in macaque PFC show many neurons active in one categorisation scheme and largely silent in the other, indicating partitioning of task representations across neurons.",
            "citation_title": "Prefrontal cortex activity during flexible categorization.",
            "mention_or_use": "mention",
            "task_1_name": "Categorisation scheme 1 of morphed animal images",
            "task_2_name": "Categorisation scheme 2 (independent) of same morphed images",
            "additional_tasks": null,
            "task_similarity": "same visual stimuli but different categorisation rules (overlapping stimuli, different rules)",
            "brain_regions": "prefrontal cortex (PFC)",
            "overlap_type": "partial overlap in same region; many neurons are scheme-specific, small fraction active in both",
            "overlap_measurement": "single-unit recordings (spike activity during two classification schemes)",
            "learning_schedule": "tasks learned/experienced by animals (details in original study); contexts alternated during experiments",
            "behavioral_interference": "Not quantified here; observed neural partitioning suggests a mechanism to reduce interference between the two categorisations",
            "behavioral_transfer": "Not directly reported in this citation within the review",
            "neural_allocation_mechanism": "partitioning of tasks across largely non-overlapping subsets of neurons in PFC",
            "representational_changes": "neurons become selective for one task but not the other (task-specific recruitment)",
            "capacity_limits": null,
            "plasticity_mechanisms": "changes in single-neuron task tuning (implied synaptic/functional plasticity)",
            "species": "macaque (monkey)",
            "methods": "single-unit electrophysiology during categorisation tasks",
            "key_finding": "Many PFC neurons become active selectively for one classification scheme and not the other, indicating partitioning of task knowledge across neuronal subpopulations to reduce interference.",
            "uuid": "e228.2",
            "source_info": {
                "paper_title": "Continual task learning in natural and artificial agents",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Task specificity in mouse parietal cortex",
            "name_full": "Task specificity in mouse parietal cortex recorded with two-photon imaging",
            "brief_description": "Two-photon imaging in mouse parietal cortex shows that over half of recorded neurons are active in at least one of two tasks but only a small fraction are active in both, demonstrating task-specific recruitment.",
            "citation_title": "Task specificity in mouse parietal cortex.",
            "mention_or_use": "mention",
            "task_1_name": "Grating discrimination task",
            "task_2_name": "T-maze navigation task",
            "additional_tasks": null,
            "task_similarity": "distinct behaviours using overlapping parietal circuitry but differing sensory/motor demands (partially overlapping stimulus/behavioral features)",
            "brain_regions": "parietal cortex (mouse)",
            "overlap_type": "partial overlap in same region but largely segregated neuron populations across tasks",
            "overlap_measurement": "two-photon calcium imaging of population activity",
            "learning_schedule": "tasks trained separately; recordings during performance of each task",
            "behavioral_interference": "Not quantified in review; neural partitioning suggests limited interference",
            "behavioral_transfer": "Not reported in the cited summary",
            "neural_allocation_mechanism": "task-specific recruitment of largely non-overlapping neuronal ensembles",
            "representational_changes": "many neurons active in at least one task, few active across both tasks (sparse overlap)",
            "capacity_limits": null,
            "plasticity_mechanisms": "experience-dependent changes in which neurons are recruited (implied functional plasticity)",
            "species": "mouse",
            "methods": "two-photon calcium imaging, behavioural training",
            "key_finding": "Parietal cortex neurons are largely task-specific across two distinct tasks, with substantial partitioning of neural resources and only modest overlap between task-activated ensembles.",
            "uuid": "e228.3",
            "source_info": {
                "paper_title": "Continual task learning in natural and artificial agents",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Stable dendritic spines (structural protection)",
            "name_full": "Stably maintained dendritic spines associated with lifelong memories",
            "brief_description": "Two-photon tracking of dendritic spines in mice shows that spines formed during learning of distinct tasks are maintained long-term and tend to occupy different dendritic branches, suggesting structural allocation and protection of memory traces.",
            "citation_title": "Stably maintained dendritic spines are associated with lifelong memories.",
            "mention_or_use": "mention",
            "task_1_name": "Running forward (example motor task used in study)",
            "task_2_name": "Running backwards (another motor task used in study)",
            "additional_tasks": null,
            "task_similarity": "distinct motor tasks (different behavioural outputs but same animal)",
            "brain_regions": "cortex (dendritic spines observed on apical tuft branches of cortical neurons)",
            "overlap_type": "differential structural allocation across dendritic branches (reduced overlap at synaptic structural level)",
            "overlap_measurement": "two-photon microscopy tracking of dendritic spine formation and maintenance",
            "learning_schedule": "sequential training on unrelated tasks (in original study)",
            "behavioral_interference": "Not directly quantified; structural maintenance suggests preservation of earlier task memories against later learning",
            "behavioral_transfer": "Not reported here",
            "neural_allocation_mechanism": "structural allocation: formation and long-term maintenance of task-associated dendritic spines on separate branches, effectively protecting synapses from later modification",
            "representational_changes": "formation of new, stable spines correlated with learned tasks; long-term retention of task-specific synaptic structures",
            "capacity_limits": null,
            "plasticity_mechanisms": "structural synaptic plasticity (dendritic spine formation and stabilization)",
            "species": "mouse",
            "methods": "two-photon longitudinal imaging of dendritic spines during and after task learning",
            "key_finding": "New spines formed during learning of different tasks are allocated across distinct dendritic branches and remain stable long-term, suggesting a synaptic structural mechanism to preserve old memories and reduce interference.",
            "uuid": "e228.4",
            "source_info": {
                "paper_title": "Continual task learning in natural and artificial agents",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Complementary Learning Systems / Replay",
            "name_full": "Complementary Learning Systems (hippocampus fast learning + neocortex slow consolidation) and replay-driven consolidation",
            "brief_description": "Theory and empirical evidence that fast hippocampal encoding plus slow neocortical consolidation (including replay) reduces interference between sequentially learned tasks by internally interleaving experiences.",
            "citation_title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.",
            "mention_or_use": "mention",
            "task_1_name": "Generic previously learned task A (various domains)",
            "task_2_name": "New task B learned subsequently",
            "additional_tasks": null,
            "task_similarity": "can be similar or different; CLS addresses sequentially acquired tasks generally",
            "brain_regions": "hippocampus (fast encoding, replay), neocortex (slow consolidation and storage)",
            "overlap_type": "overlapping representational substrates over time (hippocampus → neocortex transfer), but replay reduces destructive overwriting",
            "overlap_measurement": "rodent hippocampal replay electrophysiology, human intracranial recordings, computational simulations",
            "learning_schedule": "sequential learning in naturalistic settings; consolidation via replay interleaves tasks internally",
            "behavioral_interference": "Replay/consolidation reduces catastrophic interference and supports retention of earlier tasks during acquisition of new tasks (demonstrated in models and supported by replay evidence in animals/humans)",
            "behavioral_transfer": "Consolidated neocortical knowledge can be reused to facilitate future learning (positive transfer) depending on overlap",
            "neural_allocation_mechanism": "time-separated storage with hippocampal replay driving gradual neocortical integration, effectively allocating memory traces across systems",
            "representational_changes": "gradual consolidation produces stable neocortical representations of prior tasks while hippocampus retains rapid, labile codes",
            "capacity_limits": null,
            "plasticity_mechanisms": "fast hippocampal synaptic plasticity and slower neocortical plasticity; replay-driven synaptic changes",
            "species": "rodent (empirical replay), human (evidence for replay), general theoretical framework",
            "methods": "computational modelling (CLS), electrophysiological recordings of hippocampal replay, human intracranial/fMRI studies",
            "key_finding": "Fast hippocampal encoding plus replay-driven slow consolidation to neocortex allows internal interleaving of experiences, preventing catastrophic forgetting by allocating memories across complementary systems.",
            "uuid": "e228.5",
            "source_info": {
                "paper_title": "Continual task learning in natural and artificial agents",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Blocked vs interleaved continual learning (behavioural)",
            "name_full": "Comparing continual task learning in minds and machines (trees task behavioural and modeling study)",
            "brief_description": "Human behavioural study showing that blocked training improves later interleaved test performance on two context-dependent categorisation rules, contrasting with artificial networks which suffer catastrophic forgetting under blocked training.",
            "citation_title": "Comparing continual task learning in minds and machines.",
            "mention_or_use": "use",
            "task_1_name": "Tree categorisation in Garden A (leafiness relevant)",
            "task_2_name": "Tree categorisation in Garden B (branchiness relevant)",
            "additional_tasks": null,
            "task_similarity": "same stimuli but different decision rules (overlapping stimuli, different task rules)",
            "brain_regions": "behavioural study (no direct brain recordings in this paper); linked to later fMRI work showing parietal/prefrontal representations",
            "overlap_type": "overlapping stimulus representations but participants learned to partition rules behaviorally",
            "overlap_measurement": "behavioural performance comparisons; computational model simulations",
            "learning_schedule": "blocked versus interleaved training schedules compared experimentally",
            "behavioral_interference": "Interleaved human learners performed worse on subsequent interleaved test than blocked learners; artificial neural networks show catastrophic forgetting under blocked training (large performance drop on earlier task after training on second task)",
            "behavioral_transfer": "Blocked training led to better independent learning of category boundaries (facilitated application of task-specific rules)",
            "neural_allocation_mechanism": "behavioural evidence consistent with participants partitioning task knowledge across contexts (implying subspace separation), models showed that partitioning reduces interference",
            "representational_changes": "blocked learning promoted representations that support independent application of category boundaries (inferred from behavioural patterns and later fMRI results)",
            "capacity_limits": "demonstrates catastrophic forgetting in standard ANNs under blocked schedules (evidence of capacity/optimization limitations in models)",
            "plasticity_mechanisms": "behavioural; modelling work links observed behaviour to Hebbian gating and context signals",
            "species": "human (behavioural) and artificial agents (models)",
            "methods": "behavioural experiments (blocked vs interleaved training), computational neural network simulations",
            "key_finding": "Humans learn context-dependent rules better under blocked training by effectively partitioning knowledge, while standard artificial neural networks suffer catastrophic forgetting under the same blocked schedule, highlighting different resource allocation strategies.",
            "uuid": "e228.6",
            "source_info": {
                "paper_title": "Continual task learning in natural and artificial agents",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Hebbian context gating model",
            "name_full": "Hebbian context gating with exponentially decaying task signals for continual learning",
            "brief_description": "A computational model combining Hebbian (Oja-like) unsupervised updates with sluggish/contextual task signals that produces anticorrelated task input weights and ReLU-driven partitioning of hidden units, thereby reducing interference in sequential task learning and reproducing human blocked-learning benefits.",
            "citation_title": "Modelling continual learning in humans with Hebbian context gating and exponentially decaying task signals.",
            "mention_or_use": "use",
            "task_1_name": "Context A (task A) in context-dependent categorisation tasks (e.g., leafiness)",
            "task_2_name": "Context B (task B) in context-dependent categorisation tasks (e.g., branchiness)",
            "additional_tasks": null,
            "task_similarity": "overlapping stimuli with different context-specific relevant dimensions",
            "brain_regions": "modeling study (maps conceptually to cortical regions implementing context gating and to hippocampus for replay in complementary frameworks)",
            "overlap_type": "model produces partitioning across hidden units (reduced representational overlap between contexts)",
            "overlap_measurement": "computational analyses of network weight vectors and hidden-layer representations (simulated RDMs and geometry)",
            "learning_schedule": "sequential/blocked and interleaved schedules simulated; model accounts for advantage of blocked schedules when context signals are temporally autocorrelated",
            "behavioral_interference": "Model reproduces human pattern: interleaved training reduces partitioning and leads to worse test performance; standard error-driven networks show catastrophic interference under blocked schedules",
            "behavioral_transfer": "Model permits compositional transfer when Hebbian step learns independent factors; unsupervised structure learning (pretraining) also facilitates transfer",
            "neural_allocation_mechanism": "Oja/Hebbian-driven orthogonalisation of task-input weights and context-gated recruitment of separate hidden units (representational separation via Hebbian context gating)",
            "representational_changes": "hidden units become anticorrelated with respect to task inputs and selective for one context due to ReLU rectification, producing distinct task representations and reduced overlap",
            "capacity_limits": null,
            "plasticity_mechanisms": "Hebbian synaptic updates (Oja's rule), combined with supervised gradient updates and exponentially decaying (sluggish) context signals",
            "species": "modeling of human behaviour (simulations aligned to human data)",
            "methods": "computational neural network modelling, representational analyses, comparison to human behavioural data",
            "key_finding": "Combining Hebbian unsupervised updates with sluggish context signals produces anticorrelated task inputs and context-gated hidden units, enabling partitioning of representations that prevents interference and reproduces human blocked-learning advantages.",
            "uuid": "e228.7",
            "source_info": {
                "paper_title": "Continual task learning in natural and artificial agents",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Oja's rule (Hebbian PCA)",
            "name_full": "Oja's rule: simplified neuron model as a principal component analyzer",
            "brief_description": "A Hebbian unsupervised learning rule that strengthens connections between covarying units and converges to principal components, thereby grouping commonly coactivated inputs and tending to orthogonalize different input sources over time.",
            "citation_title": "Simplified neuron model as a principal component analyzer.",
            "mention_or_use": "mention",
            "task_1_name": "generic task-context input A",
            "task_2_name": "generic task-context input B",
            "additional_tasks": null,
            "task_similarity": "can operate on inputs that are independent or share features (overlapping sensory contexts)",
            "brain_regions": "theoretical/mechanistic rule applicable to cortical circuits (no specific region assigned)",
            "overlap_type": "mechanism tends to orthogonalize weight vectors associated with different inputs, reducing overlap",
            "overlap_measurement": "theoretical analysis and computational simulation (convergence to principal components)",
            "learning_schedule": "operates on ongoing, temporally correlated inputs; temporal autocorrelation influences outcomes",
            "behavioral_interference": "By orthogonalising inputs, Oja-like Hebbian learning can reduce interference between contexts when inputs are temporally structured; not an empirical behavioural report here",
            "behavioral_transfer": "Can group shared features and thereby support reuse of common components across tasks (potential positive transfer)",
            "neural_allocation_mechanism": "unsupervised Hebbian-driven orthogonalisation / PCA-like grouping of inputs leading to partitioning of downstream units",
            "representational_changes": "weights converge to principal components of inputs; commonly coactivated inputs are grouped while independent inputs become orthogonal",
            "capacity_limits": null,
            "plasticity_mechanisms": "unsupervised Hebbian synaptic updates (Oja's rule)",
            "species": "theoretical / applicable across species",
            "methods": "mathematical analysis, computational simulation; cited as a candidate mechanism for biological partitioning",
            "key_finding": "Oja's Hebbian rule provides a biologically plausible mechanism to group coactive contextual inputs and orthogonalize independent task signals, supporting partitioning of neural resources across tasks.",
            "uuid": "e228.8",
            "source_info": {
                "paper_title": "Continual task learning in natural and artificial agents",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The importance of mixed selectivity in complex cognitive tasks.",
            "rating": 2,
            "sanitized_title": "the_importance_of_mixed_selectivity_in_complex_cognitive_tasks"
        },
        {
            "paper_title": "Prefrontal cortex activity during flexible categorization.",
            "rating": 2,
            "sanitized_title": "prefrontal_cortex_activity_during_flexible_categorization"
        },
        {
            "paper_title": "Task specificity in mouse parietal cortex.",
            "rating": 2,
            "sanitized_title": "task_specificity_in_mouse_parietal_cortex"
        },
        {
            "paper_title": "Stably maintained dendritic spines are associated with lifelong memories.",
            "rating": 2,
            "sanitized_title": "stably_maintained_dendritic_spines_are_associated_with_lifelong_memories"
        },
        {
            "paper_title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.",
            "rating": 2,
            "sanitized_title": "why_there_are_complementary_learning_systems_in_the_hippocampus_and_neocortex_insights_from_the_successes_and_failures_of_connectionist_models_of_learning_and_memory"
        },
        {
            "paper_title": "Comparing continual task learning in minds and machines.",
            "rating": 2,
            "sanitized_title": "comparing_continual_task_learning_in_minds_and_machines"
        },
        {
            "paper_title": "Modelling continual learning in humans with Hebbian context gating and exponentially decaying task signals.",
            "rating": 2,
            "sanitized_title": "modelling_continual_learning_in_humans_with_hebbian_context_gating_and_exponentially_decaying_task_signals"
        },
        {
            "paper_title": "Alleviating catastrophic forgetting using contextdependent gating and synaptic stabilization.",
            "rating": 2,
            "sanitized_title": "alleviating_catastrophic_forgetting_using_contextdependent_gating_and_synaptic_stabilization"
        },
        {
            "paper_title": "Rotational dynamics reduce interference between sensory and memory representations.",
            "rating": 2,
            "sanitized_title": "rotational_dynamics_reduce_interference_between_sensory_and_memory_representations"
        },
        {
            "paper_title": "Learning orthogonalizes visual cortical population codes.",
            "rating": 1,
            "sanitized_title": "learning_orthogonalizes_visual_cortical_population_codes"
        }
    ],
    "cost": 0.01945175,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Continual task learning in natural and artificial agents</h1>
<p>Timo Flesch ${ }^{1}$, Andrew Saxe ${ }^{2}$ and Christopher Summerfield ${ }^{1}$<br>${ }^{1}$ Department of Experimental Psychology, University of Oxford, Oxford, UK.<br>${ }^{2}$ Gatsby Computational Neuroscience Unit \&amp; Sainsbury Wellcome Centre, UCL, London, UK. Correspondence: {timo.flesch, christopher.summerfield}@psy.ox.ac.uk</p>
<h4>Abstract</h4>
<p>How do humans and other animals learn new tasks? A wave of brain recording studies has investigated how neural representations change during task learning, with a focus on how tasks can be acquired and coded in ways that minimise mutual interference. We review recent work that has explored the geometry and dimensionality of neural task representations in neocortex, and computational models that have exploited these findings to understand how the brain may partition knowledge between tasks. We discuss how ideas from machine learning, including those that combine supervised and unsupervised learning, are helping neuroscientists understand how natural tasks are learned and coded in biological brains.</p>
<p>Keywords
Continual learning, neural networks, representational geometry, Hebbian gating.</p>
<h2>Highlights</h2>
<ul>
<li>Both natural and artificial agents face the challenge of learning in ways that support effective future behaviour</li>
<li>This may be achieved by different learning regimes, associated with distinct dynamics, and differing dimensionality and geometry of neural task representations</li>
<li>Where two different tasks are learned, neural codes for task-relevant information may be factorised in neocortex</li>
<li>Combinations of supervised and unsupervised learning mechanisms may help partition task knowledge and avoid catastrophic interference</li>
</ul>
<h2>Declaration</h2>
<p>Authors declare no competing interests</p>
<h1>1. Natural tasks</h1>
<p>In the natural world, humans and other animals behave in temporally structured ways that depend on environmental context. For example, many mammals cycle systematically through daily activities such as foraging, grooming, napping, and socialising. Humans live in complex societies in which labour is shared among group members, with each adult performing multiple successive roles, such as securing resources, caring for young, or exchanging social information. In many settings, we can describe the behaviour of natural agents as comprising a succession of distinct tasks for which a desired outcome (reward) is achieved by taking actions (responses) to observations (stimuli) through the learning of latent causal processes (rules).</p>
<p>The nature of task-driven behaviour, and the way that tasks are represented and implemented in neural circuits, have been widely studied by cognitive and neural scientists. One important finding is that switching between distinct tasks incurs a cost in decision accuracy and latency [1]. This switch cost implies the existence of control mechanisms that ensure we remain "on task", possibly protecting ongoing behavioural routines from interference [2,3]. In primates, there is good evidence that control signals originate in the prefrontal cortex (PFC) and encourage task-appropriate behaviours by biasing neural activity in sensory and motor regions [4]. For example, single cells in the PFC have been observed to respond to task rules [5], and patients with PFC damage tend to select tasks erroneously, leading to disinhibited or inappropriate behaviours [6].</p>
<p>How, then, are tasks coded in the PFC and interconnected regions? One key insight is that mutual interference among tasks can be mitigated when they are coded in independent subspaces of neural activity, such that the neural population vector evoked during task A is uncorrelated with that occurring during task B [7,8]. Over the past decade, evidence for this coding principle has emerged in domains as varied as skilled motor control [9], auditory prediction [10], memory processes [11,12], and visual categorisation [13,14]. However, the precise computational mechanisms by which tasks are encoded and implemented remain a matter of ongoing debate.</p>
<h2>2. Rich and lazy learning</h2>
<p>One way to study how tasks could be neurally encoded is to simulate learning in a simple class of computational model - a neural network trained with gradient descent. Neural networks uniquely allow researchers to form hypotheses about how neural codes form in biological brains, because their representations emerge through optimisation rather than being handcrafted by the researcher [15]. One recent observation is that neural networks can learn to perform tasks in different regimes that are characterised by qualitatively diverging learning dynamics and distinct neural patterns at convergence [16,17]. In the lazy regime, which occurs when network weights are initialised with a broader range of values (e.g., higher connection strengths), the dimensionality of the input signals is rapidly expanded via random projections to the hidden layer such that learning is mostly confined to the readout weights, and error decreases exponentially [17-20]. By contrast, in the rich regime, which occurs when weights are initialised with low variance (weak connectivity), the hidden units learn highly structured representations that are tailored to the specific demands of the task, and the loss curve tends to pass through one or more saddle points before convergence [21-24]. We illustrate using a simple example - learning an "exclusive or" (XOR) problem - in Fig. 1A-D.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Rich and Lazy learning in neural networks. (A) The XOR (exclusive or) problem requires to provide the same response A when either one or the other of two input units are set to one, and the response B when both are 0 or both are 1. A linear classifier can't learn to distinguish between the two classes. (B) Feedforward neural network architecture that can solve the XOR task. The inputs are mapped into a hidden layer with non-linear outputs, and from there into a non-linear response layer. (C) Effect of different initial weight variances on the training dynamics of the network shown in (B). We distinguish between rich learning (with small initial weight variance, light blue) and lazy learning (with large initial weight variance, dark blue). The change of the magnitude of the input-to hidden (left) and hidden-to output (middle) depends strongly on initialisation strength. In lazy initialised networks, the input-to hidden weights remain very close to their initial values and learning is confined to the readout weights. In rich initialised networks, all weights adapt substantially. Moreover, rich-initialised networks learn much slower than lazy-initialised networks (right). (D) Learned input-to-hidden weights after rich (top) and lazy (bottom) learning. Under rich learning, the weights learn to point to the four input types. In contrast, under lazy learning, the weights point in arbitrary directions, effectively performing a random mapping into a higher-dimensional space.</p>
<p>These schemes may have complementary costs and benefits. High-dimensional coding schemes maximise the number of discriminations that can be linearly read out from the network, allowing agents to rapidly learn a new decision rule for a task [25]. Low-dimensional coding schemes confer robustness through redundancy, because neurons exhibit overlapping tuning properties, and promote generalisation, because they tend to correspond to simpler input-output functions when the neural manifold extends in fewer directions [26,27].</p>
<p>Neural recordings have offered evidence for both rich and lazy coding schemes. One important observation is that the variables that define a task - observations, actions, outcomes and rules - are often encoded jointly by single neurons. For example, when monkeys make choices on the basis of distinct cues, single cells tend to multiplex input and choice variables [28,29]. In another study, the dimensionality of neural codes recorded during performance of a dual memory task was found to approach its theoretical maximum, implying that neurons represent every possible combination of relevant variables [12]. This finding is consistent with "lazy" learning, implying that brains encode tasks via high-dimensional representations that enmesh multiple task-relevant variables across the neural population.</p>
<p>However, there is also important evidence that neural systems learn representations that mirror the structure of the task, as might be predicted in the "rich" regime. For example, it is often observed that neurons active in one task are silent during another, and vice versa. For example, when macaques were trained to categorise morphed animal images according to two independent classification schemes, $29 \%$ of PFC neurons became active during a single scheme, whereas only $2 \%$ of neurons were active during both [30]. Much more recently, a similar finding was reported using modern two-photon imaging methods in the parietal cortex of mice trained to perform both a grating discrimination task and a T-maze task. Over half of the recorded neurons were active in at least one task, but a much smaller fraction was active in both tasks [31]. In other words, the brain learns to partition task knowledge across independent sets of neurons.</p>
<p>One recent study used a neural network model to explicitly compare the predictions of the rich and lazy learning schemes to neural signals recorded from the human brain [13]. They developed a task (similar to ref [30]) that involved discriminating naturalistic images in two independent contexts. Human participants learn to make "plant / don't plant" decisions about quasi-naturalistic images of trees with continuously varying branch density and leaf density, whose growth success was determined by leaf density in one "garden" (task A) and branch density in the other (task B) (Fig. 2A). Neural networks could be trained to perform a stylised version of this task under either rich or lazy learning schemes by varying the different initial connection strengths (Fig. 2B). Multivariate methods used to visualise the representational dissimilarity matrix (RDM) and corresponding neural geometry for the network hidden layer under either scheme revealed that they made quite different predictions (Fig. 2C). Under lazy learning, the network learned a high dimensional solution whose RDM simply recapitulated the organisation of the input signals (into a grid defined by "leafiness" and "branchiness"). This is expected because randomly expanding the dimensionality of the inputs does not distort their similarity structure. However, under the rich scheme, the network compressed information that was irrelevant dimension to each context, so that the hidden layer represented the relevant input dimensions (leafiness and branchiness) on two neural planes lying at right angles in neural state space (Fig. 2C-D). Strikingly, BOLD signals in the posterior parietal cortex (PPC) and dorsomedial prefrontal cortex (dmPFC) exhibited a similar dimensionality and RDMs revealed a comparable geometric arrangement onto "orthogonal planes", providing evidence in support of "rich" task representations in the human brain (Fig. 2E-F).</p>
<p>Neural systems can thus learn both structured, low-dimensional task representations, and unstructured, high-dimensional codes. In artificial neural networks, the emerging regime depends on the magnitude of initial connection strengths in the network [15]. In the brain, these regimes may arise through other mechanisms such as pressure toward metabolic efficiency (regularisation), or architectural constraints that enforce unlearned nonlinear expansions. Whilst it remains unclear when, how and why either coding scheme might be adopted in the biological brain, neural theory is emerging that may help clarify this issue. One recent paper explored how representational structure is shaped by specific task demands [32]. Comparing recurrent artificial neural networks trained on different cognitive tasks, the authors found that those tasks that required flexible input-output mappings, such as the context-dependent decision task outlined above, induced task-specific representations, similar to the ones observed under rich learning in feedforward networks. In contrast, for tasks that did not require such a flexible mapping, the authors observed completely random, unstructured representations. This suggests that representational geometry is not only determined by intrinsic factors such as initial connection strength, but flexibly adapts to the computational demands of specific tasks. Rich task-specific representations might therefore arise when there</p>
<p>is a need to minimise interference between different tasks and perform flexible context-specific responses to the same stimuli.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Structured task representations for context-dependent decision making. (A) Context-dependent decision making task with human participants. Stimuli were fractal images of trees that varied in their density of leaves (leafiness) and branches (branchiness). In each context/task only one of the two dimensions was relevant, indicated by a reward/penalty participants would receive for "accepting" the tree on a given trial. (B) Simplified version of task described in(A) with images of Gaussian "blobs" instead of trees. The mean of these blobs was varied in five steps along the x - and y -axis. In each context, only one of the two dimensions was relevant. A neural network (right) was trained to predict the context-specific feature value. (C) Hidden layer representations under lazy (left) and rich (right) learning. Under lazy learning, the network recapitulates the structure of the stimulus space. Under rich learning, it compresses along the task-irrelevant axes, forming "orthogonal" task-specific representations of relevant features. (D) Estimated compression rates as a function of the weight variance (rich vs lazy learning). Rich networks compress irrelevant dimensions more. (E) Visualisation of variance in human fMRI recordings from early visual cortex (EVC) and dorsolateral prefrontal cortex (DLPFC) explained by a model with free parameters for the compression rate, distance between tasks and rotation of individual task representations. Clearly evident are task-agnostic grid-like representations in EVC and "orthogonal" task-specific representational in DLPFC. (F) Estimated compression rates in different brain regions for human fMRI data, after training on the task described in (A).</p>
<h1>3. The problem of continual learning</h1>
<p>The natural world is structured so that different tasks tend to occur in succession. For example, many animals (such as bears and llamas) are able to both run and swim but they cannot do so at the same time. Similarly, most humans can perform many different tasks (such as playing the violin and the trumpet) but may not do so simultaneously. This aspect of the world presents a well-known challenge for task learning, because when task B is learned after task A, there is</p>
<p>a risk that knowledge of task A is erased - a phenomenon known as "catastrophic forgetting" [33] (Fig. 3A). Catastrophic forgetting can occur when a neural network is optimised through adjustment of a finite set of connections, because there are no guarantees that any weight configuration that solves a novel task B will also simultaneously solve an existing task A (Fig. 3B). Building neural networks that avoid catastrophic forgetting and adapt continually to novel tasks in an open-ended environment is a grand challenge in AI research, where even powerful existing systems are often poor at adapting flexibly to new tasks that are introduced late in training [34-36]. Humans, by contrast, seem to have evolved ways to circumvent this problem, allowing people to solve new problems deep into older age. How might the neural representation of tasks allow biological agents to learn continually?</p>
<p>One possibility is that synapses that encode existing knowledge can be protected during new learning. Evidence for this idea comes from two-photon microscopy, which can be used to track dendritic spine formation in the rodent brain. When mice are trained to perform two unrelated tasks, such as running forward and backwards, new spines form across different apical tuft branches. Remarkably, many of these new spines are maintained stably across the entire lifespan of the animal, despite the many further experiences to which the animal is exposed, as if they were being protected from further change [37]. In machine learning, methods that earmark synapses that are critical for performing current tasks, and explicitly protect them during learning of new tasks, have helped neural networks learn several types of image recognition task or multiple Atari video games in succession [38,39].
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Continual learning in minds and machines. (A) Performance under blocked ("continual") and interleaved training in humans and deep neural networks. ANNs suffer from catastrophic forgetting under blocked training (top) but reach ceiling performance under interleaved training (bottom). The opposite is true for humans, wo perform worse under interleaved training. (B) Solution spaces in a neural network. Solutions for different tasks require different parameter configurations. Training on a second task moves the configuration out of the solution space for the first task. (C) Gating theory for continual learning. If the context signal was able to inhibit taskirrelevant units (which are relevant for another task), those should not be affected by gradient updates. (D) Hidden layer representations without (left) and with (right) Hebbian gating signals. The standard neural network treats the first task like the second. The gated network learns two separate task representations. (E) Sluggish context signals with exponentially weighted moving averages. The higher the "sluggishness", the more are context signals biased by the recent trial history. (F) Comparison of category boundary estimation error in humans (left) and a neural network either trained with standard error-corrective learning (mddle) or with Hebbian gating and</p>
<p>Another promising approach to continual learning capitalises on the fact that in mammals, new information tends to be rapidly encoded in hippocampal circuits and only slowly consolidated to neocortex [40]. Consolidation allows existing learning (task A) to be internally "replayed" during acquisition of task B, removing the temporal structure from the learning problem by mentally interleaving tasks A and B. This allows the network to sample the two objectives simultaneously - and thus to gravitate towards a parameter setting that jointly solves both tasks [41]. This idea, known as "complementary learning systems" (or CLS) theory, influenced AI researchers designing the first deep neural networks to solve complex, dynamic control problems (such Atari games) [42]. By introducing a virtual "buffer" in which game states were stored and intermittently replayed, akin to the fast rehearsal of successive mental states during short wave ripples observed in both rodents [43] and humans [44], the network was able to overcome the challenge presented by a nonstationary training distribution, and to learn to perform dozens of games better than an expert human player [45]. More recently, new methods have been developed which prioritise replay of those states with highest prediction error [46], just as biological replay seems to privilege rewarding events [47], or that replay samples from a generative model of the environment, allowing agents to incorporate plausible but counterfactual events and outcomes into a joint task schema [48].</p>
<h1>4. The benefit of temporal structure</h1>
<p>Complementary learning systems theory offers a clear story about how the brain learns tasks in a temporally structured world - by mentally mixing them up. Indeed, there is evidence from cognitive psychology that injecting variety into the training set helps people learn sports [49], foreign languages [50] and even to acquire abstract mathematical concepts [51] or recognise the painting styles of famous artists [52]. However, as every student knows, simply selecting training examples at random rarely makes a good curriculum. When researchers want a novice animal to learn a complex task, they carefully design the series of training levels - for example by first teaching a mouse to lick a waterspout, and only later to discriminate a tone for reward. Pigeons struggle to learn the concept of "same" vs. "different" in visual discrimination if trained on random examples, but learn effectively if the training set starts small and grows gradually [53]; for related work in monkeys see [54]. Similarly, when training human children, teachers typically organise information into discrete blocks, so that for example students might study French before Spanish, but not both languages in the same lesson. Experimental animals may even spontaneously structure their own curricula, for example learning independently to lick a waterspout and discriminate a tone at different times [55]. In other words, although there are good theoretical reasons why mixing exemplars during training should help, in practice animals seem to learn more readily under temporally autocorrelated curricula.</p>
<p>Building on this intuition, one study used the trees task to ask whether curricula that block or interleave tasks (or "gardens", each associated with an independent visual discrimination rule) over trials facilitate learning [56]. Different groups of participants learned to perform the task by purely trial and error under either a blocked curriculum (involving hundreds of trials of consecutive practice for each task) or an interleaved curriculum (in which the task switched from trial to trial), before being tested without feedback on the interleaved task. Surprisingly, test performance was better for groups that learned under the blocked curriculum, despite the fact that other groups could in theory benefit from the shared structure of training and test. For</p>
<p>example, the test involved many task switch trials, which the interleaved group had practiced extensively, but the blocked group had only experienced once. Detailed behavioural analysis showed that rather than increasing psychophysical sensitivity, or decreasing generalised errors (lapses), the blocked curriculum helped participants learn to apply the two category boundaries independently - in other words, it facilitated effective partitioning of the two tasks [56] (Fig. 3F).</p>
<p>Why does temporal structure assist learning in humans and other animals - but not in neural networks, who suffer catastrophic forgetting during blocked training? One possibility is that biological brains have evolved ways to partition knowledge during initial task acquisition, so that learning can occur in independent subspaces of neural population activity, thereby precluding interference between tasks A and B. Indeed, the orthogonal neural geometry of the BOLD signal observed for the trees task [13] occurred after blocked training - as if human participants, despite the temporal structure, had learned to represent the task in separate neural subspaces. As a proof of concept, methods that project learning updates (gradients) for new tasks into independent neural subspaces have been shown to help with continual learning in machine learning models, including recurrent networks [57,58]. However, implementing these methods often requires expensive computations, such as computing and maintaining an unwieldy projection matrix in memory.</p>
<p>To understand how the brain might partition new task learning across neurons, it is helpful to return to the simple neural network model of task learning. The widely-studied case where inputs such as trees [13,56], animals [30], or colour moving dots [29,59,60] are classified according to two independent category boundaries has XOR structure and thus can only be solved by networks equipped with nonlinear transformations, for example with rectified linear units (ReLU). When the task varies from trial to trial, the network is provided with "task inputs" denoting whether the current task is A or B (e.g., motion or colour discrimination). At initialisation, the weights connecting each of the task inputs to the hidden layer units are entirely random, but over the course of training they become anticorrelated, so that a hidden unit that responds positively to task A will tend to respond negatively to task B and vice versa (Fig. 3C). The additional rectification step - in which the ReLU sets the negative portion of an input to zero - means that hidden layer units become responsive to either one task input unit or the other, but not both, consistent with the finding that tasks tend to be partitioned across neurons, for example in mouse parietal cortex [31] or monkey PFC [30]. The partitioning allows tasks to be learned independently, and indeed if task input units weights are manually forced to be anticorrelated, the network has no problem learning both tasks A and B even when they are presented in successive blocks of training [61,62] (Fig. 3D). This is consistent with earlier proposals of context-based gating as a potential solution to continual learning and control [63-65].</p>
<h1>5. Knowledge partitioning via Hebbian learning</h1>
<p>The question remains, however, of how projection into partitioned hidden units might be achieved during online learning. A broad hint comes from considering the relationship between sensory input and the demands of natural tasks. In the real world, different tasks tend to occur in different contexts. For example, a bear might learn to walk on land and swim in water, and not vice versa. An adult in a professional role might work in the office, and drink the local bar, but not vice versa. Sensory context, thus - whether the backdrop is computers and desks, or beer and jukebox - offers strong clues about which tasks we should be performing [66]. Thus, a mechanism that learned to correlate task inputs that shared sensory features, and to</p>
<p>orthogonalize those that did not, would allow knowledge to be partitioned by task. Fortunately, one popular learning algorithm neatly meets these requirements. An implementation of Hebbian learning called Oja's rule strengthens connections between neurons with covarying activity [67]. It thus converges to the first principal component of mean-centred input signals and will inevitably group together those hidden units that are connected to commonly activated inputs. For example, in a setting where tasks A and B occur independently, Oja's rule will tend to orthogonalize the weights from the two task input units to the hidden layer. This effect will be enhanced where tasks are accompanied by shared sensory features (such as land and water).</p>
<p>To capture the empirically observed advantage of blocked over interleaved learning, however, one further assumption is required - namely, that the window of temporal integration for the inputs is longer than a single trial. In other words, we assume that the input to the network on any given trial contains a mixture of the current and past information, so that decisions on any given trial may be biased by the recent trial history. Indeed sequential effects across trials are a ubiquitous feature of behavioural data gathered in the lab [68-70]. For context-dependent decisions, choice history will have a different effect where tasks are blocked and interleaved, because it effectively smooths the input signals so that independence between tasks (and thus partitioning) is promoted when tasks are blocked but decreased when they are interleaved. Intuitively, where task A and B are interleaved over trials, each input contains a mixture of signals from both tasks, reducing their independence (Fig. 3E). This may also disrupt performance, because the previous task may bias responses on the current trial. Bringing these ideas together, one recent modelling study showed that the introduction of history effects and Hebbian updating together allow networks to learn in ways that avoid catastrophic interference, and also to capture rich pattern of behaviour observed when human participants perform the trees task under blocked and interleaved conditions, including the advantage of blocking over interleaving, and the observation that this benefit stems from a more accurate estimate of the category boundary [61] (Fig. 3F).</p>
<h1>6. Learning tasks with and without supervision</h1>
<p>Machine learning models are often more likely to converge to optimal solutions when training data are sampled to be independent and identically distributed (i.i.d.) - in other words, with curricula that are as random as possible. Where the data distribution is stationary, i.i.d. sampling reduces bias during learning, and has powered machine learning models towards superhuman performance in domains such as object recognition [71]. However, the data distribution in the natural world is not stationary: instead, it is highly autocorrelated within a single context, but shifts abruptly at context boundaries, such as when a penguin emerges from the sea onto the ice or when you leave your warm house and head out into the wintry street. Some machine learning methods treat this structure as a nuisance, and have found clever ways to try and remove it [42]. However, the theory above suggests that the brain has instead evolved to capitalise on this structure. It proposes that by using unsupervised learning methods - such as Hebbian learning - the brain learns to group ongoing experience into contexts, and to partition neural resources to learn about each context independently. By orthogonalizing task signals, behavioural routines can be stored in ways that minimise interference.</p>
<p>This idea taps into a longstanding theme in machine learning research - namely, that unsupervised and error-driven learning can work together to help agents solve natural tasks. In fact, early successes in deep learning employed unsupervised pre-training methods to structure neural representations before supervised fine-tuning [72]. When deep convolutional networks are trained to solve the trees task from pixels alone, pre-training on the tree dataset using a</p>
<p>beta-variational autoencoder ( $\beta$-VAE) accelerates subsequent supervised learning. The $\beta$-VAE uses self-supervised methods to learn the latent factors in the data (i.e., leafiness and branchiness), thus structuring representations according to the two subsequent decisionrelevant dimensions. In a similar vein, when human participants were first asked to arrange samples of trees by their similarity (without knowing the decision-relevant axes) those whose prior tendency was to organise by leafiness and branchiness received more benefit from blocked training [56]. In other words, learning the structure of the world can help both humans and neural networks organise information along task-relevant axes, and may by at the heart of biological solutions to continual learning.</p>
<p>However, there is an important caveat to this theory. When we encounter a novel task, we rarely want to ignore everything we know about other tasks - in fact, past task knowledge can help as well as hinder current task learning. For example, a chef learning a new recipe will benefit from past cooking experience, or a programmer learning Python will probably benefit from past proficiency in MATLAB [73]. Thus, we often want to share representations between tasks, but strict partitioning of task knowledge into orthogonal subspaces reduces the positive as well as the negative effects of this transfer. In fact, knowing how to learn new tasks in a way that negotiates the trade-off between interference (negative transfer) and generalisation (positive transfer) is a key unsolved challenge in both neuroscience and AI research[34,74].</p>
<p>Answers to this question are only beginning to emerge, but one possibility is that the brain is particularly adept at factorising existing task knowledge into reusable subcomponents, which can then be recomposed to tackle novel challenges [75]. For example, when making predictions about sequences of dots presented on a ring, participants seem to combine primitives involving rotation and symmetry [76]. Neural signals seem to code independently for task factors, such as the position and identity of an object in a sequence [77,78]. By coding reusable task factors in independent subspaces of neural activity, they can be recombined in novel ways - for example, if a chef has learned to knead dough when baking bread, and make a tomato passata when cooking spaghetti, these skills can be combined when making pizza for the first time.</p>
<p>One recent study showed that Hebbian learning may also contribute to compositional solutions to difficult transfer problems [79]. Human participants were trained to map coloured shapes onto spatial responses made a mouse click, with each feature (e.g., colour) mapping onto a spatial dimension (e.g., the horizontal axis in Cartesian coordinates, or radial axis in polar coordinates). Critically, they were trained with feedback on a single exemplar from each dimension (e.g., all red shapes and all coloured squares) and then asked to make inferences about the location associated with novel objects (e.g., blue triangles). As in the trees task, performance was improved when training of each dimension was blocked (e.g., all red items preceded all squares). Neural networks learned to perform perfectly on training trials but failed to transfer, unless they were equipped with a Hebbian learning step that helped them learn independently about colour and shape. With the combination of Hebbian and supervised learning, networks learned to perform the task in ways that closely resembled humans [79].</p>
<h1>7. Concluding remarks</h1>
<p>A renewed interest in connectionist models as theories of brain function [15], and the advent of high-throughput recording and multivariate analysis methods [27] have collectively reinvigorated research into task learning and its neural substrates. However, exactly how (and to what extent) neural representations form as biological agents learn new tasks remains a mystery. Some theories propose that task-related neurons are supremely adaptive, especially in</p>
<p>PFC, implying that new tasks automatically beget new geometries of representation [80]. Indeed, neural codes measured in BOLD signals have been shown to adjust rapidly when participants are taught new relations among objects or positions, and this occurs both in the medial temporal lobe and frontoparietal network [81-84]. There is even one report that orientation selectivity in V1 can adjust as people learn to classify gratings over just a few hours of practice, as if the basic building blocks of vision were themselves quite labile [85]. However, neural signals recorded from experimental animals seem to change much more gradually with learning, and it is unclear if the latter is a quirk of humans - or perhaps of BOLD signals. Understanding exactly how representations change in both hippocampus and neocortex during new task learning is currently a major outstanding challenge for $21^{\text {st }}$ century neuroscience. See Outstanding Questions box for further unresolved issues.</p>
<h1>Outstanding questions</h1>
<ul>
<li>Past learning can interfere with current task performance, but at other times it can be beneficial. How does the brain code for tasks in a way that trades off the costs and benefits of negative and positive transfer?</li>
<li>Given that neural circuits exhibit experience-dependent plasticity during learning, how can old learning be preserved?</li>
<li>What are the respective roles of different brain regions, including the hippocampus and neocortex, in facilitating continual learning?</li>
</ul>
<h1>REFERENCES</h1>
<ol>
<li>Monsell S. Task switching. Trends Cogn Sci. 2003/03/18 ed. 2003;7: 134-140. doi:10.1016/s1364-6613(03)00028-7</li>
<li>Botvinick MM, Braver TS, Barch DM, Carter CS, Cohen JD. Conflict monitoring and cognitive control. Psychol Rev. 2001/08/08 ed. 2001;108: 624-52.</li>
<li>Badre D. On task: how our brain gets things done. Princeton: Princeton University Press; 2020.</li>
<li>Miller EK, Cohen JD. An integrative theory of prefrontal cortex function. Annu Rev Neurosci. 2001/04/03 ed. 2001;24: 167-202. doi:10.1146/annurev.neuro.24.1.167</li>
<li>Freedman DJ, Assad JA. Neuronal Mechanisms of Visual Categorization: An Abstract View on Decision Making. Annu Rev Neurosci. 2016;39: 129-47. doi:10.1146/annurev-neuro-071714-033919</li>
<li>Shallice T, Burgess PW. Deficits in strategy application following frontal lobe damage in man. Brain. 1991;114 ( Pt 2): 727-41.</li>
<li>Lewandowsky S, Li S-C. Catastrophic interference in neural networks. Interference and Inhibition in Cognition. Elsevier; 1995. pp. 329-361. doi:10.1016/B978-012208930-5/50011-8</li>
<li>Willshaw DJ, Buneman OP, Longuet-Higgins HC. Non-Holographic Associative Memory. Nature. 1969;222: 960-962. doi:10.1038/222960a0</li>
<li>Kaufman MT, Churchland MM, Ryu SI, Shenoy KV. Cortical activity in the null space: permitting preparation without movement. Nat Neurosci. 2014;17: 440-448. doi:10.1038/nn. 3643</li>
<li>Libby A, Buschman TJ. Rotational dynamics reduce interference between sensory and memory representations. Nat Neurosci. 2021;24: 715-726. doi:10.1038/s41593-021-00821-9</li>
<li>Xie Y, Hu P, Li J, Chen J, Song W, Wang X-J, et al. Geometry of sequence working memory in macaque prefrontal cortex. Science. 2022;375: 632-639. doi:10.1126/science.abm0204</li>
<li>Rigotti M, Barak O, Warden MR, Wang XJ, Daw ND, Miller EK, et al. The importance of mixed selectivity in complex cognitive tasks. Nature. 2013;497: 585-90. doi:10.1038/nature12160</li>
<li>
<p>Flesch T, Juechems K, Dumbalska T, Saxe A, Summerfield C. Orthogonal representations for robust context-dependent task performance in brains and neural networks. Neuron. 2022; S0896627322000058. doi:10.1016/j.neuron.2022.01.005</p>
</li>
<li>
<p>Failor SW, Carandini M, Harris KD. Learning orthogonalizes visual cortical population codes. Neuroscience; 2021 May. doi:10.1101/2021.05.23.445338</p>
</li>
<li>Saxe A, Nelli S, Summerfield C. If deep learning is the answer, what is the question? Nat Rev Neurosci. 2021;22: 55-67. doi:10.1038/s41583-020-00395-8</li>
<li>Woodworth B, Gunasekar S, Lee JD, Moroshko E, Savarese P, Golan I, et al. Kernel and Rich Regimes in Overparametrized Models. arXiv:200209277 [cs, stat]. 2020 [cited 17 Jan 2021]. Available: http://arxiv.org/abs/2002.09277</li>
<li>Chizat L, Oyallon E, Bach F. On Lazy Training in Differentiable Programming. NeurIPS. 2018. Available: http://arxiv.org/abs/1812.07956</li>
<li>Jacot A, Gabriel F, Hongler C. Neural tangent kernel: Convergence and generalization in neural networks. 2018. pp. 8571-8580.</li>
<li>Arora S, Ge R, Neyshabur B, Zhang Y. Stronger generalization bounds for deep nets via a compression approach. 2018. Available: http://arxiv.org/abs/1802.05296</li>
<li>Lee J, Xiao L, Schoenholz SS, Bahri Y, Novak R, Sohl-Dickstein J, et al. Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent. arXiv. 2019. Available: http://arxiv.org/abs/1902.06720</li>
<li>Saxe AM, McClelland JL, Ganguli S. A mathematical theory of semantic development in deep neural networks. Proc Natl Acad Sci U S A. 2019;116: 11537-11546. doi:10.1073/pnas. 1820226116</li>
<li>Geiger M, Jacot A, Spigler S, Gabriel F, Sagun L, d'Ascoli S, et al. Scaling description of generalization with number of parameters in deep learning. J Stat Mech. 2020;2020: 023401. doi:10.1088/1742-5468/ab633c</li>
<li>Paccolat J, Petrini L, Geiger M, Tyloo K, Wyart M. Geometric compression of invariant manifolds in neural nets. arXiv:200711471 [cs, stat]. 2021 [cited 22 Mar 2021]. Available: http://arxiv.org/abs/2007.11471</li>
<li>Saxe A, Sodhani S, Lewallen SJ. Neural Race Reduction: Dynamics of Abstraction in Gated Networks. Proceedings of the 39th International Conference on Machine Learning,. 2022. pp. 19287-19309.</li>
<li>Fusi S, Miller EK, Rigotti M. Why neurons mix: high dimensionality for higher cognition. Curr Opin Neurobiol. 2016;37: 66-74. doi:10.1016/j.conb.2016.01.010</li>
<li>Gao P, Trautmann E, Yu B, Santhanam G, Ryu S, Shenoy K, et al. A theory of multineuronal dimensionality, dynamics and measurement. BiorXiv. 2019. Available: https://doi.org/10.1101/214262</li>
<li>
<p>Gao P, Ganguli S. On simplicity and complexity in the brave new world of large-scale neuroscience. Curr Opin Neurobiol. 2015/05/02 ed. 2015;32: 148-55. doi:10.1016/j.conb.2015.04.003</p>
</li>
<li>
<p>Raposo D, Kaufman MT, Churchland AK. A category-free neural population supports evolving demands during decision-making. Nature neuroscience. 2014;17: 1784-92. doi:10.1038/nn. 3865</p>
</li>
<li>Mante V, Sussillo D, Shenoy KV, Newsome WT. Context-dependent computation by recurrent dynamics in prefrontal cortex. Nature. 2013;503: 78-84. doi:10.1038/nature12742</li>
<li>Roy JE, Riesenhuber M, Poggio T, Miller EK. Prefrontal cortex activity during flexible categorization. J Neurosci. 2010;30: 8519-28. doi:10.1523/JNEUROSCI.4837-09.2010</li>
<li>Lee JJ, Krumin M, Harris KD, Carandini M. Task specificity in mouse parietal cortex. Neuron. 2022; S0896627322006626. doi:10.1016/j.neuron.2022.07.017</li>
<li>Dubreuil A, Valente A, Beiran M, Mastrogiuseppe F, Ostojic S. The role of population structure in computations through neural dynamics. Nat Neurosci. 2022;25: 783-794. doi:10.1038/s41593-022-01088-4</li>
<li>French RM. Catastrophic forgetting in connectionist networks. Trends Cogn Sci. 1999;3: 128-135. doi:10.1016/s1364-6613(99)01294-2</li>
<li>Parisi G, Kemker R, Part JL, Kanan C, Wermter S. Continual Lifelong Learning with Neural Networks: A Review. Neural Networks. 2019. doi:10.1016/j.neunet.2019.01.012</li>
<li>Hadsell R, Rao D, Rusu AA, Pascanu R. Embracing Change: Continual Learning in Deep Neural Networks. Trends in Cognitive Sciences. 2020;24: 1028-1040. doi:10.1016/j.tics.2020.09.004</li>
<li>Dohare S, Sutton RS, Mahmood AR. Continual Backprop: Stochastic Gradient Descent with Persistent Randomness. 2021 [cited 1 Oct 2022]. doi:10.48550/ARXIV.2108.06325</li>
<li>Yang G, Pan F, Gan W-B. Stably maintained dendritic spines are associated with lifelong memories. Nature. 2009;462: 920-924. doi:10.1038/nature08577</li>
<li>Kirkpatrick J, Pascanu R, Rabinowitz N, Veness J, Desjardins G, Rusu AA, et al. Overcoming catastrophic forgetting in neural networks. Proc Natl Acad Sci U S A. 2017;114: 35213526. doi:10.1073/pnas. 1611835114</li>
<li>Zenke F, Poole B, Ganguli S. Continual Learning Through Synaptic Intelligence. arXiv:170304200. 2017.</li>
<li>Alvarez P, Squire LR. Memory consolidation and the medial temporal lobe: a simple network model. Proc Natl Acad Sci USA. 1994;91: 7041-7045. doi:10.1073/pnas.91.15.7041</li>
<li>
<p>McClelland JL, McNaughton BL, O'Reilly RC. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychol Rev. 1995;102: 419-57.</p>
</li>
<li>
<p>Kumaran D, Hassabis D, McClelland JL. What Learning Systems do Intelligent Agents Need? Complementary Learning Systems Theory Updated. Trends Cogn Sci. 2016;20: 512-534. doi:10.1016/j.tics.2016.05.004</p>
</li>
<li>Foster DJ. Replay Comes of Age. Annu Rev Neurosci. 2017;40: 581-602. doi:10.1146/annurev-neuro-072116-031538</li>
<li>Vaz AP, Wittig JH, Inati SK, Zaghloul KA. Replay of cortical spiking sequences during human memory retrieval. Science. 2020;367: 1131-1134. doi:10.1126/science.aba0672</li>
<li>Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, et al. Human-level control through deep reinforcement learning. Nature. 2015;518: 529-33. doi:10.1038/nature14236</li>
<li>Schaul T, Quan J, Antonoglou I, Silver D. Prioritized Experience Replay. arXiv:151105952 [cs]. 2016 [cited 24 Dec 2021]. Available: http://arxiv.org/abs/1511.05952</li>
<li>Ambrose RE, Pfeiffer BE, Foster DJ. Reverse Replay of Hippocampal Place Cells Is Uniquely Modulated by Changing Reward. Neuron. 2016;91: 1124-1136. doi:10.1016/j.neuron.2016.07.047</li>
<li>van de Ven GM, Tolias AS. Generative replay with feedback connections as a general strategy for continual learning. arXiv:180910635 [cs, stat]. 2019 [cited 25 Sep 2020]. Available: http://arxiv.org/abs/1809.10635</li>
<li>Goode S, Magill RA. Contextual Interference Effects in Learning Three Badminton Serves. Research Quarterly for Exercise and Sport. 1986;57: 308-314. doi:10.1080/02701367.1986.10608091</li>
<li>Richland LE, Bjork, R.A. Differentiating the Contextual Interference Effect from the Spacing Effect. 2004.</li>
<li>Rohrer D, Dedrick RF, Stershic S. Interleaved practice improves mathematics learning. Journal of Educational Psychology. 2015;107: 900-908. doi:10.1037/edu0000001</li>
<li>Kornell N, Bjork RA. Learning Concepts and Categories: Is Spacing the "Enemy of Induction"? Psychol Sci. 2008;19: 585-592. doi:10.1111/j.1467-9280.2008.02127.x</li>
<li>Katz JS, Wright AA. Same/different abstract-concept learning by pigeons. Journal of Experimental Psychology: Animal Behavior Processes. 2006;32: 80-86. doi:10.1037/0097-7403.32.1.80</li>
<li>Antzoulatos EG, Miller EK. Differences between Neural Activity in Prefrontal Cortex and Striatum during Learning of Novel Abstract Categories. Neuron. 2011;71: 243-9.</li>
<li>
<p>Kuchibhotla KV, Hindmarsh Sten T, Papadoyannis ES, Elnozahy S, Fogelson KA, Kumar R, et al. Dissociating task acquisition from expression during learning reveals latent knowledge. Nat Commun. 2019;10: 2151. doi:10.1038/s41467-019-10089-0</p>
</li>
<li>
<p>Flesch T, Balaguer J, Dekker R, Nili H, Summerfield C. Comparing continual task learning in minds and machines. Proc Natl Acad Sci U S A. 2018;115: E10313-E10322. doi:10.1073/pnas. 1800755115</p>
</li>
<li>Zeng G, Chen Y, Cui B, Yu S. Continual learning of context-dependent processing in neural networks. Nature Machine Intelligence. 2019;1: 364-372.</li>
<li>Duncker L, Driscoll L, Shenoy KV, Sahani M, Sussillo D. Organizing recurrent network dynamics by task-computation to enable continual learning. 2020.</li>
<li>Takagi Y, Hunt L, Woolrich MW, Behrens EJ, Klein-Flugge MC. Projections of non-invasive human recordings into state space show unfolding of spontaneous and over-trained choice. BiorXiv. 2020. Available: https://doi.org/10.1101/2020.02.24.962290</li>
<li>Brincat SL, Siegel M, von Nicolai C, Miller EK. Gradual progression from sensory to taskrelated processing in cerebral cortex. Proc Natl Acad Sci USA. 2018;115. doi:10.1073/pnas. 1717075115</li>
<li>Flesch T, Nagy DG, Saxe A, Summerfield C. Modelling continual learning in humans with Hebbian context gating and exponentially decaying task signals. 2022 [cited 15 Sep 2022]. doi:10.48550/ARXIV.2203.11560</li>
<li>Russin J, Zolfaghar M, Park SA, Boorman E, O'Reilly RC. A Neural Network Model of Continual Learning with Cognitive Control. arXiv:220204773 [cs, q-bio]. 2022 [cited 7 Mar 2022]. Available: http://arxiv.org/abs/2202.04773</li>
<li>Masse NY, Grant GD, Freedman DJ. Alleviating catastrophic forgetting using contextdependent gating and synaptic stabilization. Proc Natl Acad Sci U S A. 2018;115: E10467E10475. doi:10.1073/pnas. 1803839115</li>
<li>Tsuda B, Tye KM, Siegelmann HT, Sejnowski TJ. A modeling framework for adaptive lifelong learning with transfer and savings through gating in the prefrontal cortex. Proc Natl Acad Sci USA. 2020;117: 29872-29882. doi:10.1073/pnas. 2009591117</li>
<li>Cohen JD, Dunbar K, McClelland JL. On the control of automatic processes: A parallel distributed processing account of the Stroop effect. Psychological Review. 1990;97:332361. doi:10.1037/0033-295X.97.3.332</li>
<li>Bar M. Visual objects in context. Nat Rev Neurosci. 2004/07/21 ed. 2004;5: 617-29. doi:10.1038/nrn1476</li>
<li>Oja E. Simplified neuron model as a principal component analyzer. J Math Biology. 1982;15: 267-273. doi:10.1007/BF00275687</li>
<li>
<p>Yu A, Cohen J. Sequential effects: superstition or rational behavior? In: Koller D, Schuurmans D, Bengio Y, Bottou L, editors. Advances in Neural Information Processing Systems,. Vancouver; 2009. pp. 1873-1880.</p>
</li>
<li>
<p>Cho RY, Nystrom LE, Brown ET, Jones AD, Braver TS, Holmes PJ, et al. Mechanisms underlying dependencies of performance on stimulus history in a two-alternative forced-choice task. Cognitive, Affective, \&amp; Behavioral Neuroscience. 2002;2: 283-299. doi:10.3758/CABN.2.4.283</p>
</li>
<li>Akaishi R, Umeda K, Nagase A, Sakai K. Autonomous Mechanism of Internal Choice Estimate Underlies Decision Inertia. Neuron. 2014;81: 195-206. doi:10.1016/j.neuron.2013.10.018</li>
<li>Krizhevsky A, Sutskever I, Hinton GE. ImageNet Classification with Deep ConvolutionalNeural Networks. Lake Tahoe, Nevada; 2012.</li>
<li>Bengio Y, Lamblin P, Popovici D, Larochelle H. Greedy layer-wise training of deep networks. 2006.</li>
<li>Sheahan H, Luyckx F, Nelli S, Teupe C, Summerfield C. Neural state space alignment for magnitude generalization in humans and recurrent networks. Neuron. 2021;109: 12141226.e8. doi:10.1016/j.neuron.2021.02.004</li>
<li>Musslick S, Saxe A, Hoskin AN, Reichman D, Cohen JD. On the Rational Boundedness of Cognitive Control: Shared Versus Separated Representations. PsyArXiv. 2020 [cited 17 Mar 2022]. Available: https://psyarxiv.com/jkhdf/</li>
<li>Behrens TEJ, Muller TH, Whittington JCR, Mark S, Baram AB, Stachenfeld KL, et al. What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior. Neuron. 2018;100: 490-509. doi:10.1016/j.neuron.2018.10.002</li>
<li>Amalric M, Wang L, Pica P, Figueira S, Sigman M, Dehaene S. The language of geometry: Fast comprehension of geometrical primitives and rules in human adults and preschoolers. Gallistel R, editor. PLoS Comput Biol. 2017;13: e1005273. doi:10.1371/journal.pcbi. 1005273</li>
<li>Liu Y, Dolan RJ, Kurth-Nelson Z, Behrens TEJ. Human Replay Spontaneously Reorganizes Experience. Cell. 2019;178: 640-652 e14. doi:10.1016/j.cell.2019.06.012</li>
<li>Al Roumi F, Marti S, Wang L, Amalric M, Dehaene S. Mental compression of spatial sequences in human working memory using numerical and geometrical primitives. Neuron. 2021;109: 2627-2639.e4. doi:10.1016/j.neuron.2021.06.009</li>
<li>Dekker R, Otto F, Summerfield C. Determinants of human compositional generalization. PsyArXiv; 2022 Mar. doi:10.31234/osf.io/qnpw6</li>
<li>Duncan J. An adaptive coding model of neural function in prefrontal cortex. Nature reviews Neuroscience. 2001;2: 820-9. doi:10.1038/35097575</li>
<li>
<p>Nelli S, Braun L, Dumbalska T, Saxe A, Summerfield C. Neural knowledge assembly in humans and deep networks. Neuroscience; 2021 Oct. doi:10.1101/2021.10.21.465374</p>
</li>
<li>
<p>Milivojevic B, Vicente-Grabovetsky A, Doeller CF. Insight reconfigures hippocampalprefrontal memories. Curr Biol. 2015;25: 821-30. doi:10.1016/j.cub.2015.01.033</p>
</li>
<li>Morton NW, Schlichting ML, Preston AR. Representations of common event structure in medial temporal lobe and frontoparietal cortex support efficient inference. Proc Natl Acad Sci USA. 2020;117: 29338-29345. doi:10.1073/pnas. 1912338117</li>
<li>Schapiro AC, Rogers TT, Cordova NI, Turk-Browne NB, Botvinick MM. Neural representations of events arise from temporal community structure. Nat Neurosci. 2013;16: 486-92. doi:10.1038/nn. 3331</li>
<li>Ester EF, Sprague TC, Serences JT. Categorical Biases in Human Occipitoparietal Cortex. J Neurosci. 2020;40: 917-931. doi:10.1523/JNEUROSCI.2700-19.2019</li>
</ol>            </div>
        </div>

    </div>
</body>
</html>
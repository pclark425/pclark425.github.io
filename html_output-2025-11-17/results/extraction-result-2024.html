<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2024 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2024</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2024</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-48.html">extraction-schema-48</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <p><strong>Paper ID:</strong> paper-1cff6353f155b28314d2cdd6783c13dcf48b72a3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1cff6353f155b28314d2cdd6783c13dcf48b72a3" target="_blank">Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning</a></p>
                <p><strong>Paper TL;DR:</strong> This work investigates out-of-distribution (OOD) generalization in Transformer networks using a GSM8K-style modular arithmetic on computational graphs task as a testbed and introduces and explores a set of four architectural mechanisms aimed at enhancing OOD generalization.</p>
                <p><strong>Paper Abstract:</strong> Systematic, compositional generalization beyond the training distribution remains a core challenge in machine learning -- and a critical bottleneck for the emergent reasoning abilities of modern language models. This work investigates out-of-distribution (OOD) generalization in Transformer networks using a GSM8K-style modular arithmetic on computational graphs task as a testbed. We introduce and explore a set of four architectural mechanisms aimed at enhancing OOD generalization: (i) input-adaptive recurrence; (ii) algorithmic supervision; (iii) anchored latent representations via a discrete bottleneck; and (iv) an explicit error-correction mechanism. Collectively, these mechanisms yield an architectural approach for native and scalable latent space reasoning in Transformer networks with robust algorithmic generalization capabilities. We complement these empirical results with a detailed mechanistic interpretability analysis that reveals how these mechanisms give rise to robust OOD generalization abilities.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2024.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2024.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Modular-Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular Arithmetic on Computational Graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synthetic mathematical-reasoning benchmark where directed acyclic graphs (DAGs) of variable size encode modular (+, -, × mod 23) computations; the model must compute the value of every node. Complexity is controlled by graph size (N) and graph depth, enabling precise OOD/length-generalization tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Modular arithmetic on computational graphs</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Each instance is a DAG with N nodes (leaf nodes = L) where leaves are assigned integers in {0,...,22} and each non-leaf node is defined by operations in {+, -, ×} modulo p=23 over its parent nodes. Instances are tokenized (variable names, values, ops, [sep]). The objective is to compute the exact value of every node; an instance is correct only if all node values match ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Graphs up to N=128 nodes (training N ≤ 32; test up to 128). Depth corresponds to graph layer depth; training regime covers depths implicit in N ≤ 32, OOD tests up to 4× larger graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>Function composition / layered arithmetic computations (layer-by-layer evaluation of nodes via operations composed over parents).</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>Length/size OOD split: train on randomly generated graphs with N ≤ 32, test on IID graphs and out-of-distribution larger graphs up to N=128 (longer sequences / deeper computation).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised learning from scratch on randomly generated DAGs (algorithmic intermediate supervision is used by some methods; baselines include end-to-end and chain-of-thought).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Paper evaluates End-to-End (feedforward & recurrent), Chain-of-Thought, Continuous Latent Space Supervision, Discrete Latent Space Supervision, and Discrete Latent Space Supervision with self-correction across IID and OOD graph sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2024.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2024.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DLS+SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discrete Latent Space Supervision with Self-Correction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent Transformer architecture that (1) applies input-adaptive recurrence, (2) supervises latent states (algorithmic alignment loss at each iteration), (3) discretizes latent states into structured factors (syntax, variable, operation, value) between iterations, and (4) trains with stochastic latent-value corruptions to learn self-correction; achieves robust OOD/length generalization on the modular-graph task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent Transformer (2-layer, 16 attention heads, hidden dim 256) with discrete latent supervision and self-correction</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single recurrent Transformer block applied T times (T adaptive to input depth). After each recurrent step, continuous hidden states are projected via factor-wise argmax into a structured discrete latent (syntax, variable, operation, value), re-embedded, and fed to next iteration. A shared linear readout predicts node values at each iteration for algorithmic supervision; occasional random corruption of discrete value factors during training teaches self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Input-adaptive recurrence; latent algorithmic supervision (step-wise loss); discrete structured latent bottleneck (factorized); explicit error-correction training (stochastic corruptions); attention-based retrieval (induction-head-like patterns).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Modular arithmetic on computational graphs</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same DAG modular arithmetic task; model expected to compute node values iteratively (one effective graph-layer per recurrent iteration), scaling T at test time to handle deeper graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Trained on graphs with N ≤ 32, tested up to N = 128 (4× larger).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>Layered function composition (iterative evaluation of composed arithmetic operations).</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>Train on N ≤ 32, test on progressively larger randomly generated graphs up to N=128 (length/size generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised training from scratch with AlgorithmAlignmentLoss (cross-entropy on predicted node values at each iteration for nodes with depth ≤ t). Additionally, during training a small probability of random corruption is applied to value factors to teach self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>Small-probability random corruption of discrete latent value factors during training to create corrupted intermediate states for the model to learn to correct. (No numeric corruption rate reported in main text.)</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Near-perfect in-distribution performance (N ≤ 32) — described as near-perfect / perfect in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Near-perfect / effectively perfect out-of-distribution generalization across examined OOD splits up to N = 128 when test-time recurrent iterations are scaled appropriately; paper states 'near-perfect OOD generalization' and 'perfect generalization on inputs that are several times larger than those seen during training'.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Reported as negligible — near-zero gap between IID (N ≤ 32) and tested OOD sizes when all mechanisms (recurrence, latent supervision, discretization, self-correction) are present.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Paper shows solved fraction remains ~1.0 across tested graph sizes up to N=128 when computation time (T) is scaled; exact per-depth percentages not tabulated in text, but figures and narrative report near-perfect solving across OOD depths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against Feedforward End-to-End (no mechanisms), Recurrent End-to-End (recurrence but no latent supervision), Chain-of-Thought (autoregressive CoT supervision), Continuous Latent Space Supervision (recurrence + latent supervision, continuous), and Discrete Latent Space Supervision (discretization but no self-correction). DLS+SC outperforms all baselines on OOD generalization; in particular: End-to-End fails beyond small graphs, Chain-of-Thought yields near-perfect IID but limited OOD (≈ up to N≈40), Continuous latent outperforms CoT but slowly degrades with larger N, Discrete latent (without SC) improves further, and adding self-correction yields near-perfect robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Architectures compared: (a) feedforward Transformer (no recurrence), (b) recurrent Transformer end-to-end (recurrence only), (c) autoregressive CoT Transformer (chain-of-thought token supervision), (d) recurrent Transformer with continuous latent supervision, (e) recurrent Transformer with discrete latent supervision, (f) discrete latent supervision + self-correction (this method). Relative performance increases monotonically across (a)→(f), with (f) achieving best OOD results.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining input-adaptive recurrence, latent algorithmic supervision, anchored discrete latent representations, and explicit error-correction produces robust, depth-invariant algorithms in recurrent Transformers that generalize nearly perfectly to graphs up to 4× larger than training; discretization anchors representations preventing drift over many iterations, and self-correction eliminates residual error accumulation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Paper notes that effective error-correction variants required larger recurrent-block capacity (more layers) — i.e., error-correction is more demanding computationally per recurrent iteration. No other systematic failure modes quantitatively reported.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Success requires: (1) input-adaptive recurrence (scale T at test time), (2) step-wise latent supervision aligning intermediate states to algorithmic progress, (3) discrete factorized latent bottleneck to anchor representations across iterations, and (4) training with stochastic corruptions to enable self-correction; without these, OOD generalization degrades.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2024.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2024.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiscreteLatent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discrete Latent Space Supervision (no self-correction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recurrent Transformer trained with algorithmic supervision on latent states and with a discrete, factorized latent bottleneck between recurrent iterations (syntax, variable, operation, value) but without explicit corruption-based self-correction training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent Transformer with discrete factorized latent bottleneck</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same recurrent block and latent-factor discretization as DLS+SC; after each recurrent step hidden states are argmax-ed per factor and re-embedded, and a shared readout predicts node values for algorithm-alignment loss at each iteration. Does not include training-time corruption for explicit self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Input-adaptive recurrence; latent algorithmic supervision; discrete structured latent bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Modular arithmetic on computational graphs</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same DAG modular-arithmetic task used across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Trained on N ≤ 32; evaluated up to N = 128.</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>Layered function composition.</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>Length/size OOD split (train N ≤ 32, test up to 128).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised with AlgorithmAlignmentLoss across recurrent iterations; no corruption-based self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>High / near-perfect in-distribution (N ≤ 32).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Significantly improved OOD generalization relative to continuous-latent and CoT baselines; discretization 'anchors' representations and allows much deeper recurrence before performance degrades. Not claimed to be perfectly robust without self-correction (performance better than continuous but not as perfect as with self-correction).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Substantially smaller than baselines (CoT/continuous), but non-zero compared to DLS+SC which attains near-perfect robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Maintains strong performance out to larger N than baselines; performance begins to degrade at very large depths but more slowly than continuous latent variants (exact per-depth numerics not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Outperforms Continuous Latent Space Supervision and Chain-of-Thought on OOD sizes; improves stability of latent dynamics enabling greater recurrent depth.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Compared directly against Continuous Latent (same supervision but no discretization) and CoT/End-to-End baselines; discretization yields clear advantage for long-run iterative computation.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Anchoring latent states via discretization greatly reduces representational drift and allows recurrence to scale to greater depths compared to continuous latent supervision; discretization is a core ingredient for robust depth-invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Without explicit self-correction training, occasional errors can still accumulate at extreme depths; performance eventually degrades as iterations lengthen beyond what training covered.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Requires algorithmic latent supervision plus discrete bottleneck and input-adaptive recurrence; benefits are strongest when re-embedding is shared across iterations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2024.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2024.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ContLatent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continuous Latent Space Supervision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recurrent Transformer with algorithmic supervision applied to continuous latent states at each iteration (no discretization), enabling iterative computation but susceptible to representational drift and error accumulation when scaled beyond training depths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent Transformer with continuous latent supervision</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent Transformer block with AlgorithmAlignmentLoss applied to distributed (continuous) embeddings at each iteration; no argmax discretization between iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Input-adaptive recurrence; latent algorithmic supervision; continuous distributed latent states (no symbolic bottleneck).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Modular arithmetic on computational graphs</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same DAG modular arithmetic task.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Trained on N ≤ 32; tested up to N = 128.</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>Layered function composition.</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>Length/size OOD split (train N ≤ 32, test larger graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised with AlgorithmAlignmentLoss across recurrent iterations; continuous latent states persist across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Good / high in-distribution (near CoT-level) — described as outperforming End-to-End baselines in-distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Outperforms Chain-of-Thought and End-to-End on moderate OOD sizes, but performance slowly degrades as test graph size (and recurrent depth) grows due to accumulation of noise/drift in continuous representations.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Smaller than Chain-of-Thought and End-to-End but notable as N increases; described qualitatively (no numeric gap provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Performance declines gradually with increasing required recurrent iterations; discrete variant mitigates this decline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Better OOD than Chain-of-Thought and End-to-End but worse than Discrete Latent variants; demonstrates importance of discretization for extreme OOD scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Direct ablation vs discrete-latent (same supervision but continuous vs discrete) shows discretization improves long-range iterative stability.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Latent supervision applied to continuous states helps but is insufficient for robust scaling; representational drift accumulates with recurrent depth causing gradual OOD degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Representational drift and noise accumulation in continuous latent vectors across many recurrent iterations leads to failure at larger OOD depths.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Works well for moderate extrapolation if recurrence depth is not too far beyond training range; benefits further from discretization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2024.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2024.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Supervision Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Autoregressive Transformer trained to generate tokenized intermediate reasoning steps (CoT) that produce the final solution; provides strong in-distribution performance but limited length-generalization due to token-sequentialization of computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Autoregressive Transformer trained with chain-of-thought token supervision</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Causal Transformer that is trained to output a sequence of intermediate computation steps (the chain-of-thought) culminating in final node values; computation is materialized in token-space rather than latent-space recurrence.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Autoregressive decoding / chain-of-thought token supervision; no explicit latent-state supervision or discretization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Chain-of-Thought training on modular computation graphs</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Model receives problem prompt then generates a tokenized topological-order computation trace (equations and computed values) culminating in final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Trained on graphs with N ≤ 32; CoT length grows with graph size (requires longer autoregressive traces for larger graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>Sequential tokenized execution of algorithmic steps.</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>Length/size OOD split (train N ≤ 32, test larger graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised autoregressive CoT training on step-by-step sequences (CoT examples for each training instance).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Near-perfect in-distribution performance (N ≤ 32) according to paper narrative.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Limited OOD generalization: CoT models generalize somewhat to moderately larger graphs (approx. up to N ≈ 40 reported qualitatively) but performance rapidly deteriorates as graph size increases beyond training range.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Substantial: near-perfect IID (N ≤ 32) dropping to poor performance at much larger N (qualitative description; exact numeric gap not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Good for N near training regime; degrades quickly as required CoT length grows beyond training lengths (paper cites N ≈ 40 as rough upper bound for observed generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against End-to-End, Continuous Latent, Discrete Latent, and Discrete+SC. CoT yields strong IID performance but weaker OOD than discrete-latent methods.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>CoT partially implements input-adaptive computation (via longer token traces) and partial intermediate supervision, but lacks latent-state anchoring and parallelism afforded by recurrence.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chain-of-thought supervision greatly improves in-distribution performance versus naive End-to-End training but remains brittle for strong length generalization; converting algorithmic supervision from token-space to latent-space with recurrence and discretization yields much stronger OOD results.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>CoT's token-by-token format creates brittle, serial algorithms that degrade with longer traces because computation is not carried out in the model's native parallel latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Effective for in-distribution training and modest extrapolation; fails when required CoT trace length is far beyond training lengths.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2024.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2024.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>End2End</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-End Supervised Baselines (feedforward & recurrent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard Transformers trained to directly output final node values from the input prompt without intermediate supervision; includes feedforward and recurrent variants. These baselines perform poorly on the fully-solved metric beyond small graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Feedforward Transformer / Recurrent Transformer trained end-to-end</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models trained to map input token sequence to final node values directly (no CoT, no latent supervision). Variants: feedforward (no recurrence) and recurrent (recurrent block applied but no algorithmic latent supervision).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Standard Transformer architectures (feedforward and recurrent variants) with no explicit algorithmic latent supervision or discrete bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>End-to-End learning on modular computation graphs</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict all node values given input description without intermediate steps; evaluated on same OOD splits.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Trained on N ≤ 32; tested up to N = 128.</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>Direct mapping of input graph to final composed values (no explicit iterative structure enforced).</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>Length/size OOD (train N ≤ 32, test larger graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised end-to-end training from scratch; greedy decoding at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Poor to moderate: feedforward and recurrent variants fail to effectively learn the task beyond small graphs; recurrent slightly better in-distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Fail to generalize: both feedforward and recurrent End-to-End baselines fail to fully solve graphs beyond small sizes; perform substantially worse than CoT and latent-supervision methods on OOD graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Large gap: weak IID and very poor OOD; quantitative gap not provided but described as rapid degradation beyond small N.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Very poor beyond shallow depths; recurrent variant slightly outperforms feedforward but still inadequate for N≫training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to CoT, Continuous Latent, Discrete Latent, and DLS+SC; End-to-End is the weakest performer especially on OOD lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Feedforward (no recurrence) vs recurrent (recurrence but no supervision) shows modest improvement from recurrence alone but not enough for strong OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>End-to-End supervision without intermediate algorithmic guidance or architectural inductive biases fails to produce algorithms that generalize in length/depth; recurrence alone gives only slight improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Models do not learn depth-invariant iterative computation; they fail on the stringent 'fully solved' metric as graph size grows.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>None observed for large OOD extrapolation under pure end-to-end training in this work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2024.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2024.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RecurrentBlock</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Transformer Block (input-adaptive recurrence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single Transformer block applied recurrently T times to the entire context; T is scaled at test time proportional to input complexity (graph depth), enabling parallel latent-space iterative computation rather than token-sequential CoT traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent Transformer Block (as in Universal/Looped/Applied Recurrently)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The same Transformer layer (attention + MLP) is iteratively applied to the whole sequence; intermediate states are fed back for the next iteration. Number of iterations T is input-adaptive to allow scaling of computation at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Recurrence across depth (iterative application of same block), input-adaptive computation time (scale T at test time), ability to perform parallel updates across tokens per iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Used as building block for recurrent models on modular-graph task</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Enables implementing layer-by-layer algorithm in latent space by applying same computation per iteration to compute one additional graph layer per step.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>Supports iterative composition by repeated application of block.</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Used within supervised schemes (end-to-end or latent supervised) and combined with discretization and self-correction in full method.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>When used with latent supervision and discretization, enables scaling to deeper graphs by increasing T; alone gives modest gains vs feedforward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Recurrent End-to-End (recurrence only) modestly outperforms feedforward End-to-End; recurrence is necessary but not sufficient for robust OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Compared implicitly against autoregressive CoT (serial token computation) and feedforward (no recurrence); recurrence prefers latent iterative algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Input-adaptive recurrence is a crucial inductive bias enabling iterative, depth-invariant algorithms in latent space; scaling T at test time lets models solve larger instances than training when supported by proper latent anchoring and supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Recurrence without appropriate anchoring or supervision can still lead to drift or brittle algorithms when pushed far beyond training iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Works best when combined with latent algorithmic supervision and discrete re-embedding to avoid drift.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Universal Transformers <em>(Rating: 2)</em></li>
                <li>Exploring Length Generalization in Large Language Models <em>(Rating: 2)</em></li>
                <li>End-to-End Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking <em>(Rating: 2)</em></li>
                <li>In-context Learning and Induction Heads <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2024",
    "paper_id": "paper-1cff6353f155b28314d2cdd6783c13dcf48b72a3",
    "extraction_schema_id": "extraction-schema-48",
    "extracted_data": [
        {
            "name_short": "Modular-Graph",
            "name_full": "Modular Arithmetic on Computational Graphs",
            "brief_description": "Synthetic mathematical-reasoning benchmark where directed acyclic graphs (DAGs) of variable size encode modular (+, -, × mod 23) computations; the model must compute the value of every node. Complexity is controlled by graph size (N) and graph depth, enabling precise OOD/length-generalization tests.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "is_pretrained": null,
            "architectural_features": null,
            "task_domain": "mathematical reasoning",
            "task_name": "Modular arithmetic on computational graphs",
            "task_description": "Each instance is a DAG with N nodes (leaf nodes = L) where leaves are assigned integers in {0,...,22} and each non-leaf node is defined by operations in {+, -, ×} modulo p=23 over its parent nodes. Instances are tokenized (variable names, values, ops, [sep]). The objective is to compute the exact value of every node; an instance is correct only if all node values match ground truth.",
            "compositional_depth": "Graphs up to N=128 nodes (training N ≤ 32; test up to 128). Depth corresponds to graph layer depth; training regime covers depths implicit in N ≤ 32, OOD tests up to 4× larger graphs.",
            "composition_type": "Function composition / layered arithmetic computations (layer-by-layer evaluation of nodes via operations composed over parents).",
            "split_type": "Length/size OOD split: train on randomly generated graphs with N ≤ 32, test on IID graphs and out-of-distribution larger graphs up to N=128 (longer sequences / deeper computation).",
            "training_strategy": "Supervised learning from scratch on randomly generated DAGs (algorithmic intermediate supervision is used by some methods; baselines include end-to-end and chain-of-thought).",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": null,
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Paper evaluates End-to-End (feedforward & recurrent), Chain-of-Thought, Continuous Latent Space Supervision, Discrete Latent Space Supervision, and Discrete Latent Space Supervision with self-correction across IID and OOD graph sizes.",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": null,
            "failure_analysis": null,
            "success_conditions": null,
            "uuid": "e2024.0"
        },
        {
            "name_short": "DLS+SC",
            "name_full": "Discrete Latent Space Supervision with Self-Correction",
            "brief_description": "A recurrent Transformer architecture that (1) applies input-adaptive recurrence, (2) supervises latent states (algorithmic alignment loss at each iteration), (3) discretizes latent states into structured factors (syntax, variable, operation, value) between iterations, and (4) trains with stochastic latent-value corruptions to learn self-correction; achieves robust OOD/length generalization on the modular-graph task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Recurrent Transformer (2-layer, 16 attention heads, hidden dim 256) with discrete latent supervision and self-correction",
            "model_description": "Single recurrent Transformer block applied T times (T adaptive to input depth). After each recurrent step, continuous hidden states are projected via factor-wise argmax into a structured discrete latent (syntax, variable, operation, value), re-embedded, and fed to next iteration. A shared linear readout predicts node values at each iteration for algorithmic supervision; occasional random corruption of discrete value factors during training teaches self-correction.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "Input-adaptive recurrence; latent algorithmic supervision (step-wise loss); discrete structured latent bottleneck (factorized); explicit error-correction training (stochastic corruptions); attention-based retrieval (induction-head-like patterns).",
            "task_domain": "mathematical reasoning",
            "task_name": "Modular arithmetic on computational graphs",
            "task_description": "Same DAG modular arithmetic task; model expected to compute node values iteratively (one effective graph-layer per recurrent iteration), scaling T at test time to handle deeper graphs.",
            "compositional_depth": "Trained on graphs with N ≤ 32, tested up to N = 128 (4× larger).",
            "composition_type": "Layered function composition (iterative evaluation of composed arithmetic operations).",
            "split_type": "Train on N ≤ 32, test on progressively larger randomly generated graphs up to N=128 (length/size generalization).",
            "training_strategy": "Supervised training from scratch with AlgorithmAlignmentLoss (cross-entropy on predicted node values at each iteration for nodes with depth ≤ t). Additionally, during training a small probability of random corruption is applied to value factors to teach self-correction.",
            "curriculum_details": null,
            "inoculation_details": "Small-probability random corruption of discrete latent value factors during training to create corrupted intermediate states for the model to learn to correct. (No numeric corruption rate reported in main text.)",
            "iid_performance": "Near-perfect in-distribution performance (N ≤ 32) — described as near-perfect / perfect in main text.",
            "compositional_performance": "Near-perfect / effectively perfect out-of-distribution generalization across examined OOD splits up to N = 128 when test-time recurrent iterations are scaled appropriately; paper states 'near-perfect OOD generalization' and 'perfect generalization on inputs that are several times larger than those seen during training'.",
            "generalization_gap": "Reported as negligible — near-zero gap between IID (N ≤ 32) and tested OOD sizes when all mechanisms (recurrence, latent supervision, discretization, self-correction) are present.",
            "performance_by_depth": "Paper shows solved fraction remains ~1.0 across tested graph sizes up to N=128 when computation time (T) is scaled; exact per-depth percentages not tabulated in text, but figures and narrative report near-perfect solving across OOD depths.",
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared against Feedforward End-to-End (no mechanisms), Recurrent End-to-End (recurrence but no latent supervision), Chain-of-Thought (autoregressive CoT supervision), Continuous Latent Space Supervision (recurrence + latent supervision, continuous), and Discrete Latent Space Supervision (discretization but no self-correction). DLS+SC outperforms all baselines on OOD generalization; in particular: End-to-End fails beyond small graphs, Chain-of-Thought yields near-perfect IID but limited OOD (≈ up to N≈40), Continuous latent outperforms CoT but slowly degrades with larger N, Discrete latent (without SC) improves further, and adding self-correction yields near-perfect robustness.",
            "architectural_comparison": "Architectures compared: (a) feedforward Transformer (no recurrence), (b) recurrent Transformer end-to-end (recurrence only), (c) autoregressive CoT Transformer (chain-of-thought token supervision), (d) recurrent Transformer with continuous latent supervision, (e) recurrent Transformer with discrete latent supervision, (f) discrete latent supervision + self-correction (this method). Relative performance increases monotonically across (a)→(f), with (f) achieving best OOD results.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Combining input-adaptive recurrence, latent algorithmic supervision, anchored discrete latent representations, and explicit error-correction produces robust, depth-invariant algorithms in recurrent Transformers that generalize nearly perfectly to graphs up to 4× larger than training; discretization anchors representations preventing drift over many iterations, and self-correction eliminates residual error accumulation.",
            "failure_analysis": "Paper notes that effective error-correction variants required larger recurrent-block capacity (more layers) — i.e., error-correction is more demanding computationally per recurrent iteration. No other systematic failure modes quantitatively reported.",
            "success_conditions": "Success requires: (1) input-adaptive recurrence (scale T at test time), (2) step-wise latent supervision aligning intermediate states to algorithmic progress, (3) discrete factorized latent bottleneck to anchor representations across iterations, and (4) training with stochastic corruptions to enable self-correction; without these, OOD generalization degrades.",
            "uuid": "e2024.1"
        },
        {
            "name_short": "DiscreteLatent",
            "name_full": "Discrete Latent Space Supervision (no self-correction)",
            "brief_description": "Recurrent Transformer trained with algorithmic supervision on latent states and with a discrete, factorized latent bottleneck between recurrent iterations (syntax, variable, operation, value) but without explicit corruption-based self-correction training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Recurrent Transformer with discrete factorized latent bottleneck",
            "model_description": "Same recurrent block and latent-factor discretization as DLS+SC; after each recurrent step hidden states are argmax-ed per factor and re-embedded, and a shared readout predicts node values for algorithm-alignment loss at each iteration. Does not include training-time corruption for explicit self-correction.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "Input-adaptive recurrence; latent algorithmic supervision; discrete structured latent bottleneck.",
            "task_domain": "mathematical reasoning",
            "task_name": "Modular arithmetic on computational graphs",
            "task_description": "Same DAG modular-arithmetic task used across experiments.",
            "compositional_depth": "Trained on N ≤ 32; evaluated up to N = 128.",
            "composition_type": "Layered function composition.",
            "split_type": "Length/size OOD split (train N ≤ 32, test up to 128).",
            "training_strategy": "Supervised with AlgorithmAlignmentLoss across recurrent iterations; no corruption-based self-correction.",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "High / near-perfect in-distribution (N ≤ 32).",
            "compositional_performance": "Significantly improved OOD generalization relative to continuous-latent and CoT baselines; discretization 'anchors' representations and allows much deeper recurrence before performance degrades. Not claimed to be perfectly robust without self-correction (performance better than continuous but not as perfect as with self-correction).",
            "generalization_gap": "Substantially smaller than baselines (CoT/continuous), but non-zero compared to DLS+SC which attains near-perfect robustness.",
            "performance_by_depth": "Maintains strong performance out to larger N than baselines; performance begins to degrade at very large depths but more slowly than continuous latent variants (exact per-depth numerics not provided).",
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Outperforms Continuous Latent Space Supervision and Chain-of-Thought on OOD sizes; improves stability of latent dynamics enabling greater recurrent depth.",
            "architectural_comparison": "Compared directly against Continuous Latent (same supervision but no discretization) and CoT/End-to-End baselines; discretization yields clear advantage for long-run iterative computation.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Anchoring latent states via discretization greatly reduces representational drift and allows recurrence to scale to greater depths compared to continuous latent supervision; discretization is a core ingredient for robust depth-invariance.",
            "failure_analysis": "Without explicit self-correction training, occasional errors can still accumulate at extreme depths; performance eventually degrades as iterations lengthen beyond what training covered.",
            "success_conditions": "Requires algorithmic latent supervision plus discrete bottleneck and input-adaptive recurrence; benefits are strongest when re-embedding is shared across iterations.",
            "uuid": "e2024.2"
        },
        {
            "name_short": "ContLatent",
            "name_full": "Continuous Latent Space Supervision",
            "brief_description": "Recurrent Transformer with algorithmic supervision applied to continuous latent states at each iteration (no discretization), enabling iterative computation but susceptible to representational drift and error accumulation when scaled beyond training depths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Recurrent Transformer with continuous latent supervision",
            "model_description": "Recurrent Transformer block with AlgorithmAlignmentLoss applied to distributed (continuous) embeddings at each iteration; no argmax discretization between iterations.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "Input-adaptive recurrence; latent algorithmic supervision; continuous distributed latent states (no symbolic bottleneck).",
            "task_domain": "mathematical reasoning",
            "task_name": "Modular arithmetic on computational graphs",
            "task_description": "Same DAG modular arithmetic task.",
            "compositional_depth": "Trained on N ≤ 32; tested up to N = 128.",
            "composition_type": "Layered function composition.",
            "split_type": "Length/size OOD split (train N ≤ 32, test larger graphs).",
            "training_strategy": "Supervised with AlgorithmAlignmentLoss across recurrent iterations; continuous latent states persist across steps.",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Good / high in-distribution (near CoT-level) — described as outperforming End-to-End baselines in-distribution.",
            "compositional_performance": "Outperforms Chain-of-Thought and End-to-End on moderate OOD sizes, but performance slowly degrades as test graph size (and recurrent depth) grows due to accumulation of noise/drift in continuous representations.",
            "generalization_gap": "Smaller than Chain-of-Thought and End-to-End but notable as N increases; described qualitatively (no numeric gap provided).",
            "performance_by_depth": "Performance declines gradually with increasing required recurrent iterations; discrete variant mitigates this decline.",
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Better OOD than Chain-of-Thought and End-to-End but worse than Discrete Latent variants; demonstrates importance of discretization for extreme OOD scaling.",
            "architectural_comparison": "Direct ablation vs discrete-latent (same supervision but continuous vs discrete) shows discretization improves long-range iterative stability.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Latent supervision applied to continuous states helps but is insufficient for robust scaling; representational drift accumulates with recurrent depth causing gradual OOD degradation.",
            "failure_analysis": "Representational drift and noise accumulation in continuous latent vectors across many recurrent iterations leads to failure at larger OOD depths.",
            "success_conditions": "Works well for moderate extrapolation if recurrence depth is not too far beyond training range; benefits further from discretization.",
            "uuid": "e2024.3"
        },
        {
            "name_short": "CoT-baseline",
            "name_full": "Chain-of-Thought Supervision Baseline",
            "brief_description": "Autoregressive Transformer trained to generate tokenized intermediate reasoning steps (CoT) that produce the final solution; provides strong in-distribution performance but limited length-generalization due to token-sequentialization of computation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Autoregressive Transformer trained with chain-of-thought token supervision",
            "model_description": "Causal Transformer that is trained to output a sequence of intermediate computation steps (the chain-of-thought) culminating in final node values; computation is materialized in token-space rather than latent-space recurrence.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "Autoregressive decoding / chain-of-thought token supervision; no explicit latent-state supervision or discretization.",
            "task_domain": "mathematical reasoning",
            "task_name": "Chain-of-Thought training on modular computation graphs",
            "task_description": "Model receives problem prompt then generates a tokenized topological-order computation trace (equations and computed values) culminating in final answers.",
            "compositional_depth": "Trained on graphs with N ≤ 32; CoT length grows with graph size (requires longer autoregressive traces for larger graphs).",
            "composition_type": "Sequential tokenized execution of algorithmic steps.",
            "split_type": "Length/size OOD split (train N ≤ 32, test larger graphs).",
            "training_strategy": "Supervised autoregressive CoT training on step-by-step sequences (CoT examples for each training instance).",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Near-perfect in-distribution performance (N ≤ 32) according to paper narrative.",
            "compositional_performance": "Limited OOD generalization: CoT models generalize somewhat to moderately larger graphs (approx. up to N ≈ 40 reported qualitatively) but performance rapidly deteriorates as graph size increases beyond training range.",
            "generalization_gap": "Substantial: near-perfect IID (N ≤ 32) dropping to poor performance at much larger N (qualitative description; exact numeric gap not reported).",
            "performance_by_depth": "Good for N near training regime; degrades quickly as required CoT length grows beyond training lengths (paper cites N ≈ 40 as rough upper bound for observed generalization).",
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared against End-to-End, Continuous Latent, Discrete Latent, and Discrete+SC. CoT yields strong IID performance but weaker OOD than discrete-latent methods.",
            "architectural_comparison": "CoT partially implements input-adaptive computation (via longer token traces) and partial intermediate supervision, but lacks latent-state anchoring and parallelism afforded by recurrence.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Chain-of-thought supervision greatly improves in-distribution performance versus naive End-to-End training but remains brittle for strong length generalization; converting algorithmic supervision from token-space to latent-space with recurrence and discretization yields much stronger OOD results.",
            "failure_analysis": "CoT's token-by-token format creates brittle, serial algorithms that degrade with longer traces because computation is not carried out in the model's native parallel latent space.",
            "success_conditions": "Effective for in-distribution training and modest extrapolation; fails when required CoT trace length is far beyond training lengths.",
            "uuid": "e2024.4"
        },
        {
            "name_short": "End2End",
            "name_full": "End-to-End Supervised Baselines (feedforward & recurrent)",
            "brief_description": "Standard Transformers trained to directly output final node values from the input prompt without intermediate supervision; includes feedforward and recurrent variants. These baselines perform poorly on the fully-solved metric beyond small graphs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Feedforward Transformer / Recurrent Transformer trained end-to-end",
            "model_description": "Models trained to map input token sequence to final node values directly (no CoT, no latent supervision). Variants: feedforward (no recurrence) and recurrent (recurrent block applied but no algorithmic latent supervision).",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "Standard Transformer architectures (feedforward and recurrent variants) with no explicit algorithmic latent supervision or discrete bottleneck.",
            "task_domain": "mathematical reasoning",
            "task_name": "End-to-End learning on modular computation graphs",
            "task_description": "Predict all node values given input description without intermediate steps; evaluated on same OOD splits.",
            "compositional_depth": "Trained on N ≤ 32; tested up to N = 128.",
            "composition_type": "Direct mapping of input graph to final composed values (no explicit iterative structure enforced).",
            "split_type": "Length/size OOD (train N ≤ 32, test larger graphs).",
            "training_strategy": "Supervised end-to-end training from scratch; greedy decoding at inference.",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Poor to moderate: feedforward and recurrent variants fail to effectively learn the task beyond small graphs; recurrent slightly better in-distribution.",
            "compositional_performance": "Fail to generalize: both feedforward and recurrent End-to-End baselines fail to fully solve graphs beyond small sizes; perform substantially worse than CoT and latent-supervision methods on OOD graphs.",
            "generalization_gap": "Large gap: weak IID and very poor OOD; quantitative gap not provided but described as rapid degradation beyond small N.",
            "performance_by_depth": "Very poor beyond shallow depths; recurrent variant slightly outperforms feedforward but still inadequate for N≫training.",
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared to CoT, Continuous Latent, Discrete Latent, and DLS+SC; End-to-End is the weakest performer especially on OOD lengths.",
            "architectural_comparison": "Feedforward (no recurrence) vs recurrent (recurrence but no supervision) shows modest improvement from recurrence alone but not enough for strong OOD generalization.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "End-to-End supervision without intermediate algorithmic guidance or architectural inductive biases fails to produce algorithms that generalize in length/depth; recurrence alone gives only slight improvement.",
            "failure_analysis": "Models do not learn depth-invariant iterative computation; they fail on the stringent 'fully solved' metric as graph size grows.",
            "success_conditions": "None observed for large OOD extrapolation under pure end-to-end training in this work.",
            "uuid": "e2024.5"
        },
        {
            "name_short": "RecurrentBlock",
            "name_full": "Recurrent Transformer Block (input-adaptive recurrence)",
            "brief_description": "A single Transformer block applied recurrently T times to the entire context; T is scaled at test time proportional to input complexity (graph depth), enabling parallel latent-space iterative computation rather than token-sequential CoT traces.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Recurrent Transformer Block (as in Universal/Looped/Applied Recurrently)",
            "model_description": "The same Transformer layer (attention + MLP) is iteratively applied to the whole sequence; intermediate states are fed back for the next iteration. Number of iterations T is input-adaptive to allow scaling of computation at inference.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "Recurrence across depth (iterative application of same block), input-adaptive computation time (scale T at test time), ability to perform parallel updates across tokens per iteration.",
            "task_domain": "mathematical reasoning",
            "task_name": "Used as building block for recurrent models on modular-graph task",
            "task_description": "Enables implementing layer-by-layer algorithm in latent space by applying same computation per iteration to compute one additional graph layer per step.",
            "compositional_depth": null,
            "composition_type": "Supports iterative composition by repeated application of block.",
            "split_type": null,
            "training_strategy": "Used within supervised schemes (end-to-end or latent supervised) and combined with discretization and self-correction in full method.",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": null,
            "generalization_gap": null,
            "performance_by_depth": "When used with latent supervision and discretization, enables scaling to deeper graphs by increasing T; alone gives modest gains vs feedforward.",
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Recurrent End-to-End (recurrence only) modestly outperforms feedforward End-to-End; recurrence is necessary but not sufficient for robust OOD generalization.",
            "architectural_comparison": "Compared implicitly against autoregressive CoT (serial token computation) and feedforward (no recurrence); recurrence prefers latent iterative algorithms.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "Input-adaptive recurrence is a crucial inductive bias enabling iterative, depth-invariant algorithms in latent space; scaling T at test time lets models solve larger instances than training when supported by proper latent anchoring and supervision.",
            "failure_analysis": "Recurrence without appropriate anchoring or supervision can still lead to drift or brittle algorithms when pushed far beyond training iterations.",
            "success_conditions": "Works best when combined with latent algorithmic supervision and discrete re-embedding to avoid drift.",
            "uuid": "e2024.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Universal Transformers",
            "rating": 2
        },
        {
            "paper_title": "Exploring Length Generalization in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "End-to-End Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking",
            "rating": 2
        },
        {
            "paper_title": "In-context Learning and Induction Heads",
            "rating": 2
        }
    ],
    "cost": 0.020572749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning</h1>
<p>Awni Altabaa Siyu Chen John Lafferty Zhuoran Yang<br>Department of Statistics \&amp; Data Science, Yale University<br>{awni.altabaa, siyu.chen.sc3226, john.lafferty, zhuoran.yang}@yale.edu</p>
<h4>Abstract</h4>
<p>Systematic, compositional generalization beyond the training distribution remains a core challenge in machine learning-and a critical bottleneck for the emergent reasoning abilities of modern language models. This work investigates out-of-distribution (OOD) generalization in Transformer networks using a GSM8K-style modular arithmetic on computational graphs task as a testbed. We introduce and explore a set of four architectural mechanisms aimed at enhancing OOD generalization: (i) input-adaptive recurrence; (ii) algorithmic supervision; (iii) anchored latent representations via a discrete bottleneck; and (iv) an explicit error-correction mechanism. Collectively, these mechanisms yield an architectural approach for native and scalable latent space reasoning in Transformer networks with robust algorithmic generalization capabilities. We complement these empirical results with a detailed mechanistic interpretability analysis that reveals how these mechanisms give rise to robust OOD generalization abilities.</p>
<p>Date: October 17, 2025
Code: https://github.com/Awni00/algorithmic-generalization-transformer-architectures
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Four key mechanisms enabling robust out-of-distribution generalization in transformer architectures. (a) Recurrence and input-adaptive computation allows models to dynamically allocate computational resources based on problem complexity. (b) Algorithmic supervision guides the learning process through structured intermediate representations. (c) Anchored discrete latent spaces provide stable reference points for compositional reasoning. (d) Error correction mechanisms enable iterative refinement of predictions through feedback loops. Together, these mechanisms enable transformers to develop recursive reasoning patterns that generalize beyond their training distribution.</p>
<h1>Contents</h1>
<p>1 Introduction ..... 3
2 Related Work ..... 4
3 Problem Setup ..... 6
3.1 Task Description: Modular Arithmetic on Computational Graphs ..... 6
3.2 Limitations of Standard Transformers with CoT Training ..... 8
4 Reasoning in Latent Space with Algorithmic Supervision ..... 9
4.1 Mechanisms for Effective OOD Generalization. ..... 9
4.2 Experimental Results \&amp; Discussion ..... 12
5 Mechanistic Interpretability ..... 15
6 Conclusion ..... 17
A Experimental Details on Chain-of-Thought \&amp; End-to-End Baselines ..... 24
A. 1 End-to-End Baselines ..... 24
A. 2 Chain-of-Thought Baselines ..... 27
B Details on Latent State Supervision ..... 33
B. 1 Latent State Embedding Structure ..... 33
B. 2 Latent State Supervision ..... 33
B. 3 Discretization of Intermediate States ..... 34
B. 4 Self-Correction Mechanism ..... 34
B. 5 Experiment Details \&amp; Additional Results ..... 35
C Details of Mechanistic Interpretability Analysis ..... 38
C. 1 Technique Overview ..... 39
C. 2 First Layer Attention: Variable Copying ..... 42
C. 3 Second Layer Attention: Value Copying ..... 46
C. 4 Second Layer MLP: Module Addition in the Frequency Domain ..... 47
C. 5 Error Analysis ..... 51</p>
<h1>1. Introduction</h1>
<p>Systematic algorithmic generalization stands as a critical milestone and a grand challenge in machine learning research (B. Lake and Baroni, 2018; Pollack, 1990; Socher et al., 2012; Veličković and Blundell, 2021). This ability is fundamental to human cognition, stemming from our capacity for systematic compositionalityalgebraically producing novel combinations from known components and making strong generalizations from limited data (Chomsky, 1957; Fodor and Pylyshyn, 1988; B. M. Lake et al., 2017). Achieving such generalization necessitates learning universal, scalable problem-solving algorithms. Even in humans, acquiring such algorithmic understanding often requires explicit step-by-step supervision. Once an algorithm is learned, however, humans can generalize its application far beyond the domain of previously encountered stimuli or problems (John R Anderson, 1982; Singley and John Robert Anderson, 1989).</p>
<p>The reasoning capabilities of artificial intelligence systems have advanced rapidly in recent years, built upon the foundation of large language models. In particular, chain-of-thought (CoT) techniques have been central to enhancing the reasoning capabilities of these systems (Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et al., 2022; Kojima et al., 2022; Liu et al., 2023; Wei et al., 2022), especially in domains like mathematics (Cobbe et al., 2021; Lewkowycz et al., 2022; Lightman et al., 2023; Shao et al., 2024). CoT enables a model to receive supervision on learning a reference problem-solving procedure during training and allows the model to emulate this procedure at test-time. This progress presents a unique opportunity to make significant strides on foundational challenges related to reasoning in artificial intelligence.</p>
<p>Despite these advancements, out-of-distribution (OOD) generalization-particularly the type of length generalization involved in algorithmic reasoning (i.e., generalizing from simpler or smaller problem instances to larger or more complex ones)—has remained a central challenge and limitation for Transformerbased (Vaswani et al., 2017) language models (Anil et al., 2022; Jelassi et al., 2023; Kazemnejad et al., 2023; H. Zhou et al., 2024). While chain-of-thought techniques alleviate this to some degree by enabling the learning of more complex algorithmic procedures, the ability to generalize far outside the training distribution remains a significant obstacle (Stechly, Valmeekam, and Kambhampati, 2024; Y. Zhou et al., 2024).</p>
<p>In this work, we investigate the architectural and methodological mechanisms that underpin algorithmic OOD generalization in Transformer networks. To facilitate a systematic investigation, we focus our study on a simple yet scalable mathematical reasoning task: performing modular arithmetic on computational graphs. This task allows us to study OOD and algorithmic generalization in a controlled manner-with complexity directly parameterized by graph size and depth—while also capturing the core essence of established mathematical reasoning benchmarks like GSM8K (Cobbe et al., 2021), which are central to evaluating the reasoning capabilities of large language models. Furthermore, this task possesses a compositional nature; it can be solved by learning a core set of skills (e.g., a set of modular arithmetic operations and the ability to traverse the graph one layer at a time) and scaling up their application to solve larger and more complex problem instances. We use this task to explore the following guiding question:</p>
<p>What are the architectural mechanisms and inductive biases needed for robust OOD algorithmic generalization in Transformers?</p>
<p>We find that while standard CoT training techniques enable good in-distribution performance and a limited degree of OOD generalization, the learned solutions are not robust or universal, and their performance</p>
<p>rapidly degrades as test inputs grow in complexity beyond the training regime. We propose and explore a set of four simple architectural and methodological mechanisms, built upon the Transformer architecture, to facilitate the learning of robust and generalizable algorithmic solutions: (i) input-adaptive recurrence; (ii) algorithmic supervision; (iii) anchored latent representations via a discrete bottleneck; and (iv) an explicit error-correction mechanism. When combined, these mechanisms yield an architectural approach for native and scalable latent space reasoning in Transformer networks, demonstrating robust algorithmic generalization capabilities. In particular, on our mathematical reasoning task, our method achieves perfect generalization on inputs that are several times larger than those seen during training. We complement our architectural proposal and empirical results with a mechanistic interpretability analysis to reveal how these architectural proposals enable sharp OOD generalization, what circuits they learn, and why those circuits facilitate robust OOD generalization.</p>
<h1>2. Related Work</h1>
<p>Our work is related to several strands of fundamental machine learning research, including issues of out-ofdistribution generalization, architectural mechanisms such as recurrence and discretization, chain-of-thought and intermediate supervision methods, and work on mechanistic interpretability techniques.</p>
<p>Out-of-Distribution Generalization. Out-of-distribution (OOD) generalization, along with related capabilities such as compositionality and systematicity, poses a fundamental challenge in machine learning research (Barrett et al., 2018; Baxter, 2000; Hupkes et al., 2020; Pollack, 1990; Socher et al., 2012). These capabilities are crucial for developing AI systems that can reliably apply learned knowledge to novel scenarios, a hallmark of robust intelligence (Fodor and Pylyshyn, 1988; Goyal and Bengio, 2022; B. M. Lake et al., 2017). A particularly important type of OOD generalization, especially for algorithmic reasoning tasks, is length generalization-the ability to generalize from simpler or shorter training instances to significantly longer and more structurally complex instances. This has proven to be a key limitation of Transformer-based (Vaswani et al., 2017) language models (Anil et al., 2022; Jelassi et al., 2023; Kazemnejad et al., 2023; H. Zhou et al., 2024). While chain-of-thought techniques alleviate this to some degree by enabling the learning of more complex algorithmic procedures, the ability to generalize far outside the training distribution remains a significant obstacle (Stechly, Valmeekam, and Kambhampati, 2024; Y. Zhou et al., 2024).</p>
<p>Recurrence. Recurrence forms a foundational architectural principle in neural networks, particularly for tasks that involve sequential data or inherently iterative processes (Elman, 1990; Hochreiter and Schmidhuber, 1997; Jordan, 1997). These architectures are designed to emulate step-by-step computations by maintaining and updating an internal state, making them well-aligned with problems that have a recursive or layered solution structure. Sequence-to-sequence recurrent architectures for sequence transduction and neural machine translation advanced the state of the art (Cho et al., 2014; Sutskever, Vinyals, and Le, 2014), and were instrumental to the development of attention mechanisms and the Transformer architecture (Vaswani et al., 2017). While standard Transformers do not possess a recurrent structure, recurrent variants of the Transformer architecture were explored soon after its introduction (Dehghani et al., 2019). Whereas standard recurrent neural networks apply their recurrence across time or sequence length, recurrent Transformer architectures are parallel in time due to the parallel attention mechanism, but recurrent across computational depth—that is, the same Transformer layer is applied iteratively to the sequence as a whole. The recurrent inductive biases have been demonstrated to confer certain advantages in generalization (Fan et al., 2024; Yang</p>
<p>et al., 2024). In our work, recurrence is a key architectural mechanism encoding important inductive biases that aid the discovery of scalable recursive algorithms for solving the underlying mathematical problem.</p>
<p>Adaptive Computation. A critical challenge is handling inputs with varying complexity, where a fixed amount of computation may be inefficient or insufficient. This motivates the concept of adaptive computation, wherein a model can dynamically adjust its computation time, for example by varying the number of recurrent iterations, based on the demands of the input. An important work in this domain is the Adaptive Computation Time (ACT) mechanism proposed by Graves (2017) for recurrent neural networks, which explicitly models and learns how many computational steps are needed as a function of the input. A version of the ACT mechanism is incorporated in the recurrent Transformer architecture proposed by Dehghani et al. (2019). However, a drawback of such mechanisms is their complexity and difficulty of training. Although efforts have been made to explore simpler adaptive computation methods (Banino, Balaguer, and Blundell, 2021), an even simpler approach is explored by Bansal et al. (2022) and Schwarzschild et al. (2021), where the halting time is not explicitly modeled by the network, and instead the number of recurrent iterations is scaled at inference time based on the size of the input. This simpler approach can be easier to train, and has been shown to improve out-of-distribution generalization. More recently, Geiping et al. (2025) explored the viability of this approach as a way to perform test-time scaling in large language models. In our work, we similarly scale computation time by proportionately scaling the number of recurrent iterations in order to solve more complex problem instances, generalizing far beyond the training distribution.</p>
<p>Discreteness in Neural Networks. Symbolic AI systems derive their power from manipulating discrete symbols according to well-defined rules, which enables robust, precise, and interpretable reasoning (Fodor and Pylyshyn, 1988; Newel and Simon, 1976). Given this rich tradition of using discrete symbolic states in artificial intelligence, many works have subsequently explored incorporating such discrete latent representations into neural networks (Agustsson et al., 2017; Courville, Bergstra, and Bengio, 2011; Garcez, Lamb, and Gabbay, 2008; Oord, Vinyals, and Kavukcuoglu, 2018; Salakhutdinov and Hinton, 2009). Additionally, discreteness is often a central characteristic of constructions of Transformer networks for specific tasks. For example, Weiss, Goldberg, and Yahav (2021) develops a programming language that represents Transformerbased computation with discrete internal mechanisms. Additionally, Smolensky et al. (2024) constructs a Transformer network for a compositional in-context learning task, which features discreteness in both its latent states and attention mechanism. In our work, we explore the use of discrete latent states as a means of anchoring the latent representation to a common, depth-invariant space to enable scaling computation far beyond the training distribution while avoiding representational shift across computational depth.</p>
<p>Chain-of-Thought \&amp; Algorithmic Supervision. Chain-of-thought techniques have been central to enhancing the reasoning capabilities of large language models. Early usage of the term "chain-of-thought" referred to prompting techniques that condition a model to generate a sequence of intermediate steps before arriving at the final answer (Kojima et al., 2022; Nye et al., 2021; Wei et al., 2022). For example, Wei et al. (2022) demonstrated that prompting the LLM with a few CoT exemplars caused the model to generate an analogous step-by-step solution, which significantly improved performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. Kojima et al. (2022) showed that LLMs can be "zero-shot" reasoners in the sense that simply asking the model to reason step-by-step, without providing in-context learning CoT exemplars, can be sufficient to elicit chain-of-thought-style reasoning and improve performance. Modern usage of the term "chain-of-thought" has extended beyond prompting methods, as it now forms a key component of the training pipeline of LLMs, wherein a model is explicitly trained on demonstrations of</p>
<p>step-by-step solutions to problems of interest, such as mathematical reasoning (Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et al., 2024; Lewkowycz et al., 2022; Liu et al., 2023). In some situations, chain-of-thought training can be interpreted as providing explicit supervision to align the model to a particular algorithm or procedure for solving a problem, as opposed to simply providing supervision via input-output examples. In our work, we explore traditional chain-of-thought training techniques as baselines, as well as incorporate algorithmic supervision to the internal states of our proposed method.</p>
<p>Mechanistic Interpretability. In our work, we carry out a mechanistic interpretability analysis to probe how the model has learned to solve the task and why it can do so robustly, generalizing far outside the training distribution. In recent years, there has been a resurgence in work on interpretability, with new techniques being introduced that aim to understand modern large language models (Ameisen et al., 2025; Bricken et al., 2023; Elhage, Hume, et al., 2022; Elhage, Nanda, et al., 2021; Meng et al., 2022; Olsson et al., 2022). Elhage, Nanda, et al. (2021) is an influential work in this area of research, introducing a conceptual framework and new terminology that continues to be used in subsequent work. A key early achievement in this line of work is the discovery of "induction head" circuits in large language models (Olsson et al., 2022), which perform a two-step copying operation that is crucial for in-context learning. In our work, we identify a similar mechanism in our recurrent models that is used to copy previously computed variable values. This involves first retrieving the parent variables' names in the first layer, then using these variable names to retrieve their values in the second layer, which are computed elsewhere in the sequence of latent states. Such work is often described as circuit analysis, where the goal is to identify sub-networks that are responsible for particular functions. A key method for validating hypotheses about the functions of different model components is causal interventions like activation patching or ablations (Geiger, Lu, et al., 2021; Geiger, Wu, et al., 2024; Meng et al., 2022), which involves systematically modifying parts of the model or input to observe effects on behavior or internal states. We use related causal intervention techniques in our own mechanistic interpretability analysis in this work. Finally, the work by Nanda et al. (2023) and Tian (2024) is relevant as it specifically investigated how Transformers perform arithmetic, reverse-engineering a modular addition algorithm learned by the feedforward network in a Transformer layer-a phenomenon we also observe in our models.</p>
<h1>3. Problem Setup</h1>
<h3>3.1. Task Description: Modular Arithmetic on Computational Graphs</h3>
<p>We formally introduce the task of modular arithmetic on computational graphs as follows.
Task Description. A computation graph is a directed acyclic graph (DAG) representing a network of mathematical computations, where nodes correspond to variables and edges describe the dependencies between them. As illustrated in Figure 2 with an example, the leaf nodes in this DAG are directly assigned numerical values (e.g., $x_{7} \leftarrow 20$ ). All other non-leaf nodes are defined as functions of their parent nodes in the computation graph. In particular, the value of each non-leaf node is computed by applying one or more specified operations to the values of its parent nodes. In our experiments, we consider modular arithmetic operations (addition, multiplication, or subtraction), with the prime number $p=23$ as the modular base. For example, in Figure 2 we have $x_{23} \leftarrow x_{7}+x_{42}(\bmod p)$ and $x_{101} \leftarrow x_{23} \times x_{91}(\bmod p)$. In the following, we let $N$ and $L$ denote the total number of nodes and the number of leaf nodes, respectively. We consider graphs with up to</p>
<p>128 nodes, and let $\mathcal{V}=\left{x_{1}, \ldots, x_{128}\right}$ denote the set of variable names.
Data Generation Process. A problem instance in this task is specified by the values of the leaf nodes and $a$ computation graph depicting the computations that determine the values of all non-leaf nodes. In particular, given parameters $N$ and $L$, an input instance is generated as follows:
(i) Randomly generate a DAG with $N$ nodes, $L$ of which are leaf nodes.
(ii) Randomly assign a variable name from $\mathcal{V}$ to each node.
(iii) Randomly assign numerical values to the leaf nodes from $\mathcal{N}={0,1, \ldots, 22}$.
(iv) For each non-leaf node, randomly assign operations from $\mathcal{O}={+,-, \times}$ to define its computation based on its parent nodes</p>
<p>The instance generated by (i)-(iv) is stored as a token sequence, where each variable name, numerical value, and operation is assigned a unique token. A special separation token $[\mathrm{sep}]$ is used to separate different formulas. For example, the instance depicted in Figure 2 is represented as the following token sequence:</p>
<p>$$
\begin{aligned}
&amp; \langle 20\rangle\langle\rightarrow\rangle\langle x_{7}\rangle[\operatorname{sep}]\langle 2\rangle\langle\rightarrow\rangle\left\langle x_{42}\right\rangle[\operatorname{sep}]\langle 6\rangle\langle\rightarrow\rangle\left\langle x_{88}\right\rangle[\operatorname{sep}]\langle 14\rangle\langle\rightarrow\rangle\left\langle x_{115}\right\rangle \
&amp; \langle x_{7}\rangle\langle+\rangle\left\langle x_{42}\right\rangle\langle\rightarrow\rangle\left\langle x_{23}\right\rangle[\operatorname{sep}]\left\langle x_{42}\right\rangle\langle+\rangle\left\langle x_{88}\right\rangle\langle\rightarrow\rangle\left\langle x_{91}\right\rangle[\operatorname{sep}]\left\langle x_{88}\right\rangle\langle\times\rangle\left\langle x_{115}\right\rangle\langle\rightarrow\rangle\left\langle x_{55}\right\rangle \
&amp; \left\langle x_{23}\right\rangle\langle\times\rangle\left\langle x_{91}\right\rangle\langle\rightarrow\rangle\left\langle x_{101}\right\rangle[\operatorname{sep}]\left\langle x_{91}\right\rangle\langle-\rangle\left\langle x_{88}\right\rangle\langle+\rangle\left\langle x_{55}\right\rangle\langle\rightarrow\rangle\left\langle x_{30}\right\rangle
\end{aligned}
$$</p>
<p>Target Output \&amp; Evaluation Metric. Given a generated problem instance, the task is to compute the value of every node in the computation graph; these values are uniquely determined by steps (i)-(iv) above. We consider the model output to be correct only if all node values are computed correctly (i.e., the input graph is fully solved).</p>
<p>Out-of-Distribution Generalization. Our primary focus in this work is to investigate the ability of Transformer networks to learn general problem-solving procedures or algorithms that enable out-of-distribution (OOD) generalization. The complexity of each problem instance can be explicitly parameterized by graph size, enabling precise measurement of a model's ability to generalize to inputs more complex than those encountered during training. In particular, in this mathematical reasoning task, OOD generalization is evaluated by training Transformer models on problem instances with $N \leq 32$ nodes and testing them on instances of varying sizes, up to $N=128$ (a fourfold increase). Such generalization requires the ability to process larger inputs and adaptively scale computation time during testing, beyond what was encountered in the training regime.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. An illustration of an instance in modular arithmetic on computational graphs task. The goal is to compute the values of all nodes in the graph. For example, here $x_{23}=20+2=22$ and $x_{55}=6 \times 14=15$. Recall that we consider modular arithmetic with base $p=23$.</p>
<p>This synthetic task captures the core essence of mathematical reasoning benchmarks like GSM8K (Cobbe et al., 2021), which are pivotal for evaluating the reasoning capabilities of large language models. Similar to GSM8K, our task involves a combinatorial structure combined with arithmetic computations. However, a key simplification is that variable names</p>
<p>are directly tokenized, bypassing natural language representation. This focused design, while retaining the critical combinatorial structure and rule-based nature inherent in mathematical reasoning, facilitates a more straightforward and modular interpretation of the learned Transformer model's internal mechanisms, as will be shown in Section 5.</p>
<h1>3.2. Limitations of Standard Transformers with CoT Training</h1>
<p>To establish a baseline and motivate the need for alternative approaches, we evaluate standard Transformer architectures on our synthetic task using two primary training paradigms.</p>
<p>End-to-End Training. The first baseline is End-to-End training, where the Transformer models are trained to directly output the final values of all nodes given the problem input, without explicit intermediate steps. The input token sequences are in the form of (1), and we employ various Transformer models with diverse architectures. See Appendix A for details.</p>
<p>Chain-of-Thought (CoT) Training. The second baseline is based on autoregressive Chain-of-Thought (CoT) training (Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et al., 2024; Cobbe et al., 2021; Lewkowycz et al., 2022; Wei et al., 2022; Ye et al., 2024), a prevalent technique for enabling multi-step reasoning in LLMs. Instead of directly outputting the final answer, CoT trains a model to generate a sequence of intermediate reasoning steps (the "thought process") that culminates in the solution. For our task, CoT intermediate steps consist of explicit demonstrations of the step-by-step computation of nodes within a given computation graph. In particular, in CoT training, the Transformer model receives an input prompt consisting of the token representation of the computation graph (as in (1)), followed by a special $\langle$ CoT $\rangle$ token. This special token signals the beginning of the CoT reasoning, which outlines the computation of each node in topological order. Each step in the trajectory involves: (1) recalling the equation defining the node's value, (2) recalling the values of its dependent nodes, and (3) performing the arithmetic computation. For example, computing node $\left\langle x_{101}\right\rangle$ from Figure 2 would appear in the CoT as:</p>
<p>$$
[\ldots \text { Input Prompt } \ldots]\langle\text { CoT }\rangle[\ldots]\left\langle x_{101}\right\rangle=\left\langle x_{23}\right\rangle\langle\times\rangle\left\langle x_{91}\right\rangle=\langle 22\rangle\langle\times\rangle\langle 8\rangle=\langle 15\rangle
$$</p>
<p>Here, the [..Input Prompt...] gives the description of the problem instance, and [...] denotes the preceding portion of the chain-of-thought trajectory up to node $\left\langle x_{91}\right\rangle$, which in particular includes the computation of the values of $\left\langle x_{23}\right\rangle$ and $\left\langle x_{91}\right\rangle$. An example of a full CoT example from the training data is provided in Appendix A.2.</p>
<p>Implementation. We train causal Transformer models from scratch using both End-to-End and CoT supervision on randomly generated problem instances with graph sizes $N \leq 32$. At inference time, models are prompted with the input and generation is performed using greedy decoding. End-to-End models directly output all node values given the input, while CoT models autoregressively generate the solution, including the full CoT trajectory. We evaluate performance based on the proportion of instances where the model computes all node values correctly, with a particular focus on OOD generalization to new, randomly generated graphs of varying sizes up to $N=128$. For all methods, an extensive hyperparameter search was conducted (covering layers, model dimension, and positional encoding), and the best-performing configuration of each method was selected for comparison. A detailed experimental setup for these baseline experiments is provided in Appendix A.</p>
<p>Observed OOD Generalization Deficiencies. We find that Chain-of-Thought training enables models to solve larger graphs compared to those trained End-to-End without chain-of-thought supervision (Figure 4). While the best-performing CoT models exhibit a limited degree of OOD generalization to moderately larger graphs ( $N \leq 32 \sim N \approx 40$ ), this capability rapidly deteriorates as graph sizes exceed the training regime. In the next section, we propose a series of architectural mechanisms that address these generalization challenges.</p>
<h1>4. Reasoning in Latent Space with Algorithmic Supervision</h1>
<h3>4.1. Mechanisms for Effective OOD Generalization.</h3>
<p>Effective OOD generalization on complex reasoning tasks hinges on a model's ability to learn and emulate an underlying scalable algorithm. This requires the model to, implicitly or explicitly, execute an iterative procedure that adapts to input complexity. Designing inductive biases to support the discovery of such scalable, compositional solutions is a central challenge in machine learning (Barrett et al., 2018; Baxter, 2000; Goyal and Bengio, 2022; B. Lake and Baroni, 2018). Chain-of-thought (CoT) techniques attempt this by having the model sequentially generate a token representation of a computational process. However, this restriction to a token-based, autoregressive format often yields brittle "algorithms" that fail to generalize robustly, especially as longer CoT sequences are needed for more complex inputs. These well-documented length generalization issues (Anil et al., 2022; Jelassi et al., 2023; Kazemnejad et al., 2023; Stechly, Valmeekam, and Kambhampati, 2024; H. Zhou et al., 2024; Y. Zhou et al., 2024) underscore CoT's limitations in effectively emulating truly scalable algorithmic procedures. This work, therefore, proposes alternative mechanisms to facilitate the learning of such iterative algorithms directly within a model's latent processing.</p>
<p>Our proposal features four key architectural mechanisms: (i) recurrent Transformer blocks, (ii) algorithmic supervision, (iii) discretization in latent space, and (iv) a self-correction scheme. Collectively, these mechanisms constitute an architecture enabling native latent-space reasoning, leading to effective OOD generalization. Figure 1 illustrates the four mechanisms as individual components, while Figure 3 depicts the unified architectural proposal. In the following, we present the four proposed mechanisms and the essence of their implementation, deferring certain implementation details to Appendix B.</p>
<p>Algorithm to Emulate. To solve this task, a natural algorithmic solution that is well-aligned with the Transformer architecture is to compute the values in the computation graph one layer at a time. This can be realized through a recursive process that iteratively applies the same computational modules. Specifically, each iteration of the algorithm computes values one layer deeper in the computation graph by fetching the necessary dependent values for nodes at the current layer and then performing the required modular arithmetic. In particular, for the example in Figure 2, in the first iteration, we evaluate variables $\left{x_{7}, x_{42}, x_{88}, x_{115}\right}$. In the second iteration, we evaluate $\left{x_{23}, x_{91}, x_{55}\right}$. In the last iteration, we evaluate $\left{x_{101}, x_{30}\right}$. Note that each iteration involves the same type of computation, providing a succinct and scalable recursive problem-solving algorithm.</p>
<p>Mechanism 1: Recurrence \&amp; Input-Adaptive Computation. The iterative and recursive structure of the target layer-by-layer algorithm naturally motivates a recurrent architecture. We employ a recurrent Transformer block (Dehghani et al., 2019) with the goal that each application emulates one algorithmic iteration-that is, computing values for one additional layer of the computation graph. An input instance</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Latent Space Supervision
Figure 3. Overview of the proposed architecture for OOD generalization. It features a recurrent Transformer block, latent algorithmic supervision, and a discretization mechanism to anchor representations across iterations. Self-correction mechanism is not represented here.
is represented as a sequence of $n$ tokens $X=\left(x_{1}, \ldots, x_{n}\right)$, as described in (1). This is embedded to form a sequence of embedding vectors $E_{1}^{(0)}, \ldots, E_{n}^{(0)}$, and recurrently processed with the recurrent transformer block</p>
<p>$$
\left(E_{1}^{(t+1)}, \ldots, E_{n}^{(t+1)}\right) \leftarrow \operatorname{RecurrentTransformerBlock}\left(E_{1}^{(t)}, \ldots, E_{n}^{(t)}\right), t=1,2, \ldots, T
$$</p>
<p>The output is linearly read out from the final embedding states $E_{1}^{(T)}, \ldots, E_{n}^{(T)}$. Crucially, the number of recurrent iterations, $T$, is not fixed but adapts to input complexity, scaling linearly with the depth of the computation graph. This input-adaptive recurrence allows the model to dynamically scale its computation time proportionate to the problem's requirements, a key capability for OOD generalization to larger graphs. Unlike CoT methods that scale computation by generating progressively longer linear sequences of tokens, recurrence introduces inductive biases favoring recursive solution structures, which are inherently more scalable. This recurrent structure also provides key computational advantages compared to autoregressive chain-of-thought methods: in our recurrent architecture, each step can perform parallel processing across the entire context instead of being constrained to perform computation sequentially token-by-token, yielding more efficient use of working memory since the full computational trace is not serially materialized. The use of recurrence to adaptively scale computation time is a well-established concept for tackling tasks with variable complexity (Banino, Balaguer, and Blundell, 2021; Bansal et al., 2022; Dehghani et al., 2019; Fan et al., 2024; Geiping et al., 2025; Graves, 2017; Schwarzschild et al., 2021).</p>
<p>Mechanism 2: Latent State Algorithmic Supervision. While recurrence (Mechanism 1) provides the capacity for iterative computation, it does not inherently guarantee that the model will learn the desired layer-by-layer algorithmic procedure. To instill this structure, we introduce latent state algorithmic supervision. Unlike CoT, which supervises intermediate computation in token space, our mechanism provides supervision directly within the model's latent representation space at each recurrent step, steering the internal states to align with the step-by-step execution of our target algorithm. Specifically, at each recurrent iteration $t$, a shared linear readout layer is used to predict node values from their current latent embeddings $E_{i}^{(t)}$. The training loss applied to these predictions at each recurrent iteration is designed to align the model with the target layer-by-layer algorithm. In particular, for each iteration $t$, it penalizes errors in the predicted values</p>
<p>for nodes that are algorithmically computable within $t$ processing steps (i.e., of depth $t$ or less) as follows</p>
<p>$$
\text { AlgorithmAlignmentLoss }=\sum_{t=1}^{T} \sum_{i \in[n]} \mathbf{1}\left{\operatorname{Depth}\left(x_{i}\right) \leq t\right} \cdot \ell\left(W_{\text {value }} E_{i}^{(t)}, \operatorname{Value}\left(x_{i}\right)\right)
$$</p>
<p>where $\operatorname{Depth}\left(x_{i}\right)$ is the node's depth in the computation graph, Value $\left(x_{i}\right)$ is its ground-truth value, and $\ell$ is the cross-entropy loss. Thus, the algorithm alignment loss supervises the model such that at iteration $t$, it computes the values of all nodes in the input at computational depth less than or equal to $t$. For example, in Figure 2, supervision at $t=1$ applies to leaf nodes (e.g., $x_{7}$ ), while at $t=2$ it extends to include second-layer nodes (e.g., $x_{23}$ ), and so on. This iterative supervision encourages the model to progressively build up the solution, computing the graph one effective layer deeper with each recurrent step.</p>
<p>Mechanism 3: Anchoring Latent Representation via Discretization. Recurrent models can suffer from representational drift across recurrent iterations during extended out-of-distribution computation, arising from error accumulation when computation scales beyond the training regime. To mitigate this and ensure stable processing across many iterations, we introduce a discretization mechanism that anchors the model's latent representation while scaling computation through recurrence. Specifically, after each iteration, the model's continuous hidden states are projected into a structured, discrete symbolic space and then immediately re-embedded to form the input for the next recurrent step. This forces the intermediate representations at each iteration to begin and end in a shared structured space, thereby maintaining semantic stability even when computation extends beyond the training regime. Ultimately, this anchoring constrains the model to learn a depth-invariant computational process, which is key to generalizing to longer computational depths than seen during training.</p>
<p>We implement this anchoring using a structured tokenization and embedding scheme, enabling each token's internal state to evolve recurrently while remaining grounded in a shared discrete space. In our task of modular arithmetic on computational graphs, the discrete latent space is structured as a product of four factors: token syntax, variable identity, numerical value, and operation type. To illustrate the structure of the discrete space, consider the input sequence " $\langle 17\rangle\langle=\rangle\left\langle x_{42}\right\rangle[\operatorname{sep}]$ ". This sequence is tokenized into symbolic factors as follows:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">syntax</th>
<th style="text-align: center;">variable</th>
<th style="text-align: center;">operation</th>
<th style="text-align: center;">value</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\langle 17\rangle$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">value</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">$\langle=\rangle$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">$\langle=\rangle$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">$\left\langle x_{42}\right\rangle$</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">variable</td>
<td style="text-align: center;">$x_{42}$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">empty</td>
</tr>
<tr>
<td style="text-align: center;">[sep]</td>
<td style="text-align: center;">$\rightarrow$</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">[sep]</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
</tbody>
</table>
<p>Note that the value factor of variable tokens (e.g., $\left\langle x_{42}\right\rangle$ above) is empty at the input layer. As the model processes the input recurrently, it iteratively computes the values of different variables, updating the value factor of the discrete latent state. This yields a latent representation that is discrete, shared across steps, and scalable to extended computation. To map the discrete states to distributed embeddings, we train a separate embedding layer for each factor and combine the factor embeddings by summation.</p>
<p>At each iteration, we first apply the RecurrentTransformerBlock, as in Equation (2), forming the core computation of the recurrent step. The processed distributed representations are then discretized via argmax decoding across each symbolic factor, projecting the latent representation to a common structured space. We</p>
<p>then re-embed the discrete state to form the vectorized input for the next iteration.</p>
<p>$$
\begin{aligned}
&amp; \left(\tilde{E}<em n="n">{1}^{(t+1)}, \ldots, \tilde{E}</em>\right) \
&amp; z_{i, \text { factor }}^{(t+1)} \leftarrow \arg \max \left{W_{\text {factor }} \tilde{E}}^{(t+1)}\right) \leftarrow \operatorname{RecurrentTransformerBlock}\left(E_{1}^{(t)}, \ldots, E_{n}^{(t)<em _="{" _text="\text" factor="factor" i_="i,">{i}^{(t+1)}\right} \quad \text { factor } \in{\text { syntax, variable, operation, value }} \
&amp; E</em>} \
&amp; E_{i}^{(t+1)} \leftarrow E_{i, \text { syntax }}^{(t+1)}+E_{i, \text { variable }}^{(t+1)}+E_{i, \text { operation }}^{(t+1)}+E_{i, \text { value }}^{(t+1)} .
\end{aligned}
$$}}^{(t+1)} \leftarrow \operatorname{FactorEmbed}\left(z_{i, \text { factor }}^{(t+1)}\right) \quad \text { factor } \in{\text { syntax, variable, operation, value </p>
<p>Mechanism 4: Learning to Self-Correct. Finally, to enhance the robustness of the learned algorithm, especially as the number of computational steps increases and makes the process more susceptible to error propagation, we introduce a self-correction scheme. This mechanism aims to equip the model with the ability to recover from such intermediate mistakes. To facilitate this robustness, we train the model by intentionally introducing errors into its reasoning process. Specifically, at each recurrent iteration, with a small probability, we randomly corrupt a selection of the value components within the model's discrete latent states. This training regimen forces the model to learn to detect when a previously computed value is incorrect (due to our induced corruption or its own misstep) and then to correct this error in a subsequent computational step before proceeding with the task.</p>
<h1>4.2. Experimental Results \&amp; Discussion</h1>
<p>Combining these mechanisms yields an architecture capable of effectively generalizing far beyond the training distribution to much larger and more complex inputs. To evaluate the effects of the different mechanisms we propose, we study a collection of methods, each implementing a different subset of these mechanisms.</p>
<p>These methods are listed in Table 1. The Feedforward End-to-End method does not implement any of the proposed mechanisms. The Recurrent End-to-End method partially implements Mechanism 1 as it uses recurrence but lacks input-adaptive computation. The Chain-of-Thought method partially implements Mechanism 1 since the length of the chain-of-thought trajectory scales with the complexity of the problem. It also partially implements Mechanism 2 because the next-token prediction objective on the chain-of-thought sequences provides supervision on the intermediate steps, although this supervision is not directly applied to the latent states. The Continuous Latent Space Supervision method fully implements Mechanisms $1 \mathcal{E}$ 2. It is a recurrent model featuring input-adaptive computation and latent state algorithmic supervision. However, we omit the discretization mechanism (Mechanism 3), thereby maintaining continuous distributed latent states. The Discrete Latent Space Supervision method incorporates the discretization mechanism, implementing Mechanisms 1, 2, \&amp; 3. Finally, the Discrete Latent Space Supervision $\odot$ method further incorporates the error correction mechanisms, thus implementing all four mechanisms.</p>
<p>Enabling Robust Algorithmic OOD Generalization. Figure 4 depicts the OOD generalization performance of our methods, ablating across the ingredients described above, as well as the aforementioned Chain-ofThought and End-to-End baselines. As previously mentioned, we find that the End-to-End models (both recurrent and feedforward) fail to effectively learn the task (with respect to our stringent "fully solved" metric) beyond small graph sizes, even in-distribution. The recurrent models slightly outperform the feedforward models. Chain-of-Thought supervision enables a significant improvement, yielding nearperfect performance in-distribution $(N \leq 32)$, and a limited degree of out-of-distribution generalization. To</p>
<p>Table 1. Guide to Implementation of Proposed Mechanisms in Baselines. The leftmost column shows the method names of the different baselines and ablations we consider, matching the figure legends. (1) indicates that a method implements the given mechanism, $\bigcirc$ indicate that the mechanism is not implemented, and (1) indicate that it is partially implemented.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method / Mechanism</th>
<th style="text-align: center;">Mechanism 1</th>
<th style="text-align: center;">Mechanism 2</th>
<th style="text-align: center;">Mechanism 3</th>
<th style="text-align: center;">Mechanism 4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Feedforward End-to-End</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
</tr>
<tr>
<td style="text-align: center;">Recurrent End-to-End</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
</tr>
<tr>
<td style="text-align: center;">Chain-of-Thought</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
</tr>
<tr>
<td style="text-align: center;">Continuous Latent Space Supervision</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">$\bigcirc$</td>
<td style="text-align: center;">$\bigcirc$</td>
</tr>
<tr>
<td style="text-align: center;">Discrete Latent Space Supervision</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">$\bigcirc$</td>
</tr>
<tr>
<td style="text-align: center;">Discrete Latent Space Supervision $\bigcirc$</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">(1)</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Out-of-Distribution generalization performance of different methods on the mathematical reasoning task.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Effective out-of-distribution generalization via input-adaptive scaling of computation time. This depicts Discrete Latent Space Supervision $\bigcirc$
assess our proposed mechanisms for robust OOD generalization in Transformers, we evaluate three classes of models incorporating different subsets of those ingredients. We find that this enables a dramatic improvement in OOD generalization, with performance improving further as more ingredients are incorporated. When all proposed ingredients are incorporated, i.e., Discrete Latent Space Supervision $\bigcirc^{1}$, the model robustly achieves near-perfect performance across all OOD splits we examined.</p>
<p>Depth-Invariance for Scalable Reasoning. Generalizing to problem instances more complex than those seen during training requires some mechanism of scaling computation proportionately. The chain-ofthought solution to this challenge is to scale the length of the autoregressively generated CoT trace, carrying out computation through the sequential generation of tokens. While this can yield some success, it is inherently limited: computation is forced into a token-by-token format rather than the model's native latent representation space, constraining efficiency and robustness. In this work, we explore a different approach based on recurrence with input-adaptive recurrent depth, introducing inductive biases that enforce a depthinvariant structure in the learned solution. That is, the model learns a solution such that the computational</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>description at every step of the solution process is the same, making it possible to scale it to depths far larger than those seen during training. This notion parallels other architectural invariances studied in geometric deep learning - such as translation, rotation, or permutation equivariance - where networks preserve behavior under transformations aligned with the task structure (Michael M Bronstein et al., 2017, 2021; Gerken et al., 2023). Here, the recurrence imposes invariance under the network's own iterative action, yielding a scalable, recursive algorithm capable of solving much larger and more complex instances.</p>
<p>The Importance of Anchored Discrete Representations. In Figure 4, Continuous Latent Space Supervision denotes a recurrent model where the continuous latent states receive step-by-step algorithmic supervision, but the latent states are not discretized in between recurrent block iterations as they are in Discrete Latent Space Supervision. We see that, while this outperforms the Chain-of-Thought baseline, which is limited to linear reasoning paths, its out-of-distribution performance slowly degrades as we test on progressively larger inputs, which require increasing recurrent depth and computation time. We attribute this to accumulating noise in the continuous vector representations - a phenomenon exacerbated when scaling test-time compute for larger problem instances - which eventually causes representations to drift from the semantically meaningful manifold learned during training. In Discrete Latent Space Supervision, the model receives step-by-step algorithmic supervision as with its continuous counterpart, but now we additionally discretize the latent representation, then re-embed using a common embedder that is shared across recurrent iterations. This has the effect of "anchoring" the latent states to a common, semantically-consistent representation space, allowing the model to scale up computational depth without accumulating noise. We observe that this yields significantly improved OOD generalization.</p>
<p>Error-Correction Leads to Greater Robustness in Scaling. In Discrete Latent Space Supervision $\odot$, we introduce explicit supervision for error correction by randomly corrupting the model's latent space with some small probability during training. While the model may make occasional errors, it is able to correct them in the next recurrent iteration, thereby yielding near-perfect OOD generalization. Interestingly, we find that error correction requires more layers in the recurrent block in order to succeed. An intuitive explanation is that effective error correction requires greater computational depth per step: the model must first identify and correct errors from prior steps before executing the current step's computation.</p>
<p>Robust Test-time Scaling. On many tasks, the computation time required to solve a problem instance is proportional to its size or complexity. Consequently, solving problems larger than those encountered during training necessitates scaling computation time beyond the training regime. In our setting, where the model's reasoning process is latent, we achieve this by increasing the number of recurrent iterations. Figure 5 depicts the proportion of input instances solved as a function of the number of recurrent iterations. Increasing the number of iterations enables solving incrementally larger and harder problem instances. Our architectural mechanisms enable this robust scaling beyond the training regime.</p>
<p>Details, Extensions $\mathcal{E}$ Further Ablations. In the appendices, we provide further discussion and present additional experimental results. Here, we briefly highlight a few aspects of these extensions. Across all methods, we find that hyperparameter choice can be critical. In particular, we find that the choice of positional encoding and model depth is especially important. In the above results, we always report the best model within each method after a hyperparameter search, the details of which are provided in the appendix. Additionally, for the chain-of-thought baselines, we explore multiple schemes for the design of the reasoning chains and present the best results here.</p>
<p>Now that we have demonstrated the effectiveness of the proposed architectural mechanisms for robust OOD generalization, we next conduct a mechanistic interpretability analysis to probe the precise computational circuits learned by each component of our model.</p>
<h1>5. Mechanistic Interpretability</h1>
<p>In this section, we aim to answer the following questions via a detailed study of the model's inner workings:
(i) What algorithm does the trained model implement?
(ii) Why is the trained model able to generalize to OOD data?</p>
<p>To answer these questions, we first propose hypotheses on the functionality of each model block: first-layer attention, second-layer attention, and the final MLP. For each of these hypotheses, we conduct controlled experiments where we apply causal interventions to specific parts of the input and isolate the effect on model activations to identify the function of each component. Our methodology builds on prior work on causal interpretability in neural networks (Geiger, Lu, et al., 2021; Geiger, Wu, et al., 2024; Meng et al., 2022), but is tailored specifically to interpreting recurrent transformer models. We provide complete details of our experimental methodology in the appendix.</p>
<h2>Induction Head \&amp; Modular Addition Mechanism</h2>
<p>To understand the algorithm implemented by the trained model, we analyze in detail the recurrent Transformer model trained with our proposed Discrete Latent Space Supervision method on the mathematical reasoning task. The recurrent Transformer model is configured with two layers, 16 attention heads, and a hidden state dimension of 256. For more details on the model configuration, please refer to Appendix C. We summarize our mechanism analysis results in Figure 6, where we reveal an induction head mechanism operating within the two-layer attention block and a modular addition mechanism in the final feedforward layer. To better understand the model's behavior, let us take an example equation in the following format:</p>
<p>$$
\text { [sep] }\langle\text { var } 0\rangle\langle+\rangle\langle\text { var } 1\rangle\langle+\rangle\langle\text { var } 2\rangle\langle=\rangle\langle\text { rhs }\rangle .
$$</p>
<p>We can break down the model's computation into three main components at the Right-Hand Side (RHS) position:</p>
<ul>
<li>The first layer attention heads copy the "variable" factored embeddings of variables $\langle$ var0 $\rangle$, $\langle$ var1 $\rangle$, and $\langle$ var2 $\rangle$ to the RHS position, which let the model know the variable names at the RHS position.</li>
<li>The second layer attention heads use the copied variable names to retrieve the computed values of variables $\langle$ var0 $\rangle$, $\langle$ var1 $\rangle$, and $\langle$ var2 $\rangle$ from the previous equations through an induction-head mechanism.</li>
<li>The last feedforward layer computes the sum of the values of the variables on the LHS and outputs the result to the RHS position.</li>
</ul>
<p>First Layer Attention Performs Variable Copying. The attention heads in the first layer are grouped by the variable position they attend to, reflecting an attention pattern that is dependent on relative position,</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Illustration of the two-layer model performing the modular addition task. The colored squares represent attention heads, grouped by the variable positions they attend to. Black rectangles indicate the embedding components chosen by the value projection matrix. $\langle\cdot\rangle$ denotes tokens, and ' $\cdot$ ' denotes embedding components.
as illustrated in Figure 7 (left). For the token embeddings of $\langle$ var0 $\rangle,\langle$ var1 $\rangle$, and $\langle$ var2 $\rangle$, which comprise four separate factored embedding types (syntax, variable, operation, and value), the value and output projection matrices of each head group select a subspace of these token embeddings containing only the variable embeddings. This is evident in Figure 7 (right), which plots the norm amplification for different factored embedding types. More details on the norm amplification calculation can be found in Appendix C.</p>
<p>This shows that the first layer attention copies the variable names of its parents, which will later be used to obtain their values in the second layer.</p>
<p>Second Layer Attention Implements Variable-Dependent Induction Head Mechanism. The second layer's attention heads then retrieve the corresponding values of variables $\langle$ var0 $\rangle,\langle$ var1 $\rangle$, and $\langle$ var2 $\rangle$ from the previous equations through an induction-head mechanism (Olsson et al., 2022). Specifically, all the attention heads are also grouped by which variable value they are retrieving. For example, let us suppose that the first head group is responsible for retrieving the value of $\langle$ var0 $\rangle$. Then, the attention heads within this group will find the first occurrence of $\langle$ var0 $\rangle$, which will be the RHS of some previous equation. This particular position is the first time the value of $\langle$ var0 $\rangle$ is computed. And these attention heads will then copy the "value" factored embedding of $\langle$ var0 $\rangle$ also to the current RHS position. In summary, the variable names copied in the first layer are used as queries to retrieve these variables' values, searching over the RHS of previous equations.</p>
<p>Feedforward Layer Performs Modular Addition. The second layer MLP implements a sophisticated modular addition mechanism that computes the sum of the three variable values modulo 23. The MLP receives as input the sum of three transformed value embeddings from the attention layer - one for each variable position. These embeddings exhibit a periodic structure that naturally lends itself to frequency domain analysis.</p>
<p>Through systematic experimentation where we vary all three input values from 0 to 22 and apply threedimensional Discrete Fourier Transform (DFT) analysis, we observe a fascinating computational pattern. At the MLP's pre-activation stage, the representation is dominated by a bias term (the $(0,0,0)$ frequency</p>
<p>component). As signals propagate through the MLP layers, this bias progressively diminishes while diagonal frequency components of the form $(a, a, a)$ are amplified, where $a \in{1, \ldots, 22}$. That is, the Fourier components where $\langle$ var0 $\rangle,\langle$ var1 $\rangle$, and $\langle$ var2 $\rangle$ have the same frequency are amplified. These diagonal frequencies encode precisely the information needed for modular arithmetic: they represent sinusoidal functions of the sum $x+y+z$.</p>
<p>The MLP essentially performs the computation through combinations of terms like $\cos (2 \pi a(x+y+z) / 23)$, where the periodic nature of trigonometric functions naturally handles the modulo operation. This frequencybased approach aligns with recent findings on how neural networks implement modular arithmetic (Doshi et al., 2024; Nanda et al., 2023; Tian, 2024). We provide detailed experimental evidence and visualizations of this mechanism in Appendix C, including DFT analysis at multiple network positions showing the progressive amplification of sum-encoding frequencies.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Left. An illustration of the functionality of attention heads by groups in the first attention layer. Head 4 and 8 attend to the first variable position, Head 5 and 12 attend to the second variable position, Head $3,7,11,14$ attend to the third variable position, and the remaining heads attend to the RHS position or do not show a clear attention pattern. Right. Norm amplification of each factor's embeddings passed through the combined attention OV matrix by head groups. 〈others〉 exhibits significantly higher norm amplification, primarily because head 15 performs a self-copy operation at the RHS position.</p>
<p>OOD Generalization of the Trained Model. The model's robust OOD generalization can be traced back to the architectural mechanisms of Discrete Latent Space Supervision guiding the model towards learning a universal and robust algorithm. In particular, the algorithm implements a variable-dependent induction head mechanism that is invariant to length, leveraging both relative-positional and variable-dependent attention patterns, which enables the model to operate over contexts of arbitrary lengths. Thus, despite being trained on graphs with limited size, the input-adaptive recurrence, intermediate supervision, and discretization mechanisms enable the model to learn a scalable algorithm capable of solving problems of increased complexity.</p>
<h1>6. Conclusion</h1>
<p>This work investigated algorithmic generalization in Transformers for scalable mathematical reasoning, a domain where standard chain-of-thought approaches fail on out-of-distribution inputs. We introduced a novel architecture integrating input-adaptive recurrence, latent algorithmic supervision, state discretization, and self-correction mechanisms. Collectively, these mechanisms enabled our models to achieve near-perfect OOD performance by facilitating robust, scalable reasoning directly within their internal latent representations, overcoming the brittleness of sequential token-based methods. Mechanistic interpretability further</p>
<p>illuminated how these components achieve systematic generalization. While our synthetic mathematical reasoning task offers analytical clarity for investigating fundamental principles-such as adaptive recurrence and discrete latent bottlenecks-future work should explore extending these principles to more diverse, less-structured, and multi-task settings.</p>
<h1>References</h1>
<p>Agustsson, Eirikur, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, and Luc V Gool (2017). "Soft-to-hard vector quantization for end-to-end learning compressible representations". In: Advances in neural information processing systems (cited on page 5).
Ameisen, Emmanuel et al. (2025). "Circuit Tracing: Revealing Computational Graphs in Language Models". In: Transformer Circuits Thread (cited on page 6).
Anderson, John R (1982). "Acquisition of cognitive skill." In: Psychological review (cited on page 3).
Anil, Cem, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur (Dec. 6, 2022). "Exploring Length Generalization in Large Language Models". In: Advances in Neural Information Processing Systems (cited on pages 3, 4, 9).
Banino, Andrea, Jan Balaguer, and Charles Blundell (Sept. 2, 2021). "PonderNet: Learning to Ponder". arXiv: 2107.05407. Pre-published (cited on pages 5, 10).</p>
<p>Bansal, Arpit, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein (Oct. 14, 2022). "End-to-End Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking". arXiv: 2202.05826 [cs]. Pre-published (cited on pages 5, 10).
Barrett, David, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap (2018). "Measuring abstract reasoning in neural networks". In: International conference on machine learning. PMLR (cited on pages 4, 9).
Baxter, Jonathan (2000). "A model of inductive bias learning". In: Journal of artificial intelligence research (cited on pages $4,9)$.
Bricken, Trenton et al. (2023). "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning". In: Transformer Circuits Thread (cited on page 6).
Bronstein, Michael M, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst (2017). "Geometric deep learning: going beyond euclidean data". In: IEEE Signal Processing Magazine (cited on page 14).
Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Veličković (2021). "Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges". arXiv: 2104.13478 [cs.LG] (cited on page 14).
Cho, Kyunghyun, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio (Oct. 2014). "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation". In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Ed. by Alessandro Moschitti, Bo Pang, and Walter Daelemans. Doha, Qatar: Association for Computational Linguistics (cited on page 4).
Chomsky, Noam (1957). "Syntactic structures". Mouton de Gruyter (cited on page 3).
Chung, Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. (2024). "Scaling instruction-finetuned language models". In: Journal of Machine Learning Research (cited on pages 6, 8).
Chung, Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. (2022). "Scaling Instruction-Finetuned Language Models". arXiv: 2210.11416 [cs.LG] (cited on page 3).
Cobbe, Karl, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Łukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman (Nov. 2021). "Training Verifiers to Solve Math Word Problems". arXiv:2110.14168 [cs] (cited on pages 3, 7, 8).</p>
<p>Courville, Aaron, James Bergstra, and Yoshua Bengio (2011). "A spike and slab restricted Boltzmann machine". In: Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings (cited on page 5).
Dehghani, Mostafa, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser (Mar. 5, 2019). "Universal Transformers". arXiv: 1807.03819 [cs, stat]. Pre-published (cited on pages 4, 5, 9, 10).
Doshi, Darshil, Aritra Das, Tianyu He, and Andrey Gromov (2024). "To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets". In: Bulletin of the American Physical Society (cited on pages 17, 41).
Elhage, Nelson, Tristan Hume, et al. (2022). "Toy Models of Superposition". In: Transformer Circuits Thread (cited on page 6).
Elhage, Nelson, Neel Nanda, et al. (2021). "A Mathematical Framework for Transformer Circuits". In: Transformer Circuits Thread (cited on page 6).
Elman, Jeffrey L (1990). "Finding structure in time". In: Cognitive science (cited on page 4).
Fan, Ying, Yilun Du, Kannan Ramchandran, and Kangwook Lee (Sept. 25, 2024). "Looped Transformers for Length Generalization". arXiv: 2409.15647. Pre-published (cited on pages 4, 10).
Fodor, Jerry A and Zenon W Pylyshyn (1988). "Connectionism and cognitive architecture: A critical analysis". In: Cognition (cited on pages 3-5).
Garcez, Artur SD'Avila, Luis C Lamb, and Dov M Gabbay (2008). "Neural-symbolic cognitive reasoning". Springer Science \&amp; Business Media (cited on page 5).
Geiger, Atticus, Hanson Lu, Thomas F Icard, and Christopher Potts (2021). "Causal Abstractions of Neural Networks". In: Advances in Neural Information Processing Systems. Ed. by A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (cited on pages 6, 15).</p>
<p>Geiger, Atticus, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah Goodman (2024). "Finding alignments between interpretable causal variables and distributed neural representations". In: Causal Learning and Reasoning. PMLR (cited on pages 6, 15).
Geiping, Jonas, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein (Feb. 17, 2025). "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach". arXiv: 2502.05171 [cs]. Pre-published (cited on pages 5, 10).</p>
<p>Gerken, Jan E, Jimmy Aronsson, Oscar Carlsson, Hampus Linander, Fredrik Ohlsson, Christoffer Petersson, and Daniel Persson (2023). "Geometric deep learning and equivariant neural networks". In: Artificial Intelligence Review (cited on page 14).
Goyal, Anirudh and Yoshua Bengio (2022). "Inductive biases for deep learning of higher-level cognition". In: Proceedings of the Royal Society A (cited on pages 4, 9).
Graves, Alex (Feb. 21, 2017). "Adaptive Computation Time for Recurrent Neural Networks". arXiv: 1603. 08983 [cs]. Pre-published (cited on pages 5, 10).
He, Pengcheng, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen (2021). "DeBERTa: Decoding-enhanced BERT with Disentangled Attention". arXiv: 2006.03654 [cs.CL] (cited on page 24).
Hochreiter, Sepp and Jürgen Schmidhuber (1997). "Long short-term memory". In: Neural computation (cited on page 4).
Hupkes, Dieuwke, Verna Dankers, Mathijs Mul, and Elia Bruni (2020). "Compositionality decomposed: How do neural networks generalise?" In: Journal of Artificial Intelligence Research (cited on page 4).</p>
<p>Jelassi, Samy, Stéphane d'Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and François Charton (2023). "Length Generalization in Arithmetic Transformers". arXiv: 2306.15400 [cs.LG] (cited on pages 3, $4,9)$.
Jordan, Michael I (1997). "Serial order: A parallel distributed processing approach". In: Advances in psychology. Elsevier (cited on page 4).
Kazemnejad, Amirhossein, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy (Dec. 15, 2023). "The Impact of Positional Encoding on Length Generalization in Transformers". In: Advances in Neural Information Processing Systems (cited on pages 3, 4, 9, 24).
Kojima, Takeshi, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa (2022). "Large language models are zero-shot reasoners". In: Advances in neural information processing systems (cited on pages 3,5).
Lake, Brenden and Marco Baroni (2018). "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks". In: International conference on machine learning. PMLR (cited on pages 3,9 ).
Lake, Brenden M, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman (2017). "Building machines that learn and think like people". In: Behavioral and brain sciences (cited on pages 3, 4).
Lewkowycz, Aitor, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra (2022). "Solving Quantitative Reasoning Problems with Language Models". In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (cited on pages 3, 6, 8).
Lightman, Hunter, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe (2023). "Let's Verify Step by Step". arXiv: 2305.20050 [cs.LG] (cited on page 3).
Liu, Hanmeng, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, and Yue Zhang (Dec. 2023). "LogiCoT: Logical Chain-of-Thought Instruction Tuning". In: Findings of the Association for Computational Linguistics: EMNLP 2023. Ed. by Houda Bouamor, Juan Pino, and Kalika Bali. Singapore: Association for Computational Linguistics (cited on pages 3, 6).
Meng, Kevin, David Bau, Alex Andonian, and Yonatan Belinkov (2022). "Locating and editing factual associations in gpt". In: Advances in neural information processing systems (cited on pages 6, 15).
Nanda, Neel, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt (2023). "Progress measures for grokking via mechanistic interpretability". In: The Eleventh International Conference on Learning Representations (cited on pages 6, 17, 41).
Newel, Allen and Herbert A Simon (1976). "Computer science as empirical inquiry: Symbols and search". In: Communications of the ACM (cited on page 5).
Nye, Maxwell, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena (2021). "Show Your Work: Scratchpads for Intermediate Computation with Language Models". arXiv: 2112.00114 [cs.LG] (cited on page 5).</p>
<p>Olsson, Catherine et al. (2022). "In-context Learning and Induction Heads". In: Transformer Circuits Thread (cited on pages 6,16 ).
Oord, Aaron van den, Oriol Vinyals, and Koray Kavukcuoglu (May 30, 2018). "Neural Discrete Representation Learning". arXiv: 1711.00937 [cs]. Pre-published (cited on page 5).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Here, $\bigcirc$ denotes self-correction.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
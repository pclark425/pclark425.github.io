<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1839 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1839</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1839</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-1bf46fd55008c3fe2dd531c5cdb97dceafd6b217</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1bf46fd55008c3fe2dd531c5cdb97dceafd6b217" target="_blank">Semantic Exploration from Language Abstractions and Pretrained Representations</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work evaluates vision-language representations, pretrained on natural image captioning datasets, and shows that these pretrained representations drive meaningful, task-relevant exploration and improve performance on 3D simulated environments.</p>
                <p><strong>Paper Abstract:</strong> Effective exploration is a challenge in reinforcement learning (RL). Novelty-based exploration methods can suffer in high-dimensional state spaces, such as continuous partially-observable 3D environments. We address this challenge by defining novelty using semantically meaningful state abstractions, which can be found in learned representations shaped by natural language. In particular, we evaluate vision-language representations, pretrained on natural image captioning datasets. We show that these pretrained representations drive meaningful, task-relevant exploration and improve performance on 3D simulated environments. We also characterize why and how language provides useful abstractions for exploration by considering the impacts of using representations from a pretrained model, a language oracle, and several ablations. We demonstrate the benefits of our approach in two very different task domains -- one that stresses the identification and manipulation of everyday objects, and one that requires navigational exploration in an expansive world. Our results suggest that using language-shaped representations could improve exploration for various algorithms and agents in challenging environments.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1839.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1839.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lang-NGU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-augmented Never Give Up (Lang-NGU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An NGU episodic-novelty exploration agent that computes intrinsic rewards using frozen pretrained language encoders (BERT, CLIP_text, or ALM_text) applied to environment captions; the policy itself is trained from scratch on visual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Lang-NGU</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Variant of the Never Give Up (NGU) exploration agent where the episodic non-parametric memory stores embeddings produced by a frozen pretrained language encoder (text embedding of the environment caption O_L). The policy and any other auxiliary networks are trained from scratch and never receive the oracle caption at test time; pretrained encoder is only used to compute intrinsic rewards during training.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Natural language captions / image–text supervision (text encoders pretrained on language)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Uses frozen pretrained text encoders including BERT and text encoders from CLIP/ALM. The paper states these encoders were pretrained on image captioning / vision-language corpora (e.g. ALIGN for ALM); model parameter sizes reported (e.g. ALM text encoder ~77M parameters, CLIP text embedding size 512, BERT-based text encoders ~70–90M). No environment-specific finetuning was performed.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Playroom (lift, put, find) and City (explore)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>First-person 3D Unity environments: Playroom includes single-room lift/put object-manipulation tasks and multi-room find/search tasks involving 3–5 objects and randomized object appearances; City is a large-scale urban exploration environment where the agent is rewarded for coverage (unique map bins visited) over long episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A (text pretraining provides semantic representations, not actions); text pretraining involves natural language corpora/captions (no explicit action vocabulary used).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete embodied action spaces: Playroom uses 46 discrete actions (locomotion primitives plus object manipulation actions like hold and rotate); City uses a smaller navigation set (move_forward/backward/left/right, look_left/right, and combined move_forward_and_look_left/right).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>No direct mapping of textual actions to motor actions. Pretrained language embeddings are used only to compute an intrinsic novelty reward (distance in embedding space / k-NN kernel) and the policy learns to map visual observations O_V and goals to low-level actions from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB visual observations (O_V); during training some variants use oracle captions (O_L) to compute embeddings; no depth or additional sensors required.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>On Playroom tasks, Lang-NGU variants speed learning: sample-efficiency improvements reported as 50–70% faster on lift and put tasks and 18–38% faster on the find task (depending on pretraining model). In City, Lang-NGU (text embeddings of O_L) visits up to ~3x more area than Vis-NGU baseline (coverage numbers in Table S4: Lang-NGU with CLIP ~225 bins, with Small-ALM ~241 bins).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Vis-NGU baseline (controllable-state inverse dynamics features) is substantially slower: e.g., Vis-NGU coverage in City ~83 bins (Table S4); Vis-NGU learns object interaction behaviors (foveate/hold) later — typically >60k updates compared to ~40k with language-shaped representations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Empirical indicators: LSE-NGU and Lang-NGU agents typically begin frequent foveation/holding of objects by ~40k learning updates; task learning curves reach comparable success earlier by ~18–70% fewer samples depending on task and model. In City, coverage per episode increased (no single-step count given) and agents reached much larger coverage within same training budget.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Vis-NGU required ~60k updates to reach comparable object interaction frequency and required substantially more updates (relative measure: 18–70% slower to reach similar performance on Playroom tasks). City coverage for Vis-NGU was ~83 bins vs 225–241 for Lang-NGU under same training budget.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Reported relative gains: 50–70% reduction in sample complexity on lift/put, 18–38% on find; City coverage increased up to 3x (interpretable as much more efficient exploration per episode under the same training budget).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Language encoders provide semantically-structured, abstract state coarsening that groups semantically equivalent views; pretrained text encoders reflect human-relevant abstractions and omit visual nuisances, producing compact and sufficient embeddings for novelty estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limitations include potential mismatch between pretraining domain and target environment (non-photorealistic Unity visuals), reduced effectiveness on dense multi-object scenes for current VL models, and inference cost concerns for large pretrained models leading to sparse embedding computation (every 8 timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using frozen pretrained text encoders to compute intrinsic novelty for NGU yields large improvements in sample efficiency and exploration coverage: language-shaped embeddings coarsen the state space semantically, improving novelty signals and accelerating learning in manipulation, search, and long-horizon navigation tasks without exposing the policy to language at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration from Language Abstractions and Pretrained Representations', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1839.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1839.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSE-NGU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-Supervised Embedding NGU (LSE-NGU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>NGU variant that uses frozen pretrained image encoders (trained with language supervision) to embed visual observations directly; these image embeddings (LSEs) are used for episodic novelty in NGU to drive exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LSE-NGU</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>NGU agent whose non-parametric episodic memory stores frozen pretrained image embeddings (from CLIP_image or ALM_image) that were trained with language supervision; the embeddings act as language-shaped visual representations for computing intrinsic novelty, while the policy is trained from scratch using raw visual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Vision–language pretraining (image encoders trained with image–text/caption supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Uses image encoders from CLIP and ALM that were trained on large-scale image-caption datasets (paper refers generally to captioning datasets and ALIGN for ALM). Reported encoder sizes: Small-ALM image encoder (26M ResNet-50), Med-ALM (71M NFNet). No environment-specific finetuning performed.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Playroom (lift, put, find) and City (explore)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same Unity 3D embodied tasks as Lang-NGU: Playroom object manipulation and search across rooms; City large-scale urban exploration with coverage objective.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A (pretraining is vision-language; image encoders do not provide action tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete actions: Playroom 46 discrete locomotion/manipulation actions; City limited discrete navigation/look actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>No explicit semantic-to-action mapping: pretrained image embeddings produce a language-supervised feature space used to compute novelty (k-NN kernel); agent learns desired action policies from scratch mapping visual inputs to discrete actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB visual inputs; pretrained image encoder produces semantic embeddings from images during training (computed periodically to limit inference cost).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>LSE-NGU agents improved sample efficiency on Playroom: 50–70% faster on lift and put tasks, and 18–38% faster on find (depending on pretraining model). In City, LSE-NGU achieved ~2x coverage compared to baseline (Table S4 shows LSE-NGU with CLIP ~153 bins, with Small-ALM ~162 bins vs Vis-NGU ~83 bins).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Vis-NGU baseline slower (see previous entry): lower coverage in City (~83 bins) and slower to learn object interaction (≥60k updates). ImageNet pretrained control (NFNet) provided smaller or negative effects on some tasks (hurt find task performance).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Agents achieved earlier object interaction (~40k updates to reliably foveate/hold) and converged faster on downstream tasks (50–70% fewer samples for some tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Vis-NGU typically required ~60k updates or more to reach similar object interaction frequencies; learning curves indicate up to ~2x slower exploration in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Approximately 50–70% reduction in samples to reach comparable performance on lift/put; 18–38% on find; ~2x improvement in City coverage relative to Vis-NGU under same training budget.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Image encoders trained with language supervision inherit semantic groupings from captions (object- and scene-level descriptors) enabling compact, task-relevant feature spaces that improve novelty estimation; works without needing an environment language oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Pretraining distribution mismatch (caption datasets vs synthetic environment visuals), limited multi-object scene understanding in current VL models, and computational/inference overhead for large encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Frozen image encoders trained with language supervision (LSEs) provide strong improvement in exploration when used to compute novelty for NGU: they accelerate learning across manipulation, search and exploration tasks and can be applied even when the environment does not supply captions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration from Language Abstractions and Pretrained Representations', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1839.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1839.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALM-ND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALM Network Distillation (ALM-ND)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RND-inspired method where the target function is a frozen pretrained ALM image/text encoder and the trainable network learns to reproduce those pretrained representations; prediction error serves as intrinsic reward for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>ALM-ND (Text/Image)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Network Distillation (ND) variant in which the RND target is not a random network but a frozen pretrained ALM encoder (either text- or image-based); a smaller trainable network is trained to predict the pretrained embeddings from observations and the squared prediction error is used as intrinsic reward.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Vision–language pretraining (ALIGN dataset / noisy image–text pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Uses ALM (ALIGN-trained) encoders: Small-ALM (26M ResNet image encoder + BERT-based text encoder) and Med-ALM (71M NFNet image encoder + 77M BERT text encoder). ALM was trained with contrastive loss on the ALIGN dataset (reference provided in paper); models used frozen weights with no environment-specific finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Playroom (lift, put, find)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Unity-based Playroom tasks testing object manipulation and search in both single-room (lift/put) and multi-room (find) settings.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Playroom 46 discrete actions including locomotion and manipulation primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>No explicit mapping; trainable network learns to predict ALM embeddings from visual inputs, and the resulting prediction error provides intrinsic reward; policy maps visual inputs to actions from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB inputs required; the trainable network and frozen ALM provide embedding-level supervision for novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>ND agents (ALM-ND) are significantly faster than Vis-RND baselines—reported as learning 41% faster on the find task (numbers in main text and Figures). ALM-ND (text/image) variants improved sample efficiency across Playroom tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Vis-RND baseline (random target network) learned more slowly; for find task ALM-ND showed 41% faster learning compared to Vis-RND.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Reported relative gain: ~41% faster on find task; exact absolute update counts not specified beyond comparative percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Vis-RND baseline required about ~1.7x the training seen by ALM-ND on find (implied from 41% faster claim), exact update counts not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Approximately 41% reduction in time-to-learn on the find task compared to Vis-RND baseline; other tasks show consistent speedups.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Pretrained ALM encodes human-relevant concepts and multi-object scene semantics that make prediction-error novelty more meaningful; distillation uses those semantics to focus exploration on useful, semantically distinct states.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Distillation can be harder if target networks are very large; requires balancing trainable network capacity and inference/memory considerations. Pretraining–environment domain mismatch can limit transfer if visual domain diverges strongly from pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using pretrained vision-language encoders as RND target functions (ND) yields substantial exploration and sample-efficiency gains: distilling semantic representations into a trainable predictor produces a more informative intrinsic reward than random targets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration from Language Abstractions and Pretrained Representations', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1839.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1839.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision–language model trained with contrastive image–text supervision whose image and text encoders produce aligned embeddings; used frozen in this paper to provide text or image embeddings for intrinsic-reward computation in exploration agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>CLIP (image and text encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pretrained contrastive vision–language model providing image and text embedding functions (image encoder outputs size 512 in this paper) that map images and captions into a shared embedding space; used frozen to compute novelty for NGU (CLIP_text for Lang-NGU; CLIP_image for LSE-NGU) or as ND target.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale image–text (caption) supervision / natural language captions</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Referred to in paper as pretrained on natural image captioning datasets (CLIP image encoder dimension reported as 512). The original CLIP work trained on large-scale web image–text pairs; this paper uses frozen CLIP encoders without environment-specific finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Playroom (lift, put, find) and City (explore) when used as an embedding source for Lang-NGU / LSE-NGU</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same Unity 3D environments used across experiments; CLIP embeddings are used only to compute intrinsic novelty signals during training.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete locomotion/manipulation actions (Playroom) and navigation/look actions (City).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>No explicit action mapping; CLIP embeddings provide semantic state representations used to compute novelty (k-NN in NGU or distillation targets in ND); the agent learns action mapping from raw visual inputs independently.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images (image encoder), and in Lang-NGU CLIP_text consumes oracle captions during training (text input), but policy acts only on RGB at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>When used as LSE or Lang embeddings, CLIP-based agents produced notable improvements: e.g., in City Lang-NGU with CLIP ~225 bins coverage (vs 83 baseline), and in Playroom substantial sample-efficiency gains (percentages depend on task).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baselines using learned controllable states or ImageNet-only encoders performed worse in several tasks (e.g., Vis-NGU coverage ~83 bins; ImageNet embeddings ~111 bins per Table S4).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Relative improvements reported (e.g., 50–70% speedups on some Playroom tasks; 2–3x improvements in City coverage depending on configuration).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Vis-NGU and Vis-RND required significantly more updates to reach similar behaviors (see other entries for specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Substantial relative gains reported; e.g., up to ~3x area visited in City and 50–70% fewer samples to learn manipulation tasks in Playroom compared to vis-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>CLIP's alignment of images and natural language yields representations that are robust to viewpoint/color variations and emphasize semantic content, enabling better novelty judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Pretraining domain mismatch and limited multi-object scene understanding can limit benefits; inference cost for large encoders requires sparse computation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CLIP image/text encoders, when used frozen to form language-shaped embeddings, materially improve novelty-driven exploration in 3D embodied tasks by providing semantically-meaningful state abstractions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration from Language Abstractions and Pretrained Representations', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1839.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1839.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALM (ALIGN models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALM (Models trained with ALIGN noisy text supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vision–language models trained with contrastive loss on the ALIGN dataset; used here as frozen image/text encoders (Small-ALM and Med-ALM) to provide language-shaped embeddings for intrinsic rewards in NGU/ND.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling up visual and vision-language representation learning with noisy text supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>ALM (Small-ALM, Med-ALM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pretrained vision–language encoders trained contrastively on ALIGN; Small-ALM uses a 26M ResNet-50 image encoder and ~44M BERT text encoder, while Med-ALM uses a 71M NFNet image encoder and ~77M BERT text encoder. Used frozen for Lang-NGU, LSE-NGU and as ND targets.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale noisy image–text pairs (ALIGN dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>ALM models were trained with contrastive loss on the ALIGN dataset (reference provided); the paper reports internal model sizes for Small and Med variants and uses them without finetuning in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Playroom (lift, put, find) and City (explore) as embedding sources</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same Unity 3D Playroom and City tasks; ALM embeddings are used to compute intrinsic novelty or serve as distillation targets.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete manipulation and navigation actions as per environment.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>No direct mapping; ALM produces embeddings (image/text) used for novelty/prediction-error intrinsic rewards; policy maps visual inputs to actions independently.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB visual observations; optionally oracle captions for text-embedding variants during training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Small-ALM and Med-ALM based agents show improved sample efficiency and coverage: e.g., Lang-NGU with Small-ALM achieved ~241 bins in City (Table S4) and strong speedups on Playroom tasks (50–70% for lift/put, 18–38% for find depending on model).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baselines without ALM (Vis-NGU, Vis-RND) performed worse (e.g., Vis-NGU coverage ~83 bins); ImageNet-only encoder gave weaker/noisy benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Reported relative gains: similar ranges as CLIP cases (50–70% speedups on some tasks; earlier object interaction at ~40k updates).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>See Vis-NGU / Vis-RND baselines (slower learning and lower coverage under same budgets).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Approximately 50–70% reduction in samples for some manipulation tasks and up to ~3x increase in exploration coverage in City in best configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>ALIGN pretraining provides large-scale noisy image–text supervision that produces robust, language-shaped embeddings capturing object and scene semantics relevant to exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Large pretrained targets can be harder to distill; pretraining–environment domain mismatch and limitations in multi-object scene understanding reduce gains on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ALM-derived embeddings are effective for driving intrinsic-reward-based exploration in 3D embodied tasks, producing substantial sample-efficiency and coverage improvements when used as frozen targets or embedding sources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration from Language Abstractions and Pretrained Representations', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1839.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1839.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (text encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (Bidirectional Encoder Representations from Transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General-purpose pretrained Transformer text encoder used here to embed oracle captions; BERT embeddings were used as frozen representations in Lang-NGU to compute intrinsic novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bert: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>BERT (frozen text encoder within Lang-NGU)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Standard BERT-based text encoder (reported sizes ~70–90M parameters in this paper's variants) used frozen to embed environment captions O_L for computing intrinsic novelty in Lang-NGU.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale text corpora (standard BERT pretraining on language corpora cited in BERT literature)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Paper uses BERT-based text backbones (70–90M parameters). The authors do not finetune on environment captions and rely on the general language knowledge encoded by BERT; specific pretraining corpora are those used in the BERT literature (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Playroom (lift, put, find) and City (explore) when used as Lang-NGU embedding source</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Unity 3D embodied tasks where BERT embeddings of oracle captions are used to compute intrinsic novelty signals during training.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete action sets (Playroom: 46 actions; City: navigation/look actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>No direct mapping; BERT embeddings are used only to compute novelty rewards (k-NN distances) while the policy maps visual inputs to low-level actions learned from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB visual observation O_V; oracle caption O_L during training to be embedded by BERT.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>BERT-based Lang-NGU variants produced improved sample efficiency (figures show comparable gains to other text encoders; exact per-model percentages vary). For some Playroom tasks BERT Lang-NGU achieved similar acceleration compared to CLIP/ALM text embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Vis-NGU baseline slower and less exploratory (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Part of the reported 50–70% speedups on lift/put tasks and 18–38% on find tasks when language-shaped embeddings are used; exact per-BERT numbers reported in figures/tables within the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Baselines required more updates (see Vis-NGU ~60k updates to reach object interaction levels that Lang/BERT-enabled agents reached by ~40k updates).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Comparable percentage reductions in sample complexity as other language-shaped encoders (task-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>BERT encodes abstract linguistic categories that coarsen and semantically structure state representations, improving novelty signals for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>BERT is a pure text encoder and relies on the quality and fidelity of the oracle captions; if captions are low-fidelity or misaligned with visual scenes, benefits may be limited.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using pretrained text encoders like BERT to embed state captions and compute intrinsic novelty yields meaningful exploration improvements, demonstrating that general language pretraining can provide useful abstractions for embodied RL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration from Language Abstractions and Pretrained Representations', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Scaling up visual and vision-language representation learning with noisy text supervision <em>(Rating: 2)</em></li>
                <li>Bert: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>Exploration by random network distillation <em>(Rating: 1)</em></li>
                <li>Never give up: Learning directed exploration strategies <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1839",
    "paper_id": "paper-1bf46fd55008c3fe2dd531c5cdb97dceafd6b217",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "Lang-NGU",
            "name_full": "Language-augmented Never Give Up (Lang-NGU)",
            "brief_description": "An NGU episodic-novelty exploration agent that computes intrinsic rewards using frozen pretrained language encoders (BERT, CLIP_text, or ALM_text) applied to environment captions; the policy itself is trained from scratch on visual inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "Lang-NGU",
            "model_agent_description": "Variant of the Never Give Up (NGU) exploration agent where the episodic non-parametric memory stores embeddings produced by a frozen pretrained language encoder (text embedding of the environment caption O_L). The policy and any other auxiliary networks are trained from scratch and never receive the oracle caption at test time; pretrained encoder is only used to compute intrinsic rewards during training.",
            "pretraining_data_type": "Natural language captions / image–text supervision (text encoders pretrained on language)",
            "pretraining_data_details": "Uses frozen pretrained text encoders including BERT and text encoders from CLIP/ALM. The paper states these encoders were pretrained on image captioning / vision-language corpora (e.g. ALIGN for ALM); model parameter sizes reported (e.g. ALM text encoder ~77M parameters, CLIP text embedding size 512, BERT-based text encoders ~70–90M). No environment-specific finetuning was performed.",
            "embodied_task_name": "Playroom (lift, put, find) and City (explore)",
            "embodied_task_description": "First-person 3D Unity environments: Playroom includes single-room lift/put object-manipulation tasks and multi-room find/search tasks involving 3–5 objects and randomized object appearances; City is a large-scale urban exploration environment where the agent is rewarded for coverage (unique map bins visited) over long episodes.",
            "action_space_text": "N/A (text pretraining provides semantic representations, not actions); text pretraining involves natural language corpora/captions (no explicit action vocabulary used).",
            "action_space_embodied": "Discrete embodied action spaces: Playroom uses 46 discrete actions (locomotion primitives plus object manipulation actions like hold and rotate); City uses a smaller navigation set (move_forward/backward/left/right, look_left/right, and combined move_forward_and_look_left/right).",
            "action_mapping_method": "No direct mapping of textual actions to motor actions. Pretrained language embeddings are used only to compute an intrinsic novelty reward (distance in embedding space / k-NN kernel) and the policy learns to map visual observations O_V and goals to low-level actions from scratch.",
            "perception_requirements": "RGB visual observations (O_V); during training some variants use oracle captions (O_L) to compute embeddings; no depth or additional sensors required.",
            "transfer_successful": true,
            "performance_with_pretraining": "On Playroom tasks, Lang-NGU variants speed learning: sample-efficiency improvements reported as 50–70% faster on lift and put tasks and 18–38% faster on the find task (depending on pretraining model). In City, Lang-NGU (text embeddings of O_L) visits up to ~3x more area than Vis-NGU baseline (coverage numbers in Table S4: Lang-NGU with CLIP ~225 bins, with Small-ALM ~241 bins).",
            "performance_without_pretraining": "Vis-NGU baseline (controllable-state inverse dynamics features) is substantially slower: e.g., Vis-NGU coverage in City ~83 bins (Table S4); Vis-NGU learns object interaction behaviors (foveate/hold) later — typically &gt;60k updates compared to ~40k with language-shaped representations.",
            "sample_complexity_with_pretraining": "Empirical indicators: LSE-NGU and Lang-NGU agents typically begin frequent foveation/holding of objects by ~40k learning updates; task learning curves reach comparable success earlier by ~18–70% fewer samples depending on task and model. In City, coverage per episode increased (no single-step count given) and agents reached much larger coverage within same training budget.",
            "sample_complexity_without_pretraining": "Vis-NGU required ~60k updates to reach comparable object interaction frequency and required substantially more updates (relative measure: 18–70% slower to reach similar performance on Playroom tasks). City coverage for Vis-NGU was ~83 bins vs 225–241 for Lang-NGU under same training budget.",
            "sample_complexity_gain": "Reported relative gains: 50–70% reduction in sample complexity on lift/put, 18–38% on find; City coverage increased up to 3x (interpretable as much more efficient exploration per episode under the same training budget).",
            "transfer_success_factors": "Language encoders provide semantically-structured, abstract state coarsening that groups semantically equivalent views; pretrained text encoders reflect human-relevant abstractions and omit visual nuisances, producing compact and sufficient embeddings for novelty estimation.",
            "transfer_failure_factors": "Limitations include potential mismatch between pretraining domain and target environment (non-photorealistic Unity visuals), reduced effectiveness on dense multi-object scenes for current VL models, and inference cost concerns for large pretrained models leading to sparse embedding computation (every 8 timesteps).",
            "key_findings": "Using frozen pretrained text encoders to compute intrinsic novelty for NGU yields large improvements in sample efficiency and exploration coverage: language-shaped embeddings coarsen the state space semantically, improving novelty signals and accelerating learning in manipulation, search, and long-horizon navigation tasks without exposing the policy to language at test time.",
            "uuid": "e1839.0",
            "source_info": {
                "paper_title": "Semantic Exploration from Language Abstractions and Pretrained Representations",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "LSE-NGU",
            "name_full": "Language-Supervised Embedding NGU (LSE-NGU)",
            "brief_description": "NGU variant that uses frozen pretrained image encoders (trained with language supervision) to embed visual observations directly; these image embeddings (LSEs) are used for episodic novelty in NGU to drive exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "LSE-NGU",
            "model_agent_description": "NGU agent whose non-parametric episodic memory stores frozen pretrained image embeddings (from CLIP_image or ALM_image) that were trained with language supervision; the embeddings act as language-shaped visual representations for computing intrinsic novelty, while the policy is trained from scratch using raw visual inputs.",
            "pretraining_data_type": "Vision–language pretraining (image encoders trained with image–text/caption supervision)",
            "pretraining_data_details": "Uses image encoders from CLIP and ALM that were trained on large-scale image-caption datasets (paper refers generally to captioning datasets and ALIGN for ALM). Reported encoder sizes: Small-ALM image encoder (26M ResNet-50), Med-ALM (71M NFNet). No environment-specific finetuning performed.",
            "embodied_task_name": "Playroom (lift, put, find) and City (explore)",
            "embodied_task_description": "Same Unity 3D embodied tasks as Lang-NGU: Playroom object manipulation and search across rooms; City large-scale urban exploration with coverage objective.",
            "action_space_text": "N/A (pretraining is vision-language; image encoders do not provide action tokens)",
            "action_space_embodied": "Discrete actions: Playroom 46 discrete locomotion/manipulation actions; City limited discrete navigation/look actions.",
            "action_mapping_method": "No explicit semantic-to-action mapping: pretrained image embeddings produce a language-supervised feature space used to compute novelty (k-NN kernel); agent learns desired action policies from scratch mapping visual inputs to discrete actions.",
            "perception_requirements": "RGB visual inputs; pretrained image encoder produces semantic embeddings from images during training (computed periodically to limit inference cost).",
            "transfer_successful": true,
            "performance_with_pretraining": "LSE-NGU agents improved sample efficiency on Playroom: 50–70% faster on lift and put tasks, and 18–38% faster on find (depending on pretraining model). In City, LSE-NGU achieved ~2x coverage compared to baseline (Table S4 shows LSE-NGU with CLIP ~153 bins, with Small-ALM ~162 bins vs Vis-NGU ~83 bins).",
            "performance_without_pretraining": "Vis-NGU baseline slower (see previous entry): lower coverage in City (~83 bins) and slower to learn object interaction (≥60k updates). ImageNet pretrained control (NFNet) provided smaller or negative effects on some tasks (hurt find task performance).",
            "sample_complexity_with_pretraining": "Agents achieved earlier object interaction (~40k updates to reliably foveate/hold) and converged faster on downstream tasks (50–70% fewer samples for some tasks).",
            "sample_complexity_without_pretraining": "Vis-NGU typically required ~60k updates or more to reach similar object interaction frequencies; learning curves indicate up to ~2x slower exploration in some settings.",
            "sample_complexity_gain": "Approximately 50–70% reduction in samples to reach comparable performance on lift/put; 18–38% on find; ~2x improvement in City coverage relative to Vis-NGU under same training budget.",
            "transfer_success_factors": "Image encoders trained with language supervision inherit semantic groupings from captions (object- and scene-level descriptors) enabling compact, task-relevant feature spaces that improve novelty estimation; works without needing an environment language oracle.",
            "transfer_failure_factors": "Pretraining distribution mismatch (caption datasets vs synthetic environment visuals), limited multi-object scene understanding in current VL models, and computational/inference overhead for large encoders.",
            "key_findings": "Frozen image encoders trained with language supervision (LSEs) provide strong improvement in exploration when used to compute novelty for NGU: they accelerate learning across manipulation, search and exploration tasks and can be applied even when the environment does not supply captions.",
            "uuid": "e1839.1",
            "source_info": {
                "paper_title": "Semantic Exploration from Language Abstractions and Pretrained Representations",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "ALM-ND",
            "name_full": "ALM Network Distillation (ALM-ND)",
            "brief_description": "An RND-inspired method where the target function is a frozen pretrained ALM image/text encoder and the trainable network learns to reproduce those pretrained representations; prediction error serves as intrinsic reward for exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "ALM-ND (Text/Image)",
            "model_agent_description": "Network Distillation (ND) variant in which the RND target is not a random network but a frozen pretrained ALM encoder (either text- or image-based); a smaller trainable network is trained to predict the pretrained embeddings from observations and the squared prediction error is used as intrinsic reward.",
            "pretraining_data_type": "Vision–language pretraining (ALIGN dataset / noisy image–text pairs)",
            "pretraining_data_details": "Uses ALM (ALIGN-trained) encoders: Small-ALM (26M ResNet image encoder + BERT-based text encoder) and Med-ALM (71M NFNet image encoder + 77M BERT text encoder). ALM was trained with contrastive loss on the ALIGN dataset (reference provided in paper); models used frozen weights with no environment-specific finetuning.",
            "embodied_task_name": "Playroom (lift, put, find)",
            "embodied_task_description": "Unity-based Playroom tasks testing object manipulation and search in both single-room (lift/put) and multi-room (find) settings.",
            "action_space_text": "N/A",
            "action_space_embodied": "Playroom 46 discrete actions including locomotion and manipulation primitives.",
            "action_mapping_method": "No explicit mapping; trainable network learns to predict ALM embeddings from visual inputs, and the resulting prediction error provides intrinsic reward; policy maps visual inputs to actions from scratch.",
            "perception_requirements": "RGB inputs required; the trainable network and frozen ALM provide embedding-level supervision for novelty.",
            "transfer_successful": true,
            "performance_with_pretraining": "ND agents (ALM-ND) are significantly faster than Vis-RND baselines—reported as learning 41% faster on the find task (numbers in main text and Figures). ALM-ND (text/image) variants improved sample efficiency across Playroom tasks.",
            "performance_without_pretraining": "Vis-RND baseline (random target network) learned more slowly; for find task ALM-ND showed 41% faster learning compared to Vis-RND.",
            "sample_complexity_with_pretraining": "Reported relative gain: ~41% faster on find task; exact absolute update counts not specified beyond comparative percentages.",
            "sample_complexity_without_pretraining": "Vis-RND baseline required about ~1.7x the training seen by ALM-ND on find (implied from 41% faster claim), exact update counts not provided.",
            "sample_complexity_gain": "Approximately 41% reduction in time-to-learn on the find task compared to Vis-RND baseline; other tasks show consistent speedups.",
            "transfer_success_factors": "Pretrained ALM encodes human-relevant concepts and multi-object scene semantics that make prediction-error novelty more meaningful; distillation uses those semantics to focus exploration on useful, semantically distinct states.",
            "transfer_failure_factors": "Distillation can be harder if target networks are very large; requires balancing trainable network capacity and inference/memory considerations. Pretraining–environment domain mismatch can limit transfer if visual domain diverges strongly from pretraining data.",
            "key_findings": "Using pretrained vision-language encoders as RND target functions (ND) yields substantial exploration and sample-efficiency gains: distilling semantic representations into a trainable predictor produces a more informative intrinsic reward than random targets.",
            "uuid": "e1839.2",
            "source_info": {
                "paper_title": "Semantic Exploration from Language Abstractions and Pretrained Representations",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pretraining)",
            "brief_description": "A vision–language model trained with contrastive image–text supervision whose image and text encoders produce aligned embeddings; used frozen in this paper to provide text or image embeddings for intrinsic-reward computation in exploration agents.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_agent_name": "CLIP (image and text encoders)",
            "model_agent_description": "Pretrained contrastive vision–language model providing image and text embedding functions (image encoder outputs size 512 in this paper) that map images and captions into a shared embedding space; used frozen to compute novelty for NGU (CLIP_text for Lang-NGU; CLIP_image for LSE-NGU) or as ND target.",
            "pretraining_data_type": "Large-scale image–text (caption) supervision / natural language captions",
            "pretraining_data_details": "Referred to in paper as pretrained on natural image captioning datasets (CLIP image encoder dimension reported as 512). The original CLIP work trained on large-scale web image–text pairs; this paper uses frozen CLIP encoders without environment-specific finetuning.",
            "embodied_task_name": "Playroom (lift, put, find) and City (explore) when used as an embedding source for Lang-NGU / LSE-NGU",
            "embodied_task_description": "Same Unity 3D environments used across experiments; CLIP embeddings are used only to compute intrinsic novelty signals during training.",
            "action_space_text": "N/A",
            "action_space_embodied": "Discrete locomotion/manipulation actions (Playroom) and navigation/look actions (City).",
            "action_mapping_method": "No explicit action mapping; CLIP embeddings provide semantic state representations used to compute novelty (k-NN in NGU or distillation targets in ND); the agent learns action mapping from raw visual inputs independently.",
            "perception_requirements": "RGB images (image encoder), and in Lang-NGU CLIP_text consumes oracle captions during training (text input), but policy acts only on RGB at test time.",
            "transfer_successful": true,
            "performance_with_pretraining": "When used as LSE or Lang embeddings, CLIP-based agents produced notable improvements: e.g., in City Lang-NGU with CLIP ~225 bins coverage (vs 83 baseline), and in Playroom substantial sample-efficiency gains (percentages depend on task).",
            "performance_without_pretraining": "Baselines using learned controllable states or ImageNet-only encoders performed worse in several tasks (e.g., Vis-NGU coverage ~83 bins; ImageNet embeddings ~111 bins per Table S4).",
            "sample_complexity_with_pretraining": "Relative improvements reported (e.g., 50–70% speedups on some Playroom tasks; 2–3x improvements in City coverage depending on configuration).",
            "sample_complexity_without_pretraining": "Vis-NGU and Vis-RND required significantly more updates to reach similar behaviors (see other entries for specifics).",
            "sample_complexity_gain": "Substantial relative gains reported; e.g., up to ~3x area visited in City and 50–70% fewer samples to learn manipulation tasks in Playroom compared to vis-only baselines.",
            "transfer_success_factors": "CLIP's alignment of images and natural language yields representations that are robust to viewpoint/color variations and emphasize semantic content, enabling better novelty judgments.",
            "transfer_failure_factors": "Pretraining domain mismatch and limited multi-object scene understanding can limit benefits; inference cost for large encoders requires sparse computation strategies.",
            "key_findings": "CLIP image/text encoders, when used frozen to form language-shaped embeddings, materially improve novelty-driven exploration in 3D embodied tasks by providing semantically-meaningful state abstractions.",
            "uuid": "e1839.3",
            "source_info": {
                "paper_title": "Semantic Exploration from Language Abstractions and Pretrained Representations",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "ALM (ALIGN models)",
            "name_full": "ALM (Models trained with ALIGN noisy text supervision)",
            "brief_description": "Vision–language models trained with contrastive loss on the ALIGN dataset; used here as frozen image/text encoders (Small-ALM and Med-ALM) to provide language-shaped embeddings for intrinsic rewards in NGU/ND.",
            "citation_title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "mention_or_use": "use",
            "model_agent_name": "ALM (Small-ALM, Med-ALM)",
            "model_agent_description": "Pretrained vision–language encoders trained contrastively on ALIGN; Small-ALM uses a 26M ResNet-50 image encoder and ~44M BERT text encoder, while Med-ALM uses a 71M NFNet image encoder and ~77M BERT text encoder. Used frozen for Lang-NGU, LSE-NGU and as ND targets.",
            "pretraining_data_type": "Large-scale noisy image–text pairs (ALIGN dataset)",
            "pretraining_data_details": "ALM models were trained with contrastive loss on the ALIGN dataset (reference provided); the paper reports internal model sizes for Small and Med variants and uses them without finetuning in experiments.",
            "embodied_task_name": "Playroom (lift, put, find) and City (explore) as embedding sources",
            "embodied_task_description": "Same Unity 3D Playroom and City tasks; ALM embeddings are used to compute intrinsic novelty or serve as distillation targets.",
            "action_space_text": "N/A",
            "action_space_embodied": "Discrete manipulation and navigation actions as per environment.",
            "action_mapping_method": "No direct mapping; ALM produces embeddings (image/text) used for novelty/prediction-error intrinsic rewards; policy maps visual inputs to actions independently.",
            "perception_requirements": "RGB visual observations; optionally oracle captions for text-embedding variants during training.",
            "transfer_successful": true,
            "performance_with_pretraining": "Small-ALM and Med-ALM based agents show improved sample efficiency and coverage: e.g., Lang-NGU with Small-ALM achieved ~241 bins in City (Table S4) and strong speedups on Playroom tasks (50–70% for lift/put, 18–38% for find depending on model).",
            "performance_without_pretraining": "Baselines without ALM (Vis-NGU, Vis-RND) performed worse (e.g., Vis-NGU coverage ~83 bins); ImageNet-only encoder gave weaker/noisy benefits.",
            "sample_complexity_with_pretraining": "Reported relative gains: similar ranges as CLIP cases (50–70% speedups on some tasks; earlier object interaction at ~40k updates).",
            "sample_complexity_without_pretraining": "See Vis-NGU / Vis-RND baselines (slower learning and lower coverage under same budgets).",
            "sample_complexity_gain": "Approximately 50–70% reduction in samples for some manipulation tasks and up to ~3x increase in exploration coverage in City in best configurations.",
            "transfer_success_factors": "ALIGN pretraining provides large-scale noisy image–text supervision that produces robust, language-shaped embeddings capturing object and scene semantics relevant to exploration.",
            "transfer_failure_factors": "Large pretrained targets can be harder to distill; pretraining–environment domain mismatch and limitations in multi-object scene understanding reduce gains on some tasks.",
            "key_findings": "ALM-derived embeddings are effective for driving intrinsic-reward-based exploration in 3D embodied tasks, producing substantial sample-efficiency and coverage improvements when used as frozen targets or embedding sources.",
            "uuid": "e1839.4",
            "source_info": {
                "paper_title": "Semantic Exploration from Language Abstractions and Pretrained Representations",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "BERT (text encoder)",
            "name_full": "BERT (Bidirectional Encoder Representations from Transformers)",
            "brief_description": "General-purpose pretrained Transformer text encoder used here to embed oracle captions; BERT embeddings were used as frozen representations in Lang-NGU to compute intrinsic novelty.",
            "citation_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "use",
            "model_agent_name": "BERT (frozen text encoder within Lang-NGU)",
            "model_agent_description": "Standard BERT-based text encoder (reported sizes ~70–90M parameters in this paper's variants) used frozen to embed environment captions O_L for computing intrinsic novelty in Lang-NGU.",
            "pretraining_data_type": "Large-scale text corpora (standard BERT pretraining on language corpora cited in BERT literature)",
            "pretraining_data_details": "Paper uses BERT-based text backbones (70–90M parameters). The authors do not finetune on environment captions and rely on the general language knowledge encoded by BERT; specific pretraining corpora are those used in the BERT literature (cited).",
            "embodied_task_name": "Playroom (lift, put, find) and City (explore) when used as Lang-NGU embedding source",
            "embodied_task_description": "Unity 3D embodied tasks where BERT embeddings of oracle captions are used to compute intrinsic novelty signals during training.",
            "action_space_text": "N/A",
            "action_space_embodied": "Discrete action sets (Playroom: 46 actions; City: navigation/look actions).",
            "action_mapping_method": "No direct mapping; BERT embeddings are used only to compute novelty rewards (k-NN distances) while the policy maps visual inputs to low-level actions learned from scratch.",
            "perception_requirements": "RGB visual observation O_V; oracle caption O_L during training to be embedded by BERT.",
            "transfer_successful": true,
            "performance_with_pretraining": "BERT-based Lang-NGU variants produced improved sample efficiency (figures show comparable gains to other text encoders; exact per-model percentages vary). For some Playroom tasks BERT Lang-NGU achieved similar acceleration compared to CLIP/ALM text embeddings.",
            "performance_without_pretraining": "Vis-NGU baseline slower and less exploratory (see other entries).",
            "sample_complexity_with_pretraining": "Part of the reported 50–70% speedups on lift/put tasks and 18–38% on find tasks when language-shaped embeddings are used; exact per-BERT numbers reported in figures/tables within the paper.",
            "sample_complexity_without_pretraining": "Baselines required more updates (see Vis-NGU ~60k updates to reach object interaction levels that Lang/BERT-enabled agents reached by ~40k updates).",
            "sample_complexity_gain": "Comparable percentage reductions in sample complexity as other language-shaped encoders (task-dependent).",
            "transfer_success_factors": "BERT encodes abstract linguistic categories that coarsen and semantically structure state representations, improving novelty signals for exploration.",
            "transfer_failure_factors": "BERT is a pure text encoder and relies on the quality and fidelity of the oracle captions; if captions are low-fidelity or misaligned with visual scenes, benefits may be limited.",
            "key_findings": "Using pretrained text encoders like BERT to embed state captions and compute intrinsic novelty yields meaningful exploration improvements, demonstrating that general language pretraining can provide useful abstractions for embodied RL.",
            "uuid": "e1839.5",
            "source_info": {
                "paper_title": "Semantic Exploration from Language Abstractions and Pretrained Representations",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2
        },
        {
            "paper_title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "rating": 2
        },
        {
            "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 2
        },
        {
            "paper_title": "Exploration by random network distillation",
            "rating": 1
        },
        {
            "paper_title": "Never give up: Learning directed exploration strategies",
            "rating": 1
        }
    ],
    "cost": 0.0207475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Semantic Exploration from Language Abstractions and Pretrained Representations</h1>
<p>Allison C. Tam<br>DeepMind<br>London, UK<br>actam@deepmind.com<br>Nicholas A. Roy<br>DeepMind<br>London, UK<br>nroy@deepmind.com</p>
<p>Jane X. Wang ${ }^{+}$
DeepMind
London, UK
wangjane@deepmind.com</p>
<p>Neil C. Rabinowitz
DeepMind
London, UK
ncr@deepmind.com</p>
<p>Stephanie C. Y. Chan
DeepMind
London, UK
scychan@deepmind.com</p>
<p>Andrea Banino ${ }^{+}$
DeepMind
London, UK
abanino@deepmind.com</p>
<p>Andrew K. Lampinen DeepMind London, UK
lampinen@deepmind.com</p>
<h2>DJ Strouse</h2>
<p>DeepMind
London, UK
strouse@deepmind.com</p>
<p>Felix Hill ${ }^{+}$
DeepMind
London, UK
felixhill@deepmind.com</p>
<h2>Abstract</h2>
<p>Effective exploration is a challenge in reinforcement learning (RL). Novelty-based exploration methods can suffer in high-dimensional state spaces, such as continuous partially-observable 3D environments. We address this challenge by defining novelty using semantically meaningful state abstractions, which can be found in learned representations shaped by natural language. In particular, we evaluate vision-language representations, pretrained on natural image captioning datasets. We show that these pretrained representations drive meaningful, task-relevant exploration and improve performance on 3D simulated environments. We also characterize why and how language provides useful abstractions for exploration by considering the impacts of using representations from a pretrained model, a language oracle, and several ablations. We demonstrate the benefits of our approach with on- and off-policy RL algorithms and in two very different task domainsone that stresses the identification and manipulation of everyday objects, and one that requires navigational exploration in an expansive world. Our results suggest that using language-shaped representations could improve exploration for various algorithms and agents in challenging environments.</p>
<h2>1 Introduction</h2>
<p>Exploration is one of the central challenges of reinforcement learning (RL). A popular way to increase an agent's tendency to explore is to augment trajectories with intrinsic rewards for reaching novel environment states. However, the success of this approach depends critically on which states are considered novel, which can in turn depend on how environment states are represented.</p>
<p>The literature on novelty-driven exploration describes several approaches to deriving state representations [7]. One popular method employs random features and represents the state by embedding</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the visual observation with a fixed, randomly initialized target network [Random Network Distillation; 6]. Another method uses learned visual features, taken from an inverse dynamics model [Never Give Up; 3]. These approaches work well in classic 2D environments like Atari, but it is less clear whether they are as effective in high-dimensional, partially-observable settings such as 3D environments. For instance, in 3D settings, different viewpoints of the same scene may map to distinct visual states/features, despite being semantically similar. The difficulty of identifying a good mapping between visual state and feature space is exacerbated by the fact that useful state abstractions are highly task dependent. For example, a task involving tool use requires object-affordance abstractions, whereas navigation does not. Thus, acquiring state representations that support effective exploration is a chicken-and-egg problem—knowing whether two states should be considered similar requires the type of understanding that an agent can only acquire after effectively exploring its environment.</p>
<p>To overcome these challenges, we propose giving agents access to prior knowledge during training, in the form of abstractions derived from large vision-language models [e.g. 41] that are pretrained on image captioning data. We use these pretrained models to derive a intrinsic reward that reflects meaningful novelty. We hypothesize that representations acquired by vision-language pretraining drive effective, semantic exploration in 3D environments, because the representations are shaped by the unique abstract nature of natural language.</p>
<p>Several aspects of natural language suggest that it could be useful to direct novelty-based exploration. First, language is inherently abstract: language links superficially distinct, but causally-related situations by describing them similarly, and contrasts between causally-distinct states by describing them differently, thus outlining useful concepts [29, 28]. Second, humans use language to communicate important information efficiently, without overspecifying [20, 21]. Thus, human language omits distracting irrelevant information and focuses on important aspects of the world. For example, it is often observed that an agent rewarded for seeking novel experience would be attracted forever to a TV with uncontrollable and unpredictable random static [7]. However, a human would likely caption this scene "a TV with no signal" regardless of the particular pattern; thus an agent exploring with language abstractions would quickly leave the TV behind. Figure 1 shows another conceptual example of how language abstractions can accelerate exploration.</p>
<p>We first perform motivating proof-of-concept experiments using a language oracle. We show that language is a useful abstraction for exploration not only because it coarsens the state space, but also because it coarsens the state space in a way that reflects the semantics of the environment. We then demonstrate that our results scale to environments without a language oracle using pretrained vision encoders, which are only supervised with language during pretraining. This work strives to enhance the representations used in novelty-based exploration, rather than compare various exploration methods.</p>
<p>We consider two popular novelty-based exploration methods from the literature, Never Give Up (NGU; Badia et al. [3]) and Random Network Distillation (RND; Burda et al. [7]), and compare them to their language-augmented variants, Lang-NGU/LSE-NGU and Lang-RND. We evaluate performance and sample efficiency on object manipulation, search, and navigation tasks in two challenging 3D environments simulated in Unity: Playroom (a house containing toys and furniture) and City (a large-scale urban setting). Our results show that language-based exploration with pretrained vision-language representations improves sample efficiency on Playroom tasks by 18-70%. It also doubles</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Navy dashed lines delineate semantically meaningful states. By using representations that align well with these boundaries (i.e. language), then agents more effectively explore the wider state space (orange trajectory). If the representations do not reflect these boundaries and instead are amenable to visual noise (i.e. different colors, viewpoints, etc.), then agents may only focus on a visually novel, yet narrow subset of states (red trajectory).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(b) Example instances of $O_{V}$ and $O_{L}$ from City. Many different scenes can be associated with the same caption.</p>
<p>Figure 2: Visual observations from the environment and example captions generated by the language oracle. Appendix Figure S4 contains more example captions.
the visited areas in City, compared to baseline methods. We show that language-based exploration is effective for both on-policy (IMPALA [17]) and off-policy (R2D2 [25]) agents.</p>
<h1>2 Related Work</h1>
<p>Exploration in RL Classical exploration strategies include $\epsilon$-greedy action selection [51], statecounting [49, 4, 32, 30, 3], curiosity driven exploration [44], and intrinsic motivation methods [36]. Our work is part of this last class of methods, where the agent is given an intrinsic reward for visiting diverse states over time [35]. Intrinsic rewards can be derived from various measures: novelty [43, 55, 6, 56], prediction-error [38, 3], ensemble disagreement [11, 39, 48, 46, 18, 50], or information gain [23]. One family of methods gives intrinsic reward for following a curriculum of goals [8, 12, 40]. Others use novelty measures to identify interesting states from which they can perform additional learning [16, 54]. These methods encourage exploration in different ways, but they all rely on visual state representations that are learned jointly with the policy. Although we focus on novelty-based intrinsic reward and demonstrate the benefits of language in NGU and RND, our methodology is relatively agnostic to the exploration method. We suggest that many other exploration methods could be improved by using language abstractions and pretrained embeddings to represent the state space.</p>
<p>Pretraining representations for RL Pretraining has been used in RL to improve the representations of the policy network. Self-supervised representation learning techniques distill knowledge from external datasets to produce downstream features that are helpful in virtual environments [15, 53]. Some recent work shows benefits from pretraining on more general, large-scale datasets. Pretrained CLIP features have been used in a number of recent robotics papers to speed up control and navigation tasks. These features can condition the policy network [26], or can be fused throughout the visual encoder to integrate semantic information about the environment [37]. The goal of these works is to improve perception in the policy. Pretrained language models can also provide useful initializations for training policies to imitate offline trajectories [42, 27]. These successes demonstrate that large pretrained models contain prior knowledge that can be useful for RL. While the existing literature uses pretrained embeddings directly in the agent, we instead allow the policy network to learn from scratchm and only utilize pretrained embeddings to guide exploration during training (Figure S2). We imagine that future work may benefit from combining both approaches.</p>
<p>Language for exploration Some recent works have used language to guide agent learning, by either using language subgoals for exploration/planning or providing task-specific reward shaping [47, 33, 13, 19]. Schwartz et al. [45] use a custom semantic parser for VizDoom and show that representing states with language, rather than vision, leads to faster learning by simplifying policy inputs. Chaplot et al. [10] tackle navigation in 3D by constructing a semantic map of the environment from pretrained SLAM modules, language-defined object categories, and agent location. This approach lends itself to navigation, but it is unclear how it would extend easily to more generic settings or other types of tasks, such as manipulation. Work concurrent to ours by Mu et al. [34] shows how language, in the form of hand-crafted BabyAI annotations, can help improve exploration in 2D environments. These works demonstrate the value of language abstractions: the ability to ignore extraneous noise and highlight important environment features. However, these prior methods rely on environment-specific semantic parsers or annotations, which may limit the settings to which they can be applied. In contrast, by exploiting powerful pretrained vision-language models, our approach can be applied to any visually-naturalistic environment, including 3D settings, which have not been widely studied in prior exploration work. We additionally do not require any language from the environment itself. Our method could even potentially improve exploration for physical robots, but we leave that for future work.</p>
<h2>3 Method</h2>
<p>We consider a goal-conditioned Markov decision process defined by a tuple $(\mathcal{S},\mathcal{A},\mathcal{G},P,R_{e},\gamma)$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{G}$ is the goal space, $P: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$ specifies the environment dynamics, $R_{e}: \mathcal{S} \times \mathcal{G} \rightarrow R_{e}$ is the extrinsic reward, and $\gamma$ is the discount factor. State $\mathbf{s}<em V="V">{\mathbf{t}}$ is presented to the agent as a visual observation $O</em>$. We later remove the need for a language oracle by using pretrained models.}$. In some cases, in order to calculate intrinsic reward, we use a language oracle $\mathcal{O}: \mathcal{S} \rightarrow \mathcal{L}$ that provides natural language descriptions of the state, $O_{L}$. Note that $O_{L}$ is distinct from the language instruction $g \in \mathcal{G}$, which is sampled from a goal distribution at the start of an episode-the agent never observes $O_{L</p>
<p>We use goal-conditioned reinforcement learning to produce a policy $\pi_{g}\left(\cdot \mid O_{V}\right)$ that maximizes the expected reward $\mathbb{E}\left[\sum_{t=0}^{H} \gamma^{t}\left(r_{t}^{e}+\right.\right.$ $\left.\left.\beta r_{t}^{i}\right)\right]$, where $H$ is the horizon, $r_{t}^{e}$ is the extrinsic reward, $r_{t}^{i}$ is the intrinsic reward, and $\beta$ is a tuned hyperparameter. The intrinsic reward is goal-agnostic and is computed with access to either $O_{V}$ or $O_{L}$. Note that neither $O_{L}$ nor pretrained embeddings are used by the policy, and thus we only use them during training to compute the intrinsic reward (Figure 3).</p>
<p>Our approach builds on two popular exploration algorithms: Never Give Up (NGU; Badia et al. [3]) and Random Network Distillation (RND; Burda et al. [7]). These algorithms were chosen to demonstrate the value of language under two different exploration paradigms. While both methods reward visiting novel states, they differ on several dimensions: the novelty horizon (episodic versus lifetime), how the history of past visited states is retained (non-parametric versus parametric), and how states are represented (learned controllable states versus random features).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The agent is trained from scratch using RL to optimize extrinsic and intrinsic reward. It acts using the image observation $O_{V}$ and goal $g$. During training, the novelty-based intrinsic reward is calculated using an auxiliary component that does not share parameters with the agent (dashed box). The auxiliary component may incorporate a pretrained language (pictured above) or image encoder, which may respectively require $O_{L}$ or $O_{V}$. The latter does not rely on language provided by the environment. See Figure S2 for more details.</p>
<h3>3.1 Never Give Up (NGU)</h3>
<p>To more clearly isolate our impact, we focus only on the episodic novelty component of the NGU agent [3]. State representations along the trajectory are written to a non-parametric episodic memory</p>
<p>buffer. The intrinsic reward reflects how novel the current state is relative to the states visited so far in the episode. Novelty is a function of the L2 distances between the current state and the $k$-nearest neighbor representations stored in the memory buffer. Intrinsic reward is higher for larger distances.</p>
<p>Full details can be found in the original paper; however, we make two key simplifications. While Badia et al. [3] proposes learning a family of policy networks that are capable of different levels of exploration, we train one policy network that maximizes reward $r=r_{e}+\beta r_{i}$ for a fixed hyperparameter $\beta$. We also fix the long-term novelty modulator $\alpha$ to be 1, essentially removing it.</p>
<p>The published baseline method, which we refer to as Vis-NGU, uses a controllable state taken from an inverse dynamics model. The inverse dynamics model is trained jointly with the policy, but the two networks do not share any parameters.</p>
<p>Table 1: Summary of NGU variants.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Embedding Type</th>
<th style="text-align: left;">Required Input</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Vis-NGU</td>
<td style="text-align: left;">Controllable State</td>
<td style="text-align: left;">Vision</td>
</tr>
<tr>
<td style="text-align: left;">Lang-NGU</td>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">Language</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">CLIP $_{\text {text }}$</td>
<td style="text-align: left;">Language</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ALM $_{\text {text }}$</td>
<td style="text-align: left;">Language</td>
</tr>
<tr>
<td style="text-align: left;">LSE-NGU</td>
<td style="text-align: left;">CLIP $_{\text {image }}$</td>
<td style="text-align: left;">Vision</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ALM $_{\text {image }}$</td>
<td style="text-align: left;">Vision</td>
</tr>
</tbody>
</table>
<p>Table 2: Summary of family of RND-inspired methods. Intrinsic reward is derived from the prediction error between the trainable network and frozen target function.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: center;">Trainable Network</th>
<th style="text-align: left;">Target Function</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Vis-RND</td>
<td style="text-align: center;">$f_{V}: O_{V} \rightarrow \mathbb{R}^{k}$</td>
<td style="text-align: left;">randomly initialized, fixed $\hat{f}$</td>
</tr>
<tr>
<td style="text-align: left;">ND</td>
<td style="text-align: center;">$f_{{V, L}}: O_{{V, L}} \rightarrow \mathbb{R}^{k}$</td>
<td style="text-align: left;">pretrained ALM $_{\text {[image, text] }}$</td>
</tr>
<tr>
<td style="text-align: left;">Lang-RND</td>
<td style="text-align: center;">$f_{L}: O_{L} \rightarrow \mathbb{R}^{k}$</td>
<td style="text-align: left;">randomly initialized, fixed $\hat{f}$</td>
</tr>
<tr>
<td style="text-align: left;">LD</td>
<td style="text-align: center;">$f_{C}: O_{V} \rightarrow O_{L}$</td>
<td style="text-align: left;">$O_{L}$ from language oracle</td>
</tr>
</tbody>
</table>
<p>The intrinsic reward relies on directly comparing state representations from the buffer, so our approach focuses on modifying the embedding function to influence exploration (Table 1). LangNGU uses a frozen pretrained language encoder to embed the oracle caption $O_{L}$. We compare language embeddings from BERT [14], CLIP [41], Small-ALM, and Med-ALM. The ALMs (ALign Models) are trained with a contrastive loss on the ALIGN dataset [24]. Small-ALM uses a 26M parameter ResNet-50 image encoder [22]; Med-ALM uses a 71M parameter NFNet [5]. The language backbones are based on BERT and are all in the range of 70-90M parameters. We do not finetune on environment-specific data; this preserves the real world knowledge acquired during pretraining and demonstrates its benefit without requiring any environment-specific captions.
LSE-NGU does not use the language oracle. Instead, it uses a frozen pretrained image encoder to embed the visual observation $O_{V}$. We use the image encoder from CLIP or ALM, which are trained on captioning datasets to produce outputs that are close to the corresponding language embeddings. The human-generated captions structure the visual embedding space to reflect features most pertinent to humans and human language [31], so the resulting representations can be thought of as Language Supervised Embeddings (LSE). The primary benefit of LSE-NGU is that it can be applied to environments without a language oracle or annotations. CLIP and ALM are trained on real-world data, so they would work best on realistic 3D environments. However, we imagine that in future work the pretraining process or dataset could be tailored to maximize transfer to a desired target environment.</p>
<h1>3.2 Random Network Distillation (RND)</h1>
<p>Our RND-inspired family of methods rewards lifetime novelty. Generically, the intrinsic reward is derived from the prediction error between a trainable network and some target value generated by a frozen function (Table 2). The trainable network is learned jointly with the policy network, although they do not share any parameters. As the agent trains over the course of its lifetime, the prediction error for frequently-visited states decreases, and the associated intrinsic reward consequently diminishes. Intuitively, the weights of the trainable network implicitly store the state visitation counts.
For clarity, we refer to the baseline published by Burda et al. [7] as Vis-RND. The trainable network $f_{V}: O_{V} \rightarrow \mathbb{R}^{k}$ maps the visual state to random features. The random features are produced by a fixed, randomly initialized network $\hat{f_{V}}$. Both $f_{V}$ and $\hat{f_{V}}$ share the same architecture: a ResNet followed by a MLP. The intrinsic reward is the mean squared error $\left|f_{V}\left(O_{V}\right)-\hat{f_{V}}\left(O_{V}\right)\right|^{2}$.
In network distillation (ND), the target function is not random, but is instead a pretrained text or image encoder from CLIP/ALM. The trainable network $f$ learns to reproduce the pretrained representations. To manage inference time, $f$ is a simpler network than the target (see Appendix A.2). The intrinsic loss is the mean squared error between $f$ and the large pretrained network. Like the respective Lang-NGU and LSE-NGU counterparts, text-based ND requires a language oracle, but image-based ND does not.</p>
<p>In Section 5.1 we compare against two additional methods to motivate why language is a useful abstraction. The first, Lang-RND, is a variant in which the trainable network $f_{L}: O_{L} \rightarrow \mathbb{R}^{k}$ maps the oracle caption to random features. The intrinsic reward is the mean squared error between the outputs of $f_{L}$ and fixed $\hat{f}<em L="L">{L}$ with random initialization. Both $f</em>}$ and $\hat{f<em C="C">{L}$ networks are of the same architecture.
The second method, language distillation (LD), is loosely inspired by RND in that the novelty signal comes from a prediction error. However, instead of learning to produce random features, the trainable network learns to caption the visual state, i.e. $f</em>$. In LD, the exploration dynamics not only depend on how frequently states are visited but also the alignment between language and the visual world. We test whether this caption-denoted alignment is necessary for directing semantic exploration by comparing LD to a variant with shuffled image-language alignment (S-LD) in Section 5.1.}: O_{V} \rightarrow O_{L}$. The network architecture consists of a CNN encoder and LSTM decoder. The intrinsic reward is the negative log-likelihood of the oracle caption under the trainable model $f_{C</p>
<h1>4 Experimental Setup</h1>
<h3>4.1 Environments</h3>
<p>Previous exploration work benchmarked algorithms on video games, such as 2D grid-world MiniHack and Montezuma's Revenge, or 3D first-person shooter Vizdoom. In this paper, we focus on first-person Unity-based 3D environments that are meant to mimic familiar scenes from the real world (Figure 2).</p>
<p>Playroom Our first domain, Playroom [1, 52], is a randomly-generated house containing everyday household items (e.g. bed, bathtub, tables, chairs, toys). The agent's action set consists of 46 discrete actions that involve locomotion primitives and object manipulation, such as holding and rotating.
We study two settings in Playroom. In the first setting, the agent is confined to a single room with 3-5 objects and is given a lift or put instruction. At the start of an episode, the set of objects are sampled from a larger set of everyday objects (i.e. a candle, cup, hairdryer). Object colors and sizes are also randomized, adding superficial variance to different semantic categories. The instructions take the form: "Lift a <object>" or "Put a <object> on a {bed, tray}". With a lift goal, the episode ends with reward 1 or 0 whenever any object is lifted. With a put goal, the episode ends with reward 1 when the condition is fulfilled. This setting tests spatial rearrangement skills.
In the second setting, the agent is placed in a house with 3-5 different rooms, and is given a find instruction of the form "Find a {teddy bear, rubber duck}". Every episode, the house is randomly generated with the teddy and duck hidden amongst many objects, furniture, and decorations. The target objects can appear in any room- either on the floor, on top of tables, or inside bookshelves. The agent is randomly initialized and can travel throughout the house and freely rearrange objects. The episode ends with reward 1 when the agent pauses in front of the desired object. The find task requires navigation/search skills and tests the ability to ignore the numerous distractor objects.</p>
<p>City Our second domain, City, is an expansive, large-scale urban environment. Each episode, a new map is generated; shops, parks, and buildings are randomly arranged in city blocks. Daylight is simulated, such that the episode starts during the morning and ends at nighttime. The agent is randomly initialized and is instructed to "explore the city." It is trained to maximize its intrinsic reward and can take the following actions: move_{forward, backward, left, right}, look_{left, right}, and move_forward_and_look_{left, right}. We divide up the map into a $32 \times 32$ grid and track how many unique bins are visited in an episode.
Additionally, City does not provide explicit visual or verbal signage to disambiguate locations. As such, systematic exploration is needed to maximize coverage. In contrast to Playroom, City tests long horizon exploration. A Playroom episode lasts only 600 timesteps, whereas a City episode lasts 11,250 and requires hundreds of timesteps to fully traverse the map even once. The City covers a 270-by-270 meter square area, which models a 2-by-2 grid of real world blocks.</p>
<h3>4.2 Captioning Engine</h3>
<p>We equip the environment with a language oracle that generates language descriptions of the scene, $O_{L}$, based on the Unity state, $»$ (Figure 2). In Playroom, the caption describes if and how the agent</p>
<p>interacts with objects and lists what is currently visible to it. In City, $O_{L}$ generally describes the object that the agent is directly looking at, but the captions alone do not disambiguate the agent's locations. Since these captions are generated from a Unity state, these descriptions may not be as varied or rich as a human's, but they can be generated accurately and reliably, and at scale.</p>
<h1>4.3 Training Details</h1>
<p>At test time, the agent receives image observation $O_{V}$ and language-specified goal $g$. The policy network never requires caption $O_{L}$ to act. During training, the exploration method calculates the intrinsic reward from $O_{L}$ or $O_{V}$.</p>
<p>We show that language-based exploration is compatible with both policy gradient and Q-learning algorithms. We use Impala [17] on Playroom and R2D2 on City [25]. Q-learning is more suitable for the City, because the action space is more restricted compared to the one needed for Playroom tasks.</p>
<p>For both environments, the agent architecture consists of an image ResNet encoder and a language LSTM encoder that feed into a memory LSTM module. The policy and value heads are MLPs that receive the memory state as input. If the exploration method requires additional networks, such as the trainable network in RND or inverse dynamics model in NGU, they do not share any parameters with the policy or value networks. Figure S2 is a visualization of an Impala agent that uses languageaugmented exploration. Hyperparameters and additional details are found in Appendix A.</p>
<h2>5 Results</h2>
<h3>5.1 Motivation: Language is a Meaningful Abstraction</h3>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Lang-RND outperforms Vis-RND by creating a coarser, more compact state space.</p>
<p>Comparison of S-LD to LD and Vis-RND
<img alt="img-4.jpeg" src="img-4.jpeg" />
(b) LD outperforms S-LD. It is important how language abstractions carve up the state space.</p>
<p>Figure 4: Our comparisons demonstrate that language is useful for exploration, because it outlines a more abstract, semantically-meaningful state space. Results are shown with a $95 \%$ confidence band.</p>
<p>We share the intuition with other work [e.g. 34] that language can improve exploration. We design a set of experiments to show how and why this may be the case. Our analysis follows the desiderata outlined by Burda et al. [6]-prediction-error exploration ought to use a feature space that filters irrelevant information (compact) and contains necessary information (sufficient). Burda et al. [6] specifically studies RND and notes that the random feature space, the outputs of the random network, may not fully satisfy either condition. As such, we use the language variants of RND to frame this discussion.</p>
<p>We hypothesize that language abstractions are useful, because they (1) create a coarser state space and (2) divide the state space in a way that meaningfully aligns with the world (i.e. using semantics). First, if language provides a coarser state space, then the random feature space becomes more compact,</p>
<p>leading to better exploration. We compare Lang-RND to Vis-RND to test this claim. Lang-RND learns the lift task 33% faster and solves the put task as Vis-RND starts to learn (Figure 4a).</p>
<p>Second, we ask whether semantics – that is <em>how</em> language divides up the state space – is critical for effective exploration. We use LD to test this hypothesis, precisely because the exploration in LD is motivated by modeling the semantic relationship between language and vision.</p>
<p>We compare LD to a shuffled variant S-LD, where we replace the particular semantic state abstraction that language offers with a statistically-matched randomized abstraction (Figure 5). S-LD is similar to LD; the intrinsic reward is the prediction error of the captioning network. However, instead of targeting the language oracle output, the S-LD trainable network produces a different target caption $\overrightarrow{O_{L}}$ that may not match the image. $\overrightarrow{O_{L}}$ is produced by a fixed, random mapping $\hat{f_{S}}:O_{V}\rightarrow\overrightarrow{O_{L}}$. $\hat{f_{S}}$ is constrained such that the marginal distributions $P(O_{L}) \approx P(\overrightarrow{O_{L}})$ are matched under trajectories produced by policy $\pi_{LD}$. See Appendix A.4 for full details on the construction of S-LD.</p>
<p>Thus, whereas the LD captions parcel up state space in a way that reflects the abstractions that language offers, the randomized mapping $\hat{f_{S}}$ parcels up state space in a way that abstracts over random features of the visual space (Figure 5). We control for the compactness and coarseness of the resulting representation by maintaining the same marginal distribution of captions.</p>
<p>If semantics is crucial for exploration, then we expect to see LD outperform S-LD. This indeed holds experimentally (Figure 4b). We can also view these results under the <em>Burda et al. [6]</em> framework. The S-LD abstractions group together visually similar, but semantically distinct states. A single sampled caption likely fails to capture the group in a manner that is representative of all the encompassing states. In other words, $\hat{f_{S}}$ produces a compact feature space that may not be sufficient. This may explain why S-LD learns faster than Vis-RND on the simpler lift task but fails on the more complex put and find tasks. The S-LD experiments imply that language abstractions are helpful for exploration because they expose not only a more compact, but also a more semantically meaningful state space.</p>
<h3>5.2 Pretrained Vision-Language Representations Improve Exploration</h3>
<p>Having shown how language can be helpful for exploration, we now incorporate pretrained vision-language representations into NGU and RND to improve exploration. Such representations (e.g. from the image encoder in CLIP/ALM) offer the benefits of explicit language abstractions, without the need to rely on a language oracle. We also compare language-shaped representations to pretrained ImageNet embeddings to isolate the effect of language. To keep the number of experiments tractable, we only perform a full comparison on the Playroom tasks.</p>
<p>City We first compare how representations affect performance in a pure exploration setting. With no extrinsic reward, the agent is motivated solely by the NGU intrinsic reward to explore the City. We report how many unique areas the agent visits in an episode in Figure 6. While optimizing coverage only requires knowledge of an agent’s global location rather than generic scene understanding, vision-language representations are still useful simply because meaningful exploration is inherently semantic. Lang-NGU, which uses text embeddings of $O_{L}$, visits an area up to 3 times larger. LSE-NGU achieves 2 times the coverage even without querying a language oracle (Appendix Figure S5).</p>
<p>Playroom We next show that pretrained vision-language representations significantly speed up learning across all Playroom tasks (Figure 7). The LSE-NGU and Lang-NGU agents improve sample efficiency by $50-70 \%$ on the lift and put tasks and $18-38 \%$ on the find task, depending on the pretraining model used. The ND agents are significantly faster than VisRND, learning $41 \%$ faster on the find task. We also measure agent-object interactions. Nearly all LSE-NGU and Lang-NGU agents learn to foveate on and hold objects within 40 k learning updates, whereas Vis-NGU agent takes at least 60 k updates to do so with the same frequency (Appendix Figure S7). Although LSENGU and image-based ND agents do not access a language oracle, they are similarly effective as their annotation-dependent counterparts in the Playroom tasks (Appendix Figure S6), suggesting that our method could be robust to the availability of a language oracle.</p>
<p>To demonstrate the value of rich language, we compare LSE-NGU agents to a control agent that instead uses pretrained ImageNet embeddings from a 70M NFNet [5]. ImageNet embeddings optimize for single-object classification, so they confer some benefit to the most objectfocused tasks, lift and put. However, ImageNet embeddings hurt exploration in the find task, where agents encounters more complex scenes (Figure 7b). By contrast, the language-shaped representations are well-suited for not only describing simple objects, but also have capacity for multi-object, complex scenes. Of course, current CLIPstyle models can be further improved in their ability to understand multi-object scenes, which may explain why the benefits are less pronounced for the find task. However, as the performance of pretrained vision-language models improve, we expect to see those benefits transfer to this method and drive even better exploration.</p>
<h2>6 Discussion</h2>
<p>We have shown that language abstractions and pretrained vision-language representations improve the sample efficiency of existing exploration methods. This benefit is seen across on-policy and off-policy algorithms (Impala and R2D2), different exploration methods (RND and NGU), different 3D domains (Playroom and City), and various task specifications (lifting/putting, searching, and intrinsically motivated navigation). Furthermore, we carefully designed control experiments to understand how language contributes to better exploration. Our results are consistent with cognitive perspectives on human language-language is powerful because it groups together situations according to semantic similarity. In terms of the desiderata that Burda et al. [6] present, language is both compact and sufficient. Finally, we note that using pretrained vision-language representations to embed image observations enables more effective exploration even if language is not available during agent training. This is vital for scaling to environments that do not have a language oracle or annotations.</p>
<p>Limitations and future directions We highlight several avenues for extending our work. First, additional research could provide a more comprehensive understanding of how language abstractions affect representations. This could include comparing different types of captions offering varying levels of detail, or task-dependent descriptions. These captions could be dynamically generated at scale by prompting a large multimodal model [2]. Second, it would be useful to investigate how to improve pretrained vision-language representations for exploration by finetuning on relevant datasets. The semantics of a dataset could even be tailored to task-specific abstractions to increase the quality of the learnt representations. Such approaches would potentially allow applying our method to virtual</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" />
(c) ND intrinsic rewards derive from the prediction error of the representations from a pretrained ALM network.</p>
<p>Figure 7: Agents that use pretrained language-shaped representations to explore (ALM-ND, LangNGU, LSE-NGU) learn faster than baseline agents. ALM-ND (Text/Image) refer to the ND variants in Table 2. Results shown with a $95 \%$ confidence interval.
environments that are farther from the pretraining distribution, such as Atari. In contrast, compared to our experiments, we believe that the current pretrained representations would deliver even more benefit for entirely photorealistic, visually rich environments, such as Matterport3D [9]. Finally, we note that a limitation of this approach is that current pretrained vision-language models may be less effective on multi-object scenes. Future pretraining innovations or larger models would presumably produce more robust representations and thus lead to even more effective exploration.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>We would like to thank Iain Barr for ALM models and Nathaniel Wong and Arthur Brussee for the Playroom environment. For the City environment, we would like to thank Nick Young, Tom Hudson, Alex Platonov, Bethanie Brownfield, Sarah Chakera, Dario de Cesare, Marjorie Limont, Benigno Uria, Borja Ibarz and Charles Blundell. Moreover, for the City, we would like to extend our special thanks to Jayd Matthias, Jason Sanmiya, Marcus Wainwright, Max Cant and the rest of the Worlds Team. Finally, we thank Hamza Merzic, Andre Saraiva, and Tim Scholtes for their helpful support and advice.</p>
<h2>References</h2>
<p>[1] J. Abramson, A. Ahuja, I. Barr, A. Brussee, F. Carnevale, M. Cassin, R. Chhaparia, S. Clark, B. Damoc, A. Dudzik, et al. Imitating interactive intelligence. arXiv preprint arXiv:2012.05672,</p>
<p>2020.
[2] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.
[3] A. P. Badia, P. Sprechmann, A. Vitvitskyi, D. Guo, B. Piot, S. Kapturowski, O. Tieleman, M. Arjovsky, A. Pritzel, A. Bolt, et al. Never give up: Learning directed exploration strategies. arXiv preprint arXiv:2002.06038, 2020.
[4] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29, 2016.
[5] A. Brock, S. De, S. L. Smith, and K. Simonyan. High-performance large-scale image recognition without normalization. In International Conference on Machine Learning, pages 1059-1071. PMLR, 2021.
[6] Y. Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell, and A. A. Efros. Large-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018.
[7] Y. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.
[8] A. Campero, R. Raileanu, H. Küttler, J. B. Tenenbaum, T. Rocktäschel, and E. Grefenstette. Learning with amigo: Adversarially motivated intrinsic goals. arXiv preprint arXiv:2006.12122, 2020.
[9] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017.
[10] D. S. Chaplot, D. P. Gandhi, A. Gupta, and R. R. Salakhutdinov. Object goal navigation using goal-oriented semantic exploration. Advances in Neural Information Processing Systems, 33: $4247-4258,2020$.
[11] R. Y. Chen, S. Sidor, P. Abbeel, and J. Schulman. UCB exploration via Q-ensembles. arXiv preprint arXiv:1706.01502, 2017.
[12] C. Colas, P. Fournier, M. Chetouani, O. Sigaud, and P.-Y. Oudeyer. Curious: intrinsically motivated modular multi-goal reinforcement learning. In International conference on machine learning, pages 1331-1340. PMLR, 2019.
[13] C. Colas, T. Karch, N. Lair, J.-M. Dussoux, C. Moulin-Frier, P. Dominey, and P.-Y. Oudeyer. Language as a cognitive tool to imagine goals in curiosity driven exploration. Advances in Neural Information Processing Systems, 33:3761-3774, 2020.
[14] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[15] Y. Du, C. Gan, and P. Isola. Curious representation learning for embodied intelligence. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1040810417, 2021.
[16] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune. First return, then explore. Nature, 590(7847):580-586, 2021.
[17] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International Conference on Machine Learning, pages 1407-1416. PMLR, 2018.
[18] S. Flennerhag, J. X. Wang, P. Sprechmann, F. Visin, A. Galashov, S. Kapturowski, D. L. Borsa, N. Heess, A. Barreto, and R. Pascanu. Temporal difference uncertainties as a signal for exploration. arXiv preprint arXiv:2010.02255, 2020.</p>
<p>[19] P. Goyal, S. Niekum, and R. J. Mooney. Using natural language for reward shaping in reinforcement learning. arXiv preprint arXiv:1903.02020, 2019.
[20] H. P. Grice. Logic and conversation. In Speech acts, pages 41-58. Brill, 1975.
[21] M. Hahn, D. Jurafsky, and R. Futrell. Universals of word order reflect optimization of grammars for efficient communication. Proceedings of the National Academy of Sciences, 117(5):23472353, 2020.
[22] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016.
[23] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel. Vime: Variational information maximizing exploration. Advances in neural information processing systems, 29, 2016.
[24] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904-4916. PMLR, 2021.
[25] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney. Recurrent experience replay in distributed reinforcement learning. In International conference on learning representations, 2018.
[26] A. Khandelwal, L. Weihs, R. Mottaghi, and A. Kembhavi. Simple but effective: Clip embeddings for embodied ai. arXiv preprint arXiv:2111.09888, 2021.
[27] S. Li, X. Puig, Y. Du, C. Wang, E. Akyurek, A. Torralba, J. Andreas, and I. Mordatch. Pre-trained language models for interactive decision-making. arXiv preprint arXiv:2202.01771, 2022.
[28] G. Lupyan. What do words do? toward a theory of language-augmented thought. In Psychology of learning and motivation, volume 57, pages 255-297. Elsevier, 2012.
[29] G. Lupyan, D. H. Rakison, and J. L. McClelland. Language is not just for talking: Redundant labels facilitate learning of novel categories. Psychological science, 18(12):1077-1083, 2007.
[30] M. C. Machado, M. G. Bellemare, and M. Bowling. Count-based exploration with the successor representation. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04):51255133, 2020.
[31] R. Marjieh, P. van Rijn, I. Sucholutsky, T. R. Sumers, H. Lee, T. L. Griffiths, and N. Jacoby. Words are all you need? capturing human sensory similarity with textual descriptors. arXiv preprint arXiv:2206.04105, 2022.
[32] J. Martin, S. N. Sasikumar, T. Everitt, and M. Hutter. Count-based exploration in feature space for reinforcement learning. arXiv preprint arXiv:1706.08090, 2017.
[33] S. Mirchandani, S. Karamcheti, and D. Sadigh. Ella: Exploration through learned language abstraction. Advances in Neural Information Processing Systems, 34, 2021.
[34] J. Mu, V. Zhong, R. Raileanu, M. Jiang, N. Goodman, T. Rocktäschel, and E. Grefenstette. Improving intrinsic exploration with language abstractions. arXiv preprint arXiv:2202.08938, 2022.
[35] P.-Y. Oudeyer and F. Kaplan. What is intrinsic motivation? a typology of computational approaches. Frontiers in neurorobotics, 1:6, 2009.
[36] P.-Y. Oudeyer, F. Kaplan, and V. V. Hafner. Intrinsic motivation systems for autonomous mental development. IEEE transactions on evolutionary computation, 11(2):265-286, 2007.
[37] S. Parisi, A. Rajeswaran, S. Purushwalkam, and A. Gupta. The unsurprising effectiveness of pre-trained vision models for control. arXiv preprint arXiv:2203.03580, 2022.</p>
<p>[38] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by selfsupervised prediction. In International conference on machine learning, pages 2778-2787. PMLR, 2017.
[39] D. Pathak, D. Gandhi, and A. Gupta. Self-supervised exploration via disagreement. In International conference on machine learning, pages 5062-5071. PMLR, 2019.
[40] S. Racaniere, A. Lampinen, A. Santoro, D. Reichert, V. Firoiu, and T. Lillicrap. Automated curriculum generation through setter-solver interactions. In International conference on learning representations, 2019.
[41] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021.
[42] M. Reid, Y. Yamada, and S. S. Gu. Can wikipedia help offline reinforcement learning? arXiv preprint arXiv:2201.12122, 2022.
[43] N. Savinov, A. Raichuk, R. Marinier, D. Vincent, M. Pollefeys, T. Lillicrap, and S. Gelly. Episodic curiosity through reachability. arXiv preprint arXiv:1810.02274, 2018.
[44] J. Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. In Proc. of the international conference on simulation of adaptive behavior: From animals to animats, pages 222-227, 1991.
[45] E. Schwartz, G. Tennenholtz, C. Tessler, and S. Mannor. Language is power: Representing states using natural language in reinforcement learning. arXiv preprint arXiv:1910.02789, 2019.
[46] R. Sekar, O. Rybkin, K. Daniilidis, P. Abbeel, D. Hafner, and D. Pathak. Planning to explore via self-supervised world models. In International Conference on Machine Learning (ICML), 2020.
[47] M. Shridhar, X. Yuan, M.-A. Côté, Y. Bisk, A. Trischler, and M. Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020.
[48] P. Shyam, W. Jaśkowski, and F. Gomez. Model-based active exploration. In International Conference on Machine Learning (ICML), 2019.
[49] A. L. Strehl and M. L. Littman. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309-1331, 2008.
[50] D. Strouse, K. Baumli, D. Warde-Farley, V. Mnih, and S. Hansen. Learning more skills through optimistic exploration. In International Conference on Learning Representations (ICLR), 2022.
[51] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.
[52] D. I. A. Team, J. Abramson, A. Ahuja, A. Brussee, F. Carnevale, M. Cassin, F. Fischer, P. Georgiev, A. Goldin, T. Harley, et al. Creating multimodal interactive agents with imitation and self-supervised learning. arXiv preprint arXiv:2112.03763, 2021.
[53] T. Xiao, I. Radosavovic, T. Darrell, and J. Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022.
[54] D. Zha, W. Ma, L. Yuan, X. Hu, and J. Liu. Rank the episodes: A simple approach for exploration in procedurally-generated environments. arXiv preprint arXiv:2101.08152, 2021.
[55] T. Zhang, P. Rashidinejad, J. Jiao, Y. Tian, J. E. Gonzalez, and S. Russell. Made: Exploration via maximizing deviation from explored regions. Advances in Neural Information Processing Systems, 34, 2021.
[56] T. Zhang, H. Xu, X. Wang, Y. Wu, K. Keutzer, J. E. Gonzalez, and Y. Tian. Noveld: A simple yet effective exploration criterion. Advances in Neural Information Processing Systems, 34, 2021.</p>
<h1>Appendix</h1>
<h2>A Training Details</h2>
<p>We use a distributed RL training setup with 256 parallel actors. For Impala agents, the learner samples from a replay buffer that acts like a queue. For R2D2 agents, the learner samples from a replay buffer using prioritized replay. Training took 8-36 hours per experiment on a $2 \times 2$ TPUv2.</p>
<p>All agents share the same policy network architecture and hyperparameters (Table S1). We use an Adam optimizer for all experiments. The hyperparameters used to train Impala [17] and R2D2 [25] are mostly taken from the original implementations. For Impala, we set the unroll length to 128, the policy network cost to 0.85 , and state-value function cost to 1.0 . We also use two heads to estimate the state-value functions for extrinsic and intrinsic reward separately. For R2D2, we set the unroll length to 100 , burn-in period to 20 , priority exponent to 0.9 , Q-network target update period to 400 , and replay buffer size to 10,000 .</p>
<p>Table S1: Common architecture and hyperparameters for all agents.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: right;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Image resolution:</td>
<td style="text-align: right;">$96 \times 72 \times 3$</td>
</tr>
<tr>
<td style="text-align: left;">Number of action repeats:</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;">Batch size:</td>
<td style="text-align: right;">32</td>
</tr>
<tr>
<td style="text-align: left;">Agent discount $\gamma$ :</td>
<td style="text-align: right;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate:</td>
<td style="text-align: right;">0.0003</td>
</tr>
<tr>
<td style="text-align: left;">ResNet num channels (policy):</td>
<td style="text-align: right;">$16,32,32$</td>
</tr>
<tr>
<td style="text-align: left;">LSTM hidden units (memory):</td>
<td style="text-align: right;">256</td>
</tr>
</tbody>
</table>
<h2>A. 1 Training Details for NGU Variants</h2>
<p>We use a simplified version of the full NGU agent as to focus on the episodic novelty component. One major difference is that we only learn one value function, associated with a single intrinsic reward scale $\beta$ and discount factor $\gamma$. The discount factor $\gamma$ is 0.99 and we sweep for $\beta$ (Table S2). Another major difference is that our lifetime novelty factor $\alpha$ is always set to 1 .</p>
<p>The NGU memory buffer is set to 12,000 , so that it can always has the capacity to store the the entire episode. The buffer is reset at the start of the episode. The intrinsic reward is calculated from a kernel operation over state representations stored in the memory buffer. We use the same kernel function and associated hyperparameters (e.g. number of neighbors, cluster distance, maximum similarity) found in Badia et al. [3].</p>
<p>For Vis-NGU, the 32-dimension controllable states come from a learned inverse dynamics model. The inverse dynamics model is trained with an Adam optimizer (learning rate $5 \mathrm{e}-4, \beta_{1}=0.0, \beta_{2}=0.95$, $\epsilon$ is $6 \mathrm{e}-6$ ). For the variants, we use frozen pretrained representations from BERT, ALM, or CLIP. The ALM pretrained embeddings are size 768 and the CLIP embeddings are 512. Med-ALM comprises a 71 M parameter NFNet image encoder and 77 M BERT text encoder. Small-ALM comprises a 25M Resnet image encoder and 44M BERT text encoder. Using Small-ALM can help mitigate the increased inference time. To manage training time, we use Small-ALM in the City environment, where the episode is magnitudes longer than Playroom. For the LSE-NGU ImageNet control, we use the representations from a frozen 71M parameter NFNet pretrained on ImageNet (F0 from Brock et al. [5]). This roughly matches the size of the CLIP and Med-ALM image encoders.</p>
<p>We notice that it is crucial for the pretrained representations to only be added to the buffer every 8 timesteps. Meanwhile, Vis-NGU adds controllable states every timestep, which is as or more effective than every 8 . This may be due to some interactions between the kernel function and the smoothness of the learned controllable states. We also find that normalizing the intrinsic reward, like in RND, is helpful in some settings. We use normalization for Lang-NGU and LSE-NGU on the Playroom tasks.</p>
<p>Table S2: Hyperparameters for the family of NGU agents on the Playroom tasks. All agents except Lang-NGU use a scaling factor of 0.01 in City. Lang-NGU uses 0.1 .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Environment</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Embedding Type</th>
<th style="text-align: center;">Intrinsic Reward Scale $\beta$</th>
<th style="text-align: center;">Entropy cost</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Playroom lift, put</td>
<td style="text-align: center;">Vis-NGU</td>
<td style="text-align: center;">Controllable State</td>
<td style="text-align: center;">$3.1 \mathrm{e}-7$</td>
<td style="text-align: center;">$6.2 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lang-NGU</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">0.029</td>
<td style="text-align: center;">$2.4 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLIP $_{\text {text }}$</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">$2.3 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSE-NGU</td>
<td style="text-align: center;">ALM $_{\text {text }}$</td>
<td style="text-align: center;">0.0035</td>
<td style="text-align: center;">$9.1 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLIP $_{\text {image }}$</td>
<td style="text-align: center;">0.029</td>
<td style="text-align: center;">$2.6 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ALM $_{\text {image }}$</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">$1.6 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ImageNet</td>
<td style="text-align: center;">0.0072</td>
<td style="text-align: center;">$6.4 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: center;">Playroom find</td>
<td style="text-align: center;">Vis-NGU</td>
<td style="text-align: center;">Controllable State</td>
<td style="text-align: center;">$3.1 \mathrm{e}-6$</td>
<td style="text-align: center;">$2.6 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lang-NGU</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">0.029</td>
<td style="text-align: center;">$2.4 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLIP $_{\text {text }}$</td>
<td style="text-align: center;">0.0047</td>
<td style="text-align: center;">$4.1 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ALM $_{\text {text }}$</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">$1.9 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSE-NGU</td>
<td style="text-align: center;">CLIP $_{\text {image }}$</td>
<td style="text-align: center;">0.0083</td>
<td style="text-align: center;">$6.7 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ALM $_{\text {image }}$</td>
<td style="text-align: center;">0.0051</td>
<td style="text-align: center;">$1.2 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ImageNet</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">$1.0 \mathrm{e}-4$</td>
</tr>
</tbody>
</table>
<h1>A. 2 Training Details for RND-Inspired Agents</h1>
<p>For the RND-inspired exploration agents, the hyperparameters for training the trainable network are taken from the original implementation [7]. We perform a random hyperparameter search over a range of values for the intrinsic reward scale, V-Trace entropy cost, and the learning rate for the trainable network. The values can be found in Table S3. We also normalize all intrinsic reward with the rolling mean and standard deviation.</p>
<p>Table S3: Hyperparameters for the family of RND-inspired agents. Learning rate is used for the trainable network.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Environment</th>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Intrinsic reward scale $\beta$</th>
<th style="text-align: center;">Entropy cost</th>
<th style="text-align: center;">Learning rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Playroom lift, put</td>
<td style="text-align: center;">Vis-RND</td>
<td style="text-align: center;">$1.4 \mathrm{e}-4$</td>
<td style="text-align: center;">$1.2 \mathrm{e}-4$</td>
<td style="text-align: center;">$1.2 \mathrm{e}-3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lang-RND</td>
<td style="text-align: center;">$3.2 \mathrm{e}-6$</td>
<td style="text-align: center;">$8.1 \mathrm{e}-5$</td>
<td style="text-align: center;">$5.3 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ALM-ND (Text)</td>
<td style="text-align: center;">$8.4 \mathrm{e}-6$</td>
<td style="text-align: center;">$2.5 \mathrm{e}-5$</td>
<td style="text-align: center;">$3.1 \mathrm{e}-3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ALM-ND (Image)</td>
<td style="text-align: center;">$1.1 \mathrm{e}-5$</td>
<td style="text-align: center;">$2.2 \mathrm{e}-5$</td>
<td style="text-align: center;">$2.1 \mathrm{e}-3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LD</td>
<td style="text-align: center;">$1.1 \mathrm{e}-5$</td>
<td style="text-align: center;">$2.7 \mathrm{e}-5$</td>
<td style="text-align: center;">$1.7 \mathrm{e}-3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-LD</td>
<td style="text-align: center;">$1.4 \mathrm{e}-6$</td>
<td style="text-align: center;">$7.6 \mathrm{e}-5$</td>
<td style="text-align: center;">$1.8 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;">Playroom find</td>
<td style="text-align: center;">Vis-RND</td>
<td style="text-align: center;">$9.9 \mathrm{e}-5$</td>
<td style="text-align: center;">$4.4 \mathrm{e}-5$</td>
<td style="text-align: center;">$5.4 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lang-RND</td>
<td style="text-align: center;">$7.2 \mathrm{e}-4$</td>
<td style="text-align: center;">$8 \mathrm{e}-5$</td>
<td style="text-align: center;">$1 \mathrm{e}-3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ALM-ND (Text)</td>
<td style="text-align: center;">$2.3 \mathrm{e}-4$</td>
<td style="text-align: center;">$3.9 \mathrm{e}-5$</td>
<td style="text-align: center;">$9.6 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ALM-ND (Image)</td>
<td style="text-align: center;">$3 \mathrm{e} .0-3$</td>
<td style="text-align: center;">$9.5 \mathrm{e}-5$</td>
<td style="text-align: center;">$2.1 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LD</td>
<td style="text-align: center;">$4.1 \mathrm{e}-5$</td>
<td style="text-align: center;">$4.3 \mathrm{e}-5$</td>
<td style="text-align: center;">$2 \mathrm{e}-3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S-LD</td>
<td style="text-align: center;">$4.1 \mathrm{e}-5$</td>
<td style="text-align: center;">$4.3 \mathrm{e}-5$</td>
<td style="text-align: center;">$2 . \mathrm{e}-3$</td>
</tr>
</tbody>
</table>
<p>The various RND-inspired agents differ in the input and output space of the trainable network and target functions (Figure S1). Some trainable networks and target networks involve a convolutional network, which consists of $(64,128,128,128)$ channels with $(7,5,3,3)$ kernel and $(4,2,1,1)$ stride. For ND, we balance the model capacity of the trainable network with the memory required for learning larger networks. We notice that using larger networks as the target function can make distillation harder and therefore, requires more careful parameter tuning. Our experiments use the 26 M parameter Small-ALM vision encoder for generating target outputs, which minimizes the discrepancy in complexity between the trainable network and target function. This also managed the memory requirements during training.</p>
<h2>A. 3 Engineering Considerations</h2>
<p>Integrating large pretrained models into RL frameworks is nontrivial. High quality pretrained models are orders of magnitudes larger than policy networks, introducing challenges with inference speed.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure S1: Architecture diagrams of the trainable network and frozen target functions for the RNDinspired family of methods. The teal square boxes are paired outputs, such that the trainable function is trained to match the frozen target function. None of the parameters here are shared with the policy or value network.</p>
<p>Slow inference not only increases iteration and training times but also may push learning further off-policy in distributed RL setups [e.g. 17, 25]. We mitigate this issue in Lang-NGU and LSENGU by only performing inference computations when it is necessary. We compute the pretrained representations only when they are required for adding to the NGU buffer (i.e. every 8 timesteps).</p>
<h1>A. 4 S-LD Construction</h1>
<p>As explained in Section 5.1, we compare the efficacy of exploration induced by the language distillation (LD) method with that induced by a shuffled variant, S-LD. LD employs an exploration bonus based on the prediction error of a captioning network $f_{C}: O_{V} \rightarrow O_{L}$, where the target value is the oracle caption. Our goal with S-LD is to determine whether the efficacy of the exploration is due to the specific abstractions induced by $f_{C}$, or whether it is due to some low level statistical property (e.g. the discrete nature of $O_{L}$, or the particular marginal output distribution).
We sample a fixed, random mapping $\hat{f}<em V="V">{S}: O</em>(L)$.} \rightarrow O_{L}$ while trying to match the low-level statistics of $f_{C}$. To do this, we construct a (fixed, random) smooth partitioning of $O_{V}$ into discrete regions, which in turn are each assigned to a unique caption from the output space of $f_{C}$. The regions are sized to maintain the same marginal distribution of captions, $P_{\pi_{L D}}(L) \approx P_{\pi_{S-L D}</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure S2: Architecture diagram for a generic Impala agent used in our Playroom experiments. We feed the image observation and language instruction to the policy and value networks. During training, we also use the scene caption to calculate the intrinsic reward, which corresponds to the gray shaded box. There is no parameter sharing between the networks inside and outside of the gray box.</p>
<p>More precisely, our procedure for constructing $\hat{f}<em S="S">{S}$ is as follows. We build $\hat{f}</em>}$ as the composition of three functions, $\hat{f<em 3="3">{S}=g</em>$ :} \circ g_{2} \circ g_{1</p>
<ul>
<li>$g_{1}: O_{V} \rightarrow \mathbb{R}$ is a fixed, random function, implemented using a neural network as in VisRND.</li>
<li>$g_{2}: \mathbb{R} \rightarrow \operatorname{Cat}(K)$ is a one-hot operation, segmenting $g_{1}\left(O_{V}\right)$ into $K$ non-overlapping, contiguous intervals.</li>
<li>$g_{3}: \operatorname{Cat}(K) \rightarrow O_{L}$ is a fixed mapping from the discrete categories to set of captions produced by the language oracle, $f_{C}$.</li>
</ul>
<p>To ensure that the marginal distribution of captions seen in $f_{C}$ was preserved in $f_{S}$, we first measure the empirical distribution of oracle captions encountered by $\pi_{L D}$, a policy trained to maximize the LD intrinsic reward. We denote the observed marginal probability of observing caption $l_{i}$ as $P_{\pi_{L D}}\left(O_{L}=l_{i}\right)=q_{i}$ (where the ordering of captions $l_{i} \in O_{L}$ is fixed and random). We then measure the empirical distribution of $g_{1}$ under the same action of the same policy, $P_{\pi_{L D}}\left(g_{1}\left(O_{V}\right)\right)$, and define the boundaries of the intervals of $g_{2}$ by the quantiles of this empirical distribution so as to match the CDF of $\mathbf{q}$, i.e. so that $P_{\pi_{L D}}\left(g_{2} \circ g_{1}\left(O_{V}\right)=i\right)=q_{i}$. Finally, we define $g_{3}$ to map from the $i^{\text {th }}$ category to the corresponding caption $l_{i}$, so that $P_{\pi_{L D}}\left(g_{3} \circ g_{2} \circ g_{1}\left(O_{V}\right)=l_{i}\right)=q_{i}$.</p>
<h1>B Additional ablation: Pretrained controllable states</h1>
<p>The state representations used by Vis-NGU are trained jointly with the policy, whereas the pretrained representations used by Lang-NGU and LSE-NGU are frozen during training. To isolate the effect of the knowledge encoded by the representations, we perform an additional experiment where we pretrain the controllable states and freeze them during training. We use the weights of the inverse dynamics model from a previously trained Vis-NGU agent. Figure S3 shows that pretrained VisNGU learns at the same rate as Vis-NGU (if not slower). Thus, the increased performance in LangNGU and LSE-NGU agents is due to the way the vision-language embeddings are pretrained on captioning datasets. This furthermore suggests that the converged controllable states do not fully</p>
<p>capture the knowledge needed for efficient exploration and in fact may even hurt exploration at the start of training by focusing on the wrong parts of the visual observation.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure S3: Vis-NGU with a pretrained inverse dynamics model learns slower than the baseline VisNGU agent that uses online learned controllable states.</p>
<h1>C Additional Figures</h1>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>You can see two arm chairs, an ottoman, and a shelf.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>You can see a bed, a shelf, a bookcase, a teddy, and a rubber duck.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>You can see an armchair, an ottoman, a shelf, and a teddy.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>You can see a bathtub and a rubber duck.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>You are holding a helicopter. You can see a bed.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>You can see a storage tray, a train, a robot, and a car.
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>You are holding a potted plant. You can see a storage tray and a bed.
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>You can see a bed, a storage tray, a rocket, a bus, and a mug.
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>You are looking at a post office in a light grey stucco shops in a apartment building.
<img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>You are looking at a blue grey brick offices in a office building.
<img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>You are looking at a green door in a dark grey painted brick ground floor apartments in a apartment building.
<img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>You are looking at a fire hydrant in a park.</p>
<p>Figure S4: Example scenes and associated captions from multi-room Playroom used for the find task (left column), single-room Playroom used for the lift and put tasks (middle column), and City (right column).</p>
<p>Table S4: Mean and standard error of coverage (number of bins reached on map) by variants of NGU agents using different state representations. The City consists of 1024 total bins, although not all are reachable. With the ground truth (continuous) embedding type, the NGU state representation is the global coordinate of the agent location. With the ground truth (discrete) embedding type, the representation is a one-hot encoding of the bins. A non-adaptive, uniform random policy is also included as baseline ('N/A - Random Actions').</p>
<table>
<thead>
<tr>
<th>Embedding Type</th>
<th>Coverage (number of bins)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ground Truth (continuous)</td>
<td>$346 \pm 3.2$</td>
</tr>
<tr>
<td>Ground Truth (discrete)</td>
<td>$539 \pm 3.5$</td>
</tr>
<tr>
<td>N/A - Random Actions</td>
<td>$60 \pm 0.53$</td>
</tr>
<tr>
<td>NGU with Controllable State</td>
<td>$83 \pm 6.9$</td>
</tr>
<tr>
<td>NGU with ImageNet</td>
<td>$111 \pm 10.6$</td>
</tr>
<tr>
<td>Lang-NGU with CLIP</td>
<td>$225 \pm 8.9$</td>
</tr>
<tr>
<td>Lang-NGU with Small-ALM</td>
<td>$241 \pm 9.0$</td>
</tr>
<tr>
<td>LSE-NGU with CLIP</td>
<td>$153 \pm 7.1$</td>
</tr>
<tr>
<td>LSE-NGU with Small-ALM</td>
<td>$162 \pm 5.6$</td>
</tr>
</tbody>
</table>
<p><img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Figure S5: Heatmaps of agent coverage over the City environment.</p>
<p><img alt="img-22.jpeg" src="img-22.jpeg" /></p>
<p>Figure S6: For both CLIP and ALM representations, LSE-NGU and Lang-NGU learn at comparable speeds, suggesting that the method can transfer well to environments without language annotations.</p>
<p><img alt="img-23.jpeg" src="img-23.jpeg" /></p>
<p>Figure S7: Lang-NGU and LSE-NGU agents learn to interact with objects (holding and foveating) earlier in training compared to the Vis-NGU agent. The benefit is larger for the put task, where the extrinsic reward also reinforces object interaction.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{+}$Equal contribution&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
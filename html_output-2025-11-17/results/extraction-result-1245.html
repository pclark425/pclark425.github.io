<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1245 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1245</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1245</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-227305267</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2012.02271v1.pdf" target="_blank">LAMP: Learning a Motion Policy to Repeatedly Navigate in an Uncertain Environment</a></p>
                <p><strong>Paper Abstract:</strong> Mobile robots are often tasked with repeatedly navigating through an environment whose traversability changes over time. These changes may exhibit some hidden structure, which can be learned. Many studies consider reactive algorithms for online planning, however, these algorithms do not take advantage of the past executions of the navigation task for future tasks. In this paper, we formalize the problem of minimizing the total expected cost to perform multiple start-to-goal navigation tasks on a roadmap by introducing the Learned Reactive Planning Problem. We propose a method that captures information from past executions to learn a motion policy to handle obstacles that the robot has seen before. We propose the LAMP framework, which integrates the generated motion policy with an existing navigation stack. Finally, an extensive set of experiments in simulated and real-world environments show that the proposed method outperforms the state-of-the-art algorithms by 10% to 40% in terms of expected time to travel from start to goal. We also evaluate the robustness of the proposed method in the presence of localization and mapping errors on a real robot.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1245.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1245.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Discrete envs (Env1-3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discrete floor-plan navigation graphs (Environments 1, 2, and 3 used in simulations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manually drawn indoor navigation graphs derived from grocery-store floorplans with multiple stochastic realizations (blocked/unblocked edges), used to evaluate planning policies under structured traversability uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Discrete floor-plan graphs (Environments 1-3)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Topological graphs representing indoor navigation (portal/portal-centre vertices, edges represent traversable connections between regions). Each environment has multiple realizations in which certain edges (portals) are blocked or unblocked; realizations are correlated (structured uncertainty).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Portals (graph edges) can be blocked or unblocked per environment realization; blockage is stochastic and can be correlated across multiple portals (conditional blocking across edges), but there are no explicit key/lock mechanics — constraint is 'edge present/absent' depending on environment realization.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Sparse navigation graph (portal-based topological graph); connectivity varies by realization because edges may be blocked, producing subgraphs with varying connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RPP-Hybrid (compared to Optimistic, UCT-CTP, Factor Graph)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RPP-Hybrid: a learned reactive planning policy that stores compact 'super maps' of observed subgraphs and constructs a decision-tree policy (observations + traversal actions); switches to an optimistic shortest-path policy when encountering previously unseen realizations. Baselines: optimistic A* (assume unknown edges unblocked), UCT-CTP (rollout-based with independent edge probabilities), Factor-graph approach (approximate joint pmf + rollouts).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Expected cost / average cost relative to optimal shortest path (i.e., expected travel cost to goal; reported as % of optimal).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>RPP-Hybrid average cost: Env1 129% of optimal (last-10: 117%); Env2 100% (last-10:100%); Env3 107% (last-10:104%). Optimistic and UCT-CTP are much worse in Env1 (~218% of optimal).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-based learned reactive policy that exploits correlations between edge states (RPP-Hybrid / learned decision tree with observations).</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Environments with structured/correlated uncertainties (a small subset of subgraphs dominating the pmf) enable RPP-Hybrid to learn and exploit correlations, reducing expected cost; unstructured independent-edge uncertainty (assumed by UCT-CTP) yields little improvement over optimistic reactive planners. High correlation -> fewer super maps -> faster convergence and lower expected cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Across Environments 1–3 RPP-Hybrid consistently outperformed optimistic, UCT-CTP, and Factor-Graph baselines; performance reached (near) optimal in Envs 2 and 3 and improved substantially in Env1 where other methods failed to exploit correlations. UCT-CTP (independent probs) provided no improvement over optimistic in these topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Optimal policies are represented as decision trees mixing observation and traversal actions; incorporating memory (super maps) and choosing observations to resolve correlated edge states improves performance. Policies must balance exploration (observations) and exploitation (moving to goal); switching to optimistic policy provides robustness when encountering unseen subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LAMP: Learning a Motion Policy to Repeatedly Navigate in an Uncertain Environment', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1245.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1245.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gazebo office sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gazebo office environment (hybrid topological-metric map, simulated Jackal)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous simulated office environment converted to a hybrid topological-metric representation (submaps + portals → navigation graph) with stochastic portal-blocking obstacles (correlated columns) and additional dynamic debris; used to test LAMP integrated with a full navigation stack.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Gazebo office simulation (hybrid map)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>20m×20m simulated indoor office with submap decomposition; portals correspond to edges in a navigation graph. Obstacles appear at portal locations or in submaps according to correlated probabilities (columns of obstacles are perfectly correlated).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Portals can be blocked by spawned barriers (binary blocked/unblocked). Correlation structure: some sets of portals (columns) are either all blocked or all unblocked in a realization; no key/lock mechanics.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Sparse navigation graph; connectivity depends on spawned barriers leading to different passable subgraphs (32 possible portal-blocking configurations reported for the chosen obstacle set).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>16 vertices, 33 edges (navigation graph); 32 possible configurations from correlated obstacle set (not counting random debris placements).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RPP-Hybrid (with LAMP integration) / Optimistic / Simple</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RPP-Hybrid implemented within LAMP: uses super map memory filter, edge resolver from costmap, constructs discrete RPP decision-tree policies offline between tasks and instructs existing navigation stack; Optimistic: send goal to stack and assume unknown free; Simple: baseline sending goal to navigation stack without topological planner.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Average distance travelled to goal (compared to Simple baseline) and number of switches to optimistic policy (rate over tasks); also number of super maps stored over tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>RPP-Hybrid reduced distance by ≈20–30% relative to the Simple baseline (average over trials); switch-to-optimistic rate decreased exponentially with tasks; number of stored super maps plateaued (max 20 after 100 tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Learned reactive (RPP-Hybrid) integrated with metric navigation stack; policies that exploit across-task correlations produce shorter expected travel distances.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Correlation structure (columns of perfectly correlated portals) reduces effective entropy of environment realizations; RPP-Hybrid leverages this to learn routines (paths) that avoid blocked regions and thus reduces distance. Larger submaps increase reliance on continuous planner and edge-resolver runtime; smaller submaps increase sensitivity and more super maps stored.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Within this simulated topology, RPP-Hybrid consistently reduced travel distance vs Simple and Optimistic; the presence of correlated portal blockages (structured uncertainty) made learning more beneficial. Edge-observer runtime and submap size tradeoffs were shown to affect practical performance.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies constructed as decision trees that include observation actions to resolve portal states, and that fall back to optimistic shortest-path planning when encountering unseen realizations, performed best in practice; the edge-resolver can resolve many edges before full traversal, reducing need for exploration in some submaps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LAMP: Learning a Motion Policy to Repeatedly Navigate in an Uncertain Environment', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1245.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1245.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Real-robot env</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physical event-space navigation graph (real Jackal experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Real-world hybrid topological-metric environment (event space) with manually annotated base occupancy grid, submaps/portals and stochastic portal/blocker locations; used to validate LAMP under localization/mapping noise.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Real-world event-space navigation graph</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Approximately 20m×10m event space mapped into submaps and portals; start/goal repeated navigation tasks with a small set of possible portal-blocking realizations (8 realizations reported) and additional debris; tested on a Clearpath Jackal with LiDAR.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Portals may be blocked by obstacles in a realization (binary blocked/unblocked). Real-world sensor/localization errors can cause incorrect edge resolution (false blocked/unblocked).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Sparse navigation graph; connectivity varies by realization; 14 vertices and 35 edges reported for the test graph.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>14 vertices, 35 edges; 8 reported realizations for the test environment.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RPP-Hybrid (via LAMP) and Simple baseline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RPP-Hybrid implemented on real robot: constructs super maps from observed edge states, uses edge resolver with costmap to resolve portal states and builds decision-tree policies interleaving observations and traversals; Simple baseline uses out-of-the-box ROS navigation stack sending global goal directly.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Distance travelled to reach goal (meters) and average travel distance compared to Simple baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Trial 1: average distance Simple 39.5 m vs RPP-Hybrid 35.4 m (≈10.4% improvement). Trial 2: Simple 46.6 m vs RPP-Hybrid 44.3 m (≈5.1% improvement). Performance degraded with mapping/localization errors but still improved on average.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Learned reactive policy (RPP-Hybrid) that leverages past executions; hybrid fallback to optimistic ensures completeness and robustness under novel realizations.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Even with a small number of realizations, RPP-Hybrid can learn structure and reduce travel distance; however, incorrect edge resolutions due to localization/mapping errors can corrupt the learned super maps and degrade later performance—early mapping errors are more detrimental.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>No systematic comparison across different physical topologies in the real-robot trials (only two short trials), but results confirm simulation trends: structured, repeatable portal blockages give opportunities to learn efficient policies; sensor/localization noise moderates gains.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies must be robust to imperfect edge-resolution; LAMP augments policy execution with edge-resolver and temporary robot-position vertices when encountering blocked edges during traversal; switching to optimistic policy mitigates worst-case performance when encountering novel or mis-modeled realizations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LAMP: Learning a Motion Policy to Repeatedly Navigate in an Uncertain Environment', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1245.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1245.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RPP-Hybrid / LAMP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RPP-Hybrid policy and LAMP framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RPP-Hybrid: a learned reactive planning policy that builds complete decision-tree policies from compact 'super map' estimates of previously observed subgraphs and switches to an optimistic planner when necessary; LAMP: framework integrating RPP-Hybrid with a hybrid topological-metric map, edge resolver, and standard navigation stack for continuous robot deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple (discrete test graphs, Gazebo sim, real robot)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Framework/policy applicable to repeated start-to-goal navigation tasks in environments modelled as graphs with stochastic blocked/unblocked edges (structured uncertainty).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Handles portals represented as graph edges that can be blocked/unblocked per realization; correlations between portal states are learned and exploited; no explicit key/lock mechanics.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Designed for sparse portal-based navigation graphs derived from occupancy grids; connectivity represented as weighted undirected graph between portal centers.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Used in experiments ranging from small discrete graphs (size unspecified) to 16-vertex/33-edge simulated graph and 14-vertex/35-edge real graph.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RPP-Hybrid (policy) / LAMP (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy structure: complete decision-tree mapping belief states (which super maps are consistent) and current vertex to actions (observe edge, traverse, terminate). Uses Map Memory Filter to store 'super maps' (merged agreeing maps), an edge-resolver to mark portal edges (blocked/unblocked/unknown) using costmaps, and a hybrid policy that falls back to optimistic shortest-path planning when required. Map merging is greedy (online First-Fit-like) to limit stored super maps.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Expected task cost (expected time/distance to goal), average distance travelled, percentage cost relative to optimal; rate of switching to optimistic policy; number of stored super maps.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Reported improvements: 10%–40% reduction in expected travel time over state-of-the-art in continuous experiments (abstract); discrete results show RPP-Hybrid at 129% of optimal vs optimistic at ~218% in one test (Env1); Gazebo sim: 20–30% reduction in distance vs simple policy; robot trials: 10.4% and 5.1% improvements in two trials.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-augmented reactive planning (decision-tree policies using past task observations), hybridizing learned policy with optimistic planner for completeness and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Performance gains scale with the degree to which environment traversability exhibits learnable structure (correlations across edges/subgraphs). Map merging and the compact super-map representation reduce policy search complexity when many realizations share structure; poorly chosen submap granularity or large numbers of super maps increase computation and can cause conservative policies. Policies can get trapped suboptimally if perceived probabilities of edges make informative exploration appear too costly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>RPP-Hybrid outperforms baselines more strongly in topologies with stronger structure/correlations; factor-graph and rollout-based methods eventually improve but require many more trials/rollouts; independent-edge probability approaches (UCT-CTP with independent assumptions) often fail to improve in these structured topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Decision-tree (belief + vertex) policies that include explicit observation actions are effective; adding an optimistic fallback ensures completeness. The number and composition of stored super maps (dependent on map-merge strategy and submap decomposition) shapes policy conservativeness and exploration behavior: more stored blocked edges -> more conservative future policies; greedy merge strategies can favor exploration by minimizing added blocked edges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LAMP: Learning a Motion Policy to Repeatedly Navigate in an Uncertain Environment', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Shortest paths without a map <em>(Rating: 2)</em></li>
                <li>High-quality policies for the canadian traveler's problem <em>(Rating: 2)</em></li>
                <li>An AO* based exact algorithm for the Canadian traveler problem <em>(Rating: 2)</em></li>
                <li>Long-term robot navigation in indoor environments estimating patterns in traversability changes <em>(Rating: 2)</em></li>
                <li>Learning motion planning policies in uncertain environments through repeated task executions <em>(Rating: 2)</em></li>
                <li>Risk-aware graph search with dynamic edge cost discovery <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1245",
    "paper_id": "paper-227305267",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "Discrete envs (Env1-3)",
            "name_full": "Discrete floor-plan navigation graphs (Environments 1, 2, and 3 used in simulations)",
            "brief_description": "Manually drawn indoor navigation graphs derived from grocery-store floorplans with multiple stochastic realizations (blocked/unblocked edges), used to evaluate planning policies under structured traversability uncertainty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Discrete floor-plan graphs (Environments 1-3)",
            "environment_description": "Topological graphs representing indoor navigation (portal/portal-centre vertices, edges represent traversable connections between regions). Each environment has multiple realizations in which certain edges (portals) are blocked or unblocked; realizations are correlated (structured uncertainty).",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Portals (graph edges) can be blocked or unblocked per environment realization; blockage is stochastic and can be correlated across multiple portals (conditional blocking across edges), but there are no explicit key/lock mechanics — constraint is 'edge present/absent' depending on environment realization.",
            "graph_connectivity": "Sparse navigation graph (portal-based topological graph); connectivity varies by realization because edges may be blocked, producing subgraphs with varying connectivity.",
            "environment_size": null,
            "agent_name": "RPP-Hybrid (compared to Optimistic, UCT-CTP, Factor Graph)",
            "agent_description": "RPP-Hybrid: a learned reactive planning policy that stores compact 'super maps' of observed subgraphs and constructs a decision-tree policy (observations + traversal actions); switches to an optimistic shortest-path policy when encountering previously unseen realizations. Baselines: optimistic A* (assume unknown edges unblocked), UCT-CTP (rollout-based with independent edge probabilities), Factor-graph approach (approximate joint pmf + rollouts).",
            "exploration_efficiency_metric": "Expected cost / average cost relative to optimal shortest path (i.e., expected travel cost to goal; reported as % of optimal).",
            "exploration_efficiency_value": "RPP-Hybrid average cost: Env1 129% of optimal (last-10: 117%); Env2 100% (last-10:100%); Env3 107% (last-10:104%). Optimistic and UCT-CTP are much worse in Env1 (~218% of optimal).",
            "success_rate": null,
            "optimal_policy_type": "Memory-based learned reactive policy that exploits correlations between edge states (RPP-Hybrid / learned decision tree with observations).",
            "topology_performance_relationship": "Environments with structured/correlated uncertainties (a small subset of subgraphs dominating the pmf) enable RPP-Hybrid to learn and exploit correlations, reducing expected cost; unstructured independent-edge uncertainty (assumed by UCT-CTP) yields little improvement over optimistic reactive planners. High correlation -&gt; fewer super maps -&gt; faster convergence and lower expected cost.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Across Environments 1–3 RPP-Hybrid consistently outperformed optimistic, UCT-CTP, and Factor-Graph baselines; performance reached (near) optimal in Envs 2 and 3 and improved substantially in Env1 where other methods failed to exploit correlations. UCT-CTP (independent probs) provided no improvement over optimistic in these topologies.",
            "policy_structure_findings": "Optimal policies are represented as decision trees mixing observation and traversal actions; incorporating memory (super maps) and choosing observations to resolve correlated edge states improves performance. Policies must balance exploration (observations) and exploitation (moving to goal); switching to optimistic policy provides robustness when encountering unseen subgraphs.",
            "uuid": "e1245.0",
            "source_info": {
                "paper_title": "LAMP: Learning a Motion Policy to Repeatedly Navigate in an Uncertain Environment",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Gazebo office sim",
            "name_full": "Gazebo office environment (hybrid topological-metric map, simulated Jackal)",
            "brief_description": "A continuous simulated office environment converted to a hybrid topological-metric representation (submaps + portals → navigation graph) with stochastic portal-blocking obstacles (correlated columns) and additional dynamic debris; used to test LAMP integrated with a full navigation stack.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Gazebo office simulation (hybrid map)",
            "environment_description": "20m×20m simulated indoor office with submap decomposition; portals correspond to edges in a navigation graph. Obstacles appear at portal locations or in submaps according to correlated probabilities (columns of obstacles are perfectly correlated).",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Portals can be blocked by spawned barriers (binary blocked/unblocked). Correlation structure: some sets of portals (columns) are either all blocked or all unblocked in a realization; no key/lock mechanics.",
            "graph_connectivity": "Sparse navigation graph; connectivity depends on spawned barriers leading to different passable subgraphs (32 possible portal-blocking configurations reported for the chosen obstacle set).",
            "environment_size": "16 vertices, 33 edges (navigation graph); 32 possible configurations from correlated obstacle set (not counting random debris placements).",
            "agent_name": "RPP-Hybrid (with LAMP integration) / Optimistic / Simple",
            "agent_description": "RPP-Hybrid implemented within LAMP: uses super map memory filter, edge resolver from costmap, constructs discrete RPP decision-tree policies offline between tasks and instructs existing navigation stack; Optimistic: send goal to stack and assume unknown free; Simple: baseline sending goal to navigation stack without topological planner.",
            "exploration_efficiency_metric": "Average distance travelled to goal (compared to Simple baseline) and number of switches to optimistic policy (rate over tasks); also number of super maps stored over tasks.",
            "exploration_efficiency_value": "RPP-Hybrid reduced distance by ≈20–30% relative to the Simple baseline (average over trials); switch-to-optimistic rate decreased exponentially with tasks; number of stored super maps plateaued (max 20 after 100 tasks).",
            "success_rate": null,
            "optimal_policy_type": "Learned reactive (RPP-Hybrid) integrated with metric navigation stack; policies that exploit across-task correlations produce shorter expected travel distances.",
            "topology_performance_relationship": "Correlation structure (columns of perfectly correlated portals) reduces effective entropy of environment realizations; RPP-Hybrid leverages this to learn routines (paths) that avoid blocked regions and thus reduces distance. Larger submaps increase reliance on continuous planner and edge-resolver runtime; smaller submaps increase sensitivity and more super maps stored.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Within this simulated topology, RPP-Hybrid consistently reduced travel distance vs Simple and Optimistic; the presence of correlated portal blockages (structured uncertainty) made learning more beneficial. Edge-observer runtime and submap size tradeoffs were shown to affect practical performance.",
            "policy_structure_findings": "Policies constructed as decision trees that include observation actions to resolve portal states, and that fall back to optimistic shortest-path planning when encountering unseen realizations, performed best in practice; the edge-resolver can resolve many edges before full traversal, reducing need for exploration in some submaps.",
            "uuid": "e1245.1",
            "source_info": {
                "paper_title": "LAMP: Learning a Motion Policy to Repeatedly Navigate in an Uncertain Environment",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Real-robot env",
            "name_full": "Physical event-space navigation graph (real Jackal experiments)",
            "brief_description": "Real-world hybrid topological-metric environment (event space) with manually annotated base occupancy grid, submaps/portals and stochastic portal/blocker locations; used to validate LAMP under localization/mapping noise.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Real-world event-space navigation graph",
            "environment_description": "Approximately 20m×10m event space mapped into submaps and portals; start/goal repeated navigation tasks with a small set of possible portal-blocking realizations (8 realizations reported) and additional debris; tested on a Clearpath Jackal with LiDAR.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Portals may be blocked by obstacles in a realization (binary blocked/unblocked). Real-world sensor/localization errors can cause incorrect edge resolution (false blocked/unblocked).",
            "graph_connectivity": "Sparse navigation graph; connectivity varies by realization; 14 vertices and 35 edges reported for the test graph.",
            "environment_size": "14 vertices, 35 edges; 8 reported realizations for the test environment.",
            "agent_name": "RPP-Hybrid (via LAMP) and Simple baseline",
            "agent_description": "RPP-Hybrid implemented on real robot: constructs super maps from observed edge states, uses edge resolver with costmap to resolve portal states and builds decision-tree policies interleaving observations and traversals; Simple baseline uses out-of-the-box ROS navigation stack sending global goal directly.",
            "exploration_efficiency_metric": "Distance travelled to reach goal (meters) and average travel distance compared to Simple baseline.",
            "exploration_efficiency_value": "Trial 1: average distance Simple 39.5 m vs RPP-Hybrid 35.4 m (≈10.4% improvement). Trial 2: Simple 46.6 m vs RPP-Hybrid 44.3 m (≈5.1% improvement). Performance degraded with mapping/localization errors but still improved on average.",
            "success_rate": null,
            "optimal_policy_type": "Learned reactive policy (RPP-Hybrid) that leverages past executions; hybrid fallback to optimistic ensures completeness and robustness under novel realizations.",
            "topology_performance_relationship": "Even with a small number of realizations, RPP-Hybrid can learn structure and reduce travel distance; however, incorrect edge resolutions due to localization/mapping errors can corrupt the learned super maps and degrade later performance—early mapping errors are more detrimental.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "No systematic comparison across different physical topologies in the real-robot trials (only two short trials), but results confirm simulation trends: structured, repeatable portal blockages give opportunities to learn efficient policies; sensor/localization noise moderates gains.",
            "policy_structure_findings": "Policies must be robust to imperfect edge-resolution; LAMP augments policy execution with edge-resolver and temporary robot-position vertices when encountering blocked edges during traversal; switching to optimistic policy mitigates worst-case performance when encountering novel or mis-modeled realizations.",
            "uuid": "e1245.2",
            "source_info": {
                "paper_title": "LAMP: Learning a Motion Policy to Repeatedly Navigate in an Uncertain Environment",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "RPP-Hybrid / LAMP",
            "name_full": "RPP-Hybrid policy and LAMP framework",
            "brief_description": "RPP-Hybrid: a learned reactive planning policy that builds complete decision-tree policies from compact 'super map' estimates of previously observed subgraphs and switches to an optimistic planner when necessary; LAMP: framework integrating RPP-Hybrid with a hybrid topological-metric map, edge resolver, and standard navigation stack for continuous robot deployment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Multiple (discrete test graphs, Gazebo sim, real robot)",
            "environment_description": "Framework/policy applicable to repeated start-to-goal navigation tasks in environments modelled as graphs with stochastic blocked/unblocked edges (structured uncertainty).",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Handles portals represented as graph edges that can be blocked/unblocked per realization; correlations between portal states are learned and exploited; no explicit key/lock mechanics.",
            "graph_connectivity": "Designed for sparse portal-based navigation graphs derived from occupancy grids; connectivity represented as weighted undirected graph between portal centers.",
            "environment_size": "Used in experiments ranging from small discrete graphs (size unspecified) to 16-vertex/33-edge simulated graph and 14-vertex/35-edge real graph.",
            "agent_name": "RPP-Hybrid (policy) / LAMP (framework)",
            "agent_description": "Policy structure: complete decision-tree mapping belief states (which super maps are consistent) and current vertex to actions (observe edge, traverse, terminate). Uses Map Memory Filter to store 'super maps' (merged agreeing maps), an edge-resolver to mark portal edges (blocked/unblocked/unknown) using costmaps, and a hybrid policy that falls back to optimistic shortest-path planning when required. Map merging is greedy (online First-Fit-like) to limit stored super maps.",
            "exploration_efficiency_metric": "Expected task cost (expected time/distance to goal), average distance travelled, percentage cost relative to optimal; rate of switching to optimistic policy; number of stored super maps.",
            "exploration_efficiency_value": "Reported improvements: 10%–40% reduction in expected travel time over state-of-the-art in continuous experiments (abstract); discrete results show RPP-Hybrid at 129% of optimal vs optimistic at ~218% in one test (Env1); Gazebo sim: 20–30% reduction in distance vs simple policy; robot trials: 10.4% and 5.1% improvements in two trials.",
            "success_rate": null,
            "optimal_policy_type": "Memory-augmented reactive planning (decision-tree policies using past task observations), hybridizing learned policy with optimistic planner for completeness and robustness.",
            "topology_performance_relationship": "Performance gains scale with the degree to which environment traversability exhibits learnable structure (correlations across edges/subgraphs). Map merging and the compact super-map representation reduce policy search complexity when many realizations share structure; poorly chosen submap granularity or large numbers of super maps increase computation and can cause conservative policies. Policies can get trapped suboptimally if perceived probabilities of edges make informative exploration appear too costly.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "RPP-Hybrid outperforms baselines more strongly in topologies with stronger structure/correlations; factor-graph and rollout-based methods eventually improve but require many more trials/rollouts; independent-edge probability approaches (UCT-CTP with independent assumptions) often fail to improve in these structured topologies.",
            "policy_structure_findings": "Decision-tree (belief + vertex) policies that include explicit observation actions are effective; adding an optimistic fallback ensures completeness. The number and composition of stored super maps (dependent on map-merge strategy and submap decomposition) shapes policy conservativeness and exploration behavior: more stored blocked edges -&gt; more conservative future policies; greedy merge strategies can favor exploration by minimizing added blocked edges.",
            "uuid": "e1245.3",
            "source_info": {
                "paper_title": "LAMP: Learning a Motion Policy to Repeatedly Navigate in an Uncertain Environment",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Shortest paths without a map",
            "rating": 2,
            "sanitized_title": "shortest_paths_without_a_map"
        },
        {
            "paper_title": "High-quality policies for the canadian traveler's problem",
            "rating": 2,
            "sanitized_title": "highquality_policies_for_the_canadian_travelers_problem"
        },
        {
            "paper_title": "An AO* based exact algorithm for the Canadian traveler problem",
            "rating": 2,
            "sanitized_title": "an_ao_based_exact_algorithm_for_the_canadian_traveler_problem"
        },
        {
            "paper_title": "Long-term robot navigation in indoor environments estimating patterns in traversability changes",
            "rating": 2,
            "sanitized_title": "longterm_robot_navigation_in_indoor_environments_estimating_patterns_in_traversability_changes"
        },
        {
            "paper_title": "Learning motion planning policies in uncertain environments through repeated task executions",
            "rating": 2,
            "sanitized_title": "learning_motion_planning_policies_in_uncertain_environments_through_repeated_task_executions"
        },
        {
            "paper_title": "Risk-aware graph search with dynamic edge cost discovery",
            "rating": 1,
            "sanitized_title": "riskaware_graph_search_with_dynamic_edge_cost_discovery"
        }
    ],
    "cost": 0.01770325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LAMP: Learning a Motion Policy to Repeatedly Navigate in an Uncertain Environment</p>
<p>Florence Tsang 
Tristan Walker 
Ryan A Macdonald 
Armin Sadeghi 
Stephen L Smith 
LAMP: Learning a Motion Policy to Repeatedly Navigate in an Uncertain Environment
1Motion and Path PlanningLearning and Adap- tive SystemsReactive and Sensor-Based Planningand Active Sensing
Mobile robots are often tasked with repeatedly navigating through an environment whose traversability changes over time. These changes may exhibit some hidden structure, which can be learned. Many studies consider reactive algorithms for online planning, however, these algorithms do not take advantage of the past executions of the navigation task for future tasks. In this paper, we formalize the problem of minimizing the total expected cost to perform multiple start-togoal navigation tasks on a roadmap by introducing the Learned Reactive Planning Problem. We propose a method that captures information from past executions to learn a motion policy to handle obstacles that the robot has seen before. We propose the LAMP framework, which integrates the generated motion policy with an existing navigation stack. Finally, an extensive set of experiments in simulated and real-world environments show that the proposed method outperforms the state-of-theart algorithms by 10% to 40% in terms of expected time to travel from start to goal. We also evaluate the robustness of the proposed method in the presence of localization and mapping errors on a real robot.</p>
<p>I. INTRODUCTION</p>
<p>A common approach to dealing with uncertainties while navigating is to plan a shortest path given the current map. If the path is blocked, then the map is updated and path is re-planned. There are many shortest path algorithms that can be utilized to accomplish this (see [1], [2], [3], [4]). This is known as an optimistic policy, which is suitable for situations where the robot is not expected to operate in the same environment for a long period of time (e.g. search and rescue missions, exploration missions). However, from a long-term operation perspective (e.g. deployment in a mall or warehouse), this policy is not ideal: It only considers the obstacles given by the map and other obstacles within its immediate sensor range when forming its strategy, without considering past executions, which could result in consistently 'bad' routes to the goal. Figure 1 illustrates an example of an optimistic policy performing poorly. Consider the case where the robot operates in the environment shown in Figure 1b 75%    with their probability of occurrence. In environment (b), the path a robot may take given an optimistic planner is shown in orange, while the optimal path is shown in blue. time and operates in the environment shown in Figure 1c otherwise. An optimistic approach will always attempt the orange path in environment (b), resulting in a considerably longer travel distance than the optimal path, shown in blue. One could plan to always execute the blue path, but that would be considerably longer than optimal when the robot is in environment (c). However, what if the robot eventually learns to take the blue path only in environment (b) as it realizes that the door on the right will never be open if the door on the left is closed?</p>
<p>In this paper, we present the Learning a Motion Policy (LAMP) framework which uses an alternative strategy; one that can learn and exploit such hidden properties of a realworld environment, so that over time, the expected cost of the path to reach the goal is minimized. The LAMP framework is designed to be easily deployable on a real robot such as the one shown in Figure 1a.</p>
<p>Related Work: The optimistic policy has been the most prevalent navigation approach because of its simplicity to implement. Examples include D<em>Lite [5], LPA</em> [6], etc.</p>
<p>More sophisticated algorithms such as Risk-Aware Graph Search [7] aim to improve performance in unstructured environments. However, as shown by the example in Figure  1, sometimes there may be structure in the traversability of the environment that can be exploited. Kucner et al. [8] presents a method for learning motion patterns over time, but this work focuses more on dynamic uncertainties such as moving obstacles as opposed to the changes in traversability patterns considered here. A closely related problem to navigation with uncertainties is the Canadian Traveller's Problem (CTP). Informally, the CTP is the problem of, given a graph, finding the optimal path for a robot to take from start to goal. However, some of the edges are blocked (the robot cannot traverse that edge), and the state of an edge is only revealed to the robot when it reaches an endpoint of the edge. A common variant of this problem is the stochastic CTP, where each edge has a known probability of being blocked. The original problem was defined by Papadimitriou et al. [9], and many other variations have been suggested (see [10], [11], [12]).</p>
<p>Optimal policies for the stochastic CTP can be calculated using algorithms such as CAO* [13] or value iteration. However, they must limit the size of the problem or the number of observations the robot is allowed to make to keep the problem tractable. Some algorithms for the stochastic CTP, such as UCT-CTP [14], optimize a policy by sampling environments based on the edge probabilities and estimating the expected cost by simulating the policy over this series of environments. These runs are known as rollouts. The more rollouts that are performed, the more accurate the cost estimate will be, and generally the better the resulting policy at the cost of higher runtimes. Other algorithms use a heuristic to approximate the next step in a policy, such as the Hedged Shortest Path under Determinization (HSPD) algorithm proposed in [11] or the algorithm proposed for the Reactive Planning Problem [15]. Both assume there is structure to the environment uncertainties that can be exploited to find a minimum cost policy.</p>
<p>There has been very little work on two fronts. First, existing CTP variations and subsequently, their solutions, assume the probability of an edge being blocked is known, which is generally not true in practice. Only the work by Nardi and Stachniss [16], and Tsang et al. [17] addresses the problem of capturing this information from the environment. In [16] authors propose a factor graph method to approximate the correlation between edge traversabilities. The proposed structure of the factor graph limits their approach to only capture binary correlations between edges. In comparison, our proposed method stores the observed environments during the task executions as subgraphs of the complete environment, which allows it to learn higher-order correlations between edges.</p>
<p>The second shortcoming of the existing literature is that there is little, if any, work that combines these CTP algo-rithms with existing navigation systems to form a pipeline that can be implemented on a physical platform. Many of these algorithms are graph-based, which need to be adapted to fit the continuous world the robot operates in. Additionally, it is typically assumed that the edge state can be determined at an edge endpoint, which is not necessarily true, especially in indoor scenarios. For example, a robot may not realize a door on the side of a hallway is closed or locked until it gets closer to the door. Therefore, in this paper, we propose an edge resolver algorithm (i.e. determining whether an edge is blocked or unblocked) under uncertainties using a costmap [18].</p>
<p>Aksakalli et al. [13] showed that the stochastic CTP can be modelled as a Markov Decision Process (MDP) and a deterministic Partially Observable Markov Decision Process (POMDP), both of which are formulations frequently used in reinforcement learning. However, generic POMDP solvers cannot be used to find optimal policies for the CTP in practical applications because the state space for the stochastic CTP is exponential.</p>
<p>There have been advances to utilize neural networks to approximate value functions for MDPs with large state and action spaces. This combination of neural networks and traditional reinforcement learning is coined Deep Reinforcement Learning (DRL). Kanezaki et al. [19] proposed a path planning algorithm based on DRL that performs well in the presence of many unexpected obstacles, but the behaviour is that of an optimistic algorithm. Furthermore, Lillicrap and Hunt et al. [20] introduced the Deep Deterministic Policy Gradient (DDPG), an algorithm to solve reinforcement learning problems with continuous action spaces (for example, balancing a quadruped, running, etc). Deep learning is also applied to predict an environment given a certain structure ( [21], [22], [23]). The drawback of using these techniques for navigation with uncertainty is the need for training before the algorithm can be used in practice. These predictive approaches are trained for a specific type of environment (e.g. tunnels, offices), and can only be applied to these specific scenarios. Unlike tasks such as learning to play a specific video game or learning how to walk, the state space for the CTP can vary significantly with each new environment. If the deployment environment is different from the training data, the algorithm may perform poorly and have to be retrained. Obtaining data for training such an algorithm is also a challenge. In contrast, our solution does not require any training, and can be deployed with minimal setup in a variety of scenarios.</p>
<p>Contributions: The contributions of this paper are fourfold: First, we formulate the Learned Reactive Planning Problem (LRPP) as the problem of repeatedly navigating in an unknown environment while minimizing the total expected cost. Then we prove that a special case of the problem with known probabilities of environment configurations is PSPACE-hard. Second, we provide the RPP-Hybrid policy, a method in discrete environments to learn correlations between obstacles from past executions and propose an adaptive planning algorithm, which improves its policy with every task. Third, we propose the LAMP framework for navigating in continuous environments, which utilizes a hybrid topological representation of the environment, and integrates our discrete planner with an off-the-shelf navigation stack. Finally, we evaluate the performance of the proposed algorithms in discrete and continuous environments with an extensive set of simulated and real-world experiments.</p>
<p>A preliminary version of this work appeared in the conference paper [17]. This initial work introduced the LRPP in discrete environments and provided simulation results in a grid world. The key additional contributions of this paper are the extension of the problem to continuous environments, the hybrid decomposition of the environment, and the LAMP framework to integrate our discrete planner and a navigation stack. We also greatly expanded the evaluation: we provide a comparison of the discrete algorithm with the state-of-theart method proposed in [16], an extensive set of experiments evaluating the LAMP framework in high-fidelity Gazebo simulations, and results of the LAMP framework in physical experiments. These latter experiments also evaluate the robustness of the proposed method in presence of localization and mapping errors. We have also revamped and improved the presentation of the discrete algorithm, and have now formally proved several key properties.</p>
<p>An outline of the LAMP framework is shown in Figure 2. The paper is organized as follows. In Section II, we formulate the problem and in Section III we provide our solution approach in discrete environments, the RPP-Hybrid policy, which is used in the high level planner. In Section IV we provide the implementation of the solution approach for continuous environments, which forms the LAMP Framework component. We present and discuss our simulation and experimental results in Section V.</p>
<p>II. PROBLEM FORMULATION</p>
<p>In this section, we formulate the problem of executing a sequence of T navigation tasks in an environment with uncertainties.</p>
<p>A. Environment Model</p>
<p>Consider a robot executing a sequence of T ≥ 1 start-togoal tasks. The environment is represented as an undirected graph G = (V, E), where V is the vertex set, E is the set of edges between the vertices and let c : E → R + be a function assigning the cost of traversal to the edges. A path P in the graph is defined by a sequence of vertices v 1 , . . . , v k that satisfies (v i , v i+1 ) ∈ E for all i ∈ N k−1 . With a slight abuse of notation, we denote the cost of executing a path P by c(P ) =
k−1 i=1 c(v i , v i+1 )
, and we denote the cost of the minimum cost path between v, u ∈ V by c(v, u).</p>
<p>For each task t ∈ N T , there is a set of unblocked edges, which is a subset of E; edges not in this set are blocked and cannot be traversed. This set can be different for each task t. There are m = 2 |E| edge subsets of E. Let the complete set of subgraphs of G be denoted by G = {G 1 , . . . , G m }, where G i = (V, E i ) and E i ⊆ E. Thus, for every task t, the robot is operating in a subgraph drawn from G. The robot knows G, but it does not know which subgraph it is in.</p>
<p>Mathematically, the robot operates in a sequence of T random graphs G X1 , . . . , G X T , where X 1 , . . . , X T are independent and identically distributed random variables according to an unknown probability mass function (pmf) p 1 , . . . , p m . That is, the probability that subgraph t is G i is given by P(X t = i) = p i . We let G Xt denote the random subgraph for task t, and G xt its realization. When referring to the pmf, we drop the index and use random variable X.</p>
<p>We are interested in environments that exhibit a form of structured uncertainty in their traversability, which can be learned over time. By structured, we mean that absence or presence of particular obstacles for a given task t exhibits some correlation (i.e., if the loading area of the warehouse is not traversable, then it is more likely that the packing area will also not be traversable). In our model, this corresponds to the case where only a small subset of G dominates the unknown pmf.</p>
<p>B. Robot Observation Model</p>
<p>Consider a robot positioned at v ∈ V that wishes to traverse (v, u) ∈ E. The robot is not aware of the current realization G x = (V, E x ) of the environment. Therefore, the robot must determine if the edge (v, u) is traversable by observing the state (i.e. if it is blocked and thus not traversable). We assume that the robot is capable of sensing the edges incident to its current vertex v. Let I v ⊆ E be the set of edges incident to v, then the sensing action at v is a mapping γ v : I v → {blocked, unblocked} where γ v (e) = unblocked for e ∈ E x and γ v (e) = blocked otherwise. With this observation model, we define the robot state as a tuple (v, E b , E u ) where v is the vertex occupied by the robot and E b , E u ⊆ E are the sets of observed blocked and unblocked edges, respectively. For simplicity, we assume that the robot does not incur a cost to observe the state of an edge, however, the cost of observation can be added without significant change to the problem formulation or solution.</p>
<p>C. The Learned Reactive Planning Problem</p>
<p>Given the environment setting and the robot observation model in the environment, the goal is to find a policy that outputs a sequence of actions for each realization of the environment and drives the robot towards the goal state. In more detail, a policy is a mapping from each robot state (v, E b , E u ) to one of the following actions: 1) traverse an edge in I v ; 2) observe an edge in I v ; or 3) terminate. In this work, we are interested in a class of policies for the navigation problem, namely complete policies.</p>
<p>Definition II.1 (Complete Policy [15]). Consider a graph G with start v s and goal v g , along with its complete set of subgraphs G. A policy π is complete if for every subgraph G i ∈ G it produces a finite sequence of actions that either reaches the goal state, or correctly determines that the goal cannot be reached in G i . Given a graph G i , let A π Gi = a 1 , . . . , a l be the sequence of actions produced by a policy π. With a slight abuse of notation, we denote the cost of an action a with c a (a) where the observation and termination actions have zero costs and the cost of a traversal action across (u, v) is given by c(u, v). Then the total cost of executing A π Gi is cost(A π Gi ) = l i=1 c a (a i ). Therefore, the expected cost to complete a task under policy π is
E X cost(π)] = i∈Nm P(X = i)cost(A π Gi ).
We consider a sequence of T tasks where the goal is to minimize the total expected cost of executing the tasks. In the execution of task t, the robot may use the information collected during tasks 1, . . . , t−1 on the traversability of the different regions of the environment. Formally, we define this as the Learned Reactive Planning Problem.</p>
<p>Problem II.2 (Learned Reactive Planning Problem (LRPP)). Given a graph G with unknown pmf over all subgraphs G, a start and goal v s , v g ∈ V and number of tasks T , find a sequence of T complete policies, π 1 , . . . , π T , that minimizes T t=1 E X cost(π t )], where π t may depend on the observations made in tasks 1, . . . , t − 1.</p>
<p>The following result provides some insight into the complexity of the Learned Reactive Planning Problem. Proposition II.3. Suppose that the robot has completed T − 1 tasks, which is sufficiently large to have learned a function that can return the probability of any subgraph in polynomial time. Then, the problem of computing the policy π T that minimizes E X T cost(π T )] is PSPACE-hard.</p>
<p>Proof. Consider an instance of the stochastic Canadian Travelers problem (CTP). This consists of a graph G CTP = (V, E), a cost on each edge c CTP : E → R &gt;0 , and a probability for each edge p : E → [0, 1], giving the probability p(e) that the edge e ∈ E is unblocked. The goal is to find a policy that minimizes the expected cost from start to goal. To reduce this problem to the T th task in the LRPP, we create the following instance with a known pmf: We set G = G CT P , c = c CTP , and we letp(i) denote the learned function that for each subgraph
G i = (V, E i ) ∈ G, returns its probability P(X t = i) = p i as p(i) = p i = e∈Ei p(e) e∈E\Ei 1 − p(e) .
For each i, this function returns a probability in polynomial time, as it requires computing the product of |E| numbers. Thus, the reduction from CTP to task T of the LRPP with a learned function can be performed in polynomial time.</p>
<p>An optimal policy π T for task T minimizes E X T cost(π T )], and thus this policy is also optimal for the stochastic CTP. Since the stochastic CTP is PSPACE-hard [24], the problem of computing the optimal policy for the T th task is also PSPACE-hard.</p>
<p>Note, the above result characterizes the complexity of only one task in the LRPP, and only in the case when we have access to a function that can return the probability of any given subgraph. For problems with an unknown input such as the general LRPP, notions of complexity are considerably more subtle; see for example advice complexity [25] or the complexity of trial-and-error [26]. However, we believe the above result is sufficient to demonstrate the complexity of the proposed problem.</p>
<p>D. A Special Case of LRPP</p>
<p>A special case of the LRPP was formally posed by MacDonald et al. [15] as the Reactive Planning Problem (RPP) where there is a single navigation task (i.e., T = 1) and the pmf over X is known. It is assumed that only a small subset G ⊂ G of the subgraphs have non-zero probability.</p>
<p>The proposed policy in [15] for the RPP tries to balance the act of taking observations (known as exploration) and moving towards the goal (known as exploitation). At each step, the policy finds the next observation vertex that minimizes a product of the expected entropy of taking an observation and a linear combination of the travel cost to the observation vertex, the cost of observation and the expected cost from the observation vertex to the goal. The proposed policy is complete and is represented as a decision tree where Authors in [15] show that a complete policy exists for all start-to-goal navigation tasks in undirected graphs.</p>
<p>For more details on the RPP and the solution outlined in this section, refer to [15]. For the purposes of this paper, the reader only needs to understand that by providing 1) an undirected graph G, 2) the start and goal vertices, v s and v g , 3) a set of subgraphs G , and 4) a corresponding pmf over the subgraphs, a solution to the RPP will return a complete policy for G .</p>
<p>III. LRPP SOLUTION APPROACH</p>
<p>There are three key challenges to address when considering the approach in [15] to solve the LRPP. First, in the LRPP the pmf over the subgraphs is unknown. To address this, in Section III.A, we propose a Map Memory Filter to add a new map M t to the set of collected maps M t and estimate the pmf. Second, while executing a task, the robot might encounter an environment that it has not experienced before. To handle such situations, we introduce the idea of switching to an optimistic policy in Section III.B. Finally, the goal is to minimize the expected cost of executing a sequence of tasks by learning a model of the environment. In Section III.C we explain how the policy generating algorithm proposed by [15] can be utilized to generate and update a complete policy π t based on changes to the estimated pmf. Figure 3 summarizes our proposed solution to the LRPP.</p>
<p>A. Map Memory Filter</p>
<p>After the robot performs n actions within the environment, let E t,n ⊆ E denote the set of edges with a known state. We define the robot's understanding, or map of G xt , after its nth action, as the tuple M t,n = (E b t,n , E u t,n ) with known blocked edges E b t,n = {e ∈ E t,n |e ∈ E xt } and known unblocked E u t,n = {e ∈ E t,n |e ∈ E xt }. Note that E b t,n and E u t,n form a partition of E t,n . When the task is finished, the robot stores the map in the list M t = [M 1 , . . . , M t ], where n is removed to indicate the task is completed.</p>
<p>With the map defined, we introduce a filter to reduce the amount of map information stored in memory and subsequently the search space of the policy construction algorithm. We only store maps that do not agree, which means that each stored map corresponds to a different realization of G. A formal definition of the map agreement is given below. Definition III.1 (Map Agreement). Given maps M 1 and M 2 , Note that the robot does not need to know the whole environment to accomplish its task and can leave regions unmapped, which may result in different realizations seeming identical to the robot.
we say M 2 agrees with M 1 if E b 2 ∩E u 1 = ∅ and E u 2 ∩E b 1 = ∅.
To minimize the number of maps stored, we introduce a compact representation of maps that agree with each other, which we call super maps.
Definition III.2 (Super Maps). A map M j with j ∈ N t is a super map if all M i for j = i ∈ N t that agree with M j satisfy E b i ⊆ E b j and E u i ⊆ E u j .
The problem of computing a minimal set of super maps can be formalized as follows. We define merging a set of maps that are in agreement with each other as the union of the blocked and unblocked edges in the maps. Formally, let M i and M j be two agreeing maps, then the map obtained from merging them is
(E b i ∪ E b j , E u i ∪ E u j ).
Observe that merging the maps in a subset forms a super map, and thus the solution to the Map Merging Problem provides a compressed representation of the robot's past experiences. We redefine M t as the set of super maps at the end of task t.</p>
<p>Prior to providing our algorithm for the Map Merging Problem, we show that this problem is NP-hard by a reduction from the Minimum Graph Coloring (MGC) problem. Given a graph, the MGC problem is the problem of assigning a color to each vertex such that no two incident vertices have the same color and the total number of colors is minimized. The MGC problem is a well-known NP-hard problem [27].
Lemma III.4. The Map Merging Problem is NP-hard.
Proof. Consider a graph G = (V, E) as an instance of the MGC problem. For each vertex v ∈ V , we construct a map M v such that if (u, v) ∈ E then M u and M v do not agree with each other. The construction of these maps is given below. Consider a complete graph G = (V, E ) as the base map, then we have: 
(i) For all u ∈ V and for all w, v = u, the edge (w, v) ∈ E is an unknown edge in the map M u , (ii) for all (u, v) ∈ E, the edge (u, v) is unblocked in both M u and M v ; and finally (iii) for all (u, v) ∈ E, the edge (u, v) is blocked in M u and is unblocked in M v . Let S = {S 1 , . . . , S k } beif (u, v) ∈ E. Hence, for all M u , M v ∈ S i , the edge (u, v) ∈ E.
Therefore, the corresponding vertices in each subset do not share any edge and these vertices can be colored with the same color. Therefore, the graph G can be colored with k colors. Now suppose there exists a coloring of G with k &lt; k colors, then by the construction of the Map Merging Problem instance, there is a partition S of maps with size k such that the maps in each subset of the partition agree with each other. This is a contradiction.</p>
<p>Observe that the maps in the set M T are added sequentially with each task execution and so the online map merging problem is a form of online graph coloring [28]. Algorithm 1 details our map merging method, which is a greedy approach that immediately adds a map to a subset via merging if it agrees with an existing super map. It is analogous to the First Fit approach to the online graph coloring problem, which, while not an approximation algorithm [29], provides good performance in practice. The algorithm takes the new map M t and the previous super maps M t−1 as inputs. The function AGREEINGMAPS finds the subset of super maps in M t−1 that agree with M t according to Definition III.1. If there is no such super map, then we add the new map to the set of super maps (Line 3). If the set of agreeing super maps S is not empty, then the function BESTMAPTOMERGE finds the best super map in S to merge M t with. The simplest definition of this function is to merge M t with the first agreeing map in S. A more sophisticated version of this function is to find the super map that adds the minimum number of additional blocked edges to the super map upon merging, i.e., arg min
Mj ∈S |E b j ∪ E b t | − |E b j |.
If there are multiple minimizers, break the tie by randomly selecting one of them. Observe that large numbers of blocked edges in the super maps may result in conservative policies by the robot in future task executions. Therefore, the method proposed in BESTMAPTOMERGE favors exploration by greedily minimizing the number of blocked edges in the collected super maps.
Algorithm 1: MAPFILTER Input: M t , M t−1 Output: M t 1 S ←AGREEINGMAPS(M t−1 , M t ) S ⊆ M t−1 2 if S = ∅ then 3 return M t−1 ∪ M t 4 M j ←BESTMAPTOMERGE(S, M t ) M j ∈ S 5 M j ←MERGEMAPS(M j , M t ) 6 return M t−1
Using this method of storage, the expected cost estimate of each policy can be calculated as
E X [cost(π t )] = Mj ∈Mt n j + 1 t cost πt (M j ) ,(1)
where n j is the number of maps the robot has experienced by task t that have been merged with super map M j and cost πt (M j ) is the cost of executing policy π t in super map M j . Thus the estimated probability of encountering M j in the next task isp Mj = (
n j + 1)/t. Let P = [p M0 ,p M1 , . . . ,p Mt ] where M j ∈ M t for all j in N T , forming our estimate of the pmf of G. Note that M 0 = {(∅, E)
} and initialize n 0 = 0, leading to the robot's initial assumption that all edges in G are unblocked.</p>
<p>B. Switching to an Optimistic Policy</p>
<p>Recall that an optimistic policy plans the shortest possible path from the robot's current location to the goal given the known map, assuming all unknown edges are unblocked. If the move cannot be completed because of an unexpected blocked edge, then it recomputes and follows the shortest path in the updated map. Formally we define an optimistic policy as follows: Definition III.5 (Optimistic Policy λ). An optimistic policy λ computes a sequence of move commands online to lead the robot to the goal state. Such a policy must be complete.</p>
<p>For a given task t, the robot starts by following the preplanned paths in the RPP policy π t until either 1) an obstacle prevents the robot from continuing (in which case the robot is in a new map) or 2) all super maps that are consistent with the robot's observations have no path to the goal v g . In either case, the robot switches to λ to finish the task. This satisfies the complete policy requirement, and the policy is updated each time a new task is completed. The preplanned paths are expected to be more efficient at reaching v g than λ, and as such the probability of the robot switching to λ should be minimized. This policy will be referred to as the RPP-Hybrid policy.</p>
<p>C. Policy Construction and Update</p>
<p>A policy for task t can be constructed as a binary tree π t = (N, L) by using the super maps M t−1 and estimated pmfP t−1 collected thus far by the robot as inputs into the RPP problem. The nodes N of the tree are given by tuples
(Y, v, e) for belief Y = {i ∈ M t−1 |M i agrees with M t,n } at vertex v ∈ V .
The edge e is an observation at vertex v.</p>
<p>For each node, L contains a path from the parent node to the current node. There are two possible outcomes for each observation, one corresponding to e being unblocked, i.e., e ∈ E xt and the other corresponding to e being blocked, i.e., e ∈ E xt . If e = ∅, then either v = v g or there is no path to goal in any of the agreeing super maps, resulting in an incomplete policy. The policy can be made complete by augmenting it to allow the robot to switch to the optimistic policy for the remainder of the task. To use M t as an input to the RPP solution, it first needs to be converted into a set of edge subsets, where each subset represents the passable edges in each super map. Since each super map in M t may only be a partial representation of a realization, it is necessary to make some assumptions to fill in missing information. If the state of an edge in super map M j is unknown (i.e., e / ∈ E b j and e / ∈ E u j ), we assume it to be unblocked. Formally, the edge subset for M j will be
E u j ∪ (E \ (E u j ∪ E b j )
). This choice encourages the robot to explore, as it will attempt to traverse an unknown edge if it is beneficial.</p>
<p>For example, consider the scenario in Figure 5. Assuming the robot only has an empty grid for M 0 in M t , it attempts to execute the task in (a) using the green path. However, it must switch to the optimistic policy, and takes the blue shaded path. The robot stores the map from this execution as M 1 (see Figure 5b), where the state of the grey squares are unknown. When building the policy, an observation for the edge ((1,1),(2,2)) will be selected, and in the case this edge is blocked, a path will be calculated from s to g in M 1 since no other agreeing maps exist. If only the partial map M 1 was available, the blue shaded path would be used in the policy. However, since we are assuming the grey squares are unblocked, the algorithm will select the green path in (b). Even if that assumption was proven wrong during task execution it will result in more knowledge of the realization, and the next time the policy is built, the algorithm will not repeat the same path for that particular super map.</p>
<p>Finally, we present our entire solution in Algorithm 2, which covers task execution and policy construction. In Line 1, we initialize the set of super maps M t with E as a set of unblocked edges. In other words, the robot is aware of all edges that it could potentially move across. Such information could come from a floor plan of the environment, containing all permanent obstacles. The robot initially assumes that all edges are unblocked. This assumption ensures that the optimistic policy λ will always initially attempt the shortest possible path to v g . In line 3, the policy π t is constructed by the RPP algorithm using the set of super maps M t , which contains the edge subsets, and the estimated pmfP . In lines 5 − 8, the robot executes the task by following the policy π t constructed by the RPP algorithm until it reaches a terminal state, updating its set of super maps, along with the estimated pmf and edge subsets, in lines 9 and 10 before executing the task again.
Algorithm 2: SEQUENTIALTASKCOMPLETION Input: G = (V, E), v s , v g 1 M 0 = [(E, ∅)] 2 for t = 1, . . . , T do 3 π t = BUILDPOLICY(G, M t−1 ,P t−1 ,v s ,v g ) 4 Initialize state R t,n = (v s , ∅, ∅) for n = 0; 5 while R t,n not terminal do 6 Execute π t (R t,n ) if switched policies, wait until λ terminates 7 Update R t,n 8 Increment n 9 M t = (E b t,n , E u t,n ) from R t,n 10 M t = MAPFILTER(M t ,M t−1 ) 11
UpdateP t</p>
<p>IV. LAMP -FROM THEORY TO APPLICATION</p>
<p>In this section, we discuss our approach in applying the solution outlined in Section III to realistic scenarios. First, we discuss how the robot interprets its environment as a graph, given a floor plan. Second, we introduce an edge resolving algorithm for determining whether an edge is traversable or not, given an updated costmap. Third, we discuss how the robot executes the RPP-Hybrid policy in a real-world setting where some of the assumptions from the LRPP no longer apply.</p>
<p>A. High Level Idea</p>
<p>The architecture of our solution, the Learn a Motion Policy (LAMP) framework is shown in Figure 2. The framework is meant to be added to a standard navigation stack. Although the navigation stack in Figure 2 is based on the ROS navigation stack [30], others have a similar structure [31].</p>
<p>The edge observer takes the costmap and the estimated robot's pose from the localization system to resolve edge states (i.e. determine if they are blocked or unblocked). The edge states are given to the high level planner, which uses it to build maps of the environment. The high level planner then constructs a policy between tasks and implements them during task execution by instructing the path planner on which vertex to go to next.</p>
<p>The local planner is responsible for planning and executing a kinematically feasible path from the robot's current position to the next vertex dictated by the high level planner. There are a myriad of solutions available for these lower level components of a standard navigation stack, and their solutions will not be discussed in this paper.</p>
<p>B. Interpreting the Environment</p>
<p>Since we are concerned with blocked edges that may significantly alter the route the robot may take, we propose converting a base occupancy grid (taken from a scaled floor plan where only permanent obstacles are marked) into a hybrid topological-metric map. A hybrid map combines a topological graph and an occupancy grid. For more details on the reasoning behind this choice of map structure, please refer to Section 5.2.1 of the thesis written by Tsang [32]. Constructing a topological graph from an occupancy grid is not a new idea; algorithms have been proposed by Thrun [33], Liu et al. [34] and Blöchliger et al. [35]. While we will not be proposing a new algorithm for converting an occupancy grid into a graph, we will briefly cover the process and borrow some terminology from [35].</p>
<p>The base occupancy grid is decomposed into regions of free space, which we will refer to as submaps. For example in an indoor environment, rooms and hallways would become regions. If rooms or hallways are divided into multiple regions, our approach is still valid. In Figure 6, these submaps are represented by different coloured areas. The submap boundaries where the robot can cross from one region to another are referred to as portals (or critical lines in [33], [34]), represented by the white rectangles in Figure 6. The topological graph is referred to as a navigation graph in this work. The navigation graph is a weighted, undirected graph and is defined by G = (V, E) with a cost on each edge c : E → R ≥0 . The vertices and edges are defined by the following:</p>
<p>• Vertices are the center points of each portal. Thus, each v ∈ V represents a point v ∈ R 2 . Additional vertices that represent an area of interest (ex. start, goal) can be added to the graph. • Edges are abstract representations of the connectivity between vertices. If there exists a path from one portal to another in the base occupancy grid without crossing any other portals, then we add an edge between the two portals. We initialize the cost with the minimum distance between the two vertices. We use this navigation graph by instructing the robot to travel to the spatial coordinates of the portals corresponding to the vertices dictated by the policy. Localization is performed using existing algorithms such as AMCL [36] on the base occupancy grid. These algorithms are robust and can still localize reasonably well in the presence of unexpected obstacles. The robot will store the base occupancy grid, submap divisions, and the supermaps, which are subgraphs of the navigation graph. This allows the robot to remember changes in the environment that cause an edge to no longer be traversable, rather than storing a different occupancy grid for each new configuration. This means that changes to the environment that do not significantly alter the path a robot would take to reach the goal are not stored in memory. For ease of explanation, we represented a map M as the tuple (E b , E u ). However, in practice M is stored as an adjacency table representing a graph with edges marked as blocked, unblocked, or unknown. Therefore, we will also use M to represent the navigation graph induced by (E b , E u ).</p>
<p>One concern is the possibility of creating a multigraph where multiple edges share the same endpoints, such as in Figure 7. While this scenario does not need to be avoided, if an implementation allows for the construction of a navigation multigraph, then each edge will need an explicit label instead of representing it with the endpoints (u, v) u, v ∈ V , as is done traditionally. This enables the robot to differentiate the edges during planning and traversal.</p>
<p>C. Resolving Edges</p>
<p>The robot requires a method to determine if an edge is blocked or unblocked, i.e., to resolve the edge. The up-todate costmap and the current location of the robot can be utilized to update the edge states of the navigation graph during task execution. An edge can have one of three possible states: blocked, unblocked, or unknown. Resolving an edge means to set the state to be blocked or unblocked. The edges of the navigation graph are set to be unknown at the beginning of every task. While the robot is guaranteed to resolve an edge when it has traversed the entire length of the edge, a few properties of a 2D environment can be exploited to resolve an edge earlier. Figure 8 shows a robot trying to traverse 3 different edges. The area that is observed by the robot is shown in yellow, and areas that the robot has not observed are shown in white, with obstacles that the robot observes and cannot traverse shown in black. In (a), the robot can see a path from u to v, and so we can resolve the edge to be unblocked. In (b), the robot can see that there is no path from u to v in the submap, so we can resolve the edge as blocked. In (c), there may or may not be a path to goal in the unobserved part of the submap so the state of the edge is unknown. Formally, let the true free space be denoted by C where C ⊆ R 2 . This is the area of the environment the robot can occupy without colliding with an obstacle. We assume a disc-shaped robot with a disc-shaped safety region so that orientation does not have to be considered. We also assume C does not change throughout the task execution. Finally, C is hidden from the robot.</p>
<p>Let the known free space be denoted by C k where C k ⊆ C. This is the area of the environment the robot has observed to be free space since the beginning of the task. Let the unknown space be denoted by C u where C u ⊆ R 2 . This is the area of the environment that the robot has not observed yet, but is marked as free space by the base occupancy grid. Note that C u ∩ C k = ∅. Finally, let the optimistic free space be denoted by C o where C o = C k ∪ C u . This is the area of the environment the robot believes it can occupy, assuming unknown space is also free. We will also assume that C ⊆ C o , which means obstacles in the base occupancy grid will be assumed to always exist.</p>
<p>Let S denote a submap, then C(S), C k (S), C u (S), C o (S) is the true free space, known free space, unknown space, and the optimistic free space respectively, in submap S. Let x R ∈ R 2 be the current location of the robot. Now a formal definition of an unblocked and blocked edge can be given: Definition IV.1 (Unblocked and Blocked Edges). Given an edge (u, v) ∈ E where u, v ∈ V , and S(u, v) ⊂ R 2 is the submap associated with edge (u, v), the edge is unblocked if and only if there exists a path, P ⊆ C(S(u, v)) from u to v. Otherwise the edge is blocked. Lemma IV.2. An edge (u, v) is unblocked if there exists a path from u to v in C k (S(u, v)).</p>
<p>Proof. Let P (u, v) denote the path in C k (S(u, v)). Since C k (S(u, v)) ⊆ C(S(u, v)), then it follows that P (u, v) ⊆ C (S(u, v)).</p>
<p>Lemma IV.3. An edge (u, v) is blocked if there does not exist a path from u to v in C o (S(u, v)).</p>
<p>Proof. By the definition of an unblocked edge, suppose there is a path P (u, v) ⊆ C(S(u, v)) but there does not exist a path from u to v in C o (S(u, v)). This implies there exists (S(u, v)). But that is a contradiction since C(S(u, v)) ⊆ C o (S(u, v)). Therefore, (S(u, v)).
x ∈ C(S(u, v)) where x / ∈ C oP (u, v) ⊆ C o
Algorithm 3 exploits these properties to resolve edge states. This algorithm is continuously run at a user-specified rate because C k (S), C u (S) and C o (S) are dependent on the time passed since the beginning of the task. In practice, the maximum rate of the algorithm depends on the computation time of the algorithm used to find a path in lines 7 and 9.</p>
<p>To avoid checking edges that are not immediately relevant to the robot, an edge has to meet at least one of the following conditions to be observed (line 4):</p>
<p>(i) The edge is in the same submap that the robot is currently positioned in. (ii) At least one of the endpoints of the edge are within the maximum observation range of the robot at x R . The maximum observation range of the robot is a circle with a radius of r max , which is based on sensor parameters and costmap implementation.</p>
<p>Condition 1 forces the algorithm to check the states of all the edges in the current submap. If this condition was not in place, the robot would not resolve edges in submaps that were larger than the maximum observation range. However, if the submap is smaller than the maximum observation range, then only checking edges in that submap would be short-sighted if the robot is capable of resolving edges in other submaps that are close by. Condition 2 takes advantage of the robot's maximum observation range if possible. Line 5 checks if either of the endpoints of edge (u, v) are in an obstacle; if they are in an obstacle, then the edge is marked as blocked. In chapter 5 of [32], an extension of edge resolution is presented to be more robust by considering the entire length of a portal instead of just the center point.</p>
<p>Algorithm 3: edgeResolver
Input: C k , C u , r max , x R 1 C o = C k ∪ C u ; 2 S curr = CURRENTSUBMAP(x R ); 3 for each (u, v) ∈ E do 4 if ||x R − u|| 2 ≤ r OR ||x R − v|| 2 ≤ r OR S(u, v) = S curr then 5 if u or v / ∈ C o (S(u, v)) then 6 Set (u, v) as blocked; 7 else if there exists P (u, v) ⊆ C k (S(u, v))
Line 7 exploits Lemma IV.2 to set (u, v) as unblocked. In line 9, a path planner is run on C o (S(u, v)) to find a path from u to v. If no path is found, then the edge is blocked. When selecting a path planner, it only needs to return if a path exists or not, thus optimality is not a concern. Minimizing the computation speed should be the primary objective. Planners that use heuristics (such as A*) to speed up computation are good options.</p>
<p>If a path is found, but u and v do not satisfy the conditions in lines 5 or 7, this implies the robot still does not know whether the edge is blocked or unblocked, since the path could be blocked by an unseen obstacle, as illustrated by (c) in Figure 8. Then edge (u, v) cannot be resolved and the algorithm moves on to checking the next edge.</p>
<p>Edges resolved during task t are used to update the map
M t = (E b t , E u t ).
Computationally fast path planning is crucial to running edge resolver at the desired rate. Since the robot's current location is an input to Algorithm 3, if the path finder (lines 7 and 9) is slow, it can result in edges not being resolved because the robot has moved far away by the time Algorithm 3 is re-run. Large submaps may also slow down the the rate of the edge resolver as most path finders will take longer to run on large maps.</p>
<p>D. Policy Execution</p>
<p>The RPP-Hybrid policy returned in Section III assumes that the robot can resolve an edge at an edge endpoint. As the previous subsection has shown, this may not always be true. As a result, minor modifications need to be made to the policy which can be applied during the policy execution.</p>
<p>In the RPP-Hybrid policy, when observing an edge (ex. (v, u)), the leg ends at one of the edge's endpoints (ex. v). To ensure that the robot completes the observation (i.e. resolves the edge), u needs to be appended to the leg. The robot may or may not arrive at u before completing the observation. After the observation is completed, the robot can return to v or if the next leg in the policy begins with the edge (v, w), it can set w as the next vertex to move to.</p>
<p>When moving from one vertex to the next, the robot may not be at a vertex when it discovers an edge is blocked, so care must be taken when recalculating a path for the optimistic policy. Suppose the robot is traversing edge (v, u), and discovers it is blocked. We first insert a temporary vertex v R containing the robot's current position x R into the navigation graph. Second, a temporary edge is inserted for each edge from v to another portal in the same submap that is not blocked. An edge is also added to v. The cost of each edge is the euclidean distance to x R , i.e., ||x R − u|| 2 . Finally, we run the shortest path algorithm in the modified graph with v R as the starting vertex and v g as the goal.</p>
<p>V. RESULTS</p>
<p>In this section, we evaluate the performance of the RPP-Hybrid policy and the LAMP framework in the following experiments: 1) the RPP-Hybrid policy is simulated in a discrete environment and compared with the existing UCT-CTP algorithm [14] and the factor graph approach described in [16]; 2) LAMP is simulated in a continuous environment with a real navigation stack to verify that our assumptions hold; and 3) LAMP is implemented in a scenario with a real robot, demonstrating the performance in real-world conditions such as non-ideal localization. The implementation of the RPP-Hybrid policy used in the experiments uses the simple approach to map merging where new maps are merged with the first agreeing super map. We have included a supplementary video showing trials from the LAMP simulations and the real robot experiment. This will be available at http://ieeexplore.ieee.org.</p>
<p>A. RPP-Hybrid Simulation Results</p>
<p>To evaluate the RPP-Hybrid policy, simulations were run and the performance of RPP-Hybrid was compared to a purely optimistic planner, an implementation of the UCT-CTP algorithm using independent edge probabilities, and the factor graph approach described in [16]. The optimistic planner is an implementation of A* that initially assumes all edges are unblocked and updates the map and replans when it encounters a blocked edge, with no information saved between each trial. The implementation of UCT-CTP keeps count of the number of times it has observed each edge in either state and calculates independent probabilities for each edge. The algorithm utilizes these probabilities to compute a new policy before each task. The factor graph approach maintains a compact approximation of the joint probability mass function of the edges, and uses rollouts similar to UCT-CTP to generate a path. In these experiments, we only use the observations along the robot's path to update the robot's model of the environment between tasks. The UCT-CTP approach, factor graph approach, and the optimistic approach are all calculated online. The RPP-Hybrid policy is computed offline (i.e., prior to each task execution), however an online version of the policy is described in [32].</p>
<p>The environments for these simulations are graphs that were manually drawn from the floor-plan of a grocery store and are shown in Figures 9 and 10. Different realizations of Environment 1 are given in Figure 9 and different realizations of Environments 2 and 3 are given in Table I. Each algorithm was run in 100 trials of 100 tasks each and was given the same sequence of realizations, which was obtained by randomly selecting a realization for each trial with uniform probability.</p>
<p>In these experiments, we consider the optimal cost to be the cost of the shortest path in the given realization. The average costs compared to the optimal are shown in Table II, as well as the average cost of the last 10 trials to show the performance each algorithm converged to. Figure 11 shows the average cost of each policy in Environment 1 as the number of tasks completed increases.</p>
<p>The UCT-CTP policy with independent edge probabilities performs no better than the optimistic policy in all envi- Fig. 9. Realizations of test environment 1 that the robot may operate in to traverse from start to goal, shown in green and red respectively (a) Test environment 2 (b) Test environment 3 Fig. 10. Additional test environments. Coloured edges are blocked in different realizations according to Table I.   Fig. 11. Cost of optimistic, UCT-CTP, factor graph, and RPP-Hybrid policy relative to the optimal cost in Environment 1. ronments, and is not able to make significant improvements to its policy as it completes tasks. The Factor Graph approach, although approximating the correlation between the obstacles, is initially outperformed by the simple policies like the optimistic policy and UCT-CTP which ignore the correlations in the environment. This method eventually provides better solutions as it completes more tasks and builds a better model of the environment. Similar behaviour of the Factor Graph approach is shown for different environments in [16]. The RPP-Hybrid approach outperforms all the policies in every environment. In Environment 1, it performs 89% better than the optimistic and UCT-CTP approaches, and 78% better than the Factor Graph approach. Also, its performance does not suffer in the first few trials as it learns the environment. Observe that the solution provided by the RPP-Hybrid policy approaches the optimal cost in Environments 2 and 3 and near optimal cost in Environment 1 with a sufficient number of task executions.
(a) (b) (c) (d) (e) (f)
One shortcoming of the UCT-CTP algorithm, factor graph approach, and the RPP-Hybrid policy is that with incomplete observations, they can converge to a sub-optimal policy where the cost of taking informative observations that would improve the policy is too high according to the model of the environment that the robot has. The UCT-CTP algorithm  [18] and the factor graph approach, both of which use rollouts, are particularly susceptible to this. Another weakness of the factor graph approach in this scenario is that it is loosely structured and lacks mechanisms to prevent excessive exploration during the earlier trials, and to purposely exploit known correlations in later trials. The RPP-Hybrid approach switches to an optimistic policy when it encounters a previously unseen environment, allowing its worst-case performance to stay close to that of the optimistic policy. The runtime of each algorithm is primarily dictated by its number of shortest-paths calculations. The optimistic policy only performs this calculation when it observes a blocked edge on its current path, and was able to complete the 10000 tasks in Environment 1 in under a minute. The RPP-Hybrid policy performs this calculation once for every super map, and thus the runtime scales with the number of supermaps collected. In our experiments, since the number of supermaps remains low, the runtime is in the same order of magnitude of the optimistic policy, completing the same tasks in approximately 8 minutes. The UCT-CTP algorithm and factor graph approach compute a shortest path for each rollout, which presents a tradeoff: with the number of rollouts set in the 50-100 range, the algorithms can be implemented online. However, the authors of the original UCT-CTP [14] algorithm note that performance continues to improve past 1000 rollouts, beyond what is feasible on a modern desktop CPU. With the number of rollouts set to 100 in our experiments, the two algorithms were each able to complete the 10000 tasks for Environment 1 in approximately 10 hours.</p>
<p>B. LAMP Simulation Results</p>
<p>An office environment was constructed and simulated in Gazebo. The navigation graph used for policy generation was manually constructed such that it is not a multigraph. All of the code for the policy generator and edge observer is in Python 2.7, which is integrated as a package for ROS Kinetic. The Visilibity library [38] was used to estimate the known free space for the edge observer. The LAMP framework was built on top of the ROS navigation stack, which already includes all the components in the standard navigation stack in Figure 2. In the ROS navigation stack, the path planner has two components for planning and executing trajectories, namely global planner and local planner. Table III outlines which ROS packages and therefore which algorithms were used for each component of the navigation stack in our implementation. Our high level planner uses the move base action library to interact with the ROS navigation stack. The simulations were run on a PC with a 4.2 GHz Intel Core i7 processor with 32 GB memory and a GeForce GTX 1060 GPU with 6 GB of VRAM, using the ROS Gazebo simulator. The robot platform is a Clearpath Jackal (see Figure 1a), a small differential drive unmanned ground vehicle (0.5m × 0.5m footprint) that has been equipped with a SICK LMS111 LiDAR. Figure 12 depicts a topdown view of the environment the robot operated in and Table IV shows the probability of edge blocking obstacles appearing. Each column of obstacles in Table IV is perfectly correlated, meaning either all the obstacles in the column are present, or none of them are present. If an obstacle appears on a portal, a white barrier blocks the portal, as shown in Figure 12b. If the region i is to have an obstacle appear, then two rectangular obstacles can appear anywhere in the submap, but in the same orientation as in Figure 12b. In addition, to simulate scattered debris, up to 10 blue cylinders will spawn in random locations throughout the environment that will not affect the traversability of submaps, but will require the local planner to react. The occupancy grid was converted into a graph with 16 vertices and 33 edges. The uncertainties in Table IV resulted  in 32 possible configurations for the test environment, not accounting for all the different configurations the blue cylinders could be in.</p>
<p>The robot is tasked with repeatedly going from s to g. We ran 10 trials with 100 task executions each. A selection of these trials are shown in the supplementary video. The environment configuration for each execution was randomly selected according to the probabilities in Table IV. We tested three policies for each trial: Simple, Optimistic, and RPP-Hybrid. The Simple policy does not use the navigation graph, we simply send the goal position g to the navigation stack and the path planner guides the robot to the goal. This is the baseline we compare our policies to. The reasoning is that this is available out-of-the-box by ROS and presumably many robots will use this method of navigation. Figure 13 shows the average distance travelled by the RPP-Hybrid policy after performing a certain number of task executions, as a percentage of the distance travelled by the simple policy. The results show that the RPP-Hybrid outperforms the optimistic and the simple policy by 20−30% and the mean performance improves monotonically over time. Note that the high-level policies for the simple policy and the optimistic policy are similar, however, the slight difference in their performance is due to small differences in distances in the costmap used by the simple policy and the navigation graph used by the optimistic policy. Figure 14 shows the rate of switches to the Optimistic policy decreasing exponentially as the number of completed task executions increases, which is the desired behavior.</p>
<p>The map filter reduces the number of environment configurations that must be remembered by only storing maps with important differences. Figure 14 shows the number of super maps plateauing as the number of completed task executions increases. In all the trials, the maximum number of super maps that were stored after 100 tasks was 20. Figure 15 contains two examples of different environments where the maps generated by the robot were in agreement, resulting in considerably fewer maps being stored than if we were to compare the occupancy grids directly.</p>
<p>Although minimizing computation time is not the focus of this work, it should be noted that during the simulations, the edge observer was operating at a rate of roughly 3 Hz, or observing 3 edges per second. While this was slower than desired, resulting in some edges remaining unknown when they should have been resolved, the RPP-Hybrid policy still performed better than the Simple policy. However, there is room for optimization and employing a faster path finder than A* would likely increase the edge observation rate.  </p>
<p>C. Robot Experiment</p>
<p>In our physical experiments a Clearpath Jackal was used, similar to the simulation. This was equipped with a Velodyne Puck, a 360 • LiDAR. The environment that the robot operated in is shown in Figure 16, which is an event space in the Engineering 7 building on the University of Waterloo campus. The walls in this space are primarily glass, raising several challenges for localization. The probability of obstacles appearing is shown in Table V. The occupancy grid provided to the robot in Figure 16a was generated by running the Cartographer SLAM algorithm [39] in the empty area, with the walls of the maze later drawn manually.</p>
<p>This test environment has 14 vertices and 35 edges, with 8 different realizations. We ran 2 trials of 10 tasks, comparing only the Simple and RPP-Hybrid policies. The environment realization for each task execution was randomly selected according to the probabilities in Table V. A selection of these trials are shown in the supplementary video.</p>
<p>The distance travelled by the robot in meters for each task is shown in Table VI. Due to localization errors, the robot incorrectly resolved some edges. In the table, we mark trials as having "minor mapping errors" if between 1 and 4 edges were incorrectly resolved. We mark trials as having "major mapping errors" if more than 4 edges were resolved incorrectly. Trial 1 shows the performance of the algorithm when there are few mapping errors during task execution, which results in a 10.4% improvement in the average travel distance with respect to the simple policy. Trial 2 shows the robustness of the proposed algorithm in the presence of many mapping errors due to non-ideal conditions for sensors and localization. Although 7 out of 10 trials were completed with mapping errors, the proposed algorithm still outperforms the simple policy by 5.1%. Note that mapping errors in the initial tasks are more detrimental, as it results in an incorrect model of the environment that must be used in later tasks. The effect of this can be seen in task 6, where the robot experiences no mapping errors, but is given a policy that performs worse than the simple policy. While these mapping errors did happen in the simulation, it was rare and they were not as impactful. Because of the continued COVID-19 restrictions, we are able to only present data collected prior to the initial lockdown, thus the limited number of trials. We hope to perform more trials in future work.</p>
<p>VI. DISCUSSION</p>
<p>An important consideration is how the environment is decomposed into submaps. The size of the submaps will affect the performance of the LAMP framework. Increasing the submap size can decrease the size of the resulting The minor mapping errors in task execution are denoted by * and the major mapping errors are denoted by .</p>
<p>navigation graph and reduce the sensitivity of the learning to changes in the environment. However, doing so increases the robot's reliance on the path planning component to navigate, and increases the computation time of the edge observer. Conversely, decreasing the submap size can reduce the computation time of the edge observer to find a path between vertices, but increases the robot's sensitivity to small changes in the environment, causing it to remember more super maps. As noted in Section V.A, there is a possibility for the RPP-Hybrid policy to become trapped in a suboptimal policy. This can happen when the robot encounters an improbable series of realizations that is not representative of the true pmf of the environment. If the robot's perceived probability of an edge is too low, the expected cost of traversing that edge may be high enough that the policy will never explore that edge again, even if its probability estimate of that edge is poor. To remedy this, one could implement a way for the robot to forget old observations such as using a sliding window of observations for policy construction. This would ensure the robot's observations are consistently updated.</p>
<p>In the robot experiments, there were many challenges with localization that we planned to mitigate given more time. For example, there were some windows that could be blocked to improve LiDAR measurements. Another improvement could be to use SLAM to generate the base map with all permanent obstacles present, rather than an idealized floorplan that lacks the additional obstacle texture and depth details. However, despite these issues, LAMP was still able to learn structure, and exploit it to improve performance.</p>
<p>VII. CONCLUSIONS</p>
<p>This paper introduced the LAMP framework, which can be easily added to an existing navigation stack. This framework allows for the robot to utilize past experience to improve navigation of a familiar environment. We detailed the hybrid map structure that uses an occupancy grid and a navigation graph for the robot to navigate in when using LAMP. We then proposed the edge resolver algorithm so the robot can navigate to the goal using the hybrid map. We reviewed how the Optimistic and RPP-Hybrid policies were constructed, explaining how the high level planner functions. The potential of the LAMP framework to improve navigation was supported by our experimental results, which showed a reduction in average travel cost over time, even with imperfect localization and observations.</p>
<p>One direction of future work would be to draw conclusions about the environment based on more general observations, such as identifying signs, specific objects, or even sounds that could indicate edge traversability. Another area of potential investigation is to examine the impact of using different strategies to choose the best map to merge. Current research is looking into the potential of dynamically forming a policy during task execution based on observations, as opposed to the current approach where a complete policy is calculated before executing the task.</p>
<p>Fig. 1 .
1Two configurations of the environment are shown in (b) and (c)</p>
<p>Fig. 3 .
3Flow chart of the proposed algorithm for the LRPP the size of the representation is polynomial in the number of the subgraphs in G and the number of vertices in V .</p>
<p>Fig. 4. Example of map agreement</p>
<p>Figure 4
4shows three maps obtained while executing tasks in different realizations. Solid edges represent edges that are observed and traversable, dashed edges represent unobserved edges and the lack of an edge represents an observed and blocked edge. Map (a) agrees with Map (c), but not Map (b) because edge (1, 2) is unblocked in Maps (a) and (c) and blocked in Map (b).</p>
<p>Problem III.3 (Map Merging Problem). Given a set of collected maps from each task, M T = [M 1 , M 2 , . . . , M T ], find a minimum partition of M T such that every map in each subset agree with each other.</p>
<p>the minimum size partition of the constructed Map Merging problem such that the maps in the subsets agree with each other. By the construction of the instance, two maps M u and M v agree with each other if and only</p>
<p>Fig. 5 .
5(a) shows a realization for task t, and (b) is the collected Mt. The green line in (a) is the path determined by the policy πt, in (b) by the policy π t+1 . The blue squares are the path that the robot actually took, the grey squares are unknown.</p>
<p>Fig. 7 .
7Example of a decomposition that could form a multigraph.</p>
<p>Fig. 8 .
8Examples of different states for edge(v, u)  given what the robot knows: a) unblocked, b) blocked, c) unknown.</p>
<p>then 8 Set
8(u, v) as unblocked; 9 else if there does not exist P (u, v) ⊆ C o (S(u, v)) then 10 Set (u, v) as blocked;</p>
<p>Fig. 12 .
12(a) Base occupancy grid of the Gazebo test environment (20 m × 20 m) showing submap decomposition, labelled portals, and a potential obstacle i. The start and goal vertices are labelled as s and g respectively. (b) Example of environment configuration with obstacles. In this example, obstacles appeared in 2, 6, 7, 8, 9, and i, along with some random debris.</p>
<p>Fig. 14 .Fig. 15 .
1415(blue) Average number of super maps stored compared to the number of tasks that have been executed. (red) Percentage of executed tasks where the high level planner switched to the optimistic policy. Two examples of different environment configurations that satisfy map agreement. The top environments agree because in task 15, the robot did not see the barriers in the center along its route (light blue). The bottom environments agree despite the blue cylinders being in different locations. (a) (b) Fig. 16. (a) Base occupancy grid of real environment (20 m × 10 m) with submap decomposition, labelled portals, and potential obstacle locations. The start and goal vertices are labelled as s and g respectively. (b) Example of environment configuration with obstacles. In this example, i, the dark blue, and pink obstacles exist, along with some debris in the yellow submap.</p>
<p>of the This research is partially supported by the Natural Sciences and Engineering Research Council of Canada (NSERC). T. Walker, A. Sadeghi and S. L. Smith are with the Department of Electrical and Computer Engineering, University of Waterloo, Waterloo ON, N2L 3G1 Canada (tlwalker@uwaterloo.ca; a6sadegh@uwaterloo.ca; stephen.smith@uwaterloo.ca) F. Tsang and R. A. MacDonald were with the Department of Electrical and Computer Engineering at the University of Waterloo when this research was conducted (f4tsang@uwaterloo.ca; ryan.macdonald@uwaterloo.ca) (a) The Jackal using LAMP to navigate a changing maze.</p>
<p>Fig. 2. Overall LAMP architecture. Arrows represent data flow between components. The parallelograms represent inputs and outputs while rectangles are nodes that process data.LAMP Framework 
Navigation Stack </p>
<p>Base Map 
s, g </p>
<p>Navigation 
Graph </p>
<p>Map 
Decomposition </p>
<p>Edge 
Resolver </p>
<p>High Level 
Planner 
Policy </p>
<p>SLAM </p>
<p>Path 
Planner </p>
<p>Sensor 
Input </p>
<p>TABLE I DIFFERENT
IREALIZATIONS OF TEST ENVIRONMENTS 2 AND 3.Environment Realization Blocked Edges </p>
<p>2 </p>
<p>1 
none 
2 
Blue 
3 
Blue, Orange </p>
<p>3 </p>
<p>1 
none 
2 
Orange 
3 
Blue, Cyan 
4 
Blue, Purple </p>
<p>TABLE II POLICY
IICOST AS A PERCENTAGE OF OPTIMAL.Environment Policy 
Average (%) Last 10 Trials (%) </p>
<p>1 </p>
<p>Optimistic 
218 
219 
UCT-CTP 
218 
218 
Factor Graph 
207 
179 
RPP-Hybrid 
129 
117 </p>
<p>2 </p>
<p>Optimistic 
110 
110 
UCT-CTP 
114 
113 
Factor Graph 
117 
110 
RPP-Hybrid 
100 
100 </p>
<p>3 </p>
<p>Optimistic 
123 
122 
UCT-CTP 
129 
125 
Factor Graph 
131 
121 
RPP-Hybrid 
107 
104 </p>
<p>100 </p>
<p>200 </p>
<p>Optimistic 
UCT-CTP </p>
<p>Factor Graph 
RPP-Hybrid </p>
<p>100 </p>
<p>200 </p>
<p>100 </p>
<p>200 </p>
<p>20 
40 
60 
80 
100 
100 </p>
<p>200 </p>
<p>Number of Tasks 
Cost as a Percentange of the Optimal </p>
<p>TABLE III EXISTING
IIIALGORITHMS USED IN EXPERIMENT.Component 
ROS Package 
Algorithm </p>
<p>Global Planner global planner 
A* 
Local Planner 
base local planner Dynamic Window Approach 
[37] 
Localization 
amcl 
Augmented Monte Carlo Lo-
calization [36] 
Costmap 
costmap 2d 
Layered costmap </p>
<p>TABLE IV PROBABILITIES
IVOF OBSTACLES APPEARING IN THE SIMULATION. Cost as a Percentage of the Simple Policy RPP-Hybrid Optimistic Fig. 13. Cost savings of RPP-Hybrid policy normalized to the simple policy.Obstacles 
1 
7, 8, 9 2, 6 3, 10, 12 
i </p>
<p>Probability 0.1 
0.4 
0.5 
0.6 
0.9 </p>
<p>5 
10 
20 
30 
40 
50 
60 
70 
80 
90 
100 
Number of Tasks </p>
<p>65 </p>
<p>70 </p>
<p>75 </p>
<p>80 </p>
<p>85 </p>
<p>90 </p>
<p>95 </p>
<p>TABLE V PROBABILITIES
VOF OBSTACLES APPEARING IN THE ENVIRONMENT.Obstacle 
dark blue 
purple 
pink 
i </p>
<p>Probability 
0.6 
0.3 
0.9 
1.0 </p>
<p>TABLE VI RESULTS
VIFROM ROBOT EXPERIMENTSTask 
First Trial 
Second Trial </p>
<p>Simple 
Ours 
Simple 
Ours </p>
<p>1 
10.2 
10.7 
74.5 
60.4  <em><br />
2 
74.5 
67.4  </em><br />
70.2 
39.2 
3 
65.1 
57.6 
10.3 
32.8 
4 
10.2 
10.8 
59.7 
53.5  <em><br />
5 
65.1 
31.9 
60.8 
53.0 
6 
10.2 
11.4 
10.1 
45.4 
7 
10.2 
10.9 
23.8 
13.4  </em><br />
8 
65.1 
87.8 
75.1 
57.3 
9 
10.2 
10.8 
22.1 
42.8 
10 
74.5 
54.8 
59.9 
45.0  *  </p>
<p>Average 
39.5 
35.4 
46.6 
44.3 </p>
<p>Rapidly-exploring random trees: Progress and prospects. S Lavalle, J Kuffner, Algorithmic and Computational Robotics: New DirectionsS. Lavalle and J. Kuffner, "Rapidly-exploring random trees: Progress and prospects," Algorithmic and Computational Robotics: New Direc- tions, 01 2000.</p>
<p>RRT-connect: An efficient approach to single-query path planning. J J Kuffner, S M Lavalle, IEEE International Conference on Robotics and Automation. 2J. J. Kuffner and S. M. LaValle, "RRT-connect: An efficient approach to single-query path planning," in IEEE International Conference on Robotics and Automation, vol. 2, 2000, pp. 9950-1001.</p>
<p>Parallelizing RRT on distributed-memory architectures. D Devaurs, T Siméon, J Cortés, 2011 IEEE International Conference on Robotics and Automation. D. Devaurs, T. Siméon, and J. Cortés, "Parallelizing RRT on distributed-memory architectures," in 2011 IEEE International Con- ference on Robotics and Automation, 2011, pp. 2261-2266.</p>
<p>Probabilistic roadmaps for path planning in high-dimensional configuration spaces. L E Kavraki, P Svestka, J.-C Latombe, M H Overmars, IEEE transactions on Robotics and Automation. 124L. E. Kavraki, P. Svestka, J.-C. Latombe, and M. H. Overmars, "Prob- abilistic roadmaps for path planning in high-dimensional configuration spaces," IEEE transactions on Robotics and Automation, vol. 12, no. 4, pp. 566-580, 1996.</p>
<p>S Koenig, M Likhachev ; D<em>lite, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence15S. Koenig and M. Likhachev, "D</em>Lite," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 15, 2002, pp. 476-483.</p>
<p>Lifelong planning A<em>. S Koenig, M Likhachev, D Furcy, Artificial Intelligence. 1551-2S. Koenig, M. Likhachev, and D. Furcy, "Lifelong planning A</em>," Artificial Intelligence, vol. 155, no. 1-2, pp. 93-146, 2004.</p>
<p>Risk-aware graph search with dynamic edge cost discovery. J J Chung, A J Smith, R Skeele, G A Hollinger, The International Journal of Robotics Research. 382-3J. J. Chung, A. J. Smith, R. Skeele, and G. A. Hollinger, "Risk-aware graph search with dynamic edge cost discovery," The International Journal of Robotics Research, vol. 38, no. 2-3, pp. 182-195, 2019.</p>
<p>Conditional transition maps: Learning motion patterns in dynamic environments. T Kucner, J Saarinen, M Magnusson, A J , IEEE/RSJ International Conference on Intelligent Robots and Systems. T. Kucner, J. Saarinen, M. Magnusson, and A. J. Lilienthal, "Condi- tional transition maps: Learning motion patterns in dynamic environ- ments," in IEEE/RSJ International Conference on Intelligent Robots and Systems, 2013, pp. 1196-1201.</p>
<p>Shortest paths without a map. C H Papadimitriou, M Yannakakis, Theoretical Computer Science. 841C. H. Papadimitriou and M. Yannakakis, "Shortest paths without a map," Theoretical Computer Science, vol. 84, no. 1, pp. 127-150, 1991.</p>
<p>Canadian traveler problem with remote sensing. Z Bnaya, A Felner, S E Shimony, International Joint Conference on Artificial Intelligence. Z. Bnaya, A. Felner, and S. E. Shimony, "Canadian traveler problem with remote sensing," International Joint Conference on Artificial Intelligence, pp. 437-442, 2009.</p>
<p>Shortest path under uncertainty: Exploration versus exploitation. Z W Lim, D Hsu, W S Lee, Uncertainty in Artificial Intelligence -Proceedings of the 33rd Conference. Z. W. Lim, D. Hsu, and W. S. Lee, "Shortest path under uncertainty: Exploration versus exploitation," in Uncertainty in Artificial Intelli- gence -Proceedings of the 33rd Conference, UAI 2017, 2017.</p>
<p>The Robust Canadian Traveler Problem Applied to Robot Routing. H Guo, T D Barfoot, IEEE International Conference on Robotics and Automation. H. Guo and T. D. Barfoot, "The Robust Canadian Traveler Prob- lem Applied to Robot Routing," IEEE International Conference on Robotics and Automation, pp. 5523-5529, 2019.</p>
<p>An AO<em> based exact algorithm for the Canadian traveler problem. V Aksakalli, O F Sahin, I Ari, INFORMS Journal on Computing. 281V. Aksakalli, O. F. Sahin, and I. Ari, "An AO</em> based exact algorithm for the Canadian traveler problem," INFORMS Journal on Computing, vol. 28, no. 1, pp. 96-111, 2016.</p>
<p>High-quality policies for the canadian traveler's problem. P Eyerich, T Keller, M Helmert, Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence. the Twenty-Fourth AAAI Conference on Artificial IntelligenceP. Eyerich, T. Keller, and M. Helmert, "High-quality policies for the canadian traveler's problem," in Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, 2010, pp. 51-58.</p>
<p>Active sensing for motion planning in uncertain environments via mutual information policies. R A Macdonald, S L Smith, The International Journal of Robotics Research. 382-3R. A. MacDonald and S. L. Smith, "Active sensing for motion planning in uncertain environments via mutual information policies," The International Journal of Robotics Research, vol. 38, no. 2-3, pp. 146-161, 2019.</p>
<p>Long-term robot navigation in indoor environments estimating patterns in traversability changes. L Nardi, C Stachniss, IEEE International Conference on Robotics and Automation. IEEEL. Nardi and C. Stachniss, "Long-term robot navigation in indoor environments estimating patterns in traversability changes," in IEEE International Conference on Robotics and Automation. IEEE, 2020, pp. 300-306.</p>
<p>Learning motion planning policies in uncertain environments through repeated task executions. F Tsang, R A Macdonald, S L Smith, International Conference on Robotics and Automation. F. Tsang, R. A. Macdonald, and S. L. Smith, "Learning motion planning policies in uncertain environments through repeated task executions," in International Conference on Robotics and Automation, 2019, pp. 8-14.</p>
<p>Layered costmaps for context-sensitive navigation. D V Lu, D Hershberger, W D Smart, IEEE International Conference on Intelligent Robots and Systems. D. V. Lu, D. Hershberger, and W. D. Smart, "Layered costmaps for context-sensitive navigation," IEEE International Conference on Intelligent Robots and Systems, pp. 709-715, 2014.</p>
<p>Goselo: Goal-directed obstacle and self-location map for robot navigation using reactive neural networks. A Kanezaki, J Nitta, Y Sasaki, IEEE Robotics and Automation Letters. 32A. Kanezaki, J. Nitta, and Y. Sasaki, "Goselo: Goal-directed obstacle and self-location map for robot navigation using reactive neural networks," IEEE Robotics and Automation Letters, vol. 3, no. 2, pp. 696-703, April 2018.</p>
<p>Continuous control with deep reinforcement learning. T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971arXiv preprintT. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, "Continuous control with deep reinforce- ment learning," arXiv preprint arXiv:1509.02971, 2015.</p>
<p>Online exploration of tunnel networks leveraging topological cnn-based world predictions. M Saroya, G Best, G A Hollinger, IEEE/RSJ International Conference on Intelligent Robots and Systems. M. Saroya, G. Best, and G. A. Hollinger, "Online exploration of tunnel networks leveraging topological cnn-based world predictions," in IEEE/RSJ International Conference on Intelligent Robots and Systems, 2020.</p>
<p>Deep learning of structured environments for robot search. J A Caley, N R Lawrance, G A Hollinger, Autonomous Robots. 437J. A. Caley, N. R. Lawrance, and G. A. Hollinger, "Deep learning of structured environments for robot search," Autonomous Robots, vol. 43, no. 7, pp. 1695-1714, 2019.</p>
<p>Learned map prediction for enhanced mobile robot exploration. R Shrestha, F Tian, W Feng, P Tan, R Vaughan, 2019 International Conference on Robotics and Automation. R. Shrestha, F. Tian, W. Feng, P. Tan, and R. Vaughan, "Learned map prediction for enhanced mobile robot exploration," in 2019 International Conference on Robotics and Automation, 2019, pp. 1197-1204.</p>
<p>Complexity of Canadian traveler problem variants. D Fried, S E Shimony, A Benbassat, C Wenner, Theoretical Computer Science. 487D. Fried, S. E. Shimony, A. Benbassat, and C. Wenner, "Complexity of Canadian traveler problem variants," Theoretical Computer Science, vol. 487, pp. 1-16, 2013.</p>
<p>Computational complexity: a modern approach. S Arora, B Barak, Cambridge University PressS. Arora and B. Barak, Computational complexity: a modern ap- proach. Cambridge University Press, 2009.</p>
<p>On the complexity of trial and error for constraint satisfaction problems. G Ivanyos, R Kulkarni, Y Qiao, M Santha, A Sundaram, Journal of Computer and System Sciences. 92G. Ivanyos, R. Kulkarni, Y. Qiao, M. Santha, and A. Sundaram, "On the complexity of trial and error for constraint satisfaction problems," Journal of Computer and System Sciences, vol. 92, pp. 48-64, 2018.</p>
<p>Reducibility among combinatorial problems. R M Karp, Complexity of Computer Computations. SpringerR. M. Karp, "Reducibility among combinatorial problems," in Com- plexity of Computer Computations. Springer, 1972, pp. 85-103.</p>
<p>Data reduction and exact algorithms for clique cover. J Gramm, J Guo, F Hüffner, R Niedermeier, Journal of Experimental Algorithmics. 13J. Gramm, J. Guo, F. Hüffner, and R. Niedermeier, "Data reduction and exact algorithms for clique cover," Journal of Experimental Algorithmics, vol. 13, pp. 2-2, 2009.</p>
<p>Randomized Online Graph Coloring. S Vishwanathan, Journal of Algorithms. 669S. Vishwanathan, "Randomized Online Graph Coloring," Journal of Algorithms, vol. 669, pp. 464-469, 1992.</p>
<p>The office marathon: Robust navigation in an indoor office environment. E Marder-Eppstein, E Berger, T Foote, B Gerkey, K Konolige, Proceedings -IEEE International Conference on Robotics and Automation. -IEEE International Conference on Robotics and AutomationE. Marder-Eppstein, E. Berger, T. Foote, B. Gerkey, and K. Konolige, "The office marathon: Robust navigation in an indoor office environ- ment," Proceedings -IEEE International Conference on Robotics and Automation, pp. 300-307, 2010.</p>
<p>GeRoNa: Generic Robot Navigation: A Modular Framework for Robot Navigation and Control. G Huskić, S Buck, A Zell, Journal of Intelligent and Robotic Systems: Theory and Applications. 952G. Huskić, S. Buck, and A. Zell, "GeRoNa: Generic Robot Navi- gation: A Modular Framework for Robot Navigation and Control," Journal of Intelligent and Robotic Systems: Theory and Applications, vol. 95, no. 2, pp. 419-442, 2019.</p>
<p>Learning a Motion Policy to Navigate Environments With Structured Uncertainty. F Tsang, University of WaterlooMaster's thesisF. Tsang, "Learning a Motion Policy to Navigate Environments With Structured Uncertainty," Master's thesis, University of Waterloo, 2020. [Online]. Available: http://hdl.handle.net/10012/15562</p>
<p>Artificial Intelligence Learning metric-topological maps for indoor mobile robot navigation. S Thrun, Artificial Intelligence. 991S. Thrun, "Artificial Intelligence Learning metric-topological maps for indoor mobile robot navigation," Artificial Intelligence, vol. 99, no. 1, pp. 21-71, 1998.</p>
<p>Incremental topological segmentation for semi-structured environments using discretized GVG. M Liu, F Colas, L Oth, R Siegwart, Autonomous Robots. 382M. Liu, F. Colas, L. Oth, and R. Siegwart, "Incremental topologi- cal segmentation for semi-structured environments using discretized GVG," Autonomous Robots, vol. 38, no. 2, pp. 143-160, 2014.</p>
<p>Topomap: Topological mapping and navigation based on visual slam maps. F Blochliger, M Fehr, M Dymczyk, T Schneider, R Siegwart, IEEE International Conference on Robotics and Automation. F. Blochliger, M. Fehr, M. Dymczyk, T. Schneider, and R. Siegwart, "Topomap: Topological mapping and navigation based on visual slam maps," in IEEE International Conference on Robotics and Automation, May 2018, pp. 1-9.</p>
<p>Robust monte-carlo localization using adaptive likelihood models. P Pfaff, W Burgard, D Fox, European robotics symposium. SpringerP. Pfaff, W. Burgard, and D. Fox, "Robust monte-carlo localization using adaptive likelihood models," in European robotics symposium 2006. Springer, 2006, pp. 181-194.</p>
<p>The dynamic window approach to collision avoidance. D Fox, W Burgard, S Thrun, IEEE Robotics Automation Magazine. 41D. Fox, W. Burgard, and S. Thrun, "The dynamic window approach to collision avoidance," IEEE Robotics Automation Magazine, vol. 4, no. 1, pp. 23-33, 1997.</p>
<p>VisiLibity: A c++ library for visibility computations in planar polygonal environments. K J Obermeyer, Contributors , 1K. J. Obermeyer and Contributors, "VisiLibity: A c++ library for visibility computations in planar polygonal environments," http://www.VisiLibity.org, 2008, r-1.</p>
<p>Real-Time Loop Closure in 2D LIDAR SLAM. W Hess, D Kohler, H Rapp, D Andor, IEEE Conference on Robotics and Automation. W. Hess, D. Kohler, H. Rapp, and D. Andor, "Real-Time Loop Closure in 2D LIDAR SLAM," in IEEE Conference on Robotics and Automation, 2016, pp. 1271-1278.</p>            </div>
        </div>

    </div>
</body>
</html>
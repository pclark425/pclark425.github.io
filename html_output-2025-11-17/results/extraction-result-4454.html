<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4454 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4454</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4454</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-280337434</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.21695v1.pdf" target="_blank">Towards a Large Physics Benchmark</a></p>
                <p><strong>Paper Abstract:</strong> We introduce a benchmark framework developed by and for the scientific community to evaluate, monitor and steer large language model development in fundamental physics. Building on philosophical concepts of scientific understanding and creativity, we develop a scoring system in which each question is scored by an expert for its correctness, difficulty, and surprise. The questions are of three forms: (i) multiple-choice questions for conceptual understanding, (ii) analytical problems requiring mathematical derivation, and (iii) openended tasks requiring complex problem solving. Our current dataset contains diverse set of examples, including a machine learning challenge to classify high-energy physics events, such as the four top quark signal. To ensure continued relevance, we propose a living benchmark, where physicists contribute questions, for instance alongside new publications. We invite contributions via: http://www.physicsbenchmarks.org/. We hope that this benchmark will enable a targeted AI development that can make a meaningful contribution to fundamental physics research.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4454.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4454.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Large Physics Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Physics Benchmark for Scientific Understanding and Creativity (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A community-driven, philosophically grounded benchmark framework to evaluate LLMs on scientific understanding and creativity in fundamental physics using multi-format questions, expert ratings, and (where possible) quantitative challenge metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Large Physics Benchmark (multi-format, expert-rated)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>A three-part benchmarking framework combining (i) multiple-choice conceptual questions, (ii) analytically-unique open mathematical problems requiring stepwise derivations, and (iii) open-ended code-based challenges with a single scalar performance metric. Each QA pair is authored/curated by experts, peer-reviewed by at least three additional experts, and scored along difficulty and surprise; Type 1 answers are judged correct/incorrect, Type 2 answers are verified with symbolic/numeric verification tools, and Type 3 tasks return a continuous score (0-1) that is mapped to discrete difficulty/surprise bins.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness (binary or continuous value), Difficulty (expert-rated 1-5), Surprise/Novelty (expert-rated 1-5, specifically I-surprise relative to model training data), Value (operationalized as correctness), and single-task scalar performance for code challenges (e.g., AUC).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics (fundamental/high-energy/particle physics and related subdomains)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>explanatory/mechanistic and predictive problem solutions (Mathematical derivations, conceptual explanations, and data-driven predictive models)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Preliminary: multiple-state-of-the-art LLMs (Chat-GPT 4o, Gemini 2.5 Pro, DeepSeek v3, Claude Sonnet 4) were run on example Type 1 and Type 2 questions and all models answered those example items correctly (Table 3). For a Type 3 four-top classification example, exemplar AUCs are reported in appendices (e.g., an example Gemini run reported AUC=0.847 in an example response). Results are reported per-question-type (difficulty and surprise averaged per type) and aggregated into final Difficulty_F and Surprise_F scores (simple average over the three types).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: human expert judgments (difficulty and surprise ratings, peer review) combined with automated verification for analytic answers (symbolic/numeric tools) and automated continuous metrics for code challenges (e.g., AUC).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>QA pairs are validated by: (i) author initial ratings, (ii) peer-review by at least three independent domain experts with averaging of scores and fatal-flaw flagging, (iii) formal correctness checks using verification software for Type 2 questions (e.g., Mathematica or numerical Python routines), and (iv) comparative baselines for Type 3 using established model/human performance ceilings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Difficulties include distinguishing memorization from genuine reasoning (especially for models trained on large corpora), identifying duplicate/near-duplicate problems across datasets (affecting surprise estimation), making meaningful human-vs-model comparisons (requires identical context and test conditions), dependence of surprise assessment on training cut-off dates, and the need to avoid training/evaluation data leakage (hence sandboxing).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Living, versioned database of QA-pairs across three types; includes a Type 3 four-top-quark classification dataset described in the paper (simulated pp collisions at 13 TeV, ~302,072 events, ~50% signal, evaluation metric AUC).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Large Physics Benchmark', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4454.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4454.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Three-Type Question Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Three Question-Type Evaluation Framework (Type 1 / Type 2 / Type 3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A taxonomy of question formats to probe different aspects of scientific competence: Type 1 multiple-choice (conceptual), Type 2 analytically-unique derivations (stepwise mathematical answers), and Type 3 open-ended coding challenges (end-to-end scientific workflows optimized for a scalar metric).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Three-Type Question Framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Type 1: closed multiple-choice with one correct option for automated, scalable correctness checks. Type 2: open-form analytic problems with mathematically unique answers requiring step-by-step derivations; correctness validated via symbolic or numeric checks. Type 3: code-generation tasks where models output runnable code; performance measured by a single scalar metric (0-1). Each type is separately scored for difficulty and surprise, then aggregated.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness (Type 1 binary match; Type 2 verified expressions), stepwise reasoning (required for Type 2), epistemic depth (difficulty rating), novelty/surprise (expert rating), and quantitative task performance (Type 3 continuous metric).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>conceptual explanations, analytic derivations, data-driven predictive solutions</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper reports example outcomes: all evaluated LLMs solved the provided Type 1 and Type 2 sample questions correctly (Table 3). Type 3 example (four-top challenge) produced reported AUCs in appendices and figures (ROC/AUC plots); exemplar AUC value in appendix example: 0.847 for a Gemini run.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: Type 1 automated correctness; Type 2 automated verification plus human judgement where needed; Type 3 automated metric scoring supplemented by expert-defined binning for difficulty/surprise.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Type 2 verification via symbolic/numeric routines (explicitly required by question creator); Type 3 calibrated by comparing model scores to human or existing model baselines to assign surprise bins; inter-expert averaging for difficulty/surprise labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>For Type 1, correct responses may be guessed; for Type 2 and 3, ensuring the answer is novel relative to model training data is challenging; Type 3 fairness requires sandboxed execution and hardware-independence; mapping continuous Type 3 metrics to discrete surprise/difficulty bins requires careful calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Uses curated question sets across the three types; specific example dataset: the four-top (FOURTOPS) simulated event dataset used in Type 3 challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Large Physics Benchmark', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4454.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4454.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scoring Methodology (D,S formulas)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scoring Methodology: Difficulty and Surprise Aggregation (D1,2,S1,2 and D3,S3 mapping; DF,SF aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A quantitative scoring scheme that converts per-question expert difficulty and surprise ratings, together with correctness or continuous task scores, into aggregated Difficulty and Surprise scores per question type and a final pair of benchmark-level scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Difficulty/Surprise Scoring & Aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For Type 1 and Type 2: D_{1,2} = α_{1,2} Σ_i c_i d_i and S_{1,2} = β_{1,2} Σ_i c_i s_i, where c_i is correctness (1/0), d_i and s_i are averaged expert difficulty and surprise ratings, and α,β normalize by question counts. For Type 3: model continuous score c∈(0,1) is mapped via predefined threshold bins t_0...t_5 into discrete d_i(c) and s_i(c) ∈ {0..5}; then D_3 and S_3 are computed as weighted sums. Final benchmark scores: D_F = (D_1 + D_2 + D_3)/3 and S_F = (S_1 + S_2 + S_3)/3 (equal weighting by default).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness (c_i), expert difficulty (d_i, 1-5), expert surprise (s_i, 1-5), continuous task performance mapped to discrete bins for Type 3.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>analytic and predictive outputs (derivations, explanations, model performance)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Method is applied to produce per-type and aggregate D and S scores; preliminary example: models achieved correctness on example Type 1/2 items, contributing positively to D_{1,2} and S_{1,2}; Type 3 AUCs from example runs were used to map into discrete surprise/difficulty bins (paper shows ROC/AUC plots but does not publish final aggregated DF,SF numeric values for sampled models in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated correctness/metric values feed into formulas; expert human ratings provide d_i and s_i inputs; normalization constants and bin thresholds are expert-defined.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validation relies on peer-reviewed expert labels (three independent referees) and the explicit use of verification tools for Type 2; Type 3 mapping validated by comparison to known baselines and human/state-of-the-art ceilings where available.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Normalization weights (α,β) and bin thresholds require community calibration; reliance on subjective expert ratings introduces inter-rater variability (mitigated by averaging); mapping continuous performance to surprise/difficulty is challenge-prone and may create coarse discretization artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Large Physics Benchmark', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4454.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4454.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Type 3 Single-Scalar Protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Type 3 Single-Scalar Evaluation Protocol (code-challenge rules and sandboxing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operational rules for code-based open-ended challenges: models must submit a single prompt zero-shot generated code solution which is evaluated by a single scalar metric in a sandboxed environment with strict separation between training and evaluation data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Type 3 Single-Scalar Protocol</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Type 3 challenge rules require (1) a single scalar metric in [0,1] (e.g., AUC) that determines task performance, (2) sandboxed execution to prevent data leakage and ensure comparability, (3) single-prompt zero-shot submission (no iterative refinement), (4) sufficient freedom for model-implemented solutions, and (5) hardware-independence of scoring as much as feasible. Difficulty and surprise are derived from the continuous metric via task-specific threshold bins to yield discrete 0-5 ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary: continuous task metric (value). Secondary: mapped difficulty and surprise bins, reproducibility, and isolation from internet/tools that could access the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics (example: particle-event classification)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>predictive/data-driven classification/regression models (end-to-end scientific workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Example provided: the four-top classification Type 3 challenge uses AUC as scalar metric; example runs (appendix) report AUC values and ROC curves (e.g., example Gemini run with AUC=0.847). The framework allows assigning surprise if model score exceeds human or SOTA baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated primary scoring (single scalar) with human-defined bin thresholds and expert assessment for contextual surprise; hybrid overall.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison of model metric against baselines (other models, specialized architectures, or human-derived ceilings) to determine surprise bins; sandboxed harness ensures reproducibility and mitigates leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Ensuring fairness (hardware, random seeds), avoiding leakage from models with internet/tool access, calibrating bin thresholds with human/SOTA references, and distinguishing meaningful contribution from random or trivial improvements (e.g., marginal AUC gains).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Large Physics Benchmark', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4454.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4454.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expert Peer-Review Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-in-the-loop Expert Peer-Review and Scoring Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curation and validation process where each QA-pair is authored/annotated by domain experts, then independently reviewed by at least three experts for correctness, difficulty, and surprise, with averaging and fatal-flaw rejection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Expert Peer-Review Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Authors provide initial difficulty and surprise ratings for each QA pair. Each QA pair is then vetted by at least three independent domain experts who judge formal correctness and assign difficulty and surprise scores. If any reviewer flags a fatal flaw the QA pair is returned or discarded. Final per-question difficulty and surprise are the average of reviewer scores. Contributors must have PhD-level or equivalent experience.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Formal correctness, difficulty (1-5), surprise (1-5), identification of fatal flaws, representativeness across difficulty distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>applies to explanatory, analytic and predictive question content</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Workflow ensures that each QA-pair has stable difficulty/surprise labels for use in scoring formulas; preliminary dataset creation and vetting procedures described, but no large-scale inter-rater reliability statistics published in paper (authors mention averaging across reviewers to reduce subjectivity).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based: primary labeling and vetting performed by experts; automated tools are used for Type 2 correctness verification but labels rely on human judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Peer review with at least three independent experts; fatal-flaw flagging mechanism; monitoring of difficulty distribution to maintain balance across the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Subjectivity in difficulty/surprise ratings (mitigated by averaging), resource-intensiveness to scale, potential contributor bias, and difficulty in ensuring consistent surprise evaluation across changing public knowledge (requires tracking of publication dates relative to model cut-offs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Large Physics Benchmark', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4454.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4454.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Philosophical Creativity Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Philosophically Grounded Creativity Criteria (I-novelty, I-surprise, Characteristic Value)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operationalization of philosophical creativity concepts for LLM outputs: use of I-novelty and I-surprise (individual/LLM-relative novelty/surprise) and a context-dependent 'characteristic value' (here equated with correctness) as conditions for scientific creativity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Philosophical Creativity Metrics (I-novelty, I-surprise, Value)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Adopts Boden's individual novelty (I-novelty) adapted to LLMs (novel relative to model training data) and defines I-surprise as surprise relative to the model's training data (or publication date cut-offs); adds a value condition operationalized as correctness (binary or metric-based) to rule out surprising but worthless outputs. Surprise is practically annotated by experts for known correct answers and mapped into 1-5 scores; novelty detection considers whether answers were available before model cut-off date.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Novelty relative to model training data (I-novelty), Surprise (I-surprise) judged by experts and constrained by historical availability, Value (characteristic value operationalized as correctness), and exclusion of trivial novelty via the surprise condition.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general scientific outputs, applied here to physics</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>creative theoretical contributions, new methods/tricks, novel applications of existing methods</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Framework prescribes expert annotation of surprise for known correct answers; example workflow notes that if a model produces an answer deemed surprising by experts (and not in its training data), the model is considered to have internally recreated parts of the novel reasoning. No global quantification provided in paper beyond the methodological proposal.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-focused: novelty and surprise judgments are expert annotations (automated novelty estimation noted as future work), while correctness/value can be automated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Expert annotation cross-checked against publication dates and known public knowledge relative to model cut-off; peer review to vet surprising items; allowances made for recording the date from which an answer could be found online.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Hard to definitively determine whether a model 'memorized' versus 'generated' a novel solution, dependency on accurate knowledge of model training cut-off, difficulty detecting near-duplicates or small perturbations that produce apparent novelty, and the practicality of automating surprise/novelty estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Large Physics Benchmark', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Theoretical Physics Benchmark (TPBench)-A Dataset and Study of AI Reasoning Capabilities in Theoretical Physics <em>(Rating: 2)</em></li>
                <li>Humanity's Last Exam <em>(Rating: 2)</em></li>
                <li>Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications <em>(Rating: 2)</em></li>
                <li>SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation <em>(Rating: 1)</em></li>
                <li>GPQA: A Graduate-Level Google-Proof Q&A Benchmark <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4454",
    "paper_id": "paper-280337434",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Large Physics Benchmark",
            "name_full": "Large Physics Benchmark for Scientific Understanding and Creativity (this work)",
            "brief_description": "A community-driven, philosophically grounded benchmark framework to evaluate LLMs on scientific understanding and creativity in fundamental physics using multi-format questions, expert ratings, and (where possible) quantitative challenge metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Large Physics Benchmark (multi-format, expert-rated)",
            "evaluation_method_description": "A three-part benchmarking framework combining (i) multiple-choice conceptual questions, (ii) analytically-unique open mathematical problems requiring stepwise derivations, and (iii) open-ended code-based challenges with a single scalar performance metric. Each QA pair is authored/curated by experts, peer-reviewed by at least three additional experts, and scored along difficulty and surprise; Type 1 answers are judged correct/incorrect, Type 2 answers are verified with symbolic/numeric verification tools, and Type 3 tasks return a continuous score (0-1) that is mapped to discrete difficulty/surprise bins.",
            "evaluation_criteria": "Correctness (binary or continuous value), Difficulty (expert-rated 1-5), Surprise/Novelty (expert-rated 1-5, specifically I-surprise relative to model training data), Value (operationalized as correctness), and single-task scalar performance for code challenges (e.g., AUC).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "physics (fundamental/high-energy/particle physics and related subdomains)",
            "theory_type": "explanatory/mechanistic and predictive problem solutions (Mathematical derivations, conceptual explanations, and data-driven predictive models)",
            "human_comparison": true,
            "evaluation_results": "Preliminary: multiple-state-of-the-art LLMs (Chat-GPT 4o, Gemini 2.5 Pro, DeepSeek v3, Claude Sonnet 4) were run on example Type 1 and Type 2 questions and all models answered those example items correctly (Table 3). For a Type 3 four-top classification example, exemplar AUCs are reported in appendices (e.g., an example Gemini run reported AUC=0.847 in an example response). Results are reported per-question-type (difficulty and surprise averaged per type) and aggregated into final Difficulty_F and Surprise_F scores (simple average over the three types).",
            "automated_vs_human_evaluation": "Hybrid: human expert judgments (difficulty and surprise ratings, peer review) combined with automated verification for analytic answers (symbolic/numeric tools) and automated continuous metrics for code challenges (e.g., AUC).",
            "validation_method": "QA pairs are validated by: (i) author initial ratings, (ii) peer-review by at least three independent domain experts with averaging of scores and fatal-flaw flagging, (iii) formal correctness checks using verification software for Type 2 questions (e.g., Mathematica or numerical Python routines), and (iv) comparative baselines for Type 3 using established model/human performance ceilings.",
            "limitations_challenges": "Difficulties include distinguishing memorization from genuine reasoning (especially for models trained on large corpora), identifying duplicate/near-duplicate problems across datasets (affecting surprise estimation), making meaningful human-vs-model comparisons (requires identical context and test conditions), dependence of surprise assessment on training cut-off dates, and the need to avoid training/evaluation data leakage (hence sandboxing).",
            "benchmark_dataset": "Living, versioned database of QA-pairs across three types; includes a Type 3 four-top-quark classification dataset described in the paper (simulated pp collisions at 13 TeV, ~302,072 events, ~50% signal, evaluation metric AUC).",
            "uuid": "e4454.0",
            "source_info": {
                "paper_title": "Towards a Large Physics Benchmark",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Three-Type Question Framework",
            "name_full": "Three Question-Type Evaluation Framework (Type 1 / Type 2 / Type 3)",
            "brief_description": "A taxonomy of question formats to probe different aspects of scientific competence: Type 1 multiple-choice (conceptual), Type 2 analytically-unique derivations (stepwise mathematical answers), and Type 3 open-ended coding challenges (end-to-end scientific workflows optimized for a scalar metric).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Three-Type Question Framework",
            "evaluation_method_description": "Type 1: closed multiple-choice with one correct option for automated, scalable correctness checks. Type 2: open-form analytic problems with mathematically unique answers requiring step-by-step derivations; correctness validated via symbolic or numeric checks. Type 3: code-generation tasks where models output runnable code; performance measured by a single scalar metric (0-1). Each type is separately scored for difficulty and surprise, then aggregated.",
            "evaluation_criteria": "Correctness (Type 1 binary match; Type 2 verified expressions), stepwise reasoning (required for Type 2), epistemic depth (difficulty rating), novelty/surprise (expert rating), and quantitative task performance (Type 3 continuous metric).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "physics",
            "theory_type": "conceptual explanations, analytic derivations, data-driven predictive solutions",
            "human_comparison": true,
            "evaluation_results": "Paper reports example outcomes: all evaluated LLMs solved the provided Type 1 and Type 2 sample questions correctly (Table 3). Type 3 example (four-top challenge) produced reported AUCs in appendices and figures (ROC/AUC plots); exemplar AUC value in appendix example: 0.847 for a Gemini run.",
            "automated_vs_human_evaluation": "Hybrid: Type 1 automated correctness; Type 2 automated verification plus human judgement where needed; Type 3 automated metric scoring supplemented by expert-defined binning for difficulty/surprise.",
            "validation_method": "Type 2 verification via symbolic/numeric routines (explicitly required by question creator); Type 3 calibrated by comparing model scores to human or existing model baselines to assign surprise bins; inter-expert averaging for difficulty/surprise labels.",
            "limitations_challenges": "For Type 1, correct responses may be guessed; for Type 2 and 3, ensuring the answer is novel relative to model training data is challenging; Type 3 fairness requires sandboxed execution and hardware-independence; mapping continuous Type 3 metrics to discrete surprise/difficulty bins requires careful calibration.",
            "benchmark_dataset": "Uses curated question sets across the three types; specific example dataset: the four-top (FOURTOPS) simulated event dataset used in Type 3 challenges.",
            "uuid": "e4454.1",
            "source_info": {
                "paper_title": "Towards a Large Physics Benchmark",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Scoring Methodology (D,S formulas)",
            "name_full": "Scoring Methodology: Difficulty and Surprise Aggregation (D1,2,S1,2 and D3,S3 mapping; DF,SF aggregation)",
            "brief_description": "A quantitative scoring scheme that converts per-question expert difficulty and surprise ratings, together with correctness or continuous task scores, into aggregated Difficulty and Surprise scores per question type and a final pair of benchmark-level scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Difficulty/Surprise Scoring & Aggregation",
            "evaluation_method_description": "For Type 1 and Type 2: D_{1,2} = α_{1,2} Σ_i c_i d_i and S_{1,2} = β_{1,2} Σ_i c_i s_i, where c_i is correctness (1/0), d_i and s_i are averaged expert difficulty and surprise ratings, and α,β normalize by question counts. For Type 3: model continuous score c∈(0,1) is mapped via predefined threshold bins t_0...t_5 into discrete d_i(c) and s_i(c) ∈ {0..5}; then D_3 and S_3 are computed as weighted sums. Final benchmark scores: D_F = (D_1 + D_2 + D_3)/3 and S_F = (S_1 + S_2 + S_3)/3 (equal weighting by default).",
            "evaluation_criteria": "Correctness (c_i), expert difficulty (d_i, 1-5), expert surprise (s_i, 1-5), continuous task performance mapped to discrete bins for Type 3.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "physics",
            "theory_type": "analytic and predictive outputs (derivations, explanations, model performance)",
            "human_comparison": true,
            "evaluation_results": "Method is applied to produce per-type and aggregate D and S scores; preliminary example: models achieved correctness on example Type 1/2 items, contributing positively to D_{1,2} and S_{1,2}; Type 3 AUCs from example runs were used to map into discrete surprise/difficulty bins (paper shows ROC/AUC plots but does not publish final aggregated DF,SF numeric values for sampled models in main text).",
            "automated_vs_human_evaluation": "Hybrid: automated correctness/metric values feed into formulas; expert human ratings provide d_i and s_i inputs; normalization constants and bin thresholds are expert-defined.",
            "validation_method": "Validation relies on peer-reviewed expert labels (three independent referees) and the explicit use of verification tools for Type 2; Type 3 mapping validated by comparison to known baselines and human/state-of-the-art ceilings where available.",
            "limitations_challenges": "Normalization weights (α,β) and bin thresholds require community calibration; reliance on subjective expert ratings introduces inter-rater variability (mitigated by averaging); mapping continuous performance to surprise/difficulty is challenge-prone and may create coarse discretization artifacts.",
            "uuid": "e4454.2",
            "source_info": {
                "paper_title": "Towards a Large Physics Benchmark",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Type 3 Single-Scalar Protocol",
            "name_full": "Type 3 Single-Scalar Evaluation Protocol (code-challenge rules and sandboxing)",
            "brief_description": "Operational rules for code-based open-ended challenges: models must submit a single prompt zero-shot generated code solution which is evaluated by a single scalar metric in a sandboxed environment with strict separation between training and evaluation data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Type 3 Single-Scalar Protocol",
            "evaluation_method_description": "Type 3 challenge rules require (1) a single scalar metric in [0,1] (e.g., AUC) that determines task performance, (2) sandboxed execution to prevent data leakage and ensure comparability, (3) single-prompt zero-shot submission (no iterative refinement), (4) sufficient freedom for model-implemented solutions, and (5) hardware-independence of scoring as much as feasible. Difficulty and surprise are derived from the continuous metric via task-specific threshold bins to yield discrete 0-5 ratings.",
            "evaluation_criteria": "Primary: continuous task metric (value). Secondary: mapped difficulty and surprise bins, reproducibility, and isolation from internet/tools that could access the benchmark.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "physics (example: particle-event classification)",
            "theory_type": "predictive/data-driven classification/regression models (end-to-end scientific workflows)",
            "human_comparison": true,
            "evaluation_results": "Example provided: the four-top classification Type 3 challenge uses AUC as scalar metric; example runs (appendix) report AUC values and ROC curves (e.g., example Gemini run with AUC=0.847). The framework allows assigning surprise if model score exceeds human or SOTA baselines.",
            "automated_vs_human_evaluation": "Automated primary scoring (single scalar) with human-defined bin thresholds and expert assessment for contextual surprise; hybrid overall.",
            "validation_method": "Comparison of model metric against baselines (other models, specialized architectures, or human-derived ceilings) to determine surprise bins; sandboxed harness ensures reproducibility and mitigates leakage.",
            "limitations_challenges": "Ensuring fairness (hardware, random seeds), avoiding leakage from models with internet/tool access, calibrating bin thresholds with human/SOTA references, and distinguishing meaningful contribution from random or trivial improvements (e.g., marginal AUC gains).",
            "uuid": "e4454.3",
            "source_info": {
                "paper_title": "Towards a Large Physics Benchmark",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Expert Peer-Review Pipeline",
            "name_full": "Human-in-the-loop Expert Peer-Review and Scoring Pipeline",
            "brief_description": "A curation and validation process where each QA-pair is authored/annotated by domain experts, then independently reviewed by at least three experts for correctness, difficulty, and surprise, with averaging and fatal-flaw rejection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Expert Peer-Review Pipeline",
            "evaluation_method_description": "Authors provide initial difficulty and surprise ratings for each QA pair. Each QA pair is then vetted by at least three independent domain experts who judge formal correctness and assign difficulty and surprise scores. If any reviewer flags a fatal flaw the QA pair is returned or discarded. Final per-question difficulty and surprise are the average of reviewer scores. Contributors must have PhD-level or equivalent experience.",
            "evaluation_criteria": "Formal correctness, difficulty (1-5), surprise (1-5), identification of fatal flaws, representativeness across difficulty distribution.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "physics",
            "theory_type": "applies to explanatory, analytic and predictive question content",
            "human_comparison": true,
            "evaluation_results": "Workflow ensures that each QA-pair has stable difficulty/surprise labels for use in scoring formulas; preliminary dataset creation and vetting procedures described, but no large-scale inter-rater reliability statistics published in paper (authors mention averaging across reviewers to reduce subjectivity).",
            "automated_vs_human_evaluation": "Human-based: primary labeling and vetting performed by experts; automated tools are used for Type 2 correctness verification but labels rely on human judgement.",
            "validation_method": "Peer review with at least three independent experts; fatal-flaw flagging mechanism; monitoring of difficulty distribution to maintain balance across the benchmark.",
            "limitations_challenges": "Subjectivity in difficulty/surprise ratings (mitigated by averaging), resource-intensiveness to scale, potential contributor bias, and difficulty in ensuring consistent surprise evaluation across changing public knowledge (requires tracking of publication dates relative to model cut-offs).",
            "uuid": "e4454.4",
            "source_info": {
                "paper_title": "Towards a Large Physics Benchmark",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Philosophical Creativity Metrics",
            "name_full": "Philosophically Grounded Creativity Criteria (I-novelty, I-surprise, Characteristic Value)",
            "brief_description": "Operationalization of philosophical creativity concepts for LLM outputs: use of I-novelty and I-surprise (individual/LLM-relative novelty/surprise) and a context-dependent 'characteristic value' (here equated with correctness) as conditions for scientific creativity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Philosophical Creativity Metrics (I-novelty, I-surprise, Value)",
            "evaluation_method_description": "Adopts Boden's individual novelty (I-novelty) adapted to LLMs (novel relative to model training data) and defines I-surprise as surprise relative to the model's training data (or publication date cut-offs); adds a value condition operationalized as correctness (binary or metric-based) to rule out surprising but worthless outputs. Surprise is practically annotated by experts for known correct answers and mapped into 1-5 scores; novelty detection considers whether answers were available before model cut-off date.",
            "evaluation_criteria": "Novelty relative to model training data (I-novelty), Surprise (I-surprise) judged by experts and constrained by historical availability, Value (characteristic value operationalized as correctness), and exclusion of trivial novelty via the surprise condition.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general scientific outputs, applied here to physics",
            "theory_type": "creative theoretical contributions, new methods/tricks, novel applications of existing methods",
            "human_comparison": null,
            "evaluation_results": "Framework prescribes expert annotation of surprise for known correct answers; example workflow notes that if a model produces an answer deemed surprising by experts (and not in its training data), the model is considered to have internally recreated parts of the novel reasoning. No global quantification provided in paper beyond the methodological proposal.",
            "automated_vs_human_evaluation": "Human-focused: novelty and surprise judgments are expert annotations (automated novelty estimation noted as future work), while correctness/value can be automated.",
            "validation_method": "Expert annotation cross-checked against publication dates and known public knowledge relative to model cut-off; peer review to vet surprising items; allowances made for recording the date from which an answer could be found online.",
            "limitations_challenges": "Hard to definitively determine whether a model 'memorized' versus 'generated' a novel solution, dependency on accurate knowledge of model training cut-off, difficulty detecting near-duplicates or small perturbations that produce apparent novelty, and the practicality of automating surprise/novelty estimation.",
            "uuid": "e4454.5",
            "source_info": {
                "paper_title": "Towards a Large Physics Benchmark",
                "publication_date_yy_mm": "2025-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Theoretical Physics Benchmark (TPBench)-A Dataset and Study of AI Reasoning Capabilities in Theoretical Physics",
            "rating": 2,
            "sanitized_title": "theoretical_physics_benchmark_tpbencha_dataset_and_study_of_ai_reasoning_capabilities_in_theoretical_physics"
        },
        {
            "paper_title": "Humanity's Last Exam",
            "rating": 2,
            "sanitized_title": "humanitys_last_exam"
        },
        {
            "paper_title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications",
            "rating": 2,
            "sanitized_title": "evaluating_and_enhancing_large_language_models_for_novelty_assessment_in_scholarly_publications"
        },
        {
            "paper_title": "SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation",
            "rating": 1,
            "sanitized_title": "sciqag_a_framework_for_autogenerated_science_question_answering_dataset_with_finegrained_evaluation"
        },
        {
            "paper_title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
            "rating": 1,
            "sanitized_title": "gpqa_a_graduatelevel_googleproof_qa_benchmark"
        }
    ],
    "cost": 0.013747,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards a Large Physics Benchmark
July 30, 2025</p>
<p>Kristian G Barman 
Ghent University</p>
<p>Sascha Caron scaron@nikhef.nl 
IMAPP and ICIS
Radboud University
3 NikhefNL</p>
<p>Faegheh Hasibi 
IMAPP and ICIS
Radboud University
3 NikhefNL</p>
<p>Eugene Shalugin 
IMAPP and ICIS
Radboud University
3 NikhefNL</p>
<p>Yoris Marcet 
IMAPP and ICIS
Radboud University
3 NikhefNL</p>
<p>Johannes Otte 
IMAPP and ICIS
Radboud University
3 NikhefNL</p>
<p>Henk W De Regt 
IMAPP and ICIS
Radboud University
3 NikhefNL</p>
<p>Merijn Moody 
Dutch Institute of Emergent Phenomena
University of Amsterdam</p>
<p>Institute of Physics
University of Amsterdam</p>
<p>Towards a Large Physics Benchmark
July 30, 20256BB94541A942E89BFAA2A8AC743E8D7EarXiv:2507.21695v1[physics.data-an]
We introduce a benchmark framework developed by and for the scientific community to evaluate, monitor and steer large language model development in fundamental physics.Building on philosophical concepts of scientific understanding and creativity, we develop a scoring system in which each question is scored by an expert for its correctness, difficulty, and surprise.The questions are of three forms: (i) multiple-choice questions for conceptual understanding, (ii) analytical problems requiring mathematical derivation, and (iii) openended tasks requiring complex problem solving.Our current dataset contains diverse set of examples, including a machine learning challenge to classify high-energy physics events, such as the four top quark signal.To ensure continued relevance, we propose a "living" benchmark, where physicists contribute questions, for instance alongside new publications.We invite contributions via: http://www.physicsbenchmarks.org/.We hope that this benchmark will enable a targeted AI development that can make a meaningful contribution to fundamental physics research.</p>
<p>Introduction</p>
<p>The rapid development of Large Language Models (LLMs) has led to a growing interest in assessing their capabilities in various domains.While there are a variety of benchmarks for general purposes, there is still a lack of benchmarks that assess specific scientific understanding and creativity, particularly in fundamental physics.Current benchmarks often fall short in several key dimensions when evaluating sophisticated scientific reasoning.First, they typically lack the depth necessary to assess understanding beyond undergraduate or master's level knowledge.Second, they rarely distinguish between mere knowledge retrieval and genuine scientific reasoning.Third, they are often susceptible to gaming [1].Fourth, they seldom incorporate evaluation metrics for the (potential) novelty, surprise, and usefulness of the output (i.e., creativity).Finally, there is still no discussion on how to build a large, community-based and long-lasting benchmark in physics.</p>
<p>This paper introduces a framework for a benchmark designed specifically for the fundamental physics research community to evaluate both the scientific understanding and creative capabilities of LLMs in physics.The benchmark spans three formats (multiple-choice, analytically unique answers, and open-ended code challenges) and includes expert ratings of question difficulty and surprise.</p>
<p>Our goal is to equip both the AI and physics communities with a benchmark that not only measures current capabilities, but also serves as a guidepost for future progress, i.e., toward models that can meaningfully contribute to scientific advancement.The idea for such a framework is a joint development of philosophers of science and physicists and is a further development of our discussion of scientific understanding of humans and AI [2] and the proposal of large physics models [3].Our contribution can be summarized as:</p>
<p>• Philosophically grounded methodology: We draw from contemporary philosophy of science to operationalise scientific understanding and creativity.</p>
<p>• Benchmark development: We develop a framework for a benchmark developed and evaluated by experts in fundamental physics, incorporating a human-in-the-loop pipeline for both question creation and evaluation.</p>
<p>• Multi-Format evaluation: The benchmark spans multiple question formats to assess a model's physics capabilities, from factual recall to complex problem-solving.Model responses are evaluated not only for correctness but also for their difficulty and innovative quality (e.g., the surprise or novelty of the answer), which are taken to be proxies of scientific understanding and creativity.</p>
<p>• Question examples: We present examples of different question types, in particular an evaluation of the coding capabilities of LLMs for a real scientific problem in particle physics, the selection of events with 4 top quarks from the background.</p>
<p>The paper is organized as follows: Section 2 reviews existing scientific question-answering benchmarks and their limitations.Section 3 provides background on philosophical definitions of scientific understanding and creativity, and relates it to the types of questions and scoring methodology.Section 4 describes the practical framework implementation for benchmark generation.Section 5 presents example questions and preliminary results.Section 6 focuses on evaluating models and benchmark generation, including performance reporting for different LLMs.</p>
<p>Benchmarks for Physics</p>
<p>Several benchmarks have emerged to evaluate LLMs in scientific contexts, each with varying scope and depth.SciQAG [4] presents a system that automatically generates over 188,000 questionanswer pairs from scientific literature across 24 domains, evaluated via criteria like relevance and completeness.While scalable, its focus on factual Question-Answering (QA) and minimal expert validation limits its ability to measure deep understanding.Similarly, GPQA [5] provides 448 graduate-level, multiple-choice questions in physics, biology, and chemistry, crafted by PhD-level contributors to resist search-based answers.Though rigorous, it remains restricted to multiplechoice questions, as extensive, non-automated evaluation of questions makes it complicated to scale and update the dataset.SciEval [6] and SciFact [7] are broad multi-domain and claimverification datasets that primarily test surface-level reasoning and factual consistency rather than generative or creative capabilities.</p>
<p>Recent efforts like BRIGHT and SchNovel move beyond traditional formats.BRIGHT [8] introduces a reasoning-intensive retrieval benchmark that challenges models to identify relevant documents based on underlying principles and multi-step logic, rather than relying on surfacelevel lexical or semantic similarity.SchNovel [9] evaluates a model's capacity to assess novelty in scientific research by comparing paired scholarly papers published years apart, assuming temporal novelty progression, and introducing a RAG-based simulation of peer review for improved prediction accuracy.</p>
<p>Humanity's Last Exam (HLE) [10] is one of the most challenging evaluation of LLMs' scientific reasoning across modalities and disciplines (partly due to question difficulty and uncertaintyawareness).While HLE focuses on structured academic problems and emphasises correctness under uncertainty, our benchmark is designed to complement this by targeting deeper dimensions of understanding and creativity.Additionally, we introduce a multi-format evaluation that includes open-ended problem solving, counterfactual reasoning, and code-based challenges, enabling assessment of a model's capacity not only to reason accurately, but to generate novel and valuable scientific ideas.Where HLE tests mastery over known content, our approach aims to measure how far models can go in pushing the boundaries of scientific inquiry.</p>
<p>Among recent efforts, TPBench (Theoretical Physics Benchmark) [11] stands out for focusing specifically on high-energy theory and cosmology, offering 57 carefully curated problems ranging from undergraduate to research level.While TPBench successfully introduces novel problems and emphasises challenges like auto-verifiability and holistic AI grading, its small dataset size and static problem set inherently limit scalability compared to our approach.Unlike continuously updated framework that systematically collects new, expert-level problems and integrates diverse question types (including code-based challenges), TPBench is constrained by a single release of a modest-sized dataset.Additionally, while TPBench focuses on theoretical physics, it does not explicitly incorporate tasks to evaluate creativity.</p>
<p>Most existing benchmarks focus on evaluating model accuracy, typically through multiple-choice or short-form QA tasks where success is defined by selecting or producing the correct answer.</p>
<p>While useful for measuring surface-level competence, this approach often overlooks deeper dimensions of understanding and creativity.Selecting the right option doesn't necessarily imply reasoning or insight, and incorrect responses in the open-ended questions offer little granularity in assessing the nature of a model's mistake.In contrast, our benchmark is not designed to ask "how accurate is the model on this dataset," but rather "how well does the model understand the subject?" and "how creative can it be within that domain?"Our goal is to evaluate not only the correctness of given answers but also the capacity to generate novel insights, shifting the focus from only correctness under uncertainty to the broader, more human-like capabilities of understanding and scientific creativity.</p>
<p>We introduce tasks that are carefully constructed and rated by physics experts along the dimensions of difficulty and surprise.Our benchmark introduces three distinct types of questions (multiple choice, analytically unique, and open ended problem-solving challenges) to capture a wider spectrum of cognitive and creative abilities in LLMs.We explain these in Section 3.3.A summary comparison of existing benchmarks is presented in Table 1, highlighting the distinctive contributions and improvements introduced by our approach.</p>
<p>3 Measuring Scientific Understanding and Creativity
SciQAG-24D [4] ✓ ✗ ✗ ✓ ✗ ✗ (P) (P) ✓ ✗ ✗ ✗ GPQA [5] ✓ ✗ ✓ ✗ ✗ ✓ ✓ ✓ ✗ ✓ ✗ ✗ SciEval [6] ✓ ✓ ✓ ✓ ✗ ✓ (P) ✗ ✓ ✓ Limited ✗ SciFact [7] ✓ ✗ ✗ ✓ ✗ ✗ ✓ ✓ ✓ ✓ ✗ ✗ BRIGHT [8] ✗ ✗ ✗ ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✗ ✗ SchNovel [9] ✗ ✗ ✗ ✓ ✗ ✓ ✓ ✓ ✓ ✗ ✓ ✗ HLE [10] ✓ ✗ ✓ ✓ ✗ ✓ ✓ ✓ ✓ ✓ Limited ✗ TPBench [11] ✗ ✗ ✗ ✓ ✓(auto-verifiable) ✓ ✓ (P) ✓ ✓ Limited ✗ Ours (This Work) ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Table 1: Comparison of scientific benchmarks evaluating LLMs across dimensions such as reasoning, creativity, and philosophical grounding.(P) = partial or implicit expert involvement.</p>
<p>measures fall short of capturing deeper scientific understanding; namely, the ability to explain, generalize, and reason beyond surface-level information.This is where philosophy of science becomes relevant: it offers conceptual tools for distinguishing between knowledge, explanation, and understanding, and helps define what it is we are trying to measure when we talk about a model "understanding" physics.</p>
<p>According to an influential account [12], scientific understanding of a phenomenon stems from it being explained within an intelligible theoretical framework; one that enables scientists to qualitatively predict the target system's behavior (while respecting empirical adequacy and internal consistency).Several philosophers [13,14] [15], [2] further argue that understanding depends on counterfactual reasoning: the ability to explore how phenomena would behave under different conditions.Together, these perspectives converge on a definition of understanding not as the passive possession of facts, but as the active capacity to apply, explain, and reason within theoretical systems.In this light, QA would involve not testing what a model knows but how it can use knowledge flexibly and insightfully.Hence, assessing understanding requires questions of a varying degree of difficulty, where difficulty is not just a practical label but reflects the epistemic structure of the task.When domain experts rate a question as more difficult, they are assessing the level of conceptual engagement required.But more importantly, certain tasks require going through key steps, without which, the correct answer cannot be obtained consistently ( for an example see Type 2 questions in Section 5.2)</p>
<p>These definitions are not new, but this is the first attempt to integrate them into a scalable, operational benchmark for LLMs through a question-based evaluation that encodes difficulty as a proxy for epistemic depth in such a way that answering said questions accurately reflects the degree of scientific understanding.</p>
<p>Scientific Creativity</p>
<p>In philosophy, creative products are typically defined as novel, valuable, and surprising [16].</p>
<p>The necessity of the novelty condition is obvious, as it captures core intuitions about creativity.An important distinction [17] is between H-novelty (historical novelty), where a product is new in all of history, and P-novelty (psychological novelty), where it is new to the creator's mind.While P-novelty applies to humans, it cannot be easily extended to LLMs, as they are trained on vast datasets, often containing the entire internet, but cannot explicitly memorize all this data.For a given problem, it is difficult to determine whether an LLM recalls a solution or generates it differently.To circumvent this issue, we extend P-novelty to LLMs by defining an LLM-generated product to be P-novel if it is novel with respect to the LLMs training data.Since P-novelty has anthropomorphic connotations, we adopt Boden's [18] I-novelty (individual novelty), which generalizes to non-human entities.So we call an LLM's I-novel if it is novel relative to its training data.</p>
<p>The concept of novelty requires elaboration because, when taken in the strictest sense, it is a weak and ill-defined concept: strictly speaking one can change one word in a book and create a novel text, which is not a creative endeavor.In reality almost all created products are H-novel.</p>
<p>To exclude cases of trivial novelty, philosophers [17] introduced the surprise condition.Surprise measures how well the product can be explained using pre-established generative principles and one can view it as a more sophisticated measure of novelty.For instance, a mathematical proof or physical derivation can follow a well-known step plan, which we would not say to be very creative.This step plan is part of the generative principles of the theory, i.e., it is a known set of rules that can be applied in specific situations to create new products within the theory.Producing products in such a way is not creative.What is creative, is going beyond pre-established generative principles, for instance by using a new proof method, a new trick or by applying methods in areas where they were previously not known to be useful.For surprise we can make the same distinction between H-surprise and I-surprise for LLMs, where H-surprise means a product is surprising with respect to all of history and I-surprise means a product is surprising with respect to the LLM's training data.</p>
<p>The value condition is added to rule out surprising nonsense from being creative: anyone can create something that does not follow any generative principles by generating random sequences, but we would not call this creative [19,20].Nonetheless, we should also not limit 'value' to societal benefit [19,20,21,22].A general and apt definition of value is therefore the notion of characteristic value [23] : "a property or set of properties may count as a value in an entity of a particular kind because it is desirable for an entity of that kind."Since this is contextually determined, in our case we will mostly associate value with how correctly an answer answers a question.</p>
<p>A Question-Based Benchmark Framework</p>
<p>Leveraging this philosophical basis of scientific understanding and creativity, we introduce a scalable and (semi-)automated framework for evaluating LLMs using distinct question types.We base our question types loosely on [2], where the broader philosophical reasoning for these question types is made explicit and argued for.</p>
<p>The three types of questions generated in this benchmark can be categorized into the following categories: multiple choice, analytically unique, and open-ended coding challenges.Examples of all types of questions, including prompting strategies, can be seen in Section 5.Each of these three question types will range from simple problems to real frontier scientific problems.</p>
<p>Type 1: Multiple Choice.The simplest question type consists of multiple choice answers (A, B, C, D), with exactly one correct and three incorrect answers.This format allows for straightforward, easily scalable, and automated question generation and evaluation.</p>
<p>Type 2: Analytically Unique.The second type of question is similar to the first except that no answers are given and the answer is limited to a mathematically unique expression.A step-bystep solution should be provided, not just the end result.This format allows the use of open ended questions without introducing complex and intricate nuances of natural language.Concealing the correct answer allows for the elimination of possible biases in next-token prediction inference when the answer is included in the prompt.</p>
<p>Type 3: Open-Ended Coding Challenges.The third question type takes the form of openended, code-based problem-solving challenges where the primary objective is to maximize a scalar performance metric (score).Type 3 questions prompt the LLM to generate (Python) code that addresses specific physics problems.Each challenge question will be accompanied by detailed yet deliberately limited context, including clear problem and data descriptions, alongside an elaborate code template that carefully balances detailed instructions with the necessary freedom to foster creative code generation.</p>
<p>Type 3 questions challenge the model with the construction and linking of various steps in a (machine learning) code base.This may include data pre-processing, feature engineering, model design, training procedures, process simulation, and parameter fitting.Rather than testing isolated skills, these tasks assess the model's ability to reason through and construct a structured workflow that mirrors real scientific problem solving.Because the model can, from a one-shot prompt, generate the whole code, it is feasible to test the whole process.More importantly, because its results can be easily quantified, it is easy to show how close, or even if they can surpass human abilities per single question.General rules that this format must follow are:</p>
<p>• Single scalar metric: Evaluation results are determined by a clearly defined metric in the form of a single score ([0,1]) with higher scores indicating better performance.</p>
<p>• Sand-boxed execution: Model training and inference must be compartmentalized and isolated from evaluation to ensure strict separation of stages.This is to prevent any form of data leakage or cheating.</p>
<p>• Single-prompt, zero-shot: Models receive exactly one prompt without iterative refinement, mitigating prompting biases and memorization.</p>
<p>Additionally, note that scoring should not be strongly dependent on hardware to ensure a fair comparison between runs.Moreover, models should be granted sufficient freedom to implement their desired solutions within reasonable bounds that will enable testing the solution.</p>
<p>Scoring Methodology</p>
<p>The evaluation of LLM performance in this benchmark framework translates into quantifiable scores for scientific understanding and creativity, rated on a 1-5 scale.The scoring approach differs slightly for Type 1/2 questions versus Type 3 challenges.The following details the application of our criteria to Type 1 and Type 2 questions.</p>
<p>For scientific understanding, the core metric is difficulty.To translate this into a score we let domain experts assign a difficulty score of (1-5) to each question.A higher score indicates that more understanding is required to answer the question correctly.While there is some leeway in how evaluators may subjectively assign difficulty, with enough (or representative enough) evaluators, the differences in interpretation will average into what the community believes is 'difficult'.</p>
<p>For scientific creativity we have to evaluate whether the answers are surprising and valuable.To measure value, we have to specify the characteristic value of the answer to a question.The main value of an answer in our context is the correctness of the answer.Fortunately, the correctness is precisely what is evaluated in the benchmark.So value here is merely a binary score of 1 or 0 that indicates whether the answer was correct or not.Evaluating the surprise of each generated answer directly is unfeasible.As a practical alternative, we let experts evaluate the known correct answers of the questions on surprise again with a score of (1-5).In general, surprise should be evaluated as I-surprise, which in this case means surprise with respect to what was available on the internet before the cut-off date of the model.However, if a result is recent or obscure, we also allow questions where the answer might be partly found online; in such cases, this will be noted, and the date from which the answer could be found online will be recorded so that the product can still be counted as I-surprising for models with a cut-off date before the answer appeared online.We then assume that if an LLM is able to solve one of the questions whose answer has been deemed surprising, it has internally recreated at least part of the reasoning of this surprising answer and therefore created a surprising product.This assumption is more reasonable for type 2 questions than for type 1 questions, as it is possible to answer type 1 questions correctly by an educated guess.Note that a similar argument holds for difficulty, i.e., if an LLM can consistently answer type 2 questions that are deemed difficult, it means it can (at least partially) recreate the steps involved.The underlying idea here is that while it's possible to arrive at a correct answer by chance or through flawed reasoning, this becomes increasingly unlikely as the number of examples, the complexity of the reasoning, or the number of steps increases.</p>
<p>For Type 3 questions the situation is slightly different as the solution achieves a scalar score based on a predefined metric.This score itself serves as the measure of its value.Both difficulty and surprise are then derived from this score using a step-function that maps the continuous score to a discrete score of (0-5).For difficulty, experts will define six distinct regions within the possible score range.A rating from 0 to 5 is assigned based on the region into which the model's final score falls, as achieving a higher score on these problems is more difficult.Similarly, for surprise, the score range is partitioned into six regions, each corresponding to a surprise value from 0 to 5.This rating can, for instance, be determined by comparing the model's score against established benchmarks, such as the performance of other models or the state-of-the-art achieved by human researchers.A score that surpasses these known ceilings would thus be assigned a higher surprise value.</p>
<p>Implementation of the Framework</p>
<p>To probe a model's scientific understanding and creativity at scale, we treat question-answer (QA) creation and QA evaluation as distinct tasks.Each QA pair1 is scored along two dimensions.</p>
<p>The full QA creation and evaluation pipeline follows a three step pipeline, illustrated in Figure 1.</p>
<p>Figure 1: Framework for Physics Creativity and Reasoning Benchmark 1) Question Generation.We distinguish between two QA-generation methods.Either questions are generated by (i) exclusively human experts, or by (ii) human experts collaborating with large language models.In the case of the latter, we make an additional distinction between: (ii)a mainly human-generated and LLM assisted, or (ii)b mainly LLM generated and human-assisted.</p>
<p>2) Expert Evaluation.Alongside QA-generation each human expert is expected to score their question on difficulty and the correct answer on surprise.Additionally, questions will be tested against chosen LLMs as shown in Figure 2a.The benchmark will provide information on the ability of chosen models to answer submitted questions.</p>
<p>3) Peer Evaluation.Each QA-pair will be evaluated by three additional experts on both formal correctness as well as surprise and difficulty as shown in Figure 2b.Then, for each metric a final average score will be determined for each QA-pair.The result will be an extensive database of quality questions created and evaluated by the physics community itself.</p>
<p>Every QA pair will be peer reviewed according to the following points:</p>
<p>• Author scores -the author assigns an initial difficulty label and rates the correct answer's surprise and difficulty as defined in section 3.4.</p>
<p>• Triple check -Each QA pair is verified by, at least, three independent experts on (i) Formal correctness and (ii) difficulty and surprise.Once a QA has been vetted an average score will be calculated for each of the three figures of merit.</p>
<p>If any reviewer flags a fatal flaw, the QA pair is returned for revision or discarded.Overall difficulty distributions are monitored to keep the benchmark balanced.Expert Pool To maximise the quality of questions, we invite contributors holding a PhD (or equivalent research experience) in fundamental physics (e.g.particle physics, astrophysics, nuclear physics) and supply a detailed question-design manual.2Contributors will be rewarded for their contribution with co-authorship in future publications.</p>
<p>Transparency All public questions, scores, and version histories will be published through a versioned release approach and searchable at the project site.This transparency lets the community audit both human and LLM contributions and facilitates re-sampling for future benchmark releases.To maintain the integrity of the benchmark, only a small subset of the question database can be made public with each iteration.Models consisting of tools which have access to the internet must be prohibited to gain access to the QA-pairs, and the questions themselves should not be used for training.</p>
<p>Prompting &amp; Question Examples</p>
<p>This chapter provides examples for each of the three question types introduced in section 3.3.For each type, we present representative physics questions reflecting the distinct format and level of complexity involved.Detailed prompting strategies and templates associated with each question type are documented separately in the appendix 9.</p>
<p>Type 1 question -multiple choice</p>
<p>Example question 1: Why does the Higgs boson decay dominantly to b quarks?</p>
<p>Multiple choice answers:</p>
<p>A. b quarks are the lightest quarks.</p>
<p>B. The top quark is too heavy for the Higgs decay.</p>
<p>C. The b quarks have the right electric charge.</p>
<p>D. The Higgs dominantly decays to photons.</p>
<p>Correct answer: B</p>
<p>Example question 2: Why did we introduce the Higgs mechanism?</p>
<p>Multiple choice answers:</p>
<p>A. We wanted to give masses to all particles.The Higgs field makes all particles heavier.</p>
<p>B. The Higgs field gives mass to the proton and neutron.</p>
<p>C. When adding simple mass terms to the theory one encounters mathematical problems (e.g.divergences), and this is solved with the Higgs field.</p>
<p>D. There was a problem with helicity.The Higgs field explains why we have only left-handed particles.</p>
<p>Correct answer: C</p>
<p>Type 2 question: Open ended with a determined analytical answer</p>
<p>Example question 1: In a two-body scattering event, A + B → C + D, it is convenient to introduce the Mandelstam variables:
s ≡ (p A + p B ) 2 , t ≡ (p A − p C ) 2 , u ≡ (p A − p D ) 2 .
Find the energy of A in the center-of-mass frame of A and B, in terms of s, t, u and the rest masses.</p>
<p>Correct answer:
E A = s + m 2 A − m 2 B 2 √ s .
Example question 2: The coupling of the Standard-Model Higgs boson to fermions is described by a vertex factor im f /v where m f is the rest mass of the fermion and v is the vacuum expectation value of the Higgs field (= 2m W /g W ).</p>
<p>Calculate the matrix element M for the Higgs boson decaying into a fermion/antifermion pair.Express the amplitude as a function of m H and m f , where m H is the Higgs mass, and show the average over all possible spin configurations as a final answer (if needed, neglect the color factors).</p>
<p>Correct answer:
⟨|M | 2 ⟩ = 2m 2 f v 2 m 2 H − 4m 2 f .
Preliminary results from evaluating four state-of-the-art large language models (LLMs)-Chat-GPT 4o, Gemini 2.5 Pro, DeepSeek v3, and Claude Sonnet 4 -on two type 1 and 2 questions are shown in Table 3.</p>
<p>Table 3: Preliminary evaluation of large language models (LLMs) on two types of physics questions.All models answered both multiple choice (Type 1) and analytical (Type 2) questions correctly.</p>
<p>Model</p>
<p>Type 3 Question -Maximizing Score</p>
<p>The first Type 3 question consists of a challenge of binary classification of signal versus background data of proton-proton collision events.The signal consists of the detection of two top and anti-top quark pairs.An example of a specific challenge question is as follows:</p>
<p>"Write Python code for a binary classification model focusing on maximizing the AUC using the code template above.You may freely choose any pre-processing methods and techniques as well as model architecture and training conventions.Do absolutely everything in your power to achieve the highest possible AUC."</p>
<p>The full prompt, code template and LLM generated code can be found in the appendix, section 9.2.Each with and without integrated physics.In case of the former the models include pairwise features (int.) and SM running coupling constant (SM).Full description can be found in reference [24].</p>
<p>Preliminary Results:</p>
<p>Model Evaluation</p>
<p>Large language models are evaluated for their scientific understanding and creativity, where scores for difficulty and surprise serve as proxies for these two metrics, respectively.Each of the three question types produces a separate averaged score for difficulty and surprise, resulting in six total scores.Subsequently, these scores are aggregated into two final values.LLMs will be tested out-of-the-box, meaning we do not consider modifying various parameters such as temperature, nucleus-or top k-sampling.</p>
<p>Type 1 questions are evaluated through a straightforward comparison between the LLM-generated answer and the known correct answer, yielding a boolean result (correct or incorrect).Type 2 questions, which produce open-ended mathematical expressions, are evaluated using suitable verification software (e.g., Mathematica or numerical Python routines).A suitable verification method must be explicitly specified by the creator of each question.</p>
<p>Mathematically, the scores for Type 1 and 2 questions can be expressed as follows:
D 1,2 = α 1,2 N,M i=1 c i d i , S 1,2 = β 1,2 N,M i=1 c i s i , c i = 1, if correct 0, if incorrect(1)
Here, D represents difficulty, S represents surprise and c represents correctness as a proxy for value.The terms d i and s i denote the average difficulty and surprise ratings for each individual question, respectively.The normalization weights α 1,2 and β 1,2 are inversely proportional to the total number of questions per type in the benchmark, under the assumption that the effort of creating questions correlates with their relative epistemic depth.N and M are the total number of Type 1 and Type 2 questions, respectively.</p>
<p>For Type 3 questions, evaluation depends directly on a continuous scoring metric specific to each challenge.Let the scoring metric be c ∈ (0, 1).Then the scores for Type 3 questions can be defined as follows:
D 3 = α 3 K i=1 d i , S 3 = β 3 K i=1 s i , d i , s i (c) =              0, t d,s 0 ≤ c ≤ t d,s 1 1, t d,s 1 &lt; c ≤ t d,s 2 . . . 5, t d,s 4 &lt; c ≤ t d,s 5(2)
Here, d i (c) and s i (c) represent disjoint discretized mappings of difficulty and surprise derived from the continuous score c, using predefined interval breakpoints t 0 &lt; t 1 &lt; • • • &lt; t 5 ∈ (0, 1).The mapping for surprise is challenge-specific and the mapping for difficulty is question-specific.</p>
<p>The difficulty between two questions in the same challenge may differ due to additional contextual information provided or omitted the model.It is important to emphasize that correctness in Type 3 questions is implicitly encoded: even scores greater than zero may reflect no actual contribution.For instance, in a binary classification task, an accuracy score of 0.5 merely indicates random guessing and thus carries no meaningful contribution.</p>
<p>Finally, the scores for each of the three question types can be aggregated into two averaged scores for understanding and creativity:
D F = 1 3 D 1 + D 2 + D 3 , S F = 1 3 S 1 + S 2 + S 3(3)
The quantities D F and S F represent the final scores of the Large Physics Benchmark.Note that it is possible to add further weights to also reflect structural differences in type 1,2,3.At this stage we opt for starting with equal weights, leaving room to gather evidence before committing to a more nuanced scheme.</p>
<p>coding challenge tasks that require maximizing a defined metric.Each question is scored along dimensions of difficulty and surprise by domain experts.Our framework incorporates expertauthored and LLM-assisted question creation, human-in-the-loop evaluation, and transparent public documentation, aiming to serve both as a diagnostic tool and a guidepost for future LLM development in scientific reasoning.</p>
<p>Nonetheless, there are some noteworthy limitations.Identifying duplicate or near-identical problems across large datasets remains a challenge for fair surprise evaluation.Additionally, comparing human and model performance is also difficult, as meaningful comparisons require questions with sufficient context, where both must reason from the same information; replicating such environments is challenging.</p>
<p>Future work should focus on expanding the diversity and number of high-quality questions, particularly in underrepresented physics subdomains.We aim to improve automatic surprise estimation and explore richer formats for evaluating creativity, such as multi-step reasoning chains or model-generated questions.Incorporating human-model collaboration tasks and longitudinal evaluation of models as they evolve across versions may offer deeper insights into scientific understanding.Additional future directions include refining the benchmark's ability to distinguish between memorization and reasoning, integrating multi-modal tasks involving visual or symbolic physics representations, and developing adaptive benchmarking tools that adjust question difficulty based on model responses and are ammenable to tayloring context provided.Cross-domain transfer tasks and meta-reasoning challenges may also help evaluate generalization beyond physics.We invite the physics community to participate in the creation of questions and answers and to become an co-author of a follow-up publication.If you are interested, please visit http://www.physicsbenchmarks.org/.</p>
<p>Acknowledgments</p>
<p>The work of Caron, De Regt, Barman was supported by an IRP grant from FNWI, Radboud University.Barman's work was also supported by FWO grant 1229124N.</p>
<p>Appendix</p>
<p>9.1 Type 2 Database 1 { 2 " author " : " &lt; author id &gt; " ,</p>
<p>3</p>
<p>" question " : " &lt; text of the question &gt; " ,</p>
<p>4</p>
<p>" answer " : " &lt; correct answer &gt; " ,</p>
<p>5</p>
<p>" scoring " : {</p>
<p>6</p>
<p>" referee_ 1 " : { 7 " author " : " &lt; author id &gt; " ,</p>
<p>8</p>
<p>" correctness " : " &lt; score &gt; " ,</p>
<p>9</p>
<p>" difficulty " : " &lt; score &gt; " , 10 " surprise " : " &lt; score &gt; " 11 } ,</p>
<p>12</p>
<p>" referee_ 2 " : {} ,</p>
<p>13</p>
<p>" referee_ 3 " : {} ,</p>
<p>14</p>
<p>" baseline_models " : { 15 " o 4 " : " &lt; solved / not solved &gt; " ,</p>
<p>16</p>
<p>" DeepSeek " : " &lt; solved / not solved &gt; " ,</p>
<p>17</p>
<p>" Llama " : " &lt; solved / not solved &gt; " , ** Instructions ** You are an expert at programming in Python, machine learning, particle and high energy physics.You will help me answer a question in a machine learning challenge format where you strive to maximise a scalar metric in order to learn more about your scientific creativity and scientific understanding.You will follow all of the instructions to your best capabilities.Your first priority is to produce a correct solution in terms of runnable python code.Your second priority is to maximise the scoring metric defined below.<strong> Problem Description ** A major task in particle physics is the measurement of rare signal processes with very small cross-sections.With the unprecedented amount of data provided by the upcoming runs of the Large Hadron Collider (LHC), one can start to measure these processes.An example is the recent observation of four top quarks originating from a single proton-proton collision event.Accurate classification of these events is crucial, as even a small reduction in background noise on the order of a few tens of percent while maintaining the same signal detection efficiency can lead to a profound increase in sensitivity.</strong> Evaluation Metric ** The evaluation metric for this classification task is the area under the curve (AUC), specified by the area under the receiver operating characteristic (ROC) curve.The AUC summarizes a model's ability to distinguish between positive and negative classes.The higher the score the better.** Dataset Description ** The dataset used for this problem consists of simulated proton-proton collision at a center of mass energy of 13 TeV.The signal process is defined as pp → t tt t.The relevant production processes of the backgrounds are t t + X where X = Z, W + , W + W − .The dataset includes 302072 events, of which roughly 50% is signal and 50% are background processes.All background processes have an equal number of events.There is no cut on the maximum number of objects and there is no order.</p>
<p>The contents of the datasets (X train &amp; X val) are given below.IMPORTANT: The specific line format of the dataset is as follows:
E miss T , ϕ miss Et , obj 1 , E 1 , p T 1 , η 1 , ϕ 1 , obj 2 , E 2 , p t2 , η 2 , ϕ 2 , ...
Such that each object is represented by a string that starts with an identifier "obj n ", which is an integer value representing a particular object in the event.The object identifier is followed by its kinematic properties in the form of a four-vector containing the full energy "E" and the transverse momentum "p T" in units of MeV, as well as the pseudo-rapidity "η" and the azimuthal angle "ϕ".The other three quantities are "weight" given by the cross-section of the process divided by the total number of events generated."E miss T " is the magnitude of the missing transverse energy in units of MeV and "ϕ miss E T " is the azimuthal angle of the missing transverse energy.</p>
<p>Since the length of the events is variable, the data is zero-padded to the largest number of objects found in the events within the entire dataset.The dataset is fairly sparse and not pre-processed.</p>
<p>The relevant datasets are pytorch tensors with the following properties:    -------------------------START OF LLM BLOCK -----------------------------</p>
<p>Figure 2 :
2
Figure 2: Framework for Physics Creativity and Reasoning Benchmark: (a) Question Generation Step (b) Scoring by Peers Step</p>
<p>Table 4 :
4
Side-by-side comparison of preliminary results of LLM performance on the fourtops challenge (left) with specialized models (right) on the same fourtops dataset.The two specialized models depicted are Particle Net (PN) a graph neural network and Particle Transformer (ParT).</p>
<p>Figure 3 :
3
Figure 3: Training and validation accuracy versus epoch for six evaluated models of the example question of the FOURTOPS challenge.The number of epochs changes per model as per their own choice.</p>
<p>Figure 4 :
4
Figure 4: ROC Curves and AUC scores for the example question of the FOURTOPS challenge.</p>
<p>18 "
18
... " : " ... "</p>
<p>1 #
1
Name: X train, shape: [241657, 92], dtype: torch.float32,Name: Y train, shape: [241657], dtype: torch.int64,Name: X val, shape: [30272, 92], dtype: torch.float32,Name: Y val, shape: [30272], dtype: torch.int64IMPORTANT: Each of these tensors are pre-loaded and available.----------------START HARNESS WRAPPER PREFIX ( FOR CONTEXT ) ----------------</p>
<p>. read_csv ( DATASET [ " X_train " ] , dtype = np .float32 ) .to_numpy ( copy = False ) 18 Y_train = pd .read_csv ( DATASET [ " Y_train " ] , dtype = np .int64 ) .to_numpy ( copy = False ) .ravel () 19 X_val = pd .read_csv ( DATASET [ " X_val " ] , dtype = np .float32 ) .to_numpy ( copy = False ) 20 Y_val = pd .read_csv ( DATASET [ ' Y_val '] , dtype = np .int64 ) .to_numpy ( copy = False ) .ravel () .from_numpy ( X_train ) , torch .from_numpy ( Y_train ) , 25 torch .from_numpy ( X_val ) , torch .from_numpy ( Y_val ) ) 26 27 class PairDataset ( Dataset ) : 28 def <strong>init</strong> ( self , x , y ) : self .x, ( tuple , list ) ) and all ( torch .is_tensor ( t ) for t in self .x ) : 38 return ( tuple ( t [ idx ] for t in self .x ) , self .y [ idx ]) 39 else : 40 return ( self .x [ idx ] , self .y [ idx ]) 41 42 def _make_dataset (x , y ) : 43 custom = globals () .get ( " make_dataset " , None ) 44 if callable ( custom ) : 45 ds = custom (x , y ) 46 if ds is not None : X_train , Y_train , X_val , Y_val , * , batch =512 , collate_fn = None , loader_cls = None ) : 51 train_ds = _make_dataset ( X_train , Y_train ) --------------END HARNESS WRAPPER PREFIX ( FOR CONTEXT ) ----------------65 # -</p>
<p>----------IMPORTS ----------</p>
<h2>160 # 3 .</h2>
<dl>
<dt>1603</dt>
<dd>Apply optional raw data reshaping logic here &gt; 111 return X # Returns identify by default 112 113 Uncomment to implement custom collate function .114 # @staticmethod 115 # def <em>collate_fn ( batch : list ) : 116 # &lt; LLM : Apply optional custom collate logic here &gt; e</em> l oa de r_ c fg ( self ) : 120 Return dict or None .If dict , evaluator uses it to rebuild loader : self , X ) : 135 # &lt; LLM : Apply pre -processing logic &gt; 136 return X # must return an indexable , picklable object 137 138 def fit_transform ( self , X , y = None ) : 139 self .fit (X , y ) 140 return self .transform ( X ) 141 142 def m a k e _ p r e p r o c e s s o r () : ----------MODEL DEFINITION ----------146 class B i n a r y C l a s s i f i e r ( nn .Module ) : 147 def <strong>init</strong> ( self , sample_object ) : 148 super () .<strong>init</strong> () 149 # &lt; LLM : Define and initialize any stateful components here &gt; 150 151 # &lt; LLM : optionally build extra layers here &gt; 152 153 def forward ( self , * data ) : 154 # &lt; LLM : Define your model 's forward pass here &gt; 155 pass 156 157 def make_model ( examp le_obje ct ) : 158 return B i n a r yC l a s s i f i e r ( e xample_ object ) 159 ----------MODEL TRAINING ----------161 EPOCHS = 10 # &lt; LLM : adjust if you wish &gt; 162 def train_model ( model : nn .Module , train_loader , val_loader , epochs : int ) : Do NOT pass " verbose =" to any PyTorch scheduler ( not supported in this image ) .165 # Must return trained_model , train_loss , val_loss , train_acc , val_acc 166 # Implement early -stopping .167 # Forward signature must match .</dd>
<dt>8 9 16 # 17 # 18 # 19 # 20 # 21 # ... 22 ####</dt>
<dt>816171819202122</dt>
<dd>Write code to define training loop &gt; 170 # &lt; LLM : Implement early stopping if possible &gt; 171 return trained_model , train_loss , val_loss , train_acc , val_acc 172 173 # IMPORTANT : DO NOT execute the pipeline here -the harness will do that .-------------------------ENDOF LLM -CODE BLOCK ---------------------------178 # ----------------START HARNESS WRAPPER SUFFIX ( FOR CONTEXT ) ----------------179 180 def <em>import _dotted ( path : str ) : 181 mod , name = path .rsplit ( " ." , 1) 182 module = importlib .import_module ( mod ) 183 return getattr ( module , name ) 184 185 def _plot ( series_train , series_val , name , out_path ) : 186 plt .figure () 187 epochs = range (1 , len ( series_train ) + 1) 188 plt .plot ( epochs , series_train , label = f " Train { name } " ) 189 plt .plot ( epochs , series_val , label = f " Val { name } " ) 190 plt .title ( name ) ; plt .xlabel ( " Epoch " ) ; plt .legend () 191 plt .savefig ( out_path ) ; plt .close () , X_val , Y_val = X_train [:200] , Y_train [:200] , X_val [:20] , Y_val [:20] 198 pre = m a k e _ p r e p r o c e s s o r () .fit ( X_train , Y_train ) pre , " m ak e</em> l oa de r_ c fg " , lambda : None ) () or {} 204 loader_cls = _impo rt_dotte d ( cfg [ " loader_class " ]) if " loader_class " in cfg else None 205 train_loader , val_loader = make_loaders ( X_train , Y_train , X_val , Y_val , 206 batch = cfg .get ( " batch_size " , 512) , 207 collate_fn = collate , 208 loader_cls = loader_cls ) exam ple_samp le ) cally) correct code.Prioritise what you can implement reliably above all else.Then prioritise maximising the metric.9.2.2 Example Response: Google Gemini (23/06) AUC: 0.847 1 # 0. ----------IMPORTS ----------2 # NOTE : Some imports ( torch , nn , numpy , DataLoader ) are already available ( see prefix ) . 3 # Only import extra std -lib modules , torch , scipy , sklearn ( sub -) modules you actually use .4 from sklearn .preprocessing import St andardS caler 5 from torch .optim .lr_scheduler import R e d u c e L R O n P l a t e a u 6 import copy 7 # 2. ----------PRE -PROCESSING ----------10 class MyP reproce ssor : Total flat length per event ( X_train &amp; X_val ) : 92 Index 0 : missing -ET magnitude ( E_T_miss ) Index 1 : missing -ET azimuth ( phi_Et_miss ) Indices 2 -6 : object 1 -&gt; obj_1 , E_1 , p_T1 , eta_1 , phi_1 Indices 7 -11 : object 2 -&gt; obj_2 , E_2 , p_T_2 , eta_2 , phi_2 Indices 88 -92 : object 18 -&gt; obj_18 , E_18 , p_T_18 , eta_18 , phi_1823 When modifying data features or feature engineering : annotate tensor size as comments after 29 # each tensor operation to reduce dimension mismatches .All state must be picklable with the std -lib pickle module .33 May allocate NumPy arrays or Torch tensors internally , but : 34 # transform () must be deterministic .35 Store only derived parameters needed for transform i .e .do not store the raw data 36 # itself in the preprocessor object .</dd>
</dl>
<h1></h1>
<p>Define and initialize any stateful components here 40 self .global_scaler = St andardSc aler () g lo ba l_ f ea tu re s = torch .stack ([ log_E_T_miss , met_x , met_y ] , dim =1) # [-Feature engineering for object features ---69 # The first feature is obj_id , which we treat as categorical 70 obj_ids = objects_raw [: , : , 0] # [N , 18]</p>
<p>Table 2 :
2
Description of scores to be assigned by experts to individual questions for difficulty and surprise.No contribution is exclusive to Type 3 questions, in order to retain the possibility of nullifying a question's overall value.
Score Difficulty and Surprise5Excellent4High3Good2Reasonable1Minimal0No contribution*</p>
<p>16 #
16
NOTE : Some imports ( torch , nn , numpy , DataLoader ) are already available ( see prefix ) .All state must be picklable with the std -lib pickle module .Store only derived parameters needed for transform i .e .do not store the raw data
71# Only import extra std -lib modules , torch , scipy , sklearn ( sub -) modules youactually use .72# &lt; LLM : Import modules &gt;7374# 2. ----------PRE -PROCESSING ----------75class MyP reproce ssor :76#Must implement :77#-fit (...)-&gt; self78#-transform ( X : ???)-&gt; ???7980# DATA SPECIFICS81# Total flat length per event ( X_train &amp; X_val ) : 9282# Index 0 : missing -ET magnitude ( E_T_miss )83# Index 1 : missing -ET azimuth( phi_Et_miss )84# Indices 2 -6 : object 1 -&gt; obj_1 , E_1 , p_T1 , eta_1 , phi_185# Indices 7 -11 : object 2 -&gt; obj_2 , E_2 , p_T_2 , eta_2 , phi_286# ...87# Indices 88 -92 : object 18 -&gt; obj_18 , E_18 , p_T_18 , eta_18 , phi_1888# Global features= 289# Per -object slice size = 590# Max objects encoded= 189192# TIPS93# When modifying data features or feature engineering : annotate tensorsize as comments after94# each tensor operation to reduce dimension mismatches .9596# REQUIREMENTS97# IMPORTANT :
98# May allocate NumPy arrays or Torch tensors internally , but : 99 # transform () must be deterministic .100 # 101 # itself in the preprocessor object .</p>
<p>For type-3 questions QA-pair generation refers to the entire challenge including sub-questions, but we will call it QA-generation for simplicity.
The manual and a submission portal will be hosted at https://physicsbenchmarks.org
In this paper, we introduced a benchmark framework designed to assess scientific understanding and creativity in Large Language Models (LLMs) within the domain of fundamental physics. The benchmark comprises three distinct question types: multiple-choice, analytically unique, and
n _ g l o b a l _ f e a t u r e s = globals .shape[1]148 n _ o b j _ c o n t i n u o u s _ f e a t u r e s = obj_feats .shape[2]149 150 # Infer max object ID from the first batch .This is a pragmatic approach .
The Leaderboard Illusion. Shivalika Singh, arXiv:2504.20879[cs.LG]2025</p>
<p>Towards a Benchmark for Scientific Understanding in Humans and Machines. Kristian G Barman, 10.1007/s11023-024-09657-1Minds and Machines. 3462024</p>
<p>Large Physics Models: Towards a collaborative approach with Large Language Models and Foundation Models. Kristian G Barman, arXiv:2501.05382Jan. 2025physics.data-an</p>
<p>SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation. Yuwei Wan, arXiv:2405.09939[cs.CL]2024</p>
<p>GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark. David Rein, arXiv:2311.12022[cs.AI]Proceedings of the First Conference on Language Modeling. the First Conference on Language Modeling2024</p>
<p>Scieval: A multi-level large language model evaluation benchmark for scientific research. Liangtai Sun, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Fact or Fiction: Verifying Scientific Claims. David Wadden, arXiv:2004.14974[cs.CL]2020</p>
<p>BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval. Hongjin Su, arXiv:2407.12883[cs.IR]2024</p>
<p>Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications. Ethan Lin, Zhiyuan Peng, Yi Fang, arXiv:2409.16605[cs.CL]2024</p>
<p>Humanity's Last Exam. Long Phan, arXiv:2501.14249[cs.AI]2025</p>
<p>Theoretical Physics Benchmark (TPBench)-A Dataset and Study of AI Reasoning Capabilities in Theoretical Physics. J H Daniel, Chung, arXiv:2502.158152025physics.comp-ph</p>
<p>Understanding Scientific Understanding. W Henk, De Regt, 2017Oxford University Press</p>
<p>Is Understanding a Species of Knowledge?. R Stephen, Grimm, The British Journal for the Philosophy of Science. 5732006</p>
<p>Understanding. R Stephen, Grimm, The Routledge Companion to Epistemology. Sven Bernecker, Duncan Pritchard, Routledge, 2010</p>
<p>External Representations and Scientific Understanding. Jaakko Kuorikoski, Petri Ylikoski, Synthese. 1922015</p>
<p>Attributing creativity. Dustin Stokes, Elliot Samuel, Paul , 10.4324/9781351199797-6Creativity and philosophy. Kieran Gaut, Feb. 20189781351199797</p>
<p>The creative mind: Myths and mechanisms. Margaret A Boden, 2004Routledge</p>
<p>Creativity and biology. Margaret Boden, 10.4324/9781351199797-6Creativity and philosophy. Kieran Gaut, Feb. 20189781351199797</p>
<p>The philosophy of creativity. Berys Gaut, Philosophy Compass. 52010</p>
<p>The value of creativity. Berys Gaut, 10.4324/9781351199797-6Creativity and philosophy. Ed. by Gaut and KieranFeb. 20189781351199797</p>
<p>Explanations of Creativity. David Novitz, The Creation of Art: New Essays in Philosophical Aesthetics. Berys Gaut, Paisley Livingston, 2003</p>
<p>Creativity and Constraint. David Novitz, 10.1080/00048409912348811Australasian Journal of Philosophy. 771999</p>
<p>Values in Science. Ernan Mcmullin, PSA: Proceedings of the Biennial Meeting of the Philosophy of Science Association 1982. 1982Visited on 12/22/2024</p>
<p>Attention to the strengths of physical interactions: Transformer and graph-based event classification for particle physics experiments. Luc Builtjes, arXiv:2211.05143Nov. 2022hep-ph</p>
<p>Do not add any formatting. such as markdown, to the response</p>
<p>Before finalizing your answer, double-check that your code runs without errors and meets all requirements (all functions implemented, correct tensor shapes. </p>
<p>To prevent dimensional mismatches make sure to annotate tensor sizes as comments. </p>
<p>IMPORTANT: Remember, your first, and most important priority is to produce (syntacti. </p>            </div>
        </div>

    </div>
</body>
</html>
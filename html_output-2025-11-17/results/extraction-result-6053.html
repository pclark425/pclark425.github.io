<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6053 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6053</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6053</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-270619930</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.13232v1.pdf" target="_blank">Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Open Domain Question Answering (ODQA) within natural language processing involves building systems that answer factual questions using large-scale knowledge corpora. Recent advances stem from the confluence of several factors, such as large-scale training datasets, deep learning techniques, and the rise of large language models. High-quality datasets are used to train models on realistic scenarios and enable the evaluation of the system on potentially unseen data. Standardized metrics facilitate comparisons between different ODQA systems, allowing researchers to objectively track advancements in the field. Our study presents a thorough examination of the current landscape of ODQA benchmarking by reviewing 52 datasets and 20 evaluation techniques across textual and multimodal modalities. We introduce a novel taxonomy for ODQA datasets that incorporates both the modality and difficulty of the question types. Additionally, we present a structured organization of ODQA evaluation metrics along with a critical analysis of their inherent trade-offs. Our study aims to empower researchers by providing a framework for the robust evaluation of modern question-answering systems. We conclude by identifying the current challenges and outlining promising avenues for future research and development.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6053.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6053.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models used as evaluation judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The practice of using large pretrained language models to automatically judge or score generated answers (or other NLG outputs) as an alternative or complement to human evaluation, leveraging LLMs' knowledge and reasoning capabilities via prompting or fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-domain question answering / general NLG evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>various LLMs (examples cited: GPT-3.5, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human rating described generally (crowd or recruited raters using explicit criteria such as factual correctness, relevance, completeness, clarity; Likert scales or binary correctness; inter-rater reliability (IRR) is important). The paper cites prior human-evaluation setups but does not run new human annotations itself.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Agreement rate / correlation with human judgments, ability to match human preferences (percentage agreement), HEQ-style comparisons when applicable, and detection of factuality/unattributability.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>LLM judges can often approximate human judgments (strong LLMs like GPT-4 reported to achieve >80% agreement with humans), but differences remain: LLM judges are sensitive to prompts, can miss unattributability in long-form answers, and may not reliably grade math/reasoning tasks; semantic automatic metrics and human ratings still catch some failure modes LLM judges miss.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Prompt-sensitivity; positional bias (favor certain positions); verbosity bias (favor longer answers); self-enhancement bias (favoring outputs generated by the same model); limited ability on math and some reasoning; susceptibility to hallucinations; inability in some cases to assess attributability or provenance of long-form claims.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>LLM-based zero-shot evaluation with references is promising but 'not suitable for detecting issues such as unattributability in long-form answers' (paper cites this finding); difficulty grading math/reasoning; judges favoring verbose but incorrect answers; preference for outputs generated by themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Prompt engineering and standardized prompts; chain-of-thought prompting for evaluators (e.g., G-Eval style); using multiple or ensemble judges; fine-tuning judge models on human preference data (JudgeLM); decomposing long answers into atomic facts and verifying each (FActScore/SAFE); augmenting judge reasoning with external retrieval (search-based verification).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6053.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6053.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zheng et al. (LLM-judge analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena (Zheng et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical analysis showing that strong LLM judges (e.g., GPT-4) can match controlled and crowdsourced human preferences, achieving agreement rates comparable to human inter-annotator agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General LLM / chatbot evaluation (including open-ended responses)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (as an example of a strong LLM judge)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Comparison against controlled and crowdsourced human preference judgments (the source paper measured human agreement levels and compared LLM-human agreement to human-human agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Percentage agreement between LLM judge and humans; baseline human-human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Reported that strong LLM judges such as GPT-4 can match human preferences well, achieving >80% agreement—approximately the same level as agreement between humans.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Despite high agreement overall, LLM judges still exhibit the general limitations discussed in the review (biases, prompt sensitivity, weaknesses on certain reasoning/math tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>The review does not enumerate detailed per-case failures from this study beyond noting residual biases and limitations of LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use of stronger LLMs, careful evaluation design, and complementary methods (e.g., ensembles or human spot-checking) are implied as ways to ensure robust evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6053.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6053.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kamalloo et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluating Open-Domain Question Answering in the Era of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study that examined LLM-based evaluation methods for ODQA, including zero-shot prompting with reference answers, and analyzed which failure modes such automated evaluation can and cannot detect.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Open-Domain Question Answering in the Era of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-domain question answering evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-3.5 (and similar LLMs used in zero-shot evaluator roles)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Compared LLM-based zero-shot prompting (with reference answers) against human judgments; specifics depend on the cited study but the review reports comparative analysis rather than novel annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Correlation/agreement with human judgments; detection of unattributability/factuality in long-form answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Zero-shot LLM evaluators with access to reference answers are promising as alternatives to human evaluation, but they are limited: notably they may not detect unattributability in long-form answers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Fails to reliably detect unattributability in long-form answers; other general LLM limitations (prompt sensitivity, bias) apply.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Long-form answers containing unattributed claims that humans flag as problematic but that zero-shot LLM evaluators do not detect.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Combining LLM-based evaluation with specialized factuality checks and provenance/attribution verification; using decomposition-based fact checking (see FActScore/SAFE) as a complementary step.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6053.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6053.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wang (Evaluating Open-QA Evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluating Open-QA Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An analysis of evaluation methods for open-domain QA which includes experiments with LLM-based evaluation (GPT-3.5) and reports that LLM-based evaluations can achieve reasonably good performance across several datasets compared with other automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Open-QA Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-domain question answering evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-3.5 (as used in cited experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Comparison of LLM-based evaluation outputs against human judgments across multiple datasets (paper cites these experimental comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Correlation/agreement with human judgments, dataset-wise performance of LLM evaluators vs. other automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>LLM-based evaluation (GPT-3.5) achieved reasonably good performance relative to other automatic methods, but not perfect alignment with human judgments in all cases.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Dataset-dependent performance variability; does not fully capture some human-evaluated phenomena such as attributability or nuanced factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Instances where GPT-3.5-based evaluation deviates from human ratings, especially on long-form or attribution-sensitive items.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Recommend using LLM-based evaluation alongside other metrics and human spot checks; possibly improve prompts or use more capable LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6053.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6053.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based LLM evaluator that uses chain-of-thought style prompting in GPT-4 to produce structured, human-aligned evaluation scores and justifications for NLG outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Natural language generation evaluation, applicable to ODQA</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (used for chain-of-thought evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>G-Eval compares its structured LLM judgments against human ratings to measure alignment; the review cites G-Eval as an example of chain-of-thought prompting used to improve LLM judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Alignment/correlation with human ratings across qualitative dimensions (relevance, fluency, coherence, factuality); use of CoT to justify scores.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>G-Eval style chain-of-thought prompting can improve the interpretability and alignment of LLM judgments with humans, but still inherits LLM vulnerabilities (prompt sensitivity, potential verbosity bias).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Higher sensitivity to prompt design; generated CoT may introduce additional surface artifacts; still subject to hallucination and biases if not paired with verification.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Noted general LLM judge failure modes (verbosity bias, positional bias) can persist even when using CoT prompting if not carefully designed.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use CoT prompting to elicit stepwise evaluation, combine CoT outputs with external verification (retrieval/search), and calibrate prompts to reduce verbosity/positional biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6053.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6053.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JudgeLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>JudgeLM: Fine-tuned Large Language Models are Scalable Judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that fine-tunes LLMs on human preference data so they serve as scalable judges capable of distinguishing high-quality from low-quality responses within task-specific contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>JudgeLM: Fine-tuned Large Language Models are Scalable Judges</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended NLG evaluation including ODQA</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Fine-tuned LLMs (examples in cited work range across 7B–33B parameter models)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>JudgeLM models are trained on human preference labels (human comparisons) to learn human-aligned scoring; evaluation compares model judges to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Agreement with human preference judgments; effectiveness at ranking response quality.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Fine-tuned judge models can more reliably mimic human preferences than zero-shot LLM judges and scale better across prompts/tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Coverage limited to the kinds of preferences seen during fine-tuning; may overfit to training distribution and still miss external-verification dependent faults (e.g., hallucinations not flagged in training data).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Potential domain shift issues where fine-tuned judges disagree with humans on novel tasks not seen during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Train on diverse human preference datasets, include examples that surface known failure modes (math, attribution), and combine fine-tuned judges with retrieval-based factual verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6053.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6053.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FActScore / SAFE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FActScore and SAFE: Atomic-fact-based factuality evaluation methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>FActScore decomposes long-form outputs into atomic facts and verifies each against trusted sources; SAFE extends this by using an LLM to rate atomic facts against an external search (e.g., Google) to assess factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Long-form answer factuality evaluation within ODQA / NLG</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>LLM used as part of the pipeline (SAFE uses an LLM to rate atomic facts, combined with external search verification); FActScore is an evaluation methodology that can be automated.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>These methods are designed to complement or replace some aspects of human factuality checks by providing fine-grained, per-atomic-fact verification; paper cites them as approaches to detect hallucinations that LLM judges may miss.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Fine-grained factual precision (per-atomic-fact correctness), aggregate factuality scores compared to human judgments of factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Atomic-fact verification reduces missed factual errors relative to coarse LLM-judge overall judgments; helps find hallucinations LLM-judges or semantic metrics might miss.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Dependence on external search quality and on ability to correctly decompose claims; the LLM component can still misinterpret or mis-evaluate facts if prompts or retrieval fail.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Cases where atomic decomposition misses implied claims or where external search does not return authoritative evidence, leading to false negatives/positives relative to human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Combine atomic-fact decomposition with robust retrieval, human spot-checking of ambiguous facts, and multi-source corroboration to reduce reliance on single search results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment <em>(Rating: 2)</em></li>
                <li>JudgeLM: Fine-tuned Large Language Models are Scalable Judges <em>(Rating: 2)</em></li>
                <li>FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation <em>(Rating: 2)</em></li>
                <li>Evaluating Open-Domain Question Answering in the Era of Large Language Models <em>(Rating: 2)</em></li>
                <li>Evaluating Open-QA Evaluation <em>(Rating: 2)</em></li>
                <li>GPTScore: Evaluate as You Desire <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6053",
    "paper_id": "paper-270619930",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-Judge",
            "name_full": "Large Language Models used as evaluation judges",
            "brief_description": "The practice of using large pretrained language models to automatically judge or score generated answers (or other NLG outputs) as an alternative or complement to human evaluation, leveraging LLMs' knowledge and reasoning capabilities via prompting or fine-tuning.",
            "citation_title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "mention_or_use": "mention",
            "task_domain": "Open-domain question answering / general NLG evaluation",
            "llm_judge_model": "various LLMs (examples cited: GPT-3.5, GPT-4)",
            "human_evaluation_setup": "Human rating described generally (crowd or recruited raters using explicit criteria such as factual correctness, relevance, completeness, clarity; Likert scales or binary correctness; inter-rater reliability (IRR) is important). The paper cites prior human-evaluation setups but does not run new human annotations itself.",
            "metrics_compared": "Agreement rate / correlation with human judgments, ability to match human preferences (percentage agreement), HEQ-style comparisons when applicable, and detection of factuality/unattributability.",
            "reported_differences": "LLM judges can often approximate human judgments (strong LLMs like GPT-4 reported to achieve &gt;80% agreement with humans), but differences remain: LLM judges are sensitive to prompts, can miss unattributability in long-form answers, and may not reliably grade math/reasoning tasks; semantic automatic metrics and human ratings still catch some failure modes LLM judges miss.",
            "llm_specific_limitations": "Prompt-sensitivity; positional bias (favor certain positions); verbosity bias (favor longer answers); self-enhancement bias (favoring outputs generated by the same model); limited ability on math and some reasoning; susceptibility to hallucinations; inability in some cases to assess attributability or provenance of long-form claims.",
            "notable_failure_cases": "LLM-based zero-shot evaluation with references is promising but 'not suitable for detecting issues such as unattributability in long-form answers' (paper cites this finding); difficulty grading math/reasoning; judges favoring verbose but incorrect answers; preference for outputs generated by themselves.",
            "mitigation_strategies": "Prompt engineering and standardized prompts; chain-of-thought prompting for evaluators (e.g., G-Eval style); using multiple or ensemble judges; fine-tuning judge models on human preference data (JudgeLM); decomposing long answers into atomic facts and verifying each (FActScore/SAFE); augmenting judge reasoning with external retrieval (search-based verification).",
            "uuid": "e6053.0",
            "source_info": {
                "paper_title": "Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Zheng et al. (LLM-judge analysis)",
            "name_full": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena (Zheng et al.)",
            "brief_description": "Empirical analysis showing that strong LLM judges (e.g., GPT-4) can match controlled and crowdsourced human preferences, achieving agreement rates comparable to human inter-annotator agreement.",
            "citation_title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "mention_or_use": "mention",
            "task_domain": "General LLM / chatbot evaluation (including open-ended responses)",
            "llm_judge_model": "GPT-4 (as an example of a strong LLM judge)",
            "human_evaluation_setup": "Comparison against controlled and crowdsourced human preference judgments (the source paper measured human agreement levels and compared LLM-human agreement to human-human agreement).",
            "metrics_compared": "Percentage agreement between LLM judge and humans; baseline human-human agreement.",
            "reported_differences": "Reported that strong LLM judges such as GPT-4 can match human preferences well, achieving &gt;80% agreement—approximately the same level as agreement between humans.",
            "llm_specific_limitations": "Despite high agreement overall, LLM judges still exhibit the general limitations discussed in the review (biases, prompt sensitivity, weaknesses on certain reasoning/math tasks).",
            "notable_failure_cases": "The review does not enumerate detailed per-case failures from this study beyond noting residual biases and limitations of LLM judges.",
            "mitigation_strategies": "Use of stronger LLMs, careful evaluation design, and complementary methods (e.g., ensembles or human spot-checking) are implied as ways to ensure robust evaluation.",
            "uuid": "e6053.1",
            "source_info": {
                "paper_title": "Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Kamalloo et al.",
            "name_full": "Evaluating Open-Domain Question Answering in the Era of Large Language Models",
            "brief_description": "A study that examined LLM-based evaluation methods for ODQA, including zero-shot prompting with reference answers, and analyzed which failure modes such automated evaluation can and cannot detect.",
            "citation_title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models",
            "mention_or_use": "mention",
            "task_domain": "Open-domain question answering evaluation",
            "llm_judge_model": "GPT-3.5 (and similar LLMs used in zero-shot evaluator roles)",
            "human_evaluation_setup": "Compared LLM-based zero-shot prompting (with reference answers) against human judgments; specifics depend on the cited study but the review reports comparative analysis rather than novel annotations.",
            "metrics_compared": "Correlation/agreement with human judgments; detection of unattributability/factuality in long-form answers.",
            "reported_differences": "Zero-shot LLM evaluators with access to reference answers are promising as alternatives to human evaluation, but they are limited: notably they may not detect unattributability in long-form answers.",
            "llm_specific_limitations": "Fails to reliably detect unattributability in long-form answers; other general LLM limitations (prompt sensitivity, bias) apply.",
            "notable_failure_cases": "Long-form answers containing unattributed claims that humans flag as problematic but that zero-shot LLM evaluators do not detect.",
            "mitigation_strategies": "Combining LLM-based evaluation with specialized factuality checks and provenance/attribution verification; using decomposition-based fact checking (see FActScore/SAFE) as a complementary step.",
            "uuid": "e6053.2",
            "source_info": {
                "paper_title": "Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Wang (Evaluating Open-QA Evaluation)",
            "name_full": "Evaluating Open-QA Evaluation",
            "brief_description": "An analysis of evaluation methods for open-domain QA which includes experiments with LLM-based evaluation (GPT-3.5) and reports that LLM-based evaluations can achieve reasonably good performance across several datasets compared with other automatic metrics.",
            "citation_title": "Evaluating Open-QA Evaluation",
            "mention_or_use": "mention",
            "task_domain": "Open-domain question answering evaluation",
            "llm_judge_model": "GPT-3.5 (as used in cited experiments)",
            "human_evaluation_setup": "Comparison of LLM-based evaluation outputs against human judgments across multiple datasets (paper cites these experimental comparisons).",
            "metrics_compared": "Correlation/agreement with human judgments, dataset-wise performance of LLM evaluators vs. other automatic metrics.",
            "reported_differences": "LLM-based evaluation (GPT-3.5) achieved reasonably good performance relative to other automatic methods, but not perfect alignment with human judgments in all cases.",
            "llm_specific_limitations": "Dataset-dependent performance variability; does not fully capture some human-evaluated phenomena such as attributability or nuanced factuality.",
            "notable_failure_cases": "Instances where GPT-3.5-based evaluation deviates from human ratings, especially on long-form or attribution-sensitive items.",
            "mitigation_strategies": "Recommend using LLM-based evaluation alongside other metrics and human spot checks; possibly improve prompts or use more capable LLMs.",
            "uuid": "e6053.3",
            "source_info": {
                "paper_title": "Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "G-Eval",
            "name_full": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
            "brief_description": "A prompt-based LLM evaluator that uses chain-of-thought style prompting in GPT-4 to produce structured, human-aligned evaluation scores and justifications for NLG outputs.",
            "citation_title": "G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment",
            "mention_or_use": "mention",
            "task_domain": "Natural language generation evaluation, applicable to ODQA",
            "llm_judge_model": "GPT-4 (used for chain-of-thought evaluation)",
            "human_evaluation_setup": "G-Eval compares its structured LLM judgments against human ratings to measure alignment; the review cites G-Eval as an example of chain-of-thought prompting used to improve LLM judgement.",
            "metrics_compared": "Alignment/correlation with human ratings across qualitative dimensions (relevance, fluency, coherence, factuality); use of CoT to justify scores.",
            "reported_differences": "G-Eval style chain-of-thought prompting can improve the interpretability and alignment of LLM judgments with humans, but still inherits LLM vulnerabilities (prompt sensitivity, potential verbosity bias).",
            "llm_specific_limitations": "Higher sensitivity to prompt design; generated CoT may introduce additional surface artifacts; still subject to hallucination and biases if not paired with verification.",
            "notable_failure_cases": "Noted general LLM judge failure modes (verbosity bias, positional bias) can persist even when using CoT prompting if not carefully designed.",
            "mitigation_strategies": "Use CoT prompting to elicit stepwise evaluation, combine CoT outputs with external verification (retrieval/search), and calibrate prompts to reduce verbosity/positional biases.",
            "uuid": "e6053.4",
            "source_info": {
                "paper_title": "Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "JudgeLM",
            "name_full": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
            "brief_description": "An approach that fine-tunes LLMs on human preference data so they serve as scalable judges capable of distinguishing high-quality from low-quality responses within task-specific contexts.",
            "citation_title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
            "mention_or_use": "mention",
            "task_domain": "Open-ended NLG evaluation including ODQA",
            "llm_judge_model": "Fine-tuned LLMs (examples in cited work range across 7B–33B parameter models)",
            "human_evaluation_setup": "JudgeLM models are trained on human preference labels (human comparisons) to learn human-aligned scoring; evaluation compares model judges to human judgments.",
            "metrics_compared": "Agreement with human preference judgments; effectiveness at ranking response quality.",
            "reported_differences": "Fine-tuned judge models can more reliably mimic human preferences than zero-shot LLM judges and scale better across prompts/tasks.",
            "llm_specific_limitations": "Coverage limited to the kinds of preferences seen during fine-tuning; may overfit to training distribution and still miss external-verification dependent faults (e.g., hallucinations not flagged in training data).",
            "notable_failure_cases": "Potential domain shift issues where fine-tuned judges disagree with humans on novel tasks not seen during fine-tuning.",
            "mitigation_strategies": "Train on diverse human preference datasets, include examples that surface known failure modes (math, attribution), and combine fine-tuned judges with retrieval-based factual verification.",
            "uuid": "e6053.5",
            "source_info": {
                "paper_title": "Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "FActScore / SAFE",
            "name_full": "FActScore and SAFE: Atomic-fact-based factuality evaluation methods",
            "brief_description": "FActScore decomposes long-form outputs into atomic facts and verifies each against trusted sources; SAFE extends this by using an LLM to rate atomic facts against an external search (e.g., Google) to assess factuality.",
            "citation_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
            "mention_or_use": "mention",
            "task_domain": "Long-form answer factuality evaluation within ODQA / NLG",
            "llm_judge_model": "LLM used as part of the pipeline (SAFE uses an LLM to rate atomic facts, combined with external search verification); FActScore is an evaluation methodology that can be automated.",
            "human_evaluation_setup": "These methods are designed to complement or replace some aspects of human factuality checks by providing fine-grained, per-atomic-fact verification; paper cites them as approaches to detect hallucinations that LLM judges may miss.",
            "metrics_compared": "Fine-grained factual precision (per-atomic-fact correctness), aggregate factuality scores compared to human judgments of factuality.",
            "reported_differences": "Atomic-fact verification reduces missed factual errors relative to coarse LLM-judge overall judgments; helps find hallucinations LLM-judges or semantic metrics might miss.",
            "llm_specific_limitations": "Dependence on external search quality and on ability to correctly decompose claims; the LLM component can still misinterpret or mis-evaluate facts if prompts or retrieval fail.",
            "notable_failure_cases": "Cases where atomic decomposition misses implied claims or where external search does not return authoritative evidence, leading to false negatives/positives relative to human judgment.",
            "mitigation_strategies": "Combine atomic-fact decomposition with robust retrieval, human spot-checking of ambiguous facts, and multi-source corroboration to reduce reliance on single search results.",
            "uuid": "e6053.6",
            "source_info": {
                "paper_title": "Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
            "rating": 2,
            "sanitized_title": "judgelm_finetuned_large_language_models_are_scalable_judges"
        },
        {
            "paper_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
            "rating": 2,
            "sanitized_title": "factscore_finegrained_atomic_evaluation_of_factual_precision_in_long_form_text_generation"
        },
        {
            "paper_title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models",
            "rating": 2,
            "sanitized_title": "evaluating_opendomain_question_answering_in_the_era_of_large_language_models"
        },
        {
            "paper_title": "Evaluating Open-QA Evaluation",
            "rating": 2,
            "sanitized_title": "evaluating_openqa_evaluation"
        },
        {
            "paper_title": "GPTScore: Evaluate as You Desire",
            "rating": 1,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        }
    ],
    "cost": 0.018398249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models</p>
<p>Akchay Srivastava akchay_srivastava@apple.com 
Apple Inc
95014CupertinoCAUSA</p>
<p>Apple Inc
95014CupertinoCAUSA</p>
<p>Atif Memon 
Apple Inc
95014CupertinoCAUSA</p>
<p>Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models
B147CF1C7521635A8E94067215738ED7Artificial intelligencedatasetslarge language modelsmachine learningmetricsmultimodalnatural language processingopen domain question answeringreviewtaxonomy
Open Domain Question Answering (ODQA) within natural language processing involves building systems that answer factual questions using large-scale knowledge corpora.Recent advances stem from the confluence of several factors, such as large-scale training datasets, deep learning techniques, and the rise of large language models.High-quality datasets are used to train models on realistic scenarios and enable the evaluation of the system on potentially unseen data.Standardized metrics facilitate comparisons between different ODQA systems, allowing researchers to objectively track advancements in the field.Our study presents a thorough examination of the current landscape of ODQA benchmarking by reviewing 52 datasets and 20 evaluation techniques across textual and multimodal modalities.We introduce a novel taxonomy for ODQA datasets that incorporates both the modality and difficulty of the question types.Additionally, we present a structured organization of ODQA evaluation metrics along with a critical analysis of their inherent trade-offs.Our study aims to empower researchers by providing a framework for the robust evaluation of modern question-answering systems.We conclude by identifying the current challenges and outlining promising avenues for future research and development.</p>
<p>I. INTRODUCTION</p>
<p>In Natural Language Processing (NLP), Question Answering (QA) is a core task focused on delivering precise answers to user-posed questions in natural language.This long-standing task dates back to the 1960s [1].Traditional Machine Reading Comprehension (MRC) systems aim to read and comprehend provided context passages to answer a given question [2].Conversely, Open Domain Question Answering (ODQA) systems retrieve information from vast unstructured knowledge sources, such as Wikipedia or the Web, to address user queries [3].This means that the system can answer questions on any topic, such as "Who is the Prime Minister of Japan?" or "What are the different types of clouds?".Compared to MRC, ODQA offers broader applicability and better reflects real-world human behavior.MRC can be considered a stepping stone for achieving ODQA capabilities.Current research in ODQA systems centers around three main approaches, which we now examine.</p>
<p>The first is the retriever-reader [3] approach, which is based on the idea of combining Information Retrieval (IR) and MRC techniques.The retriever component gathers relevant information from external knowledge sources that the reader module leverages to comprehend and formulate an appropriate answer.Notable information retrieval techniques include TF-IDF [3], BM25 [4], and DPR [5].Transformer-based models have emerged as leading techniques for advanced reading tasks.This includes prominent models such as BERT [6], RoBERTa [7], T5 [8], BART [9], and GPT-3 [10].Within the retriever-reader ODQA systems, readers can fall into two distinct categories: extractive and generative.Extractive readers directly extract answers from the provided context by identifying the start and end positions of the answer span within the evidence [5], [6].In contrast to extractive readers, generative readers employ autoregressive token prediction to create fluent answers that extend beyond the given context [8], [11].</p>
<p>The second is the retriever-only approach, which tackles ODQA tasks with a single retriever, eliminating the reading or generating step altogether.A typical category of the retrieveronly approach is phrase-based systems [12], [13].</p>
<p>The third is the generator-only approach, which is normally based on single generators, mainly generative Large Language Models (LLMs) such as T5 [8], BART [9], and GPT-3 [10].Pre-trained on massive Wikipedia corpora, these models encode corpus knowledge within their parameters, thereby enabling direct answer generation without the need for explicit information retrieval.However, evidence has indicated that LLMs can generate hallucinations or other content that contradicts reality [14], [15].Consequently, ensuring factuality has become a primary concern.The emergence of LLMs has spurred a surge in ODQA research, therefore necessitating a reevaluation of the current landscape.Our study conducts a critical review of datasets and evaluation metrics, two crucial factors influencing the benchmarking of ODQA models, as shown in Fig. 1.Our study did not require the downloading or manipulation of any datasets.</p>
<p>Related Work: Several prior reviews have established a strong foundation in this area.For instance, [16] reviewed the construction methodologies used for textual MRC datasets and provided valuable insights regarding the leaderboard details. [17] predominantly explored trends in research architecture for textual ODQA, while also incorporating some examination of the datasets.With a specific focus on the English language only, [18] investigated textual MRC datasets and associated evaluation metrics.Broadening the scope, [19] studied the landscape of datasets and metrics used for evaluating both textual MRC and ODQA tasks.[20] further expanded this reach by encompassing textual and visual QA, with a focus on the datasets only.Building upon recent research in NLP, [21] performed a thorough analysis of the latest textual MRC and QA datasets.While [22] prioritized conversational QA datasets in their investigation, [23] primarily focused on visual QA datasets, offering a more specialized perspective.</p>
<p>Our review distinguishes itself by comprehensively reviewing the datasets and evaluation metrics across all major modalities for both traditional and emerging ODQA tasks.Our review includes the original ODQA datasets, along with hybrid versions that incorporate both external knowledge sources and the provided context.Additionally, we include non-ODQA datasets that have been curated for ODQA purposes.We also include datasets with diverse questions and context topics that are potentially adaptable to ODQA settings through access to external knowledge.We include datasets with answer types including word spans, yes/no responses, and free-form answers.We deliberately exclude datasets with multiple-choice and cloze-style (fillin-the-missing-word) question formats.We also exclude datasets for closed domain QA, which is limited to specific domains (e.g., medicine, law), and Knowledge Base QA (KBQA), which deals mainly with structured data organized in the form of a knowledge graph.</p>
<p>The main contributions of our review are:</p>
<p>1) Novel Taxonomy: We propose a new classification system for textual and multimodal ODQA datasets and their evaluation metrics.</p>
<p>2) Cross-lingual Dataset Analysis: To enrich our analysis and provide a more comprehensive understanding of the field, we leverage cross-lingual datasets wherever possible.</p>
<p>3) Emerging Textual ODQA Tasks: Our study inaugurates the review of datasets designed for emerging tasks within text-based ODQA.These tasks include counterfactual reasoning, handling ambiguity, time-sensitive information retrieval, and paraphrase comprehension.4) Evaluation Metrics for the LLM Era: Our study paves the way for the review of modern evaluation metrics used for ODQA systems in the context of LLMs.This includes the study of semantic similarity and LLM-based metrics.</p>
<p>5) Dataset Statistics and Resources:</p>
<p>We provide key statistics for each dataset included in our review, along with pointers to relevant online resources.</p>
<p>Structure of our Review:</p>
<p>Our review commences with a comprehensive analysis of datasets employed in modern ODQA systems.Section II categorizes these datasets into two distinct classes: textual (Section II-A) and multimodal (Section II-B).A list of links to all publicly available datasets reviewed in our study is provided at the end of Section II.After a comprehensive data analysis, Section III explores the various evaluation metrics used to assess the ODQA system's performance.These metrics are further categorized into two primary approaches: human-based (Section III-A) and automatic (Section III-B) evaluations.In each subsection, a critical discussion is presented that highlights the strengths and limitations of each evaluation technique.Finally, Section IV culminates our work by presenting key observations gleaned from the analysis, pertinent research gaps within the field, and proposing promising avenues for future research endeavors.</p>
<p>II. DATASETS</p>
<p>We propose a novel taxonomy for modern ODQA datasets.This classification scheme centers on the modality of both the input data and the knowledge sources that the system leverages.As shown in Fig. 2, our analysis divides the datasets into two fundamental categories: textual and multimodal.To further refine our analysis, we propose a subcategorization of textual datasets based on the question types that they are designed to answer.This approach allows us to explore the spectrum of challenges inherent in these datasets.In the context of multimodal datasets, we propose further sub-classification based on specific modalities integrated within a system.</p>
<p>We present a concise overview of each dataset category, including the publication year, evaluation metrics employed, number of questions, and knowledge source leveraged for answer prediction.This format is consistently applied throughout the study to facilitate comparison across datasets.</p>
<p>A. TEXTUAL ODQA DATASETS</p>
<p>Textual ODQA systems aim to answer open-ended questions solely based on textual information.In this section, we elaborate on the nine subcategories of the textual datasets within our proposed taxonomy.</p>
<p>1) SHORT-FORM</p>
<p>Short-form questions are unambiguous questions that seek precise factual information and are usually answered in short phrases or sentences.Questions starting with "wh," namely what, when, where, and why, fall under this category.Illustrative examples of short-form questions include "Where was Joe Biden born?" and "What is the capital of California?".These questions that ask for verifiable facts are also known as factoid questions.</p>
<p>a) WebQuestions</p>
<p>WebQuestions [24] is a dataset of 6,642 question-answer pairs, where the Google Suggest API was used to generate questions that begin with a "wh" word and contain exactly one named entity.The questions are supposed to be answerable on Freebase, a large knowledge graph.In DrQA [3], WebQuestions was used as an ODQA dataset by changing answer identifiers from Freebase to humanreadable entity names and leveraging English Wikipedia as the external knowledge source.The system was evaluated using Exact Match (EM) and F1 scores.</p>
<p>b) CuratedTREC</p>
<p>CuratedTREC [25] is a curated version of the TREC QA corpus [26], which is an MRC dataset.TREC stands for the "Text Retrieval Conference," which was started by the U.S. Department of Defense and the National Institute of Standards and Technology in 1992.The dataset consists of 2,180 question-answer pairs from the conference in 1999, 2000, 2001, and 2002.DrQA [3] used CuratedTREC to evaluate open domain questions by leveraging English Wikipedia as the external knowledge source.EM and F1 scores were used for the evaluation.</p>
<p>c) WikiQA</p>
<p>WikiQA [27] is a dataset of 3,047 questions sampled from real queries issued on Bing without editorial revisions.The dataset was based on user clicks, and each question was associated with a Wikipedia passage.The initial identification of question-like queries relied on simple heuristics, such as the presence of question words and question marks within the query.It also contained questions that could not be answered using Wikipedia passages.The F1 score was used for the evaluation.</p>
<p>d) SQuAD</p>
<p>SQuAD [28], which represents the "Stanford Question Answering Dataset," is an MRC dataset consisting of questions posed by crowd workers on a set of Wikipedia articles.The SQuAD dataset comes in two iterations, launched in 2016 and 2018.SQuAD 1.1 [28] consists of 107,785 question-answer pairs.SQuAD 2.0 [29] adds 53,775 unanswerable questions written adversarially by crowd workers to resemble answerable questions.DrQA [3] used SQuAD for the ODQA task by selecting QA pairs from the development set of SQuAD 1.1, discarding the associated context passages, and instead leveraging the entire English Wikipedia as the external knowledge source.EM and F1 scores were used for the evaluation.</p>
<p>e) MS Marco</p>
<p>MS Marco [30] consists of user-submitted questions sampled from the search query logs of Bing or Cortana.The first version was released in 2016 and features 100,000 questions.The most recent version, released in 2018, consists of 1,010,916 questions, along with 8,841,823 passages extracted from 3,563,535 web pages retrieved by Bing.The answers were generated by human editors in their own words instead of selecting a span of text.ROUGE-L [31] and BLEU-1 [32] scores were used to evaluate the system's performance.f) Quasar-T Quasar-T [33] stands for "Question Answering by Search and Reading (for Trivia)".The Quasar benchmark consists of two datasets.Quasar-S consists of QA pairs based on the software engineering website, Stack Overflow.However, because Quasar-S is a domain-specific dataset, it was excluded from our study.Quasar-T consists of 43,000 open domain trivia questions from multiple internet sources.ClueWeb09 [34], which contains approximately 1B web pages collected between January and February 2009, served as the background corpus for extracting answers.EM and F1 scores were used for the evaluation.</p>
<p>g) SearchQA</p>
<p>SearchQA [35] is a dataset that sources questions from the J! Archive (an archive of the American TV quiz show called Jeopardy).The questions were then submitted as queries to the Google search engine.To find potential answers, the system extracted snippets from the top 40 retrieved documents.The answers are short, exact spans of text, typically averaging between 1 and 2 words.The dataset consists of 140,461 QA pairs.Accuracy and F1 scores were used for the evaluation.</p>
<p>h) TriviaQA</p>
<p>TriviaQA [36] is a dataset consisting of 95,956 QA pairs written by trivia enthusiasts with independently gathered evidence.Documents for knowledge were collected from the Web and Wikipedia.Trivia-Web used the top 10 Web documents returned by the Bing API as the context for each question.Trivia-Wiki used the content of all Wikipedia pages that contained the entities in the question as the context.EM and F1 scores were used for the evaluation.</p>
<p>i) BoolQ</p>
<p>BoolQ [37] is a dataset of 15,942 naturally occurring yes/no questions; that is, the questions are generated in unprompted and unconstrained settings.These questions require a wide range of inference abilities to solve and were gathered from anonymized, aggregated queries issued to the Google search engine.Wikipedia served as the external knowledge source.The system was evaluated using the Accuracy score.</p>
<p>j) Natural Questions</p>
<p>Natural Questions [38] consist of 323,045 anonymized and aggregated questions issued to the Google search engine.Human annotators identified different forms of answers within the relevant Wikipedia passages for each question.ORQA [39] used Natural Questions as an ODQA benchmark by only keeping the questions with short answers (answers with 5 tokens or fewer) and using English Wikipedia as the external knowledge source.In practice, this version is most frequently referred to as NQ-Open [39].EM and F1 scores were used for the evaluation.</p>
<p>Our analysis of short-form datasets reveals a significant trend toward collections specifically curated for ODQA tasks.Several recent studies exemplify this trend by using these curated datasets to benchmark ODQA models.Among these, notable examples are: DPR [5], T5 [8], GPT-3 [10], Fusion-in-Decoder [11], DenSPI [12], ORQA [39], R3 [40], Multi-passage BERT [41], REALM [42], and RAG [43].</p>
<p>Table I summarizes the short-form datasets released between 2013 and 2019.For datasets with multiple versions, the "#Questions" column reflects the count for the most recent one.Short-form datasets vary in size, with question counts ranging from a few thousand to over one million.The most commonly used evaluation metrics are the Exact Match (EM) and F1 scores.However, for datasets containing abstractive answer formats, such as MS Marco, metrics like ROUGE-L and BLEU-1 scores are preferred.Across these datasets, Wikipedia emerges as the dominant knowledge source, although online search engines show increasing prevalence in more recent collections.</p>
<p>a) AMBIGNQ</p>
<p>AMBIGNQ [45] discussed the challenges of finding clear and unambiguous answers to open-ended factoid questions.A total of 14,042 ambiguous questions were sourced from NQ-Open [39].The dataset consists of QA pairs, with each answer having a disambiguated rewrite of the original question.The question "Where is the telephone area code 571 located?" is a prime example of an ambiguous question because it is not clear whether the user is asking for counties, cities, or a general area.A disambiguated version of this question provided by AmbigNQ is "What cities is the telephone area code 571 located?".Wikipedia served as the external knowledge source.F1 and BLEU scores were used for the evaluation.</p>
<p>b) ASQA</p>
<p>ASQA [46], which stands for "Answer Summaries for Questions which are Ambiguous," is a dataset that focuses on ambiguous factoid questions.In total, 6,316 questions were sourced from the AmbigNQ [45]  Table III summarizes the two datasets for evaluating the models on ambiguous open domain questions.AmbigNQ, released in 2020, focuses on predicting multiple possible answers, along with question rewrites that disambiguate the original intent.In contrast, ASQA emphasizes generating detailed, factual summaries that resolve ambiguity.Both datasets leverage Wikipedia as the external knowledge source and employ the F1 score along with an abstractive metric (BLEU or ROUGE-L) for the evaluation.Noticeably, AmbigNQ offers a larger collection of questions.</p>
<p>4) MULTI-HOP</p>
<p>Multi-hop questions are questions that cannot be answered directly from a single source of information.These questions necessitate a multi-step reasoning process in which the system iteratively retrieves relevant information from various sources and connects these pieces to arrive at a comprehensive answer.For instance, questions of this nature could include, "Which school that Sir Ernest Rutherford attended has the latest founding date?" and "The actress that had the role of Martha Alston plays what role in Finding Nemo?".</p>
<p>a) HotPotQA</p>
<p>HotPotQA [47] is a diverse multi-hop dataset with 112,779 Wikipedia-based QA pairs.Crowd workers were shown multiple supporting Wikipedia documents and were asked explicitly to come up with questions requiring reasoning about all documents.Crowd workers were also asked to supply sentence-level supporting facts that they used to answer the question, allowing for strong supervision, which is also provided as part of the dataset.EM and F1 scores were used as the evaluation metrics.</p>
<p>b) DROP</p>
<p>DROP [48], which stands for "Discrete Reasoning Over the Content of Paragraphs," is a crowdsourced dataset covering a variety of categories of Wikipedia pages, particularly those with a high proportion of numbers (sports game summaries and history passages).The dataset consists of 96,567 question-answer pairs.These questions require an ODQA system to be able to resolve references in a question, possibly to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting).The system was evaluated using the EM and F1 scores.
c) 2WikiMultiHopQA
2WikiMultiHopQA [49] is a multi-hop dataset consisting of 192,606 QA pairs.The original questions were sourced from HotPotQA [47] after removing the single-hop and contextdependent multi-hop questions.The final questions were generated programmatically based on predefined templates.</p>
<p>Statements from Wikidata (a collaboratively edited open knowledge source) and Wikipedia article summaries that described entities were collectively used as the external knowledge source.EM and F1 scores were used for the evaluation.</p>
<p>d) MultiSpanQA</p>
<p>MultiSpanQA [50] consists of questions that require answers to be extracted as multiple discontinuous spans from a text passage.The dataset was created by extracting raw questions and Wikipedia spans from the Natural Questions [38] dataset and then re-annotating them to identify multispan answers.The dataset consists of over 6,000 multi-span questions in its basic version and expands to 19,608 examples, including unanswerable questions and those with single and multi-span answers.EM and F1 scores were used as the evaluation metrics.</p>
<p>e) QAMPARI QAMPARI [51], which stands for "Questions with Many Answers over Multiple Paragraphs, Indeed," is a crowdsourced dataset of 63,911 questions where the answers are lists of entities spread across many Wikipedia paragraphs.All questions had at least 5 answers, with an average of 13.Examples were generated semi-automatically using the tables from Wikidata and Wikipedia.The questions were then verified to ensure that they could be answered using Wikipedia passages.Finally, the questions were paraphrased in natural language, which makes them suitable for ODQA.The F1 score was used as the evaluation metric.</p>
<p>f) ConcurrentQA</p>
<p>ConcurrentQA [52] is a multi-hop crowdsourced dataset of 18,439 questions spanning Wikipedia passages in the public domain and an open-source email collection in the private domain.The format, privacy settings, noise, entity distributions, length of emails, and Wikipedia passages differ in several ways, making this dataset challenging.The system was evaluated using the EM and F1 scores.</p>
<p>The multi-hop datasets released between 2018 and 2023 are summarized in Table IV.The most prevalent evaluation metrics employed are EM and F1 scores.Wikipedia emerges as the most frequent knowledge source across these datasets.However, ConcurrentQA introduces a novel knowledge source by incorporating emails with Wikipedia.2WikiMultiHopQA stands out with the largest collection of multi-hop questions.</p>
<p>5) CONVERSATIONAL</p>
<p>Conversational questions are questions that emulate natural conversation patterns.In contrast to traditional QA, which is designed to address isolated questions, conversational questions aim to comprehend the overarching context of human dialogue and facilitate a series of interconnected questions.</p>
<p>a) QuAC</p>
<p>QuAC [53], which stands for "Question Answering in Context," is a conversational QA dataset in which questionanswer pairs are created based on sections of Wikipedia articles.The dataset consists of 98,407 questions and 13,594 conversations.Each conversation involved two crowd workers: the first posed a student who asked a few questions to learn about a hidden passage from Wikipedia, and the second acted as a teacher to answer questions by providing short excerpts from the Wikipedia passage.The questions were more open-ended, unanswerable, or meaningful only within the conversation.The Human Equivalence Score (HEQ) [53] and F1 score were used for the evaluation.</p>
<p>b) QBLink</p>
<p>QBLink [54] consists of 56,000 questions for sequential question answering, where the questioner asks multiple related questions about the same concept one-by-one.After each question, the answerer provided an answer before the next question was asked.The questions were fully humanauthored, and Wikipedia was used as the external knowledge source.The system was evaluated using the Exact Match score.</p>
<p>c) CoQA</p>
<p>CoQA [55] is a conversational question answering dataset that consists of passages and human conversations which involves a sequence of question-answer pairs about a passage.The dataset consists of approximately 127,000 question-answer pairs related to about 8,000 conversations.The passages were extracted from documents in seven different domains: child stories, literature, middle and high school English exams, news articles, Wikipedia articles, Reddit articles, and science articles.The F1 score was used for the evaluation of the system's performance.</p>
<p>AbgCoQA [56] is an augmented version of the CoQA dataset that contains ambiguous questions.</p>
<p>d) OR-QuAC</p>
<p>OR-QuAC [57] is a dataset that enhances QuAC [53] by adapting it to an open-retrieval setting, making it more suitable for ODQA.The dataset consists of 40,527 questions related to 5,644 conversations.It is an aggregation of two existing datasets: (1) the QuAC dataset, which offers information-seeking conversations; and (2) the Canard dataset [58], which consists of context-independent rewrites of QuAC questions.The Wikipedia corpus served as the external knowledge source for answering the questions.The HEQ and F1 scores were used for the evaluation.</p>
<p>e) QReCC</p>
<p>QReCC [59], which stands for "Question Rewriting in Conversational Context," is an open domain conversational QA dataset that consists of 13,700 conversations with 81,000 question-answer pairs.The task in QReCC was to find answers to conversational questions within a collection of 10 million web pages (split into 54 million passages).The answer may span across multiple webpages.EM and F1 scores were used to evaluate the system's performance.</p>
<p>f) TopiOCQA</p>
<p>TopiOCQA [60] is an open domain conversational dataset with topic switches based on Wikipedia.It consists of 3,920 conversations and 50,574 QA pairs.Conversations start with a real information-seeking question from the Natural Questions [38] dataset, which determines a seed topic (document), and then the questioner may shift to other related topics as the conversation progresses.EM and F1 scores were used for the evaluation.</p>
<p>g) Topical-Chat</p>
<p>Topical-Chat [61] is an open domain knowledge-grounded conversation dataset without explicit roles for conversation partners and with transitions in conversations.The dataset consists of 10,784 conversations with question-answer pairs embedded in them.External knowledge was collected specifically for 300 popular entities from Wikipedia, Reddit, and the Washington Post.The system was evaluated using the F1 score.</p>
<p>Table V summarizes the conversational datasets released between 2018 and 2023.Common evaluation metrics include EM and F1 scores, with some datasets adopting the novel Human Equivalence Score (HEQ).Wikipedia reigns as the primary knowledge source, while newer datasets like QReCC and Topical-Chat explore websites for this purpose.CoQA stands out as the dataset with the most extensive question collection.</p>
<p>6) CROSS-LINGUAL</p>
<p>While large and annotated datasets exist for English QA, creating such resources for all languages is practically unfeasible, especially for low-resource ones.Thus, crosslingual ODQA offers a solution to this challenge.Crosslingual questions are questions posed in a target language (e.g., Turkish), but the system relies on languageindependent features and leverages answers from a source language (e.g., English) with abundant training data.</p>
<p>a) XQA</p>
<p>XQA [62] is a cross-lingual dataset where the questionanswer pairs were sourced from the "Did you know" section of Wikipedia pages.The dataset consists of 90,610 QA pairs in 9 languages: English, French, German, Portuguese, Polish, Chinese, Russian, Ukrainian, and Tamil.EM and F1 scores were used for the evaluation.</p>
<p>b) MLQA</p>
<p>MLQA [63] consists of QA pairs in 7 languages: English, Arabic, German, Spanish, Hindi, Vietnamese, and Simplified Chinese.It consists of 12,738 extractive QA pairs in English and between 5,029 and 6,006 pairs in each of the other languages.The combined dataset for all languages consists of 46,444 question-answer pairs.Wikipedia article excerpts served as the knowledge source for the system.EM and F1 scores were used for the evaluation.</p>
<p>c) XQuAD</p>
<p>XQuAD [64] is a dataset that consists of 240 Wikipedia paragraphs and 1,190 question-answer pairs from the development set of SQuAD v1.1 [28], together with their professional translations in 10 languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi.EM and F1 scores were used for the evaluation.</p>
<p>d) TyDiQA</p>
<p>TyDiQA [65], which stands for "Typologically Diverse Question Answering," is a dataset covering 11 typologically diverse languages (English, Arabic, Bengali, Finnish, Indonesian, Japanese, Kiswahili, Korean, Russian, Telugu, and Thai).The dataset consists of 204,337 question-answer pairs.The questions were generated by annotators by showing them text segments containing the first 100 characters of Wikipedia articles.The annotators were encouraged to ask about anything interesting that came to mind, regardless of whether the questions were unrelated.The F1 score was used as the evaluation metric.</p>
<p>e) XTREME</p>
<p>XTREME [66] is a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multi-lingual representations across 40 typologically diverse languages spanning 12 language families.The dataset consists of 142,154 question-answer pairs.These tasks included MLQA [63], XQuAD [64], and TyDiQA-GoldP.TyDiQA-GoldP is the gold passage version of the TyDiQA [65] benchmark, which uses only the gold passage as a context and excludes unanswerable questions.The system was evaluated using the EM and F1 scores.</p>
<p>f) XOR-TyDiQA</p>
<p>XOR-TyDiQA [67] consists of 40,000 information-seeking questions across 7 diverse non-English languages (Arabic, Bengali, Finnish, Japanese, Korean, Russian, and Telugu) for which TyDiQA [65] could not find same-language answers.English Wikipedia and the specific language's Wikipedia were used as the external knowledge sources.EM, F1, and BLEU scores were used for the evaluation.</p>
<p>g) MKQA</p>
<p>MKQA [68] is a large-scale open domain dataset that consists of 10,000 question-answer pairs across each of the 26 typologically diverse languages (260,000 questionanswer pairs in total).English Wikipedia and the specific language's Wikipedia served as the external knowledge sources.EM and F1 scores were used for the evaluation of the system's performance.</p>
<p>h) GEN-TyDiQA</p>
<p>GEN-TyDiQA [69] is an open domain generative question answering dataset.The dataset consists of 2,686 humangenerated answers for 4,038 questions.It is an extension of the TyDiQA [65] dataset with human-generated, naturalsounding, and complete answers for 5 languages: Arabic, Bengali, English, Japanese, and Russian.Wikipedia was used as the external knowledge source.Accuracy, BLEU, and ROUGE-L scores were used for the evaluation.</p>
<p>Cross-lingual datasets released between 2019 and 2022 are summarized in Table VI.While traditional evaluation metrics, such as EM and F1 scores, remain prevalent, newer datasets have adopted metrics suited to their tasks.For instance, XOR-TyDiQA employs BLEU, and GEN-TyDiQA leverages both BLEU and ROUGE-L to assess the quality of the generated text.Wikipedia serves as the dominant source of knowledge across these datasets.Notably, MKQA boasts the largest question collection, whereas XTREME stands out for its inclusion of the greatest number of typologically diverse languages.</p>
<p>7) TIME-SENSITIVE</p>
<p>Time-sensitive questions are those for which the answer can vary depending on the specific time the question is posed.These questions necessitate the system to consider the current time or a designated point in time to deliver an accurate response.Examples of such questions include "How old is Barack Obama?" and "Who won the Oscars for the best actor in 2016?".</p>
<p>a) SituatedQA</p>
<p>SituatedQA [70] consists of 11,000 questions that are dependent on time or geography.Time dependent questions were sourced from a variety of existing datasets, such as WebQuestions [24], MS Marco [30], NQ-Open [39], and TyDiQA [65].Geographically dependent questions were generated separately by modifying existing questions from the NQ-Open dataset based on heuristics.Wikipedia was used as the external knowledge source.EM and F1 scores were used for the evaluation.</p>
<p>b) TimeQA</p>
<p>TimeQA [71] is a dataset of 41,172 questions with timeevolving facts.The questions were first identified from Wikidata, and crowd workers then annotated the boundaries of these facts by aligning them with Wikipedia passages.The questions were classified into 'easy' or 'hard' categories based on the difficulty of both temporal understanding and reasoning.The system was evaluated using the EM and F1 scores.</p>
<p>c) FreshQA</p>
<p>FreshQA [72] consists of 600 crowdsourced questions that require fast-changing world knowledge, as well as questions with false premises that need to be debunked.The annotators were asked to write questions that involved fresh knowledge and appeared natural.The authors used their dataset to benchmark pre-trained LLMs that do not have access to realtime data or the ability to browse the internet for current information, and they manually evaluated the answers themselves under two different settings.</p>
<p>Table VII summarizes the time-sensitive datasets released between 2021 and 2023.Although traditional evaluation metrics such as EM and F1 scores remain prevalent, newer, potentially smaller datasets like FreshQA have facilitated the use of human evaluation.Wikipedia serves as the primary knowledge source for most of these datasets.Conversely, FreshQA explores a unique approach by leveraging pretrained LLMs without any external knowledge injection.SituatedQA stands out as the collection that boasts the largest volume of such time-sensitive questions.</p>
<p>8) PARAPHRASED</p>
<p>Paraphrased questions represent formulations of the same fundamental question, using alternative wording or syntactic structures.An example of a paraphrased set of questions is "What causes seasons on Earth?" and "What causes the change of seasons on Earth?".</p>
<p>a) ComQA</p>
<p>ComQA [73] consists of 11,214 real-user questions collected from the WikiAnswers community QA website.Questions were grouped into 4,834 paraphrase clusters along with answers through a large-scale crowdsourcing effort, which captures lexical and syntactic variety.Crowd workers were shown pairs of questions from a cluster and were asked to make a binary decision on whether the two questions were paraphrases.Wikipedia served as the external knowledge source.The F1 score was used for the evaluation.</p>
<p>b) Paraphrased-SQuAD</p>
<p>Paraphrased-SQuAD [74] consists of 2 sets of paraphrased questions obtained using a paraphrasing model and the paraphrase database PPDB [75].The first dataset is a collection of 1,062 non-adversarial paraphrases.These questions have been rewritten with minor variations from their original forms.The second dataset is an adversarial paraphrased set that consists of 56 questions paraphrased using context words near a confusing candidate answer.The original questions were sourced from the development set of SQuAD [28], which uses Wikipedia paragraphs as the knowledge source.Semantically dissimilar paraphrases were discarded after the paraphrase generation.EM and F1 scores were used for the evaluation.</p>
<p>The paraphrased datasets are summarized in Table VIII.Traditional evaluation metrics, such as EM and F1 scores, are commonly used.Wikipedia is the primary knowledge source across these datasets.Although published in the same year (2019), these datasets employ distinct paraphrase identification methods.ComQA leverages crowdsourcing to identify paraphrased questions.Conversely, Paraphrased-SQuAD utilizes a paraphrased database and model generation.ComQA boasts a larger collection of questions.CREPE [76] consists of 8,400 Reddit questions, 25% of which contain false presuppositions (especially about unfamiliar topics).Because of the inherent debatability of these false presuppositions, the authors used the most upvoted comments written by community users to annotate the data efficiently.False presuppositions and their corrections were also annotated.Wikipedia was used as the external knowledge source.F1 and BLEU scores were used for the evaluation.</p>
<p>b) TruthfulQA</p>
<p>TruthfulQA [77] is a dataset of questions designed to cause imitative falsehoods.To perform well, models must avoid generating false answers learned by imitating human texts.The dataset consists of 817 questions spanning 38 categories, including health, law, finance, and politics.Each question has sets of true and false reference answers and a source that supports the answers.All questions were written by the authors and were designed to be adversarial, i.e., testing for a weakness in the truthfulness of ODQA systems.Pre-trained LLMs were used to generate the answers.Human ratings and LLM-based metrics were used for the evaluation.</p>
<p>c) IfQA</p>
<p>IfQA [78] consists of questions with counterfactual presuppositions via an "if" clause.The dataset consists of 3,800 questions that were annotated by crowd workers on relevant Wikipedia passages after filtering out those passages that were not related to describing causal events.EM and F1 scores were used for the evaluation.</p>
<p>Counterfactual question datasets, released between 2022 and 2023, are summarized in Table IX</p>
<p>B. MULTIMODAL ODQA DATASETS</p>
<p>In this section, we explore datasets classified according to their utilization of multiple modalities.Multimodal ODQA leverages information from diverse modalities to enhance its comprehension and answer generation capabilities.This allows the system to understand and answer questions in a more comprehensive manner, closer to how humans process information naturally.After text, images and tables are the most commonly used non-textual modalities.The next subsections examine datasets that use textual and image data, followed by datasets that use textual data with tabular data.Finally, the last subsection analyzes datasets that exploit all three modalities: text, image, and table.Examples of multimodal questions include "What is the capital of France and what does it look like?" and "What do these animals usually eat?" (based on Fig. 3(a) and 3(b) as the inputs).</p>
<p>1) TEXT AND IMAGES</p>
<p>In this subsection, we focus on multimodal datasets that combine text and images.</p>
<p>a) OK-VQA</p>
<p>OK-VQA [79], which stands for "Outside Knowledge (OK) Visual Question Answering (VQA)," is a dataset consisting of 14,055 open-ended questions where the image content is not sufficient to answer the questions, therefore requiring access to external knowledge.The images were sourced from the COCO [80] image dataset, and passages from Wikipedia articles were provided as external knowledge.The VQA Accuracy [81] score was used for the evaluation.</p>
<p>b) S3VQA</p>
<p>S3VQA [82] involves the Select, Substitute, and Search (SSS) technique for open domain visual question answering.S3 reaches the end result (i.e., natural language answer) for the VQA-type query by first reformulating the input question (using Select and Substitute) and then retrieving external knowledge source facts (using Search).The dataset is based on the Open Image Collection [83] and consists of 6,765 question-answer pairs.Wikipedia articles were provided as external knowledge.The VQA Accuracy score was used for the evaluation of the system.</p>
<p>c) MIMOQA</p>
<p>MIMOQA [84] is a dataset in which both input and output are multimodal, incorporating textual and visual elements.Those QA pairs were selected from the MS Marco [30] and Natural Question [38] datasets that were originally extracted from Wikipedia.All images were scraped from the original articles.The dataset consists of 56,693 question-answer pairs.ROUGE and BLEU scores were used as the evaluation metrics for text.Precision@1, 2, and 3 scores were used as the evaluation metrics for images.</p>
<p>d) A-OKVQA</p>
<p>A-OKVQA [85] is an augmented version of the OK-VQA [79] dataset.It consists of a larger and more diverse set of 24,903 QA pairs, requiring a broad base of both commonsense reasoning and different types of world knowledge to answer.To ease working with unbounded knowledge sources, questions in the training set were paired with rationales that supply facts and snippets of reasoning needed to answer them.The VQA Accuracy score was used as the evaluation metric.</p>
<p>e) WebQA</p>
<p>WebQA [86] is a multi-hop, multimodal dataset with 46,700 QA pairs.It is structured to mimic human web interaction, which involves asking a question, selecting sources, and generating a fluent language response.The system identifies the relevant information across modalities and combines it with reasoning to answer the query.Wikipedia and Bing image search were used as the external knowledge sources.</p>
<p>The BARTScore [87] for fluency and the F1 score for accuracy were used for the evaluation.</p>
<p>Table X summarizes the multimodal datasets that combine text and images.Although VQA Accuracy remains prevalent, newer datasets explore alternative evaluation metrics.MIMOQA uses abstractive evaluation metrics such as ROUGE and BLEU for text and Precision@k (where k = 1, 2, and 3) for image outputs.WebQA employs the BARTScore metric to assess the semantic similarity of the generated text.Wikipedia continues to be the primary knowledge source.Notably, MIMOQA boasts the most extensive question collection.</p>
<p>2) TEXT AND TABLES</p>
<p>In this subsection, we focus on multimodal datasets that combine text and tables.</p>
<p>a) HybridQA</p>
<p>HybridQA [88] is a QA dataset over both tabular and textual data.Each question is aligned with a Wikipedia table and multiple free-form corpora linked to the entities in the table.The questions were designed to aggregate both tabular and textual information.The dataset consists of 70,000 QA pairs aligned with 13,000 Wikipedia tables.EM and F1 scores were used as the evaluation metrics.</p>
<p>b) OTT-QA</p>
<p>OTT-QA [89] is a dataset of 45,814 questions that requires multi-hop reasoning across tabular data and unstructured text.The dataset was built on top of HybridQA [88].OTT-QA requires a system to retrieve relevant tables and text.EM and F1 scores were used for the evaluation.</p>
<p>In Table XI, we summarize multimodal datasets containing text and tables.Both datasets leverage traditional evaluation metrics such as EM and F1 scores.Notably, Wikipedia serves as the primary knowledge source for these datasets, while HybridQA offers a larger question set.</p>
<p>3) TEXT, IMAGES AND TABLES</p>
<p>In this subsection, we explore multimodal datasets that combine text, images, and tables.</p>
<p>a) ManyModalQA</p>
<p>ManyModalQA [90] is a dataset of 10,190 questions for multimodal question answering, which leverages three different modalities: text, images, and tables.The dataset for this challenge was created by scraping Wikipedia and then generating QA pairs through crowdsourcing.A distinctive feature of the dataset is the ambiguity of its questions, which were crafted such that the modality containing the answer is not immediately apparent from the question.The Accuracy score was used for the evaluation.</p>
<p>b) MultiModalQA</p>
<p>MultiModalQA [91] is a dataset that requires joint reasoning over text, tables, and images.The questions in the dataset were generated using tables from Wikipedia.The images and text paragraphs of entities that appeared in each table were then attached.The dataset consists of 29,918 QA pairs, 35.7% of which required cross-modal reasoning.EM and F1 scores were used for the evaluation.</p>
<p>c) MMConvQA</p>
<p>MMConvQA [92] is a dataset for conversational question answering that uses multiple knowledge sources of different modalities, such as text, tables, and images, through multiturn conversations.The dataset consists of 1,179 conversations and 5,753 QA pairs.The questions were annotated with natural language answers, corresponding evidence, and decontextualized self-contained questions.EM and F1 scores were used for the evaluation.</p>
<p>Table XII summarizes multimodal datasets containing text, images, and tables.Traditional evaluation metrics like EM and F1 scores remain common.Notably, Wikipedia serves as the primary knowledge source for these datasets.Among them, MultiModalQA boasts the largest collection of question-answer pairs.With this, we conclude Section II, which provided an overview of various datasets for textual and multimodal ODQA.To facilitate further research, we have provided references for accessing the publicly available datasets in Table XIII.The Gen-TyDiQA and MIMOQA datasets are not included in the table because we could not find any public links to them.</p>
<p>III. EVALUATION</p>
<p>ODQA evaluation can be broadly categorized into two main approaches: Human Evaluation and Automatic Evaluation (Fig. 4).We further decompose Automatic Evaluation into three categories: Lexical (focusing on word-level properties), Semantic (assessing meaning representation), and LLM-based (utilizing large language models for evaluation).To conclude the analysis of each evaluation method, we examine its strengths, limitations, and relevant findings from the existing literature.We will delve into Human Evaluation in Section III-A and Automatic Evaluation in Section III-B.</p>
<p>A. HUMAN EVALUATION</p>
<p>Human evaluation provides a holistic assessment of an ODQA system's ability to understand, reason, and generate informative responses that align with the user's intent.Prior studies have relied on human evaluations to assess the quality of QA models [93], [94].</p>
<p>Human Rating is the most common form of human evaluation, which refers to the establishment of a welldefined evaluation process involving a recruited group of human raters in the relevant domain.These raters assess the answers based on a pre-determined set of clear and objective criteria.The criteria might include:</p>
<p>• Factual correctness of the answer.</p>
<p>• Relevance to the question and its intent.</p>
<p>• Completeness and informativeness of the answer.</p>
<p>• Clarity and fluency of the language used.• Originality and insightfulness of the answer.</p>
<p>An interface is set up where raters can see the question, the ODQA system's answer, and provide ratings based on the defined criteria.Some evaluations might involve a binary rating (correct or incorrect) for factual questions, while others might use a Likert scale (1-5) for subjective criteria, such as relevancy, as shown in Fig. 5. Once the evaluations are complete, the data is analyzed to assess the overall performance of the ODQA system.This might involve calculating the average scores across different criteria or identifying areas where the system struggles.Strengths: In the realm of ODQA systems, human evaluation remains the gold standard for assessing system quality.This is due to the inherent ability of human evaluators to assess a system's capability for nuanced language comprehension.Evaluators can effectively judge a system's proficiency in grasping the subtleties of natural language use and crafting responses that are not only factually accurate but also contextually appropriate.</p>
<p>Limitations: Inter-rater reliability (IRR) [95], a statistical measure of agreement between multiple raters, is paramount for ensuring the consistency and validity of human evaluations.A low IRR can introduce noise into the data and undermine the generalizability of the findings.Several factors can influence the IRR, such as rater training, subjectivity in ratings, and clarity of the evaluation criteria.Furthermore, human evaluation can be time-consuming and resource-intensive, particularly for large-scale datasets.</p>
<p>Related Analysis:</p>
<p>The authors of [94] found that the performance of QA systems increased by 23% on average when using human judgments for evaluation.</p>
<p>B. AUTOMATIC EVALUATION</p>
<p>The automatic evaluation of an ODQA system offers a valuable complement to human assessment.Automatic evaluation refers to the process of using computer programs to assess how well the system performs.This contrasts with human evaluation, in which people judge the quality of the answers.In automatic evaluation, reference answers play a critical role.These high-quality answers represent the expected output for a given question and serve as a benchmark for assessing the performance of the system.We discuss three primary categories of automatic evaluation techniques in the following subsections: Lexical Matching, Semantic Similarity, and LLM-based.Each category provides distinct insights into the performance of the ODQA systems.</p>
<p>1) LEXICAL EVALUATION</p>
<p>Lexical evaluation is used to assess how similar two pieces of text are based on their syntactic vocabulary.This technique focuses on the surface level, looking at characters, words, and n-grams (sequences of words).An n-gram is a way of describing a set of n consecutive words in a sentence.</p>
<p>In the sentence "The sky is blue," we can identify various ngrams, as shown below:</p>
<p>• 1-gram (unigram): "The", "sky", "is", "blue" • 2-gram (bigram): "The sky", "sky is", "is blue" • 3-gram (trigram): "The sky is", "sky is blue" • 4-gram: "The sky is blue" Note that the words in an n-gram are taken in order, so "the sky blue is" is not a valid 4-gram.Traditional lexical matching methods have been quite popular in ODQA evaluation [3], [11], [39].</p>
<p>a) Accuracy</p>
<p>Accuracy reflects the proportion of questions that a system answers correctly, as shown in (1).The definition of correctness can vary depending on the use case and is dependent on the implementation.A question could correspond to one or more correct answers (and could be words, phrases, or sentences).
Accuracy = Number of Correct Answers Number of Questions(1) b) Exact Match
Exact Match (EM) refers to the proportion of questions where the system's response aligns precisely with the reference answer, with no word-level discrepancies.Even a small deviation results in a score of 0. Although it is easy to understand and implement, exact matches can be overly strict.Natural language often allows for paraphrasing and slight variations, and a system might be penalized for not capturing the exact wording even if the meaning is conveyed accurately.</p>
<p>c) Precision</p>
<p>Precision refers to the proportion of correctly predicted words in the system's response compared to the total number of words in that response, as shown in (2).Precision focuses on accuracy.</p>
<p>Precision = Number of Correctly Predicted Words Number of Words in the Prediction</p>
<p>(2)</p>
<p>d) Recall</p>
<p>Recall refers to the proportion of correctly predicted words in the system's response compared to the total number of words in the reference answer, as shown in (3).Recall focuses on coverage and is also referred to as Sensitivity.</p>
<p>Recall = Number of Correctly Predicted Words Number of Words in the Reference</p>
<p>(3)
e) F1 Score
F1 score is defined as the harmonic mean of precision and recall.It is a widely used metric because it provides a single score that considers both precision and recall, as shown in (4).A high F1 score indicates that the system is both good at finding correct answers (high recall) and avoiding incorrect answers (high precision).
F1 = 2 × Precision × Recall Precision + Recall(4)
f) ROUGE ROUGE [31] stands for "Recall-Oriented Understudy for Gisting Evaluation."It is a set of metrics originally proposed to evaluate text generation models (summarization or machine translation).Currently, ROUGE is commonly used for the evaluation of ODQA systems.ROUGE-N is the ngram recall score between predicted and reference answers.Similarly, ROUGE-L is calculated based on the longest common subsequence between predicted and reference answers.ROUGE-S is a skip-gram concurrence metric that allows one to search for consecutive words from the reference answer that appear in the predicted answer but are separated by one or more other words, thereby adding a degree of leniency.Another variant, ROUGE-W, assesses weighted word overlap, assigning more weight to specific words or phrases.ROUGE emphasizes recall.</p>
<p>g) BLEU</p>
<p>BLEU [32], which stands for "Bilingual Evaluation Understudy," is a metric originally proposed for the evaluation of machine translation.The BLEU score is an ngram precision score between the predicted and reference answers.It incorporates a brevity penalty with an exponential decay that penalizes predicted answers that are too short compared to the reference answers.The brevity penalty addresses the absence of a recall component in the BLEU score.</p>
<p>h) METEOR</p>
<p>METEOR [96] stands for "Metric for Evaluation of Translation with Explicit Ordering."Unlike BLEU [32], METEOR considers both precision and recall.This provides a more balanced evaluation.In addition, it also includes features such as synonym matching.The final meteor score combines the F1 score computed from precision and recall with the chunk penalty.Meteor-next [97], a new metric, includes improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words.</p>
<p>i) HEQ</p>
<p>HEQ [53] stands for "Human Equivalence Score."It is an evaluation metric that has been used in conversational question answering systems such as QuAC [53].For systems with multiple valid answers, the F1 score may be misleading.HEQ is an evaluation metric for judging whether the output of the system is as good as that of an ordinary person.Given n references, the average of the maximum F1 computed from each n-1 subset with respect to the held-out reference is considered the F1 score of humans.Following that, let's suppose a QA task contains N questions, and the number of questions for which the token-level F1 performance of the system exceeds or reaches the token-level F1 score of humans is M. The HEQ score can then be computed, as shown in (5).</p>
<p>HEQ = 𝑀 𝑁</p>
<p>(5)</p>
<p>j) VQA Accuracy</p>
<p>VQA Accuracy [81] is a commonly used metric in visual question answering that is designed to mitigate inter-rater variability in answer phrasing.It is calculated as the minimum between the ratio of human agreements to a fixed value (typically 3) and 1, as shown in (6).Therefore, this scoring method grants a full point for a question if the system's answer matches that of three or more human annotators.The selection of matching techniques, such as exact matching or another form of lexical matching, depends on the specific application and its requirements.</p>
<p>VQA Accuracy = min .#humans that provided the answer 3 , 1&lt;</p>
<p>Strengths: Lexical metrics are straightforward to comprehend and implement because they focus on textual features of the language.Furthermore, their well-defined nature allows for easy automation and scalability, making them suitable for large-scale text processing applications.</p>
<p>Limitations: Lexical metrics exhibit limitations in their ability to capture semantic nuances.They primarily focus on surface-level text features and struggle to account for variations in text structures, synonyms, and paraphrasing.</p>
<p>Related Analysis:</p>
<p>Several studies have investigated the applicability of ROUGE and BLEU across various Natural Language Generation (NLG) tasks.[98] demonstrated that, in the context of question generation, lexical metrics exhibit a weak correlation with the concept of answerability.In their work, [99] investigated the limitations of ROUGE and BLEU in evaluating QA systems, particularly for yes-no and entity question types.For such datasets, a single word difference between the generated and reference answer can represent a significant semantic difference (i.e., a correct answer becomes incorrect).Consequently, [99] argued that ROUGE and BLEU scores in these scenarios might not accurately reflect the true quality of the generated answer.</p>
<p>Based on their study, [100] indicated that while the F1 score can be a suitable metric for many span-based QA datasets, its performance can be impacted by the specific characteristics of the questions and answers, therefore highlighting the importance of considering these factors when selecting or adapting the evaluation metrics for QA tasks.In addition, [100] advocated the use of the METEOR metric for evaluating generative QA systems.Their findings demonstrate that METEOR achieves the strongest correlation with human judgments of quality.</p>
<p>2) SEMANTIC EVALUATION</p>
<p>Semantic evaluation leverages semantic representations to capture the meaning of an answer by focusing on semantic similarity rather than word overlap.This enables semantic metrics to account for variations in phrasing.Evaluation can be seen as a classification task in which the goal is to determine whether a reference answer and a predicted answer are semantically equivalent.These metrics have been instrumental in assessing the effectiveness of various NLG tasks, including machine translation, image captioning [101], dialogue systems [102], and text summarization [87].For the task of ODQA, the efficacy of semantic metrics has been studied and evaluated by [100], [103], and [104].</p>
<p>a) Word Mover's Distance</p>
<p>Word Mover's Distance (WMD) [105] is based on word embeddings that learn semantically meaningful representations for words from local co-occurrences in sentences.The WMD measures the dissimilarity between two text documents as the minimum distance that the embedded words of one document need to "travel" to reach the embedded words of another document.Because WMD uses word embeddings only, it might not fully capture the nuances of context within a sentence (grouping of words).</p>
<p>b) Sentence Mover's Similarity</p>
<p>Sentence Mover's Similarity (SMS) [106] is a metric based on sentence embeddings instead of word embeddings, therefore providing the metric access to higher-level representations of text.SMS uses sentence representations that are the averages of their word embeddings.</p>
<p>c) BERTScore</p>
<p>BERTScore [101] is an automatic evaluation metric for text generation that first obtains representations for each word in the generated answer and the reference answer by feeding them separately through the BERT [6] (Bidirectional Encoder Representations from Transformers) model and then further proceeds to compute pairwise cosine similarity scores.</p>
<p>d) BLEURT</p>
<p>BLEURT [102] is an end-to-end trained metric based on the BERT [6] model.It was designed to model human judgment more effectively.The model is trained on a combination of real and synthetic data.Therefore, its effectiveness is particularly notable in scenarios where training data is scarce or out-of-distribution.</p>
<p>e) MoverScore</p>
<p>MoverScore [107] is a metric that takes inspiration from WMD [105] to formulate another optimal matching metric that uses contextualized embeddings to compute the Euclidean distances between words or n-grams.In contrast to BERTScore [101], which allows one-to-one hard matching of words, MoverScore allows many-to-one matching because it uses soft or partial alignments.</p>
<p>f) BARTScore</p>
<p>BARTScore [87] is a metric based on the idea that the evaluation of generated text should be considered as a text generation problem, directly evaluating text through the lens of its probability of being generated from or generating other textual inputs and outputs, using pre-trained encoderdecoder models.It can better support the evaluation of generated text from different perspectives (e.g., informativeness, coherence, and factuality) by adjusting the inputs and outputs of the conditional text-generation problem.</p>
<p>g) Semantic Answer Similarity</p>
<p>Semantic Answer Similarity (SAS) [108] is a metric that uses a cross-encoder architecture and transformer-based language models that are pre-trained on Semantic Textual Similarity (STS) datasets.The reference and generated answers are separated by a special token to calculate the similarity score.</p>
<p>Strengths: Semantic similarity metrics offer an objective approach for quantifying relationships between texts.By capturing the meaning and intent of the language, these metrics enable systems to effectively handle variations in sentence structure, formation, phrasing, and expression.</p>
<p>Limitations: Since semantic similarity metrics provide a relative assessment of the similarity between text pieces, they are not capable of determining an absolute measure of truth.</p>
<p>Related Analysis:</p>
<p>The authors of [100] argued that while semantic similarity metrics perform well for evaluating summarization and translation, that does not necessarily indicate success in QA evaluation since they were shown to exhibit poor correlation with human judgments.Although [104] attempted to improve semantic similarity evaluation by incorporating both question and answer during similarity computation, they also concluded that such metrics remain inadequate for assessing the performance of QA tasks.Furthermore, [104] highlighted the sensitivity of these metrics to the selection of a similarity threshold, which can lead to inconsistencies in the evaluation results.[109] noted that these metrics lack the ability to capture attributability.</p>
<p>3) LLM-BASED EVALUATION</p>
<p>Large language models (LLMs) have emerged as a focal point in recent research.This is primarily driven by their proficiency in handling intricate linguistic and reasoning tasks, along with their ability to generate coherent textual outputs.The GPT series, including models such as GPT-3.5 and its chat variant ChatGPT-3.5 (https://chatgpt.com),are prominent examples of this technological advancement.LLMs are evolving beyond answer generation to encompass potential applications for evaluation and judgment tasks.This approach leverages the strengths of LLMs trained on massive amounts of text data to evaluate the outputs generated by other potentially smaller LLMs.The idea is to use LLMs to score the generated answer under the assumption that the LLMs have learned to assign higher probabilities to high-quality and fluent texts.In scenarios where reference answers are unavailable, the internal knowledge of LLMs serves as the primary mechanism for evaluating the plausibility and correctness of the generated answer.However, when reference answers are provided, LLMs can leverage them to verify the accuracy of the generated answer, thereby enhancing the overall reliability, as shown in Fig. 6 and 7. Beyond factual correctness, LLMs can additionally generate scores for qualitative dimensions of the response, such as:</p>
<p>• Relevance: Does the answer directly correspond to the information sought in the question?• Coherence: Does the answer exhibit a clear and logical structure that facilitates comprehension of the key points?• Fluency: Does the answer exhibit both natural language fluency and grammatical accuracy?</p>
<p>Furthermore, a growing body of research has demonstrated that LLMs possess a surprising range of emergent capabilities when coupled with appropriate tuning or prompting methodologies.These capabilities include incontext learning [110], chain-of-thought (CoT) reasoning [111], and zero-shot instruction following [112].This highlights the potential of using such techniques to develop more robust evaluation methods.[103] and [104] studied LLM-based evaluation techniques for the ODQA task.</p>
<p>a) GPTScore</p>
<p>GPTScore [113] is a novel evaluation metric that utilizes the emergent abilities (e.g., zero-shot instruction) of generative pre-trained models to score generated texts.GPTScore used GPT3.5 with a model size of 175B, achieving multi-aspect, customized, and training-free evaluation.</p>
<p>b) G-Eval</p>
<p>G-Eval [114] is a prompt-based evaluator that uses LLMs with chain-of-thought reasoning [111] to evaluate the quality of generated text in a form-filling paradigm.The task introduction and the evaluation criteria are only fed as prompts, and LLMs are probed to generate a CoT of detailed evaluation steps.The prompt and the generated CoT are then used to evaluate the NLG outputs.The evaluator output is formatted as a form.Strengths: LLMs demonstrate exceptional promise in evaluating the accuracy of predicted answers, owing to their remarkable capabilities in language processing and reasoning.LLMs can further streamline qualitative assessments by automating time-consuming and resourceintensive tasks that previously relied on human judgment.</p>
<p>Limitations: LLMs are inherently susceptible to the way they are prompted.The phrasing, style, and content of a prompt can significantly influence the output of LLMs.[115] noted and examined several potential limitations of the "LLM-as-a-Judge" approach, including position bias, verbosity bias, self-enhancement bias, and limited reasoning ability, as well as mitigation strategies for each one of them.The limitations include the following:</p>
<p>• Positional Bias: LLM judges favor certain positions over others.</p>
<p>• Verbosity Bias: LLM judges favor longer, verbose responses, even if they are not as clear, highquality, or accurate as shorter alternatives.• Self-Enhancement Bias: LLM judges may favor the answers generated by themselves.• Limited capability in grading math and reasoning questions: LLMs are known to have limited math and reasoning capability [116].</p>
<p>Related Analysis: In their study, [103] investigated the potential of zero-shot prompting with reference answers as an alternative to human evaluation for assessing ODQA.</p>
<p>Their findings suggested that this method holds promise, although it may not be suitable for detecting issues such as unattributability in long-form answers.Similarly, the authors of [104] evaluated the performance of LLM-based evaluations using the GPT-3.5 model for ODQA assessment.Their findings demonstrated that the LLM-based evaluation achieved reasonably good performance across a variety of datasets when compared to other methods.The analysis of LLM-as-a-Judge [115] revealed that strong LLM judges such as GPT-4 [117] can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans.This study laid the critical groundwork for the development of evaluation frameworks based on LLMs.JudgeLM [118] proposed fine-tuned LLMs as scalable judges to efficiently and effectively evaluate other LLMs in open-ended benchmarks.The JudgeLM models were trained at various scales ranging from 7B to 33B parameters.This fine-tuning process allows the judge models to acquire the ability to distinguish high-quality from low-quality responses within the context of a defined task set.A recent study proposed FACTSCORE [119] to address the challenge of identifying factual errors (hallucinations) generated by LLMs.This novel evaluation method assesses the factual accuracy of the generated answers by decomposing them into individual atomic facts and verifying them against a trustworthy knowledge source.Building on this foundation, SAFE [120] introduced a comprehensive evaluation approach by leveraging an LLM itself to rate the factuality of atomic facts against the Google search engine.While LLM-based judgment remains a nascent field, it presents intriguing possibilities for the future of ODQA evaluation.As LLMs continue their development, they have the potential to emerge as valuable tools for generating comprehensive and multifaceted assessments of ODQA models.However, further research is needed to solidify best practices and understanding in this area.</p>
<p>Our comprehensive analysis demonstrates that each evaluation technique possesses inherent strengths and weaknesses.Consequently, reliance on a single metric for performance assessment is inadequate.A multifaceted assessment approach that incorporates multiple metrics is essential for comprehensively evaluating the efficacy of a QA system.</p>
<p>IV. CONCLUSION</p>
<p>Our thorough review of 52 datasets and 20 evaluation metrics encompassing a broad spectrum of Open Domain Question Answering tasks reveals several key findings and illuminates promising directions for future research endeavors.</p>
<p>A. LIMITATIONS OF AUTOMATIC EVALUATION METRICS</p>
<p>The recent success of LLMs is expected to drive advancements in Generative QA systems.However, a key challenge lies in developing automatic evaluation metrics that reflect human judgment of LLM-powered responses.While traditional ODQA relied on lexical metrics, the shift towards semantic and LLM-based evaluations highlights the need for more effective automatic metrics.We anticipate continued research focus in this area.Additionally, LLMs exhibit a concerning tendency to produce hallucinatory outputs: answers that seem plausible yet lack a factual basis.Developing robust and automated methods for detecting hallucinations within LLM responses is an essential avenue for further research.</p>
<p>B. LACK OF BENCHMARKS IN PUBLIC-PRIVATE DATA SETTINGS</p>
<p>Our study identified a dearth of benchmarks that comprehensively address the challenges of ODQA in hybrid environments, where both public and private data sources are utilized.This paucity presents a significant hurdle for researchers to develop privacy-preserving conversational ODQA agents.As these agents become increasingly prevalent, we expect a future upswing in research towards the creation of novel datasets and evaluation methods that can assess performance within this complex landscape.</p>
<p>C. SCARCITY OF COMPLEX-QUESTION DATASETS IN TEXTUAL ODQA</p>
<p>While a wealth of robust datasets exist for factoid and shortform textual question answering, our review revealed a critical shortcoming: the lack of datasets that address more intricate and real-world scenarios.These scenarios represent several challenging question types, including long-form, ambiguous, time-sensitive questions, and questions with multiple valid answers.</p>
<p>D. DATA LIMITATIONS FOR NON-TEXTUAL MODALITIES</p>
<p>The incorporation of non-textual modalities, particularly videos, into ODQA is an emerging area of research and application.While current datasets for video-based question answering are limited in scale and scope, focusing on closed domain tasks, we anticipate an increase in research and the development of datasets that integrate video in a multimodal open domain setting.Our investigation of image and table modalities highlighted a limitation in the availability of datasets that cater to diverse question formats and difficulty levels.Furthermore, our analysis revealed a lack of datasets that provide support for languages beyond English for non-textual modalities.We anticipate that as this field progresses, there will be a shift towards the development of more comprehensive datasets that bridge these current gaps.</p>
<p>E. FUTURE DIRECTIONS</p>
<p>Based on the trends observed in this field, we posit that research on multimodal datasets and advanced evaluation metrics will continue to be a focal point.These advancements will be instrumental in paving the way for artificial general intelligence (AGI) by enabling the creation of systems that can effectively process and answer complex questions across diverse modalities, akin to human intelligence.</p>
<p>In this study, we presented a comprehensive review of datasets and evaluation metrics encompassing various modalities for ODQA.We hope that this analysis will serve as a valuable resource for researchers, aiding in the development of novel QA systems and promoting the advancement of robust evaluation methodologies.</p>
<p>FIGURE 1 .
1
FIGURE 1. Role of Datasets and Evaluation Metrics in Benchmarking.</p>
<p>FIGURE 2 .
2
FIGURE 2. Modality and Difficulty-Based Taxonomy of Modern Open Domain Question Answering Datasets.</p>
<p>FIGURE 3 (
3
FIGURE 3(a) and 3(b).Example of images as inputs for Multimodal ODQA.</p>
<p>FIGURE 4 .
4
FIGURE 4. Taxonomy of Modern ODQA Evaluation Metrics.</p>
<p>FIGURE 5 .
5
FIGURE 5. Example of a five-point Likert Scale.</p>
<p>FIGURE 6 .
6
FIGURE 6. Example of a Prompt sent to the LLM for evaluation.</p>
<p>FIGURE 7 .
7
FIGURE 7. Score and Justification returned by the LLM.</p>
<p>TABLE I :
I
SHORT-FORM DATASETS
2) LONG-FORMLong-form questions are open-ended questions that cannotbe easily answered with a short response or simply byextracting a phrase from a knowledge source. The answersusually consist of multiple sentences or a paragraph. Forinstance, examples of long-form questions include "How dojellyfish function without brains or nervous systems?" and"How does my car engine work?".a) ELI5ELI5 [44] is a long-form question answering dataset. Thedataset consists of 272,000 open-ended and diversequestions sourced from the Reddit forum "Explain Like I'mFive" (ELI5). Common Crawl's July 2018 web index wasused as the external knowledge source. ROUGE-1, ROUGE-2, and ROUGE-L scores were used to evaluate the system'sperformance.The long-form datasets are summarized in Table II. It isnoteworthy that only one long-form dataset is available forODQA. Due to the inherent natural language and abstractivenature of long-form answers, various ROUGE-based metricsare used for the evaluation.DatasetYearMetrics#QuestionsKnowledge1. WebQuestions2013EM, F16,642Wikipedia2. CuratedTREC2015EM, F12,180Wikipedia3. WikiQA2015F13,047Wikipedia4. SQuAD 1.1, 2.02016,EM, F1161,650Wikipedia20185. MS Marco2016,ROUGE-L,1,010,916Bing Search2018BLEU-16. Quasar-T2017EM, F143,000WebCorpus7. SearchQA2017Accuracy,140,461GoogleF1Search8. TriviaQA2017EM, F195,956BingSearch,Wikipedia9. BoolQ2019Accuracy15,942Wikipedia10. Natural Questions2019EM, F1323,045Wikipedia</p>
<p>TABLE II :
II
LONG-FORM DATASETS
DatasetYearMetrics#QuestionsKnowledge1. ELI52019ROUGE-1, ROUGE-2,272,000Web CorpusROUGE-L3) AMBIGUOUSAmbiguous questions are questions that are characterized bymultiple possible interpretations or meanings. They hinderthe ability of QA systems to arrive at a single, definitiveanswer because of the lack of sufficient context. Illustrativeexamples are included in each dataset discussion to aidcomprehension.</p>
<p>TABLE III :
III
AMBIGUOUS DATASETS
DatasetYearMetrics#QuestionsKnowledge1. AMBIGNQ2020F1, BLEU14,042Wikipedia2. ASQA2022EM, F1, ROUGE-L6,316Wikipedia</p>
<p>TABLE IV :
IV
MULTI-HOP DATASETS
DatasetYearMetrics#QuestionsKnowledge1. HotPotQA2018EM, F1112,779Wikipedia2. DROP2019EM, F196,567Wikipedia3. 2WikiMultiHopQA2020EM, F1192,606Wikidata,Wikipedia4. MultiSpanQA2022EM, F119,608Wikipedia5. QAMPARI2023F163,911Wikipedia6. ConcurrentQA2023EM, F118,439Emails,Wikipedia</p>
<p>TABLE V :
V
CONVERSATIONAL DATASETS
DatasetYearMetrics#QuestionsKnowledge1. QuAC2018HEQ, F198,407Wikipedia2. QBLink2018EM56,000Wikipedia3. CoQA2019F1127,000Multiple Sources4. OR-QuAC2020HEQ, F140,527Wikipedia5. QReCC2020EM, F181,000Web Corpus6. TopiOCQA2021EM, F150,574Wikipedia7. Topical-Chat2023F110,784Wikipedia,Reddit,Washington Post</p>
<p>TABLE VI :
VI
CROSS-LINGUAL DATASETS
DatasetYearMetrics#QuestionsKnowledge1. XQA2019EM, F190,610Wikipedia2. MLQA2020EM, F146,444Wikipedia3. XQuAD2020EM, F11,190Wikipedia4. TyDiQA2020F1204,337Wikipedia5. XTREME2020EM, F1142,154Wikipedia6. XOR-TyDiQA2020EM, F1, BLEU40,000Wikipedia7. MKQA2021EM, F1260,000Wikipedia8. GEN-TyDiQA2022Accuracy,4,038WikipediaBLEU,ROUGE-L</p>
<p>TABLE VII :
VII
TIME-SENSITIVE DATASETS
DatasetYearMetrics#QuestionsKnowledge1. SituatedQA2021EM, F111,000Wikipedia2. TimeQA2021EM, F141,172Wikipedia3. FreshQA2023Human Rating600Pre-trainedLLMs</p>
<p>TABLE VIII :
VIII
PARAPHRASED DATASETS
DatasetYearMetrics#QuestionsKnowledge1. ComQA2019F111,214Wikipedia2. Paraphrased-SQuAD2019EM, F11,118Wikipedia9) COUNTERFACTUAL
Counterfactual questions are questions with false presuppositions or false misconceptions.Presuppositions are background beliefs or assumptions that a speaker treats as shared world knowledge.Examples of such questions include "If Los Angeles was on the east coast of the U.S., what would be the time difference between Los Angeles and Paris?" and "Can coughing effectively stop a heart attack?".a) CREPE</p>
<p>. Lexical metrics, such as the F1 score, are commonly used for evaluating factual consistency.CREPE additionally employs the BLEU score for abstractive evaluation, whereas TruthfulQA explores LLM-based evaluation methods.Question sources vary: CREPE leverages online sources such as Reddit, IfQA utilizes Wikipedia passages, and TruthfulQA relies on handwritten questions.CREPE boasts the most extensive question collection.For knowledge sources, Wikipedia is common, with TruthfulQA employing pre-trained LLMs.</p>
<p>TABLE IX :
IX
COUNTERFACTUAL DATASETS
DatasetYearMetrics#QuestionsKnowledge1. CREPE2022F1, BLEU8,400Wikipedia2. TruthfulQA2022Human Rating,817Pre-trainedLLM-basedLLMs3. IfQA2023EM, F13,800Wikipedia</p>
<p>TABLE X :
X
TEXT AND IMAGES DATASETS
DatasetYearMetrics#QuestionsKnowledge1. OK-VQA2019VQA Accuracy14,055Wikipedia2. S3VQA2021VQA Accuracy6,765Wikipedia3. MIMOQA2021ROUGE, BLEU,56,693WikipediaPrecision@1, 2, 34. A-OKVQA2022VQA Accuracy24,903Wikipedia5. WebQA2022F1, BARTScore46,700Wikipedia,Bing Search</p>
<p>TABLE XI :
XI
TEXT AND TABLES DATASETS
DatasetYearMetrics#QuestionsKnowledge1. HybridQA2020EM, F170,000Wikipedia2. OTTQA2021EM, F145,814Wikipedia</p>
<p>TABLE XII :
XII
TEXT, IMAGES AND TABLES DATASETS
DatasetYearMetrics#QuestionsKnowledge1. ManyModalQA2020Accuracy10,190Wikipedia2. MultiModalQA2021EM, F129,918Wikipedia3. MMConvQA2022EM, F15,753Wikipedia
ACKNOWLEDGMENTWe are grateful to Ceren Gerzic, Rajani Samidi, and Stephen Pulman for their invaluable feedback on this paper.We would also like to thank Jessica Wang, Kevin Hsu, and Mohamed Soliman for their support.DatasetLink 1. WebQuestions https://github.com/brmson/dataset-factoid-webquestions2. CuratedTREC https://github.com/brmson/dataset-factoid-curated3. WikiQA https://www.microsoft.com/en-us/download/details.aspx?id=52419 4. SQuAD https://rajpurkar.github.io/SQuAD-explorer 5. MS Marco http://www.msmarco.org6. Quasar-T https://github.com/bdhingra/quasar7. SearchQA https://github.com/nyu-dl/dl4ir-searchQA8. TriviaQA https://nlp.cs.washington.edu/triviaqa/9. BoolQ https://github.com/google-research-datasets/boolean-questions10.Natural Questions https://github.com/google-research-datasets/natural-questions11.ELI5 https://facebookresearch.github.io/ELI5/12. AMBIGNQ https://nlp.cs.washington.edu/ambigqa/13.ASQA https://github.com/google-research/language/tree/master/language/asqa14.HotPotQA https://hotpotqa.github.io/15.DROP https://allenai.org/data/drop16. 2WikiMultiHopQA https://github.com/Alab-NII/2wikimultihop17.MultiSpanQA https://multi-span.github.io/18. QAMPARI https://samsam3232.github.io/qampari/19.ConcurrentQA https://github.com/facebookresearch/concurrentqa20.QuAC https://quac.ai/21.QBLink https://sites.google.com/view/qanta/projects/qblink22. CoQA https://stanfordnlp.github.io/coqa/23.OR-QuAC https://github.com/prdwb/orconvqa-release24.QReCC https://github.com/apple/ml-qrecc25.TopiOCQA https://mcgill-nlp.github.io/topiocqa/26.Topical-Chat https://github.com/alexa/Topical-Chat27.XQA https://github.com/thunlp/XQA28.MLQA https://github.com/facebookresearch/MLQA29.XQuAD https://github.com/google-deepmind/xquad30.TyDiQA https://ai.google.com/research/tydiqa31.XTREME https://sites.research.google/xtreme32.XOR-TyDiQA https://nlp.cs.washington.edu/xorqa/33.MKQA https://github.com/apple/ml-mkqa34.SituatedQA https://situatedqa.github.io/index.html35.TimeQA https://github.com/wenhuchen/Time-Sensitive-QA36.FreshQA https://github.com/freshllms/freshqa37. ComQA https://qa.mpi-inf.mpg.de/comqa/38.Paraphrased-SQuAD https://github.com/nusnlp/paraphrasing-squad39.CREPE https://github.com/velocityCavalry/CREPE40.TruthfulQA https://github.com/sylinrl/TruthfulQA41.IfQA https://github.com/wyu97/IfQA42.OK-VQA https://okvqa.allenai.org/43.S3VQA https://s3vqa.github.io/44.A-OKVQA https://allenai.org/project/a-okvqa/home45.WebQA https://github.com/WebQnA/WebQA46.HybridQA https://github.com/wenhuchen/HybridQA47.OTTQA https://github.com/wenhuchen/OTT-QA48.ManyModalQA https://github.com/hannandarryl/ManyModalQA49.MultiModalQA https://github.com/allenai/multimodalqa50.MMConvQA https://github.com/liyongqi67/MMCoQA
Baseball: an automatic question-answerer. B F Green, A K Wolf, C Chomsky, K Laughery, 10.1145/1460690.1460714western joint IRE-AIEE-ACM computer conference. Western). New York, NY, USAAssociation for Computing MachineryMay 9-11, 1961. May 1961in Papers</p>
<p>Deep Read: A Reading Comprehension System. L Hirschman, M Light, E Breck, J D Burger, 10.3115/1034678.1034731Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics. the 37th Annual Meeting of the Association for Computational LinguisticsCollege Park, Maryland, USAAssociation for Computational LinguisticsJun. 1999</p>
<p>Reading Wikipedia to Answer Open-Domain Questions. D Chen, A Fisch, J Weston, A Bordes, 10.18653/v1/P17-1171Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. R Barzilay, M.-Y Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJul. 20171</p>
<p>Generation-Augmented Retrieval for Open-Domain Question Answering. Y Mao, doi: 10.18653Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. C Zong, F Xia, W Li, R Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAug. 20211</p>
<p>Dense Passage Retrieval for Open-Domain Question Answering. V Karpukhin, 10.18653/v1/2020.emnlp-main.550Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. B Webber, T Cohn, Y He, Y Liu, the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNov. 2020</p>
<p>BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJun. 20191</p>
<p>RoBERTa: A Robustly Optimized BERT Pretraining Approach. Y Liu, 10.48550/arXiv.1907.11692arXiv. Jul. 26, 2019</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. C Raffel, 10.48550/arXiv.1910.10683Sep. 19, 2023</p>
<p>BART: Denoising Sequence-to-Sequence Pretraining for Natural Language Generation, Translation, and Comprehension. M Lewis, doi: 10.18653Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJul. 2020</p>
<p>Language Models are Few-Shot Learners. T B Brown, 10.48550/arXiv.2005.14165Jul. 22, 2020</p>
<p>Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. G Izacard, E Grave, doi: 10.18653Proceedings of the 16th Conference of the European Chapter. P Main Volume, J Merlo, R Tiedemann, Tsarfaty, the 16th Conference of the European ChapterAssociation for Computational LinguisticsApr. 2021</p>
<p>Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index. M Seo, J Lee, T Kwiatkowski, A Parikh, A Farhadi, H Hajishirzi, 10.18653/v1/P19-1436Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. A Korhonen, D Traum, L Màrquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJul. 2019</p>
<p>Contextualized Sparse Representations for Real-Time Open-Domain Question Answering. J Lee, M Seo, H Hajishirzi, J Kang, doi: 10.18653Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJul. 2020</p>
<p>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. Y Bang, doi: 10.18653Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. Long Papers. J C Park, Y Arase, B Hu, W Lu, D Wijaya, A Purwarianti, A A Krisnadhi, the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterBaliAssociation for Computational LinguisticsNov. 20231Nusa Dua</p>
<p>Survey of Hallucination in Natural Language Generation. Z Ji, 10.1145/3571730ACM Comput. Surv. 5512Dec. 2023</p>
<p>A review of public datasets in question answering research. B B Cambazoglu, M Sanderson, F Scholer, B Croft, 10.1145/3483382.3483389ACM SIGIR Forum. 542Aug. 2021</p>
<p>Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering. F Zhu, W Lei, C Wang, J Zheng, S Poria, T.-S Chua, 10.48550/arXiv.2101.00774May 08, 2021</p>
<p>English Machine Reading Comprehension Datasets: A Survey. D Dzendzik, J Foster, C Vogel, doi: 10.18653Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. X Moens, L Huang, S W Specia, Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNov. 2021Online and Punta Cana</p>
<p>More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering. Y Bai, D Z Wang, 10.48550/arXiv.2109.12264Feb. 04, 2022arXiv</p>
<p>Modern Question Answering Datasets and Benchmarks: A Survey. Z Wang, 10.48550/arXiv.2206.15030Jun. 30, 2022</p>
<p>QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension. A Rogers, M Gardner, I Augenstein, 10.1145/3560260ACM Comput. Surv. 5510Oct. 2023</p>
<p>M Zaib, W E Zhang, Q Z Sheng, A Mahmood, Y Zhang, 10.48550/arXiv.2106.00874Conversational Question Answering: A Survey. Jun. 02, 2021</p>
<p>Q Wu, D Teney, P Wang, C Shen, A Dick, A Van Den, Hengel, 10.48550/arXiv.1607.05910Visual Question Answering: A Survey of Methods and Datasets. Jul. 20, 2016arXiv</p>
<p>Semantic Parsing on Freebase from Question-Answer Pairs. J Berant, A Chou, R Frostig, P Liang, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. D Yarowsky, T Baldwin, A Korhonen, K Livescu, S Bethard, the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational LinguisticsOct. 2013. May 24. 2024</p>
<p>Modeling of the Question Answering Task in the YodaQA System. P Baudiš, J Šedivý, 10.1007/978-3-319-24027-5_20Sep. 2015</p>
<p>Building a question answering test collection. E M Voorhees, D M Tice, 10.1145/345508.345577Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval. the 23rd annual international ACM SIGIR conference on Research and development in information retrievalNew York, NY, USAAssociation for Computing MachineryJul. 2000SIGIR '00</p>
<p>WikiQA: A Challenge Dataset for Open-Domain Question Answering. Y Yang, W Yih, C Meek, 10.18653/v1/D15-1237Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. L Màrquez, C Callison-Burch, J Su, the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsSep. 2015</p>
<p>SQuAD: 100,000+ Questions for Machine Comprehension of Text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. J Su, K Duh, X Carreras, the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsNov. 2016</p>
<p>Know What You Don't Know: Unanswerable Questions for SQuAD. P Rajpurkar, R Jia, P Liang, 10.18653/v1/P18-2124Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Short Papers. I Gurevych, Y Miyao, the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational LinguisticsJul. 20182</p>
<p>MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. P Bajaj, 10.48550/arXiv.1611.09268Oct. 31. 2018</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. C.-Y Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational LinguisticsJul. 2004. May 26. 2024</p>
<p>Bleu: a Method for Automatic Evaluation of Machine Translation. K Papineni, S Roukos, T Ward, W.-J Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. P Isabelle, E Charniak, D Lin, the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsJul. 2002</p>
<p>Quasar: Datasets for Question Answering by Search and Reading. B Dhingra, K Mazaitis, W W Cohen, 10.48550/arXiv.1707.03904Aug. 08, 2017</p>
<p>Clueweb09 data set | BibSonomy. May 26, 2024</p>
<p>SearchQA: A New Q&amp;A Dataset Augmented with Context from a Search Engine. M Dunn, L Sagun, M Higgins, V U Guney, V Cirik, K Cho, 10.48550/arXiv.1704.05179Jun. 11, 2017arXiv</p>
<p>TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. M Joshi, E Choi, D Weld, L Zettlemoyer, 10.18653/v1/P17-1147Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. R Barzilay, M.-Y Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJul. 20171</p>
<p>BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. C Clark, K Lee, M.-W Chang, T Kwiatkowski, M Collins, K Toutanova, 10.18653/v1/N19-1300Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJun. 20191</p>
<p>Natural Questions: A Benchmark for Question Answering Research. T Kwiatkowski, 10.1162/tacl_a_00276Trans. Assoc. Comput. Linguist. 72019</p>
<p>Latent Retrieval for Weakly Supervised Open Domain Question Answering. K Lee, M.-W Chang, K Toutanova, 10.18653/v1/P19-1612Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. A Korhonen, D Traum, L Màrquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJul. 2019</p>
<p>R$^3$: Reinforced Reader-Ranker for Open-Domain Question Answering. S Wang, 10.48550/arXiv.1709.00023Nov. 21, 2017</p>
<p>Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering. Z Wang, P Ng, X Ma, R Nallapati, B Xiang, 10.18653/v1/D19-1599Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. K Inui, J Jiang, V Ng, X Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational LinguisticsNov. 2019</p>
<p>REALM: Retrieval-Augmented Language Model Pre-Training. K Guu, K Lee, Z Tung, P Pasupat, M.-W Chang, 10.48550/arXiv.2002.08909arXiv. Feb. 10, 2020</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. P Lewis, 10.48550/arXiv.2005.11401Apr. 12, 2021</p>
<p>ELI5: Long Form Question Answering. A Fan, Y Jernite, E Perez, D Grangier, J Weston, M Auli, 10.18653/v1/P19-1346Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. A Korhonen, D Traum, L Màrquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJul. 2019</p>
<p>AmbigQA: Answering Ambiguous Open-domain Questions. S Min, J Michael, H Hajishirzi, L Zettlemoyer, doi: 10.18653Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. B Webber, T Cohn, Y He, Y Liu, the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNov. 2020</p>
<p>ASQA: Factoid Questions Meet Long-Form Answers. I Stelmakh, Y Luan, B Dhingra, M.-W Chang, doi: 10.18653Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Y Goldberg, Z Kozareva, Y Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>HotpotQA: A Dataset for Diverse, Explainable Multihop Question Answering. Z Yang, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. D Riloff, J Chiang, J Hockenmaier, Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOct. 2018</p>
<p>DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs. D Dua, Y Wang, P Dasigi, G Stanovsky, S Singh, M Gardner, 10.18653/v1/N19-1246Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJun. 20191</p>
<p>Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. X Ho, A.-K Duong Nguyen, S Sugawara, A Aizawa, doi: 10.18653Proceedings of the 28th International Conference on Computational Linguistics. D Scott, N Bel, C Zong, the 28th International Conference on Computational LinguisticsBarcelona, Spain (OnlineInternational Committee on Computational LinguisticsDec. 2020</p>
<p>MultiSpanQA: A Dataset for Multi-Span Question Answering. H Li, M Tomko, M Vasardani, T Baldwin, 10.18653/v1/2022.naacl-main.90Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. M Carpuat, M.-C De Marneffe, I V Meza Ruiz, the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational LinguisticsJul. 2022</p>
<p>QAMPARI: A Benchmark for Open-domain Questions with Many Answers. S Amouyal, T Wolfson, O Rubin, O Yoran, J Herzig, J Berant, ( Metrics, S Gem), A Gehrmann, J Wang, E Sedoc, K Clark, K R Dhole, Chandu, Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and. E Santus, H Sedghamiz, the Third Workshop on Natural Language Generation, Evaluation, andSingaporeAssociation for Computational LinguisticsDec. 2023. May 26. 2024</p>
<p>Reasoning over Public and Private Data in Retrieval-Based Systems. S Arora, P Lewis, A Fan, J Kahn, C Ré, 10.1162/tacl_a_00580Trans. Assoc. Comput. Linguist. 112023</p>
<p>QuAC: Question Answering in Context. E Choi, 10.18653/v1/D18-1241Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. E Riloff, D Chiang, J Hockenmaier, J Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOct. 2018</p>
<p>A dataset and baselines for sequential open-domain question answering. A Elgohary, C Zhao, J Boyd-Graber, 10.18653/v1/D18-1134Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. E Riloff, D Chiang, J Hockenmaier, J Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOct. 2018</p>
<p>CoQA: A Conversational Question Answering Challenge. S Reddy, D Chen, C D Manning, 10.1162/tacl_a_00266Trans. Assoc. Comput. Linguist. 72019</p>
<p>M Guo, M Zhang, S Reddy, M Alikhani, Abg-CoQA: Clarifying Ambiguity in Conversational Question Answering. </p>
<p>Open-Retrieval Conversational Question Answering. C Qu, L Yang, C Chen, M Qiu, W B Croft, M Iyyer, 10.1145/3397271.3401110Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SIGIR Conference on Research and Development in Information RetrievalJul. 2020</p>
<p>Can You Unpack That? Learning to Rewrite Questions-in-Context. A Elgohary, D Peskov, J Boyd-Graber, ; Emnlp-Ijcnlp), K Inui, J Jiang, 10.18653/v1/D19-1605Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. X Ng, Eds Wan, Hong Kong, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingChinaAssociation for Computational LinguisticsNov. 2019</p>
<p>Open-Domain Question Answering Goes Conversational via Question Rewriting. R Anantha, S Vakulenko, Z Tu, S Longpre, S Pulman, S Chappidi, doi: 10.18653Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. A Toutanova, L Rumshisky, D Zettlemoyer, I Hakkani-Tur, S Beltagy, R Bethard, T Cotterell, Y Chakraborty, Zhou, the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJun. 2021</p>
<p>TopiOCQA: Open-domain Conversational Question Answering with Topic Switching. V Adlakha, S Dhuliawala, K Suleman, H Vries, S Reddy, 10.1162/tacl_a_00471Trans. Assoc. Comput. Linguist. 102022</p>
<p>Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations. K Gopalakrishnan, 10.48550/arXiv.2308.11995Aug. 23, 2023</p>
<p>XQA: A Cross-lingual Opendomain Question Answering Dataset. J Liu, Y Lin, Z Liu, M Sun, 10.18653/v1/P19-1227Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. A Korhonen, D Traum, L Màrquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJul. 2019</p>
<p>MLQA: Evaluating Cross-lingual Extractive Question Answering. P Lewis, B Oguz, R Rinott, S Riedel, H Schwenk, 10.18653/v1/2020.acl-main.653Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJul. 2020</p>
<p>On the Cross-lingual Transferability of Monolingual Representations. M Artetxe, S Ruder, D Yogatama, doi: 10.18653Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJul. 2020</p>
<p>TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages. J H Clark, 10.1162/tacl_a_00317Trans. Assoc. Comput. Linguist. 82020</p>
<p>XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization. J Hu, S Ruder, A Siddhant, G Neubig, O Firat, M Johnson, 10.48550/arXiv.2003.11080Sep. 04, 2020arXiv</p>
<p>XOR QA: Cross-lingual Open-Retrieval Question Answering. A Asai, J Kasai, J Clark, K Lee, E Choi, H Hajishirzi, doi: 10.18653Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. A Toutanova, L Rumshisky, D Zettlemoyer, I Hakkani-Tur, S Beltagy, R Bethard, T Cotterell, Y Chakraborty, Zhou, the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJun. 2021</p>
<p>MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering. S Longpre, Y Lu, J Daiber, 10.1162/tacl_a_00433Trans. Assoc. Comput. Linguist. 92021</p>
<p>Cross-Lingual Open-Domain Question Answering with Answer Sentence Generation. B Muller, L Soldaini, R Koncel-Kedziorski, E Lind, A Moschitti, Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Long Papers. Y He, H Ji, S Li, Y Liu, C.-H Chang, the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsNov. 2022. May 26. 20241</p>
<p>SituatedQA: Incorporating Extra-Linguistic Contexts into QA. M Zhang, E Choi, doi: 10.18653Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. M.-F Moens, X Huang, L Specia, S W Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNov. 2021Online and Punta Cana</p>
<p>A Dataset for Answering Time-Sensitive Questions. W Chen, X Wang, W Y Wang, 10.48550/arXiv.2108.06314Oct. 24, 2021</p>
<p>FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation. T Vu, 10.48550/arXiv.2310.03214Nov. 22, 2023</p>
<p>ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters. A Abujabal, R Saha Roy, M Yahya, G Weikum, 10.18653/v1/N19-1027Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJun. 20191</p>
<p>Improving the Robustness of Question Answering Systems to Question Paraphrasing. W C Gan, H T Ng, 10.18653/v1/P19-1610Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. A Korhonen, D Traum, L Màrquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJul. 2019</p>
<p>PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification. E Pavlick, P Rastogi, J Ganitkevitch, B Van Durme, C Callison-Burch, 10.3115/v1/P15-2070Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Short Papers. C Zong, M Strube, the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational LinguisticsJul. 20152</p>
<p>CREPE: Open-Domain Question Answering with False Presuppositions. X Yu, S Min, L Zettlemoyer, H Hajishirzi, 10.18653/v1/2023.acl-long.583Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJul. 20231</p>
<p>TruthfulQA: Measuring How Models Mimic Human Falsehoods. S Lin, J Hilton, O Evans, 10.18653/v1/2022.acl-long.229Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers, ) , S Muresan, P Nakov, A Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions. W Yu, M Jiang, P Clark, A Sabharwal, doi: 10.18653Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge. K Marino, M Rastegari, A Farhadi, R Mottaghi, 10.48550/arXiv.1906.00067Sep. 04, 2019arXiv</p>
<p>T.-Y Lin, 10.48550/arXiv.1405.0312Microsoft COCO: Common Objects in Context. Feb. 20, 2015</p>
<p>VQA: Visual Question Answering. A , arXiv. Oct. 26, 2016. May 26, 2024</p>
<p>Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering. A Jain, M Kothyari, V Kumar, P Jyothi, G Ramakrishnan, S Chakrabarti, 10.1145/3404835.3463259Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalJul. 2021</p>
<p>The Open Images Dataset V4. A Kuznetsova, 10.1007/s11263-020-01316-zInt. J. Comput. Vis. 1287Jul. 2020</p>
<p>MIMOQA: Multimodal Input Multimodal Output Question Answering. H Singh, A Nasery, D Mehta, A Agarwal, J Lamba, B V Srinivasan, doi: 10.18653Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. A Toutanova, L Rumshisky, D Zettlemoyer, I Hakkani-Tur, S Beltagy, R Bethard, T Cotterell, Y Chakraborty, Zhou, the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJun. 2021</p>
<p>A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge. D Schwenk, A Khandelwal, C Clark, K Marino, R Mottaghi, 10.48550/arXiv.2206.01718Jun. 03, 2022arXiv</p>
<p>WebQA: Multihop and Multimodal QA. Y Chang, M Narang, H Suzuki, G Cao, J Gao, Y Bisk, 10.48550/arXiv.2109.00590Mar. 27, 2022arXiv</p>
<p>BARTScore: Evaluating Generated Text as Text Generation. W Yuan, G Neubig, P Liu, 10.48550/arXiv.2106.11520Oct. 27, 2021</p>
<p>HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data. W Chen, H Zha, Z Chen, W Xiong, H Wang, W Y Wang, 10.18653/v1/2020.findings-emnlp.91Findings of the Association for Computational Linguistics: EMNLP 2020. Y Cohn, Y He, Liu, Association for Computational LinguisticsNov. 2020</p>
<p>Open Question Answering over Tables and Text. W Chen, M.-W Chang, E Schlinger, W Wang, W W Cohen, arXivFeb. 10, 2021. May 26. 2024</p>
<p>ManyModalQA: Modality Disambiguation and QA over Diverse Inputs. D Hannan, A Jain, M Bansal, 10.48550/arXiv.2001.08034Jan. 22, 2020</p>
<p>MultiModalQA: Complex Question Answering over Text, Tables and Images. A Talmor, 10.48550/arXiv.2104.06039arXiv. Apr. 13, 2021</p>
<p>MMCoQA: Conversational Question Answering over Text, Tables, and Images. Y Li, W Li, L Nie, doi: 10.18653Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers, ) , S Muresan, P Nakov, A Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>How Much Knowledge Can You Pack Into the Parameters of a Language Model?. A Roberts, C Raffel, N Shazeer, doi: 10.18653Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. B Webber, T Cohn, Y He, Y Liu, the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNov. 2020</p>
<p>EfficientQA Competition: Systems, Analyses and Lessons Learned. S Min, Proceedings of the NeurIPS 2020 Competition and Demonstration Track, PMLR. the NeurIPS 2020 Competition and Demonstration Track, PMLR2020. Aug. 2021. May 26. 2024</p>
<p>Interrater reliability: the kappa statistic. M L Mchugh, Biochem. Medica. 223Oct. 2012</p>
<p>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. S Banerjee, A Lavie, Proceedings of the ACL Workshop on Intrinsic and. J Goldstein, A Lavie, C.-Y Lin, C Voss, the ACL Workshop on Intrinsic andAnn Arbor, MichiganAssociation for Computational LinguisticsJun. 2005. May 26. 2024Extrinsic Evaluation Measures for Machine Translation and/or Summarization</p>
<p>Extending the METEOR Machine Translation Evaluation Metric to the Phrase Level. M Denkowski, A Lavie, Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. R Kaplan, J Burstein, M Harper, G Penn, Los Angeles, CaliforniaAssociation for Computational LinguisticsJun. 2010. May 26. 2024</p>
<p>Towards a Better Metric for Evaluating Question Generation Systems. P Nema, M M Khapra, 10.18653/v1/D18-1429Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. D Riloff, J Chiang, J Hockenmaier, Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOct. 2018</p>
<p>Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading Comprehension Task. A Yang, K Liu, J Liu, Y Lyu, S Li, E Choi, M Seo, D Chen, R , 10.18653/v1/W18-2611Proceedings of the Workshop on Machine Reading for Question Answering. J Jia, Berant, the Workshop on Machine Reading for Question AnsweringMelbourne, AustraliaAssociation for Computational LinguisticsJul. 2018</p>
<p>Evaluating Question Answering Evaluation. A Chen, G Stanovsky, S Singh, M Gardner, A Fisch, A Talmor, R Jia, M Seo, E Choi, D Chen, Eds , Hong Kong, 10.18653/v1/D19-5817Proceedings of the 2nd Workshop on Machine Reading for Question Answering. the 2nd Workshop on Machine Reading for Question AnsweringChinaAssociation for Computational LinguisticsNov. 2019</p>
<p>BERTScore: Evaluating Text Generation with BERT. T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, 10.48550/arXiv.1904.09675Feb. 24, 2020arXiv</p>
<p>BLEURT: Learning Robust Metrics for Text Generation. T Sellam, D Das, A Parikh, 10.18653/v1/2020.acl-main.704Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJul. 2020</p>
<p>Evaluating Open-Domain Question Answering in the Era of Large Language Models. E Kamalloo, N Dziri, C Clarke, D Rafiei, doi: 10.18653Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJul. 20231</p>
<p>Evaluating Open-QA Evaluation. C Wang, 10.48550/arXiv.2305.12421Oct. 23, 2023</p>
<p>From Word Embeddings To Document Distances. M Kusner, Y Sun, N Kolkin, K Weinberger, Proceedings of the 32nd International Conference on Machine Learning, PMLR. the 32nd International Conference on Machine Learning, PMLRJun. 2015. May 26. 2024</p>
<p>Sentence Mover's Similarity: Automatic Evaluation for Multi-Sentence Texts. E Clark, A Celikyilmaz, N A Smith, 10.18653/v1/P19-1264Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. A Korhonen, D Traum, L Màrquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJul. 2019</p>
<p>MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance. W Zhao, M Peyrard, F Liu, Y Gao, C M Meyer, S Eger, 10.18653/v1/D19-1053Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. K Inui, J Jiang, V Ng, X Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational LinguisticsNov. 2019</p>
<p>Semantic Answer Similarity for Evaluating Question Answering Models. J Risch, T Möller, J Gutsch, M Pietsch, ; A Fisch, A Talmor, D Chen, E Choi, M Seo, P Lewis, R , 10.18653/v1/2021.mrqa-1.15Proceedings of the 3rd Workshop on Machine Reading for Question Answering. S Jia, Min, the 3rd Workshop on Machine Reading for Question AnsweringPunta Cana, Dominican RepublicAssociation for Computational LinguisticsNov. 2021</p>
<p>On Faithfulness and Factuality in Abstractive Summarization. J Maynez, S Narayan, B Bohnet, R Mcdonald, doi: 10.18653Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJul. 2020</p>
<p>Q Dong, 10.48550/arXiv.2301.00234A Survey on In-context Learning. Jun. 01, 2023</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, Advances in Neural Information Processing Systems. Oct. 2022. May 26. 2024</p>
<p>Zero-data Learning of New Tasks. H Larochelle, D Erhan, Y Bengio, </p>
<p>GPTScore: Evaluate as You Desire. J Fu, S.-K Ng, Z Jiang, P Liu, 10.48550/arXiv.2302.04166arXiv. Feb. 13, 2023</p>
<p>G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment. Y Liu, D Iter, Y Xu, S Wang, R Xu, C Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. J Bouamor, K Pino, Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. L Zheng, 10.48550/arXiv.2306.05685Dec. 23, 2023</p>
<p>Training Verifiers to Solve Math Word Problems. K Cobbe, 10.48550/arXiv.2110.14168Nov. 17, 2021</p>
<p>Openai, 10.48550/arXiv.2303.08774GPT-4 Technical Report. Mar. 04, 2024</p>
<p>JudgeLM: Fine-tuned Large Language Models are Scalable Judges. L Zhu, X Wang, X Wang, 10.48550/arXiv.2310.17631arXiv. Oct. 26, 2023</p>
<p>FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. S Min, 10.18653/v1/2023.emnlp-main.741Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>Long-form factuality in large language models. J Wei, 10.48550/arXiv.2403.18802Apr. 03, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
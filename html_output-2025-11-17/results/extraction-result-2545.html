<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2545 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2545</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2545</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-5e8350467d7d19e1d5f991be23f7b4826a9303a8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5e8350467d7d19e1d5f991be23f7b4826a9303a8" target="_blank">ROMA: Multi-Agent Reinforcement Learning with Emergent Roles</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> Experiments show that the proposed role-oriented MARL framework (ROMA) can learn specialized, dynamic, and identifiable roles, which help the method push forward the state of the art on the StarCraft II micromanagement benchmark.</p>
                <p><strong>Paper Abstract:</strong> The role concept provides a useful tool to design and understand complex multi-agent systems, which allows agents with a similar role to share similar behaviors. However, existing role-based methods use prior domain knowledge and predefine role structures and behaviors. In contrast, multi-agent reinforcement learning (MARL) provides flexibility and adaptability, but less efficiency in complex tasks. In this paper, we synergize these two paradigms and propose a role-oriented MARL framework (ROMA). In this framework, roles are emergent, and agents with similar roles tend to share their learning and to be specialized on certain sub-tasks. To this end, we construct a stochastic role embedding space by introducing two novel regularizers and conditioning individual policies on roles. Experiments show that our method can learn specialized, dynamic, and identifiable roles, which help our method push forward the state of the art on the StarCraft II micromanagement benchmark. Demonstrative videos are available at this https URL.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2545.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2545.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Role-Oriented Multi-Agent Reinforcement Learning (ROMA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CTDE multi-agent RL framework that induces emergent, stochastic role embeddings conditioned on local observations and uses them to generate agent-specific policy parameters, together with two mutual-information-based regularizers that encourage roles to be identifiable (temporally stable) and specialized (clustered by sub-task).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ROMA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ROMA is a centralized-training / decentralized-execution (CTDE) MARL system. Each agent has a role encoder f(o_i; θ_ρ) that outputs parameters (μ_ρi, σ_ρi) of a multivariate Gaussian role distribution; a role ρ_i is sampled and passed through a role decoder (hyper-network) g(ρ_i; θ_h) to generate parameters θ_i of the agent's local utility (Q) network. Local utilities are combined by a state-conditioned mixing network (QMIX-style) during training to produce a global Q_tot used for TD learning. Two regularizers are added: L_I (a variational lower bound proxy to maximize I(ρ_i; τ_i | o_i)) to make roles identifiable/stable, and L_D (a surrogate encouraging role specialization via mutual-information terms plus a learned dissimilarity model d_φ between agent trajectories), which in practice is implemented via variational posterior q_ξ (a GRU trajectory encoder) and minimizing the losses alongside the TD loss.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (experiments from small teams up to >10 agents; maps include 2–30+ units)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Specialization is emergent: roles correspond to sub-tasks discovered by optimization (e.g., Medivac specializing as healer; some units specialize in frontline engagement, sacrificial maneuvers, or staying back and sniping). There are no hand-crafted role labels; agents specialize by clustering in latent-role space induced by L_I and L_D. Examples from experiments: 'Medivac' role (healing), 'frontline attacker' role, 'protector/backline' role, 'sacrificial engager' role.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>execution and learning phases of task-oriented behavior (policy learning, online execution). Not targeted at research phases like literature review or hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized training via a mixing network (QMIX) that combines local utilities into a global Q for TD updates; decentralized execution where each agent acts based on its local policy parameterized by its sampled role (no centralized controller at execution). Coordination emerges through shared training signal and role-conditioned policies (implicit coordination via shared gradients and latent-role clustering).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>No explicit inter-agent message passing during execution; communication is implicit via centralized training and shared replay buffer. Roles are conditioned only on local observations (no explicit message content format). During training agents' trajectories are fed into the trajectory encoder and dissimilarity model (internal neural inputs), but this is not an explicit runtime message-passing protocol between agents.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Global TD loss computed from mixing network provides centralized reward feedback; variational posterior q_ξ (trajectory encoder) and the dissimilarity model d_φ provide auxiliary gradient signals (regularizers L_I and L_D) that shape role distributions; replay buffer samples provide off-policy feedback. No peer-to-peer iterative feedback during execution.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>No runtime inter-agent messaging; role sampling and policy conditioning occur each timestep (roles are stochastic and drawn from the role distribution at each timestep), but agents do not send structured messages to each other.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>StarCraft II micromanagement (SMAC) benchmark — multi-agent game/micromanagement tasks (not scientific research).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primary reported metric: win rate on SMAC maps (averaged over 5 random seeds with 95% CI). Quantitative dissimilarity metrics reported for role clustering: learned dissimilarity d_φ between different unit types = 0.9556 ± 0.0009; between same unit type = 0.0780 ± 0.0019. Exact win-rate numbers per map are given in plots (qualitatively ROMA yields substantially higher win rates across easy/hard/super-hard maps).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ROMA is compared against QMIX, QMIX-NPS (no parameter sharing), QMIX-LAR (larger-parameter QMIX), MAVEN, COMA, MADDPG and others. ROMA outperforms these baselines on both homogeneous and heterogeneous SMAC maps, with larger gains on maps with >10 agents. MAVEN performs worse than QMIX on most maps in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Emergent role-based coordination provides: improved learning efficiency (faster convergence), higher win rates across SMAC maps (state-of-the-art reported in this paper), better scalability as agent count increases (>10 agents). Qualitatively, role specialization enables division of labor like front-line attackers vs protectors, which directly improves team success (e.g., ROMA begins to win challenging MMM2 after roles differentiate). Numerical benefit examples: strong separation in learned dissimilarity scores (≈0.96 vs ≈0.08) indicating meaningful role clustering that correlates with performance gains; ablation plots (Fig. 4–6) show major drops when role regularizers are removed.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Potential instability if roles change too rapidly (addressed by L_I); risk of all agents collapsing to same role if specialization regularizer isn't constrained (addressed by dissimilarity model d_φ and compactness objective); no explicit communication channel limits coordination mechanisms to what can be captured by latent roles and centralized training; performance gains rely on careful hyperparameter tuning (λ_I, λ_D) and design choices (latent dim=3, clipping variances).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Yes — ablations removing L_I and/or L_D were run: (i) L_TD only (no role losses) performs poorly (worse than QMIX on some maps); (ii) L_TD + L_I yields some stability but limited performance gains; (iii) L_TD + L_D yields most of the performance improvement, indicating the specialization loss L_D is more important for performance; removing both regularizers prevents ROMA from winning on hard tasks like MMM2.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>The paper reports chosen hyperparameters that worked across experiments: latent role dimensionality = 3, λ_I = 1e-4, λ_D = 1e-2, role variance clipped (min var ≈ 0.1), role encoder/decoder/trajectory encoder are small FC/GRU networks (12-d hidden layers mentioned), RMSprop optimizer lr=5e-4, ε-greedy annealed from 1.0 to 0.05 over 50k steps. Empirically, L_D is more critical than L_I; roles of dimension 3 allowed direct visualization without dimensionality reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The system is evaluated entirely in a game/micromanagement domain (SMAC); no application to scientific-research workflows is presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROMA: Multi-Agent Reinforcement Learning with Emergent Roles', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2545.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2545.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QMIX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QMIX (Monotonic Value Function Factorisation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CTDE value-decomposition method that represents the joint action-value as a state-conditioned monotonic mixing of individual agent utilities, using a hyper-network to generate mixing weights conditioned on the global state.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>QMIX (mixing network)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>QMIX trains individual agent utility (local Q) networks and a mixing network whose weights and biases are produced by a hyper-network conditioned on global state; the mixing network enforces a monotonic relationship between local utilities and global Q_tot to permit decentralized argmax policies.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (used across maps in experiments; identical to ROMA experimental settings)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Typically agents share network parameters (homogeneous policy) unless using no-parameter-sharing variants; specialization can be induced only via state/observation differences or via explicit non-shared parameters (QMIX-NPS).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>policy learning and execution</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized training with a mixing network that enforces a monotonic combination of per-agent utilities into a global value, enabling decentralized execution.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>No explicit inter-agent messaging; coordination is via value decomposition and shared gradients during centralized training.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Global TD loss via mixing network; each agent's utility receives gradients from the centralized loss.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Implicit during training (per timestep when computing Q_tot); no runtime agent-to-agent messages.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>StarCraft II micromanagement benchmark (used as baseline and as ROMA's mixing component).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Win rate on SMAC maps (baseline reference for ROMA comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as a main baseline; ROMA is compared against QMIX and shown to outperform it on several maps, especially larger ones.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Enables decentralized policies to be trained centrally with global credit assignment via monotonic mixing, improving learning stability/credit assignment relative to naive independent learners.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Monotonicity constraint can limit representable joint value functions and exploration; this motivates alternatives like ROMA and MAVEN.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>QMIX variants (QMIX-NPS, QMIX-LAR) used in paper to test effects of parameter sharing and parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Standard QMIX hyperparameters used (mixing net hidden dims, hyper-net sizes) as described in the paper; ROMA uses QMIX mixing net by default.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROMA: Multi-Agent Reinforcement Learning with Emergent Roles', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2545.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2545.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAVEN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAVEN (Multi-Agent Variational Exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MARL approach that augments centralized training with a latent variable to encourage diverse joint trajectories via mutual information maximization, aimed at improving exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MAVEN: Multi-Agent Variational Exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MAVEN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MAVEN conditions agent policies on a latent variable (global) to induce diverse joint behaviors and optimizes a mutual information objective between the latent variable and joint trajectories to encourage committed exploration across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (used as baseline across SMAC maps)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Latent-conditioned policies can result in different joint strategies across episodes but do not explicitly induce per-agent emergent role specialization as ROMA does.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>policy learning (exploration enhancement) and execution</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Latent-variable conditioning to coordinate exploration of the joint policy; training centralized with latent regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>No explicit agent-to-agent messaging; coordination via shared latent variable and centralized training.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Mutual-information objective between latent and joint trajectory provides auxiliary gradient signal to encourage diverse exploration; standard TD feedback as well.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Latent variable typically sampled per episode (committed exploration) rather than per timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Benchmark MARL domains including StarCraft II (SMAC) used as baseline comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Win rate on SMAC maps; in this paper MAVEN performs worse than QMIX on most tested SMAC maps.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly in experiments; ROMA outperforms MAVEN on the SMAC maps evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Improves exploration diversity in some domains by encouraging diverse joint trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Designed for committed exploration (episode-level latent) and thus differs in purpose from role-specialization; in SMAC immediate engagements reduce the importance of exploration, reducing MAVEN's advantage there.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>MAVEN compared as a baseline; no internal ablations presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROMA: Multi-Agent Reinforcement Learning with Emergent Roles', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2545.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2545.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QMIX-NPS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QMIX No Parameter Sharing (QMIX-NPS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A QMIX variant used in experiments where agents do not share network parameters (each agent has distinct local utility networks) to test effects of parameter sharing vs specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>QMIX-NPS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Same QMIX architecture but with individual agents having separate local utility networks (no shared parameters) to allow agent-specific learning and evaluate the effect of parameter sharing.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (same as map)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Agents can specialize by having separate parameters, but no explicit role induction mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>policy learning and execution</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized mixing network for training (QMIX) but local networks are distinct; coordination through mixing network and centralized training.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>No explicit inter-agent messaging.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Global TD via mixing network provides gradient to each separate local network.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Implicit per timestep during training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>SMAC benchmark (used as ablation baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Win rates compared to ROMA and QMIX; QMIX-NPS is shown to train slower than QMIX (parameter sharing speeds up training), and ROMA outperforms both.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Included in experiments to show that dynamic parameter sharing via roles (ROMA) can outperform both full parameter sharing (QMIX) and no-sharing (QMIX-NPS).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>No parameter-sharing allows more specialized policies but increases sample complexity; ROMA's dynamic sharing aims to combine benefits of both.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Higher learning complexity without parameter sharing; slower/fewer generalization benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROMA: Multi-Agent Reinforcement Learning with Emergent Roles', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2545.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2545.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QMIX-LAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QMIX with Larger Architectures (QMIX-LAR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A QMIX variant with a similar parameter count to ROMA (added layers) used to test whether ROMA's improvements come from increased model capacity rather than role mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>QMIX-LAR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>QMIX augmented with additional fully-connected layers (80 and 25 dims after GRU) so the total parameter count approximates ROMA, serving as a capacity-matched baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>No explicit role mechanism; any specialization arises only from differing observations and learned weights.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>policy learning and execution</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>QMIX mixing network</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>None explicit</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Global TD via mixing net</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Implicit during training</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>SMAC benchmark (ablation baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Compared to ROMA; ROMA outperforms QMIX-LAR, indicating improvements are not solely due to parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used to rule out parameter-count confound.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>None beyond standard QMIX benefits; used to test if capacity explains ROMA gains.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Increased parameters without role structure does not match ROMA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROMA: Multi-Agent Reinforcement Learning with Emergent Roles', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2545.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2545.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COMA (Counterfactual Multi-Agent Policy Gradients)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent policy gradient method that uses a centralized critic to compute counterfactual baselines for variance-reduced credit assignment to each agent's policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Counterfactual Multi-Agent Policy Gradients (COMA)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>COMA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Actor-critic multi-agent RL where the centralized critic computes advantage estimates and counterfactual baselines per agent to provide more accurate credit assignment for policy gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Agents have separate policies (actor networks); specialization arises during learning but no explicit role induction.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>policy learning and execution</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized critic provides improved credit assignment, enabling coordinated decentralized policies.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>No explicit agent-to-agent messaging; coordination via centralized critic during training.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Centralized critic computes advantage and counterfactual baselines used to update individual actors.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Implicit during training per timestep for gradient computation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Benchmark multi-agent domains including SMAC (used as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Win rate comparisons; included as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROMA: Multi-Agent Reinforcement Learning with Emergent Roles', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning <em>(Rating: 2)</em></li>
                <li>MAVEN: Multi-Agent Variational Exploration <em>(Rating: 2)</em></li>
                <li>Counterfactual Multi-Agent Policy Gradients (COMA) <em>(Rating: 2)</em></li>
                <li>Learning to communicate with deep multi-agent reinforcement learning <em>(Rating: 1)</em></li>
                <li>Multi-agent actor-critic for mixed cooperative-competitive environments (MADDPG) <em>(Rating: 1)</em></li>
                <li>The StarCraft Multi-Agent Challenge (SMAC) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2545",
    "paper_id": "paper-5e8350467d7d19e1d5f991be23f7b4826a9303a8",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "ROMA",
            "name_full": "Role-Oriented Multi-Agent Reinforcement Learning (ROMA)",
            "brief_description": "A CTDE multi-agent RL framework that induces emergent, stochastic role embeddings conditioned on local observations and uses them to generate agent-specific policy parameters, together with two mutual-information-based regularizers that encourage roles to be identifiable (temporally stable) and specialized (clustered by sub-task).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ROMA",
            "system_description": "ROMA is a centralized-training / decentralized-execution (CTDE) MARL system. Each agent has a role encoder f(o_i; θ_ρ) that outputs parameters (μ_ρi, σ_ρi) of a multivariate Gaussian role distribution; a role ρ_i is sampled and passed through a role decoder (hyper-network) g(ρ_i; θ_h) to generate parameters θ_i of the agent's local utility (Q) network. Local utilities are combined by a state-conditioned mixing network (QMIX-style) during training to produce a global Q_tot used for TD learning. Two regularizers are added: L_I (a variational lower bound proxy to maximize I(ρ_i; τ_i | o_i)) to make roles identifiable/stable, and L_D (a surrogate encouraging role specialization via mutual-information terms plus a learned dissimilarity model d_φ between agent trajectories), which in practice is implemented via variational posterior q_ξ (a GRU trajectory encoder) and minimizing the losses alongside the TD loss.",
            "number_of_agents": "variable (experiments from small teams up to &gt;10 agents; maps include 2–30+ units)",
            "agent_specializations": "Specialization is emergent: roles correspond to sub-tasks discovered by optimization (e.g., Medivac specializing as healer; some units specialize in frontline engagement, sacrificial maneuvers, or staying back and sniping). There are no hand-crafted role labels; agents specialize by clustering in latent-role space induced by L_I and L_D. Examples from experiments: 'Medivac' role (healing), 'frontline attacker' role, 'protector/backline' role, 'sacrificial engager' role.",
            "research_phases_covered": "execution and learning phases of task-oriented behavior (policy learning, online execution). Not targeted at research phases like literature review or hypothesis generation.",
            "coordination_mechanism": "Centralized training via a mixing network (QMIX) that combines local utilities into a global Q for TD updates; decentralized execution where each agent acts based on its local policy parameterized by its sampled role (no centralized controller at execution). Coordination emerges through shared training signal and role-conditioned policies (implicit coordination via shared gradients and latent-role clustering).",
            "communication_protocol": "No explicit inter-agent message passing during execution; communication is implicit via centralized training and shared replay buffer. Roles are conditioned only on local observations (no explicit message content format). During training agents' trajectories are fed into the trajectory encoder and dissimilarity model (internal neural inputs), but this is not an explicit runtime message-passing protocol between agents.",
            "feedback_mechanism": "Global TD loss computed from mixing network provides centralized reward feedback; variational posterior q_ξ (trajectory encoder) and the dissimilarity model d_φ provide auxiliary gradient signals (regularizers L_I and L_D) that shape role distributions; replay buffer samples provide off-policy feedback. No peer-to-peer iterative feedback during execution.",
            "communication_frequency": "No runtime inter-agent messaging; role sampling and policy conditioning occur each timestep (roles are stochastic and drawn from the role distribution at each timestep), but agents do not send structured messages to each other.",
            "task_domain": "StarCraft II micromanagement (SMAC) benchmark — multi-agent game/micromanagement tasks (not scientific research).",
            "performance_metrics": "Primary reported metric: win rate on SMAC maps (averaged over 5 random seeds with 95% CI). Quantitative dissimilarity metrics reported for role clustering: learned dissimilarity d_φ between different unit types = 0.9556 ± 0.0009; between same unit type = 0.0780 ± 0.0019. Exact win-rate numbers per map are given in plots (qualitatively ROMA yields substantially higher win rates across easy/hard/super-hard maps).",
            "baseline_comparison": "ROMA is compared against QMIX, QMIX-NPS (no parameter sharing), QMIX-LAR (larger-parameter QMIX), MAVEN, COMA, MADDPG and others. ROMA outperforms these baselines on both homogeneous and heterogeneous SMAC maps, with larger gains on maps with &gt;10 agents. MAVEN performs worse than QMIX on most maps in their experiments.",
            "coordination_benefits": "Emergent role-based coordination provides: improved learning efficiency (faster convergence), higher win rates across SMAC maps (state-of-the-art reported in this paper), better scalability as agent count increases (&gt;10 agents). Qualitatively, role specialization enables division of labor like front-line attackers vs protectors, which directly improves team success (e.g., ROMA begins to win challenging MMM2 after roles differentiate). Numerical benefit examples: strong separation in learned dissimilarity scores (≈0.96 vs ≈0.08) indicating meaningful role clustering that correlates with performance gains; ablation plots (Fig. 4–6) show major drops when role regularizers are removed.",
            "coordination_challenges": "Potential instability if roles change too rapidly (addressed by L_I); risk of all agents collapsing to same role if specialization regularizer isn't constrained (addressed by dissimilarity model d_φ and compactness objective); no explicit communication channel limits coordination mechanisms to what can be captured by latent roles and centralized training; performance gains rely on careful hyperparameter tuning (λ_I, λ_D) and design choices (latent dim=3, clipping variances).",
            "ablation_studies": "Yes — ablations removing L_I and/or L_D were run: (i) L_TD only (no role losses) performs poorly (worse than QMIX on some maps); (ii) L_TD + L_I yields some stability but limited performance gains; (iii) L_TD + L_D yields most of the performance improvement, indicating the specialization loss L_D is more important for performance; removing both regularizers prevents ROMA from winning on hard tasks like MMM2.",
            "optimal_configurations": "The paper reports chosen hyperparameters that worked across experiments: latent role dimensionality = 3, λ_I = 1e-4, λ_D = 1e-2, role variance clipped (min var ≈ 0.1), role encoder/decoder/trajectory encoder are small FC/GRU networks (12-d hidden layers mentioned), RMSprop optimizer lr=5e-4, ε-greedy annealed from 1.0 to 0.05 over 50k steps. Empirically, L_D is more critical than L_I; roles of dimension 3 allowed direct visualization without dimensionality reduction.",
            "notes": "The system is evaluated entirely in a game/micromanagement domain (SMAC); no application to scientific-research workflows is presented in this paper.",
            "uuid": "e2545.0",
            "source_info": {
                "paper_title": "ROMA: Multi-Agent Reinforcement Learning with Emergent Roles",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "QMIX",
            "name_full": "QMIX (Monotonic Value Function Factorisation)",
            "brief_description": "A CTDE value-decomposition method that represents the joint action-value as a state-conditioned monotonic mixing of individual agent utilities, using a hyper-network to generate mixing weights conditioned on the global state.",
            "citation_title": "QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning",
            "mention_or_use": "use",
            "system_name": "QMIX (mixing network)",
            "system_description": "QMIX trains individual agent utility (local Q) networks and a mixing network whose weights and biases are produced by a hyper-network conditioned on global state; the mixing network enforces a monotonic relationship between local utilities and global Q_tot to permit decentralized argmax policies.",
            "number_of_agents": "variable (used across maps in experiments; identical to ROMA experimental settings)",
            "agent_specializations": "Typically agents share network parameters (homogeneous policy) unless using no-parameter-sharing variants; specialization can be induced only via state/observation differences or via explicit non-shared parameters (QMIX-NPS).",
            "research_phases_covered": "policy learning and execution",
            "coordination_mechanism": "Centralized training with a mixing network that enforces a monotonic combination of per-agent utilities into a global value, enabling decentralized execution.",
            "communication_protocol": "No explicit inter-agent messaging; coordination is via value decomposition and shared gradients during centralized training.",
            "feedback_mechanism": "Global TD loss via mixing network; each agent's utility receives gradients from the centralized loss.",
            "communication_frequency": "Implicit during training (per timestep when computing Q_tot); no runtime agent-to-agent messages.",
            "task_domain": "StarCraft II micromanagement benchmark (used as baseline and as ROMA's mixing component).",
            "performance_metrics": "Win rate on SMAC maps (baseline reference for ROMA comparisons).",
            "baseline_comparison": "Used as a main baseline; ROMA is compared against QMIX and shown to outperform it on several maps, especially larger ones.",
            "coordination_benefits": "Enables decentralized policies to be trained centrally with global credit assignment via monotonic mixing, improving learning stability/credit assignment relative to naive independent learners.",
            "coordination_challenges": "Monotonicity constraint can limit representable joint value functions and exploration; this motivates alternatives like ROMA and MAVEN.",
            "ablation_studies": "QMIX variants (QMIX-NPS, QMIX-LAR) used in paper to test effects of parameter sharing and parameter count.",
            "optimal_configurations": "Standard QMIX hyperparameters used (mixing net hidden dims, hyper-net sizes) as described in the paper; ROMA uses QMIX mixing net by default.",
            "uuid": "e2545.1",
            "source_info": {
                "paper_title": "ROMA: Multi-Agent Reinforcement Learning with Emergent Roles",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "MAVEN",
            "name_full": "MAVEN (Multi-Agent Variational Exploration)",
            "brief_description": "A MARL approach that augments centralized training with a latent variable to encourage diverse joint trajectories via mutual information maximization, aimed at improving exploration.",
            "citation_title": "MAVEN: Multi-Agent Variational Exploration",
            "mention_or_use": "use",
            "system_name": "MAVEN",
            "system_description": "MAVEN conditions agent policies on a latent variable (global) to induce diverse joint behaviors and optimizes a mutual information objective between the latent variable and joint trajectories to encourage committed exploration across episodes.",
            "number_of_agents": "variable (used as baseline across SMAC maps)",
            "agent_specializations": "Latent-conditioned policies can result in different joint strategies across episodes but do not explicitly induce per-agent emergent role specialization as ROMA does.",
            "research_phases_covered": "policy learning (exploration enhancement) and execution",
            "coordination_mechanism": "Latent-variable conditioning to coordinate exploration of the joint policy; training centralized with latent regularization.",
            "communication_protocol": "No explicit agent-to-agent messaging; coordination via shared latent variable and centralized training.",
            "feedback_mechanism": "Mutual-information objective between latent and joint trajectory provides auxiliary gradient signal to encourage diverse exploration; standard TD feedback as well.",
            "communication_frequency": "Latent variable typically sampled per episode (committed exploration) rather than per timestep.",
            "task_domain": "Benchmark MARL domains including StarCraft II (SMAC) used as baseline comparisons in this paper.",
            "performance_metrics": "Win rate on SMAC maps; in this paper MAVEN performs worse than QMIX on most tested SMAC maps.",
            "baseline_comparison": "Compared directly in experiments; ROMA outperforms MAVEN on the SMAC maps evaluated.",
            "coordination_benefits": "Improves exploration diversity in some domains by encouraging diverse joint trajectories.",
            "coordination_challenges": "Designed for committed exploration (episode-level latent) and thus differs in purpose from role-specialization; in SMAC immediate engagements reduce the importance of exploration, reducing MAVEN's advantage there.",
            "ablation_studies": "MAVEN compared as a baseline; no internal ablations presented in this paper.",
            "uuid": "e2545.2",
            "source_info": {
                "paper_title": "ROMA: Multi-Agent Reinforcement Learning with Emergent Roles",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "QMIX-NPS",
            "name_full": "QMIX No Parameter Sharing (QMIX-NPS)",
            "brief_description": "A QMIX variant used in experiments where agents do not share network parameters (each agent has distinct local utility networks) to test effects of parameter sharing vs specialization.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "QMIX-NPS",
            "system_description": "Same QMIX architecture but with individual agents having separate local utility networks (no shared parameters) to allow agent-specific learning and evaluate the effect of parameter sharing.",
            "number_of_agents": "variable (same as map)",
            "agent_specializations": "Agents can specialize by having separate parameters, but no explicit role induction mechanism.",
            "research_phases_covered": "policy learning and execution",
            "coordination_mechanism": "Centralized mixing network for training (QMIX) but local networks are distinct; coordination through mixing network and centralized training.",
            "communication_protocol": "No explicit inter-agent messaging.",
            "feedback_mechanism": "Global TD via mixing network provides gradient to each separate local network.",
            "communication_frequency": "Implicit per timestep during training.",
            "task_domain": "SMAC benchmark (used as ablation baseline).",
            "performance_metrics": "Win rates compared to ROMA and QMIX; QMIX-NPS is shown to train slower than QMIX (parameter sharing speeds up training), and ROMA outperforms both.",
            "baseline_comparison": "Included in experiments to show that dynamic parameter sharing via roles (ROMA) can outperform both full parameter sharing (QMIX) and no-sharing (QMIX-NPS).",
            "coordination_benefits": "No parameter-sharing allows more specialized policies but increases sample complexity; ROMA's dynamic sharing aims to combine benefits of both.",
            "coordination_challenges": "Higher learning complexity without parameter sharing; slower/fewer generalization benefits.",
            "uuid": "e2545.3",
            "source_info": {
                "paper_title": "ROMA: Multi-Agent Reinforcement Learning with Emergent Roles",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "QMIX-LAR",
            "name_full": "QMIX with Larger Architectures (QMIX-LAR)",
            "brief_description": "A QMIX variant with a similar parameter count to ROMA (added layers) used to test whether ROMA's improvements come from increased model capacity rather than role mechanisms.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "QMIX-LAR",
            "system_description": "QMIX augmented with additional fully-connected layers (80 and 25 dims after GRU) so the total parameter count approximates ROMA, serving as a capacity-matched baseline.",
            "number_of_agents": "variable",
            "agent_specializations": "No explicit role mechanism; any specialization arises only from differing observations and learned weights.",
            "research_phases_covered": "policy learning and execution",
            "coordination_mechanism": "QMIX mixing network",
            "communication_protocol": "None explicit",
            "feedback_mechanism": "Global TD via mixing net",
            "communication_frequency": "Implicit during training",
            "task_domain": "SMAC benchmark (ablation baseline)",
            "performance_metrics": "Compared to ROMA; ROMA outperforms QMIX-LAR, indicating improvements are not solely due to parameter count.",
            "baseline_comparison": "Used to rule out parameter-count confound.",
            "coordination_benefits": "None beyond standard QMIX benefits; used to test if capacity explains ROMA gains.",
            "coordination_challenges": "Increased parameters without role structure does not match ROMA performance.",
            "uuid": "e2545.4",
            "source_info": {
                "paper_title": "ROMA: Multi-Agent Reinforcement Learning with Emergent Roles",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "COMA",
            "name_full": "COMA (Counterfactual Multi-Agent Policy Gradients)",
            "brief_description": "A multi-agent policy gradient method that uses a centralized critic to compute counterfactual baselines for variance-reduced credit assignment to each agent's policy.",
            "citation_title": "Counterfactual Multi-Agent Policy Gradients (COMA)",
            "mention_or_use": "use",
            "system_name": "COMA",
            "system_description": "Actor-critic multi-agent RL where the centralized critic computes advantage estimates and counterfactual baselines per agent to provide more accurate credit assignment for policy gradients.",
            "number_of_agents": "variable",
            "agent_specializations": "Agents have separate policies (actor networks); specialization arises during learning but no explicit role induction.",
            "research_phases_covered": "policy learning and execution",
            "coordination_mechanism": "Centralized critic provides improved credit assignment, enabling coordinated decentralized policies.",
            "communication_protocol": "No explicit agent-to-agent messaging; coordination via centralized critic during training.",
            "feedback_mechanism": "Centralized critic computes advantage and counterfactual baselines used to update individual actors.",
            "communication_frequency": "Implicit during training per timestep for gradient computation.",
            "task_domain": "Benchmark multi-agent domains including SMAC (used as baseline).",
            "performance_metrics": "Win rate comparisons; included as a baseline in experiments.",
            "uuid": "e2545.5",
            "source_info": {
                "paper_title": "ROMA: Multi-Agent Reinforcement Learning with Emergent Roles",
                "publication_date_yy_mm": "2020-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "MAVEN: Multi-Agent Variational Exploration",
            "rating": 2
        },
        {
            "paper_title": "Counterfactual Multi-Agent Policy Gradients (COMA)",
            "rating": 2
        },
        {
            "paper_title": "Learning to communicate with deep multi-agent reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Multi-agent actor-critic for mixed cooperative-competitive environments (MADDPG)",
            "rating": 1
        },
        {
            "paper_title": "The StarCraft Multi-Agent Challenge (SMAC)",
            "rating": 2
        }
    ],
    "cost": 0.01613675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ROMA: Multi-Agent Reinforcement Learning with Emergent Roles</h1>
<p>Tonghan Wang ${ }^{1}$ Heng Dong ${ }^{1}$ Victor Lesser ${ }^{2}$ Chongjie Zhang ${ }^{1}$</p>
<h4>Abstract</h4>
<p>The role concept provides a useful tool to design and understand complex multi-agent systems, which allows agents with a similar role to share similar behaviors. However, existing rolebased methods use prior domain knowledge and predefine role structures and behaviors. In contrast, multi-agent reinforcement learning (MARL) provides flexibility and adaptability, but less efficiency in complex tasks. In this paper, we synergize these two paradigms and propose a role-oriented MARL framework (ROMA). In this framework, roles are emergent, and agents with similar roles tend to share their learning and to be specialized on certain sub-tasks. To this end, we construct a stochastic role embedding space by introducing two novel regularizers and conditioning individual policies on roles. Experiments show that our method can learn specialized, dynamic, and identifiable roles, which help our method push forward the state of the art on the StarCraft II micromanagement benchmark. Demonstrative videos are available at https: //sites.google.com/view/romarl/.</p>
<h2>1. Introduction</h2>
<p>Many real-world systems can be modeled as multi-agent systems (MAS), such as autonomous vehicle teams (Cao et al., 2012), intelligent warehouse systems (Nowé et al., 2012), and sensor networks (Zhang \&amp; Lesser, 2011). Cooperative multi-agent reinforcement learning (MARL) provides a promising approach to developing these systems, allowing agents to deal with uncertainty and adapt to the dynamics of an environment. In recent years, cooperative MARL has achieved prominent progress, and many deep methods have been proposed (Foerster et al., 2018; Sunehag et al., 2018;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Visualization of our learned role representations at a timestep. The blue agent has the maximum health, while the red ones are dead. The corresponding policy is that agent 6 moves towards enemies to take on more firepower, so that more seriously injured agents are protected. Roles can change adaptively and will aggregate according to responsibilities that are compatible with individual characteristics, such as location, agent type, health, etc.</p>
<p>Rashid et al., 2018; Son et al., 2019; Vinyals et al., 2019; Wang et al., 2020b; Baker et al., 2020).</p>
<p>In order to achieve scalability, these deep MARL methods adopt a simple mechanism that all agents share and learn a decentralized value or policy network. However, such simple sharing is often not effective for many complex multi-agent tasks. For example, in Adam Smith's Pin Factory, workers must complete up to eighteen different tasks to create one pin (Smith, 1937). In this case, it is a heavy burden for a single shared policy to represent and learn all required skills. On the other hand, it is also unnecessary for each agent to use a distinct policy network, which leads to high learning complexity because some agents often perform similar sub-tasks from time to time. The question is how we can give full play to agents' specialization and dynamic sharing for improving learning efficiency.</p>
<p>A natural concept that comes to mind is the role. A role is a comprehensive pattern of behavior, often specialized in some tasks. Agents with similar roles will show similar behaviors, and thus can share their experiences to improve performance. The role theory has been widely studied in economics, sociology, and organization theory. Researchers have also introduced the concept of role into MAS (Becht et al., 1999; Stone \&amp; Veloso, 1999; Depke et al., 2001; Ferber et al., 2003; Odell et al., 2004; Bonjean et al., 2014; Lhaksmana et al., 2018). In these role-based frameworks, the complexity of agent design is reduced via task decomposition by defining roles associated with responsibilities</p>
<p>made up of a set of sub-tasks, so that the policy search space is effectively decomposed (Zhu \&amp; Zhou, 2008). However, these works exploit prior domain knowledge to decompose tasks and predefine the responsibilities of each role, which prevents role-based MAS from being dynamic and adaptive to uncertain environments.</p>
<p>To leverage the benefits of both role-based and learning methods, in this paper, we propose a role-oriented multi-agent reinforcement learning framework (ROMA). This framework implicitly introduces the role concept into MARL, which serves as an intermediary to enable agents with similar responsibilities to share their learning. We achieve this by ensuring that agents with similar roles have both similar policies and responsibilities. To establish the connection between roles and decentralized policies, ROMA conditions agents' policies on individual roles, which are stochastic latent variables determined by agents' local observations. To associate roles with responsibilities, we introduce two regularizers to enable roles to be identifiable by behaviors and specialized in certain sub-tasks. We show how well-formed role representations can be learned via optimizing tractable variational estimations of the proposed regularizers. In this way, our method synergizes rolebased and learning methods while avoiding their individual shortcomings - we provide a flexible and general-purpose mechanism that promotes the emergence and specialization of roles, which in turn provides an adaptive learning sharing mechanism for efficient multi-agent policy learning.</p>
<p>We test our method on StarCraft II ${ }^{1}$ micromanagement environments (Vinyals et al., 2017; Samvelyan et al., 2019). Results show that our method significantly pushes forward the state of the art of MARL algorithms, by virtue of the adaptive policy sharing among agents with similar roles. Visualization of the role representations in both homogeneous and heterogeneous agent teams demonstrates that the learned roles can adapt automatically in dynamic environments, and that agents with similar responsibilities have similar roles. In addition, the emergence and evolution process of roles is shown, highlighting the connection between role-driven sub-task specialization and improvement of team efficiency in our framework. These results provide a new perspective in understanding and promoting the emergence of cooperation among agents.</p>
<h2>2. Background</h2>
<p>In our work, we consider a fully cooperative multi-agent task that can be modelled by a Dec-POMDP (Oliehoek et al., 2016) $G=\langle I, S, A, P, R, \Omega, O, n, \gamma\rangle$, where $A$ is the finite action set, $I$ is the finite set of $n$ agents, $\gamma \in[0,1)$ is the discount factor, and $s \in S$ is the true state of the environ-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ment. We consider partially observable settings and agent $i$ only has access to an observation $o_{i} \in \Omega$ drawn according to the observation function $O(s, i)$. Each agent has a history $\tau_{i} \in \mathrm{~T} \equiv(\Omega \times A)^{*}$. At each timestep, each agent $i$ selects an action $a_{i} \in A$, forming a joint action $\boldsymbol{a} \in A^{n}$, leading to next state $s^{\prime}$ according to the transition function $P\left(s^{\prime} \mid s, \boldsymbol{a}\right)$ and a shared reward $r=R(s, \boldsymbol{a})$ for each agent. The joint policy $\pi$ induces a joint action-value function: $Q_{\text {tot }}^{\pi}(s, \boldsymbol{a})=\mathbb{E}<em 0:="0:" _infty="\infty">{s</em>}, \boldsymbol{a<em t="0">{0: \infty}}\left[\sum</em>, \pi\right]$.}^{\infty} \gamma^{t} r_{t} \mid s_{0}=s, \boldsymbol{a}_{0}=\boldsymbol{a</p>
<p>To effectively learn policies for agents, the paradigm of centralized training with decentralized execution (CTDE) (Foerster et al., 2016; 2018; Wang et al., 2020a) has recently attracted attention from deep MARL to deal with nonstationarity while learning decentralized policies. One of the promising ways to exploit the CTDE paradigm is value function decomposition (Sunehag et al., 2018; Rashid et al., 2018; Son et al., 2019; Wang et al., 2020b), which learns a decentralized utility function for each agent and uses a mixing network to combine these local utilities into a global action value. To achieve learning scalability, existing CTDE methods typically learn a shared local value or policy network for agents. However, this simple sharing mechanism is often not sufficient for learning complex tasks, where diverse responsibilities or skills are required to achieve goals. In this paper, we develop a novel role-based MARL framework to address this challenge. This framework achieves efficient shared learning while allowing agents to learn sufficiently diverse skills.</p>
<h2>3. Method</h2>
<p>In this section, we will present a novel role-oriented MARL framework (ROMA) that introduces the role concept into MARL and enables adaptive shared learning among agents. ROMA adopts the CTDE paradigm. As shown in Fig. 2, it learns local Q-value functions for agents, which are fed into a mixing network to compute a global TD loss for centralized training. During the execution, the mixing network will be removed, and each agent will act based on its local policy derived from its value function. Agents' value functions or policies are dependent on their roles, each of which is responsible for performing similar automatically identified sub-tasks. To enable efficient and effective shared learning among agents with similar behaviors, ROMA will automatically learn roles that are:
i) Dynamic: An agent's role can automatically adapt to the dynamics of the environment;
ii) Identifiable: The role of an agent contains enough information about its behaviors;
iii) Specialized: Agents with similar roles are expected to specialize in similar sub-tasks.</p>
<p>Formally, each agent $i$ has a local utility function (or an individual policy), whose parameters $\theta_{i}$ are conditioned on its role $\rho_{i}$. To learn roles with desired properties, we encode roles in a stochastic embedding space, and the role of agent $i$, $\rho_{i}$, is drawn from a multivariate Gaussian distribution $\mathcal{N}\left(\boldsymbol{\mu}<em i="i">{\rho</em>}}, \boldsymbol{\sigma<em i="i">{\rho</em>\right)$. To enable the dynamic property, ROMA conditions an agent’s role on its local observations, and uses a trainable neural network $f$ to learn the parameters of the Gaussian distribution of the role:}</p>
<p>$$
\begin{aligned}
\left(\boldsymbol{\mu}<em i="i">{\rho</em>}}, \boldsymbol{\sigma<em i="i">{\rho</em>\right) \
\rho_{i} &amp; \sim \mathcal{N}\left(\boldsymbol{\mu}}}\right) &amp; =f\left(o_{i} ; \theta_{\rho<em i="i">{\rho</em>}}, \boldsymbol{\sigma<em i="i">{\rho</em>\right)
\end{aligned}
$$}</p>
<p>where $\theta_{\rho}$ are parameters of $f$. The sampled role $\rho_{i}$ is then fed into a hyper-network $g\left(\rho_{i} ; \theta_{h}\right)$ parameterized by $\theta_{h}$ to generate the parameters for the individual policy, $\theta_{i}$. We call $f$ the role encoder and $g$ the role decoder. In the next two sub-sections, we will describe two regularizers for learning identifiable and specialized roles.</p>
<h3>3.1. Identifiable Roles</h3>
<p>Introducing latent role embedding and conditioning individual policies on this embedding does not automatically generate roles with desired properties. Intuitively, conditioning roles on local observations enables roles to be responsive to the changes in the environment. This design enables ROMA to be adaptive to dynamic environments but may cause roles to change quickly, making learning unstable. For addressing this problem, we expect roles to be temporally stable. To this end, we propose to learn roles that are identifiable by agents’ long term behaviors, which can be achieved by maximizing $I\left(\tau_{i} ; \rho_{i} \mid o_{i}\right)$, the conditional mutual information between the individual trajectory and the role given the current observation.</p>
<p>However, estimating and maximizing mutual information is often intractable. Drawing inspiration from the literature of variational inference (Wainwright et al., 2008; Alemi et al., 2017), we introduce a variational posterior estimator to derive a tractable lower bound for the mutual information objective (the proof is deferred to Appendix A.1):</p>
<p>$$
I\left(\rho_{i}^{t} ; \tau_{i}^{t-1} \mid o_{i}^{t}\right) \geq \mathbb{E}<em i="i">{\rho</em>\right]
$$}^{t}, \tau_{i}^{t-1}, o_{i}^{t}}\left[\log \frac{q_{\xi}\left(\rho_{i}^{t} \mid \tau_{i}^{t-1}, o_{i}^{t}\right)}{p\left(\rho_{i}^{t} \mid o_{i}^{t}\right)</p>
<p>where $\tau_{i}^{t-1}=\left(o_{i}^{0}, a_{i}^{0}, \cdots, o_{i}^{t-1}, a_{i}^{t-1}\right), q_{\xi}$ is the variational estimator parameterised with $\xi$. For $q_{\xi}$, we use a GRU (Cho et al., 2014) to encode an agent’s history of observations and actions, and call it the trajectory encoder. The lower bound in Eq. 2 can be further rewritten as a loss function to be minimized:</p>
<p>$$
\mathcal{L}<em _rho="\rho">{I}\left(\theta</em>}, \xi\right)=\mathbb{E<em i="i">{\left{\tau</em>\right)\right]\right]
$$}^{t-1}, o_{i}^{t}\right} \sim D}\left[D_{\mathrm{KL}}\left[p\left(\rho_{i}^{t} \mid o_{i}^{t}\right) | q_{\xi}\left(\rho_{i}^{t} \mid \tau_{i}^{t-1}, o_{i}^{t</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Schematics of our approach. The role encoder generates a role embedding distribution, from which a role is sampled and serves as the input to the role decoder. The role decoder generates the parameters of the local utility network. Local utilities are fed into a mixing network to get an estimation of the global action value. We propose two learning objectives to learn specialized and identifiable roles. The framework can be trained in an end-to-end manner.
where $\mathcal{D}$ is a replay buffer, and $D_{\mathrm{KL}}[\cdot | \cdot]$ is the KL divergence operator. The detailed derivation can be found in Appendix A.1.</p>
<h3>3.2. Specialized Roles</h3>
<p>The formulation so far does not promote sub-task specialization, which is the critical component to share learning and improve efficiency in multi-agent systems. Minimizing $\mathcal{L}_{I}$ enables roles to contain enough information about long-term behaviors but does not explicitly ensure agents with similar behaviors to have similar role embeddings.</p>
<p>For learning specialized roles, we define another rolelearning regularizer. Intuitively, to encourage sub-task specialization, for any two agents, we expect that either they have similar roles or they have quite different behaviors. However, it is usually unclear which agents will have similar roles during the process of role emergence, and the similarity between behaviors is not straightforward to measure.</p>
<p>Since roles have enough information about the behaviors (achieved by minimizing $\mathcal{L}<em i="i">{I}$ ), to encourage two agents $i$ and $j$ to have similar roles, we can maximize $I\left(\rho</em>\right)$, the mutual information between the role of agent $i$ and the trajectory of agent $j$. However, we do not know which agents will have similar roles, and directly optimizing this objective for all pairs of agents will result in all agents having the same role, and, correspondingly, the same policy, which} ; \tau_{j</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Dynamic role adaptation during an episode (means of the role distributions, $\mu_{\rho_{i}}$, are shown, without using any dimensionality reduction techniques). The role encoder learns to focus on different parts of observations according to the automatically discovered demands of the task. The role-induced strategy helps (a) quickly form the offensive arc when $t=1$; (b) protect injured agents when $t=8$; (c) protect dying agents and alternate fire when $t=19$.
will limit system performance. To settle this issue, we introduce a dissimilarity model $d_{\phi}: \mathrm{T} \times \mathrm{T} \rightarrow \mathbb{R}$, a trainable neural network taking two trajectories as input, and seek to maximize $I\left(\rho_{i} ; \tau_{j}\right)+d_{\phi}\left(\tau_{i}, \tau_{j}\right)$ while minimizing the number of non-zero elements in the matrix $D_{\phi}=\left(d_{i j}\right)$. Here, $d_{i j}=d_{\phi}\left(\tau_{i}, \tau_{j}\right)$ is the estimated dissimilarity between trajectories of agent $i$ and $j$. Such formulation makes sure that dissimilarity $d$ is high only when mutual information $I$ is low, so that the set of learned roles is compact but diverse, which help solve the given task efficiently. Formally, the following learning objective encourages sub-task specialization:</p>
<p>$$
\begin{array}{ll}
\underset{\theta_{i}, \xi, \phi}{\operatorname{minimize}} &amp; \left|D_{\phi}^{t}\right|<em i="i">{2,0} \
\text { subject to } &amp; I\left(\rho</em>\right)&gt;U, \forall i \neq j
\end{array}
$$}^{t} ; \tau_{j}^{t-1} \mid o_{j}^{t}\right)+d_{\phi}\left(\tau_{i}^{t-1}, \tau_{j}^{t-1</p>
<p>where $U$ controls the compactness of the role representation. In practice, we separately carry out min-max normalization on $I$ and $d$ to scale their values to $[0,1]$ and set $U$ to 1 . Relaxing the matrix norm $|\cdot|_{2,0}$ with the Frobenius norm, we can get the optimization objective for minimizing:</p>
<p>$$
\left|D_{\phi}^{t}\right|<em _neq="\neq" i="i" j="j">{F}-\sum</em>\right), U\right}
$$} \min \left{I\left(\rho_{i}^{t} ; \tau_{j}^{t-1} \mid o_{j}^{t}\right)+d_{\phi}\left(\tau_{i}^{t-1}, \tau_{j}^{t-1</p>
<p>However, as estimating and optimizing the mutual information term are intractable, we use the variational posterior estimator introduced in Sec. 3.1 to construct an upper bound, serving as the second regularizer of ROMA:</p>
<p>$$
\mathcal{L}<em _rho="\rho">{D}\left(\theta</em>}, \phi, \xi\right)=\mathbb{E<em _phi="\phi">{\left{\boldsymbol{\tau}^{t-1}, \boldsymbol{o}^{t}\right} \sim \mathcal{D}, \boldsymbol{\rho}^{t} \sim p\left(\boldsymbol{\rho}^{t} \mid \boldsymbol{o}^{t}\right)}\left[\left|D</em>\right.
$$}^{t}\right|_{F</p>
<p>$$
\left.-\sum_{i \neq j} \min \left{q_{\xi}\left(\rho_{i}^{t} \mid \tau_{j}^{t-1}, o_{j}^{t}\right)+d_{\phi}\left(\tau_{i}^{t-1}, \tau_{j}^{t-1}\right), U\right}\right]
$$</p>
<p>where $\mathcal{D}$ is the replay buffer, $\boldsymbol{\tau}^{t-1}$ is the joint trajectory, $\boldsymbol{o}^{t}$ is the joint observation, and $\boldsymbol{\rho}^{t}=\left\langle\rho_{1}^{t}, \rho_{2}^{t}, \cdots, \rho_{n}^{t}\right\rangle$. A detailed derivation can be found in Appendix A.2.</p>
<h3>3.3. Overall Optimization Objective</h3>
<p>We have introduced optimization objectives for learning roles to be identifiable and and specialized. Apart from these regularizers, all the parameters in the framework are updated by gradients induced by the standard TD loss of reinforcement learning. As shown in Fig. 2, to compute the global TD loss, individual utilities are fed into a mixing network whose output is the estimation of global action-value $Q_{t o t}$. In this paper, our ROMA implementation uses the mixing network introduced by QMIX (Rashid et al., 2018) (see Appendix D) for its monotonic approximation, but it can be easily replaced by other mixing methods. The parameters of the mixing network are conditioned on the global state $s$ and are generated by a hyper-net parameterized by $\theta_{m}$. Therefore, the final learning objective of ROMA is:</p>
<p>$$
\mathcal{L}(\theta)=\mathcal{L}<em I="I">{T D}(\theta)+\lambda</em>} \mathcal{L<em _rho="\rho">{I}\left(\theta</em>}, \xi\right)+\lambda_{D} \mathcal{L<em _rho="\rho">{D}\left(\theta</em>, \xi, \phi\right)
$$</p>
<p>where $\theta=\left(\theta_{\rho}, \xi, \phi, \theta_{h}, \theta_{m}\right), \lambda_{I}$ and $\lambda_{D}$ are scaling factors, and $\mathcal{L}<em _boldsymbol_a="\boldsymbol{a">{T D}(\theta)=\left[r+\gamma \max </em>$are the parameters of a periodically updated target network). In our centralized training with decentralized execution framework, only the role encoder, the role decoder, and the individual utility networks are used when execution.}^{\prime}} Q_{t o t}\left(s^{\prime}, \boldsymbol{a}^{\prime} ; \theta^{-}\right)-Q_{t o t}(s, \boldsymbol{a} ; \theta)\right]^{2}$ ( $\theta^{-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Comparison of our method against baseline algorithms. Results for more maps can be found in Appendix C.1.</p>
<h2>4. Related Works</h2>
<p>The emergence of role has been documented in many natural systems, such as bees (Jeanson et al., 2005), ants (Gordon, 1996), and humans (Butler, 2012). In these systems, the role is closely related to the division of labor and is crucial to the improvement of labor efficiency. Many multi-agent systems are inspired by these natural systems. They decompose the task, make agents with the same role specialize in certain sub-tasks, and thus reduce the design complexity (Wooldridge et al., 2000; Omicini, 2000; Padgham &amp; Winikoff, 2002; Pavón &amp; Gómez-Sanz, 2003; Cossentino et al., 2005; Zhu &amp; Zhou, 2008; Spanoudakis &amp; Moraitis, 2010; DeLoach &amp; Garcia-Ojeda, 2010; Bonjean et al., 2014). These methodologies are designed for tasks with a clear structure, such as software engineering (Bresciani et al., 2004). Therefore, they tend to use predefined roles and associated responsibilities (Lhaksmana et al., 2018). In contrast, we focus on how to implicitly introduce the concept of roles into general multi-agent sequential decision making under dynamic and uncertain environments.</p>
<p>Deep multi-agent reinforcement learning has witnessed vigorous progress in recent years. COMA (Foerster et al., 2018), MADDPG (Lowe et al., 2017), PR2 (Wen et al., 2019), and MAAC (Iqbal &amp; Sha, 2019) explore multi-agent policy gradients. Another line of research focuses on value-based multi-agent RL, and value-function factorization is the most popular method. VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018), and QTRAN (Son et al., 2019) have progressively enlarged the family of functions that can be represented by the mixing network. NDQ (Wang et al., 2020b) proposes nearly decomposable value functions to address the miscoordination problem in learning fully decentralized value functions. Emergence is a topic with increasing interest in deep MARL. Works on the emergence of communication (Foerster et al., 2016; Lazaridou et al., 2017; Das et al., 2017; Mordatch &amp; Abbeel, 2018; Wang et al., 2020b; Kang et al., 2020), the emergence of fairness (Jiang &amp; Lu, 2019), and the emergence of tool usage (Baker et al., 2020) provide a deep learning perspective in understanding both natural and artificial multi-agent systems.</p>
<p>To learn diverse and identifiable roles, we propose to optimize the mutual information between individual roles and trajectories. A recent work studying multi-agent exploration, MAVEN (Mahajan et al., 2019), uses a similar objective. Different from ROMA, MAVEN aims at committed exploration. This difference in high-level purpose leads to many technical distinctions. First, MAVEN optimizes the mutual information between the joint trajectory and a latent variable conditioned on a Gaussian or uniform random variable to encourage diverse joint trajectory. Second, apart from the mutual information objective, we propose a novel regularizer to learn specialized roles, while MAVEN adopts a hierarchical structure and encourages the latent variable to help get more environmental rewards. We empirically compare ROMA with MAVEN in Sec. 5. More related works will be discussed in Appendix D.</p>
<h2>5. Experiments</h2>
<p>Our experiments aim to answer the following questions: (1) Whether the learned roles can automatically adapt in dynamic environments? (Sec. 5.1.) (2) Can our method promote sub-task specialization? That is, agents with similar responsibilities have similar role embedding representations, while agents with different responsibilities have role embedding representations far from each other. (Sec. 5.1, 5.3.) (3) Can such sub-task specialization improve the performance of multi-agent reinforcement learning algorithms? (Sec. 5.2.) (4) How do roles evolve during training, and how do they influence team performance? (Sec. 5.4.) (5) Can the dissimilarity model dφ learn to measure the dissimilarity</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Ablation studies regarding the two role-learning losses.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Comparison of our method against ablations.</p>
<p>between agents’ trajectories? (Sec. 5.4.) Videos<sup>2</sup> of our experiments and the code<sup>3</sup> are available online.</p>
<p><strong>Baselines</strong> We compare our methods with various baselines shown in Table 1. In particular, we carry out the following ablation studies: (i) We separately omit each (or both) of the two role-learning objectives ($L_I$ and $L_D$) while leaving the other parts of ROMA unchanged. These three ablations are designed to highlight the contribution of each of the proposed regularizers. (ii) QMIX-NPS. The same as QMIX (Rashid et al., 2018), but agents do not share parameters. Our method achieves adaptive learning sharing, and comparison against QMIX (parameters are shared among agents) and QMIX-NPS tests whether this flexibility can improve learning efficiency. (iii) QMIX-LAR, QMIX with a similar number of parameters with our framework, which can test whether the superiority of our method comes from the increase in the number of parameters.</p>
<p>We carry out a grid search over the loss coefficients $\lambda_I$ and $\lambda_D$, and fix them at $10^{-4}$ and $10^{-2}$, respectively, across all the experiments. The dimensionality of latent role space is set to 3, so we did not use any dimensionality reduction techniques when visualizing the role embedding representations. Other hyperparameters are also fixed in our experiments, which are listed in Appendix B.1. For ROMA, We use elementary network structures (fully-connected networks or GRU) for the role encoder, role decoder, and trajectory encoder. The details of the architecture of our method and baselines can be found in Appendix B.</p>
<p>Table 1. Baseline algorithms.</p>
<table>
<thead>
<tr>
<th></th>
<th>Alg.</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Learned roles for 6s4z_vs_10b30z, 27m_vs_30m, and 6z4b (means of the role distributions, $\boldsymbol{\mu}<em i="i">{\rho</em>$, are shown, without using any dimensionality reduction techniques), and the related, automatically discovered responsibilities.
specialized policies. In the middle of the battle, one important tactic is to protect the injured ranged units. Our method learns this maneuver and roles cluster according to the remaining health points $(t=8,19,27)$. Healthiest agents have role representations far from those of other agents. Such representations result in differentiated strategies: healthiest agents move forward to take on more firepower while other agents move backward, firing from a distance. In the meantime, some roles also cluster according to positions (agents 3 and 8 when $t=19$ ). The corresponding behaviors are agents with different roles fire alternatively to share the firepower. We can also observe that the role representations of dead agents aggregate together, representing a special group with an increasing number of agents during the battle.}</p>
<p>These results demonstrate that our method learns dynamic roles and roles cluster clearly corresponding to automatically detected sub-tasks, in line with implicit constraints of the proposed optimization objectives.</p>
<h3>5.2. Performance on StarCraft II</h3>
<p>To test whether these roles and the corresponding sub-task specialization can improve learning efficiency, we test our method on the StarCraft II micromanagement (SMAC) benchmark (Samvelyan et al., 2019). This benchmark consists of various maps which have been classified as easy, hard, and super hard. We compare ROMA with algorithms shown in Table 1 and present results for one easy map (2s3z), three hard maps (5m_vs_6m, 8m_vs_9m \&amp; 10m_vs_11m), and two super hard maps (MMM2 \&amp; 27m_vs_30m). Although SMAC benchmark is challenging, it is not specially designed to test performance in tasks with
many agents. We thus introduce three new SMAC maps to test the scalability of our method, which are described in detail in Appendix C.</p>
<p>For evaluation, all experiments in this section are carried out with 5 different random seeds, and results are shown with a $95 \%$ confidence interval. Among these maps, four maps, MMM2, 6s4z_vs_10b30z, 6z4b, and 10z5b_vs_2z3s, feature heterogeneous agents, and the others have homogeneous agents. Fig. 4 shows that our method yields substantially better results than all the alternative approaches on both homogeneous and heterogeneous maps (additional plots can be found in Appendix C.1). MAVEN overcomes the negative effects of QMIX's monotonicity constraint on exploration. However, it performs less satisfactorily than QMIX on most maps. We believe this is because agents start engaging in the battle immediately after spawning in SMAC maps, and exploration is not the critical factor affecting performance.</p>
<p>Ablations We carry out ablation studies, comparing with the ablations shown in Table 1 and present results on three maps: MMM2 (heterogeneous), 10z5b_vs_2s3z, and 10m_vs_11m (homogeneous) in Fig. 5 and 6. The superiority of our method against $\mathcal{L}<em D="D" T="T">{T D}$ highlights the contribution of the proposed regularizers $-\mathcal{L}</em>}$ performs even worse than QMIX on two of the three maps. By comparing ROMA with $\mathcal{L<em I="I">{T D}+\mathcal{L}</em>}$ and $\mathcal{L<em D="D">{T D}+\mathcal{L}</em>}$, we can conclude that the specialization loss $\mathcal{L<em I="I">{D}$ is more important in terms of performance improvements. Introducing $\mathcal{L}</em>$ alone can only slightly improve the performance. These observations support the claim that sub-task specialization can improve labor efficiency.}$ can make training more stable (for example, on the map 10m_vs_11m), but optimizing $\mathcal{L}_{I</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Role emergence and evolution on the map MMM2 (role representations at time step 1 are shown) during training (means of the role distributions, $\mu_{\mathcal{P}_{1}}$, are shown, without using any dimensionality reduction techniques). The emergence and specialization of roles is closely connected to the improvement of team performance. Agents in MMM2 are heterogeneous, and we show role evolution process in a homogeneous team in Appendix C.3.</p>
<p>Comparison between QMIX-NPS and QMIX demonstrates that parameter sharing can, as documented (Foerster et al., 2018; Rashid et al., 2018), speed up training. As discussed in the introduction, both these two paradigms may not get the best possible performance. In contrast, our method provides a dynamic learning sharing mechanism - agents committed to a certain responsibility have similar policies. The comparison of the performance of ROMA, QMIX, and QMIX-NPS proves that such sub-task specialization can indeed improve team performance. What's more, comparison of ROMA against QMIX-LAR proves that the superiority of our method does not depend on the larger number of parameters.</p>
<p>The performance gap between ROMA and ablations is more significant on maps with more than ten agents. This observation supports discussions in previous sections - the emergence of role is more likely to improve the labor efficiency in larger populations.</p>
<h3>5.3. Role Embedding Representations</h3>
<p>To explain the superiority of ROMA, we present the learned role embedding representations for three maps in Fig. 7. Roles are representative of automatically discovered subtasks in the learned winning strategy. In the map of 6s4z_vs_10b30z, ROMA learns to sacrifice Zealots 9 and 7 to kill all the enemy Banelings. Specifically, Zealots 9 and 7 will move to the frontier one by one to minimize the splash damage, while other agents will stay away and wait until all Banelings explode. Fig. 7(a) shows the role embedding representations while performing the first sub-task where
agent 9 is sacrificed. We can see that the role of Zealot 9 is quite different from those of other agents. Correspondingly, the strategy at this time is agent 9 moving rightward while other agents keep still. Detailed analysis for the other two maps can be found in Appendix C.2.</p>
<h3>5.4. Emergence and Evolution of Roles</h3>
<p>We have shown the learned role representations and performance of our method, but the relationship between roles and performance remains unclear. To make up for this shortcoming, we visualize the emergence and evolution of roles during the training process on the map MMM2 (heterogeneous) and 10m_vs_11m (homogeneous). We discuss the results on MMM2 here and defer analysis of 10m_vs_11m to Appendix C.3.</p>
<p>In MMM2, 1 Medivac, 2 Marauders, and 7 Marines are faced with a stronger enemy team consisting of 1 Medivac, 3 Marauders, and 8 Marines. Among the three involved unit types, Medivac is the most special one for that it can heal the injured units. In Fig. 8, we show one of the learning curves of ROMA (red) and the role representations at the first environment step at three different stages. When the training begins $(T=0)$, roles are random, and the agents are exploring the environment to learn the basic dynamics and the structure of the task. By $T=6 \mathrm{M}$, ROMA has learned that the responsibilities of the Medivac are different from those of Marines and Marauders. The role, and correspondingly, the policy of the Medivac becomes quite different (Fig. 8 middle). Such differentiation in behaviors enables agents to start winning the game. Gradually, ROMA learns that</p>
<p>Table 2. The mean and standard deviation of the learned dissimilarities $d_{\phi}$ between agents’ trajectories on the map MMM2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Between different unit types</th>
<th style="text-align: left;">$0.9556 \pm 0.0009$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Between the same unit type</td>
<td style="text-align: left;">$0.0780 \pm 0.0019$</td>
</tr>
</tbody>
</table>
<p>Marines and Marauders have dissimilar characteristics and should take different sub-tasks, indicated by the differentiation of their role representations (Fig. 8 right). This further specialization facilitates the performance increase between 6 M and 10 M . After $T=10 \mathrm{M}$, the responsibilities of roles are clear, and, as a result, the win rate gradually converges (Fig. 4 top left). For comparison, ROMA without $\mathcal{L}<em D="D">{I}$ and $\mathcal{L}</em>$ in Fig. 6-left). These results demonstrate that the gradually specialized roles are indispensable in team performance improvement.}$ can not even win once on this challenging task ( $\mathcal{L}_{T D</p>
<p>Moreover, we find that the learned dissimilarity model $d_{\phi}$ introduced in Sec. 3.2 provides an empirical evaluation for identifying new roles. We use the map MMM2 as an example, where, as we discussed above, the learned roles of agents are characterized by their unit types. After scaling to $[0,1]$, the learned dissimilarity between trajectories of agents with different unit types is close to 0.96 , while the learned dissimilarity between trajectories of agents with the same unit type is around 0.08 . These results indicate that an appropriate threshold can be used to decide when an individual behavior (trajectory) can be assigned the terminology role.</p>
<p>In summary, our experiments demonstrate that ROMA can learn dynamic, identifiable, versatile, and specialized roles that effectively decompose the task. Drawing support from these emergent roles, our method significantly pushes forward the state of the art of multi-agent reinforcement learning algorithms.</p>
<h2>6. Closing Remarks</h2>
<p>We have introduced the concept of roles into deep multiagent reinforcement learning by capturing the emergent roles and encouraging them to specialize on a set of automatically detected sub-tasks. Such deep role-oriented multi-agent learning framework provides another perspective to explain and promote cooperation within agent teams, and implicitly draws connection to the division of labor, which has been practiced in many natural systems for long.</p>
<p>To our best knowledge, this paper is making a first attempt at learning roles via deep reinforcement learning. The gargantuan task of understanding the emergence of roles, the division of labor, and interactions between more complex roles in hierarchical organization still lies ahead. We believe that these topics are basic and indispensable in building effective, flexible, and general-purpose multi-agent systems
and this paper can help tackle these challenges.</p>
<h2>Acknowledgements</h2>
<p>We gratefully acknowledge Jin Zhang for his valuable discussions. We also would like to thank the reviewers for their detailed and constructive feedback. This work is partially supported by the sponsorship of Guoqiang Institute at Tsinghua University.</p>
<h2>References</h2>
<p>Alemi, A. A., Fischer, I., Dillon, J. V., and Murphy, K. Deep variational information bottleneck. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.</p>
<p>Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., and Mordatch, I. Emergent tool use from multi-agent autocurricula. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.</p>
<p>Bargiacchi, E., Verstraeten, T., Roijers, D., Nowé, A., and Hasselt, H. Learning to coordinate with coordination graphs in repeated single-stage multi-agent decision problems. In International Conference on Machine Learning, pp. 491-499, 2018.</p>
<p>Becht, M., Gurzki, T., Klarmann, J., and Muscholl, M. Rope: Role oriented programming environment for multiagent systems. In Proceedings Fourth IFCIS International Conference on Cooperative Information Systems. CoopIS 99 (Cat. No. PR00384), pp. 325-333. IEEE, 1999.</p>
<p>Bonjean, N., Mefteh, W., Gleizes, M., Maurel, C., and Migeon, F. Adelfe 2.0: Handbook on agent-oriented design processes, m. cossentino, v. hilaire, a. molesini, and v. seidita, 2014.</p>
<p>Bresciani, P., Perini, A., Giorgini, P., Giunchiglia, F., and Mylopoulos, J. Tropos: An agent-oriented software development methodology. Autonomous Agents and MultiAgent Systems, 8(3):203-236, 2004.</p>
<p>Butler, E. The condensed wealth of nations. Centre for Independent Studies, 2012.</p>
<p>Cao, Y., Yu, W., Ren, W., and Chen, G. An overview of recent progress in the study of distributed multi-agent coordination. IEEE Transactions on Industrial informatics, $9(1): 427-438,2012$.</p>
<p>Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014</p>
<p>Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1724-1734, 2014.</p>
<p>Cossentino, M., Gaglio, S., Sabatucci, L., and Seidita, V. The passi and agile passi mas meta-models compared with a unifying proposal. In International Central and Eastern European Conference on Multi-Agent Systems, pp. 183-192. Springer, 2005.</p>
<p>Das, A., Kottur, S., Moura, J. M., Lee, S., and Batra, D. Learning cooperative visual dialog agents with deep reinforcement learning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2951-2960, 2017.</p>
<p>Das, A., Gervet, T., Romoff, J., Batra, D., Parikh, D., Rabbat, M., and Pineau, J. Tarmac: Targeted multi-agent communication. In International Conference on Machine Learning, pp. 1538-1546, 2019.</p>
<p>DeLoach, S. A. and Garcia-Ojeda, J. C. O-mase: a customisable approach to designing and building complex, adaptive multi-agent systems. International Journal of AgentOriented Software Engineering, 4(3):244-280, 2010.</p>
<p>Depke, R., Heckel, R., and Küster, J. M. Roles in agentoriented modeling. International Journal of Software engineering and Knowledge engineering, 11(03):281302, 2001.</p>
<p>Ferber, J., Gutknecht, O., and Michel, F. From agents to organizations: an organizational view of multi-agent systems. In International workshop on agent-oriented software engineering, pp. 214-230. Springer, 2003.</p>
<p>Foerster, J., Assael, I. A., de Freitas, N., and Whiteson, S. Learning to communicate with deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2137-2145, 2016.</p>
<p>Foerster, J., Nardelli, N., Farquhar, G., Afouras, T., Torr, P. H., Kohli, P., and Whiteson, S. Stabilising experience replay for deep multi-agent reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1146-1155. JMLR. org, 2017.</p>
<p>Foerster, J. N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. Counterfactual multi-agent policy gradients. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</p>
<p>Gordon, D. M. The organization of work in social insect colonies. Nature, 380(6570):121-124, 1996.</p>
<p>Grover, A., Al-Shedivat, M., Gupta, J. K., Burda, Y., and Edwards, H. Evaluating generalization in multiagent systems using agent-interaction graphs. In Proceedings of
the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1944-1946. International Foundation for Autonomous Agents and Multiagent Systems, 2018.</p>
<p>Guestrin, C., Koller, D., and Parr, R. Multiagent planning with factored mdps. In Advances in neural information processing systems, pp. 1523-1530, 2002.</p>
<p>Hoshen, Y. Vain: Attentional multi-agent predictive modeling. In Advances in Neural Information Processing Systems, pp. 2701-2711, 2017.</p>
<p>Iqbal, S. and Sha, F. Actor-attention-critic for multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 2961-2970, 2019.</p>
<p>Jeanson, R., Kukuk, P. F., and Fewell, J. H. Emergence of division of labour in halictine bees: contributions of social interactions and behavioural variance. Animal behaviour, 70(5):1183-1193, 2005.</p>
<p>Jiang, J. and Lu, Z. Learning attentional communication for multi-agent cooperation. In Advances in Neural Information Processing Systems, pp. 7254-7264, 2018.</p>
<p>Jiang, J. and Lu, Z. Learning fairness in multi-agent systems. In Advances in Neural Information Processing Systems, pp. 13854-13865, 2019.</p>
<p>Kang, Y., Wang, T., and de Melo, G. Incorporating pragmatic reasoning communication into emergent language. arXiv preprint arXiv:2006.04109, 2020.</p>
<p>Kim, D., Moon, S., Hostallero, D., Kang, W. J., Lee, T., Son, K., and Yi, Y. Learning to schedule communication in multi-agent reinforcement learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.</p>
<p>Kipf, T., Fetaya, E., Wang, K.-C., Welling, M., and Zemel, R. Neural relational inference for interacting systems. In International Conference on Machine Learning, pp. 2688-2697, 2018.</p>
<p>Lazaridou, A., Peysakhovich, A., and Baroni, M. Multiagent cooperation and the emergence of (natural) language. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.</p>
<p>Lhaksmana, K. M., Murakami, Y., and Ishida, T. Role-based modeling for designing agent behavior in self-organizing multi-agent systems. International Journal of Software Engineering and Knowledge Engineering, 28(01):79-96, 2018.</p>
<p>Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O. P., and Mordatch, I. Multi-agent actor-critic for mixed</p>
<p>cooperative-competitive environments. In Advances in Neural Information Processing Systems, pp. 6379-6390, 2017.</p>
<p>Mahajan, A., Rashid, T., Samvelyan, M., and Whiteson, S. Maven: Multi-agent variational exploration. In Advances in Neural Information Processing Systems, pp. $7611-7622,2019$.</p>
<p>Mordatch, I. and Abbeel, P. Emergence of grounded compositional language in multi-agent populations. In ThirtySecond AAAI Conference on Artificial Intelligence, 2018.</p>
<p>Nguyen, D. T., Kumar, A., and Lau, H. C. Credit assignment for collective multiagent rl with global rewards. In Advances in Neural Information Processing Systems, pp. 8102-8113, 2018.</p>
<p>Nowé, A., Vrancx, P., and De Hauwere, Y.-M. Game theory and multi-agent reinforcement learning. In Reinforcement Learning, pp. 441-470. Springer, 2012.</p>
<p>Odell, J., Nodine, M., and Levy, R. A metamodel for agents, roles, and groups. In International Workshop on AgentOriented Software Engineering, pp. 78-92. Springer, 2004.</p>
<p>Oliehoek, F. A., Amato, C., et al. A concise introduction to decentralized POMDPs, volume 1. Springer, 2016.</p>
<p>Omicini, A. Soda: Societies and infrastructures in the analysis and design of agent-based systems. In International Workshop on Agent-Oriented Software Engineering, pp. 185-193. Springer, 2000.</p>
<p>Padgham, L. and Winikoff, M. Prometheus: A methodology for developing intelligent agents. In International Workshop on Agent-Oriented Software Engineering, pp. 174-185. Springer, 2002.</p>
<p>Pavón, J. and Gómez-Sanz, J. Agent oriented software engineering with ingenias. In International Central and Eastern European Conference on Multi-Agent Systems, pp. 394-403. Springer, 2003.</p>
<p>Peng, P., Wen, Y., Yang, Y., Yuan, Q., Tang, Z., Long, H., and Wang, J. Multiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning to play starcraft combat games. arXiv preprint arXiv:1703.10069, 2017.</p>
<p>Rashid, T., Samvelyan, M., Witt, C. S., Farquhar, G., Foerster, J., and Whiteson, S. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4292-4301, 2018.</p>
<p>Samvelyan, M., Rashid, T., de Witt, C. S., Farquhar, G., Nardelli, N., Rudner, T. G., Hung, C.-M., Torr, P. H., Foerster, J., and Whiteson, S. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.</p>
<p>Singh, A., Jain, T., and Sukhbaatar, S. Learning when to communicate at scale in multiagent cooperative and competitive tasks. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.</p>
<p>Smith, A. The wealth of nations [1776], 1937.
Son, K., Kim, D., Kang, W. J., Hostallero, D. E., and Yi, Y. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 5887-5896, 2019.</p>
<p>Spanoudakis, N. and Moraitis, P. Using aseme methodology for model-driven agent systems development. In International Workshop on Agent-Oriented Software Engineering, pp. 106-127. Springer, 2010.</p>
<p>Stone, P. and Veloso, M. Task decomposition, dynamic role assignment, and low-bandwidth communication for real-time strategic teamwork. Artificial Intelligence, 110 (2):241-273, 1999.</p>
<p>Sukhbaatar, S., Fergus, R., et al. Learning multiagent communication with backpropagation. In Advances in Neural Information Processing Systems, pp. 2244-2252, 2016.</p>
<p>Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo, J. Z., Tuyls, K., et al. Value-decomposition networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 20852087. International Foundation for Autonomous Agents and Multiagent Systems, 2018.</p>
<p>Tan, M. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the tenth international conference on machine learning, pp. 330-337, 1993.</p>
<p>Usunier, N., Synnaeve, G., Lin, Z., and Chintala, S. Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.</p>
<p>Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., Makhzani, A., Küttler, H., Agapiou, J., Schrittwieser, J., et al. Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017.</p>
<p>Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575 (7782):350-354, 2019.</p>
<p>Wainwright, M. J., Jordan, M. I., et al. Graphical models, exponential families, and variational inference. Foundations and Trends ${ }^{\circledR}$ in Machine Learning, 1(1-2):1-305, 2008.</p>
<p>Wang, T., Wang, J., Yi, W., and Zhang, C. Influence-based multi-agent exploration. In Proceedings of the International Conference on Learning Representations (ICLR), 2020a.</p>
<p>Wang, T., Wang, J., Zheng, C., and Zhang, C. Learning nearly decomposable value functions with communication minimization. In Proceedings of the International Conference on Learning Representations (ICLR), 2020b.</p>
<p>Wen, Y., Yang, Y., Luo, R., Wang, J., and Pan, W. Probabilistic recursive reasoning for multi-agent reinforcement learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.</p>
<p>Wooldridge, M., Jennings, N. R., and Kinny, D. The gaia methodology for agent-oriented analysis and design. $A u$ tonomous Agents and multi-agent systems, 3(3):285-312, 2000.</p>
<p>Yang, Z., Zhao, J., Dhingra, B., He, K., Cohen, W. W., Salakhutdinov, R. R., and LeCun, Y. Glomo: unsupervised learning of transferable relational graphs. In Advances in Neural Information Processing Systems, pp. 8950-8961, 2018.</p>
<p>Zhang, C. and Lesser, V. Coordinated multi-agent reinforcement learning in networked distributed pomdps. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.</p>
<p>Zhu, H. and Zhou, M. Role-based multi-agent systems. In Personalized Information Retrieval and Access: Concepts, Methods and Practices, pp. 254-285. Igi Global, 2008.</p>
<h1>A. Mathematical Derivation</h1>
<h2>A.1. Identifiable Roles</h2>
<p>For learning identifiable roles, we propose to maximize the conditional mutual information objective between roles and local observation-action histories given the current observations. In Sec. 3.1 of the paper, we introduce a posterior estimator and derive a tractable lower bound of the mutual information term:</p>
<p>$$
\begin{aligned}
&amp; I\left(\rho_{i}^{t} ; \tau_{i}^{t-1} \mid o_{i}^{t}\right)=\mathbb{E}<em i="i">{\rho</em>\right] \
&amp; =\mathbb{E}}^{t}, \tau_{i}^{t-1}, o_{i}^{t}}\left[\log \frac{p\left(\rho_{i}^{t} \mid \tau_{i}^{t-1}, o_{i}^{t}\right)}{p\left(\rho_{i}^{t} \mid o_{i}^{t}\right)<em i="i">{\rho</em>\right] \
&amp; +\mathbb{E}}^{t}, \tau_{i}^{t-1}, o_{i}^{t}}\left[\log \frac{q_{\xi}\left(\rho_{i}^{t} \mid \tau_{i}^{t-1}, o_{i}^{t}\right)}{p\left(\rho_{i}^{t} \mid o_{i}^{t}\right)<em i="i">{\tau</em>\right)\right)\right] \
&amp; \geq \mathbb{E}}^{t-1}, o_{i}^{t}}\left[D_{\mathrm{KL}}\left(p\left(\rho_{i}^{t} \mid \tau_{i}^{t-1}, o_{i}^{t}\right) | q_{\xi}\left(\rho_{i}^{t} \mid \tau_{i}^{t-1}, o_{i}^{t<em i="i">{\rho</em>\right]
\end{aligned}
$$}^{t}, \tau_{i}^{t-1}, o_{i}^{t}}\left[\log \frac{q_{\xi}\left(\rho_{i}^{t} \mid \tau_{i}^{t-1}, o_{i}^{t}\right)}{p\left(\rho_{i}^{t} \mid o_{i}^{t}\right)</p>
<p>where the last inequality holds because of the non-negativity of the KL divergence. Then it follows that:</p>
<p>$$
\begin{aligned}
&amp; \mathbb{E}<em i="i">{\rho</em>\right] \
= &amp; \mathbb{E}}^{t}, \tau_{i}^{t-1}, o_{i}^{t}}\left[\log \frac{q_{\xi}\left(\rho_{i}^{t} \mid \tau_{i}^{t-1}, o_{i}^{t}\right)}{p\left(\rho_{i}^{t} \mid o_{i}^{t}\right)<em i="i">{\rho</em>}^{t}, \tau_{i}^{t-1}, o_{i}^{t}}\left[\log q_{\xi}\left(\rho_{i}^{t} \mid \tau_{i}^{t-1}, o_{i}^{t}\right)\right]-\mathbb{E<em i="i">{\rho</em>\right)\right] \
= &amp; \mathbb{E}}^{t}, o_{i}^{t}}\left[\log p\left(\rho_{i}^{t} \mid o_{i}^{t<em i="i">{\rho</em>}^{t}, \tau_{i}^{t-1}, o_{i}^{t}}\left[\log q_{\xi}\left(\rho_{i}^{t} \mid \tau_{i}^{t-1}, o_{i}^{t}\right)\right]+\mathbb{E<em i="i">{o</em>\right)\right] \
= &amp; \mathbb{E}}^{t}}\left[H\left(\rho_{i}^{t} \mid o_{i}^{t<em i="i">{\tau</em>}^{t-1}, o_{i}^{t}}\left[\int p\left(\rho_{i}^{t} \mid \tau_{i}^{t-1}, o_{i}^{t}\right) \log q_{\xi}\left(\rho_{i}^{t} \mid \tau_{i}^{t-1}, o_{i}^{t}\right) d \rho_{i}^{t}\right]+\mathbb{E<em i="i">{o</em>\right)\right]
\end{aligned}
$$}^{t}}\left[H\left(\rho_{i}^{t} \mid o_{i}^{t</p>
<p>The role encoder is conditioned on the local observations, so given the observations, the distributions of roles, $p\left(\rho_{i}^{t}\right)$, are independent from the local histories. Thus, we have</p>
<p>$$
I\left(\rho_{i}^{t} ; \tau_{i}^{t-1} \mid o_{i}^{t}\right) \geq-\mathbb{E}<em i="i">{\tau</em>}^{t-1}, o_{i}^{t}}\left[\mathcal{C} \mathcal{E}\left[p\left(\rho_{i}^{t} \mid o_{i}^{t}\right) | q_{\xi}\left(\rho_{i}^{t} \mid \tau_{i}^{t-1}, o_{i}^{t}\right)\right]+\mathbb{E<em i="i">{o</em>\right)\right]\right.
$$}^{t}}\left[H\left(\rho_{i}^{t} \mid o_{i}^{t</p>
<p>In practice, we use a replay buffer $\mathcal{D}$ and minimize</p>
<p>$$
\mathcal{L}<em _rho="\rho">{I}\left(\theta</em>}, \xi\right)=\mathbb{E<em i="i">{\left(\tau</em>\right)\right]
$$}^{t-1}, o_{i}^{t}\right) \sim \mathcal{D}}\left[\mathcal{C} \mathcal{E}\left[p\left(\rho_{i}^{t} \mid o_{i}^{t}\right)\right| q_{\xi}\left(\rho_{i}^{t} \mid \tau_{i}^{t-1}, o_{i}^{t}\right)-H\left(\rho_{i}^{t} \mid o_{i}^{t</p>
<h2>A.2. Specialized Roles</h2>
<p>Conditioning roles on local observations enables roles to be dynamic, and optimizing $\mathcal{L}<em i="i">{I}$ enables roles to be identifiable by agents' long-term behaviors, but these formulations do not explicitly encourage specialized roles. To make up for this shortcoming, we propose a role differentiation objective in Sec. 3.2 of the paper, where a mutual information maximization objective is involved (maximizing $I\left(\rho</em>\right)$ ). Here, we derive a variational lower bound of this mutual information objective to render it feasible to be optimized.}^{t} ; \tau_{j}^{t-1} \mid o_{j}^{t</p>
<p>$$
\begin{aligned}
I\left(\rho_{i}^{t} ; \tau_{j}^{t-1} \mid o_{j}^{t}\right) &amp; =\mathbb{E}<em i="i">{\rho</em>\right] \
&amp; =\mathbb{E}}^{t}, \tau_{j}^{t-1}, o_{j}^{t}}\left[\log \frac{p\left(\rho_{i}^{t}, \tau_{j}^{t-1} \mid o_{j}^{t}\right)}{p\left(\rho_{i}^{t} \mid o_{j}^{t}\right) p\left(\tau_{j}^{t-1} \mid o_{j}^{t}\right)<em i="i">{\rho</em>\right] \
&amp; =\mathbb{E}}^{t}, \tau_{i}^{t-1}, o_{j}^{t}}\left[\log \frac{p\left(\rho_{i}^{t} \mid \tau_{j}^{t-1}, o_{j}^{t}\right)}{p\left(\rho_{i}^{t} \mid o_{j}^{t}\right)<em i="i">{\rho</em>}^{t}, \tau_{j}^{t-1}, o_{j}^{t}}\left[\log p\left(\rho_{i}^{t} \mid \tau_{j}^{t-1}, o_{j}^{t}\right)\right]+\mathbb{E<em j="j">{o</em>\right)\right] \
&amp; \geq \mathbb{E}}^{t}}\left[H\left(\rho_{i}^{t} \mid o_{j}^{t<em i="i">{\rho</em>\right)\right]
\end{aligned}
$$}^{t}, \tau_{j}^{t-1}, o_{j}^{t}}\left[\log p\left(\rho_{i}^{t} \mid \tau_{j}^{t-1}, o_{j}^{t</p>
<p>We clip the variances of role distributions at a small value ( 0.1 ) to ensure that the entropy of role distributions are always</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Additional results on the SMAC benchmark.
non-negative so that the last inequality holds. Then, it follows that:</p>
<p>$$
\begin{aligned}
&amp; \mathbb{E}<em i="i">{\rho</em>\right)\right] \
= &amp; \mathbb{E}}^{t}, \tau_{j}^{t-1}, o_{j}^{t}}\left[\log p\left(\rho_{i}^{t} \mid \tau_{j}^{t-1}, o_{j}^{t<em i="i">{\rho</em>}^{t}, \tau_{j}^{t-1}, o_{j}^{t}}\left[\log q_{\xi}\left(\rho_{i}^{t} \mid \tau_{j}^{t-1}, o_{j}^{t}\right)\right]+\mathbb{E<em j="j">{\tau</em>\right)\right]\right] \
\geq &amp; \mathbb{E}}^{t-1}, o_{j}^{t}}\left[D_{\mathrm{KL}}\left[p\left(\rho_{i}^{t} \mid \tau_{j}^{t-1}, o_{j}^{t}\right) | q_{\xi}\left(\rho_{i}^{t} \mid \tau_{j}^{t-1}, o_{j}^{t<em i="i">{\rho</em>\right)\right]
\end{aligned}
$$}^{t}, \tau_{j}^{t-1}, o_{j}^{t}}\left[\log q_{\xi}\left(\rho_{i}^{t} \mid \tau_{j}^{t-1}, o_{j}^{t</p>
<p>where $q_{\xi}$ is the trajectory encoder introduced in Sec. A.1, and the KL divergence term can be left out when deriving the lower bound because it is non-negative. Therefore, we have:</p>
<p>$$
I\left(\rho_{i}^{t} ; \tau_{j}^{t-1} \mid o_{j}^{t}\right) \geq \mathbb{E}<em i="i">{\rho</em>\right)\right]
$$}^{t}, \tau_{j}^{t-1}, o_{j}^{t}}\left[\log q_{\xi}\left(\rho_{i}^{t} \mid \tau_{j}^{t-1}, o_{j}^{t</p>
<p>Recall that, in order to learn specialized roles, we propose to minimize:</p>
<p>$$
\left|D_{\phi}^{t}\right|<em _neq="\neq" i="i" j="j">{F}-\sum</em>\right), U\right}
$$} \min \left{I\left(\rho_{i}^{t} ; \tau_{j}^{t-1} \mid o_{j}^{t}\right)+d_{\phi}\left(\tau_{i}^{t-1}, \tau_{j}^{t-1</p>
<p>where $D_{\phi}^{t}=\left(d_{i j}^{t}\right)$, and $d_{i j}^{t}=d_{\phi}\left(\tau_{i}^{t-1}, \tau_{j}^{t-1}\right)$ is the estimated dissimilarity between trajectories of agent $i$ and $j$. For the term $\min \left{I\left(\rho_{i}^{t} ; \tau_{j}^{t-1} \mid o_{j}^{t}\right)+d_{\phi}\left(\tau_{i}^{t-1}, \tau_{j}^{t-1}\right), U\right}$, we have:</p>
<p>$$
\begin{aligned}
&amp; \min \left{I\left(\rho_{i}^{t} ; \tau_{j}^{t-1} \mid o_{j}^{t}\right)+d_{\phi}\left(\tau_{i}^{t-1}, \tau_{j}^{t-1}\right), U\right} \
= &amp; \min \left{\mathbb{E}<em i="i">{\boldsymbol{\tau}^{t-1}, \boldsymbol{o}^{t}, \boldsymbol{\rho}^{t}}\left[\log \frac{p\left(\rho</em>[U]\right}
\end{aligned}
$$}^{t}, \tau_{j}^{t-1} \mid o_{j}^{t}\right)}{p\left(\rho_{i}^{t} \mid o_{j}^{t}\right) p\left(\tau_{j}^{t-1} \mid o_{j}^{t}\right)}+d_{\phi}\left(\tau_{i}^{t-1}, \tau_{j}^{t-1}\right)\right], \mathbb{E}_{\boldsymbol{\tau}^{t-1}, \boldsymbol{o}^{t}, \boldsymbol{\rho}^{t}</p>
<p>where $\boldsymbol{\tau}^{t-1}$ is the joint trajectory, $\boldsymbol{o}^{t}$ is the joint observation, and $\boldsymbol{\rho}^{t}=\left\langle\rho_{1}^{t}, \rho_{2}^{t}, \cdots, \rho_{n}^{t}\right\rangle$. We denote</p>
<p>$$
\begin{aligned}
T_{1} &amp; \equiv \log \frac{p\left(\rho_{i}^{t}, \tau_{j}^{t-1} \mid o_{j}^{t}\right)}{p\left(\rho_{i}^{t} \mid o_{j}^{t}\right) p\left(\tau_{j}^{t-1} \mid o_{j}^{t}\right)} \
T_{2} &amp; \equiv \log q_{\xi}\left(\rho_{i}^{t} \mid \tau_{j}^{t-1}, o_{j}^{t}\right)
\end{aligned}
$$</p>
<p>Because</p>
<p>$$
\begin{aligned}
T_{2} &amp; \geq \min \left{T_{2}, U\right} \
U &amp; \geq \min \left{T_{2}, U\right}
\end{aligned}
$$</p>
<p>it follows that:</p>
<p>$$
\begin{aligned}
\mathbb{E}<em 2="2">{\boldsymbol{\tau}^{t-1}, \boldsymbol{o}^{t}, \boldsymbol{\rho}^{t}}\left[T</em>}\right] &amp; \geq \mathbb{E<em 2="2">{\boldsymbol{\tau}^{t-1}, \boldsymbol{o}^{t}, \boldsymbol{\rho}^{t}}\left[\min \left{T</em>, U\right}\right] \
\mathbb{E}<em _boldsymbol_tau="\boldsymbol{\tau">{\boldsymbol{\tau}^{t-1}, \boldsymbol{o}^{t}, \boldsymbol{\rho}^{t}}[U] &amp; \geq \mathbb{E}</em>, U\right}\right]
\end{aligned}
$$}^{t-1}, \boldsymbol{o}^{t}, \boldsymbol{\rho}^{t}}\left[\min \left{T_{2</p>
<p>So that</p>
<p>$$
\begin{aligned}
&amp; \min \left{\mathbb{E}<em 1="1">{\boldsymbol{\tau}^{t-1}, \boldsymbol{o}^{t}, \boldsymbol{\rho}^{t}}\left[T</em>}\right], \mathbb{E<em _boldsymbol_tau="\boldsymbol{\tau">{\boldsymbol{\tau}^{t-1}, \boldsymbol{o}^{t}, \boldsymbol{\rho}^{t}}[U]\right} \
\geq &amp; \min \left{\mathbb{E}</em>}^{t-1}, \boldsymbol{o}^{t}, \boldsymbol{\rho}^{t}}\left[T_{2}\right], \mathbb{E<em _boldsymbol_tau="\boldsymbol{\tau">{\boldsymbol{\tau}^{t-1}, \boldsymbol{o}^{t}, \boldsymbol{\rho}^{t}}[U]\right} \quad{\text { Eq. } 14} \
\geq &amp; \mathbb{E}</em> 19}
\end{aligned}
$$}^{t-1}, \boldsymbol{o}^{t}, \boldsymbol{\rho}^{t}}\left[\min \left{T_{2}, U\right}\right] \quad{\text { Eq. </p>
<p>which means that Eq. 15 satisfies:</p>
<p>$$
\left|D_{\phi}^{t}\right|<em _neq="\neq" i="i" j="j">{F}-\sum</em>\right), U\right}
$$} \min \left{I\left(\rho_{i}^{t} ; \tau_{j}^{t-1} \mid o_{j}^{t}\right)+d_{\phi}\left(\tau_{i}^{t-1}, \tau_{j}^{t-1</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. (Reproduced from Fig. 6 in the paper, for quick reference.) Learned roles for 6s4z_vs_10b30z, 27m_vs_30m, and 6z4b (means of the role distributions, $\boldsymbol{\mu}<em i="i">{\rho</em>$, are shown, without using any dimensionality reduction techniques), and the related, automatically discovered responsibilities.}</p>
<p>$$
\begin{aligned}
&amp; =\mathbb{E}<em _phi="\phi">{\boldsymbol{\tau}^{t-1}, \boldsymbol{\sigma}^{t}, \boldsymbol{\rho}^{t}}\left[\left|D</em>\right|}^{t<em _neq="\neq" i="i" j="j">{F}\right]-\sum</em>} \min \left{\mathbb{E<em 1="1">{\boldsymbol{\tau}^{t-1}, \boldsymbol{\sigma}^{t}, \boldsymbol{\rho}^{t}}\left[T</em>\right)\right], U\right} \
&amp; \leq &amp; \mathbb{E}}+d_{\phi}\left(\tau_{i}^{t-1}, \tau_{j}^{t-1<em _phi="\phi">{\boldsymbol{\tau}^{t-1}, \boldsymbol{\sigma}^{t}, \boldsymbol{\rho}^{t}}\left[\left|D</em>\right|}^{t<em _neq="\neq" i="i" j="j">{F}\right]-\sum</em>} \mathbb{E<em 2="2">{\boldsymbol{\tau}^{t-1}, \boldsymbol{\sigma}^{t}, \boldsymbol{\rho}^{t}}\left[\min \left{T</em> 20} \
= &amp; \mathbb{E}}+d_{\phi}\left(\tau_{i}^{t-1}, \tau_{j}^{t-1}\right), U\right}\right] \quad{\text { Eq. <em _phi="\phi">{\boldsymbol{\tau}^{t-1}, \boldsymbol{\sigma}^{t}, \boldsymbol{\rho}^{t}}\left[\left|D</em>\right|}^{t<em _neq="\neq" i="i" j="j">{F}-\sum</em>\right), U\right}\right]
\end{aligned}
$$} \min \left{T_{2}+d_{\phi}\left(\tau_{i}^{t-1}, \tau_{j}^{t-1</p>
<p>We minimize this upper bound to optimize Eq. 15. In practice, we use a replay buffer, and minimize:</p>
<p>$$
\mathcal{L}<em _rho="\rho">{D}\left(\theta</em>}, \phi, \xi\right)=\mathbb{E<em _phi="\phi">{\left(\boldsymbol{\tau}^{t-1}, \boldsymbol{\sigma}^{t}\right) \sim \mathcal{D}, \boldsymbol{\rho}^{t} \sim p\left(\boldsymbol{\rho}^{t} \mid \boldsymbol{\sigma}^{t}\right)}\left[\left|D</em>\right|}^{t<em _neq="\neq" i="i" j="j">{F}-\sum</em>\right), U\right}\right]
$$} \min \left{q_{\xi}\left(\rho_{i}^{t} \mid \tau_{j}^{t-1}, o_{j}^{t}\right)+d_{\phi}\left(\tau_{i}^{t-1}, \tau_{j}^{t-1</p>
<p>where $\mathcal{D}$ is the replay buffer, $\boldsymbol{\tau}^{t-1}$ is the joint trajectory, $\boldsymbol{\sigma}^{t}$ is the joint observation, and $\boldsymbol{\rho}^{t}=\left\langle\rho_{1}^{t}, \rho_{2}^{t}, \cdots, \rho_{n}^{t}\right\rangle$.</p>
<h1>B. Architecture, Hyperparameters, and Infrastructure</h1>
<h2>B.1. ROMA</h2>
<p>In this paper, we base our algorithm on QMIX (Rashid et al., 2018), whose framework is shown in Fig. 13 and described in Appendix D. In ROMA, each agent has a neural network to approximate its local utility. The local utility network consists of three layers, a fully-connected layer, followed by a 64 bit GRU, and followed by another fully-connected layer that outputs an estimated value for each action. The local utilities are fed into a mixing network estimating the global action value. The mixing network has a 32-dimensional hidden layer with ReLU activation. Parameters of the mixing network are generated by a hyper-net conditioning on the global state. This hyper-net has a fully-connected hidden layer of 32 dimensions. These settings are the same as QMIX.</p>
<p>We use very simple network structures for the components related to role embedding learning, i.e., the role encoder, the role decoder, and the trajectory encoder. The multi-variate Gaussian distributions from which the individual roles are drawn have their means and variances generated by the role encoder, which is a fully-connected network with a 12-dimensional hidden layer with ReLU activation. The parameters in the second fully-connected layers of the local utility approximators are generated by the role decoder whose inputs are the individual roles, which are 3-dimensional in all experiments. The role decoder is also a fully-connected network with a 12-dimensional hidden layer and ReLU activation. For the trajectory</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. The process of role emergence and evolution on the map 10m_vs_11m.
encoder, we again use a fully-connected network with a 12-dimensional hidden layer and ReLU activation. The inputs of the trajectory encoder are the hidden states of the GRUs in the local utility functions after the last time step.
For all experiments, we set $\lambda_{I}=10^{-4}, \lambda_{D}=10^{-2}$, and the discount factor $\gamma=0.99$. The optimization is conducted using RMSprop with a learning rate of $5 \times 10^{-4}, \alpha$ of 0.99 , and with no momentum or weight decay. For exploration, we use $\epsilon$-greedy with $\epsilon$ annealed linearly from 1.0 to 0.05 over $50 k$ time steps and kept constant for the rest of the training. We run 8 parallel environments to collect samples. Batches of 32 episodes are sampled from the replay buffer, and the whole framework is trained end-to-end on fully unrolled episodes. All experiments on StarCraft II use the default reward and observation settings of the SMAC benchmark.</p>
<p>Experiments are carried out on NVIDIA GTX 2080 Ti GPU.</p>
<h1>B.2. Baselines and Ablations</h1>
<p>We compare ROMA with various baselines and ablations, which are listed in Table. 1 of the paper. For COMA (Foerster et al., 2018), QMIX (Rashid et al., 2018), and MAVEN (Mahajan et al., 2019), we use the codes provided by the authors where the hyper-parameters have been fin-tuned on the SMAC benchmark. QMIX-NPS uses the identical architecture as QMIX, and the only difference lies in that QMIX-NPS does not share parameters among agents. Compared to QMIX, for the local utility function of agents, QMIX-LAR adds two more fully-connected layers of 80 and 25 dimensions after the GRU layer so that it approximately has the same number of parameters as ROMA.</p>
<h2>C. Additional Experimental Results</h2>
<p>We benchmark our method on the StarCraft II unit micromanagement tasks. To test the scalability of the proposed approach, we introduce three maps. The 6z4b map features symmetry teams consisting of 4 Banelings and 6 Zerglings. In the map of 6s4z_vs_10b30z, 6 Stalkers and 4 Zealots learn to defeat 10 Banelings and 30 Zerglings. And 10z5b_vs_2z3s characterizes asymmetry teams consisting of 10 Zerglings \&amp; 5 Banelings and 2 Zealots \&amp; 3 Stalkers, respectively.</p>
<h2>C.1. Performance Comparison against Baselines</h2>
<p>Fig. 9 presents performance of ROMA against various baselines on three maps. Performance comparison on the other maps is shown in Fig. 4 of the paper. We can see that the advantage of ROMA is more significant on maps with more agents, such as 10z5b_vs_2z3s, MMM2, 27_vs_30m, and 10m_vs_11m.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13. The framework of QMIX, reproduced from the original paper (Rashid et al., 2018). (a) The architecture of the mixing network (blue), whose weights and biases are generated by a hyper-net (red) conditioned on the global state. (b) The overall QMIX structure. (c) Local utility network structure.</p>
<h1>C.2. Role Embedding Representations</h1>
<p>Fig. 7 shows various roles learned by ROMA. Roles are closely related to the sub-tasks in the learned winning strategy.
For the map 27m_vs_30m, the winning strategy is to form an offensive concave arc before engaging in the battle. Fig. 10(b) illustrates the role embedding representations at the first time step when the agents are going to set up the attack formation. We can see the roles aggregate according to the relative positions of the agents. Such role differentiation leads to different moving strategies so that agents can quickly form the arc without collisions.</p>
<p>Similar role-behavior relationships can be seen in all tasks. We present another example on the task of 6z4b. In the winning strategy learned by ROMA, Zerglings $4 \&amp; 5$ and Banelings kill most of the enemies, taking advantage of the splash damage of the Banelings, while Zerglings 6-9 hideaway, wait until the explosion is over, and then kill the remaining enemies. Fig. 10(c) shows the role embedding representations before the explosion. We can see clear clusters closely corresponding to the automatically detected sub-tasks at this time step.</p>
<p>Supported by these results, we can conclude that ROMA can automatically decompose the task and learn versatile roles, each of which is specialized in a certain sub-task.</p>
<h2>C.3. Additional Results for Role Evolution</h2>
<p>In Fig. 7 of the paper, we show how roles emerge and evolve on the map MMK2, where the involved agents are heterogeneous. In this section, we discuss the case of homogeneous agent teams. To this end, we visualize the emergence and evolution process of roles on the map 10m_vs_11m, which features 10 ally Marines facing 11 enemy Marines. In Fig. 11, we show the roles at the first time step of the battle (screenshot can be found in Fig. 12) at four different stages during the training. At this moment, agents need to form an offensive concave arc quickly. We can see that ROMA gradually learns to allocate roles according to relative positions of agents. Such roles and the corresponding differentiation in the individual policies help agents form the offensive arc more efficiently. Since setting up an attack formation is critical for winning the game, a connection between the specialization of the roles at the first time step and the improvement of the win rate can be observed.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 12. Screenshot of 10m_vs_11m, $t=1$.</p>
<h1>D. Related Works</h1>
<p>Multi-agent reinforcement learning holds the promise to solve many real-world problems and has been making vigorous progress recently. To avoid otherwise exponentially large state-action space, factorizing MDPs for multi-agent systems is proposed (Guestrin et al., 2002). Coordination graphs (Bargiacchi et al., 2018; Yang et al., 2018; Grover et al., 2018; Kipf et al., 2018) and explicit communication (Sukhbaatar et al., 2016; Hoshen, 2017; Jiang \&amp; Lu, 2018; Singh et al., 2019; Das et al., 2019; Singh et al., 2019; Kim et al., 2019) are studied to model the dependence between the decision-making processes of agents. Training decentralized policies is faced with two challenges: the issue of non-stationarity (Tan, 1993) and reward assignment (Foerster et al., 2018; Nguyen et al., 2018). To resolve these problems, Sunehag et al. (2018) propose a value decomposition method called VDN. VDN learns a global action-value function, which is factored as the sum of each agent's local Q-value. QMIX (Rashid et al., 2018) extends VDN by representing the global value function as a learnable, state-condition, and monotonic combination of the local Q-values. In this paper, we use the mixing network of QMIX. The framework of QMIX is shown in Fig. 13.</p>
<p>The StarCraft II unit micromanagement task is considered as one of the most challenging cooperative multi-agent testbeds for its high degree of control complexity and environmental stochasticity. Usunier et al. (2017) and Peng et al. (2017) study this problem from a centralized perspective. In order to facilitate decentralized control, we test our method on the SMAC benchmark (Samvelyan et al., 2019), which is the same as in (Foerster et al., 2017; 2018; Rashid et al., 2018; Mahajan et al., 2019).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ StarCraft II are trademarks of Blizzard Entertainment ${ }^{\mathrm{TM}}$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2241 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2241</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2241</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-61.html">extraction-schema-61</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <p><strong>Paper ID:</strong> paper-279155005</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.04179v1.pdf" target="_blank">SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) achieve remarkable performance across tasks but incur substantial computational costs due to their deep, multi-layered architectures. Layer pruning has emerged as a strategy to alleviate these inefficiencies, but conventional static pruning methods overlook two critical dynamics inherent to LLM inference: (1) horizontal dynamics, where token-level heterogeneity demands context-aware pruning decisions, and (2) vertical dynamics, where the distinct functional roles of MLP and self-attention layers necessitate component-specific pruning policies. We introduce SkipGPT, a dynamic layer pruning framework designed to optimize computational resource allocation through two core innovations: (1) global token-aware routing to prioritize critical tokens, and (2) decoupled pruning policies for MLP and self-attention components. To mitigate training instability, we propose a two-stage optimization paradigm: first, a disentangled training phase that learns routing strategies via soft parameterization to avoid premature pruning decisions, followed by parameter-efficient LoRA fine-tuning to restore performance impacted by layer removal. Extensive experiments demonstrate that SkipGPT reduces over 40% of model parameters while matching or exceeding the performance of the original dense model across benchmarks. By harmonizing dynamic efficiency with preserved expressivity, SkipGPT advances the practical deployment of scalable, resource-aware LLMs. Our code is publicly available at: https://github.com/EIT-NLP/SkipGPT.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2241.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2241.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SkipGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SkipGPT: Dynamic Layer Pruning with Token Awareness and Module Decoupling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic layer-pruning framework that (1) routes tokens globally in a token-aware manner to allocate computation non-uniformly across tokens (horizontal dynamics) and (2) decouples pruning policies for attention and MLP modules within layers (vertical dynamics); trained with a two-stage paradigm (router tuning then LoRA fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SkipGPT (applied to LLaMA-2/3 families)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Router before every module (linear projection) producing a binary execute/skip decision via Gumbel-Softmax + straight-through estimator; separate routers for attention and MLP; two-stage training: freeze model and tune routers (SkipGPT-RT), then freeze routers and recover/boost with LoRA (SkipGPT-RT-L). Purpose: reduce inference computation while preserving or improving downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>dynamic pruning / conditional computation via per-token routers; decoupled module-level skipping (attention vs MLP); global sparsity budget allowing non-uniform allocation across tokens and layers</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>language modeling / zero-shot evaluation on commonsense reasoning benchmarks (BoolQ, PIQA, HellaSwag, WinoGrande, ARC-easy, ARC-challenge, OpenbookQA) and perplexity on WT2 and PTB</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Router-only (SkipGPT-RT): retains >90% of dense performance at 25% parameter pruning for LLaMA2-7B and LLaMA2-13B; retains >95% at 25% and >80% at 40% pruning for LLaMA3.1-8B. After LoRA fine-tuning (SkipGPT-RT-L): fully restores and can surpass original dense model performance (reported: pruned model can match/exceed original even with ~40% parameter reduction). (Metrics reported across multiple benchmark accuracy and PPL values in the paper; summarized above from text.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Dense (uniform) model: baseline accuracies and PPL reported in paper tables; SkipGPT-RT and SkipGPT-RT-L are compared against static and other baselines and reported to match or exceed dense model after LoRA recovery. (Exact dense numbers are given in Table 1/2 of paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Parameter reduction: experiments at 25%, 40% parameter reduction; paper claims >40% parameter reduction possible while matching/exceeding performance. Router params are ≈0.007% of LLaMA2-7B parameters. Router tuning is lightweight and converges quickly — requires a single A800 (80GB) GPU and completes within ~4 hours for reported experiments. Sparsity controlled by target T; SkipGPT achieves lower computational overhead when attention FLOPs dominate (long context).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Baselines include static pruning (layer removals) and other dynamic methods; uniform dense baseline has 0% sparsity (full FLOPs). Paper reports SkipGPT yields lower FLOPs relative to many baselines at comparable parameter reduction; exact FLOP numbers not enumerated in the main text, but sparsity ratios (e.g., 25%, 40%) and PPL/accuracy trade-offs are provided in tables and figures.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Not reported explicitly beyond multi-benchmark evaluation; authors report SkipGPT-RT generalizes across tasks in commonsense reasoning benchmarks and language-modeling PPLs, with router tuning (no parameter change) preserving a large fraction of performance, suggesting routers learned generally useful compute-allocation policies. No explicit transfer/OoD metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Router analysis provided: per-token cosine-similarity analysis and module redundancy patterns (attention tends to be pruned more than MLP; redundancy shifts across layers and with context length). Authors use routers as probes to reveal module importance and token-level compute allocation patterns, providing interpretability insights into which modules are important per token.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Evaluated across several reasoning QA datasets and PPL datasets; SkipGPT-RT retains most dense performance across the set and SkipGPT-RT-L recovers or surpasses dense; authors report aggregated Avg. Acc and PPL in tables showing superiority to many baselines. No negative multi-task effects reported.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Router tuning uses negligible extra parameters (≈0.007%); entire tuning fits on one A800 GPU and completes in ≈4 hours (reported). SkipGPT achieves high sparsity (experiments up to 70% sparsity in analysis) while maintaining lower PPL than comparable static method (Joint Layer Drop).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Adaptive, token-aware routing combined with decoupled attention/MLP pruning yields large parameter/FLOP reductions (≥25–40%) with minimal to no loss in performance; router-only tuning preserves most performance and LoRA recovers or surpasses dense performance, demonstrating dynamic task-aligned computation outperforms uniform computation and many static pruning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Empirical results show dynamic, token-adaptive, and module-decoupled computation (task-aligned abstraction) outperforms or matches uniform dense execution and static pruning across accuracy and PPL, supporting the principle that allocating compute non-uniformly based on token/module importance improves efficiency and preserves expressivity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2241.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2241.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SkipGPT-RT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SkipGPT Router Tuning (SkipGPT-RT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>First-stage of SkipGPT two-stage training: freeze pretrained model weights and optimize only lightweight routers to determine per-token per-module skip/execute decisions, enabling immediate pruning with minimal parameter change.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SkipGPT-RT (router-only tuned pruned models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Routers (linear projections) before each module trained with Gumbel-Softmax + ST estimator; model parameters frozen. Objective includes language modeling loss plus sparsity penalty to hit target global sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>dynamic per-token routing (pruning) with global sparsity target; module-level (attention/MLP) decoupling</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>language modeling and commonsense QA benchmarks (same as SkipGPT experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Router tuning alone retains >90% of dense model performance at 25% parameter pruning for LLaMA2-7B and LLaMA2-13B; retains >95% at 25% and >80% at 40% for LLaMA3.1-8B. Achieves over 90% of original performance even after discarding 25% of parameters (text claims and Table 1 summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Compared to dense baselines in Table 1; SkipGPT-RT significantly outperforms many static pruning baselines and Joint Layer Drop at same parameter reduction levels (quantitative tables provided in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Router parameter overhead ≈0.007% (LLaMA2-7B); training can be done on a single A800 GPU in ≈4 hours; achieves target sparsity rates (25%, 40%, higher) with substantial parameter skipping during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Dense baseline computes 2L×S modules for a forward pass; SkipGPT-RT computes (1−sparsity)×2L×S modules. Exact FLOP counts not enumerated, but SkipGPT-RT reported lower computational overhead versus baselines in long-context settings where attention FLOPs dominate.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Router activations used to probe module importance; analysis shows consistent pruning of more attention modules than MLP and shifting redundancy patterns with context length. Router tuning alone provides insights into module importance without changing pretrained weights.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Maintains performance across multiple benchmarks (see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Router tuning is resource-light (tiny parameter budget, single GPU for a few hours).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Router-only tuning is highly effective: with negligible parameter overhead it preserves the majority of dense-model performance under substantial parameter reduction, and provides a practical, fast way to probe and apply dynamic pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>SkipGPT-RT's strong preservation of performance under substantial pruning demonstrates that dynamic, token-aware compute allocation can replace uniform computation with little performance loss.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2241.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2241.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SkipGPT-RT-L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SkipGPT Router Tuning + LoRA Fine-Tuning (SkipGPT-RT-L)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two-stage pipeline's second stage: after router tuning, freeze routers and apply parameter-efficient LoRA fine-tuning to recover or exceed original dense-model performance for the pruned model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SkipGPT-RT-L (LoRA-recovered pruned models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Low-rank adapter (LoRA) fine-tuning applied to the pruned model (routers frozen) to repair representational capacity loss due to skipped modules; recovers or surpasses dense performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>pruning + LoRA adaptation (dynamic pruning kept fixed during LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>same language modeling and benchmark suite</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>After LoRA fine-tuning, the pruned model fully restores and can even surpass original dense performance; authors report SkipGPT-RT-L matches/exceeds dense across evaluated benchmarks and ranks second only to direct LoRA on the original model for LLaMA2-7B in some metrics (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Dense model with LoRA is a strong baseline; SkipGPT-RT-L recovers to comparable or better performance while using fewer parameters in active computation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>LoRA is parameter-efficient; full training specs: LoRA tuning uses learning rate 2e-4, warmup 0.1, AdamW, trained for the stated steps (paper gives steps and batch sizes). Inference cost remains reduced due to pruned modules; router params still very small.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Dense + LoRA baseline uses full computation at inference; SkipGPT-RT-L reduces active compute proportionally to pruning/sparsity (e.g., 25–40% parameter reduction).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Recovered across multiple tasks; LoRA stage required to fully match the original model on some tasks as shown in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>LoRA stage is computationally modest compared with full fine-tuning; overall pipeline remains suitable for single-A800 training budgets as claimed.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>A frozen-router + LoRA recovery step can fully restore and sometimes improve performance of dynamically pruned models, making dynamic pruning practical for production with low tuning cost.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Shows that task-aligned, adaptive computation can be combined with lightweight adaptation (LoRA) to recover representational losses, supporting the practicality of task-aligned abstraction over uniform representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2241.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2241.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Joint Layer Drop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Joint Layer Drop (fine-grained layer pruning of attention and MLP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-grained static pruning baseline that removes attention and MLP modules independently based on an importance metric (Block Influence / similarity-based scores).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Joint Layer Drop</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Joint Layer Drop (static fine-grained pruning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Structured pruning that selects and permanently removes redundant attention and/or MLP modules (treating them as separate units) according to importance metrics derived from hidden-state transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>static structured pruning (uniform per-instance compute graph after pruning), module-level importance ranking</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>language modeling and commonsense QA benchmarks (used as baseline in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Compared in Table 1: Joint Layer Drop is generally stronger than some static baselines but is outperformed by SkipGPT-RT in accuracy and PPL at the same parameter reduction rates; specific numeric results are tabulated in Table 1 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Removes modules to achieve ~23.9–24.3% parameter reduction in presented experiments (paper lists exact ratios per model variant). Once pruned, inference is uniformly reduced but lacks per-token adaptivity.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Dense baseline: 0% parameter pruning. Joint Layer Drop reduces parameters and inference cost deterministically for all inputs; the paper reports it is less efficient/less generalizable under high sparsity compared to SkipGPT-RT.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Evaluated across multiple benchmarks in paper; generally competitive but inferior to SkipGPT-RT under comparable reductions.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Static pruning requires recovery fine-tuning to restore performance in many cases; less scalable at high sparsity according to paper analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Decoupled static pruning (attention vs MLP) is better than coarse layer removal, but still underperforms dynamic token-aware pruning (SkipGPT-RT) especially at higher sparsity levels.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Joint Layer Drop acknowledges module heterogeneity (vertical dynamics) supporting part of the Task-Aligned principle, but being static it lacks horizontal token adaptivity and thus is outperformed by dynamic, task-aligned methods.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2241.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2241.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ShortGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ShortGPT (structured layer pruning approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A static structured pruning method that removes redundant transformer layers based on a Block Influence metric derived from hidden-state transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ShortGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ShortGPT (static layer pruning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Identifies and removes less important whole layers (blocks) using Block Influence scores; static removal results in a smaller depth but same per-token compute graph for all inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>static pruning of whole layers (uniform per-input computation after pruning)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>language modeling and standard benchmarks (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>In Table 1 ShortGPT at ~25% reduction shows specific accuracy and PPL numbers; usually comparable to other static methods but worse than SkipGPT-RT on average.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Removes layers to reach stated parameter-reduction ratios (e.g., 25% in experiments); inference compute reduced uniformly for all tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Dense baseline (0% pruning) is full compute; ShortGPT reduces model depth and thus FLOPs but lacks token-level adaptivity.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Reported across multiple benchmarks; generally maintains competitive performance but is outperformed by SkipGPT-RT in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Static structured depth pruning can compress models effectively, but lacks per-input adaptivity and is typically outperformed by dynamic, token-aware methods at similar parameter reductions.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>challenges</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>As a uniform, static-pruning method it exemplifies the limits of uniform representations: it can compress models but does not realize the benefits of input-adaptive allocation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2241.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2241.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shortened-LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shortened LLaMA (PPL and Taylor variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Static depth-pruning variants applying importance metrics (perplexity-based or Taylor expansion) to remove entire Transformer layers and recover via LoRA or continued pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Shortened LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Shortened-LLaMA (static layer removal with recovery)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prunes entire decoder layers based on PPL or Taylor-expansion-based importance; uses recovery techniques such as LoRA fine-tuning or continued pretraining to regain performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>static whole-layer removal (uniform compute post-pruning); recovery via LoRA/CPT</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>language modeling and reasoning benchmarks (used as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Table 1 shows performance numbers for both Shortened-PPL and Shortened-Taylor at 25% reduction; they are competitive among static methods but inferior to SkipGPT-RT overall.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Reduces depth to achieve stated parameter ratios (e.g., 25% or 40% in experiments); inference uniformly lighter but not token-adaptive.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Reported across multiple tasks; requires recovery fine-tuning which sometimes fails to converge at aggressive pruning levels (paper note that some static methods fail to converge during recovery on LLaMA2-13B).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Static whole-layer pruning needs recovery fine-tuning and is less robust at higher sparsity compared to dynamic token-aware pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>challenges</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Again demonstrates limitations of uniform, static reductions: while effective at moderate pruning, they do not match dynamic, token-adaptive approaches under aggressive compression.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2241.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2241.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LaCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LaCo (Layer Collapse)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured pruning method that merges deeper layers into earlier ones (layer collapse) to reduce depth while preserving representations, using gradient/Hessian-based importance estimates and few-shot calibration for preserving outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LaCo</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaCo (layer collapse structured pruning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Progressively merges layers to reduce depth; integrates differences among layers to preserve outputs; uses calibration/few-shot to minimize performance loss without full retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>static layer merging (structured collapse) leading to uniform computation for all inputs after pruning</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>language modeling and benchmarks (used as baseline in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Paper lists LaCo numbers in Table 1; generally competitive among static methods but not as effective as SkipGPT-RT for dynamic pruning regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Enables 30–50% layer reduction without retraining (paper claims) and reduces inference compute uniformly; requires few-shot calibration to maintain output similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Reported across benchmarks; LaCo provides an approach to reduce depth while preserving performance without extensive retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Favors scenarios with limited retraining budgets (few-shot calibration rather than full fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Layer-merging (LaCo) allows significant depth reductions with limited retraining, but remains a static approach that cannot adapt compute per token.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>challenges</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>LaCo reduces redundant depth but is static; supports the idea that depth redundancy exists but does not leverage task-aligned, input-adaptive computation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2241.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2241.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Pruner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-Pruner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structural pruning method that removes non-critical structures using gradient-based criteria, with efficient post-training recovery using LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM-Pruner</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-Pruner (static structural pruning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Performs dependency-based structural pruning to eliminate non-critical components; supports efficient recovery with LoRA requiring modest amounts of data (paper notes 50K public samples in related descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>static structural pruning (uniform compute after pruning)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>language modeling and benchmark evaluations (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Included in Table 1; generally lags behind SkipGPT-RT under the same parameter reduction settings in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Removes structural components to reach ~24–25% parameter reduction in experiments; inference savings are uniform per input.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Paper mentions LLM-Pruner can recover with 50K public samples (from related work description), but detailed sample-efficiency experiments are not included in the main results.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Reported across multiple benchmarks; static recovery often required.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Static structural pruning can compress LLMs effectively but requires recovery fine-tuning and lacks per-input dynamic allocation advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>challenges</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Represents a uniform/static pruning paradigm that does not exploit token-level or module-level adaptivity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2241.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2241.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SliceGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SliceGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-training sparsification method that reduces embedding/width by deleting rows/columns of weight matrices and replacing them with smaller dense matrices while maintaining dense execution for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SliceGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SliceGPT (width reduction / post-training sparsification)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Shrinks embedding dimension (width) by replacing large weight matrices with smaller dense matrices via orthogonal transforms and PCA-based projections to prune redundant dimensions; dense operations preserved for efficient execution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>static width reduction (uniform per-input execution afterward)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>language modeling benchmarks (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Table 1 includes SliceGPT numbers (e.g., at ~25% compression retaining >90% accuracy in prior work claims); in this paper, SliceGPT baseline performance is listed but SkipGPT-RT outperforms it in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Achieves up to ~30% compression with retained accuracy in cited prior work; in this paper used as a width-reduction baseline to compare against depth/dynamic approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Included in multi-benchmark comparisons in Table 1 where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Width-reduction approaches like SliceGPT can reduce model size but are less effective than dynamic depth/token-adaptive pruning in preserving performance under aggressive compute reductions in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>challenges</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>SliceGPT exemplifies uniform reductions (width-based), and the paper's results favor adaptive, task-aligned depth/token methods over uniform-width compression in many settings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2241.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2241.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture-of-Depths (MoD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior dynamic computation method that allocates compute across depth dynamically by selecting top-k tokens per layer (top-k routing), thereby skipping some tokens via residual connections; cited as related work and a dynamic baseline family.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mixture-of-Depths: Dynamically allocating compute in transformer-based language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixture-of-Depths (MoD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Top-k routing per layer that selects which tokens are processed at each depth, enabling conditional computation across depth for tokens deemed important; static per-layer compute budgets are used in original formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>top-k token routing per layer (conditional compute / adaptive computation time style), but with layer-wise compute budgets</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>language modeling / dynamic inference (cited prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Cited claims: MoD achieves up to 50% faster inference while matching or surpassing full-compute transformers (from related-work summary in the paper). The current paper does not present new MoD experimental numbers but references MoD as a point of comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Reported in prior work: up to 50% faster inference (top-k dynamic routing) — cited in paper's related work; not re-evaluated in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Top-k per-layer dynamic routing approaches are conceptually related but differ in global budgeting and in whether they decouple attention/MLP; SkipGPT claims advantages by using a global token-aware budget and module decoupling.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>MoD is an example of dynamic, task-adaptive allocation that supports the advantage of non-uniform computation; SkipGPT extends and refines this idea with global budgeting and module decoupling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2241.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2241.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>D-LLM (Token-adaptive computing for LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent token-adaptive dynamic inference framework that allocates computation per token using a decision module before each layer and manages KV-cache compatibility via eviction; cited as related work claiming large computational savings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>D-LLM: A token adaptive computing resource allocation strategy for large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>D-LLM (token-adaptive dynamic inference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Introduces a per-layer decision module to determine whether to execute or skip a layer for a token, adaptively reducing computation for less critical tokens; includes KV-cache eviction policy to remain compatible with autoregressive caching.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>per-token dynamic skip decisions (adaptive computation), with KV-cache adjustments</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>language modeling / dynamic inference (cited prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Cited claim: up to 50% reduction in computational cost across various NLP tasks while maintaining strong accuracy (from paper's related-work summary). Not directly re-evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Prior work claims up to 50% reduction in compute; SkipGPT positions itself relative to D-LLM by also decoupling attention/MLP and using global budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>D-LLM demonstrates benefits of token-adaptive per-layer skipping and KV-cache-aware design; SkipGPT builds on similar token-adaptive ideas but emphasizes global sparsity allocation and module decoupling.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>D-LLM exemplifies token-adaptive compute allocation showing efficiency gains, consistent with the Task-Aligned Abstraction Principle.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mixture-of-Depths: Dynamically allocating compute in transformer-based language models. <em>(Rating: 2)</em></li>
                <li>D-LLM: A token adaptive computing resource allocation strategy for large language models. <em>(Rating: 2)</em></li>
                <li>ShortGPT: Layers in large language models are more redundant than you expect. <em>(Rating: 2)</em></li>
                <li>Joint Layer Drop <em>(Rating: 1)</em></li>
                <li>LaCo <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2241",
    "paper_id": "paper-279155005",
    "extraction_schema_id": "extraction-schema-61",
    "extracted_data": [
        {
            "name_short": "SkipGPT",
            "name_full": "SkipGPT: Dynamic Layer Pruning with Token Awareness and Module Decoupling",
            "brief_description": "A dynamic layer-pruning framework that (1) routes tokens globally in a token-aware manner to allocate computation non-uniformly across tokens (horizontal dynamics) and (2) decouples pruning policies for attention and MLP modules within layers (vertical dynamics); trained with a two-stage paradigm (router tuning then LoRA fine-tuning).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SkipGPT (applied to LLaMA-2/3 families)",
            "model_description": "Router before every module (linear projection) producing a binary execute/skip decision via Gumbel-Softmax + straight-through estimator; separate routers for attention and MLP; two-stage training: freeze model and tune routers (SkipGPT-RT), then freeze routers and recover/boost with LoRA (SkipGPT-RT-L). Purpose: reduce inference computation while preserving or improving downstream performance.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "dynamic pruning / conditional computation via per-token routers; decoupled module-level skipping (attention vs MLP); global sparsity budget allowing non-uniform allocation across tokens and layers",
            "is_dynamic_or_adaptive": true,
            "task_domain": "language modeling / zero-shot evaluation on commonsense reasoning benchmarks (BoolQ, PIQA, HellaSwag, WinoGrande, ARC-easy, ARC-challenge, OpenbookQA) and perplexity on WT2 and PTB",
            "performance_task_aligned": "Router-only (SkipGPT-RT): retains &gt;90% of dense performance at 25% parameter pruning for LLaMA2-7B and LLaMA2-13B; retains &gt;95% at 25% and &gt;80% at 40% pruning for LLaMA3.1-8B. After LoRA fine-tuning (SkipGPT-RT-L): fully restores and can surpass original dense model performance (reported: pruned model can match/exceed original even with ~40% parameter reduction). (Metrics reported across multiple benchmark accuracy and PPL values in the paper; summarized above from text.)",
            "performance_uniform_baseline": "Dense (uniform) model: baseline accuracies and PPL reported in paper tables; SkipGPT-RT and SkipGPT-RT-L are compared against static and other baselines and reported to match or exceed dense model after LoRA recovery. (Exact dense numbers are given in Table 1/2 of paper.)",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Parameter reduction: experiments at 25%, 40% parameter reduction; paper claims &gt;40% parameter reduction possible while matching/exceeding performance. Router params are ≈0.007% of LLaMA2-7B parameters. Router tuning is lightweight and converges quickly — requires a single A800 (80GB) GPU and completes within ~4 hours for reported experiments. Sparsity controlled by target T; SkipGPT achieves lower computational overhead when attention FLOPs dominate (long context).",
            "computational_efficiency_baseline": "Baselines include static pruning (layer removals) and other dynamic methods; uniform dense baseline has 0% sparsity (full FLOPs). Paper reports SkipGPT yields lower FLOPs relative to many baselines at comparable parameter reduction; exact FLOP numbers not enumerated in the main text, but sparsity ratios (e.g., 25%, 40%) and PPL/accuracy trade-offs are provided in tables and figures.",
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Not reported explicitly beyond multi-benchmark evaluation; authors report SkipGPT-RT generalizes across tasks in commonsense reasoning benchmarks and language-modeling PPLs, with router tuning (no parameter change) preserving a large fraction of performance, suggesting routers learned generally useful compute-allocation policies. No explicit transfer/OoD metrics provided.",
            "interpretability_results": "Router analysis provided: per-token cosine-similarity analysis and module redundancy patterns (attention tends to be pruned more than MLP; redundancy shifts across layers and with context length). Authors use routers as probes to reveal module importance and token-level compute allocation patterns, providing interpretability insights into which modules are important per token.",
            "multi_task_performance": "Evaluated across several reasoning QA datasets and PPL datasets; SkipGPT-RT retains most dense performance across the set and SkipGPT-RT-L recovers or surpasses dense; authors report aggregated Avg. Acc and PPL in tables showing superiority to many baselines. No negative multi-task effects reported.",
            "resource_constrained_results": "Router tuning uses negligible extra parameters (≈0.007%); entire tuning fits on one A800 GPU and completes in ≈4 hours (reported). SkipGPT achieves high sparsity (experiments up to 70% sparsity in analysis) while maintaining lower PPL than comparable static method (Joint Layer Drop).",
            "key_finding_summary": "Adaptive, token-aware routing combined with decoupled attention/MLP pruning yields large parameter/FLOP reductions (≥25–40%) with minimal to no loss in performance; router-only tuning preserves most performance and LoRA recovers or surpasses dense performance, demonstrating dynamic task-aligned computation outperforms uniform computation and many static pruning baselines.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Empirical results show dynamic, token-adaptive, and module-decoupled computation (task-aligned abstraction) outperforms or matches uniform dense execution and static pruning across accuracy and PPL, supporting the principle that allocating compute non-uniformly based on token/module importance improves efficiency and preserves expressivity.",
            "uuid": "e2241.0"
        },
        {
            "name_short": "SkipGPT-RT",
            "name_full": "SkipGPT Router Tuning (SkipGPT-RT)",
            "brief_description": "First-stage of SkipGPT two-stage training: freeze pretrained model weights and optimize only lightweight routers to determine per-token per-module skip/execute decisions, enabling immediate pruning with minimal parameter change.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SkipGPT-RT (router-only tuned pruned models)",
            "model_description": "Routers (linear projections) before each module trained with Gumbel-Softmax + ST estimator; model parameters frozen. Objective includes language modeling loss plus sparsity penalty to hit target global sparsity.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "dynamic per-token routing (pruning) with global sparsity target; module-level (attention/MLP) decoupling",
            "is_dynamic_or_adaptive": true,
            "task_domain": "language modeling and commonsense QA benchmarks (same as SkipGPT experiments)",
            "performance_task_aligned": "Router tuning alone retains &gt;90% of dense model performance at 25% parameter pruning for LLaMA2-7B and LLaMA2-13B; retains &gt;95% at 25% and &gt;80% at 40% for LLaMA3.1-8B. Achieves over 90% of original performance even after discarding 25% of parameters (text claims and Table 1 summaries).",
            "performance_uniform_baseline": "Compared to dense baselines in Table 1; SkipGPT-RT significantly outperforms many static pruning baselines and Joint Layer Drop at same parameter reduction levels (quantitative tables provided in paper).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Router parameter overhead ≈0.007% (LLaMA2-7B); training can be done on a single A800 GPU in ≈4 hours; achieves target sparsity rates (25%, 40%, higher) with substantial parameter skipping during inference.",
            "computational_efficiency_baseline": "Dense baseline computes 2L×S modules for a forward pass; SkipGPT-RT computes (1−sparsity)×2L×S modules. Exact FLOP counts not enumerated, but SkipGPT-RT reported lower computational overhead versus baselines in long-context settings where attention FLOPs dominate.",
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": "Router activations used to probe module importance; analysis shows consistent pruning of more attention modules than MLP and shifting redundancy patterns with context length. Router tuning alone provides insights into module importance without changing pretrained weights.",
            "multi_task_performance": "Maintains performance across multiple benchmarks (see paper tables).",
            "resource_constrained_results": "Router tuning is resource-light (tiny parameter budget, single GPU for a few hours).",
            "key_finding_summary": "Router-only tuning is highly effective: with negligible parameter overhead it preserves the majority of dense-model performance under substantial parameter reduction, and provides a practical, fast way to probe and apply dynamic pruning.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "SkipGPT-RT's strong preservation of performance under substantial pruning demonstrates that dynamic, token-aware compute allocation can replace uniform computation with little performance loss.",
            "uuid": "e2241.1"
        },
        {
            "name_short": "SkipGPT-RT-L",
            "name_full": "SkipGPT Router Tuning + LoRA Fine-Tuning (SkipGPT-RT-L)",
            "brief_description": "Two-stage pipeline's second stage: after router tuning, freeze routers and apply parameter-efficient LoRA fine-tuning to recover or exceed original dense-model performance for the pruned model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SkipGPT-RT-L (LoRA-recovered pruned models)",
            "model_description": "Low-rank adapter (LoRA) fine-tuning applied to the pruned model (routers frozen) to repair representational capacity loss due to skipped modules; recovers or surpasses dense performance.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "pruning + LoRA adaptation (dynamic pruning kept fixed during LoRA)",
            "is_dynamic_or_adaptive": true,
            "task_domain": "same language modeling and benchmark suite",
            "performance_task_aligned": "After LoRA fine-tuning, the pruned model fully restores and can even surpass original dense performance; authors report SkipGPT-RT-L matches/exceeds dense across evaluated benchmarks and ranks second only to direct LoRA on the original model for LLaMA2-7B in some metrics (see Table 2).",
            "performance_uniform_baseline": "Dense model with LoRA is a strong baseline; SkipGPT-RT-L recovers to comparable or better performance while using fewer parameters in active computation.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "LoRA is parameter-efficient; full training specs: LoRA tuning uses learning rate 2e-4, warmup 0.1, AdamW, trained for the stated steps (paper gives steps and batch sizes). Inference cost remains reduced due to pruned modules; router params still very small.",
            "computational_efficiency_baseline": "Dense + LoRA baseline uses full computation at inference; SkipGPT-RT-L reduces active compute proportionally to pruning/sparsity (e.g., 25–40% parameter reduction).",
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Recovered across multiple tasks; LoRA stage required to fully match the original model on some tasks as shown in Table 2.",
            "resource_constrained_results": "LoRA stage is computationally modest compared with full fine-tuning; overall pipeline remains suitable for single-A800 training budgets as claimed.",
            "key_finding_summary": "A frozen-router + LoRA recovery step can fully restore and sometimes improve performance of dynamically pruned models, making dynamic pruning practical for production with low tuning cost.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Shows that task-aligned, adaptive computation can be combined with lightweight adaptation (LoRA) to recover representational losses, supporting the practicality of task-aligned abstraction over uniform representations.",
            "uuid": "e2241.2"
        },
        {
            "name_short": "Joint Layer Drop",
            "name_full": "Joint Layer Drop (fine-grained layer pruning of attention and MLP)",
            "brief_description": "A fine-grained static pruning baseline that removes attention and MLP modules independently based on an importance metric (Block Influence / similarity-based scores).",
            "citation_title": "Joint Layer Drop",
            "mention_or_use": "use",
            "model_name": "Joint Layer Drop (static fine-grained pruning)",
            "model_description": "Structured pruning that selects and permanently removes redundant attention and/or MLP modules (treating them as separate units) according to importance metrics derived from hidden-state transformations.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "static structured pruning (uniform per-instance compute graph after pruning), module-level importance ranking",
            "is_dynamic_or_adaptive": false,
            "task_domain": "language modeling and commonsense QA benchmarks (used as baseline in comparisons)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Compared in Table 1: Joint Layer Drop is generally stronger than some static baselines but is outperformed by SkipGPT-RT in accuracy and PPL at the same parameter reduction rates; specific numeric results are tabulated in Table 1 of the paper.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Removes modules to achieve ~23.9–24.3% parameter reduction in presented experiments (paper lists exact ratios per model variant). Once pruned, inference is uniformly reduced but lacks per-token adaptivity.",
            "computational_efficiency_baseline": "Dense baseline: 0% parameter pruning. Joint Layer Drop reduces parameters and inference cost deterministically for all inputs; the paper reports it is less efficient/less generalizable under high sparsity compared to SkipGPT-RT.",
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Evaluated across multiple benchmarks in paper; generally competitive but inferior to SkipGPT-RT under comparable reductions.",
            "resource_constrained_results": "Static pruning requires recovery fine-tuning to restore performance in many cases; less scalable at high sparsity according to paper analysis.",
            "key_finding_summary": "Decoupled static pruning (attention vs MLP) is better than coarse layer removal, but still underperforms dynamic token-aware pruning (SkipGPT-RT) especially at higher sparsity levels.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "Joint Layer Drop acknowledges module heterogeneity (vertical dynamics) supporting part of the Task-Aligned principle, but being static it lacks horizontal token adaptivity and thus is outperformed by dynamic, task-aligned methods.",
            "uuid": "e2241.3"
        },
        {
            "name_short": "ShortGPT",
            "name_full": "ShortGPT (structured layer pruning approach)",
            "brief_description": "A static structured pruning method that removes redundant transformer layers based on a Block Influence metric derived from hidden-state transformations.",
            "citation_title": "ShortGPT",
            "mention_or_use": "use",
            "model_name": "ShortGPT (static layer pruning)",
            "model_description": "Identifies and removes less important whole layers (blocks) using Block Influence scores; static removal results in a smaller depth but same per-token compute graph for all inputs.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "static pruning of whole layers (uniform per-input computation after pruning)",
            "is_dynamic_or_adaptive": false,
            "task_domain": "language modeling and standard benchmarks (used as baseline)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "In Table 1 ShortGPT at ~25% reduction shows specific accuracy and PPL numbers; usually comparable to other static methods but worse than SkipGPT-RT on average.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Removes layers to reach stated parameter-reduction ratios (e.g., 25% in experiments); inference compute reduced uniformly for all tokens.",
            "computational_efficiency_baseline": "Dense baseline (0% pruning) is full compute; ShortGPT reduces model depth and thus FLOPs but lacks token-level adaptivity.",
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Reported across multiple benchmarks; generally maintains competitive performance but is outperformed by SkipGPT-RT in the paper's comparisons.",
            "resource_constrained_results": null,
            "key_finding_summary": "Static structured depth pruning can compress models effectively, but lacks per-input adaptivity and is typically outperformed by dynamic, token-aware methods at similar parameter reductions.",
            "supports_or_challenges_theory": "challenges",
            "supports_or_challenges_theory_explanation": "As a uniform, static-pruning method it exemplifies the limits of uniform representations: it can compress models but does not realize the benefits of input-adaptive allocation.",
            "uuid": "e2241.4"
        },
        {
            "name_short": "Shortened-LLaMA",
            "name_full": "Shortened LLaMA (PPL and Taylor variants)",
            "brief_description": "Static depth-pruning variants applying importance metrics (perplexity-based or Taylor expansion) to remove entire Transformer layers and recover via LoRA or continued pretraining.",
            "citation_title": "Shortened LLaMA",
            "mention_or_use": "use",
            "model_name": "Shortened-LLaMA (static layer removal with recovery)",
            "model_description": "Prunes entire decoder layers based on PPL or Taylor-expansion-based importance; uses recovery techniques such as LoRA fine-tuning or continued pretraining to regain performance.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "static whole-layer removal (uniform compute post-pruning); recovery via LoRA/CPT",
            "is_dynamic_or_adaptive": false,
            "task_domain": "language modeling and reasoning benchmarks (used as baselines)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Table 1 shows performance numbers for both Shortened-PPL and Shortened-Taylor at 25% reduction; they are competitive among static methods but inferior to SkipGPT-RT overall.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Reduces depth to achieve stated parameter ratios (e.g., 25% or 40% in experiments); inference uniformly lighter but not token-adaptive.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Reported across multiple tasks; requires recovery fine-tuning which sometimes fails to converge at aggressive pruning levels (paper note that some static methods fail to converge during recovery on LLaMA2-13B).",
            "resource_constrained_results": null,
            "key_finding_summary": "Static whole-layer pruning needs recovery fine-tuning and is less robust at higher sparsity compared to dynamic token-aware pruning.",
            "supports_or_challenges_theory": "challenges",
            "supports_or_challenges_theory_explanation": "Again demonstrates limitations of uniform, static reductions: while effective at moderate pruning, they do not match dynamic, token-adaptive approaches under aggressive compression.",
            "uuid": "e2241.5"
        },
        {
            "name_short": "LaCo",
            "name_full": "LaCo (Layer Collapse)",
            "brief_description": "A structured pruning method that merges deeper layers into earlier ones (layer collapse) to reduce depth while preserving representations, using gradient/Hessian-based importance estimates and few-shot calibration for preserving outputs.",
            "citation_title": "LaCo",
            "mention_or_use": "use",
            "model_name": "LaCo (layer collapse structured pruning)",
            "model_description": "Progressively merges layers to reduce depth; integrates differences among layers to preserve outputs; uses calibration/few-shot to minimize performance loss without full retraining.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "static layer merging (structured collapse) leading to uniform computation for all inputs after pruning",
            "is_dynamic_or_adaptive": false,
            "task_domain": "language modeling and benchmarks (used as baseline in experiments)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Paper lists LaCo numbers in Table 1; generally competitive among static methods but not as effective as SkipGPT-RT for dynamic pruning regimes.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Enables 30–50% layer reduction without retraining (paper claims) and reduces inference compute uniformly; requires few-shot calibration to maintain output similarity.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Reported across benchmarks; LaCo provides an approach to reduce depth while preserving performance without extensive retraining.",
            "resource_constrained_results": "Favors scenarios with limited retraining budgets (few-shot calibration rather than full fine-tuning).",
            "key_finding_summary": "Layer-merging (LaCo) allows significant depth reductions with limited retraining, but remains a static approach that cannot adapt compute per token.",
            "supports_or_challenges_theory": "challenges",
            "supports_or_challenges_theory_explanation": "LaCo reduces redundant depth but is static; supports the idea that depth redundancy exists but does not leverage task-aligned, input-adaptive computation.",
            "uuid": "e2241.6"
        },
        {
            "name_short": "LLM-Pruner",
            "name_full": "LLM-Pruner",
            "brief_description": "A structural pruning method that removes non-critical structures using gradient-based criteria, with efficient post-training recovery using LoRA.",
            "citation_title": "LLM-Pruner",
            "mention_or_use": "use",
            "model_name": "LLM-Pruner (static structural pruning)",
            "model_description": "Performs dependency-based structural pruning to eliminate non-critical components; supports efficient recovery with LoRA requiring modest amounts of data (paper notes 50K public samples in related descriptions).",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "static structural pruning (uniform compute after pruning)",
            "is_dynamic_or_adaptive": false,
            "task_domain": "language modeling and benchmark evaluations (used as baseline)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Included in Table 1; generally lags behind SkipGPT-RT under the same parameter reduction settings in the paper.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Removes structural components to reach ~24–25% parameter reduction in experiments; inference savings are uniform per input.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Paper mentions LLM-Pruner can recover with 50K public samples (from related work description), but detailed sample-efficiency experiments are not included in the main results.",
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Reported across multiple benchmarks; static recovery often required.",
            "resource_constrained_results": null,
            "key_finding_summary": "Static structural pruning can compress LLMs effectively but requires recovery fine-tuning and lacks per-input dynamic allocation advantages.",
            "supports_or_challenges_theory": "challenges",
            "supports_or_challenges_theory_explanation": "Represents a uniform/static pruning paradigm that does not exploit token-level or module-level adaptivity.",
            "uuid": "e2241.7"
        },
        {
            "name_short": "SliceGPT",
            "name_full": "SliceGPT",
            "brief_description": "A post-training sparsification method that reduces embedding/width by deleting rows/columns of weight matrices and replacing them with smaller dense matrices while maintaining dense execution for efficiency.",
            "citation_title": "SliceGPT",
            "mention_or_use": "use",
            "model_name": "SliceGPT (width reduction / post-training sparsification)",
            "model_description": "Shrinks embedding dimension (width) by replacing large weight matrices with smaller dense matrices via orthogonal transforms and PCA-based projections to prune redundant dimensions; dense operations preserved for efficient execution.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "static width reduction (uniform per-input execution afterward)",
            "is_dynamic_or_adaptive": false,
            "task_domain": "language modeling benchmarks (used as baseline)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Table 1 includes SliceGPT numbers (e.g., at ~25% compression retaining &gt;90% accuracy in prior work claims); in this paper, SliceGPT baseline performance is listed but SkipGPT-RT outperforms it in many cases.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Achieves up to ~30% compression with retained accuracy in cited prior work; in this paper used as a width-reduction baseline to compare against depth/dynamic approaches.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Included in multi-benchmark comparisons in Table 1 where applicable.",
            "resource_constrained_results": null,
            "key_finding_summary": "Width-reduction approaches like SliceGPT can reduce model size but are less effective than dynamic depth/token-adaptive pruning in preserving performance under aggressive compute reductions in the paper's comparisons.",
            "supports_or_challenges_theory": "challenges",
            "supports_or_challenges_theory_explanation": "SliceGPT exemplifies uniform reductions (width-based), and the paper's results favor adaptive, task-aligned depth/token methods over uniform-width compression in many settings.",
            "uuid": "e2241.8"
        },
        {
            "name_short": "MoD",
            "name_full": "Mixture-of-Depths (MoD)",
            "brief_description": "A prior dynamic computation method that allocates compute across depth dynamically by selecting top-k tokens per layer (top-k routing), thereby skipping some tokens via residual connections; cited as related work and a dynamic baseline family.",
            "citation_title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models.",
            "mention_or_use": "mention",
            "model_name": "Mixture-of-Depths (MoD)",
            "model_description": "Top-k routing per layer that selects which tokens are processed at each depth, enabling conditional computation across depth for tokens deemed important; static per-layer compute budgets are used in original formulation.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "top-k token routing per layer (conditional compute / adaptive computation time style), but with layer-wise compute budgets",
            "is_dynamic_or_adaptive": true,
            "task_domain": "language modeling / dynamic inference (cited prior work)",
            "performance_task_aligned": "Cited claims: MoD achieves up to 50% faster inference while matching or surpassing full-compute transformers (from related-work summary in the paper). The current paper does not present new MoD experimental numbers but references MoD as a point of comparison.",
            "performance_uniform_baseline": null,
            "has_direct_comparison": null,
            "computational_efficiency_task_aligned": "Reported in prior work: up to 50% faster inference (top-k dynamic routing) — cited in paper's related work; not re-evaluated in this paper's experiments.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "Top-k per-layer dynamic routing approaches are conceptually related but differ in global budgeting and in whether they decouple attention/MLP; SkipGPT claims advantages by using a global token-aware budget and module decoupling.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "MoD is an example of dynamic, task-adaptive allocation that supports the advantage of non-uniform computation; SkipGPT extends and refines this idea with global budgeting and module decoupling.",
            "uuid": "e2241.9"
        },
        {
            "name_short": "D-LLM",
            "name_full": "D-LLM (Token-adaptive computing for LLMs)",
            "brief_description": "A recent token-adaptive dynamic inference framework that allocates computation per token using a decision module before each layer and manages KV-cache compatibility via eviction; cited as related work claiming large computational savings.",
            "citation_title": "D-LLM: A token adaptive computing resource allocation strategy for large language models.",
            "mention_or_use": "mention",
            "model_name": "D-LLM (token-adaptive dynamic inference)",
            "model_description": "Introduces a per-layer decision module to determine whether to execute or skip a layer for a token, adaptively reducing computation for less critical tokens; includes KV-cache eviction policy to remain compatible with autoregressive caching.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "per-token dynamic skip decisions (adaptive computation), with KV-cache adjustments",
            "is_dynamic_or_adaptive": true,
            "task_domain": "language modeling / dynamic inference (cited prior work)",
            "performance_task_aligned": "Cited claim: up to 50% reduction in computational cost across various NLP tasks while maintaining strong accuracy (from paper's related-work summary). Not directly re-evaluated in this paper.",
            "performance_uniform_baseline": null,
            "has_direct_comparison": null,
            "computational_efficiency_task_aligned": "Prior work claims up to 50% reduction in compute; SkipGPT positions itself relative to D-LLM by also decoupling attention/MLP and using global budgets.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "D-LLM demonstrates benefits of token-adaptive per-layer skipping and KV-cache-aware design; SkipGPT builds on similar token-adaptive ideas but emphasizes global sparsity allocation and module decoupling.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "D-LLM exemplifies token-adaptive compute allocation showing efficiency gains, consistent with the Task-Aligned Abstraction Principle.",
            "uuid": "e2241.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models.",
            "rating": 2
        },
        {
            "paper_title": "D-LLM: A token adaptive computing resource allocation strategy for large language models.",
            "rating": 2
        },
        {
            "paper_title": "ShortGPT: Layers in large language models are more redundant than you expect.",
            "rating": 2
        },
        {
            "paper_title": "Joint Layer Drop",
            "rating": 1
        },
        {
            "paper_title": "LaCo",
            "rating": 1
        }
    ],
    "cost": 0.023607499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling
4 Jun 2025</p>
<p>Anhao Zhao 
Ningbo Key Laboratory of Spatial Intelligence and Digital Derivative
Institute of Digital Twin
Eastern Institute of Tech-nology
Ningbo</p>
<p>Southwest Jiaotong University 3 Tencent Inc</p>
<p>Fanghua Ye <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#102;&#97;&#110;&#103;&#104;&#117;&#97;&#46;&#121;&#101;&#46;&#50;&#49;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#102;&#97;&#110;&#103;&#104;&#117;&#97;&#46;&#121;&#101;&#46;&#50;&#49;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a> 
Yingqi Fan 
Ningbo Key Laboratory of Spatial Intelligence and Digital Derivative
Institute of Digital Twin
Eastern Institute of Tech-nology
Ningbo</p>
<p>Junlong Tong 
Ningbo Key Laboratory of Spatial Intelligence and Digital Derivative
Institute of Digital Twin
Eastern Institute of Tech-nology
Ningbo</p>
<p>Shanghai Jiao Tong University</p>
<p>Zhiwei Fei 
Nanjing University</p>
<p>Hui Su 
Meituan Inc</p>
<p>Xiaoyu Shen <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#120;&#121;&#115;&#104;&#101;&#110;&#64;&#101;&#105;&#116;&#101;&#99;&#104;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#120;&#121;&#115;&#104;&#101;&#110;&#64;&#101;&#105;&#116;&#101;&#99;&#104;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a>. 
Ningbo Key Laboratory of Spatial Intelligence and Digital Derivative
Institute of Digital Twin
Eastern Institute of Tech-nology
Ningbo</p>
<p>Xiaoyu Shen</p>
<p>SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling
4 Jun 2025E7D62B407C3E76FE1FD2FFB1789D00FEarXiv:2506.04179v1[cs.CL]
Large language models (LLMs) achieve remarkable performance across tasks but incur substantial computational costs due to their deep, multilayered architectures.Layer pruning has emerged as a strategy to alleviate these inefficiencies, but conventional static pruning methods overlook two critical dynamics inherent to LLM inference: (1) horizontal dynamics, where token-level heterogeneity demands context-aware pruning decisions, and (2) vertical dynamics, where the distinct functional roles of MLP and self-attention layers necessitate component-specific pruning policies.We introduce SkipGPT, a dynamic layer pruning framework designed to optimize computational resource allocation through two core innovations:(1) global token-aware routing to prioritize critical tokens and (2) decoupled pruning policies for MLP and self-attention components.To mitigate training instability, we propose a two-stage optimization paradigm: first, a disentangled training phase that learns routing strategies via soft parameterization to avoid premature pruning decisions, followed by parameter-efficient LoRA fine-tuning to restore performance impacted by layer removal.Extensive experiments demonstrate that SkipGPT reduces over 40% model parameters while matching or exceeding the performance of the original dense model across benchmarks.By harmonizing dynamic efficiency with preserved expressivity, SkipGPT advances the practical deployment of scalable, resource-aware LLMs.Our code is publicly available at: https: //github.com/EIT-NLP/SkipGPT.</p>
<p>Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025.Copyright 2025 by the author(s).Unlike conventional static structured pruning, SkipGPT dynamically prunes layers by considering both horizontal and vertical dynamics.In horizontal dynamics, different tokens receive varying computational allocations.In vertical dynamics, the MLP and attention modules are decoupled to account for their distinct roles within each layer.</p>
<p>Introduction</p>
<p>Large language models (LLMs) are built on a layer-wise Transformer architecture, where each layer consists of a self-attention mechanism followed by a multi-layer perceptron (MLP) (Vaswani et al., 2017).Scaling up model size has driven significant breakthroughs across a wide range of tasks (Brown, 2020;Bommasani et al., 2022;Wei et al., 2023;Zhao et al., 2024;Xin et al., 2025;Chen et al., 2025).However, this progress comes at a steep computational cost, requiring vast resources for inference (Chowdhery et al., 2022;Wan et al., 2024;OpenAI et al., 2024).In contrast, the human brain-despite its 100 trillion synaptic connections, far surpassing even the largest LLMs-operates efficiently on just 30 watts of power (Bartol et al., 2015;Samsi et al., 2023).This stark disparity underscores a fundamental inefficiency in current LLM architectures, highlighting the gap between artificial intelligence and human cognition.</p>
<p>Given their sequential layer-wise structure, LLMs struggle to fully leverage parallelism, even with abundant computational resources.This limitation makes layer pruning a crucial strategy for accelerating inference and improving efficiency (Men et al., 2024;Kim et al., 2024;Gromov et al., 2024;Chen et al., 2024b).While existing pruning methods offer some improvements, they often overlook two key aspects of pruning dynamics (see Figure 1):</p>
<ol>
<li>
<p>Horizontal Dynamics: Different tokens in an input sequence require varying levels of computation.Current methods either allocate resources to the top-k most relevant tokens per layer (Raposo et al., 2024;Zeng et al., 2023) or enforce a fixed computation ratio across all tokens (Jiang et al., 2024).These rigid approaches fail to adapt to token complexity, leading to suboptimal efficiency.To address this, we introduce a global sparsity mechanism, allowing computation budgets to be flexibly distributed across the entire forward pass rather than imposing fixed layer-wise or token-wise constraints.</p>
</li>
<li>
<p>Vertical Dynamics: The MLP and self-attention components within each layer serve distinct functions, yet most pruning methods treat them uniformly.Research suggests that MLPs function like localized neural processes, capturing task-specific interactions (Geva et al., 2021;Meng et al., 2023;Merullo et al., 2024), whereas attention mechanisms resemble higher-level cognitive functions, managing contextual relevance and information flow (Olsson et al., 2022;Kobayashi et al., 2020).Inspired by how the human brain activates different regions for different tasks, we propose decoupling the pruning of MLP and self-attention, enabling more targeted and efficient computation reduction.</p>
</li>
</ol>
<p>To achieve dynamic pruning, a few recent works have explored adaptive computation methods that introduce a router at each Transformer layer (Zeng et al., 2023;Raposo et al., 2024;Jiang et al., 2024).This router functions as a decision module, determining whether specific network units should be executed or skipped during inference.However, these approaches typically optimize the router and model parameters simultaneously in a joint training paradigm, similar to Mixture-of-Experts (MoE) (Lepikhin et al., 2020;Cai et al., 2024;Zhu et al., 2024).However, this method fails to account for a fundamental difference between pruning and pretraining-in the pruning context, the router starts from random initialization, while the model parameters have already converged to an optimal or locally optimal distribution through extensive pretraining.This mismatch may make joint training unstable and prevent dynamic pruning from reaching its full potential.</p>
<p>In this work, we first provide empirical evidence demonstrating the significance of both horizontal dynamics and vertical dynamics in LLMs.To incorporate these two dynamics, we propose SkipGPT, a novel approach that dynamically prunes layers on a per-token basis, adapting the pruning process to the complexity of each token.Furthermore, SkipGPT decouples the MLP and self-attention components within each layer, enabling more granular control over which parts of the model are pruned, thereby optimizing both computational efficiency and model performance.To fully unlock the potential of dynamic pruning, we introduce a Two-stage Training Paradigm.First, in Router Tuning, we freeze the model parameters and optimize only the router, allowing it to identify the most critical computations without disrupting the model's pretrained knowledge.Second, in LoRA Fine-Tuning, we freeze the router and fine-tune the model using LoRA to compensate for any performance degradation caused by pruning.Our results establish SkipGPT as a highly effective pruning strategy-enabling the pruned model to fully restore its performance, even surpassing the original model, despite a 40% reduction in parameters.</p>
<p>Furthermore, since router tuning does not modify the pretrained model parameters, it allows for a direct analysis of module importance in the original model.</p>
<p>Motivation</p>
<p>Measuring module importance using cosine similarity Recent work has demonstrated that the cosine similarity between a module's input and output serves as a reliable metric for evaluating the importance of each module (Men et al., 2024;Gromov et al., 2024).Notably, these studies often define a "module" as an entire Transformer layer.To enable more fine-grained analysis, we refine the definition of a module to refer to either an MLP block or an attention block within a layer.The underlying hypothesis of using cosine similarity is that redundant modules generate outputs that closely resemble their inputs, indicating minimal transformation.Conversely, important modules substantially alter their inputs, suggesting they play a critical role in the model and should be preserved.Prior approaches typically average the cosine similarity over all tokens to derive a general importance metric for each module.However, this aggregation may obscure the variability of module importance across different tokens and contexts.To better understand this variability, we analyze the cosine similarity of each module at the token level.Specifically, the cosine similarity C i,t of the i th module at token t is computed as:
C i,t = x T i,t x i+1,t ∥x i,t ∥ 2 ∥x i+1,t ∥ 2 ,(1)
where ∥ • ∥ 2 is the L2-norm and x i,t denotes the hidden state before module i at token t.To illustrate how module importance varies, we conduct a case study using a randomly selected sentence from the BookCorpus dataset (Zhu et al., 2015).We analyze cosine similarity distributions across 15 consecutive tokens in LLaMA-2-7B (Touvron et al., 2023a), with the results visualized in Figure 2.   The Necessity of Vertical Dynamics Existing pruning methods, whether dynamic or static, typically treat an entire transformer layer as the smallest pruning unit.However, as illustrated in Figure 2, we find that within the same layer, the cosine similarity distributions of attention and MLP modules can vary significantly.For example, in the second layer, the cosine similarity of most tokens in the attention module ranges from 0.1 to 0.4, while the MLP module predominantly falls within the range of 0.4 to 0.7.This indicates that for this layer, the MLP module is almost universally more redundant than the attention module.In the final layer, however, the situation is entirely reversed: all attention modules exhibit cosine similarity values between 0.9 and 1, while MLP modules fall within a completely different range, from 0 to 0.2, making the MLP module far more critical than the attention module at this stage.These findings reveal that even within the same layer, redundancy levels between attention and MLP can vary and shift across layers, underscoring the necessity of decoupling attention and MLP modules for more effective pruning.</p>
<p>The Necessity of Horizontal Dynamics Static pruning methods rely on two key assumptions: (1) the distribution of important modules is uniform across all tokens, and (2) each token is associated with the same number of important modules (Men et al., 2024;Kim et al., 2024;Gromov et al., 2024).Existing dynamic pruning methods also adopt these assumptions when setting compute budgets (Zeng et al., 2023;Raposo et al., 2024;Jiang et al., 2024).However, these assumptions do not hold in practice.First, module importance varies across layers.For instance, at a cosine similarity threshold of 0.8, the attention module in layer 31 is redundant for 15 tokens, whereas in layer 4, it is redundant for only 3-clearly disproving uniform importance.Second, token importance is not uniform within a sequence.In our case study, we analyze the number of modules with a cosine similarity below 0.6-considering them significant-for each token.The results reveal that the token "plan" is associated with 13 important modules, while "an" has only 8, confirming that different tokens require varying levels of computation.</p>
<p>SkipGPT: Dynamic Layer Pruning</p>
<p>After establishing the necessity of horizontal and vertical dynamics, we now introduce SkipGPT, our proposed framework for dynamic layer pruning.In this section, we first outline the necessary preliminaries for understanding SkipGPT's optimization process, followed by an explanation of its sparsity mechanism, routing implementation, loss function, and finally, our two-stage training paradigm for stable and effective learning.</p>
<p>Preliminaries: Gumbel-Softmax and STE</p>
<p>Gumbel-Softmax Reparametrization To optimize SkipGPT's dynamic pruning decisions, we formulate the pruning process as a discrete optimization problem, where each module (MLP or attention) is either executed or skipped based on its computed importance.However, directly optimizing discrete decisions is non-differentiable, making standard gradient-based optimization infeasible.The Gumbel-Softmax distribution is a continuous approximation of the categorical distribution, enabling differentiable sampling (Jang et al., 2022).This is achieved via reparameterization, which transforms discrete samples into differentiable continuous ones for gradient-based optimization.Let π 1 , π 2 , . . ., π k represent the class probabilities of a k-class categorical distribution.To sample from this distribution, the Gumbel-Max trick (Gumbel, 1954;Maddison et al., 2015) selects the category with the highest value of log π i + g i , where g i are i.i.d.samples from the Gumbel distribution Gumbel(0, 1)1 :
z = one hot arg max i [g i + log π i ] .(2)
Since arg max is non-differentiable, the Gumbel-Softmax reparametrization replaces it with a softmax function, producing continuous samples approximating the categorical distribution:
y i = exp log πi+gi τ k j=1 exp log πj +gj τ , i = 1, . . . , k,(3)
where τ controls the sharpness of the distribution.As τ → 0, the samples resemble one-hot vectors, recovering the original arg max operation.This differentiable approximation enables standard backpropagation for optimization.</p>
<p>Straight-Through Estimator</p>
<p>The Straight-Through (ST) Estimator (Bengio et al., 2013) enables discrete sampling while preserving differentiability for backpropagation.In the forward pass, we use Gumbel-Softmax to generate continuous samples.To discretize them, we apply arg max, but in the backward pass, gradients are computed as if using the continuous approximation.This is achieved via:
y hard = y hard − y soft • detach() + y soft ,(4)
where y soft is the continuous Gumbel-Softmax sample.This ensures a one-hot output while gradients follow y soft , enabling efficient optimization despite discrete sampling.</p>
<p>The Concept of Sparsity</p>
<p>To control the total FLOPs, we introduce the concept of sparsity, defined as the fraction of module computations (attention or MLP) skipped in a forward pass, relative to the total computations in a fully dense transformer, accounting for all layers and sequence positions.In our method, sparsity is achieved through dynamic routing, where only a subset of modules is selected for computation in each forward pass.Specifically, assuming that in a forward pass, an L-layer LLM (which consists of 2L modules, each layer containing one attention module and one MLP module) processes a sequence of length S, a dense transformer would compute 2L × S modules, corresponding to a sparsity of 0. When sparsity is greater than 0, only (1−sparsity)×2L×S modules are computed.</p>
<p>Unlike previous work, which defines the compute budget by restricting computation to the top-k tokens at each layer (Raposo et al., 2024;Zeng et al., 2023) or enforcing the same sparsity for each token (Jiang et al., 2024), we allow the computational load to be dynamically allocated across both width (the number of tokens participating in computation at each layer) and height (the number of modules involved in the computation for each token).</p>
<p>Routing Implementation</p>
<p>To enable dynamic allocation, SkipGPT assigns a router before each individual module.Specifically, we route tokens to two computational paths: (1) self-attention (for the attention router) or FFN modules (for the FFN router), and</p>
<p>(2) a residual connection.The latter is computationally inexpensive, producing an output determined entirely by the input, while the former incurs a high computational cost.</p>
<p>Suppose that we have a token embedding x l prior to the l-th transformer module f l , where this module can either be a self-attention or an FFN.Before passing through the module f l , x l first undergoes a router function, which is a simple linear projection, yielding a categorical distribution
r l = W T θ x l ∈ R 2
, where the first element represents the probability of skipping the module, and the second element represents the probability of executing it.</p>
<p>Once this categorical distribution is obtained, a natural routing strategy is the Top-1 routing.Specifically, we have:
x l+1 = r l [1] • (f l (x l ) + x l ), if arg max r l = 1, r l [0] • x l , otherwise,(5)
such that the gradients can be backpropagated.However, this routing strategy challenges precise sparsity control, as r l is merely a soft approximation of binary selection.</p>
<p>By leveraging Gumbel-Softmax and the ST Gumbel Estimator, we effectively address this issue.Specifically, after applying Gumbel-Softmax and the ST Gumbel Estimator to r l , we obtain a one-hot vector g l ∈ {0, 1} 2 .Thus, the input to the next module is computed as:
x l+1 = g l [1] • (f l (x l ) + x l ) + g l [0] • x l .(6)
During the forward pass, discrete binary values are sampled, ensuring clear pruning decisions.In the backward pass, soft probabilities facilitate gradient propagation, allowing the router weights to be updated effectively.</p>
<p>With this routing design, in principle, we can train the router independently without altering the model parameters to obtain the optimal routing solution for a given sparsity ratio.</p>
<p>Loss Function</p>
<p>With our definition in Section 3.2, the sparsity r is given by:
r = t,l g t l [0] S × 2L , (7)
where t is the token index, l represents the module index, L is the total number of layers in the LLM, and S denotes the length of the sequence.To meet different computational demands, we introduce a sparsity regularization term:
L sparsity = |T − r| ,(8)
where |•| denotes absolute value, and T is the user-defined target sparsity.The overall loss function is then given by:
L all = L lm + αL sparsity ,(9)
where L lm is the standard language modeling loss, which represents the negative log-likelihood of predicting the next token on average.The hyperparameter α controls the strength of the sparsity penalty in the overall loss function.2</p>
<p>Two-stage Training Paradigm</p>
<p>As the router starts from random initialization and the LLM has already been pretrained, direct training can lead to unstable and suboptimal pruning decisions.To address this issue, we propose a two-stage training paradigm.</p>
<p>Router Tuning In the initial stage of training, we focus exclusively on tuning the router while keeping all other model parameters frozen.This stage is highly efficient, as it requires adding only a lightweight linear layer before each module, with all router parameters combined accounting for just 0.007% of the total parameters in LLaMA2-7B.Through this stage, we achieve over 90% of the model's original performance even after discarding 25% of the parameters, as shown in Table 1.The forward and backward passes during router tuning are illustrated in Figure 3. Data During both router and LoRA tuning, we use the RedPajama-Data-1T-Sample dataset (Computer, 2023) 3 , which contains 850,000 samples (1 billion tokens) truncated to 4096 tokens each.This dataset serves two roles:</p>
<p>(1) as a calibration set (100 random samples) to compute block-level significance for pruning redundant layers (static methods), and (2) as a training set for dynamic methods and for recovering static method performance (the specific details of static and dynamic methods will be introduced later in Section 4.2).</p>
<p>Training Procedure Each model is trained for 10,000 steps with next-token prediction, using a batch size of 16 in both router and LoRA tuning.In the router tuning stage, we use a constant learning rate of 2e-34 .Additionally, the softmax temperature τ of the Gumbel-Softmax is linearly annealed from 5 to 1.In the LoRA tuning stage, the learning rate is set to 2e-4 with a warmup ratio of 0.1 and a cosine learning rate scheduler.The AdamW optimizer (Loshchilov &amp; Hutter, 2019) with β 1 = 0.9 and β 2 = 0.95 is used for gradient backpropagation.</p>
<p>Benchmarks Following Touvron et al. (2023a), we evaluate accuracy scores on a variety of commonsense reasoning datasets, including BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), HellaSwag (HeSw) (Zellers et al., 2019), Wino-Grande (Sakaguchi et al., 2021), ARC-easy (ARC-E) (Clark et al., 2018), ARC-challenge (ARC-C) (Clark et al., 2018), and OpenbookQA (OBQA) (Mihaylov et al., 2018), using the lm-evaluation-harness (Gao et al., 2024).Additionally, we report zero-shot PPL scores on WikiText2 (WT2) (Merity et al., 2016) and PTB (Marcus et al., 1993).</p>
<p>Baseline</p>
<p>Static Pruning Baselines Static pruning involves the permanent removal of redundant model components.Short-GPT (Men et al., 2024)</p>
<p>Results</p>
<p>This section provides a detailed analysis of the models after each training stage.For clarity, we define the model obtained after the first stage (Router Tuning) as SkipGPT-RT and the final model after the second stage (LoRA Fine-Tuning) as SkipGPT-RT-L.</p>
<p>Comparison Between SkipGPT-RT and Baselines</p>
<p>We present the accuracy and PPL for baseline pruning methods and SkipGPT-RT in Table 1.For all models we evaluate, including LLaMA2-7B, LLaMA2-13B, and LLaMA3.1-8B, the attention module contains approximately half as many parameters as the MLP module.Thus, in Joint Layer Drop and SkipGPT-RT, we prune proportionally more modules to maintain a consistent average parameter ratio across methods.For long-context tasks, the FLOPs of the attention module surpass those of the MLP, making SkipGPT-RT achieve lower computational overhead compared to other baselines.For LLaMA2-7B and LLaMA2-13B, we report evaluation results under a 25% parameter reduction setting, while for LLaMA3.1-8B,we provide results for both 25% and 40% parameter reduction.Additional results under varying pruning ratios are presented in Appendix D.</p>
<p>The results demonstrate that with router tuning alone (without modifying model parameters), SkipGPT-RT significantly outperforms baseline methods.Specifically, for LLaMA2-7B and LLaMA2-13B, SkipGPT-RT retains over 90% of the dense model's performance even under 25% parameter pruning.For LLaMA3.1-8B, it retains over 95% performance at 25% pruning and over 80% performance at 40% pruning -a level at which nearly all baselines collapse catastrophically.Notably, while router tuning involves training the router parameters, unlike static pruning, which permanently removes layers based on predefined metrics, its cost is minimal.This is because the router parameters constitute less than 0.01% of the total model parameters and converge rapidly (Figure 4).In practice, the tuning process requires only a single A800 (80GB) GPU and completes within four hours.Furthermore, the results in Table 1 highlight three key observations:</p>
<p>• Layer-pruning methods-such as ShortGPT, LaCo, Shortened LLaMA, and Joint Layer Drop-generally match or surpass embedding dimension reduction approaches like LLM-Pruner and SliceGPT.This suggests that LLMs have greater redundancy in depth than width, aligning with prior work (Men et al., 2024).This strongly supports focusing on depth pruning when designing SkipGPT.</p>
<p>• Joint Layer Drop, the fine-grained variant of ShortGPT that independently removes attention and MLP, outperforms ShortGPT in most cases.This result validates our motivation for emphasizing the necessity of Vertical Dynamics to achieve more effective pruning.</p>
<p>• SkipGPT-RT significantly outperforms Joint Layer Drop across all models in accuracy (reflecting LLMs' ability as general-purpose task solvers) and perplexity (involving the capability to generate coherent and fluent sentences).This strongly supports our motivation for highlighting the necessity of horizontal dynamics.</p>
<p>Comparison between SkipGPT-RT-L and Baselines</p>
<p>SkipGPT the router struggles to refine its decisions, further degrading both its effectiveness and overall model performance.</p>
<p>Ultimately, this prevents the joint training paradigm from achieving strong results.In contrast, our two-stage training paradigm effectively mitigates these challenges, enabling the pruned model to achieve superior performance compared to existing dynamic pruning methods.</p>
<p>Routing Behavior Analysis</p>
<p>Beyond efficiency, we explore if router tuning in SkipGPT-RT reveals deeper insights into the original LLMs.By analyzing the router's behavior, we aim to uncover meaningful patterns in how different modules contribute to inference.</p>
<p>Comparison of Redundancy between Attention and MLP Modules</p>
<p>Recent studies suggest that LLMs are more resilient to the removal of self-attention layers than feed-forward layers, indicating that the attention modules exhibit higher redundancy compared to MLP modules (Siddiqui et al., 2024;He et al., 2024;He et al., 2025).To explore this further, we analyze five pruned models of LLaMA-2-13B, generated through router tuning under different target sparsity levels T .For each pruned model, we measure the average sparsity of the attention and MLP modules using 50 randomly selected sentences from the WT2 dataset (Merity et al., 2016).Here, sparsity is defined as the ratio of pruned modules to the total number of modules in either the attention or MLP.As shown in Figure 5, SkipGPT-   RT consistently prunes more attention than MLP modules, across both low and high pruning rates.This observation aligns with previous findings and may indicate a structural inefficiency in the current Transformer architecture, which enforces a fixed pairing of one attention and one MLP module per layer.We hypothesize that future LLMs could achieve greater efficiency by revisiting this design and potentially reducing the proportion of attention modules.</p>
<p>Redundancy Shifts in Attention and MLP Modules</p>
<p>Del Corro et al. ( 2023) argues that as the context grows with the sequence, later tokens become more predictive and therefore require less computation.Meanwhile, He et al. (2025) shows that the MLP module has higher redundancy during the decoding phase compared to the pre-filling phase, while attention redundancy remains nearly the same in both phases.We investigate how redundancy shifts within the attention and MLP modules by analyzing a 45% sparsity model obtained through router tuning on LLaMA2-13B.We randomly select sentences from RedPajama, truncate them to the model's maximum context length, and apply a 100-token sliding window across each sentence.At each step, we record the activation ratios of attention and MLP within the sliding window and average the results over 50 examples.The final results appear in Figure 6.Contrary to previous findings, we observe that as context grows, later 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.  tokens require more attention computation but less MLP computation.We hypothesize that in the early stages, the model has not yet determined the task and has limited contextual information.As a result, it relies more on complex MLP computations for task identification and less on attention for context extraction.Later, as the task becomes clear and context accumulates, the model shifts to increased attention computation while reducing MLP computation.Developing techniques that dynamically adjust computation presents an exciting direction for future research.</p>
<p>Conclusion</p>
<p>In this work, we introduce SkipGPT, a novel dynamic pruning framework that addresses the inefficiencies of static layer pruning by incorporating horizontal and vertical dynamics.SkipGPT dynamically allocates computation per token and decouples attention and MLP pruning, enabling more fine-grained optimization.To stabilize training, we propose a two-stage training paradigm, where the router is first tuned alone before fine-tuning the model with LoRA.Experiments show that the pruned model can fully restore its performance, even surpassing the original model despite a 40% parameter reduction.Furthermore, our router analysis reveals key insights into module redundancy and token-level compute allocation, highlighting potential directions for future model efficiency improvements.</p>
<p>Impact Statement</p>
<p>Our work, SkipGPT, introduces a novel dynamic pruning framework that significantly improves the efficiency of large language models (LLMs) by adapting computation to token complexity.By decoupling MLP and attention pruning and introducing a two-stage training paradigm, our approach enhances computational efficiency while preserving, and even surpassing, original model performance.This reduces the energy demands of LLMs, making them more sustainable and accessible for deployment in resource-constrained environments, while also providing new insights into model redundancy and architectural optimization.movies: Towards story-like visual explanations by watching movies and reading books.In The IEEE International Conference on Computer Vision (ICCV), December 2015.</p>
<p>A. Related Work</p>
<p>Static Pruning.Static pruning refers to a kind of approach where the computation of the pruned LLMs remains invariant to the input instances.SparseGPT (Frantar &amp; Alistarh, 2023) simplifies the pruning problem by turning it into large-scale sparse regression tasks, which are efficiently solved using a novel approximate solver.FLAP (An et al., 2024) and LLM-Pruner (Ma et al., 2023) reduce network width by eliminating coupled structures while keeping the number of layers unchanged.Sheared-LLaMA (Xia et al., 2023) learns pruning masks at various granularities, from global ones like layers and hidden dimensions to local ones like attention heads and intermediate dimensions.SliceGPT (Ashkboos et al., 2024) reduces the network's embedding dimension by replacing each weight matrix with a smaller dense matrix.Recent works (Song et al., 2024;Gromov et al., 2024;Kim et al., 2024;Chen et al., 2024a;Yang et al., 2024;Siddiqui et al., 2024) demonstrate that it is possible to selectively drop blocks from a range of pretrained language models, sparking community interest in depth pruning.In this context, Zhang et al. (2024) and He et al. (2024) investigate the decoder layers, treating the self-attention layers and FFN layers as independent components to be pruned separately, and observe a preference for pruning the self-attention layers.Despite significant advancements in static pruning, substantial recovery fine-tuning is often necessary to preserve performance post-pruning, making the process both costly and challenging to scale.Dynamic Pruning.Dynamic pruning refers to another kind of approach where the pruning of unimportant layers is dynamically determined based on the specific input instance.A common technique in this context is Early Exit (Schuster et al., 2022;Varshney et al., 2023;Del Corro et al., 2023;Yom Din et al., 2024;Chen et al., 2024c;Fan et al., 2024).This approach dynamically evaluates whether to continue processing subsequent transformer blocks.Notably, transformer blocks that produce predictions matching the final token output of LLMs are typically located towards the end of the model.As a result, extensive training is often required to adapt LLMs for the effective use of early exit mechanisms.Therefore, early exit strategies have been rarely explored in larger SOTA LLMs.Another prominent technique is Skip Layer, which dynamically skips the execution of intermediate layers (or modules) for a given input token.This is achieved through mechanisms such as a gating function (Wang et al., 2018;Raposo et al., 2024) or a binary router (Zeng et al., 2023;Jiang et al., 2024).For instance, Mixture-of-Depths (MoD) (Raposo et al., 2024) determines which tokens to process using a top-k routing mechanism.Essentially, SkipGPT aligns with the Skip Layer paradigm, where the execution of layers for each input token is dynamically determined.To the best of our knowledge, ours is the first work to simultaneously address both horizontal and vertical dynamics.</p>
<p>B. Training Algorithm of SkipGPT</p>
<p>The detailed two-stage training paradigm is outlined in Algorithm 1.</p>
<p>C. Detailed Descriptions of All Baseline Methods</p>
<p>ShortGPT is a structured pruning method that removes redundant layers in LLMs based on a novel importance metric called Block Influence (BI).By analyzing hidden state transformations, ShortGPT assigns BI scores to measure each layer's contribution to model performance.Layers with lower BI scores are identified as redundant and pruned in ascending order.This simple yet effective approach significantly reduces model size and inference cost while maintaining competitive performance.Unlike complex pruning techniques, ShortGPT demonstrates that LLMs contain substantial layer-wise redundancy, enabling efficient compression without additional fine-tuning.</p>
<p>Shortened LLaMA is a depth pruning method that reduces the computational cost of LLMs by removing entire Transformer layers while keeping the remaining architecture intact.It determines layer importance using Perplexity (PPL) and Taylor Expansion, pruning less significant layers in a one-shot manner.Unlike width pruning, which reduces weight matrices but struggles to accelerate inference under memory constraints, Shortened LLaMA achieves significant speedups, particularly in resource-limited settings.To recover pruned models, it employs LoRA-based fine-tuning for moderate pruning and Continued Pretraining (CPT) for aggressive pruning.This method provides an efficient way to compress LLMs while maintaining strong performance.</p>
<p>LaCo (Layer Collapse) is a structured pruning method that progressively merges deeper layers into earlier ones, reducing model depth while preserving output representations.Instead of removing layers outright, LaCo employs Reserving-Differences-while-Seeking-Common (RDSC) Layer Merge, which integrates parameter differences from consecutive layers interdependent components are grouped and pruned together to minimize disruption.Importance estimation is performed using gradient-based and Hessian-based metrics, ensuring the least impactful components are removed.Unlike traditional compression methods that require extensive retraining, LLM-Pruner enables efficient post-training recovery using LoRAbased fine-tuning, requiring only 50K public samples.This approach significantly reduces computational overhead while maintaining strong zero-shot performance across multiple tasks.</p>
<p>SliceGPT is a post-training sparsification method that reduces the computational and memory demands of LLMs by removing entire rows and columns of weight matrices, effectively shrinking the embedding dimension while maintaining dense matrix operations for efficient execution.It leverages computational invariance, applying orthogonal matrix transformations to ensure minimal performance degradation.Using Principal Component Analysis (PCA), SliceGPT projects activation signals onto their principal components, allowing redundant dimensions to be pruned.This method achieves up to 30% compression on LLaMA-2, OPT, and Phi-2 models while retaining over 90% of the original model's accuracy, leading to significant inference speedups and reduced hardware requirements without additional fine-tuning.</p>
<p>Mixture-of-Depths (MoD) is a conditional computation method that dynamically allocates compute across model depth, reducing unnecessary computations in transformer-based LLMs.Unlike standard transformers, which apply the same amount of compute to all tokens at every layer, MoD employs a top-k routing mechanism to selectively process only the most important tokens in each layer, skipping others via residual connections.This allows the model to optimize compute expenditure per token, maintaining performance while significantly reducing FLOPs per forward pass.By ensuring a static computation graph with predictable efficiency gains, MoD achieves up to 50% faster inference while matching or surpassing the performance of equivalent full-compute transformers.D-LLM is a dynamic inference framework for LLMs that adaptively allocates computational resources at the token level, optimizing efficiency without compromising performance.It introduces a dynamic decision module before each transformer layer, determining whether a token should execute the layer or be skipped, thereby reducing unnecessary computation for less critical tokens and simpler tasks.To maintain compatibility with KV-cache mechanisms, D-LLM employs a KV-cache eviction policy, excluding skipped layers from subsequent attention calculations, which not only reduces storage overhead but also ensures smooth deployment in real-world applications.Experimentally, D-LLM achieves up to 50% reduction in computational cost across various NLP tasks while maintaining strong accuracy, making it a highly effective solution for resource-constrained environments.</p>
<p>D. Exploring Pruning Scaling Laws with SkipGPT-RT</p>
<p>Due to its significantly superior performance over the baseline, we believe that SkipGPT-RT is capable of identifying optimal or near-optimal routing solutions for a given sparsity ratio.This capability establishes SkipGPT not only as an effective pruning method but also as a reliable probing tool for exploring the pruning scaling laws in existing LLMs.To this end, in this section, we employ SkipGPT-RT as a "probe" to analyze both LLaMA2-7B and LLaMA2-13B, using the SOTA static pruning method, Joint Layer Drop, for comparison.Figure 7 highlights several key findings:</p>
<p>• Static pruning methods lack scalability: While static pruning methods are simple and effective in some scenarios, their ability to preserve model performance diminishes significantly under high sparsity levels.For instance, in the case of LLaMA2-13B, SkipGPT-RT demonstrates significantly lower PPL even at 70% sparsity, outperforming Joint Layer Drop, which struggles to maintain comparable performance even at a much lower sparsity of 50%.</p>
<p>• The surprising redundancy in LLMs: Our analysis reveals that LLMs exhibit far greater redundancy than expected.For instance, LLaMA2-13B only begins to show a noticeable increase in PPL at an 80% sparsity level.This phenomenon may stem from two key factors: (1) the inefficiency of the current transformer architecture, which applies uniform computation to all tokens regardless of their importance, and (2) the concentration of critical information within a small subset of modules during pretraining.We hope this finding offers valuable insights for designing future architectures and pretraining strategies.</p>
<p>E. Additional Case Studies on Motivation</p>
<p>In this section, we present additional case studies that illustrate Token-Wise Cosine Similarities Across Modules in LLaMA2-7B, as shown in Figures 8 and 9.For LLaMA2-13B, see Figures 10 and 11.</p>
<p>Figure 1 .
1
Figure1.An overview of SkipGPT.Unlike conventional static structured pruning, SkipGPT dynamically prunes layers by considering both horizontal and vertical dynamics.In horizontal dynamics, different tokens receive varying computational allocations.In vertical dynamics, the MLP and attention modules are decoupled to account for their distinct roles within each layer.</p>
<p>Figure 2 .
2
Figure 2. Token-Wise Cosine Similarities Across Modules in LLaMA-2-7B, which consists of 32 layers, corresponding to 64 modules in total.Due to space constraints, we showcase only the results for the initial and final modules.Higher values indicate greater redundancy.</p>
<p>Figure 3 .
3
Figure3.Illustration of the forward and backward passes during the router tuning stage.In the forward pass, the router makes hard decisions to either execute (1) or skip (0).In the backward pass, the gradients are propagated back using soft probabilities.</p>
<p>LoRA Fine-Tuning (Optional) While the router tuning stage is sufficient to preserve most of the model's performance, we provide an optional LoRA fine-tuning stage for those aiming to fully restore performance to the original model level.Low-Rank Adaptation (LoRA)(Hu et al., 2021) enables efficient refinement of LLMs with minimal computational overhead.Previous works, such asMa et al. (2023);Kim et al. (2024), have demonstrated LoRA's effectiveness in enhancing statically pruned models.In this study, we show that LoRA can also effectively recover the performance of dynamically pruned models.For a clearer understanding of the entire training process, please refer to the algorithm illustrated in Appendix B.4. Experimental Configuration4.1.Models and BenchmarksModels We conduct experiments utilizing LLaMA2-7B, LLaMA2-13B, and LLaMA3.1-8B(Touvron et al., 2023a;b;Dubey et al., 2024).</p>
<p>Figure 4 .
4
Training loss curves of LoRA-finetuned static and dynamic pruning baselines, and the two-stage training of SkipGPT.</p>
<p>Figure 5 .
5
Figure 5. Average sparsity of the attention and MLP modules in five pruned models of LLaMA-2-13B, generated through router tuning under different target sparsity levels T .</p>
<p>Figure 6 .
6
Figure 6.Redundancy shifts in attention and MLP modules of a 45% sparsity LLaMA2-13B model obtained via router tuning as context length grows.</p>
<p>Figure 7 .
7
Figure 7. Perplexity (PPL) of Joint Layer Drop and SkipGPT-RT under different sparsity levels.</p>
<p>Figure 8 .
8
Figure 8. Token-Wise Cosine Similarities Across Modules in LLaMA-2-7B.</p>
<p>Figure 10 .Figure 11 .
1011
Figure 10.Token-Wise Cosine Similarities Across Modules in LLaMA-2-13B.</p>
<p>.00 1.00 1.00 0.99 0.99 0.97 0.99 0.98 0.98 0.98 0.99 0.99 0.97 0.97 0.98 0.95 0.94 0.95 0.87 0.86 0.88 0.94 0.87 0.89 0.88 0.89 0.90 0.86 0.87 0.90 0.99 0.99 0.99 0.98 0.95 0.92 0.98 0.98 0.98 0.98 0.99 0.99 0.97 0.97 0.95 0.77 0.80 0.85 0.81 0.75 0.75 0.85 0.68 0.58 0.74 0.78 0.66 0.67 0.67 0.70 0.94 0.97 0.97 0.96 0.95 0.94 0.97 0.97 0.96 0.96 0.94 0.96 0.96 0.95 0.96 0.00 0.01 0.07 0.10 0.04 0.05 0.12 0.04 0.01 0.08 0.08 0.04 0.05 0.05 0.09
MLP 32 Attn 32 MLP 31 Attn 31 MLP 30 Attn 30...Attn 1 MLP 1 Attn 2 MLP 2 Attn 3 MLP 3 Attn 4 MLP 4 Attn 5 MLP 50.06 0.12 0.09 0.29 0.17 0.34 0.26 0.38 0.37 0.31 0.21 0.24 0.30 0.35 0.23 0.31 0.51 0.59 0.42 0.37 0.54 0.42 0.56 0.70 0.66 0.58 0.57 0.45 0.46 0.51 0.92 0.89 0.64 0.86 0.69 0.76 0.51 0.75 0.79 0.79 0.83 0.58 0.68 0.87 0.87 0.52 0.58 0.59 0.67 0.15 0.63 0.41 0.56 0.70 0.71 0.64 0.60 0.58 0.62 0.66 0.82 0.80 0.77 0.77 0.88 0.77 0.64 0.70 0.71 0.75 0.66 0.77 0.76 0.72 0.74 0.56 0.56 0.46 0.59 0.29 0.53 0.40 0.46 0.60 0.65 0.57 0.60 0.50 0.51 0.54 0.92 0.87 0.77 0.67 0.77 0.63 0.63 0.59 0.18 0.48 0.76 0.68 0.83 0.57 0.44 0.32 0.50 0.50 0.59 0.51 0.31 0.43 0.38 0.34 0.44 0.49 0.50 0.42 0.42 0.38t h a th a db e e nm e ga n'sp l a n w h e ns h eg o th i m d r e s s e d e a r l i e r.
10.01 0.04 0.09 0.23 0.05 0.03 0.02 0.23 0.12 0.08 0.22 0.17 0.45 0.36 0.05 0.21 0.39 0.50 0.45 0.73 0.49 0.71 0.66 0.72 0.70 0.70 0.72 0.61 0.62 0.63</p>
<p>Table 1 .
1
Comparison
ModelMethodRatioOBQA WinoGrande PIQA HeSw BoolQ ARC-E AccNorm Acc Acc AccNorm Acc AccNorm AccNorm ARC-C Avg. Acc.↑WT2 PPLPTB PPLAvg. PPL↓Dense0.00%44.2074.1978.0778.9371.6281.3652.4768.695.4720.8313.15ShortGPT25.0%34.8068.4367.6860.7762.1759.3438.4055.9425.4270.9748.20Shortened-PPL25.0%33.6052.8870.4055.1261.0755.0529.4451.0811.1549.0730.11Shortened-Taylor 25.0%35.4066.3066.9759.9062.1759.7638.6555.5923.7569.6046.68LLaMA2-7BJoint Layer Drop 23.9%38.6072.3870.0867.9340.3765.5744.5457.0729.1964.6946.94LaCo25.0%36.6067.3265.7262.7674.3758.9238.1457.6918.9647.5833.27LLM-Pruner25.3%39.0058.5673.4560.7754.7158.6337.0354.5914.1065.0939.60SliceGPT25.4%35.4065.8266.3853.4350.4369.5740.1054.457.5676.2941.93SkipGPT-RT25.5%39.6063.5472.2070.9668.8176.5244.3762.297.8130.5819.20Dense0.00%45.2076.1679.1182.2380.5284.6859.4772.484.8828.9216.90ShortGPT25.0%40.6070.8071.3872.5962.6969.1945.3161.7920.0549.5234.79Shortened-PPL25.0%39.4067.1773.1269.3162.5769.1941.1360.278.4554.6231.54Shortened-Taylor 25.0%41.8070.8070.7862.5838.1061.9938.3154.9024.4659.3441.90LLaMA2-13BJoint Layer Drop 24.3%41.8072.6673.3774.4370.2471.2546.5064.6913.3142.6327.97LaCo25.0%38.8062.7572.7463.1144.4662.8435.0754.2513.4053.9233.66LLM-Pruner24.9%44.0065.8276.9974.1559.8872.6045.8262.759.8271.4940.66SliceGPT24.7%38.6068.4364.4250.9738.8769.3640.3653.007.4399.8953.66SkipGPT-RT25.4%46.0071.9076.8874.3374.3777.6947.0866.896.7841.6924.24Dense0.00%44.8077.5180.0381.9582.1484.8557.5972.696.2410.588.41ShortGPT25.0%28.0054.1458.7631.5037.7738.0531.4039.952796.242799.462797.85Shortened-PPL25.0%33.6053.5171.8757.9842.0857.0731.7449.6915.0023.8619.43Shortened-Taylor 25.0%28.2054.0658.8731.5337.7738.0531.3139.972690.342793.252,741.80LLaMA3.1-8BJoint Layer Drop 24.2%33.0055.5661.2642.2637.3135.0630.1242.0832.3251.4041.86LaCo24.5%31.2065.1166.1655.7771.1348.6537.0353.5830.1450.3540.25LLM-Pruner24.5%37.2056.9972.0354.7556.7951.2231.3151.4725.2145.5935.40SliceGPT24.6%30.4055.1757.8338.1937.8338.6425.8540.5617.2563.1340.19SkipGPT-RT25.5%44.2075.6978.0776.8774.0682.4453.4169.2516.4726.9121.69Dense0.00%44.8077.5180.0381.9582.1484.8557.5972.696.2410.588.41ShortGPT40.6%30.0057.6258.5434.5562.2934.8129.1043.8479856.66 125507.27 102681.97Shortened-PPL40.6%27.0052.4161.4841.7457.1339.6926.4543.70157.01196.04176.53Shortened-Taylor 40.6%28.8053.2059.6837.8958.5335.1929.6943.2878424.35 125435.43 101,929.90LLaMA3.1-8BJoint Layer Drop 39.9%27.6050.1253.4826.6537.8626.3525.7735.40251.52341.25296.39LaCo40.7%27.6051.5456.3131.5355.1130.2225.8539.74269.24392.50330.87LLM-Pruner39.9%29.2051.3064.0936.2350.5236.8723.4641.67152.98161.49157.24SliceGPT39.9%25.6051.6253.9230.6437.8331.8222.1036.22143.24183.21163.23SkipGPT-RT40.2%38.0059.3573.3464.3660.3777.5345.6559.8071.2548.0559.65
of SkipGPT-RT and static pruning baselines without LoRA, where the ratio denotes the proportion of parameters (averaged per token) that do not participate in computations relative to the original model's total parameter count.</p>
<p>MoD-D allocates its computation budget layer-wise, while D-LLM allocates it token-wise.Regarding vertical dynamics, D-LLM does not decouple attention and MLP modules.Please refer to Appendix C for detailed descriptions of these baselines.
,2024) shrinks the embedding dimension by replacing largeweight matrices with smaller dense matrices. Unlike staticpruning methods, SkipGPT considers horizontal dynamics,offering a more adaptive approach.Dynamic Pruning Baselines Dynamic pruning allows forthe selective activation of model components during infer-ence. MoD (Raposo et al., 2024) employs a top-k routingmechanism to dynamically activate layers for each token.To ensure a fair comparison, we introduce a new variant,MoD-D, which differs from MoD by decoupling attentionand MLP modules, whereas MoD treats them as a singleunit.
(Ma et al., 2023)4)ayers based on Block Influence (BI) scores.Shortened LLaMA(Kim et al., 2024)prunes layers using PPL and Taylor Expansion, resulting in two variants: Shortened-PPL and Shortened-Taylor.LaCo(Yang et al., 2024)collapses layers progressively from deep to shallow, employing a threshold to prevent excessive merging.Joint Layer Drop(He et al., 2024) is a fine-grained variant of ShortGPT that separately removes attention and MLP layers based on the same BI metric.LLM-Pruner(Ma et al., 2023)selectively removes non-critical structures using gradient-based criteria.SliceGPT(Ashkboos et al.D-LLM (Jiang et al., 2024)introduces a dynamic decision module at each transformer layer, determining whether a layer should be executed or skipped.Additionally, we include SkipGPT-Joint, which adopts a joint training paradigm instead of two-stage training.From the perspective of hori-zontal dynamics,</p>
<p>Table 2 .
2
Comparison of SkipGPT-RT-L against LoRA-finetuned static pruning and dynamic pruning baselines.LoRA results for shortened-PPL, Shortened-Taylor, and SliceGPT on LLaMA2-13B are omitted as these methods fail to converge during recovery training.
-RT-L ≈ original model performance Fig-ure 4 shows training curves for SkipGPT-RT, SkipGPT-RT-L, static pruning methods with LoRA, and jointrouter+LoRA training for dynamic pruning for LLaMA2-7B and LLaMA2-13B. Results are summarized in Table 2.The Effectiveness of Two-Stage Training Paradigm Wedesignate SkipGPT-Joint, which refers to the variant whereboth the router parameters and model parameters are trainedsimultaneously, as the ablation baseline for the two-stagetraining paradigm. As shown in Table 2, its performance issignificantly worse than SkipGPT. While it may seem intu-itive that directly adopting MoE's joint training paradigmcould outperform two-stage training-since the router andLoRA parameters can gradually adapt to each other's rep-resentations-the results tell a different story. Not onlydoes SkipGPT-Joint underperform, but dynamic pruningmethods following the joint training paradigm consistentlyfail to achieve satisfactory results. This finding reinforcesour argument in Section 3.5: in the joint training paradigm,the randomly initialized router forces model parameters toadapt early on to a suboptimal, random routing strategy.
Notably, SkipGPT-RT achieves performance comparable to the best fine-tuned baseline using router tuning alone.After LoRA fine-tuning, our method not only fully restores the model's performance to the original level but even surpasses it (without fine-tuning), ranking second only to directly applying LoRA to the original model for LLaMA2-7B.For those aiming to quickly build a high-performing pruned model, router tuning is highly effective.Meanwhile, to fully match or exceed the original model, LoRA fine-tuning is an excellent choice.It is worth reiterating that the entire process for all models requires only a single A800 GPU.This misalignment disrupts the model's established parameter distribution, making it increasingly difficult for the router to identify critical modules.Over time, this leads to a reinforcing feedback loop: as the model adapts to poor routing,</p>
<p>.51 0.41 0.49 0.56 0.61 0.56 0.46 0.38 0.58 0.51 0.59 0.66 0.56 0.59 0.93 0.78 0.72 0.62 0.55 0.80 0.58 0.75 0.57 0.65 0.72 0.66 0.73 0.76 0.82 0.29 0.42 0.54 0.41 0.42 0.51 0.49 0.36 0.41 0.46 0.43 0.47 0.57 0.60 0.55
Attn 1 MLP 1 Attn 2 MLP 2 Attn 3 MLP 3 Attn 4 MLP 4 MLP 5 Attn 50.01 0.02 0.05 0.20 0.00 0.24 0.33 0.02 0.35 0.07 0.34 0.04 0.04 0.01 0.25 0.22 0.21 0.55 0.52 0.52 0.48 0.60 0.74 0.65 0.71 0.63 0.74 0.78 0.75 0.69 0.14 0.21 0.14 0.20 0.44 0.32 0.41 0.32 0.28 0.25 0.36 0.38 0.29 0.35 0.31 0.44 0.50 0.43 0.54 0.48 0.51 0.49 0.61 0.51 0.54 0.50 0.68 0.67 0.54 0.44 0.94 0.79 0.59 0.67 0.70 0.57 0.78 0.56 0.71 0.78 0.77 0.51 0.65 0.77 0.81 0.56 0.60 0.47 0.48 0.78 0.71 0.74 0.49 0.53 0.67 0.55 0.60 0.66 0.66 0.58 0.84 0.67 0.61 0.60 0.49 0.63 0.72 0.59 0.51 0.72 0.62 0.52 0.62 0.64 0.73 0.60 0</p>
<p>.16 0.00 0.12 0.13 0.35 0.01 0.26 0.02 0.00 0.30 0.07 0.16 0.04 0.10 0.23 0.34 0.60 0.61 0.69 0.53 0.69 0.62 0.77 0.64 0.68 0.61 0.73 0.73 0.79 0.13 0.11 0.25 0.21 0.23 0.32 0.25 0.37 0.09 0.46 0.44 0.45 0.22 0.40 0.34 0.47 0.53 0.53 0.50 0.55 0.52 0.59 0.42 0.34 0.61 0.57 0.68 0.23 0.67 0.60 0.93 0.89 0.25 0.48 0.63 0.71 0.22 0.75 0.60 0.73 0.84 0.82 0.58 0.62 0.81 0.54 0.53 0.39 0.47 0.51 0.66 0.39 0.75 0.59 0.72 0.63 0.68 0.61 0.62 0.56 0.77 0.73 0.69 0.66 0.68 0.72 0.62 0.81 0.65 0.69 0.78 0.77 0.71 0.60 0.78 0.63 0.56 0.43 0.43 0.56 0.55 0.55 0.63 0.56 0.73 0.56 0.61 0.51 0.61 0.48 0.92 0.84 0.67 0.71 0.61 0.83 0.71 0.81 0.66 0.49 0.61 0.56 0.45 0.32 0.57 0.30 0.51 0.44 0.37 0.48 0.46 0.47 0.50 0.46 0.55 0.46 0.41 0.43 0.49 0.45Figure 9. Token-Wise Cosine Similarities Across Modules in LLaMA-2-7B.
Attn 1 MLP 5 Attn 5 MLP 4 Attn 4 MLP 3 Attn 3 MLP 2 Attn 2 MLP 1 Attn 38 MLP 38 Attn 39 MLP 39 Attn 40 MLP 40 Attn 1 MLP 1 Attn 2 MLP 2 Attn 3 MLP 3 Attn 4 MLP 4 Attn 5 MLP 50.6 0.8 1.0 0.02 00.0 0.95 0.98 0.99 0.96 0.99 0.98 0.98 0.97 0.98 0.96 0.96 0.98 0.91 0.92 0.91 0.98 0.93 0.94 0.94 0.93 0.95 0.89 0.92 0.93 0.98 0.98 0.96 0.96 0.99 0.98 0.98 0.97 0.98 0.96 0.98 0.98 0.81 0.87 0.84 0.90 0.82 0.86 0.87 0.85 0.86 0.79 0.88 0.82 0.90 0.94 0.95 0.29 0.97 0.96 0.96 0.96 0.96 0.94 0.94 0.95 0.40 0.42 0.38 0.35 0.42 0.49 0.53 0.54 0.49 0.47 0.47 0.41 ... 0.2 0.4 0.03 0.01 0.05 0.01 0.02 0.03 0.12 0.02 0.16 0.20 0.16 0.16 0.28 0.11 0.15 0.12 0.19 0.22 0.35 0.18 0.44 0.42 0.38 0.28 0.72 0.60 0.61 0.36 0.40 0.61 0.55 0.56 0.54 0.54 0.62 0.43 0.59 0.47 0.64 0.47 0.24 0.62 0.57 0.65 0.63 0.56 0.59 0.56 0.81 0.60 0.59 0.51 0.37 0.51 0.67 0.60 0.65 0.46 0.61 0.66 0.49 0.51 0.66 0.34 0.36 0.35 0.45 0.53 0.61 0.36 0.58 0.60 0.72 0.44 0.69 0.64 0.75 0.58 0.58 0.46 0.56 0.71 0.62 0.63 0.34 0.49 0.57 0.57 0.40 0.47 0.47 0.57 0.56 0.53 0.64 0.54 0.94 0.81 0.83 0.83 0.82 0.78 0.85 0.70 0.73 0.82 0.69 0.86 0.55 0.55 0.65 0.64 0.55 0.47 0.57 0.57 0.46 0.46 0.54 0.57`à r en'ty o ub e i n gag o o db o y?' '
g can be obtained via inverse transform sampling: u ∼ Uniform(0, 1), g = − log(− log(u)).
If α is too small, it may fail to enforce the desired sparsity. If α is too large, the model may compromise the optimization of Llm. Based on our experiments, a value of 8 works well.
https://huggingface.co/datasets/ togethercomputer/RedPajama-Data-1T-Sample
A grid-search was conducted to determine that a learning rate of 2e-3 optimally ensures training stability.
Algorithm 1 Training Process of SkipGPTRequire: Pretrained model M , dataset D, target sparsity T , router parameters θ, learning rates η 1 (router) and η 2 (LoRA), maximum steps S 1 and S 2 1: Initialize router parameters θ for each module 2: Initialize LoRA parameters (Optional) 3: Stage 1: Router Tuning 4: for step s = 1 to S 1 do 5:for token t in X do 7:for module l = 1 to L do 8: r t l ← W T θ x t l with ∂L all ∂W θ being active 9:g t l ∼ Gumbel-Softmax(r t l )10:end for 12:end for 13:for token t in X do 21:if a t l = 1 then 25:x t l+1 ← f l (x t l ) + x t l 26:else 27:x t l+1 ← x t l 28:end if29:end for 30:end for 31:Compute L lm 32:Update LoRA parameters using η 2 and ∇ LoRA L lm 33: end for Return: Pruned model M ′ into a prior layer, maintaining structural integrity.To minimize performance degradation, it uses few-shot calibration samples to ensure representation similarity.This approach enables 30-50% layer reduction without retraining, significantly lowering computational costs while retaining strong performance.Additionally, post-training on pruned models further restores accuracy, making LaCo a highly effective structured pruning technique for large language models.Joint Layer Drop is a structured pruning method that enhances the efficiency of Transformer-based LLMs by jointly pruning Attention and MLP layers based on a similarity-based metric.This method first removes redundant Attention layers, as they exhibit significant redundancy while maintaining performance.Once the least important Attention layers are pruned, it selectively removes MLP layers to further compress the model.By dynamically balancing the pruning of both components, Joint Layer Drop achieves higher compression ratios with minimal accuracy degradation, making it a highly effective approach for structured model reduction.LLM-Pruner is a structured pruning method designed for task-agnostic compression of LLMs, aiming to reduce model size while preserving their general-purpose capabilities.It employs dependency-based structural pruning, where
Fluctuationbased adaptive structured pruning for large language models. Y An, X Zhao, T Yu, M Tang, J Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>S Ashkboos, M L Croci, M G D Nascimento, T Hoefler, J Hensman, Slicegpt, arXiv:2401.15024Compress large language models by deleting rows and columns. 2024arXiv preprint</p>
<p>Nanoconnectomic upper bound on the variability of synaptic plasticity. eLife, 4:e10778. Bartol, M Thomas, J Bromer, C Kinney, J Chirillo, M A Bourne, J N Harris, K M Sejnowski, T J , 10.7554/eLife.10778nov 2015</p>
<p>Estimating or propagating gradients through stochastic neurons for conditional computation. Y Bengio, Léonard, A Courville, arXiv:1308.34322013arXiv preprint</p>
<p>Piqa: Reasoning about physical commonsense in natural language. Y Bisk, R Zellers, J Gao, Y Choi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, E Brynjolfsson, S Buch, D Card, R Castellon, N Chatterji, A Chen, K Creel, J Q Davis, D Demszky, C Donahue, M Doumbouya, E Durmus, S Ermon, J Etchemendy, K Ethayarajh, L Fei-Fei, C Finn, T Gale, L Gillespie, K Goel, N Goodman, S Grossman, N Guha, T Hashimoto, P Henderson, J Hewitt, D E Ho, J Hong, K Hsu, J Huang, T Icard, S Jain, D Jurafsky, P Kalluri, S Karamcheti, G Keeling, F Khani, O Khattab, P W Koh, M Krass, R Krishna, R Kuditipudi, A Kumar, F Ladhak, M Lee, T Lee, J Leskovec, I Levent, X L Li, X Li, T Ma, A Malik, C D Manning, S Mirchandani, E Mitchell, Z Munyikwa, S Nair, A Narayan, D Narayanan, B Newman, A Nie, J C Niebles, H Nilforoshan, J Nyarko, G Ogut, L Orr, I Papadimitriou, J S Park, C Piech, E Portelance, C Potts, A Raghunathan, R Reich, H Ren, F Rong, Y Roohani, C Ruiz, J Ryan, C Ré, D Sadigh, S Sagawa, K Santhanam, A Shih, K Srinivasan, A Tamkin, R Taori, A W Thomas, F Tramèr, R E Wang, W Wang, B Wu, J Wu, Y Wu, S M Xie, M Yasunaga, J You, M Zaharia, M Zhang, T Zhang, X Zhang, Y Zhang, L Zheng, K Zhou, Liang, 2022P. On the opportunities and risks of foundation models</p>
<p>Language models are few-shot learners. T B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>A survey on mixture of experts. W Cai, J Jiang, F Wang, J Tang, S Kim, J Huang, 2024</p>
<p>Compressing large language models by streamlining the unimportant layer. X Chen, Y Hu, J Zhang, arXiv:2403.191352024aarXiv preprint</p>
<p>Compressing large language models by streamlining the unimportant layer. X Chen, Y Hu, J Zhang, arXiv:2403.191352024barXiv preprint</p>
<p>Unveiling the key factors for distilling chain-of-thought reasoning. X Chen, Z Sun, W Guo, M Zhang, Y Chen, Y Sun, H Su, Y Pan, D Klakow, W Li, X Shen, 2025</p>
<p>Eellm: Large-scale training and inference of early-exit large language models with 3d parallelism. Y Chen, X Pan, Y Li, B Ding, J Zhou, 2024c</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, P Schuh, K Shi, S Tsvyashchenko, J Maynez, A Rao, P Barnes, Y Tay, N Shazeer, V Prabhakaran, E Reif, N Du, B Hutchinson, R Pope, J Bradbury, J Austin, M Isard, G Gur-Ari, P Yin, T Duke, A Levskaya, S Ghemawat, S Dev, H Michalewski, X Garcia, V Misra, K Robinson, L Fedus, D Zhou, D Ippolito, D Luan, H Lim, B Zoph, A Spiridonov, R Sepassi, D Dohan, S Agrawal, M Omernick, A M Dai, T S Pillai, M Pellat, A Lewkowycz, E Moreira, R Child, O Polozov, K Lee, Z Zhou, X Wang, B Saeta, M Diaz, O Firat, M Catasta, J Wei, K Meier-Hellstern, D Eck, J Dean, S Petrov, N Fiedel, Palm, Scaling language modeling with pathways. 2022</p>
<p>Exploring the surprising difficulty of natural yes/no questions. C Clark, K Lee, M.-W Chang, T Kwiatkowski, M Collins, K Toutanova, Boolq, arXiv:1905.100442019arXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. P Clark, I Cowhey, O Etzioni, T Khot, A Sabharwal, C Schoenick, O Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>Redpajama: an open dataset for training large language models. T Computer, 2023</p>
<p>L Del Corro, A Del Giorno, S Agarwal, B Yu, A Awadallah, S Mukherjee, Skipdecode, arXiv:2307.02628Autoregressive skip decoding with batching and caching for efficient llm inference. 2023arXiv preprint</p>
<p>The llama 3 herd of models. A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Not all layers of llms are necessary during inference. S Fan, X Jiang, X Li, X Meng, P Han, S Shang, A Sun, Y Wang, Z Wang, 2024</p>
<p>Massive language models can be accurately pruned in one-shot. E Frantar, D Alistarh, Sparsegpt, International Conference on Machine Learning. PMLR2023</p>
<p>A framework for few-shot language model evaluation. L Gao, J Tow, B Abbasi, S Biderman, S Black, A Dipofi, C Foster, L Golding, J Hsu, A Le Noac'h, H Li, K Mcdonell, N Muennighoff, C Ociepa, J Phang, L Reynolds, H Schoelkopf, A Skowron, L Sutawika, E Tang, A Thite, B Wang, K Wang, A Zou, </p>
<p>Transformer feed-forward layers are key-value memories. M Geva, R Schuster, J Berant, O Levy, 2021</p>
<p>A Gromov, K Tirumala, H Shapourian, P Glorioso, D A Roberts, arXiv:2403.17887The unreasonable ineffectiveness of the deeper layers. 2024arXiv preprint</p>
<p>Statistical theory of extreme values and some practical applications: a series of lectures. E J Gumbel, US Government Printing Office. 331954</p>
<p>What matters in transformers? not all attention is needed. S He, G Sun, Z Shen, A Li, 2024</p>
<p>Adaskip: Adaptive sublayer skipping for accelerating long-context llm inference. Z He, Yizhen Yao, Pengfei Zuo, Bin Gao, Qinya Li, Zhenzhe Zheng, Fan Wu, 2025</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, Lora, Low-rank adaptation of large language models. 2021</p>
<p>Categorical reparameterization with gumbel-softmax. E Jang, S Gu, B Poole, International Conference on Learning Representations. 2022</p>
<p>D-llm: A token adaptive computing resource allocation strategy for large language models. Y Jiang, H Wang, L Xie, H Zhao, H Zhang Chao, Qian, J C Lui, Proceedings of the 38th Conference on Neural Information Processing Systems. the 38th Conference on Neural Information Processing SystemsNeurIPS 2024. 2024</p>
<p>B.-K Kim, G Kim, T.-H Kim, T Castells, S Choi, J Shin, H.-K Song, arXiv:2402.02834Shortened llama: A simple depth pruning for large language models. 202411arXiv preprint</p>
<p>Attention is not only a weight: Analyzing transformers with vector norms. G Kobayashi, T Kuribayashi, S Yokoi, K Inui, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing2020</p>
<p>Crafting papers on machine learning. P Langley, Proceedings of the 17th International Conference on Machine Learning (ICML 2000). P Langley, the 17th International Conference on Machine Learning (ICML 2000)Stanford, CAMorgan Kaufmann2000</p>
<p>Gshard: Scaling giant models with conditional computation and automatic sharding. D Lepikhin, H Lee, Y Xu, D Chen, O Firat, Y Huang, M Krikun, N Shazeer, Z Chen, 2020</p>
<p>Decoupled weight decay regularization. I Loshchilov, F Hutter, 2019</p>
<p>Llm-pruner: On the structural pruning of large language models. X Ma, G Fang, X Wang, Advances in neural information processing systems. 202336</p>
<p>. C J Maddison, D Tarlow, T Minka, Sampling, 2015</p>
<p>Building a large annotated corpus of english: The penn treebank. M Marcus, B Santorini, M A Marcinkiewicz, Computational linguistics. 1921993</p>
<p>X Men, M Xu, Q Zhang, B Wang, H Lin, Y Lu, X Han, W Chen, Shortgpt, arXiv:2403.03853Layers in large language models are more redundant than you expect. 2024arXiv preprint</p>
<p>Locating and editing factual associations in gpt. K Meng, D Bau, A Andonian, Y Belinkov, 2023</p>
<p>S Merity, C Xiong, J Bradbury, R Socher, arXiv:1609.07843Pointer sentinel mixture models. 2016arXiv preprint</p>
<p>Language models implement simple word2vec-style vector arithmetic. J Merullo, C Eickhoff, E Pavlick, 2024</p>
<p>Can a suit of armor conduct electricity? a new dataset for open book question answering. T Mihaylov, P Clark, T Khot, A Sabharwal, arXiv:1809.027892018arXiv preprint</p>
<p>C Olsson, N Elhage, N Nanda, N Joseph, N Dassarma, T Henighan, B Mann, A Askell, Y Bai, A Chen, T Conerly, D Drain, D Ganguli, Z Hatfield-Dodds, D Hernandez, S Johnston, A Jones, J Kernion, L Lovitt, K Ndousse, D Amodei, T Brown, J Clark, J Kaplan, S Mccandlish, C Olah, -context learning and induction heads. Transformer Circuits Thread. 2022</p>
<p>. Achiam Openai, J Adler, S Agarwal, S Ahmad, L Akkaya, I Aleman, F L Almeida, D Altenschmidt, J Altman, S Anadkat, S Avila, R Babuschkin, I Balaji, S Balcom, V Baltescu, P Bao, H Bavarian, M Belgum, J Bello, I Berdine, J Bernadett-Shapiro, G Berner, C Bogdonoff, L Boiko, O Boyd, M Brakman, A.-L Brockman, G Brooks, T Brundage, M Button, K Cai, T Campbell, R Cann, A Carey, B Carlson, C Carmichael, R Chan, B Chang, C Chantzis, F Chen, D Chen, S Chen, R Chen, J Chen, M Chess, B Cho, C Chu, C Chung, H W Cummings, D Currier, J Dai, Y Decareaux, C Degry, T Deutsch, N Deville, D Dhar, A Dohan, D Dowling, S Dunning, S Ecoffet, A Eleti, A Eloundou, T Farhi, D Fedus, L Felix, N Fishman, S P Forte, J Fulford, I Gao, L Georges, E Gibson, C Goel, V Gogineni, T Goh, G Gontijo-Lopes, R Gordon, J Grafstein, M Gray, S Greene, R Gross, J Gu, S S Guo, Y Hallacy, C Han, J Harris, J He, Y Heaton, M Heidecke, J Hesse, C Hickey, A Hickey, W Hoeschele, P Houghton, B Hsu, K Hu, S Hu, X Huizinga, J Jain, S Jain, S Jang, J Jiang, A Jiang, R Jin, H Jin, D Jomoto, S Jonn, B Jun, H Kaftan, T , Łukasz Kaiser, A Kamali, I Kanitscheider, N S Keskar, T Khan, L Kilpatrick, J W Kim, C Kim, Y Kim, J H Kirchner, J Kiros, M Knight, D Kokotajlo, Łukasz Kondraciuk, A Kondrich, A Konstantinidis, K Kosic, G Krueger, V Kuo, M Lampe, I Lan, T Lee, J Leike, J Leung, D Levy, C M Li, R Lim, M Lin, S Lin, M Litwin, T Lopez, R Lowe, P Lue, A Makanju, K Malfacini, S Manning, T Markov, Y Markovski, B Martin, K Mayer, A Mayne, B Mcgrew, S M Mckinney, C Mcleavey, P Mcmillan, J Mcneil, D Medina, A Mehta, J Menick, L Metz, A Mishchenko, P Mishkin, V Monaco, E Morikawa, D Mossing, T Mu, M Murati, O Murk, D Mély, A Nair, R Nakano, R Nayak, A Neelakantan, R Ngo, H Noh, L Ouyang, C O'keefe, J Pachocki, A Paino, J Palermo, A Pantuliano, G Parascandolo, J Parish, E Parparita, A Passos, M Pavlov, A Peng, A Perelman, F De Avila Belbute Peres, M Petrov, H P De Oliveira Pinto, Michael, Pokorny, M Pokrass, V H Pong, T Powell, A Power, B Power, E Proehl, R Puri, A Radford, J Rae, A Ramesh, C Raymond, F Real, K Rimbach, C Ross, B Rotsted, H Roussez, N Ryder, M Saltarelli, T Sanders, S Santurkar, G Sastry, H Schmidt, D Schnurr, J Schulman, D Selsam, K Sheppard, T Sherbakov, J Shieh, S Shoker, P Shyam, S Sidor, E Sigler, M Simens, J Sitkin, K Slama, I Sohl, B Sokolowsky, Y Song, N Staudacher, F P Such, N Summers, I Sutskever, J Tang, N Tezak, M B Thompson, P Tillet, A Tootoonchian, E Tseng, P Tuggle, N Turley, J Tworek, J F C Uribe, A Vallone, A Vijayvergiya, C Voss, C Wainwright, J J Wang, A Wang, B Wang, J Ward, J Wei, C Weinmann, A Welihinda, P Welinder, J Weng, L Weng, M Wiethoff, D Willner, C Winter, S Wolrich, H Wong, L Workman, S Wu, J Wu, M Wu, K Xiao, T Xu, S Yoo, K Yu, Q Yuan, W Zaremba, R Zellers, C Zhang, M Zhang, S Zhao, T Zheng, J Zhuang, W Zhuk, B Zoph, 2024Gpt-4 technical report</p>
<p>D Raposo, S Ritter, B Richards, T Lillicrap, P C Humphreys, A Santoro, arXiv:2404.02258Mixture-of-depths: Dynamically allocating compute in transformer-based language models. 2024arXiv preprint</p>
<p>An adversarial winograd schema challenge at scale. K Sakaguchi, R L Bras, C Bhagavatula, Y Choi, Winogrande, Communications of the ACM. 6492021</p>
<p>S Samsi, D Zhao, J Mcdonald, B Li, A Michaleas, M Jones, W Bergeron, J Kepner, D Tiwari, V Gadepally, From words to watts: Benchmarking the energy costs of large language model inference. 2023</p>
<p>Confident adaptive language modeling. T Schuster, A Fisch, J Gupta, M Dehghani, D Bahri, V Tran, Y Tay, D Metzler, Advances in Neural Information Processing Systems. 202235</p>
<p>S A Siddiqui, X Dong, G Heinrich, T Breuel, J Kautz, D Krueger, P Molchanov, arXiv:2407.16286A deeper look at depth pruning of llms. 2024arXiv preprint</p>
<p>J Song, K Oh, T Kim, H Kim, Y Kim, J.-J Kim, Sleb, arXiv:2402.09025Streamlining llms through redundancy verification and elimination of transformer blocks. 2024arXiv preprint</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, 2023a</p>
<p>Llama 2: Open foundation and finetuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023barXiv preprint</p>
<p>Accelerating llama inference by enabling intermediate layer decoding via instruction tuning with lite. N Varshney, A Chatterjee, M Parmar, C Baral, 2023</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L U Kaiser, I Polosukhin, I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, Z Garnett ; Wan, X Wang, C Liu, S Alam, Y Zheng, J Liu, Z Qu, S Yan, Y Zhu, Q Zhang, M Chowdhury, M Zhang, Advances in Neural Information Processing Systems. R , Curran Associates, Inc2017. 202430Efficient large language models: A survey</p>
<p>Learning dynamic routing in convolutional networks. X Wang, F Yu, Z.-Y Dou, T Darrell, J E Gonzalez, Skipnet, 2018</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 2023</p>
<p>M Xia, T Gao, Z Zeng, D Chen, arXiv:2310.06694Sheared llama: Accelerating language model pre-training via structured pruning. 2023arXiv preprint</p>
<p>Enhancing cross-domain sequential recommendation with large language models. H Xin, Y Sun, C Wang, H Xiong, Llmcdsr, ACM Transactions on Information Systems. 2025</p>
<p>Y Yang, Z Cao, H Zhao, Laco, arXiv:2402.11187Large language model pruning via layer collapse. 2024arXiv preprint</p>
<p>Jump to conclusions: Short-cutting transformers with linear transformations. A Yom Din, T Karidi, L Choshen, M Geva, N Calzolari, M.-Y Kan, V Hoste, A Lenci, S Sakti, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). N Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCLMay 2024</p>
<p>R Zellers, A Holtzman, Y Bisk, A Farhadi, Y Choi, Hellaswag, arXiv:1905.07830Can a machine really finish your sentence?. 2019arXiv preprint</p>
<p>Learning to skip for language modeling. D Zeng, N Du, T Wang, Y Xu, T Lei, Z Chen, C Cui, 2023</p>
<p>Finercut: Finergrained interpretable layer pruning for large language models. Y Zhang, Y Li, X Wang, Q Shen, B Plank, B Bischl, M Rezaei, K Kawaguchi, 2024</p>
<p>Unveiling in-context learning: A coordinate system to understand its working mechanism. A Zhao, F Ye, J Fu, X Shen, arXiv:2407.170112024arXiv preprint</p>
<p>Building mixture-of-experts from llama with continual pre-training. T Zhu, X Qu, D Dong, J Ruan, J Tong, C He, Y Cheng, Llama-Moe, 2024</p>
<p>. Y Zhu, R Kiros, R Zemel, R Salakhutdinov, R Urtasun, A Torralba, Fidler, </p>            </div>
        </div>

    </div>
</body>
</html>
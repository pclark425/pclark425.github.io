<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5107 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5107</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5107</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-873a581320d928249609d3c07229d5af182a379c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/873a581320d928249609d3c07229d5af182a379c" target="_blank">Is ChatGPT a General-Purpose Natural Language Processing Task Solver?</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> It is found that ChatGPT performs well on many tasks favoring reasoning capabilities while it still faces challenges when solving specific tasks such as sequence tagging, and with extensive empirical studies, both the effectiveness and limitations of the current version of ChatG PT are demonstrated.</p>
                <p><strong>Paper Abstract:</strong> Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5107.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5107.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI gpt-3.5-turbo, RLHF-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-3.5-series conversational large language model trained with reinforcement learning from human feedback (RLHF) and instruction tuning; evaluated in this paper in zero-shot and zero-shot chain-of-thought settings across multiple reasoning, symbolic, and logical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is ChatGPT a General-Purpose Natural Language Processing Task Solver?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based autoregressive LLM from OpenAI (GPT-3.5 series) trained with supervised instruction data and RLHF to follow instructions and behave conversationally; architecture: autoregressive Transformer; model size not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Last Letter Concatenation; Coin Flip; Date Understanding; Tracking Shuffled Objects (Object); COPA (choice of plausible alternatives) — reported under the paper's symbolic/logical reasoning group</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks covering symbolic operations (Last Letter Concatenation, Coin Flip), temporal/date reasoning (Date Understanding), object-tracking (Tracking Shuffled Objects), and small-choice causal/commonsense reasoning (COPA). Tasks require exact symbolic manipulation, logical state tracking across steps, deterministic rule application, or selection among alternatives based on causal/commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation with direct instructions; zero-shot chain-of-thought (zero-shot-CoT) two-stage prompting: stage 1 prompt 'Let's think step by step.' to induce rationale, stage 2 uses the generated rationale plus original input with a short trigger (e.g., 'Therefore, among A through E, the answer is') to extract the final answer. No fine-tuning on these specific tasks was performed for ChatGPT in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported best (between zero-shot and zero-shot-CoT) accuracy on symbolic/logical tasks (from paper Table 11): Last Letter Concatenation 70.2% accuracy, Coin Flip 65.8% accuracy, Date Understanding 72.6% accuracy, Tracking Shuffled Objects (Object) 58.7% accuracy, COPA 82.0% accuracy. (Units: percent accuracy; values are zero-shot or zero-shot-CoT results chosen per dataset as reported by the authors.)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper reports notable weaknesses on some symbolic and logical items: ChatGPT underperforms GPT-3.5 on certain symbolic tasks (e.g., Coin Flip: ChatGPT 65.8% vs GPT-3.5 97.8%); ChatGPT also can fail on tasks demanding fine-grained background knowledge or strict symbolic manipulation without suitable prompting. Chain-of-thought helps in many cases but not uniformly (CoT sometimes produces plausible rationales but incorrect final answers for commonsense reasoning). Sequence-tagging tasks and some commonsense/symbolic/logical instances remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared directly to GPT-3.5 variants (text-davinci-003 / text-davinci-002) in the paper: ChatGPT is superior on many arithmetic reasoning and some object-tracking items (e.g., Last Letter, Object), but is worse on some symbolic/logical problems (notably Coin Flip and COPA) where GPT-3.5 variants or fine-tuned models achieve higher accuracy. The paper also compares both models to few-shot/fine-tuned models (PaLM, Codex, PaLM fine-tuned, etc.), which generally outperform zero-shot ChatGPT on many of these benchmarks when task-specific fine-tuning or larger scale is used.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>The study reports the effect of applying zero-shot chain-of-thought prompting: for many symbolic and arithmetic tasks CoT substantially improves accuracy (e.g., ChatGPT's Last Letter increased from 0.4% without CoT to 70.2% with CoT in the detailed zero-shot-CoT experiments shown in Table 3 / aggregated in Table 11). However, CoT does not always help (or can hurt) commonsense reasoning where rationales may be plausible but lead to incorrect final choices. The paper provides per-task deltas and per-class analyses for related tasks (e.g., NLI class-wise accuracy) but does not present controlled ablations over model size or training data; it focuses on prompt-style (zero-shot vs zero-shot-CoT) effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is ChatGPT a General-Purpose Natural Language Processing Task Solver?', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5107.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5107.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier GPT-3.5-series instruction-following model (text-davinci-003) used as a baseline for zero-shot and zero-shot chain-of-thought comparisons across arithmetic, commonsense, symbolic, and logical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is ChatGPT a General-Purpose Natural Language Processing Task Solver?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003 / text-davinci-002 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer language model family (GPT-3.5 variants including text-davinci-002 and text-davinci-003) from OpenAI; instruction-tuned variants used in zero-shot and chain-of-thought prompting experiments; architecture: Transformer; exact parameter counts not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Last Letter Concatenation; Coin Flip; Date Understanding; Tracking Shuffled Objects (Object); COPA (choice of plausible alternatives)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same set of symbolic and logical benchmarks as above: require deterministic symbolic manipulation (last-letter concatenation, coin flip parity), temporal/date reasoning, object-tracking, and small-choice causal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated in zero-shot and zero-shot chain-of-thought (two-stage) prompting (same 'Let's think step by step.' induction used for CoT). No task-specific fine-tuning applied in these experiments; some few-shot/fine-tuned results from other models are reported for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported best (between zero-shot and zero-shot-CoT) accuracy on symbolic/logical tasks (from paper Table 11): Last Letter Concatenation 54.4% accuracy, Coin Flip 97.8% accuracy, Date Understanding 77.0% accuracy, Tracking Shuffled Objects (Object) 39.7% accuracy, COPA 93.0% accuracy. (Units: percent accuracy; values taken from aggregated Table 11 where authors show best of zero-shot / zero-shot-CoT.)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although GPT-3.5 (text-davinci variants) outperforms ChatGPT on certain symbolic/logical tasks (notably Coin Flip and COPA), it still fails many reasoning instances and benefits strongly from CoT prompting on tasks that require multi-step symbolic reasoning. Some tasks (e.g., Last Letter without CoT) have near-zero scores without CoT, indicating brittleness to prompt style. The paper notes GPT-3.5 can also produce irrelevant outputs in some cases (illustrated by example errors).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared against ChatGPT: GPT-3.5 often lags on arithmetic and some NLI reading-comprehension style reasoning but can surpass ChatGPT on several symbolic/logical datasets (Coin Flip and COPA). Compared to few-shot/fine-tuned large models (PaLM, Codex, fine-tuned T5/PaLM variants), zero-shot GPT-3.5 is generally weaker; chain-of-thought and model scaling plus fine-tuning provide further gains in the literature cited by the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Paper's principal analysis shows strong improvements from zero-shot-CoT for GPT-3.5 on symbolic tasks (e.g., Last Letter from ~0% to 54.4% with CoT; Coin Flip from 49.0% to 97.8% with CoT in the detailed table), indicating that the prompting strategy (CoT) is a major factor affecting logical/symbolic performance. The authors do not perform internal model-size ablations but compare across publicly reported models (UL2, PaLM, Codex) and prompting strategies to infer trends.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is ChatGPT a General-Purpose Natural Language Processing Task Solver?', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5107.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5107.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot Chain-of-Thought (zero-shot-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought Prompting (two-stage 'Let's think step by step' method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that induces models to generate intermediate reasoning steps (rationales) before producing a final answer, applied in a two-stage flow: (1) request 'Let's think step by step.' to obtain rationale R, (2) feed R plus original input with a short trigger to extract the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is ChatGPT a General-Purpose Natural Language Processing Task Solver?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>zero-shot chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt engineering technique (no model weight changes) that elicits step-by-step explanations from language models to improve multi-step and symbolic reasoning; implemented as a two-stage prompting process in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Applied to arithmetic, symbolic, and logical tasks including MultiArith, GSM8K, AddSub, AQUA-RAT, SingleEq, SVAMP, CSQA, StrategyQA, COPA, Last Letter Concatenation, Coin Flip, Date Understanding, Tracking Shuffled Objects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used on tasks requiring multi-step calculation, symbolic transformation, or explicit chain-like logical inference; objective is to produce intermediate steps that help the model reach a correct final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Two-stage prompting: first stage uses instruction 'Let's think step by step.' to generate a rationale R; second stage provides R combined with original input and a concise extraction prompt (e.g., 'Therefore, among A through E, the answer is') to obtain the final answer. The same two-stage flow is used for both ChatGPT and GPT-3.5 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports large, task-dependent gains from zero-shot-CoT: e.g., arithmetic datasets: MultiArith ChatGPT improved from 79.8% to 95.8% (CoT), GSM8K from 23.8% to 78.9%, etc. On symbolic/logical tasks, Last Letter for ChatGPT rose from 0.4% (no CoT) to 70.2% (CoT); for GPT-3.5 Last Letter rose from ~0% to 54.4%; Coin Flip: ChatGPT 21.8% -> 65.8% with CoT; GPT-3.5 49.0% -> 97.8% with CoT. Gains are reported in absolute percentage points (accuracy). However, CoT sometimes gives limited or negative benefit in commonsense reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>CoT is not a panacea: the paper notes CoT can yield plausible intermediate rationales that nevertheless lead to incorrect final answers in some commonsense tasks, and that CoT's effectiveness varies by task type and by model. It can help arithmetic and strict symbolic reasoning dramatically, but benefits on commonsense or background-knowledge-heavy tasks are inconsistent. The authors also mention computational/financial costs associated with multi-stage prompting and sampling (self-consistency) in other work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Zero-shot-CoT is compared to plain zero-shot prompting: across tasks CoT often yields large improvements for arithmetic and symbolic tasks (sometimes turning near-zero performance into substantial accuracy), while on some commonsense benchmarks CoT provides limited or no improvement. The paper also references other CoT-related methods (manual-CoT, self-consistency, AutoCoT, least-to-most prompting) from prior work for additional gains under different settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is ChatGPT a General-Purpose Natural Language Processing Task Solver?', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models <em>(Rating: 2)</em></li>
                <li>PaLM: Scaling language modeling with pathways <em>(Rating: 1)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5107",
    "paper_id": "paper-873a581320d928249609d3c07229d5af182a379c",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "ChatGPT (gpt-3.5-turbo)",
            "name_full": "ChatGPT (OpenAI gpt-3.5-turbo, RLHF-trained)",
            "brief_description": "A GPT-3.5-series conversational large language model trained with reinforcement learning from human feedback (RLHF) and instruction tuning; evaluated in this paper in zero-shot and zero-shot chain-of-thought settings across multiple reasoning, symbolic, and logical benchmarks.",
            "citation_title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "Transformer-based autoregressive LLM from OpenAI (GPT-3.5 series) trained with supervised instruction data and RLHF to follow instructions and behave conversationally; architecture: autoregressive Transformer; model size not specified in the paper.",
            "model_size": null,
            "logical_reasoning_task": "Last Letter Concatenation; Coin Flip; Date Understanding; Tracking Shuffled Objects (Object); COPA (choice of plausible alternatives) — reported under the paper's symbolic/logical reasoning group",
            "task_description": "Benchmarks covering symbolic operations (Last Letter Concatenation, Coin Flip), temporal/date reasoning (Date Understanding), object-tracking (Tracking Shuffled Objects), and small-choice causal/commonsense reasoning (COPA). Tasks require exact symbolic manipulation, logical state tracking across steps, deterministic rule application, or selection among alternatives based on causal/commonsense reasoning.",
            "method_or_approach": "Zero-shot evaluation with direct instructions; zero-shot chain-of-thought (zero-shot-CoT) two-stage prompting: stage 1 prompt 'Let's think step by step.' to induce rationale, stage 2 uses the generated rationale plus original input with a short trigger (e.g., 'Therefore, among A through E, the answer is') to extract the final answer. No fine-tuning on these specific tasks was performed for ChatGPT in this study.",
            "performance": "Reported best (between zero-shot and zero-shot-CoT) accuracy on symbolic/logical tasks (from paper Table 11): Last Letter Concatenation 70.2% accuracy, Coin Flip 65.8% accuracy, Date Understanding 72.6% accuracy, Tracking Shuffled Objects (Object) 58.7% accuracy, COPA 82.0% accuracy. (Units: percent accuracy; values are zero-shot or zero-shot-CoT results chosen per dataset as reported by the authors.)",
            "limitations_or_failure_cases": "Paper reports notable weaknesses on some symbolic and logical items: ChatGPT underperforms GPT-3.5 on certain symbolic tasks (e.g., Coin Flip: ChatGPT 65.8% vs GPT-3.5 97.8%); ChatGPT also can fail on tasks demanding fine-grained background knowledge or strict symbolic manipulation without suitable prompting. Chain-of-thought helps in many cases but not uniformly (CoT sometimes produces plausible rationales but incorrect final answers for commonsense reasoning). Sequence-tagging tasks and some commonsense/symbolic/logical instances remain challenging.",
            "comparison": "Compared directly to GPT-3.5 variants (text-davinci-003 / text-davinci-002) in the paper: ChatGPT is superior on many arithmetic reasoning and some object-tracking items (e.g., Last Letter, Object), but is worse on some symbolic/logical problems (notably Coin Flip and COPA) where GPT-3.5 variants or fine-tuned models achieve higher accuracy. The paper also compares both models to few-shot/fine-tuned models (PaLM, Codex, PaLM fine-tuned, etc.), which generally outperform zero-shot ChatGPT on many of these benchmarks when task-specific fine-tuning or larger scale is used.",
            "ablation_or_analysis_results": "The study reports the effect of applying zero-shot chain-of-thought prompting: for many symbolic and arithmetic tasks CoT substantially improves accuracy (e.g., ChatGPT's Last Letter increased from 0.4% without CoT to 70.2% with CoT in the detailed zero-shot-CoT experiments shown in Table 3 / aggregated in Table 11). However, CoT does not always help (or can hurt) commonsense reasoning where rationales may be plausible but lead to incorrect final choices. The paper provides per-task deltas and per-class analyses for related tasks (e.g., NLI class-wise accuracy) but does not present controlled ablations over model size or training data; it focuses on prompt-style (zero-shot vs zero-shot-CoT) effects.",
            "uuid": "e5107.0",
            "source_info": {
                "paper_title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "GPT-3.5 (text-davinci-003)",
            "name_full": "GPT-3.5 (text-davinci-003)",
            "brief_description": "An earlier GPT-3.5-series instruction-following model (text-davinci-003) used as a baseline for zero-shot and zero-shot chain-of-thought comparisons across arithmetic, commonsense, symbolic, and logical benchmarks.",
            "citation_title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003 / text-davinci-002 variants)",
            "model_description": "Autoregressive Transformer language model family (GPT-3.5 variants including text-davinci-002 and text-davinci-003) from OpenAI; instruction-tuned variants used in zero-shot and chain-of-thought prompting experiments; architecture: Transformer; exact parameter counts not provided in this paper.",
            "model_size": null,
            "logical_reasoning_task": "Last Letter Concatenation; Coin Flip; Date Understanding; Tracking Shuffled Objects (Object); COPA (choice of plausible alternatives)",
            "task_description": "Same set of symbolic and logical benchmarks as above: require deterministic symbolic manipulation (last-letter concatenation, coin flip parity), temporal/date reasoning, object-tracking, and small-choice causal reasoning.",
            "method_or_approach": "Evaluated in zero-shot and zero-shot chain-of-thought (two-stage) prompting (same 'Let's think step by step.' induction used for CoT). No task-specific fine-tuning applied in these experiments; some few-shot/fine-tuned results from other models are reported for comparison.",
            "performance": "Reported best (between zero-shot and zero-shot-CoT) accuracy on symbolic/logical tasks (from paper Table 11): Last Letter Concatenation 54.4% accuracy, Coin Flip 97.8% accuracy, Date Understanding 77.0% accuracy, Tracking Shuffled Objects (Object) 39.7% accuracy, COPA 93.0% accuracy. (Units: percent accuracy; values taken from aggregated Table 11 where authors show best of zero-shot / zero-shot-CoT.)",
            "limitations_or_failure_cases": "Although GPT-3.5 (text-davinci variants) outperforms ChatGPT on certain symbolic/logical tasks (notably Coin Flip and COPA), it still fails many reasoning instances and benefits strongly from CoT prompting on tasks that require multi-step symbolic reasoning. Some tasks (e.g., Last Letter without CoT) have near-zero scores without CoT, indicating brittleness to prompt style. The paper notes GPT-3.5 can also produce irrelevant outputs in some cases (illustrated by example errors).",
            "comparison": "Compared against ChatGPT: GPT-3.5 often lags on arithmetic and some NLI reading-comprehension style reasoning but can surpass ChatGPT on several symbolic/logical datasets (Coin Flip and COPA). Compared to few-shot/fine-tuned large models (PaLM, Codex, fine-tuned T5/PaLM variants), zero-shot GPT-3.5 is generally weaker; chain-of-thought and model scaling plus fine-tuning provide further gains in the literature cited by the paper.",
            "ablation_or_analysis_results": "Paper's principal analysis shows strong improvements from zero-shot-CoT for GPT-3.5 on symbolic tasks (e.g., Last Letter from ~0% to 54.4% with CoT; Coin Flip from 49.0% to 97.8% with CoT in the detailed table), indicating that the prompting strategy (CoT) is a major factor affecting logical/symbolic performance. The authors do not perform internal model-size ablations but compare across publicly reported models (UL2, PaLM, Codex) and prompting strategies to infer trends.",
            "uuid": "e5107.1",
            "source_info": {
                "paper_title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Zero-shot Chain-of-Thought (zero-shot-CoT)",
            "name_full": "Zero-shot Chain-of-Thought Prompting (two-stage 'Let's think step by step' method)",
            "brief_description": "A prompting technique that induces models to generate intermediate reasoning steps (rationales) before producing a final answer, applied in a two-stage flow: (1) request 'Let's think step by step.' to obtain rationale R, (2) feed R plus original input with a short trigger to extract the final answer.",
            "citation_title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
            "mention_or_use": "use",
            "model_name": "zero-shot chain-of-thought prompting",
            "model_description": "Prompt engineering technique (no model weight changes) that elicits step-by-step explanations from language models to improve multi-step and symbolic reasoning; implemented as a two-stage prompting process in this paper.",
            "model_size": null,
            "logical_reasoning_task": "Applied to arithmetic, symbolic, and logical tasks including MultiArith, GSM8K, AddSub, AQUA-RAT, SingleEq, SVAMP, CSQA, StrategyQA, COPA, Last Letter Concatenation, Coin Flip, Date Understanding, Tracking Shuffled Objects.",
            "task_description": "Used on tasks requiring multi-step calculation, symbolic transformation, or explicit chain-like logical inference; objective is to produce intermediate steps that help the model reach a correct final answer.",
            "method_or_approach": "Two-stage prompting: first stage uses instruction 'Let's think step by step.' to generate a rationale R; second stage provides R combined with original input and a concise extraction prompt (e.g., 'Therefore, among A through E, the answer is') to obtain the final answer. The same two-stage flow is used for both ChatGPT and GPT-3.5 in experiments.",
            "performance": "Paper reports large, task-dependent gains from zero-shot-CoT: e.g., arithmetic datasets: MultiArith ChatGPT improved from 79.8% to 95.8% (CoT), GSM8K from 23.8% to 78.9%, etc. On symbolic/logical tasks, Last Letter for ChatGPT rose from 0.4% (no CoT) to 70.2% (CoT); for GPT-3.5 Last Letter rose from ~0% to 54.4%; Coin Flip: ChatGPT 21.8% -&gt; 65.8% with CoT; GPT-3.5 49.0% -&gt; 97.8% with CoT. Gains are reported in absolute percentage points (accuracy). However, CoT sometimes gives limited or negative benefit in commonsense reasoning datasets.",
            "limitations_or_failure_cases": "CoT is not a panacea: the paper notes CoT can yield plausible intermediate rationales that nevertheless lead to incorrect final answers in some commonsense tasks, and that CoT's effectiveness varies by task type and by model. It can help arithmetic and strict symbolic reasoning dramatically, but benefits on commonsense or background-knowledge-heavy tasks are inconsistent. The authors also mention computational/financial costs associated with multi-stage prompting and sampling (self-consistency) in other work.",
            "comparison": "Zero-shot-CoT is compared to plain zero-shot prompting: across tasks CoT often yields large improvements for arithmetic and symbolic tasks (sometimes turning near-zero performance into substantial accuracy), while on some commonsense benchmarks CoT provides limited or no improvement. The paper also references other CoT-related methods (manual-CoT, self-consistency, AutoCoT, least-to-most prompting) from prior work for additional gains under different settings.",
            "uuid": "e5107.2",
            "source_info": {
                "paper_title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models",
            "rating": 2
        },
        {
            "paper_title": "PaLM: Scaling language modeling with pathways",
            "rating": 1
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 1
        }
    ],
    "cost": 0.01706125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Is ChatGPT a General-Purpose Natural Language Processing Task Solver?</h1>
<p>Chengwei Qin ${ }^{\dagger}$, Aston Zhang, Zhuosheng Zhang<em>, Jiaao Chen</em>, Michihiro Yasunaga<em>, Diyi Yang</em><br>${ }^{\dagger}$ Nanyang Technological University, ${ }^{\text {a }}$ Shanghai Jiao Tong University<br>${ }^{\bullet}$ Georgia Institute of Technology, ${ }^{\text {e }}$ Stanford University</p>
<h4>Abstract</h4>
<p>Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot-i.e., without adaptation on downstream data. Recently, the debut of ChatGPT ${ }^{\dagger}$ has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have been shown to be able to solve a variety of natural language processing (NLP) tasks zero shot-i.e., without relying on any training data for a given downstream task-by conditioning the model on appropriate prompts (Brown et al., 2020; Chowdhery et al., 2022a). The ability to perform new tasks based on instructions can be seen as an important step towards artificial general intelligence (Goertzel, 2014). Despite achieving reasonable performance</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>in some cases, current LLMs are still prone to various mistakes in zero-shot learning. In addition, the format of the prompt can have a substantial impact-for example, simply adding "Let's think step by step" (Kojima et al., 2022) has been shown to significantly improve the performance of InstructGPT (Ouyang et al., 2022) on reasoning tasks. These limitations illustrate that current LLMs are not truly general-purpose language systems.</p>
<p>Recently, the ChatGPT LLM released by OpenAI has attracted a great deal of attention from the NLP community. ChatGPT was created by training a GPT-3.5 series model through reinforcement learning from human feedback (RLHF) (Christiano et al., 2017) (similarly to InstructGPT). RLHF mainly includes three steps: training a language model with supervised learning, collecting comparison data based on human preferences and training a reward model, and optimizing the language model against the reward model using reinforcement learning (Ouyang et al., 2022). Through RLHF training, ChatGPT has been observed to have impressive capabilities in various aspects, including generating high-quality responses to human input, rejecting inappropriate questions, and self-correcting previous errors based on subsequent conversations (Guo et al., 2023).</p>
<p>While ChatGPT shows strong dialogic capabilities, it still remains unclear to the NLP community whether ChatGPT attains better zero-shot generalization compared with existing LLMs. To fill in this research gap, we systematically study the zeroshot learning capability of ChatGPT by evaluating it on a large collection of NLP datasets covering 7 representative task categories, including reasoning $^{2}$, natural language inference, question answering (reading comprehension), dialogue, summarization, named entity recognition, and sentiment</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Performance of ChatGPT, GPT-3.5, and models fine-tuned with task-specific data for 20 different datasets. For each reasoning dataset, the better result between zero-shot and zero-shot chain-of-thought is shown. Measures of SAMsum, CoNLL03, and the rest are ROUGE-1/2/L average, F1, accuracy, respectively.
analysis. With extensive experiments, we aim to answer the following research questions:</p>
<ul>
<li>Is ChatGPT a general-purpose NLP task solver? On what types of tasks does ChatGPT perform well?</li>
<li>If ChatGPT fell behind other models on certain tasks, why?</li>
</ul>
<p>To answer these questions, we empirically compare the performance of ChatGPT (gpt-3.5-turbo) and the previous GPT-3.5 model (text-davinci003). In addition, we report zero-shot, fine-tuned, or few-shot fine-tuned results from recent work such as FLAN (Wei et al., 2021), T0 (Sanh et al., 2021b), and PaLM (Chowdhery et al., 2022b).</p>
<p>Key takeaways To the best of our knowledge, this is the first study of the ChatGPT's zero-shot capabilities on a diverse range of NLP tasks, aiming to provide a profile of ChatGPT. The key findings and insights are summarized as follows:</p>
<ul>
<li>Although ChatGPT shows some capability as a generalist model that can perform multiple tasks (Zhang et al., 2021), it often performs worse than models that are fine-tuned on a given task (Section 4.3 and Figure 1).</li>
<li>The superior reasoning capability of ChatGPT is empirically substantiated in arithmetic reasoning tasks (Section 4.2.1). However, ChatGPT often underperforms GPT-3.5 in commonsense, symbolic, and logical reasoning tasks.</li>
<li>ChatGPT outperforms GPT-3.5 for natural language inference tasks (Section 4.2.3) and question answering (reading comprehension) tasks (Section 4.2.4) that favor reasoning capabilities, such as in determining logical relationships within text pairs. Specifically, ChatGPT is better at handling factually consistent text (i.e., better at classifying entailment rather than nonentailment).</li>
<li>ChatGPT is superior to GPT-3.5 for dialogue tasks (Section 4.2.5).</li>
<li>ChatGPT generates longer summaries and performs worse than GPT-3.5 for summarization tasks. However, explicitly limiting summary length in the zero-shot instruction harms the summarization quality, leading to even worse performance (Section 4.2.6).</li>
<li>Despite showing promise as generalist models, both ChatGPT and GPT-3.5 face challenges on certain tasks such as sequence tagging (Sec-</li>
</ul>
<p>tion 4.2.7).</p>
<ul>
<li>ChatGPT's sentiment analysis ability is better than that of GPT-3.5 (Section 4.2.8).</li>
</ul>
<h2>2 Related Work</h2>
<p>This work mainly explores the zero-shot learning capability of ChatGPT on a diverse collection of datasets including reasoning and classic NLP tasks. In light of this, we review three lines of research that form the basis of this work: large language models, zero-shot learning, and chain-of-thought prompting for reasoning.</p>
<h3>2.1 Large Language Models</h3>
<p>Ever since Brown et al. (2020); Radford et al. (2019) demonstrated that language models can perform a variety of tasks without any gradient updates by providing the model with a textual instruction (zero-shot) and/or a few examples (fewshot), a great deal of work has focused on developing better large language models (LLMs). One line of work has aimed to explore the benefits of scaling up LLMs, including Megatron-turing NLG (Smith et al., 2022) with 530 billion parameters, Gopher (Rae et al., 2021) with 280 billion parameters, and PaLM Chowdhery et al. (2022b) with 540 billion parameters. The benefits of this scale have born out on stronger performance on more difficult tasks, e.g. the finding that PaLM outperformed average humans on the challenging BIGbench benchmark (Srivastava et al., 2022). These LLMs also form the basis of better models, such as Minerva (Lewkowycz et al., 2022) which achieved state-of-the-art performance on various technical benchmarks. Rather than scaling up model size alone, a separate line of research aims to attain better performance with smaller models through longer training (Hoffmann et al., 2022) or alternative objectives Tay et al. (2022). One particularly fruitful direction has been training LLMs with supervision (Sanh et al., 2021b; Wei et al., 2021; Mishra et al., 2022; Chung et al., 2022) and/or human feedback (Ouyang et al., 2022). The strong performance of LLMs has led to a significant amount of work analyzing the abilities and behaviors of LLMs (Webson and Pavlick, 2022; Min et al., 2022; Liang et al., 2022).</p>
<h3>2.2 Zero-Shot Learning</h3>
<p>Zero-shot learning aims to solve unseen tasks without labeled training examples. It results in a big
challenge for models as they typically rely on large amounts of training data. Prior methods to solve zero-shot learning can be mainly divided into two categories: (i) model-based methods focused on how to directly learn a model for unseen samples (Fu et al., 2017; Wang et al., 2018); and (ii) instance-based methods tried to obtain labeled instances for unseen tasks to improve model learning (Zhang et al., 2017; Ye and Guo, 2017). More recent work has demonstrated the superiority of LLMs for zero-shot learning (Brown et al., 2020; Wei et al., 2021; Chowdhery et al., 2022b). The most recent breakthrough of LLMs is the debut of ChatGPT, which has shown amazing ability in various aspects related to dialogue. Going a step further, we explore the zero-shot learning capability of ChatGPT on different tasks beyond dialogue in this work.</p>
<h3>2.3 Chain-of-Thought Prompting</h3>
<p>Chain-of-thought (CoT) prompting induces LLMs to generate intermediate reasoning steps before answering (Wei et al., 2022). According to whether there are manual demonstrations, current CoT prompting methods can be divided into two main categories: manual-CoT and zero-Shot-CoT. In manual-CoT, LLMs perform CoT reasoning with manually designed demonstrations (Wei et al., 2022). Least-to-most prompting (Zhou et al., 2022) decomposed complex problems into subproblems and then sequentially solved the subproblems. Wang et al. (2022b) introduced selfconsistency to sample multiple reasoning paths, and then conducted a majority vote to determine the final answer. To generate more diverse outputs, Li et al. (2022a) and Wang et al. (2022a) explored applying randomness in the input space. In zero-Shot-CoT, Kojima et al. (2022) demonstrated that LLMs are decent zero-shot reasoners by leveraging self-generated rationales. The effectiveness of self-generated rationales was also verified by STaR (Zelikman et al., 2022), which enabled the model to self-improve through its own generated rationales. Zhang et al. (2023a) proposed AutoCoT to automatically generate rationales from test questions. Most recent studies mainly focused on how to improve manual-CoT, including optimizing the demonstration selection (Rubin et al., 2022; Fu et al., 2022; Lu et al., 2022b) and optimizing the quality of reasoning chains (Khot et al., 2022; Chen et al., 2022). In addition, researchers also</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Instructions and input formats of six different categories of tasks (sentiment analysis, natural language inference, named entity recognition, question answering, dialogue, and summarization). The task instructions are taken from or inspired by Brown et al. (2020), Ouyang et al. (2022), Zhang et al. (2023a) and Ding et al. (2022). We color the instructions in blue. After reading the entire input (circled by the green dashed box), the model generates an answer.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of reasoning tasks. We show the instruction of AQUA-RAT (Ling et al., 2017) in this figure. Other reasoning tasks have similar instructions, e.g., "The answer (arabic numerals) is" for MultiArith (Roy and Roth, 2015). Note that we also conduct zero-shot chain-of-thought (zero-shot-CoT) experiments with ChatGPT and GPT-3.5 for reasoning tasks (right part).</p>
<p>studied the feasibility of adopting CoT in multilingual scenarios (Shi et al., 2022) and in smaller language models (Magister et al., 2022; Ho et al., 2022). More recently, Zhang et al. (2023b) proposed Multimodal-CoT that incorporates vision features in CoT reasoning, with the model under 1 billion parameters outperforming GPT-3.5 by 16\% and even surpassing human performance on the ScienceQA benchmark (Lu et al., 2022a).</p>
<h2>3 Methodology</h2>
<p>As mentioned in Section 1, we mainly compare the zero-shot learning performance of ChatGPT (gpt-3.5-turbo) and GPT-3.5 (text-davinci-003) on different tasks. Given a task instruction $P$ and a test problem $X$ that are concatenated as the input, the model $f$ is expected to generate a target text $Y=f(P, X)$ to address the test problem. The instructions and input formats of different tasks are shown in Figure 2 and 3. For example, when the model performs sentiment analysis tasks, the task instruction $P$ is "For each snippet of text, label the sentiment of the text as positive or negative. The answer should be exact 'positive' or 'negative'.". After reading the instruction $P$ and the input $X$ "it 's a stunning lyrical work of considerable force and truth.", the model is expected to generate the output $Y$ "positive".</p>
<p>Different from this single-stage prompting method, we use the same two-stage prompting as Kojima et al. (2022) for zero-shot-CoT. In the first stage, we adopt "Let's think step by step." as the instruction $P_{1}$ to induce the model to generate the rationale $R$. In the second stage, we use the selfgenerated rationale $R$ along with the original input $X$ and the instruction $P_{1}$ as the new input to guide the model to generate the final answer. A new instruction $P_{2}$, e.g., "Therefore, among A through E, the answer is", serves as the trigger sentence for extracting the answer. All task instructions are taken from or inspired by Brown et al. (2020), Ouyang et al. (2022), Zhang et al. (2023a) and Ding et al. (2022).</p>
<h2>4 Experiments</h2>
<p>In this section, we first describe the tasks and datasets, and then present the experimental results.</p>
<h3>4.1 Tasks and Datasets</h3>
<p>We evaluate ChatGPT and GPT-3.5 with 20 different datasets covering 7 representative task categories: reasoning (MultiArith (Roy and Roth, 2015), GSM8K (Cobbe et al., 2021), AddSub (Hosseini et al., 2014), AQUA-RAT (Ling et al., 2017), SingleEq (Koncel-Kedziorski et al., 2015), SVAMP (Patel et al., 2021), CSQA (Talmor et al., 2019), StrategyQA (Geva et al., 2021), COPA (Roemmele et al., 2011), Last Letter Concatenation (Wei et al., 2022), Coin Flip (Wei et al., 2022), Date Understanding, and Tracking Shuffled Objects (Srivastava et al., 2022)), natural language inference (RTE (Dagan et al., 2006) and CB (De Marneffe et al., 2019)), question answering (BoolQ (Clark et al., 2019)), dialogue (MuTual (Cui et al., 2020)), summarization (SAMSum (Gliwa et al., 2019)), named entity recognition (CoNLL03 (Sang and De Meulder, 2003)), and sentiment analysis (SST2 (Socher et al., 2013)). Among these datasets, there are 4 categories of rea-</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Arithmetic</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Commonsense</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Symbolic</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MultiArith</td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">AddSub</td>
<td style="text-align: center;">AQuA</td>
<td style="text-align: center;">SingleEq</td>
<td style="text-align: center;">SVAMP</td>
<td style="text-align: center;">CSQA</td>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">Letter</td>
<td style="text-align: center;">Coin</td>
</tr>
<tr>
<td style="text-align: center;">Measure</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"># Samples</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">1319</td>
<td style="text-align: center;">395</td>
<td style="text-align: center;">254</td>
<td style="text-align: center;">508</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1221</td>
<td style="text-align: center;">2290</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">500</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Logical</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$Q A$</td>
<td style="text-align: center;">Dialogue</td>
<td style="text-align: center;">Sum</td>
<td style="text-align: center;">NER</td>
<td style="text-align: center;">Sentiment</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Date</td>
<td style="text-align: center;">Object</td>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">BoolQ</td>
<td style="text-align: center;">MuTual</td>
<td style="text-align: center;">SAMSum</td>
<td style="text-align: center;">CoNLL</td>
<td style="text-align: center;">SST2</td>
</tr>
<tr>
<td style="text-align: center;">Measure</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ROUGE</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: center;"># Samples</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">369</td>
<td style="text-align: center;">750</td>
<td style="text-align: center;">277</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">3270</td>
<td style="text-align: center;">886</td>
<td style="text-align: center;">819</td>
<td style="text-align: center;">3453</td>
<td style="text-align: center;">872</td>
</tr>
</tbody>
</table>
<p>Table 1: Information of different datasets. # Samples refers to the number of test samples.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">GPT-3.5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Q: Ready was playing a video game and had 45 lives. In a turn/part of the game she lost 4 lives. If she got 20 more lives in the next level, how many lives would she have? <br> Q: The process is rather naturalist.</td>
<td style="text-align: center;">Q: Ready was playing a video game and had 45 lives. In a turn/part of the game she lost 4 lives. If she got 20 more lives in the next level, how many lives would she have? <br> Q: The process is rather naturalist.</td>
</tr>
<tr>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: center;">Response: $\mathrm{T}=\mathrm{1}$</td>
<td style="text-align: center;">Response: CB lives</td>
</tr>
</tbody>
</table>
<p>Figure 4: A case where ChatGPT corrects the mistake made by GPT-3.5. We color the correct and wrong responses in green and red, respectively.
soning tasks: arithmetic, commonsense, symbolic, and logical reasoning. The information of different datasets is shown in Table 1. By default we use the test split for all datasets if the labels are available for evaluation. For COPA and CommonsenseQA, we use the validation split. For StrategyQA, we use the open-domain setting (question-only set) from BIG-bench collaboration (2021) following Wei et al. (2022); Zhang et al. (2023a); Kojima et al. (2022).</p>
<h3>4.2 Experimental Results</h3>
<p>We now present and analyze the empirical results of different categories of tasks.</p>
<h3>4.2.1 Arithmetic Reasoning</h3>
<p>The accuracy of ChatGPT and GPT-3.5 without or with chain-of-thought (CoT) on six arithmetic reasoning datasets is shown in Table 2. ChatGPT outperforms GPT-3.5 on five out of six datasets without CoT, demonstrating its strong arithmetic reasoning ability. Figure 4 shows a case where GPT-3.5 gives a wrong answer. On the left part of the figure, ChatGPT accurately understands "lost 8 lives" and "got 39 more lives", resulting in the correct answer " 74 lives". However, GPT-3.5 generates a wrong answer " 120 lives" that is irrelevant to the information provided, indicating that GPT3.5 does not understand the input question. Furthermore, ChatGPT achieves much better performance than GPT-3.5 when using CoT in all cases.</p>
<h3>4.2.2 Commonsense, Symbolic, and Logical Reasoning</h3>
<p>Table 3 reports the accuracy of ChatGPT compared with popular LLMs on seven commonsense, symbolic and logical reasoning datasets. We make two key observations as follows:</p>
<p>First, using CoT may not always provide better performance in commonsense reasoning tasks. According to the analysis in Kojima et al. (2022), CoT methods often produce flexible and reasonable rationales but the final prediction is not correct in commonsense reasoning tasks. The results imply that commonsense reasoning tasks may require more fine-grained background knowledge and the issue can be mitigated by scaling model size (Wei et al., 2022), mixture of denoisers (Tay et al., 2022), and majority voting on multiple predictions (selfconsistency) (Wang et al., 2022b).</p>
<p>Second, different from arithmetic reasoning, ChatGPT performs worse than GPT-3.5 in many cases, indicating that the corresponding capabilities of GPT-3.5 are stronger.</p>
<h3>4.2.3 Natural Language Inference</h3>
<p>It is worth mentioning that different from sentiment analysis tasks (Section 4.2.8), after specifying the desired output format ("entail" or "not entail") of natural language inference in task instructions, ChatGPT and GPT-3.5 can produce responses that exactly follow the requirement. Table 4 presents the results of different models on two natural language inference tasks: RTE and CB. We can see that ChatGPT can achieve much better performance than GPT-3.5, FLAN, T0, and PaLM under the zero-shot setting. This demonstrates the superior zero-shot capability of ChatGPT to infer sentence relations.</p>
<p>To take a closer look at why ChatGPT outperforms GPT-3.5 by a large margin, we report the per-class accuracy of both models in Table 5. Chat-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MultiArith</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AddSub</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AQUA-RAT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SingleEq</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">CoT</td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot Performance</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-002</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">63.7</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-003</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">73.6</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">77.5</td>
</tr>
<tr>
<td style="text-align: center;">Few-Shot Performance</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UL2</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">12.5</td>
</tr>
<tr>
<td style="text-align: center;">LaMDA</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">37.5</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-002</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">68.9</td>
</tr>
<tr>
<td style="text-align: center;">Codex</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">76.4</td>
</tr>
<tr>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">79.0</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy (\%) of different models without CoT (N/A) and with CoT on arithmetic reasoning datasets. Few-shot results are from Wei et al. (2022). We compare ChatGPT with popular techniques including UL2-20B, LaMDA-137B, PaLM-540B, and the different GPT-3.5 variants.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Commonsense</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Symbolic</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Logical</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CSQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Last Letter</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Coin Flip</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Date</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Object</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot Performance</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-002</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">31.3</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-003</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">97.8</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">27.1</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">31.6</td>
</tr>
<tr>
<td style="text-align: center;">Few-Shot Performance</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UL2</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">LaMDA</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-002</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Codex</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">23.9</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy (\%) of different models without CoT (N/A) and with CoT on commonsense, symbolic and logical reasoning datasets. Few-shot results are from Wei et al. (2022). We compare ChatGPT with popular techniques including UL2-20B, LaMDA-137B, PaLM-540B, and the different GPT-3.5 variants.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Zero-Shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Few-Tuned</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">FLAN</td>
<td style="text-align: center;">T0</td>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">PaLM</td>
</tr>
<tr>
<td style="text-align: left;">RTE</td>
<td style="text-align: center;">$\mathbf{8 5 . 9}$</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">$\mathbf{9 5 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">CB</td>
<td style="text-align: center;">$\mathbf{8 9 . 3}$</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy (\%) of different models on natural language inference tasks (RTE and CB). We compare zero-shot ChatGPT with recent models including GPT-3.5 (zero-shot) (Brown et al., 2020), FLAN (zeroshot) (Wei et al., 2021), T0 (zero-shot) (Sanh et al., 2021a), PaLM (zero-shot) (Chowdhery et al., 2022b) and PaLM-540B (fine-tuned) (Chowdhery et al., 2022b).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">GPT-3.5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Entailment</td>
<td style="text-align: center;">$\mathbf{9 2 . 5}$</td>
<td style="text-align: center;">70.6</td>
</tr>
<tr>
<td style="text-align: left;">Not Entailment</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">$\mathbf{9 0 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Per-class accuracy (\%) of ChatGPT and GPT3.5 on RTE.</p>
<p>GPT performs much better than GPT-3.5 when the premise does entail the hypothesis ( $+21.9 \%$ ). However, it underperforms GPT-3.5 on the class "Not Entailment" (-12.2\%). So we can see that ChatGPT is better at handling factual input (also favored by humans in general), which might be related to the</p>
<p>| Model | ChatGPT | GPT-3.5 | Zero-Shot |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | </p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">ChatGPT</th>
<th style="text-align: left;">GPT-3.5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">$\mathbf{8 8 . 9}(+7.8)$</td>
<td style="text-align: left;">81.1</td>
</tr>
<tr>
<td style="text-align: left;">No</td>
<td style="text-align: left;">$\mathbf{8 4 . 6}(-6.0)$</td>
<td style="text-align: left;">90.6</td>
</tr>
</tbody>
</table>
<p>Table 7: Per-class accuracy (\%) of ChatGPT and GPT3.5 on BoolQ. The number in parentheses indicates the improvement over GPT-3.5.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">GPT-3.5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Q: $i$ ' $A d i$, can you lend jenny and me some money ' $n$ for what ' $i$ ' we want to get to him for a total content.</td>
<td style="text-align: center;">Q: $i$ ' $A d i$, can you lend jenny and me some money ' $n$ for what ' $i$ ' we want to get to him for a total content.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$i$ ' $A d i$ , can you lend jenny and me some money ' $n$ for what ' $i$ ' we want to get to him for a total content.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$i$ ' $A d i$ , can you lend jenny and me some money ' $n$ for what ' $i$ ' we want to get to him for a total content.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Zero-Shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Unsupervised</th>
<th style="text-align: center;">Fine-Tuned</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">TF-IDF</td>
<td style="text-align: center;">Dual LSTM</td>
<td style="text-align: center;">DAM</td>
<td style="text-align: center;">SMN</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">GPT-2-FT</td>
<td style="text-align: center;">MDFN</td>
<td style="text-align: center;">BiDeN</td>
</tr>
<tr>
<td style="text-align: left;">Accuracy (\%)</td>
<td style="text-align: center;">$\mathbf{7 6 . 2}$</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">$\mathbf{9 3 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Accuracy on the dialogue task (MuTual). Besides GPT-3.5, we also compare ChatGPT with previous popular methods including (i) unsupervised method: TF-IDF (Lowe et al., 2015); (ii) fine-tuned models: Dual LSTM (Lowe et al., 2015), DAM (Zhou et al., 2018), SMN (Wu et al., 2017), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), fine-tuned GPT-2 (GPT-2-FT) (Radford et al., 2019), MDFN (Liu et al., 2021), and BiDeN (Li et al., 2022b).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">zero-shot</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">fine-tuned</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">ChatGPT</td>
<td style="text-align: right;">GPT-3.5</td>
<td style="text-align: right;">BART</td>
<td style="text-align: center;">CODA</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: right;">42.4</td>
<td style="text-align: right;">$\mathbf{4 4 . 0}$</td>
<td style="text-align: right;">49.1</td>
<td style="text-align: center;">$\mathbf{5 0 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: right;">17.6</td>
<td style="text-align: right;">$\mathbf{1 8 . 5}$</td>
<td style="text-align: right;">24.3</td>
<td style="text-align: center;">$\mathbf{2 4 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: right;">33.0</td>
<td style="text-align: right;">$\mathbf{3 4 . 7}$</td>
<td style="text-align: right;">45.8</td>
<td style="text-align: center;">$\mathbf{4 6 . 9}$</td>
</tr>
</tbody>
</table>
<p>Table 9: ROUGE scores of different models on the summarization dataset: SAMSum. We compare zeroshot ChatGPT with GPT-3.5 (Zero-Shot), BART-large (Fine-Tuned) (Lewis et al., 2019), and CODA (finetuned) (Chen and Yang, 2021).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Zero-Shot</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Fine-Tuned</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">ChatGPT</td>
<td style="text-align: right;">GPT-3.5</td>
<td style="text-align: right;">Flair</td>
<td style="text-align: center;">LUKE AGE</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: right;">53.2</td>
<td style="text-align: right;">$\mathbf{5 3 . 5}$</td>
<td style="text-align: right;">93.0</td>
<td style="text-align: center;">93.9</td>
</tr>
<tr>
<td style="text-align: left;">Loc</td>
<td style="text-align: right;">66.7</td>
<td style="text-align: right;">$\mathbf{6 7 . 1}$</td>
<td style="text-align: right;">94.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Per</td>
<td style="text-align: right;">$\mathbf{8 7 . 2}$</td>
<td style="text-align: right;">78.0</td>
<td style="text-align: right;">97.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Org</td>
<td style="text-align: right;">$\mathbf{5 1 . 4}$</td>
<td style="text-align: right;">50.0</td>
<td style="text-align: right;">91.9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Misc</td>
<td style="text-align: right;">4.1</td>
<td style="text-align: right;">$\mathbf{4 . 8}$</td>
<td style="text-align: right;">83.0</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 10: F1 scores of different models on named entity recognition (CoNLL03). "Loc", "Per", "Org", and "Misc" stand for "Location", "Person", "Organization", and "Miscellaneous Entity", respectively. We also compare the zero-shot ChatGPT with GPT3.5 (zero-shot) and recent state-of-the-art named entity recognition models including Flair (Akbik et al., 2018), LUKE (fine-tuned) (Yamada et al., 2020), and ACE (fine-tuned) (Wang et al., 2020).</p>
<p>On the right part, although "AMERICAN FOOTBALL CONFERENCE" is indeed an organization, it is not recognized by the ground truth annotation, indicating that the ground truth annotation might need cleaning (although in rare cases). Therefore, the poor performance on the class "Miscellaneous Entity" may be partly due to the different understanding on the scope of entities between LLMs and the ground truth annotation of the specific task dataset.</p>
<p>In addition, we design new instructions that guide GPT-3.5 to generate different types of entities separately, leading to a much lower F1 score (34.8). This reiterates the challenges faced by LLMs in solving sequence tagging tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Input: Boeing / Plume dream 8.1-8.43.1 (No. 348),</th>
<th style="text-align: left;">Input: AMERICAN FOOTBALL CONFERENCE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Major Teams</td>
<td style="text-align: left;">Label: Miscellaneous- State</td>
</tr>
<tr>
<td style="text-align: left;">Response: 512112200000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Arithmetic</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Symbolic</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Logical</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MultiArith</td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">AddSub</td>
<td style="text-align: center;">AQUA-RAT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SingleEq</td>
<td style="text-align: center;">SVAMP</td>
<td style="text-align: center;">Last Letter</td>
<td style="text-align: center;">Coin Flip</td>
<td style="text-align: center;">Date Object</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">72.6 58.7</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">97.8</td>
<td style="text-align: center;">77.0 39.7</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuning</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">65.3 23.9</td>
</tr>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">Dialogue</td>
<td style="text-align: center;">Summarization</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentiment</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CSQA</td>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">BoolQ</td>
<td style="text-align: center;">MuTual</td>
<td style="text-align: center;">SAMsum</td>
<td style="text-align: center;">CoNLL03</td>
<td style="text-align: center;">SST2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">ROUGE</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">93.7</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">88.8</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuning</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">97.5</td>
</tr>
</tbody>
</table>
<p>Table 11: Performance of ChatGPT, GPT-3.5 and the best previous full-set or few-shot fine-tuning method (among those investigated in this work) on different tasks. For each reasoning dataset, the better result between zero-shot and zero-shot chain-of-thought is shown.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Zero-Shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fine-Tuned</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">FLAN</td>
<td style="text-align: center;">T5-11B</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">$\mathbf{9 3 . 7}$</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">$\mathbf{9 7 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Pos</td>
<td style="text-align: center;">$\mathbf{9 0 . 8}$</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Neg</td>
<td style="text-align: center;">$\mathbf{9 6 . 7}$</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 12: Accuracy (\%) of different models on sentiment analysis (SST2). We compare zero-shot ChatGPT with recent models including GPT-3.5 (zeroshot) (Brown et al., 2020), FLAN (zero-shot) (Wei et al., 2021), and T5-11B (fine-tuned) (Raffel et al., 2019).
demonstrated the effectiveness and current limitations of ChatGPT in different types of NLP tasks. For example, as a powerful generalist model, on one hand, ChatGPT is good at reasoning and dialogue tasks; on the other hand, ChatGPT still faces challenges when solving specific tasks, such as sequence tagging. We hope that this study can inspire future works, such as leveraging the reasoning and dialogue capabilities of ChatGPT in NLP tasks and addressing limitations of generalist models in tasks where they currently struggle with.</p>
<h2>Limitations</h2>
<p>This work is an empirical study on the zero-shot learning ability of ChatGPT ${ }^{3}$, and it has several limitations. First, due to the cost of ChatGPT, this work excludes larger-scale datasets and more task categories, which might prevent further insights. Besides, we report the best result in the corresponding paper for models that are not publicly available (e.g., PaLM) and report the result based on the best prompt found for public models, which is consistent with the previous work (Wei et al., 2022; Kojima et al., 2022; Tay et al., 2022). A further im-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>provement could be to explore more diverse prompt templates. Finally, it still remains unclear to us how ChatGPT's few-shot in-context learning capability compares with its zero-shot learning ability across different tasks.</p>
<h2>References</h2>
<p>Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th international conference on computational linguistics, pages 1638-1649.</p>
<p>BIG-bench collaboration BIG-bench collaboration. 2021. Beyond the imitation game: Measuring and extrapolating the capabilities of language models. https://github.com/google/BIG-bench. "Accessed: 2022-05-07".
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Jiaao Chen and Diyi Yang. 2021. Simple conversational data augmentation for semi-supervised abstractive dialogue summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6605-6616, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts</p>
<p>prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022a. Palm: Scaling language modeling with pathways.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022b. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems.</p>
<p>Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang, and Ming Zhou. 2020. MuTual: A dataset for multi-turn dialogue reasoning. In Proceedings of the 58th Annual</p>
<p>Meeting of the Association for Computational Linguistics, pages 1406-1416, Online. Association for Computational Linguistics.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment: First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, pages 177-190. Springer.</p>
<p>Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pages 107-124.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Bosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing, Shafiq Joty, and Boyang Li. 2022. Is gpt-3 a good data annotator? arXiv preprint arXiv:2212.10450.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720.</p>
<p>Zhenyong Fu, Tao Xiang, Elyor Kodirov, and Shaogang Gong. 2017. Zero-shot learning on semantic class prototype graph. IEEE transactions on pattern analysis and machine intelligence, 40(8):2009-2022.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361.</p>
<p>Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. Samsum corpus: A humanannotated dialogue dataset for abstractive summarization. arXiv preprint arXiv:1911.12237.</p>
<p>Ben Goertzel. 2014. Artificial general intelligence: concept, state of the art, and future prospects. Journal of Artificial General Intelligence, 5(1):1.</p>
<p>Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654.</p>
<p>Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language models are reasoning teachers. arXiv preprint arXiv:2212.10071.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523-533, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022).</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585-597.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022a. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336.</p>
<p>Yiyang Li, Hai Zhao, and Zhuosheng Zhang. 2022b. Back to the future: Bidirectional information decoupling network for multi-turn dialogue modeling. In The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022).</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. ArXiv preprint, abs/2211.09110.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Longxiang Liu, Zhuosheng Zhang, Hai Zhao, Xi Zhou, and Xiang Zhou. 2021. Filling the gap of utteranceaware and speaker-aware representation for multiturn dialogue. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages $13406-13414$.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Ryan Lowe, Nissan Pow, Iulian Vlad Serban, and Joelle Pineau. 2015. The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 285-294.</p>
<p>Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022a. Learn to explain: Multimodal reasoning via thought chains for science question answering. ArXiv preprint, abs/2209.09513.</p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2022b. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610.</p>
<p>Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2022. Teaching small language models to reason. ArXiv preprint, abs/2212.08410.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470-3487.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online. Association for Computational Linguistics.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, page 9 .</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. $J M L R, 21(140): 1-67$.</p>
<p>Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI spring symposium: logical formalizations of commonsense reasoning, pages 90-95.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743-1752, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655-2671.</p>
<p>Erik F Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. arXiv preprint cs/0306050.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish</p>
<p>Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2021a. Multitask prompted training enables zero-shot task generalization.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021b. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.</p>
<p>Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057.</p>
<p>Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131.</p>
<p>Xiaolong Wang, Yufei Ye, and Abhinav Gupta. 2018. Zero-shot recognition via semantic embeddings and</p>
<p>knowledge graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6857-6866.</p>
<p>Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, and Kewei Tu. 2020. Automated concatenation of embeddings for structured prediction.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a. Rationaleaugmented ensembles in language models. arXiv preprint arXiv:2207.00747.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Albert Webson and Ellie Pavlick. 2022. Do promptbased models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2300-2344, Seattle, United States. Association for Computational Linguistics.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022).</p>
<p>Yu Wu, Wei Wu, Chen Xing, Ming Zhou, and Zhoujun Li. 2017. Sequential matching network: A new architecture for multi-turn response selection in retrievalbased chatbots. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 496-505.</p>
<p>Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto. 2020. Luke: Deep contextualized entity representations with entity-aware self-attention.</p>
<p>Meng Ye and Yuhong Guo. 2017. Zero-shot classification with discriminative semantic representation learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $7140-7148$.</p>
<p>Eric Zelikman, Yuhuai Wu, and Noah D Goodman. 2022. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465.</p>
<p>Aston Zhang, Zachary C Lipton, Mu Li, and Alexander J Smola. 2021. Dive into deep learning. arXiv preprint arXiv:2106.11342.</p>
<p>Li Zhang, Tao Xiang, and Shaogang Gong. 2017. Learning a deep embedding model for zero-shot learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2021-2030.</p>
<p>Zhuosheng Zhang, Shuohang Wang, Yichong Xu, Yuwei Fang, Wenhao Yu, Yang Liu, Hai Zhao, Chenguang Zhu, and Michael Zeng. 2022. Task compass: Scaling multi-task pre-training with task prefix. In Findings of The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022).</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023a. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations (ICLR 2023).</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023b. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.</p>
<p>Xiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying Chen, Wayne Xin Zhao, Dianhai Yu, and Hua Wu. 2018. Multi-turn response selection for chatbots with deep attention matching network. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages $1118-1127$.</p>
<h2>A Appendix</h2>
<h2>A. 1 Example Input and Output Pairs of ChatGPT</h2>
<p>Table 13: Example input and output pairs for MultiArith (arithmetic reasoning).</p>
<p>Q: Bianca had 45 coloring books. If she gave away 6 of them, but then bought 20 more, how many would she have total?
A: The answer (arabic numerals) is 59 .
Q: Janet was playing a video game and had 47 lives. In a hard part of the game she lost 23 lives. If she got 46 more lives in the next level, how many lives would she have?
A: The answer (arabic numerals) is 70 lives. You can find this by starting with the number of lives she had at the beginning (47) and subtracting the number of lives she lost (23). Then add the number of lives she gained (46) to that number: $47-23+46=70$.</p>
<p>Q: A toy store had 17 giant stuffed bears in stock when they got another shipment with 10 bears in it. The put the bears onto shelves with 9 on each shelf. How many shelves did they use?
A: The answer (arabic numerals) is 3. Explanation: The toy store started with 17 bears, and got another 10 , for a total of $17+10=27$ bears. To put these bears on shelves, they need 27 bears / 9 bears per shelf $=3$ shelves.</p>
<p>Q: A group of 3 friends went into a restaurant. The chef already had 6 chicken wings cooked but cooked 3 more for the group. If they each got the same amount how many would each person get?
A: The answer (arabic numerals) is 3 chicken wings per person.
Q: George had 28 socks. If he threw away 4 old ones that didn't fit and bought 36 new ones, how many socks would he have?
A: The answer (arabic numerals) is 60 . George would have 60 socks. $(28+36=60)$
Q: At the town carnival Billy rode the ferris wheel 7 times and the bumper cars 3 times. If each ride cost 5 tickets, how many tickets did he use?
A: The answer (arabic numerals) is 40 tickets.
Q: Kaleb had saved up 21 dollars. If he received another 15 dollars for his allowance, how many 6 dollar toys could he buy?
A: The answer (arabic numerals) is 5 . He would have a total of $21+15=36$ dollars, and could buy $36 / 6=6$ toys.</p>
<p>Q: Tom was putting his spare change into piles. He had 2 piles of quarters and 4 piles of dimes. If each pile had 5 coins in it, how many coins did he have total?
A: The answer (arabic numerals) is 50 coins. ( 2 piles of quarters $* 5$ coins per pile $=10$ quarters) + (4 piles of dimes * 5 coins per pile $=20$ dimes $)=50$ coins.</p>
<p>Q: Isabel uploaded 25 pictures to Facebook. She put 10 pics into one album and put the rest into 5 different albums. How many pictures were in each album?
A: The answer (arabic numerals) is 10 in one album and 5 in each of the other 5 albums.
Q: For homework Faye had 13 math problems and 2 spelling problems. If she can finish 3 problems in an hour how long will it take her to finish all the problems?
A: The answer (arabic numerals) is 8 hours. You can get this by adding the number of math problems (13) and spelling problems (2) to get a total of 15 problems. Then divide the total number of problems (15) by the number of problems Faye can finish in an hour (3) to get 5. Finally, divide 5 by 1 hour to get the answer of 5 hours.</p>
<p>Table 14: Example input and output pairs for MultiArith with chain-of-thought (arithmetic reasoning).</p>
<p>Q: Bianca had 45 coloring books. If she gave away 6 of them, but then bought 20 more, how many would she have total?
A: Let's think step by step. First, Bianca had 45 coloring books. Then, she gave away 6 of them, so she had $45-6=39$ coloring books. Finally, she bought 20 more, so she has $39+20=59$ coloring books. Therefore, the answer (arabic numerals) is 59</p>
<p>Q: Janet was playing a video game and had 47 lives. In a hard part of the game she lost 23 lives. If she got 46 more lives in the next level, how many lives would she have?
A: Let's think step by step. Janet had 47 lives, then lost 23, so she had 47-23=24 lives. Then, she got 46 more lives, so she had $24+46=70$ lives. Therefore, the answer (arabic numerals) is 70</p>
<p>Q: A toy store had 17 giant stuffed bears in stock when they got another shipment with 10 bears in it. The put the bears onto shelves with 9 on each shelf. How many shelves did they use?
A: Let's think step by step. $17+10=27$ bears. 27 bears / 9 bears/shelf $=3$ shelves. Therefore, the answer (arabic numerals) is 3</p>
<p>Q: A group of 3 friends went into a restaurant. The chef already had 6 chicken wings cooked but cooked 3 more for the group. If they each got the same amount how many would each person get?
A: Let's think step by step. Each person would get 3 chicken wings. Therefore, the answer (arabic numerals) is 3</p>
<p>Q: George had 28 socks. If he threw away 4 old ones that didn't fit and bought 36 new ones, how many socks would he have?
A: Let's think step by step. George had 28 socks, then threw away 4, so he has $28-4=24$. Then he bought 36 new ones, so he has $24+36=60$ socks. Therefore, the answer (arabic numerals) is 60 .</p>
<p>Q: At the town carnival Billy rode the ferris wheel 7 times and the bumper cars 3 times. If each ride cost 5 tickets, how many tickets did he use?
A: Let's think step by step. Billy rode the ferris wheel 7 times, so he used $7 * 5=35$ tickets. He also rode the bumper cars 3 times, so he used $3 * 5=15$ tickets. Therefore, Billy used $35+15=50$ tickets. Therefore, the answer (arabic numerals) is 50</p>
<p>Q: Kaleb had saved up 21 dollars. If he received another 15 dollars for his allowance, how many 6 dollar toys could he buy?
A: Let's think step by step. Kaleb had $21+15=36$ dollars. He can buy $36 / 6=6$ toys. Therefore, the answer (arabic numerals) is 6</p>
<p>Q: Tom was putting his spare change into piles. He had 2 piles of quarters and 4 piles of dimes. If each pile had 5 coins in it, how many coins did he have total?
A: Let's think step by step. Tom had 2 piles of quarters, which means he had $2 * 5=10$ quarters. He also had 4 piles of dimes, so he had $4 * 5=20$ dimes. The total number of coins he had is $10+20=$ 30 coins. Therefore, the answer (arabic numerals) is 30</p>
<p>Q: Isabel uploaded 25 pictures to Facebook. She put 10 pics into one album and put the rest into 5 different albums. How many pictures were in each album?
A: Let's think step by step. Isabel put 10 pics into one album, so each of the 5 other albums had $25-10=15$ pics. Answer: 15 pics/album. Therefore, the answer (arabic numerals) is 15 .</p>
<p>Q: For homework Faye had 13 math problems and 2 spelling problems. If she can finish 3 problems in an hour how long will it take her to finish all the problems?
A: Let's think step by step. 13 math problems +2 spelling problems $=15$ problems. 15 problems / 3 problems per hour $=5$ hours. Therefore, the answer (arabic numerals) is 5 hours.</p>
<p>Table 15: Example input and output pairs for GSM8K (arithmetic reasoning).</p>
<p>Q: A store offers sandwiches in 3 different package sizes: the first package with 2 sandwiches for $\$ 3$, the second with 4 sandwiches for $\$ 6$, and a third with 8 sandwiches, but the price is not indicated. Knowing that the prices are proportional, how much will the package with 8 sandwiches cost?
A: The answer (arabic numerals) is 12 dollars.
Q: The ratio of the electric poles and electric wires needed to connect and supply the electricity in a certain neighborhood is $1: 3$. If the total number of electric wires needed to connect the electricity in the neighborhood is 45 , calculate the total number of electric poles required by an electric company to supply the electricity in the neighborhood.
A: The answer (arabic numerals) is 15 electric poles are required. This is calculated by dividing the total number of electric wires (45) by the ratio of electric poles to electric wires $(1: 3) .45 /(1 / 3)=45$ * $3=135$, and 45 electric poles are needed.</p>
<p>Q: My kitchen floor has a total area of 200 SqFt. I want to install new square floor tiles that cost $\$ 12$ each, and each tile side is 1 ft in length. How much will it cost me to renovate my kitchen floor?
A: The answer (arabic numerals) is 2400 dollars. Explanation: The area of the kitchen floor is 200 sq ft , and each tile is 1 sq ft in area ( $1 \mathrm{ft} \times 1 \mathrm{ft}=1 \mathrm{sq} \mathrm{ft}$ ). So the number of tiles needed to cover the kitchen floor is $200 \mathrm{sq} \mathrm{ft} / 1 \mathrm{sq} \mathrm{ft} /$ tile $=200$ tiles. At a cost of $\$ 12$ per tile, the total cost to cover the kitchen floor with these tiles would be 200 tiles x $\$ 12 /$ tile $=\$ 2400$.</p>
<p>Q: Jeff and Brad are trying to divide 100 dollars between them. Jeff gets 4 times as much as Brad. How much does Jeff get in dollars?
A: The answer (arabic numerals) is 80 dollars.
Q: To fund his store, Mr. Josue solicited money from two banks. The first bank gave him $\$ 4000$, and the second company gave him twice as much. If he initially had $\$ 5000$ in capital, how much capital does he have now?
A: The answer (arabic numerals) is $\$ 12000$. The second bank gave him $\$ 4000 \times 2=\$ 8000$, so in total he received $\$ 4000+\$ 8000=\$ 12000$ from the banks. Adding this to his initial capital of $\$ 5000$, he now has a total of $\$ 12000+\$ 5000=\$ 17000$.</p>
<p>Q: John drives for 3 hours at a speed of 60 mph and then turns around because he realizes he forgot something very important at home. He tries to get home in 4 hours but spends the first 2 hours in standstill traffic. He spends the next half-hour driving at a speed of 30 mph , before being able to drive the remaining time of the 4 hours going at 80 mph . How far is he from home at the end of those 4 hours?
A: The answer (arabic numerals) is It is not possible to determine the distance John is from home at the end of the 4 hours based on the information provided. This is because the information provided only describes the time and speed of John's travel, but not the distance.</p>
<p>Q: Mazie has 52 marbles and gives them to twins, Dallas and Darla. If Dallas has a total of 21 marbles after dropping 4 of them in the playground, how many did Mazie give to Darla?
A: The answer (arabic numerals) is 31 marbles.
Q: There are 96 fourth-graders at Small Tree School. 43 of them are girls. On Friday, 5 fourth-grade girls and 4 fourth-grade boys were absent. How many fourth grade boys were at Small Tree School on Friday?
A: The answer (arabic numerals) is 43 boys. To find out, you can start with the total number of fourth-graders (96) and subtract the number of girls (43) to find the number of boys. Then subtract the number of absent boys (4) from that number to find the number of boys present on Friday. 96 fourth-graders - 43 girls $=53$ boys. 53 boys - 4 absent boys $=49$ present boys.</p>
<p>Table 16: Example input and output pairs for GSM8K with chain-of-thought (arithmetic reasoning).</p>
<p>Q: A store offers sandwiches in 3 different package sizes: the first package with 2 sandwiches for $\$ 3$, the second with 4 sandwiches for $\$ 6$, and a third with 8 sandwiches, but the price is not indicated. Knowing that the prices are proportional, how much will the package with 8 sandwiches cost?
A: Let's think step by step. The first package costs $\$ 3$ for 2 sandwiches, so each sandwich costs $\$ 3 / 2$ $=\$ 1.50$. The second package costs $\$ 6$ for 4 sandwiches, so each sandwich costs $\$ 6 / 4=\$ 1.50$. Since the price of each sandwich is the same, the third package with 8 sandwiches will cost $8 * \$ 1.50=\$ 12$. Therefore, the answer (arabic numerals) is $\$ 12$</p>
<p>Q: Jeff and Brad are trying to divide 100 dollars between them. Jeff gets 4 times as much as Brad. How much does Jeff get in dollars?
A: Let's think step by step. First, let's call Brad's share "x". If Jeff gets 4 times as much as Brad, then Jeff gets 4 x dollars. So, we know that $\mathrm{x}+4 \mathrm{x}=100$ dollars. Combining like terms, we get $5 \mathrm{x}=$ 100 dollars. Dividing both sides by 5 , we find that $\mathrm{x}=20$ dollars. So, Jeff gets $4 * 20=80$ dollars. Therefore, the answer (arabic numerals) is Jeff gets 80 dollars.</p>
<p>Q: To fund his store, Mr. Josue solicited money from two banks. The first bank gave him $\$ 4000$, and the second company gave him twice as much. If he initially had $\$ 5000$ in capital, how much capital does he have now?
A: Let's think step by step. The first bank gave Mr. Josue $\$ 4000$. The second bank gave him twice as much, which is $2 * \$ 4000=\$ 8000$. So Mr. Josue received a total of $\$ 4000+\$ 8000=\$ 12000$ from the two banks. Adding this to his initial capital of $\$ 5000$, he now has a total of $\$ 12000+\$ 5000=\$ 17000$ in capital. Therefore, the answer (arabic numerals) is 17000 .</p>
<p>Q: John drives for 3 hours at a speed of 60 mph and then turns around because he realizes he forgot something very important at home. He tries to get home in 4 hours but spends the first 2 hours in standstill traffic. He spends the next half-hour driving at a speed of 30 mph , before being able to drive the remaining time of the 4 hours going at 80 mph . How far is he from home at the end of those 4 hours?
A: Let's think step by step. First, John drove for 3 hours at 60 mph , so he covered a distance of $3 * 60=180$ miles. Then, he turned around and drove for 2 hours in standstill traffic, which covered no distance. After that, he drove for 0.5 hours at 30 mph , covering a distance of $0.5 * 30=15$ miles. Finally, he drove for 1.5 hours at 80 mph , covering a distance of $1.5 * 80=120$ miles. So in total, he covered a distance of $180+0+15+120=315$ miles. Therefore, the answer (arabic numerals) is 315 miles.</p>
<p>Q: Mazie has 52 marbles and gives them to twins, Dallas and Darla. If Dallas has a total of 21 marbles after dropping 4 of them in the playground, how many did Mazie give to Darla?
A: Let's think step by step. Dallas originally had $21+4=25$ marbles. So, the total number of marbles Mazie gave to both twins is $25+$ (total number of marbles Darla has) $=52$. Therefore, Darla has 52 $25=27$ marbles. Therefore, the answer (arabic numerals) is 27 .</p>
<p>Q: There are 96 fourth-graders at Small Tree School. 43 of them are girls. On Friday, 5 fourth-grade girls and 4 fourth-grade boys were absent. How many fourth grade boys were at Small Tree School on Friday?
A: Let's think step by step. First, we know that there are 43 girls in the fourth grade at Small Tree School. If 5 of them were absent on Friday, then there were $43-5=38$ girls present. Next, we know that there are 96 fourth-graders total, and we know that 43 of them are girls. So, $96-43=53$ fourth-grade boys. Finally, if 4 of the 53 fourth-grade boys were absent on Friday, then $53-4=49$ fourth-grade boys were present on Friday. Therefore, the answer (arabic numerals) is 49 .</p>
<p>Table 17: Example input and output pairs for AddSub (arithmetic reasoning).</p>
<p>Q: Shannon and her family use up a lot of strawberry and blueberry jelly, since they eat toast every morning. At the moment, they have a combined total of 6310 grams of jelly. If they have 4518 grams of blueberry jelly, how many grams of strawberry jelly do they have?
A: The answer (arabic numerals) is 1792 grams.
Q: Students at Arcadia schools are participating in a coat drive . 9437 coats have been collected so far . 6922 coats were collected from the high schools, and the rest from the elementary schools. How many coats were collected at the elementary schools?
A: The answer (arabic numerals) is 2515 coats were collected at the elementary schools. This can be found by subtracting the number of coats collected from the high schools (6922) from the total number of coats collected (9437). $9437-6922=2515$.</p>
<p>Q: An oil pipe in the sea broke. Before engineers started to fix the pipe, 6522 liters of oil leaked into the water. While the engineers worked, the pipe leaked 5165 liters of oil. In all, how many liters of oil leaked into the water?
A: The answer (arabic numerals) is 11,687 liters of oil leaked into the water. This can be determined by adding the initial amount of oil leaked ( 6522 liters) to the amount leaked while the engineers worked (5165 liters).</p>
<p>Q: Tim 's cat had kittens. He gave 3 to Jessica and 6 to Sara. He now has 9 kittens. How many kittens did he have to start with?
A: The answer (arabic numerals) is 18 . Tim gave away 3 kittens to Jessica and 6 kittens to Sara, which totals 9 kittens. To find out how many kittens he had to start with, you add the number of kittens he gave away to the number he still has, which is $9+9=18$ kittens.</p>
<p>Q: Mike went to 15 basketball games this year, but missed 41 . He went to 39 games last year. How many basketball games did Mike go to in total?
A: The answer (arabic numerals) is 53 games. (Mike went to 15 games this year +39 games last year $=54$ games - the 1 game he missed $=53$ games total)</p>
<p>Q: A waitress put leftover tarts into the fridge on Thursday night. She noticed that the restaurant had 0.08333333333333333 tart filled with cherries, 0.75 tart filled with blueberries, and 0.08333333333333333 tart filled with peaches. How many leftover tarts did the restaurant have in all ?
A: The answer (arabic numerals) is 1.0833333333333333 tarts total.
Q: There are 7 crayons in the drawer and 6 crayons on the desk. Sam placed 4 crayons and 8 scissors on the desk. How many crayons are now there in total?
A: The answer (arabic numerals) is 15 crayons.
Q: Bonnie 's science class recorded the rainfall each day. They recorded 0.16666666666666666 centimeter of rain on Monday, 0.4166666666666667 centimeter of rain on Tuesday, and 0.0833333333333333 centimeter of rain on Wednesday. How many centimeters of rain did the class record in all?
A: The answer (arabic numerals) is 0.7 centimeters of rain.</p>
<p>Table 18: Example input and output pairs for AddSub with chain-of-thought (arithmetic reasoning).</p>
<p>Q: Shannon and her family use up a lot of strawberry and blueberry jelly, since they eat toast every morning. At the moment, they have a combined total of 6310 grams of jelly. If they have 4518 grams of blueberry jelly, how many grams of strawberry jelly do they have ?
A: Let's think step by step. The total amount of jelly they have is 6310 grams, and of that, 4518 grams is blueberry jelly. To find out how much strawberry jelly they have, we can subtract the amount of blueberry jelly from the total amount of jelly: 6310 grams (total jelly) - 4518 grams (blueberry jelly) $=1792$ grams (strawberry jelly). Therefore, the answer (arabic numerals) is 1792 .</p>
<p>Q: Students at Arcadia schools are participating in a coat drive . 9437 coats have been collected so far . 6922 coats were collected from the high schools, and the rest from the elementary schools. How many coats were collected at the elementary schools?
A: Let's think step by step. Coats collected at elementary schools $=$ Total coats collected - Coats collected at high schools $=9437$ - $6922=2515$. Answer: 2515. Therefore, the answer (arabic numerals) is 2515 .</p>
<p>Q: Tim 's cat had kittens. He gave 3 to Jessica and 6 to Sara. He now has 9 kittens. How many kittens did he have to start with?
A: Let's think step by step. Tim had 9 kittens remaining after giving 3 to Jessica and 6 to Sara. To find out how many kittens he had to start with, we need to add the number of kittens he gave away to the number of kittens he still has. 9 (remaining kittens) +3 (kittens given to Jessica) +6 (kittens given to Sara) $=18$ total kittens. Therefore, Tim had 18 kittens to start with. Therefore, the answer (arabic numerals) is 18 .</p>
<p>Q: Mike went to 15 basketball games this year, but missed 41 . He went to 39 games last year. How many basketball games did Mike go to in total?
A: Let's think step by step. Mike went to 15 games this year +39 games last year $=54$ total games. Therefore, the answer (arabic numerals) is 54 .</p>
<p>Q: There are 7 crayons in the drawer and 6 crayons on the desk. Sam placed 4 crayons and 8 scissors on the desk. How many crayons are now there in total?
A: Let's think step by step. There were 7 crayons in the drawer and 6 crayons on the desk, for a total of $7+6=13$ crayons. Sam placed 4 crayons and 8 scissors on the desk, bringing the total number of crayons on the desk to $6+4=10$. Therefore, there are now a total of 13 (initial total) +10 (new crayons on the desk) $=23$ crayons in total. Therefore, the answer (arabic numerals) is 23 crayons.</p>
<p>Q: Bonnie 's science class recorded the rainfall each day. They recorded 0.16666666666666666 centimeter of rain on Monday, 0.4166666666666667 centimeter of rain on Tuesday, and 0.0833333333333333 centimeter of rain on Wednesday. How many centimeters of rain did the class record in all?
A: Let's think step by step. Monday: 0.16666666666666666 cm . Tuesday: 0.4166666666666667 cm . Wednesday: 0.08333333333333333 cm . Total: 0.6666666666666666 cm . Therefore, the answer (arabic numerals) is 0.67 cm .</p>
<p>Table 19: Example input and output pairs for AQUA-RAT (arithmetic reasoning).</p>
<p>Q: A car finishes a journey in 20 hours at the speed of $60 \mathrm{~km} / \mathrm{hr}$. If the same distance is to be covered in 10 hours, how much speed does the car gain? Answer Choices: (A) 80 kmph (B) 50 kmph (C) 120 kmph (D) 70 kmph (E) 80 kmph
A: Among A through E, the answer is C) 120 kmph .
Q: Q is as much younger than R as he is older than T . If the sum of the ages of R and T is 50 years. What is definitely the difference between R and Q's age? Answer Choices: (A) 22 (B) 27 (C) 29 (D) Cannot be determined (E) None of the above
A: Among A through E, the answer is (D) Cannot be determined. The information provided does not give enough information to determine the specific difference between R and Q's age.</p>
<p>Q: What is $60 \%$ of $30 \%$ of 1400 grams? Answer Choices: (A) 450 gms (B) 100 gms (C) 252 gms (D) 240 gms (E) None of these
A: Among A through E, the answer is (C) 252 gms. To calculate this, first calculate $30 \%$ of 1400 grams which is $(30 / 100) * 1400=420$ grams. Then take $60 \%$ of 420 grams which is $(60 / 100) * 420=$ 252 grams.</p>
<p>Q: An electric pole, 14 metres high, casts a shadow of 10 metres. Find the height of a tree that casts a shadow of 15 metres under similar conditions. Answer Choices: (A) 21 (B) 22 (C) 20 (D) 23 (E) 24 A: Among A through E, the answer is (B) 22. This is a similar triangles problem. If we let " $x$ " be the height of the tree, then we can set up the following proportion: $(14 / 10)=(\mathrm{x} / 15)$ Solving for x , we get: $\mathrm{x}=(14 * 15) / 10 \mathrm{x}=21$ Therefore, the height of the tree is 22 m .</p>
<p>Q: Alice wants to put up fencing around three sides of her rectangular yard and leave one side of 10 meters unfenced. If the yard has an area of 240 square meters, how many meters of fencing does she need? Answer Choices: (A) 58 (B) 62 (C) 66 (D) 70 (E) 74
A: Among A through E, the answer is (C) 66 meters of fencing.
Q: At my favorite fruit stand, an orange costs 18 dollars, a pineapple costs 27 dollars, and a grape costs 15 dollars. Using the same logic, can you tell how much a mango costs? Answer Choices: (A) 22 dollars (B) 15 dollars (C) 20 dollars (D) 18 dollars (E) 10 dollars
A: Among A through E, the answer is I'm sorry, I am not able to determine the cost of a mango based on the given information. The prices of the orange, pineapple, and grape do not provide enough information to infer the cost of a mango.</p>
<p>Q: A rectangle has a length of 8 centimeters and a width of 3 centimeters. Find the perimeter. Answer Choices: (A) 18 cm (B) 22 cm (C) 20 cm (D) 30 cm (E) 28 cm
A: Among A through E, the answer is (E) 28 cm . To find the perimeter of a rectangle, you add up the lengths of all four sides. In this case, the two sides of length 8 cm and the two sides of width 3 cm , so: $8 \mathrm{~cm}+8 \mathrm{~cm}+3 \mathrm{~cm}+3 \mathrm{~cm}=28 \mathrm{~cm}$</p>
<p>Table 20: Example input and output pairs for AQUA-RAT with chain-of-thought (arithmetic reasoning).</p>
<p>Q: A car finishes a journey in 20 hours at the speed of $60 \mathrm{~km} / \mathrm{hr}$. If the same distance is to be covered in 10 hours, how much speed does the car gain? Answer Choices: (A) 80 kmph (B) 50 kmph (C) 120 kmph (D) 70 kmph (E) 80 kmph
A: Let's think step by step. We know that the car's original speed is $60 \mathrm{~km} / \mathrm{hr}$ and it took 20 hours to complete the journey. To find the distance the car traveled, we can use the formula: distance $=$ speed x time. So, distance $=60 \mathrm{~km} / \mathrm{hr} \times 20$ hours $=1200 \mathrm{~km}$. Now, we know that the car wants to cover the same distance in 10 hours. To find the new speed, we can use the same formula, but with the new time: speed $=$ distance $/$ time $=1200 \mathrm{~km} / 10$ hours $=120 \mathrm{~km} / \mathrm{hr}$. Therefore, the car gains a speed of $120 \mathrm{~km} / \mathrm{hr}-60 \mathrm{~km} / \mathrm{hr}=60 \mathrm{~km} / \mathrm{hr}$. So, the answer is (C) 120 kmph . Therefore, among A through E, the answer is (C) 120 kmph .</p>
<p>Q: Last year, a Home Appliance Store sold an average(arithmetic mean) of 42 microwave ovens per month. In the first 10 months of this year,the store has sold an average(arithmetic mean) of only 20 microwave ovens per month. What was the average number of microwave ovens sold per month during the entire 22 months period ? Answer Choices: (A) 21 (B) 30 (C) 31 (D) 32 (E) None of the above
A: Let's think step by step. The total number of microwave ovens sold in the first 10 months of this year is $20 * 10=200$. The total number of microwave ovens sold last year is $42 * 12=504$. The total number of months is $10+12=22$. So, the average number of microwave ovens sold per month during the entire 22 months period is $(504+200) / 22=704 / 22=32$. Therefore, the answer is (D) 32 . Therefore, among A through E, the answer is (D) 32 .</p>
<p>Q: An electric pole, 14 metres high, casts a shadow of 10 metres. Find the height of a tree that casts a shadow of 15 metres under similar conditions. Answer Choices: (A) 21 (B) 22 (C) 20 (D) 23 (E) 24 A: Let's think step by step. We can use the proportion of the shadow lengths to determine the height of the tree. We know that the electric pole's shadow is $14 / 10=7 / 5$ the length of the electric pole's height. So, if the tree's shadow is 15 metres, and we assume the same ratio of shadow to height as the electric pole, we can set up the equation: (Tree's height) / $15=$ (Electric pole's height) / (Electric pole's shadow) Cross-multiplying and solving for the tree's height, we get: (Tree's height) $=(15$ * Electric pole's height) / (Electric pole's shadow) $=(15 * 14) / 10=21$ So the answer is (A) 21. Therefore, among A through E, the answer is (A) 21 .</p>
<p>Q: At my favorite fruit stand, an orange costs 18 dollars, a pineapple costs 27 dollars, and a grape costs 15 dollars. Using the same logic, can you tell how much a mango costs? Answer Choices: (A) 22 dollars (B) 15 dollars (C) 20 dollars (D) 18 dollars (E) 10 dollars
A: Let's think step by step. It is not possible to determine the cost of a mango based on the given information alone. The prices of the orange, pineapple, and grape do not provide any information about how the cost of a mango might be determined. Additional information would be needed to make an estimate. Therefore, among A through E, the answer is none of the above.</p>
<p>Q: A rectangle has a length of 8 centimeters and a width of 3 centimeters. Find the perimeter. Answer Choices: (A) 18 cm (B) 22 cm (C) 20 cm (D) 30 cm (E) 28 cm
A: Let's think step by step. Perimeter is the sum of all sides of a shape. In the case of a rectangle, it is the sum of the length and width multiplied by 2 . So, the perimeter of a rectangle with a length of 8 centimeters and a width of 3 centimeters would be $(8+3) \times 2=11 \times 2=22$ centimeters. Therefore, the answer is (B) 22 cm . Therefore, among A through E, the answer is (B) 22 cm .</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ experiments done between 06/15/2023 and 06/21/2023&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ Reasoning abilities can be assessed by means of linguistic tasks (Liang et al., 2022), such as with prompt templates and chains of thought in the text format.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
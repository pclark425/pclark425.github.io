<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-212 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-212</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-212</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-11.html">extraction-schema-11</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <p><strong>Paper ID:</strong> paper-c6f21c64c08295e595c82602c37f0bcac96d3907</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c6f21c64c08295e595c82602c37f0bcac96d3907" target="_blank">Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work demonstrates that the reasoning abilities of small-scale LMs can be enhanced through self-training, a process where models learn from their own outputs, and shows that the conventional self-training can be further augmented by a preference learning algorithm called Direct Preference Optimization (DPO).</p>
                <p><strong>Paper Abstract:</strong> Effective training of language models (LMs) for mathematical reasoning tasks demands high-quality supervised fine-tuning data. Besides obtaining annotations from human experts, a common alternative is sampling from larger and more powerful LMs. However, this knowledge distillation approach can be costly and unstable, particularly when relying on closed-source, proprietary LMs like GPT-4, whose behaviors are often unpredictable. In this work, we demonstrate that the reasoning abilities of small-scale LMs can be enhanced through self-training, a process where models learn from their own outputs. We also show that the conventional self-training can be further augmented by a preference learning algorithm called Direct Preference Optimization (DPO). By integrating DPO into self-training, we leverage preference data to guide LMs towards more accurate and diverse chain-of-thought reasoning. We evaluate our method across various mathematical reasoning tasks using different base models. Our experiments show that this approach not only improves LMs' reasoning performance but also offers a more cost-effective and scalable solution compared to relying on large proprietary LMs.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e212.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e212.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT (human CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning with human-written chain-of-thought rationales</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard supervised fine-tuning of pretrained LMs on human-annotated GSM8K chain-of-thought (CoT) rationales; used here as the warm-up / baseline model before self-training or DPO steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-Base / Flan-T5-Large (experiments reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning (chain-of-thought question answering)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Human-written chain-of-thought rationales from GSM8K (human demonstrations with rationale + final answer)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>GSM8K training set ~6,705 examples (reported as ≈7K in tables)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Human-written CoT rationales, exact final answers extractable by rule-based methods; high-quality, gold annotations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact-answer extraction from rationale)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Flan-T5-Base: 18.1% (GSM8K); Flan-T5-Large: 30.8% (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Human CoT SFT provides a low baseline on GSM8K; both self-training and DPO-augmented self-training substantially improve over SFT, indicating limited label-efficiency of raw SFT alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e212.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e212.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Training (ST)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classic self-training using model-generated pseudo-labels filtered by correct final answers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative semi-supervised approach where the SFT model labels unlabeled questions with generated rationales; only generated rationales whose extracted final answers match gold answers are kept as pseudo-labels for further SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-Base / Flan-T5-Large (experiments reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning (chain-of-thought question answering)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Self-generated pseudo-labels: model-sampled rationales on unlabeled questions, filtered by correctness of extracted final answer</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Unlabeled U built from GSM8K questions (~6.7K); pseudo-labels sampled K per question in experiments (e.g., 3 generated, deduplicated); exact final retained set varies after filtering/deduplication</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Filtered by correctness, deduplicated (Jaccard threshold 0.7), depends strongly on pseudo-label quality and model capability</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact-answer extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Flan-T5-Base: 25.9% (GSM8K); Flan-T5-Large: 35.6% (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>SFT: Flan-T5-Base 18.1%; Flan-T5-Large 30.8%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Flan-T5-Base: +7.8% absolute over SFT; Flan-T5-Large: +4.8% absolute over SFT</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Self-training with pseudo-labels filtered by answer correctness yields large improvements over pure SFT, demonstrating that self-generated CoT data (when filtered) is an effective, low-cost data source for improving math reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e212.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e212.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPO-aug Self-Training (DPO-ST)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-training augmented with Direct Preference Optimization (DPO) on preference pairs derived from generated rationales</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative pipeline that alternates between (1) constructing preference pairs (winning/losing model completions) and tuning the model with DPO, and (2) generating filtered pseudo-labels for supervised fine-tuning; introduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-Base / Flan-T5-Large; also applied to Llama family in paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>DPO</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning (chain-of-thought question answering)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Preference pairs constructed from multiple model-generated rationales per question (winning completions = those with correct final answer; losing = incorrect); DPO uses these pairs directly (no separate reward model)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>DPO sampling: 5 completions per question from SFT model for DPO dataset (U size ≈6.7K questions => up to ~5×6.7K completions sampled); in SFT steps generate K pseudolabels per question (K varied: 3,5,10 in ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Preference labels derived automatically from correctness of final answers (clear binary preference); encourages both higher-quality and more diverse rationales and increases the number of distinct pseudo-labels after deduplication</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact-answer extraction); Pass@K (quality + diversity)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Flan-T5-Base: 27.2% (GSM8K); Flan-T5-Large: 37.4% (GSM8K). Ablation on K for Flan-T5-Large: K=3 => 37.4%, K=5 => 39.1%, K=10 => 40.0%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Self-Training baseline: Flan-T5-Base 25.9%, Flan-T5-Large 35.6%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Vs SFT: Flan-T5-Base +9.1% abs, Flan-T5-Large +6.6% abs; Vs ST: Flan-T5-Base +1.3% abs, Flan-T5-Large +1.8% abs; Increasing sampled SFT solutions K (3→10) further yields +2.6% abs for Flan-T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>DPO-trained preference objective on model-derived preference pairs produces models that generate more and more diverse high-quality rationales (notably improving Pass@10) and yields consistent accuracy gains over classic self-training, with larger K (more pseudo-label diversity) further improving final SFT performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e212.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e212.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Distillation (LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distillation of chain-of-thought rationales from larger proprietary LLMs (e.g., Codex, PaLM, GPT-3/4) into smaller models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generating synthetic CoT training annotations by prompting much larger models and then fine-tuning smaller models on those synthetic annotations (knowledge distillation); discussed and compared in the paper's related work and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex, PaLM, GPT-3/4 as annotators; smaller models (Flan-T5, Llama) as distilled students in prior work</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT / distillation (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning (chain-of-thought question answering)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Synthetic chain-of-thought rationales produced by large LMs (e.g., Codex, PaLM, GPT-3/4); used as annotations for supervised fine-tuning of smaller models</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Varied in cited works: examples in table — Socratic CoT (GPT-3) 7K, CoT from CodeX 100K, CoT from PaLM 6K (as reported in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>High-quality and diverse rationales when generated by very large LMs, but costly (FLOPs, API usage) and sometimes unstable depending on the annotator</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact-answer extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Reported in comparisons: Socratic CoT (GPT-3 7K) 21.1% GSM8K; CoT from CodeX (100K) 20.2%; CoT from PaLM (6K) 22.2% (these are prior results shown in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Distillation from very large LMs can yield improvements in some settings but is resource-intensive; in the paper's comparisons, the authors' DPO-ST (using only human GSM8K annotations + self-generated data) outperforms several prior LLM-distillation baselines while being more label- and compute-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e212.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e212.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calculator-augmented decoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Batch decoding with an external calculator to ensure arithmetic correctness in generated rationales</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Integration of an external calculator into the decoding process that detects calculation patterns and replaces them with exact evaluated results, reducing arithmetic errors and false-positive pseudo-labels while supporting larger batch decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-Large; Llama family comparisons reported</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT / inference (affects pseudo-label quality used in ST/DPO-ST)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning (chain-of-thought question answering)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Tool-augmented decoding outputs: model-generated rationales whose calculation tokens are evaluated by an external calculator and replaced; this affects the correctness of generated pseudo-labels used for SFT/DPO</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Removes arithmetic mistakes in rationales, reduces false-positive pseudo-labels (cases where final answer looks correct but intermediate steps are flawed), improves pseudo-label quality and downstream accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact-answer extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Examples from paper: SFT (Llama-1-7b) 35.9% -> SFT w/ Calculator 40.0% (GSM8K); SFT (Llama-2-7b) 41.6% -> SFT w/ Calculator 45.1% (GSM8K). Flan-T5-Large experiments show marked drops when removing calculator (Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>SFT without calculator (see above numbers)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Llama-1-7b: +4.1% abs; Llama-2-7b: +3.5% abs (SFT with vs without calculator). Also improves downstream pseudo-label quality, increasing effective pseudolabel count and final model accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Using an external calculator during decoding markedly improves final accuracy by reducing arithmetic errors and false-positive pseudo-labels, and it enables higher-quality pseudo-label generation in self-training / DPO-ST pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Direct preference optimization: Your language model is secretly a reward model. <em>(Rating: 2)</em></li>
                <li>Star: Bootstrapping reasoning with reasoning. <em>(Rating: 2)</em></li>
                <li>Specializing smaller language models towards multi-step reasoning. <em>(Rating: 2)</em></li>
                <li>Large language models are reasoning teachers. <em>(Rating: 2)</em></li>
                <li>Calc-x and calcformers: Empowering arithmetical chain-of-thought through interaction with symbolic systems. <em>(Rating: 2)</em></li>
                <li>Scaling relationship on learning mathematical reasoning with large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-212",
    "paper_id": "paper-c6f21c64c08295e595c82602c37f0bcac96d3907",
    "extraction_schema_id": "extraction-schema-11",
    "extracted_data": [
        {
            "name_short": "SFT (human CoT)",
            "name_full": "Supervised Fine-Tuning with human-written chain-of-thought rationales",
            "brief_description": "Standard supervised fine-tuning of pretrained LMs on human-annotated GSM8K chain-of-thought (CoT) rationales; used here as the warm-up / baseline model before self-training or DPO steps.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5-Base / Flan-T5-Large (experiments reported)",
            "model_size": null,
            "training_stage": "SFT",
            "task_type": "math reasoning (chain-of-thought question answering)",
            "is_scientific_domain": false,
            "data_type": "Human-written chain-of-thought rationales from GSM8K (human demonstrations with rationale + final answer)",
            "data_size": "GSM8K training set ~6,705 examples (reported as ≈7K in tables)",
            "data_properties": "Human-written CoT rationales, exact final answers extractable by rule-based methods; high-quality, gold annotations",
            "performance_metric": "accuracy (exact-answer extraction from rationale)",
            "performance_with_data": "Flan-T5-Base: 18.1% (GSM8K); Flan-T5-Large: 30.8% (GSM8K)",
            "performance_baseline": null,
            "performance_lift": null,
            "compares_data_types": true,
            "key_finding": "Human CoT SFT provides a low baseline on GSM8K; both self-training and DPO-augmented self-training substantially improve over SFT, indicating limited label-efficiency of raw SFT alone.",
            "uuid": "e212.0",
            "source_info": {
                "paper_title": "Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Self-Training (ST)",
            "name_full": "Classic self-training using model-generated pseudo-labels filtered by correct final answers",
            "brief_description": "Iterative semi-supervised approach where the SFT model labels unlabeled questions with generated rationales; only generated rationales whose extracted final answers match gold answers are kept as pseudo-labels for further SFT.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5-Base / Flan-T5-Large (experiments reported)",
            "model_size": null,
            "training_stage": "SFT",
            "task_type": "math reasoning (chain-of-thought question answering)",
            "is_scientific_domain": false,
            "data_type": "Self-generated pseudo-labels: model-sampled rationales on unlabeled questions, filtered by correctness of extracted final answer",
            "data_size": "Unlabeled U built from GSM8K questions (~6.7K); pseudo-labels sampled K per question in experiments (e.g., 3 generated, deduplicated); exact final retained set varies after filtering/deduplication",
            "data_properties": "Filtered by correctness, deduplicated (Jaccard threshold 0.7), depends strongly on pseudo-label quality and model capability",
            "performance_metric": "accuracy (exact-answer extraction)",
            "performance_with_data": "Flan-T5-Base: 25.9% (GSM8K); Flan-T5-Large: 35.6% (GSM8K)",
            "performance_baseline": "SFT: Flan-T5-Base 18.1%; Flan-T5-Large 30.8%",
            "performance_lift": "Flan-T5-Base: +7.8% absolute over SFT; Flan-T5-Large: +4.8% absolute over SFT",
            "compares_data_types": true,
            "key_finding": "Self-training with pseudo-labels filtered by answer correctness yields large improvements over pure SFT, demonstrating that self-generated CoT data (when filtered) is an effective, low-cost data source for improving math reasoning.",
            "uuid": "e212.1",
            "source_info": {
                "paper_title": "Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "DPO-aug Self-Training (DPO-ST)",
            "name_full": "Self-training augmented with Direct Preference Optimization (DPO) on preference pairs derived from generated rationales",
            "brief_description": "Iterative pipeline that alternates between (1) constructing preference pairs (winning/losing model completions) and tuning the model with DPO, and (2) generating filtered pseudo-labels for supervised fine-tuning; introduced in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-Base / Flan-T5-Large; also applied to Llama family in paper",
            "model_size": null,
            "training_stage": "DPO",
            "task_type": "math reasoning (chain-of-thought question answering)",
            "is_scientific_domain": false,
            "data_type": "Preference pairs constructed from multiple model-generated rationales per question (winning completions = those with correct final answer; losing = incorrect); DPO uses these pairs directly (no separate reward model)",
            "data_size": "DPO sampling: 5 completions per question from SFT model for DPO dataset (U size ≈6.7K questions =&gt; up to ~5×6.7K completions sampled); in SFT steps generate K pseudolabels per question (K varied: 3,5,10 in ablations)",
            "data_properties": "Preference labels derived automatically from correctness of final answers (clear binary preference); encourages both higher-quality and more diverse rationales and increases the number of distinct pseudo-labels after deduplication",
            "performance_metric": "accuracy (exact-answer extraction); Pass@K (quality + diversity)",
            "performance_with_data": "Flan-T5-Base: 27.2% (GSM8K); Flan-T5-Large: 37.4% (GSM8K). Ablation on K for Flan-T5-Large: K=3 =&gt; 37.4%, K=5 =&gt; 39.1%, K=10 =&gt; 40.0%",
            "performance_baseline": "Self-Training baseline: Flan-T5-Base 25.9%, Flan-T5-Large 35.6%",
            "performance_lift": "Vs SFT: Flan-T5-Base +9.1% abs, Flan-T5-Large +6.6% abs; Vs ST: Flan-T5-Base +1.3% abs, Flan-T5-Large +1.8% abs; Increasing sampled SFT solutions K (3→10) further yields +2.6% abs for Flan-T5-Large",
            "compares_data_types": true,
            "key_finding": "DPO-trained preference objective on model-derived preference pairs produces models that generate more and more diverse high-quality rationales (notably improving Pass@10) and yields consistent accuracy gains over classic self-training, with larger K (more pseudo-label diversity) further improving final SFT performance.",
            "uuid": "e212.2",
            "source_info": {
                "paper_title": "Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Knowledge Distillation (LLM)",
            "name_full": "Distillation of chain-of-thought rationales from larger proprietary LLMs (e.g., Codex, PaLM, GPT-3/4) into smaller models",
            "brief_description": "Generating synthetic CoT training annotations by prompting much larger models and then fine-tuning smaller models on those synthetic annotations (knowledge distillation); discussed and compared in the paper's related work and baselines.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Codex, PaLM, GPT-3/4 as annotators; smaller models (Flan-T5, Llama) as distilled students in prior work",
            "model_size": null,
            "training_stage": "SFT / distillation (prior work)",
            "task_type": "math reasoning (chain-of-thought question answering)",
            "is_scientific_domain": false,
            "data_type": "Synthetic chain-of-thought rationales produced by large LMs (e.g., Codex, PaLM, GPT-3/4); used as annotations for supervised fine-tuning of smaller models",
            "data_size": "Varied in cited works: examples in table — Socratic CoT (GPT-3) 7K, CoT from CodeX 100K, CoT from PaLM 6K (as reported in comparisons)",
            "data_properties": "High-quality and diverse rationales when generated by very large LMs, but costly (FLOPs, API usage) and sometimes unstable depending on the annotator",
            "performance_metric": "accuracy (exact-answer extraction)",
            "performance_with_data": "Reported in comparisons: Socratic CoT (GPT-3 7K) 21.1% GSM8K; CoT from CodeX (100K) 20.2%; CoT from PaLM (6K) 22.2% (these are prior results shown in Table 3)",
            "performance_baseline": null,
            "performance_lift": null,
            "compares_data_types": true,
            "key_finding": "Distillation from very large LMs can yield improvements in some settings but is resource-intensive; in the paper's comparisons, the authors' DPO-ST (using only human GSM8K annotations + self-generated data) outperforms several prior LLM-distillation baselines while being more label- and compute-efficient.",
            "uuid": "e212.3",
            "source_info": {
                "paper_title": "Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Calculator-augmented decoding",
            "name_full": "Batch decoding with an external calculator to ensure arithmetic correctness in generated rationales",
            "brief_description": "Integration of an external calculator into the decoding process that detects calculation patterns and replaces them with exact evaluated results, reducing arithmetic errors and false-positive pseudo-labels while supporting larger batch decoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-Large; Llama family comparisons reported",
            "model_size": null,
            "training_stage": "SFT / inference (affects pseudo-label quality used in ST/DPO-ST)",
            "task_type": "math reasoning (chain-of-thought question answering)",
            "is_scientific_domain": false,
            "data_type": "Tool-augmented decoding outputs: model-generated rationales whose calculation tokens are evaluated by an external calculator and replaced; this affects the correctness of generated pseudo-labels used for SFT/DPO",
            "data_size": null,
            "data_properties": "Removes arithmetic mistakes in rationales, reduces false-positive pseudo-labels (cases where final answer looks correct but intermediate steps are flawed), improves pseudo-label quality and downstream accuracy",
            "performance_metric": "accuracy (exact-answer extraction)",
            "performance_with_data": "Examples from paper: SFT (Llama-1-7b) 35.9% -&gt; SFT w/ Calculator 40.0% (GSM8K); SFT (Llama-2-7b) 41.6% -&gt; SFT w/ Calculator 45.1% (GSM8K). Flan-T5-Large experiments show marked drops when removing calculator (Figure 7).",
            "performance_baseline": "SFT without calculator (see above numbers)",
            "performance_lift": "Llama-1-7b: +4.1% abs; Llama-2-7b: +3.5% abs (SFT with vs without calculator). Also improves downstream pseudo-label quality, increasing effective pseudolabel count and final model accuracy.",
            "compares_data_types": true,
            "key_finding": "Using an external calculator during decoding markedly improves final accuracy by reducing arithmetic errors and false-positive pseudo-labels, and it enables higher-quality pseudo-label generation in self-training / DPO-ST pipelines.",
            "uuid": "e212.4",
            "source_info": {
                "paper_title": "Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model.",
            "rating": 2,
            "sanitized_title": "direct_preference_optimization_your_language_model_is_secretly_a_reward_model"
        },
        {
            "paper_title": "Star: Bootstrapping reasoning with reasoning.",
            "rating": 2,
            "sanitized_title": "star_bootstrapping_reasoning_with_reasoning"
        },
        {
            "paper_title": "Specializing smaller language models towards multi-step reasoning.",
            "rating": 2,
            "sanitized_title": "specializing_smaller_language_models_towards_multistep_reasoning"
        },
        {
            "paper_title": "Large language models are reasoning teachers.",
            "rating": 2,
            "sanitized_title": "large_language_models_are_reasoning_teachers"
        },
        {
            "paper_title": "Calc-x and calcformers: Empowering arithmetical chain-of-thought through interaction with symbolic systems.",
            "rating": 2,
            "sanitized_title": "calcx_and_calcformers_empowering_arithmetical_chainofthought_through_interaction_with_symbolic_systems"
        },
        {
            "paper_title": "Scaling relationship on learning mathematical reasoning with large language models.",
            "rating": 1,
            "sanitized_title": "scaling_relationship_on_learning_mathematical_reasoning_with_large_language_models"
        }
    ],
    "cost": 0.0157155,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Training with Direct Preference Optimization Improves Chain-of-Thought Reasoning</h1>
<p>Tianduo Wang ${ }^{\dagger}$ Shichen $\mathbf{L i}^{\ddagger}$ Wei $\mathbf{L u}^{\dagger}$<br>${ }^{\dagger}$ StatNLP Research Group, Singapore University of Technology and Design<br>${ }^{\ddagger}$ Soochow University<br>{tianduo_wang,luwei}@sutd.edu.sg scli_21@outlook.com<br>https://github.com/tianduowang/dpo-st</p>
<h4>Abstract</h4>
<p>Effective training of language models (LMs) for mathematical reasoning tasks demands high-quality supervised fine-tuning data. Besides obtaining annotations from human experts, a common alternative is sampling from larger and more powerful LMs. However, this knowledge distillation approach can be costly and unstable, particularly when relying on closed-source, proprietary LMs like GPT4 (OpenAI, 2023), whose behaviors are often unpredictable. In this work, we demonstrate that the reasoning abilities of small-scale LMs can be enhanced through self-training, a process where models learn from their own outputs. We also show that the conventional self-training can be further augmented by a preference learning algorithm called Direct Preference Optimization (DPO) (Rafailov et al., 2023). By integrating DPO into self-training, we leverage preference data to guide LMs towards more accurate and diverse chain-of-thought reasoning. We evaluate our method across various mathematical reasoning tasks using different base models. Our experiments show that this approach not only improves LMs' reasoning performance but also offers a more cost-effective and scalable solution compared to relying on large proprietary LMs.</p>
<h2>1 Introduction</h2>
<p>Making language models (LMs) perform mathematical reasoning is a valuable, yet challenging research objective (Hendrycks et al., 2021; Cobbe et al., 2021). Recent efforts have focused on enhancing large-scale LMs' reasoning abilities through various methods, including chain-ofthought prompting (Wei et al., 2022b; Kojima et al., 2022), continual pretraining (Azerbayev et al., 2024), and adding external verifiersq (Li et al., 2023b). However, the research question of how to enhance the reasoning capabilities of smallersized LMs remains relatively under-explored.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our approach demonstrates superior performance on the GSM8K benchmark while minimizing the required compute cost, including both training and inference. Compute cost calculations are based on the methodology outlined by Yuan et al. (2023). ${ }^{\dagger}$</p>
<p>Recent studies (Fu et al., 2023; Magister et al., 2023; Li et al., 2023a) demonstrate that the reasoning capabilities of smaller LMs can be significantly enhanced through learning from the outputs of larger and more advanced LMs, such as Codex (Chen et al., 2021), PaLM (Chowdhery et al., 2022), and GPT-4 (OpenAI, 2023). While this method is straightforward to implement, the associated costs can be substantial. The computational demand, measured in floating-point operations (FLOPs), increases considerably when using large LMs. Additionally, the reliance on proprietary large LMs for data annotation not only incurs high economic costs but also raises concerns regarding the sustainability and scalability of such practices. For instance, Ho et al. (2023) highlighted that while employing large LMs as annotators can largely enhance the performance of smaller LMs, it introduces a clear trade-off between economic costs and performance gains.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Another line of research focuses on exploring enhancements through self-improvement methods <em>Zelikman et al. (2022); Gulcehre et al. (2023); Singh et al. (2023)</em>. These methods diverge from using outputs from larger models, instead encouraging LMs to learn from their own generated data. The effectiveness of these techniques is evident, yet their success largely depends upon the inherent capabilities of the base models. For example, <em>Zelikman et al. (2022)</em> initiated self-improvement by few-shot prompting GPT-J <em>Wang and Komatsuzaki (2021)</em>, a relatively large LM which has 6 billion parameters, to generate rationales an emergent ability typically reserved for large models <em>Wei et al. (2022a)</em>. However, the extent to which small-scale LMs can gain from self-improvement remains uncertain.</p>
<p>In this work, we introduce a novel enhancement to the conventional self-training framework by incorporating Direct Preference Optimization (DPO) <em>Rafailov et al. (2023)</em>. This integration specifically targets performance objectives within chain-of-thought reasoning, with a particular focus on mathematical reasoning. The clear-cut nature of mathematical solutions enables straightforward validation of a model's outputs, facilitating the creation of a preference dataset for DPO. Our empirical results indicate that this method notably enhances the reasoning capabilities of LMs while also reducing computational overhead. We visualize the relationship between the GSM8K <em>Cobbe et al. (2021)</em> performance and computational cost across various specialized models in Figure 1. It can be observed that our method not only achieves strong performance, but also reduces computational demands by effectively utilizing self-generated data for learning. Overall, the main contribution of this work can be summarized as follows:</p>
<ul>
<li>We propose a novel extension to the classic self-training framework by integrating Direct Preference Optimization, demonstrating its effectiveness across various math reasoning tasks.</li>
<li>Our method significantly enhances the reasoning abilities of language models while requiring minimal computational resources, optimizing both performance and efficiency.</li>
<li>We present an efficient method for integrating LMs with external tools, which significantly boosts downstream task performance without notably compromising inference speed.</li>
</ul>
<p>Input: pre-trained language model $f_{\theta}$ labeled dataset $\mathcal{L}=\left{\left(x^{i}, y^{i}, a^{i}\right)\right}<em i="1">{i=1}^{l}$ unlabeled dataset $\mathcal{U}=\left{\left(x^{i}, a^{i}\right)\right}</em>$
Output: fine-tuned model $f_{\theta^{\prime}}$
1: Fine-tune $f_{\theta}$ on $\mathcal{L}$ to get $f_{\theta^{\prime}}$
2: repeat
3: Build pseudo-labeled dataset $\mathcal{S}$ : $\mathcal{S}=\left{\left(x^{i}, \hat{y}^{i}, \hat{a}^{i}\right)\right}}^{u<em _theta_prime="\theta^{\prime">{i=1}^{s}$ where $x^{i} \sim \mathcal{U}$ and $\hat{y}^{i}, \hat{a}^{i} \sim f</em>\right)$
4: $\quad$ Select $\mathcal{S}^{\alpha} \subset \mathcal{S}$ when $\hat{a}^{i}=a^{i}$
5: Update $\mathcal{L} \leftarrow \mathcal{S}^{\alpha} \cup \mathcal{L}$
6: $\quad$ Train $f_{\theta}$ on $\mathcal{L}$ to get a new $f_{\theta^{\prime}}$
7: until convergence or max iteration is reached}}\left(\cdot \mid x^{i</p>
<h2>2 Background</h2>
<p>Math word problem solving The math word problem solving task <em>Cobbe et al. (2021)</em> can be formulated as a sequence-to-sequence task where the input $x$ is a question asking for an unknown value and the output $y$ is a rationale that leads to the answer $a$. Normally, the answers can be extracted from the rationales via some rule-based methods, such as regular expressions. A generated rationale $\hat{y}$ is regarded as correct if the extracted answer $\hat{a}$ matches the gold answer $a$. Formally, the labeled dataset for a math word problem solving task with $l$ instances can be represented as:</p>
<p>$$
\mathcal{L}=\left{\left(x^{i}, y^{i}, a^{i}\right)\right}_{i=1}^{l}
$$</p>
<p>A common way for specializing a LM $f_{\theta}$ towards math reasoning with the labeled dataset $\mathcal{L}$ is supervised fine-tuning (SFT). It optimizes $f_{\theta}$ by minimizing the negative log likelihood loss $\mathcal{L}_{\mathrm{SFT}}(\theta)$ :</p>
<p>$$
\underset{(x, y) \sim \mathcal{L}}{\mathbb{E}}-\left[\sum_{t=1}^{T} \log f_{\theta}\left(y_{t} \mid x, y_{1: t-1}\right)\right]
$$</p>
<p>where $T$ is the length of the rationale $y$ and we use $y_{t}$ to represent the $t$-th token in $y$.</p>
<p>Self-training Self-training is one of the earliest approaches in semi-supervised learning <em>Scudder (1965); Fralick (1967)</em> that has risen in popularity recently <em>He et al. (2020); Amini et al. (2022)</em>. This method first regards a base model trained with a labeled dataset $\mathcal{L}$ as teacher, and uses it to build a pseudo-labeled dataset $\mathcal{S}$ by annotating an unlabeled dataset $\mathcal{U}$. Then, a student model is trained on a combination of $\mathcal{L}$ and $\mathcal{S}$ that are expected to</p>
<p>outperform the teacher model. Such a framework has been shown effective across a wide range of natural language processing tasks, including natural language understanding (Vu et al., 2021) and generation (He et al., 2020). A formal description of a self-training algorithm for chain-of-thought (CoT) reasoning tasks is provided in Algorithm 1.</p>
<p>Previous studies have demonstrated that the quality of the pseudo-labels largely impacts the overall performance of the self-training algorithm (He et al., 2020; Amini et al., 2022). For example, Gulcehre et al. (2023) proposed to select high-quality pseudo-labels with a learned reward function. Zelikman et al. (2022) filtered the generated rationales to include only the ones that lead to correct answers. Although many methods are proposed to select pseudo-labels, few works discuss how to improve the fine-tuned model $f_{\theta^{\prime}}$ so that more high-quality pseudo-labels can be generated. In this paper, we present a method to enhance $f_{\theta^{\prime}}$ in each iteration so that higher-quality pseudo-labeled data can be generated.</p>
<p>Direct Preference Optimization The Reinforcement Learning from Human Feedback (RLHF) methods align LMs with human preference (Ouyang et al., 2022; Bai et al., 2022). The standard pipeline of RLHF requires to first train a reward model from human preference data. Then, the reward model is used to fine-tune language models via reinforcement learning objective, e.g., Proximal Policy Optimization (Schulman et al., 2017). A recent study propose Direct Preference Optimization (DPO) (Rafailov et al., 2023) to avoid explicitly training a reward model so that language models can be directly tuned with human preference data.</p>
<p>The DPO pipeline can be described as follows. First, given some prompt $x$, we sample several completions from the reference model $\pi_{\text {ref }}$ (normally it is the model after supervised fine-tuning):</p>
<p>$$
y_{1}, y_{2} \sim \pi_{\mathrm{ref}}(\cdot \mid x)
$$</p>
<p>Next, construct the DPO dataset $\mathcal{D}$ from the completions based on the human preference:</p>
<p>$$
\mathcal{D}=\left{\left(x^{i}, y_{w}^{i}, y_{l}^{i}\right)\right}_{i=1}^{N}
$$</p>
<p>where $y_{w}^{i}$ and $y_{l}^{i}$ represent the winning and losing completions respectively. Then, we optimize the language model $\pi_{\theta}$ to minimize $\mathcal{L}<em _theta="\theta">{\text {DPO }}\left(\pi</em>\right)$} ; \pi_{\text {ref }</p>
<div class="codehilite"><pre><span></span><code>Algorithm 2 DPO-augmented self-training
    Input: pre-trained language model \(f_{\theta}\)
        labeled dataset \(\mathcal{L}=\left\{\left(x^{i}, y^{i}, a^{i}\right)\right\}_{i=1}^{l}\)
        unlabeled dataset \(\mathcal{U}=\left\{\left(x^{i}, a^{i}\right)\right\}_{i=1}^{n}\)
    Output: fine-tuned model \(f_{\theta^{\prime}}\)
    \# Warm-up stage
    Fine-tune \(f_{\theta}\) on \(\mathcal{L}\) to get \(f_{\theta^{\prime}}\)
    repeat
        \# DPO step
        Generate DPO dataset \(\mathcal{D}\)
            \(\mathcal{D}=\left\{\left(x^{i}, y_{w}^{i}, y_{l}^{i}\right)\right\}_{i=1}^{N}\)
            where \(x^{i} \sim \mathcal{U}\) and \(y_{w}^{i}, y_{l}^{i} \sim f_{\theta^{\prime}}\left(\cdot \mid x^{i}\right)\)
            Tune \(f_{\theta^{\prime}}\) with \(\mathcal{L}_{\text {DPO }}\) on \(\mathcal{D}\) to get \(f_{\theta^{\prime}}\)
            \# SFT step
            Build pseudo-labeled dataset \(\mathcal{S}\)
            \(\mathcal{S}=\left\{\left(x^{i}, \hat{y}^{i}, \hat{a}^{i}\right)\right\}_{i=1}^{4}\)
            where \(x^{i} \sim \mathcal{U}\) and \(\hat{y}^{i}, \hat{a}^{i} \sim f_{\theta^{\prime}}\left(\cdot \mid x^{i}\right)\)
            Select \(\mathcal{S}^{\alpha} \subset \mathcal{S}\) when \(\hat{a}^{i}=a^{i}\)
            Update \(\mathcal{L} \leftarrow \mathcal{S}^{\alpha} \cup \mathcal{L}\)
            Train \(f_{\theta}\) on \(\mathcal{L}\) to get a new \(f_{\theta^{\prime}}\)
    until convergence or max iteration is reached
</code></pre></div>

<p>which can be defined as follows:</p>
<p>$$
\underset{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}{\mathbb{E}}\left[-\log \sigma\left(r\left(y_{l} \mid x\right)-r\left(y_{l} \mid x\right)\right)\right]
$$</p>
<p>where $r(\cdot \mid x)=\beta \log \frac{\pi_{\theta}(\cdot \mid x)}{\pi_{\text {ref }}(\cdot \mid x)}$ and $\beta$ is a coefficient that controls $\pi_{\theta}$ 's deviation from $\pi_{\text {ref }}$.</p>
<h2>3 Method</h2>
<p>In this section, we first describe the proposed approach. Then, we demonstrate how we integrate an external calculator into the model's decoding process which significantly improves LMs' performance on the downstream tasks.</p>
<h3>3.1 DPO-augmented Self-Training</h3>
<p>Our approach starts with a warm-up stage, and then followed by an iterative process, where each iteration is composed of two sub-steps: DPO step and SFT step. The iterative process ends when the model performance converges or reaches the maximum iteration. A formal description of the proposed method is illustrated in Algorithm 2. An illustration of our method is presented in Figure 2.</p>
<p>Warm-up stage Like classic self-training, we start by fine-tuning the base model $f_{\theta}$ to optimize $\mathcal{L}<em _theta_prime="\theta^{\prime">{\mathrm{SFT}}(\theta)$ on the labeled data $\mathcal{L}$, resulting in an updated model $f</em>$. After this stage, we assume that}</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: An illustration of the DPO-augmented Self-Training framework. Traditional self-training method uses the SFT model to generate the pseudo-labels for subsequent iterations. In contrast, our method enhances the SFT model with Direct Preference Optimization (DPO), using the optimized DPO model to produce the pseudo-labels.</p>
<p>$f_{\theta^{\prime}}$ is capable of solving certain math problems. Specifically, given a math question $x$, $f_{\theta^{\prime}}$ will generate a rationale $\hat{y}$ with answer $\hat{a}$.</p>
<p>Iterative step 1: DPO step In this step, we first sample rationales $\hat{y}$ from the fine-tuned model $f_{\theta^{\prime}}$ given some questions $x$ from $\mathcal{U}$. For each question $x$, we generate multiple rationales to build the DPO training dataset $\mathcal{D}$. As mentioned, for math problem solving tasks, it is easy to know whether a generated rationale $\hat{y}$ can be considered as correct. We label rationales with correct answers as winning completions, while consider rationales with incorrect answers as losing completions. Then, we train $f_{\theta^{\prime}}$ on $\mathcal{D}$ to optimize the objective function $\mathcal{L}<em _prime="\prime" _theta_prime="\theta^{\prime">{\text{DPO}}$ and get a DPO model $f</em>$ in the end.}</p>
<p>Iterative step 2: SFT step After obtaining $f_{\theta^{\prime \prime}}$, we use it to generate a new pseudo-labeled dataset $\mathcal{S}$ for the next-round supervised fine-tuning:</p>
<p>$\mathcal{S}={(x,\hat{y})|x\sim\mathcal{U},\hat{y}\sim f_{\theta^{\prime \prime}}(\cdot|x)}$ (6)</p>
<p>After generation, we clean $\mathcal{S}$ by eliminating rationales with incorrect answers and removing duplicates. Therefore, the pseudo-labeled dataset we obtained in the end is a subset of the original one, i.e., $\mathcal{S}^{\alpha} \subset \mathcal{S}$. The final training dataset is the combination of the original labeled dataset $\mathcal{L}$ and the newly-generated pseudo-labeled dataset $\mathcal{S}^{\alpha}$. Notice that during this process, once we collect a new dataset, we train from the original base model $f_{\theta}$ instead of continually fine-tuning $f_{\theta^{\prime}}$ to avoid overfitting, following previous practice [zelikman2022few; singh2023few].</p>
<p>Q: James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year?
A: He writes each friend
$3 * 2=\ll 3 * 2=6 \gg&gt;6$ pages a week.
So he writes
$6 * 2=\ll 6 * 2=12 \gg&gt;12$ pages every week.
That means he writes
$12 * 52=\ll 12 * 52=624 \gg&gt;624$ pages a year.
#### 624
Figure 3: An example from the GSM8K dataset. The calculation annotations are highlighted in blue. All calculation steps are wrapped within special tokens &lt;&lt;...&gt;&gt;. During decoding, the calculator will be triggered when such patterns exist and the model's output tokens will be overridden by the calculator results. Following [cobbe2021c] the calculation is performed with the in-built python function eval().</p>
<h3>3.2 Batch Decoding with Calculator</h3>
<p>Empirical observations indicate that while large LMs, such as those described in [brown2020language], demonstrate superior proficiency in basic arithmetic calculations, smaller LMs like Flan-T5Large tend to struggle with similar arithmetic tasks. This limitation significantly affects their performance in math reasoning tasks. To address this, various studies [parisi2022semi; schick2023semi; kadl2023semi] have explored augmenting small-scale models with an external calculator to boost their arithmetic capabilities. However, many of these existing methods are limited to a batch size of one during decoding. This constraint substantially reduces the inference speed and limits their practical application.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Inference speed comparison between our methods (w/ and w/o calculator) and Calcformer (Kadlčík et al., 2023) with varying batch sizes. The results are measured on a single NVIDIA A40 GPU.</p>
<p>To address this challenge, we propose a simple yet efficient method that allows for using larger batch sizes during inference with an external calculator. Our approach leverages the calculator annotations provided in the original GSM8K dataset (Cobbe et al., 2021). Figure 3 demonstrates an example of this annotation and describes how such annotations can be used during decoding. For optimal utilization of these annotations, we build our models with the Transformers library (Wolf et al., 2020). During inference, we employ a customized LogitsProcessor²–available in the Transformers documentation–to adjust the model's generation process. This LogitsProcessor acts as an interface, allowing modifications to the outputs of the model during generation and thereby enabling efficient management of larger batch sizes.</p>
<p>To demonstrate the efficiency of the proposed solution, we compare the inference speed of our methods (w/ and w/o calculator) based on Flan-T5-Large against an open-source tool-using method, Calcformer (Kadlčík et al., 2023) based on T5-Large, in Figure 4. We find that when the batch size equals 1, all three methods have a similar inference speed of around 40 tokens per second. However, as the inference batch size increases, the speedup of our methods increases significantly.</p>
<h2>4 Experiments</h2>
<p>In this section, we first outline our experiment setup and implementation details, then present our models' performance on various math reasoning tasks against competitive baselines. Finally, we analyze the effectiveness of our method empirically.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Split</th>
<th># Data</th>
</tr>
</thead>
<tbody>
<tr>
<td>GSM8K (Cobbe et al., 2021)</td>
<td>Train</td>
<td>6,705</td>
</tr>
<tr>
<td></td>
<td>Validation</td>
<td>768</td>
</tr>
<tr>
<td></td>
<td>Test</td>
<td>1,319</td>
</tr>
<tr>
<td>MultiArith (Roy and Roth, 2015)</td>
<td>Test</td>
<td>600</td>
</tr>
<tr>
<td>ASDiv (Miao et al., 2020)</td>
<td>Test</td>
<td>2,096</td>
</tr>
<tr>
<td>SVAMP (Patel et al., 2021)</td>
<td>Test</td>
<td>1,000</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of the datasets used in our experiments. The original GSM8K dataset only contains train and test split. We randomly select 768 training examples to construct the validation dataset in our experiments.</p>
<h3>4.1 Setup</h3>
<p><strong>Base models</strong> We employ Flan-T5 models (Chung et al., 2024) as our primary base models. Specifically, we consider two variants from the Flan-T5 family: Flan-T5-Base and Flan-T5-Large. We select Flan-T5 over the original T5 models (Raffel et al., 2019) as our backbone models based on the evidence from previous research (Chung et al., 2024; Fu et al., 2023), which demonstrates that instruction-tuned models like Flan-T5 outperform their pre-trained counterparts in mathematical reasoning tasks. To broaden our analysis, we also include Llama models (Touvron et al., 2023a,b; Meta, 2024) as additional base models for comparison.</p>
<p><strong>Datasets</strong> The labeled dataset L used in our experiments comes from the training split of the GSM8K dataset. Our unlabeled dataset U is also built upon GSM8K's training data by removing its annotated rationales. For evaluation, we consider three additional commonly used math reasoning tasks besides GSM8K: MultiArith, ASDiv, and SVAMP. Table 1 provides the statistics information of each dataset. Following previous practice (Fu et al., 2023), we fine-tune our base models exclusively on the GSM8K training data while utilizing the rest three datasets to evaluate our models' out-of-domain performance as they do not have an official in-domain training split.</p>
<h3>4.2 Implementation Details</h3>
<p>In the warm-up stage, we fine-tune the base models on the training set of GSM8K (Cobbe et al., 2021) with the original human-labeled annotations and obtain the initial SFT model. For subsequent DPO steps, we first sample rationales from SFT models to build the preference dataset. We sample 5 rationales per question with a temperature of 0.7. Generated rationales <em>ŷ</em> containing the correct an-</p>
<p>²https://huggingface.co/docs/transformers/internal/generation_utils#logitsprocessor</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Base Model</th>
<th>GSM8K</th>
<th>MultiArith</th>
<th>ASDiv</th>
<th>SVAMP</th>
</tr>
</thead>
<tbody>
<tr>
<td>Supervised Fine-Tuning</td>
<td>Flan-T5-Base</td>
<td>18.1</td>
<td>54.2</td>
<td>26.2</td>
<td>19.5</td>
</tr>
<tr>
<td>Self-Training</td>
<td>Flan-T5-Base</td>
<td>25.9</td>
<td>73.8</td>
<td>28.2</td>
<td>24.2</td>
</tr>
<tr>
<td>DPO-aug Self-Training (Ours)</td>
<td>Flan-T5-Base</td>
<td>27.2</td>
<td>74.3</td>
<td>29.2</td>
<td>22.6</td>
</tr>
<tr>
<td>Supervised Fine-Tuning</td>
<td>Flan-T5-Large</td>
<td>30.8</td>
<td>77.2</td>
<td>38.1</td>
<td>33.6</td>
</tr>
<tr>
<td>Self-Training</td>
<td>Flan-T5-Large</td>
<td>35.6</td>
<td>86.2</td>
<td>42.5</td>
<td>34.8</td>
</tr>
<tr>
<td>DPO-aug Self-Training (Ours)</td>
<td>Flan-T5-Large</td>
<td>37.4</td>
<td>89.0</td>
<td>42.8</td>
<td>36.8</td>
</tr>
</tbody>
</table>
<p>Table 2: Overall accuracies (%) over four math word problem solving tasks. Inspired by the previous practice <em>Fu et al. (2023)</em>, all the models in this table are only trained with the GSM8K training set <em>Cobbe et al. (2021)</em>. Hence, we report the in-distribution performance for GSM8K, while reporting the out-of-distribution performance for the other three datasets, i.e., MultiArith, ASDiv, and SVAMP.
swer are classified as winning ones $y_{w}$, while the rest are considered losing ones $y_{l}$. We set $\beta=0.1$ in the DPO learning objective $\mathcal{L}<em _theta_d="\theta^{d">{\text{DPO}}$. For the subsequent SFT steps, we generate 3 rationales per question from the DPO-tuned model $f</em>$ will be selected to build the pseudolabeled dataset. For both DPO and SFT steps, we perform simple deduplication based on the Jaccard similarity scores with a threshold of 0.7. Additional implementation details can be found in Appendix A.}}$, also with a temperature of 0.7. Only the correct generated rationales $\hat{y</p>
<p>Baselines We mainly consider two baseline methods to compare with our method: Supervised FineTuning (SFT) and Self-Training (ST). The SFT baseline corresponds to the model after the warmup stage. The Self-Training baseline adheres to the procedure outlined in Algorithm 1. To ensure a fair comparison between our proposed method and the ST baseline, we use the same set of hyperparameters for both methods at each iteration.</p>
<h3>4.3 Main Results</h3>
<p>Comparison with baselines Table 2 shows the performance of our method compared with the baselines using two base models, Flan-T5-Base and Flan-T5-Large, across four datasets. The results clearly show that both the ST baseline and our proposed DPO-augmented Self-Training method outperform the SFT baseline by a large margin, indicating the effectiveness of the self-training framework in general. Although the ST baselines make significant improvements over the SFT baselines, our DPO-augmented Self-Training models demonstrate enhanced performance on both in-domain (GSM8K) and out-of-domain (MultiArith, ASDiv, and SVAMP) tasks.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: The performance of the proposed method on GSM8K over three iterations. For "iter 0", we report the performance of the SFT baselines, which are obtained after the warm-up stage.</p>
<p>Effect of iterative training Figure 5 demonstrates the impact of iterative training on Flan-T5Base and Flan-T5-Large models, comparing our method to the ST baseline. Initially, both methods start with a warm-up stage and have similar accuracies at iteration 0 . As training progresses, our method consistently outperforms ST across iterations for both models. For Flan-T5-Base, the accuracy improvement plateaus by iteration 3, suggesting convergence. In contrast, Flan-T5-Large shows a clear and steady improvement, with our method achieving significantly higher accuracy by iteration 3. This underscores the effectiveness of our iterative training process, particularly in enhancing performance of larger models.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Base Model</th>
<th># Annotations</th>
<th>Annotator</th>
<th>Tools</th>
<th>Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Supervised fine-tuning</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CoT (Shridhar et al., 2023)</td>
<td>GPT-2-Large</td>
<td>7K</td>
<td>Human</td>
<td>$\boldsymbol{X}$</td>
<td>14.1</td>
</tr>
<tr>
<td>Self-consistency (Khalifa et al., 2023)</td>
<td>Flan-T5-Large</td>
<td>7K</td>
<td>Human</td>
<td>$\checkmark$</td>
<td>33.3</td>
</tr>
<tr>
<td>GRACE (Khalifa et al., 2023)</td>
<td>Flan-T5-Large</td>
<td>7K</td>
<td>Human</td>
<td>$\checkmark$</td>
<td>36.3</td>
</tr>
<tr>
<td>Calcformer (Kadlčík et al., 2023)</td>
<td>T5-Large</td>
<td>30K</td>
<td>Human</td>
<td>$\checkmark$</td>
<td>34.2</td>
</tr>
<tr>
<td>Knowledge Distillation</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Socratic CoT (Shridhar et al., 2023)</td>
<td>GPT-2-Large</td>
<td>7K</td>
<td>GPT-3 175B</td>
<td>$\boldsymbol{X}$</td>
<td>21.1</td>
</tr>
<tr>
<td>CoT from CodeX (Fu et al., 2023)</td>
<td>Flan-T5-Large</td>
<td>100K</td>
<td>CodeX</td>
<td>$\boldsymbol{X}$</td>
<td>20.2</td>
</tr>
<tr>
<td>CoT from PaLM (Magister et al., 2023)</td>
<td>T5-Large</td>
<td>6K</td>
<td>PaLM 540B</td>
<td>$\checkmark$</td>
<td>22.2</td>
</tr>
<tr>
<td>Ours</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DPO-aug Self-Training ( $K=3$ )</td>
<td>Flan-T5-Large</td>
<td>7K</td>
<td>Human</td>
<td>$\checkmark$</td>
<td>37.4</td>
</tr>
<tr>
<td>DPO-aug Self-Training ( $K=5$ )</td>
<td>Flan-T5-Large</td>
<td>7K</td>
<td>Human</td>
<td>$\checkmark$</td>
<td>39.1</td>
</tr>
<tr>
<td>DPO-aug Self-Training ( $K=10$ )</td>
<td>Flan-T5-Large</td>
<td>7K</td>
<td>Human</td>
<td>$\checkmark$</td>
<td>40.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Detailed comparison among existing methods with comparable model sizes on the GSM8K test set. The "Annotator" column indicates how the rationales of the training data are generated. In this column, "Human" refers to the labels from the original GSM8K dataset (Cobbe et al., 2021) that are written by human annotators. The "Tools" column indicates whether external calculators are applied during inference.</p>
<h3>4.4 Comparison with Existing Methods</h3>
<p>In this section, we compare our methods with existing approaches. To enhance our method, we increase the number of sampled pseudo-labels per question to build a more diverse and robust pseudolabel dataset. We denote this hyperparameter as $K$ following Yuan et al. (2023).</p>
<p>Table 3 presents a detailed comparison between our method and exisiting methods using a simialr base model size. The base models we considered include GPT-2-Large (Radford et al., 2019), T5-Large (Raffel et al., 2019), and Flan-T5-Large (Chung et al., 2024), each with approximately 770 million parameters. As shown in Table 3, our approach not only outperforms other methods on the GSM8K benchmark, but also demonstrates remarkable label efficiency by exclusively using the annotations from the original GSM8K dataset.</p>
<p>In Table 4, we further evaluate the effectiveness of the proposed method with the Llama model family (Touvron et al., 2023a,b; Meta, 2024), comparing it with several state-of-the-art closed-source models as well as similarly sized open-source models. We observe a substantial performance gap between proprietary and open-source models. Among the open-source models, those utilizing knowledge distillation generally outperform their counterparts without such enhancement. Notably, our models using Llama-1-7b and Llama-2-7b base models surpass other open-source alternatives that do not employ knowledge distillation, achieving accuracies of $44.7 \%$ and $54.7 \%$ respectively. Furthermore, our model employing the latest Llama-3-8b (Meta, 2024) matches or exceeds the performance of earlier models with knowledge distillation, demonstrating a significant accuracy of $68.8 \%$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Base Model</th>
<th style="text-align: left;">Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Closed-source models</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Claude-3-Opus (Anthropic, 2024)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">95.0</td>
</tr>
<tr>
<td style="text-align: left;">Claude-2 (Anthropic, 2023)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">88.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (OpenAI, 2023)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">92.0</td>
</tr>
<tr>
<td style="text-align: left;">Flan-PaLM-2 (Anil et al., 2023)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">84.7</td>
</tr>
<tr>
<td style="text-align: left;">Open-source models w/ knowledge distillation</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MAmooTH (Yue et al., 2023)</td>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: left;">53.6</td>
</tr>
<tr>
<td style="text-align: left;">LEMA (An et al., 2023)</td>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: left;">54.1</td>
</tr>
<tr>
<td style="text-align: left;">WizardMath (Luo et al., 2023)</td>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: left;">54.9</td>
</tr>
<tr>
<td style="text-align: left;">MetaMath (Yu et al., 2024)</td>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: left;">66.5</td>
</tr>
<tr>
<td style="text-align: left;">MuggleMath (Li et al., 2023a)</td>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: left;">68.4</td>
</tr>
<tr>
<td style="text-align: left;">ToRA (Gou et al., 2024)</td>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: left;">68.8</td>
</tr>
<tr>
<td style="text-align: left;">Open-source models w/o knowledge distillation</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">SFT (Yuan et al., 2023)</td>
<td style="text-align: left;">Llama-1-7b</td>
<td style="text-align: left;">35.9</td>
</tr>
<tr>
<td style="text-align: left;">SFT w/ Calculator</td>
<td style="text-align: left;">Llama-1-7b</td>
<td style="text-align: left;">40.0</td>
</tr>
<tr>
<td style="text-align: left;">RFT ( $K=100)$ (Yuan et al., 2023)</td>
<td style="text-align: left;">Llama-1-7b</td>
<td style="text-align: left;">41.7</td>
</tr>
<tr>
<td style="text-align: left;">SFT (Yuan et al., 2023)</td>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: left;">41.6</td>
</tr>
<tr>
<td style="text-align: left;">SFT w/ Calculator</td>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: left;">45.1</td>
</tr>
<tr>
<td style="text-align: left;">RFT ( $K=100)$ (Yuan et al., 2023)</td>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: left;">47.5</td>
</tr>
<tr>
<td style="text-align: left;">SFT w/ Calculator</td>
<td style="text-align: left;">Llama-3-8b</td>
<td style="text-align: left;">61.0</td>
</tr>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">DPO-ST ( $K=10)^{\hect}$</td>
<td style="text-align: left;">Llama-1-7b</td>
<td style="text-align: left;">44.7</td>
</tr>
<tr>
<td style="text-align: left;">DPO-ST ( $K=10)^{\hect}$</td>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: left;">54.7</td>
</tr>
<tr>
<td style="text-align: left;">DPO-ST ( $K=10)^{\hect}$</td>
<td style="text-align: left;">Llama-3-8b</td>
<td style="text-align: left;">$\mathbf{6 8 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison with the state-of-the-art proprietary models and Llama-based open-source models (Touvron et al., 2023a,b; Meta, 2024). : models augmented with external tools.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Effects of the DPO step. <strong>Left:</strong> we report the greedy decoding results for Pass@1. <strong>Middle:</strong> For Pass@10, the solutions are sampled with temperature 0.7. <strong>Right:</strong> We count the number of generated pseudolabels after deduplication.</p>
<h3>4.5 Effects of the DPO Step</h3>
<p>As mentioned earlier, the main difference between the proposed method and the classic self-training is the DPO step in every iterative process. We now analyze how the DPO steps improve self-training. Figure 6 compares the performance of models before and after the DPO step in the first iteration on the Pass@K metrics. Pass@K measures the probability that at least one of the <em>K</em> generated solutions for a problem is correct, which serves as a gauge for both the quality and the variety of the model-generated solutions. The models we investigate here are fine-tuned from the Flan-T5-Large.</p>
<p>As shown in Figure 6, the DPO step yields only marginal improvements over the SFT model in the Pass@1 performance on the development set. However, the performance significantly improves when multiple rationales, i.e., 10 solutions per question, are sampled with temperature 0.7 (measured with the Pass@10 metric). This indicates that the DPO training objective makes language models inclined to generate rationales of both high quality and diversity. We also compare the number of generated rationales on the training set L for models with and without the DPO step. Figure 6 (right) clearly shows that the model after the DPO step can produce more SFT data for the next iteration.</p>
<h3>4.6 Effects of External Calculator</h3>
<p>Driven by the observation that small-scale LMs frequently make basic calculation errors, we develop a simple yet efficient method that integrates an external calculator into the models' decoding process. To evaluate the impact of this integration,</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: GSM8K development set accuracy of Flan-T5-Large with and without the use of an external calculator during inference.</p>
<p>we conduct an ablation study by omitting the calculator and present the findings in Figure 7. Our results indicate that decoding without the calculator markedly reduces accuracy across all iterations. We believe that this is because models will generate large amounts of false positive pseudo-labels without calculator, that is, the generated pseudo-labels may have correct final answers but make errors in the intermediate reasoning steps.</p>
<h3>5 Related Work</h3>
<p>Learning from pseudo-labels Supervised fine-tuning (SFT) is prevalent technique employed to enhance the performance of pre-trained language models on specific downstream tasks (Ouyang et al., 2022; Chung et al., 2024). However, this method heavily depends on the availability of high-quality labeled data, which can be both expensive and labor-intensive to procure (Brown et al., 2020). To address this limitation, various strategies have been developed to generate high-quality pseudolabels using either unlabeled or synthetic data for a wide range of applications, including text classification (Xie et al., 2020), sentence representation learning (Wang and Lu, 2022), instruction tuning (Honovich et al., 2022), and math reasoning (Wang and Lu, 2023). Recent advancements in this area primarily focus on two directions: self-training and knowledge distillation. The key difference between these methods lies in the source of the pseudo-labels: self-training uses the model's own predictions on unlabeled data, while knowledge distillation utilizes the insights from larger, more powerful models.</p>
<p>Self-training in language model Recently, we have witnessed a large number of works focusing on self-training algorithms for language models <em>He et al. (2020); Zelikman et al. (2022); Yuan et al. (2023)</em>. Most of such methods are built upon the classic self-training framework <em>Scudder (1965)</em>. <em>He et al. (2020)</em> empirically studied the effectiveness of self-training in natural language generation tasks, e.g., summarization and translation. <em>Zelikman et al. (2022)</em> proposed <em>self-taught reasoner</em> (STaR), which demonstrated that language models can be iteratively improved from its own generation, even there are no gold rationales provided. <em>Yuan et al. (2023)</em> proposed <em>rejection sampling fine-tuning</em> to improve language models’ math reasoning abilities. This method can be interpreted as only executing one iteration of the self-training algorithm. <em>Singh et al. (2023)</em> proposed $\mathrm{ReST}^{E M}$, a self-improving algorithm based on expectation-maximization framework. This method demonstrates significant improvements in problem-solving tasks, e.g., math reasoning and code generation.</p>
<p>Knowledge distillation from LLMs Many of the recent research efforts demonstrated large language models (LLMs) are capable of performing math reasoning <em>Wei et al. (2022b); Gao et al. (2022); OpenAI (2023); Anil et al. (2023)</em>. As a result, there is growing interest in enhancing the reasoning abilities of smaller language models by distilling chain-of-thought pseudo-labels from LLMs. <em>Ho et al. (2023); Magister et al. (2023); Fu et al. (2023)</em>. For example, <em>Luo et al. (2023)</em> proposed Reinforcement Learning from Evol-Instruct Feedback built upon the Evol-Instruct framework <em>Xu et al. (2023)</em>, which requires ChatGPT to provide the training signals. <em>An et al. (2023)</em> demonstrated that language models can effectively learn from the mistakes that can be corrected by LLMs during supervised finetuning. Although these methods are shown to have promising experimental results, they are costly to implement as large models cost more FLOPs during inference. Our work demonstrates that small-scale language models can effectively learn from their own generations, offering a more resource-efficient alternative to knowledge distillation. Since our method is conceptually orthogonal to knowledge distillation techniques, an interesting avenue for future research would be to explore integrating knowledge distillation into our iterative training process to further enhance model performance.</p>
<h2>6 Conclusion</h2>
<p>We present an effective and resource-efficient method called DPO-augmented Self-Training (DPO-ST), which augments the original Self-Training algorithm with Direct Preference Optimization <em>Rafailov et al. (2023)</em>. Unlike previous studies that improve small-scale language models’ reasoning abilities by distilling a larger and more powerful model, we argue that small models that are trained merely on the limited human-labeled data can improve themselves significantly. We also empirically find that models trained with DPO loss can generate pseudo-labeled data with higher quality and diversity. Our experiments demonstrate that the proposed method not only outperforms existing methods with comparable model sizes on the GSM8K benchmark, but also achieves remarkable resource efficiency in terms of both computational cost and the requirements of human-labeled data.</p>
<h2>Limitations</h2>
<p>Use of unlabeled data Our method is built upon the classic self-training algorithm, which provides an effective semi-supervised learning framework capable of utilizing unlabeled data efficiently. However, this work doesn’t explore the use of unlabeled data explicitly. Future research efforts can be made to explore how to collect high-quality unlabeled data for math word problem solving. In other words, we need to find an efficient method for collecting unlabeled data $\mathcal{U}=\left{\left(x_{i},a_{i}\right)\right}<em i="i">{i=1}^{n}$ that for each math question $x</em>$, ensuring the data’s relevance and utility for enhancing model training.}$, there is a corresponding ground-truth answer $a_{i</p>
<p>Generalization to other tasks One of the limitations of this work is the narrow scope of our experiments, which were exclusively conducted on math reasoning tasks. The primary reason for this limitation is the lack of appropriate training data for other reasoning tasks. As our method requires a set of training data with chain-of-thought labels, many existing reasoning tasks lack such annotations, making it challenging to extend our experiments beyond the current scope. Future research may focus on identifying and developing suitable datasets for a wider range of reasoning tasks to fully evaluate the applicability and effectiveness of our method across different reasoning tasks.</p>
<h2>Acknowledgements</h2>
<p>This work was done when Shichen Li was a visiting student at the StatNLP Research Group of SUTD. We would like to thank the anonymous reviewers, our meta-reviewer, and senior area chairs for their constructive comments and support on this work. This research/project is supported by Ministry of Education, Singapore, under its Academic Research Fund (AcRF) Tier 2 Programme (MOE AcRF Tier 2 Award No: MOET2EP201220011), the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Program (AISG Award No: AISG2-RP-2020-016), and Ministry of Education, Singapore, under its Tier 3 Programme (The Award No.: MOET320200004). Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of the funding agencies.</p>
<h2>References</h2>
<p>Massih-Reza Amini, Vasilii Feofanov, Loïc Pauletto, Emilie Devijver, and Yury Maximov. 2022. Self-training: A survey. arXiv preprint arXiv:2202.12040.</p>
<p>Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. 2023. Learning from mistakes makes llm better reasoner. arXiv preprint arXiv:2310.20689.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>Anthropic. 2023. Claude 2. https://www. anthropic. com/news/claude-2. Accessed: 2024-05-06.</p>
<p>Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Accessed: 2024-05-06.</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2024. Llemma: An open language model for mathematics. In Proceedings of ICLR.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind</p>
<p>Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, et al. 2020. Language models are few-shot learners. In Proceedings of NeurIPS.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1-53.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Stanley C. Fralick. 1967. Learning to recognize patterns without a teacher. IEEE Trans. Inf. Theory.</p>
<p>Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023. Specializing smaller language models towards multi-step reasoning. In Proceedings of ICML.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435.</p>
<p>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. 2024. Tora: A tool-integrated reasoning agent for mathematical problem solving. In Proceedings of ACL.</p>
<p>Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. 2023. Reinforced selftraining (rest) for language modeling. arXiv preprint arXiv:2308.08998.</p>
<p>Junxian He, Jiatao Gu, Jiajun Shen, and Marc'Aurelio Ranzato. 2020. Revisiting self-training for neural sequence generation. In Proceedings of ICLR.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. In Proceedings of NeurIPS.</p>
<p>Namgyu Ho, Laura Schmid, and Se-Young Yun. 2023. Large language models are reasoning teachers. In Proceedings of ACL.</p>
<p>Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689.</p>
<p>Marek Kadlčík, Michal Štefánik, Ondřej Sotolář, and Vlastimil Martinek. 2023. Calc-x and calcformers: Empowering arithmetical chain-of-thought through interaction with symbolic systems. In Proceedings of EMNLP.</p>
<p>Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang. 2023. Grace: Discriminator-guided chain-of-thought reasoning. In Findings of EMNLP.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Proceedings of NeurIPS.</p>
<p>Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. 2023a. Query and response augmentation cannot help out-of-domain math reasoning generalization. arXiv preprint arXiv:2310.05506.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023b. Making language models better reasoners with step-aware verifier. In Proceedings of $A C L$.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In Proceedings of ICLR.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583.</p>
<p>Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2023. Teaching small language models to reason. In Proceedings of $A C L$.</p>
<p>Meta. 2024. Llama 3. https://llama.meta.com/ llama3/. Accessed: 2024-06-01.</p>
<p>Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of $A C L$.</p>
<p>OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E.</p>
<p>Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. In Proceedings of NeurIPS.</p>
<p>Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of NAACL.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. In Proceedings of NeurIPS.</p>
<p>Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of EMNLP.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. In Proceedings of NeurIPS.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.
H. J. Scudder. 1965. Probability of error of some adaptive pattern-recognition machines. IEEE Trans. Inf. Theory.</p>
<p>Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. 2023. Distilling reasoning capabilities into smaller language models. In Findings of ACL.</p>
<p>Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al. 2023. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Tu Vu, Minh-Thang Luong, Quoc Le, Grady Simon, and Mohit Iyyer. 2021. STraTA: Self-training with task augmentation for better few-shot learning. In Proceedings of EMNLP.</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax.</p>
<p>Tianduo Wang and Wei Lu. 2022. Differentiable data augmentation for contrastive sentence representation learning. In Proceedings of EMNLP.</p>
<p>Tianduo Wang and Wei Lu. 2023. Learning multi-step reasoning by solving arithmetic tasks. In Proceedings of ACL.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. Transactions on Machine Learning Research.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. In Proceedings of NeurIPS.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, et al. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of EMNLP.</p>
<p>Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. 2020. Unsupervised data augmentation for consistency training. In Proceedings of NeurIPS.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244.</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024. Metamath: Bootstrap your own mathematical questions for large language models. In Proceedings of ICLR.</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. In Proceedings of NeurIPS.</p>
<h2>Appendix A Additional Implementation Details</h2>
<p>Our models are trained using the AdamW optimizer (Loshchilov and Hutter, 2019) with a weight decay of 0.01 and gradient clipping of 1.0. We employ a cosine learning rate schedule with warm-up. During training, the maximum sequence lengths are set to 500 for T5 models and 640 for Llama models. Both T5 and Llama models undergo DPOST for three iterations, using the same set of hyperparameters for each iteration as detailed in Table 5. For each DPO step, we sample 5 pseudo-labels per question from the SFT model to build the DPO training data, and set $\beta=0.1$ during DPO training. In SFT steps, the number of model-generated solutions per question can be varied and controlled by the hyperparameter $K$. When sampling pseudolabels, we limit the maximum generated tokens to 300 and use a temperature of 0.7 .</p>
<table>
<thead>
<tr>
<th></th>
<th>Flan-T5</th>
<th></th>
<th>LLaMA</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Hyperparameters</td>
<td>SFT</td>
<td>DPO</td>
<td>SFT</td>
<td>DPO</td>
</tr>
<tr>
<td>Batch size</td>
<td>96</td>
<td>96</td>
<td>128</td>
<td>128</td>
</tr>
<tr>
<td>Epochs</td>
<td>8</td>
<td>-</td>
<td>2</td>
<td>-</td>
</tr>
<tr>
<td>Max steps</td>
<td>-</td>
<td>150</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>Learning rate</td>
<td>$3 \mathrm{e}-4$</td>
<td>$7 \mathrm{e}-7$</td>
<td>$2 \mathrm{e}-5$</td>
<td>$3 \mathrm{e}-7$</td>
</tr>
<tr>
<td>Warm-up ratio</td>
<td>0.1</td>
<td>0.1</td>
<td>0.03</td>
<td>0.03</td>
</tr>
</tbody>
</table>
<p>Table 5: Training details of SFT and DPO steps for Flan-T5 and Llama models.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\dagger}$ All methods presented here are integrated with an external calculator except for the Codex distillation by Fu et al. (2023).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
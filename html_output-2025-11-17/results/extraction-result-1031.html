<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1031 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1031</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1031</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-158046999</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1905.07193v1.pdf" target="_blank">MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Shaping in humans and animals has been shown to be a powerful tool for learning complex tasks as compared to learning in a randomized fashion. This makes the problem less complex and enables one to solve the easier sub task at hand first. Generating a curriculum for such guided learning involves subjecting the agent to easier goals first, and then gradually increasing their difficulty. This paper takes a similar direction and proposes a dual curriculum scheme for solving robotic manipulation tasks with sparse rewards, called MaMiC. It includes a macro curriculum scheme which divides the task into multiple sub-tasks followed by a micro curriculum scheme which enables the agent to learn between such discovered sub-tasks. We show how combining macro and micro curriculum strategies help in overcoming major exploratory constraints considered in robot manipulation tasks without having to engineer any complex rewards. We also illustrate the meaning of the individual curricula and how they can be used independently based on the task. The performance of such a dual curriculum scheme is analyzed on the Fetch environments.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1031.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1031.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MaMiC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Macro and Micro Curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual curriculum RL scheme that (1) extracts and sequences sub-goals from demonstrations (macro) and (2) generates progressively harder intermediate goals via a GAN-based goal generator (micro) to train goal-conditioned policies with off-policy RL (DDPG+HER).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MaMiC agent (DDPG + goal generator + sub-goal extractor)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A goal-conditioned actor-critic agent using Deep Deterministic Policy Gradients (DDPG) with Hindsight Experience Replay (HER) as the base RL algorithm; the micro curriculum supplies GAN-generated intermediate goals and the macro curriculum extracts sub-goals from demonstration trajectories to sequence sub-policies.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent / robotic manipulation policy (goal-conditioned)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Fetch manipulation domains (Push, Slide, PickAndPlace) and Receptor-PickAndPlace; also Push-far / Slide-far variants</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>MuJoCo simulated Fetch robot tasks with sparse binary reward (0 for success, -1 for failure). Complexity arises from sparse rewards, long-horizon sequencing requirements (Receptor-PickAndPlace requires passing object through receptor to activate target), object-target distances (hard variants), target placed out of immediate reach (PickAndPlace targets in air), and variable start-state distributions (Push-far/Slide-far where gripper starts far from table). Variations are introduced by randomizing object and target initial positions and sampling strategies (uniform vs. non-uniform target sampling) and by widening the gripper start-distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Qualitative measures used in paper: sparse reward structure (binary 0/-1), sequencing requirement (number of required sub-policies, e.g., go-receptor-then-go-target), object-target distance (described as 'far' or non-overlapping distributions), and start-state distribution breadth; experiments explicitly vary object/target distributions and gripper start location.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (for Receptor-PickAndPlace, push-hard, slide-hard, and PickAndPlace with target in air); medium for default Push/Slide</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Randomized initialization of object and target positions per episode; sampling strategy for targets (uniform vs non-uniform), widened initial gripper/start-state distribution (Push-far/Slide-far), and goal generator parameter α that shifts generated goal distribution between achieved and desired goals.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high (experiments include significant variation in object/target initialization and widened start-state distributions; exact counts not provided)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (binary success per episode) and episodic reward (0 for success, -1 for failure); learning curves (success rate vs training epochs) used to compare methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: MaMiC learns optimal policies for the core Push/Slide/PickAndPlace micro-tasks; for Receptor-PickAndPlace only the combined macro+micro (MaMiC) reliably succeeds. MaMiC shows faster learning and greater stability than HER on widened-start (Push-far/Slide-far). No absolute numeric success-rate values are reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly discusses trade-offs: (1) High task complexity with sequencing (Receptor-PickAndPlace) requires macro curriculum (sub-goal extraction) because micro goals alone may not generate goals along alternate necessary paths (e.g., key-door example). (2) High environment variation (widened start-state) increases instability for HER (high variance in outcomes), whereas MaMiC stabilizes learning by identifying reach sub-goals and sequencing, accelerating learning. (3) When object and target distributions do not overlap (push-hard/slide-hard), HER fails to learn to even reach the object, while micro curriculum (and MaMiC) succeed by generating achievable intermediate goals.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (dual: macro sub-goal extraction from demonstrations + micro GAN-based progressive goal generation), combined with off-policy RL (DDPG) and HER relabeling strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Goal-conditioned policies are trained under varied goal and start-state distributions; MaMiC enables learning across varied goals and widened start distributions (Push-far/Slide-far). The paper shows MaMiC generalizes across randomized object/target initializations in the tested domains better than baselines, though no explicit new-environment transfer benchmarks are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training ran up to 150 epochs, each epoch = 50 cycles; per cycle 40 DDPG training iterations are performed; GAN updated with 200 iterations after every 100 DDPG iterations; exact number of environment interactions or episodes to convergence is not reported numerically in text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining macro (sub-goal extraction from demonstrations) and micro (GAN-based progressive goals) curricula allows agents to solve sparse-reward, long-horizon manipulation tasks that require sequencing (Receptor-PickAndPlace) and to be more stable and faster when start-state or goal distributions are broadened (Push-far/Slide-far). Micro curriculum alone can solve hard variants where object and goal distributions are far (push-hard/slide-hard) by generating reachable intermediate goals, while HER alone fails in such mismatched-distribution settings; DDPG without curricula fails on these sparse tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1031.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1031.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Micro-curr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Micro Curriculum (GAN goal generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum that trains a GAN to generate intermediate goals that interpolate between the currently achieved goal distribution and the desired goal distribution (controlled by parameter α) to progressively present harder goals to the off-policy agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Micro-curriculum conditioned DDPG agent (DDPG + goal generator)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Goal-conditioned DDPG agent that uses a GAN-generated distribution of micro-goals for relabeling transitions during off-policy training; the generator shifts goals from achieved to desired goals as policy success on generated goals passes thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent / goal-conditioned policy</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Fetch Push, Slide, and PickAndPlace (including push-hard and slide-hard variants)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same Fetch/MuJoCo manipulation tasks; micro curriculum addresses tasks where object and target distributions are initially far apart (hard variants) or where goals are out of immediate reach (PickAndPlace target in air). Complexity arises from sparse reward and mismatched goal/achieved-goal distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Object-target distribution overlap (far/non-overlapping make tasks 'hard'), sparse binary rewards, and whether agent must execute single-hit strategies (Slide) or grasp (PickAndPlace).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high (push-hard/slide-hard and PickAndPlace designated as hard by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Randomized object and target initialization across episodes; micro goal generator's α parameter controls distribution shift (from achieved to desired goals).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (goal distributions randomized per episode; GAN produces varying micro-goals)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate per episode and episodic reward (0 / -1); ability to reach and move object toward generated goals used as a proxy for progression.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: Micro curriculum enables learning in push-hard and slide-hard where HER fails (HER unable to even reach the object). For PickAndPlace where goal always in air, micro helps but may diverge if used alone in sequencing tasks (e.g., Receptor-PickAndPlace requires macro for correct sequence). No numeric success rates provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Discussed: micro curriculum alleviates exploration failures when goal distributions are mismatched or when targets are far by generating intermediate (less complex) goals; however, micro alone can fail when correct solution requires visiting off-path sub-goals that demonstrations reveal (sequence requirements).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Progressive goal generation via GAN conditioned by α (micro curriculum), combined with off-policy DDPG and HER-style relabeling; mixture sampling strategies (micro-g: HER + micro goals; micro-sg: HER + desired goals).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Micro goals allow the policy to generalize from achieved-goal distributions toward the desired-goal distribution, improving learning in hard-distribution cases; explicit cross-environment generalization not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>GAN trained for 200 iterations after every 100 DDPG iterations; overall training performed up to 150 epochs (50 cycles/epoch, 40 DDPG iterations/cycle) — specific sample counts to reach specific success rates not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Micro curriculum (GAN-generated intermediate goals) accelerates learning and enables solving tasks where object-target distributions are far apart (push-hard/slide-hard) without resetting to favorable states, but it may not discover off-path sub-goals required for sequencing tasks; tuning α based on policy success on generated goals is important for curriculum progression.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1031.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1031.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Macro-curr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Macro Curriculum (Sub-goal extraction and sequencing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum that extracts sub-goals from demonstration trajectories by detecting spikes in the gradient ratio of a dense reward signal and sequences learning between those sub-goals, enabling learning of tasks that require ordered sub-policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Macro-curriculum conditioned DDPG agent (DDPG + sub-goal extractor)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A goal-conditioned DDPG agent that leverages sub-goals extracted from demonstration state trajectories (via dense reward gradient spikes) to decompose long-horizon tasks into sequential sub-policies, with each sub-policy learned using curriculum strategies (micro or other).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent / hierarchical curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Receptor-PickAndPlace; also used in Push/Slide/PickAndPlace to extract reach sub-goals</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments where successful completion requires following a particular sequence of sub-tasks (e.g., move object through receptor to activate target then place at target). Complexity comes from sequencing constraints and sparse reward that only triggers after sequence completion.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Presence of sequencing requirements (number of sequential sub-goals), sparse rewards that depend on passing through an intermediate site (receptor), and long-horizon nature of tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (Receptor-PickAndPlace explicitly high complexity due to sequencing and sparse reward)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Demonstration trajectories vary start states and goal trajectories; sub-goals are state-dependent and thus vary with start-state / goal sampling distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (sub-goals depend on start-state and goal sampling distributions; experiments use varied demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate per episode; episodic reward (0 / -1); ability to activate receptor before placing at target is critical for success.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: Macro curriculum combined with micro (MaMiC) is necessary and sufficient to solve Receptor-PickAndPlace reliably; macro alone not sufficient; numeric success rates not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — sequencing complexity (high) can defeat micro-only curricula because micro goals are generated along the direct path to the goal and may miss off-path sub-goals; macro extraction of sub-goals from demonstrations provides the alternative paths and decomposes the complex task into learnable chunks, especially under varied initializations.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Sub-goal extraction from demonstration trajectories via dense reward gradient ratio peaks; learn mapping from start states to sub-goals with an MLP; sequence sub-policies learned typically with micro curriculum in-between sub-goals.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Sub-goals are learned as a function of start state and thus the approach supports goal-parameterized policies across varied starts; demonstrated ability to extract receptor sub-goal across multiple expert trajectories and use it to learn the sequencing required for task completion.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Sub-goal extractor trained with 1000 expert trajectories (1000 training iterations) using a small MLP; authors note that ~200 expert trajectories does not much affect accuracy. Overall RL training schedule as above (150 epochs etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Macro curriculum is essential for tasks that require non-obvious sequencing and off-path sub-goals (e.g., Receptor-PickAndPlace). Micro curricula alone are insufficient for such sequence-dependent tasks because goal generation tends to remain on the direct path and ignores alternative necessary paths revealed by demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1031.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1031.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HER baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hindsight Experience Replay (HER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An off-policy goal-conditioned replay technique that relabels failed episodes with achieved goals to provide additional learning signal for sparse-reward tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hindsight experience replay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HER + DDPG baseline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DDPG actor-critic agent augmented with HER relabeling of transitions to learn from unsuccessful attempts by using achieved goals as training goals.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent / baseline algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Fetch Push, Slide, PickAndPlace; push-hard/slide-hard and Push-far/Slide-far variants; Receptor-PickAndPlace as comparison</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same Fetch/MuJoCo manipulation tasks described above; used as the principal baseline for sparse-reward multi-goal RL.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task difficulty via sparse rewards, object-target distances, start-state distribution breadth and sequencing requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies: performs well on default Push/Slide when object/target distributions overlap; fails on push-hard/slide-hard and struggles on tasks with sequencing or targets always in air</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Randomized target/object initializations and widened start-state distributions in Push-far/Slide-far experiments; HER's performance evaluated under these variations.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high (experiments explicitly widen start distributions and randomize goal positions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate and episodic reward (0 / -1); plotted learning curves (success vs epochs) in figures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: HER fails to learn in push-hard and slide-hard (unable to reach object), shows extremely high variance on Push-far/Slide-far (sometimes solves, sometimes learns nothing), performs well when object and target distributions overlap. No numeric success-rate values provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Discussed: HER is sensitive to the relationship between achieved-goal distribution and desired-goal distribution; when these distributions are mismatched (object far from target or gripper far from object), HER can fail or show high variance. MaMiC's curricula mitigate these issues by generating intermediate goals and sequencing.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Off-policy DDPG with HER relabeling; no curriculum augmentation in baseline runs.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>HER generalizes adequately when distributions overlap and starts are favorable; with widened start distributions or non-overlapping object/target distributions, generalization is poor and variability high.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Compared under same training regime as MaMiC (150 epochs, 50 cycles/epoch, 40 DDPG iterations/cycle); HER sometimes converges but with high variance; explicit sample-to-convergence numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>HER is an effective baseline for many multi-goal sparse-reward tasks but struggles when the achieved-goal distribution differs significantly from desired goals or when start-state variation is large; curricula that generate intermediate goals or extract sub-goals can substantially improve stability and learning speed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1031.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1031.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DDPG baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Deterministic Policy Gradients (DDPG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An off-policy actor-critic algorithm for continuous control used as the base RL method in experiments; when used alone on these sparse tasks, it fails to learn useful policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep Deterministic Policy Gradients (DDPG)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DDPG (no HER / no curriculum) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Standard DDPG actor-critic agent trained on the Fetch tasks without HER or curriculum enhancements; served as a lower-bound baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent / baseline</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Fetch Push, Slide, PickAndPlace (default and hard variants)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same MuJoCo Fetch manipulation tasks with sparse rewards and variable start/goal distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Sparse-reward task difficulty, object-target distance, and sequencing needs; DDPG baseline is unable to overcome sparse-reward exploration challenges here.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (for sparse tasks used), and DDPG alone fails across tasks</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Randomized initializations used in environments; DDPG evaluated under same variation but showed near-zero success.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high (as per experimental setups)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate per episode</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Quantitative statement in paper: DDPG baseline 'fails to solve any of the tasks' with 'success rate of almost 0 across all training epochs.' No exact numeric value given beyond 'almost 0'.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Implicit: DDPG without curricula cannot cope with sparse rewards and environmental variation present in these tasks; curricula and HER are necessary to provide learning signal and stability.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Vanilla DDPG (off-policy actor-critic), no curriculum, no HER in baseline runs.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Not applicable; baseline fails to learn, so generalization not demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained under same schedule (150 epochs, 50 cycles/epoch, 40 DDPG iterations/cycle) but with near-zero success; precise interaction counts not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Plain DDPG is insufficient for sparse-reward, long-horizon robotic manipulation tasks with environmental variation as used in the paper; augmentations like HER and curriculum strategies are necessary to obtain learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hindsight experience replay <em>(Rating: 2)</em></li>
                <li>Continuous control with deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Automatic goal generation for reinforcement learning agents <em>(Rating: 2)</em></li>
                <li>Reverse curriculum generation for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Stochastic neural networks for hierarchical reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1031",
    "paper_id": "paper-158046999",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "MaMiC",
            "name_full": "Macro and Micro Curriculum",
            "brief_description": "A dual curriculum RL scheme that (1) extracts and sequences sub-goals from demonstrations (macro) and (2) generates progressively harder intermediate goals via a GAN-based goal generator (micro) to train goal-conditioned policies with off-policy RL (DDPG+HER).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MaMiC agent (DDPG + goal generator + sub-goal extractor)",
            "agent_description": "A goal-conditioned actor-critic agent using Deep Deterministic Policy Gradients (DDPG) with Hindsight Experience Replay (HER) as the base RL algorithm; the micro curriculum supplies GAN-generated intermediate goals and the macro curriculum extracts sub-goals from demonstration trajectories to sequence sub-policies.",
            "agent_type": "simulated robotic agent / robotic manipulation policy (goal-conditioned)",
            "environment_name": "Fetch manipulation domains (Push, Slide, PickAndPlace) and Receptor-PickAndPlace; also Push-far / Slide-far variants",
            "environment_description": "MuJoCo simulated Fetch robot tasks with sparse binary reward (0 for success, -1 for failure). Complexity arises from sparse rewards, long-horizon sequencing requirements (Receptor-PickAndPlace requires passing object through receptor to activate target), object-target distances (hard variants), target placed out of immediate reach (PickAndPlace targets in air), and variable start-state distributions (Push-far/Slide-far where gripper starts far from table). Variations are introduced by randomizing object and target initial positions and sampling strategies (uniform vs. non-uniform target sampling) and by widening the gripper start-distribution.",
            "complexity_measure": "Qualitative measures used in paper: sparse reward structure (binary 0/-1), sequencing requirement (number of required sub-policies, e.g., go-receptor-then-go-target), object-target distance (described as 'far' or non-overlapping distributions), and start-state distribution breadth; experiments explicitly vary object/target distributions and gripper start location.",
            "complexity_level": "high (for Receptor-PickAndPlace, push-hard, slide-hard, and PickAndPlace with target in air); medium for default Push/Slide",
            "variation_measure": "Randomized initialization of object and target positions per episode; sampling strategy for targets (uniform vs non-uniform), widened initial gripper/start-state distribution (Push-far/Slide-far), and goal generator parameter α that shifts generated goal distribution between achieved and desired goals.",
            "variation_level": "medium-to-high (experiments include significant variation in object/target initialization and widened start-state distributions; exact counts not provided)",
            "performance_metric": "Success rate (binary success per episode) and episodic reward (0 for success, -1 for failure); learning curves (success rate vs training epochs) used to compare methods.",
            "performance_value": "Qualitative: MaMiC learns optimal policies for the core Push/Slide/PickAndPlace micro-tasks; for Receptor-PickAndPlace only the combined macro+micro (MaMiC) reliably succeeds. MaMiC shows faster learning and greater stability than HER on widened-start (Push-far/Slide-far). No absolute numeric success-rate values are reported in text.",
            "complexity_variation_relationship": "Yes — the paper explicitly discusses trade-offs: (1) High task complexity with sequencing (Receptor-PickAndPlace) requires macro curriculum (sub-goal extraction) because micro goals alone may not generate goals along alternate necessary paths (e.g., key-door example). (2) High environment variation (widened start-state) increases instability for HER (high variance in outcomes), whereas MaMiC stabilizes learning by identifying reach sub-goals and sequencing, accelerating learning. (3) When object and target distributions do not overlap (push-hard/slide-hard), HER fails to learn to even reach the object, while micro curriculum (and MaMiC) succeed by generating achievable intermediate goals.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (dual: macro sub-goal extraction from demonstrations + micro GAN-based progressive goal generation), combined with off-policy RL (DDPG) and HER relabeling strategies.",
            "generalization_tested": true,
            "generalization_results": "Goal-conditioned policies are trained under varied goal and start-state distributions; MaMiC enables learning across varied goals and widened start distributions (Push-far/Slide-far). The paper shows MaMiC generalizes across randomized object/target initializations in the tested domains better than baselines, though no explicit new-environment transfer benchmarks are reported.",
            "sample_efficiency": "Training ran up to 150 epochs, each epoch = 50 cycles; per cycle 40 DDPG training iterations are performed; GAN updated with 200 iterations after every 100 DDPG iterations; exact number of environment interactions or episodes to convergence is not reported numerically in text.",
            "key_findings": "Combining macro (sub-goal extraction from demonstrations) and micro (GAN-based progressive goals) curricula allows agents to solve sparse-reward, long-horizon manipulation tasks that require sequencing (Receptor-PickAndPlace) and to be more stable and faster when start-state or goal distributions are broadened (Push-far/Slide-far). Micro curriculum alone can solve hard variants where object and goal distributions are far (push-hard/slide-hard) by generating reachable intermediate goals, while HER alone fails in such mismatched-distribution settings; DDPG without curricula fails on these sparse tasks.",
            "uuid": "e1031.0",
            "source_info": {
                "paper_title": "MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Micro-curr",
            "name_full": "Micro Curriculum (GAN goal generator)",
            "brief_description": "A curriculum that trains a GAN to generate intermediate goals that interpolate between the currently achieved goal distribution and the desired goal distribution (controlled by parameter α) to progressively present harder goals to the off-policy agent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Micro-curriculum conditioned DDPG agent (DDPG + goal generator)",
            "agent_description": "Goal-conditioned DDPG agent that uses a GAN-generated distribution of micro-goals for relabeling transitions during off-policy training; the generator shifts goals from achieved to desired goals as policy success on generated goals passes thresholds.",
            "agent_type": "simulated robotic agent / goal-conditioned policy",
            "environment_name": "Fetch Push, Slide, and PickAndPlace (including push-hard and slide-hard variants)",
            "environment_description": "Same Fetch/MuJoCo manipulation tasks; micro curriculum addresses tasks where object and target distributions are initially far apart (hard variants) or where goals are out of immediate reach (PickAndPlace target in air). Complexity arises from sparse reward and mismatched goal/achieved-goal distributions.",
            "complexity_measure": "Object-target distribution overlap (far/non-overlapping make tasks 'hard'), sparse binary rewards, and whether agent must execute single-hit strategies (Slide) or grasp (PickAndPlace).",
            "complexity_level": "medium-to-high (push-hard/slide-hard and PickAndPlace designated as hard by authors)",
            "variation_measure": "Randomized object and target initialization across episodes; micro goal generator's α parameter controls distribution shift (from achieved to desired goals).",
            "variation_level": "medium (goal distributions randomized per episode; GAN produces varying micro-goals)",
            "performance_metric": "Success rate per episode and episodic reward (0 / -1); ability to reach and move object toward generated goals used as a proxy for progression.",
            "performance_value": "Qualitative: Micro curriculum enables learning in push-hard and slide-hard where HER fails (HER unable to even reach the object). For PickAndPlace where goal always in air, micro helps but may diverge if used alone in sequencing tasks (e.g., Receptor-PickAndPlace requires macro for correct sequence). No numeric success rates provided in text.",
            "complexity_variation_relationship": "Discussed: micro curriculum alleviates exploration failures when goal distributions are mismatched or when targets are far by generating intermediate (less complex) goals; however, micro alone can fail when correct solution requires visiting off-path sub-goals that demonstrations reveal (sequence requirements).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Progressive goal generation via GAN conditioned by α (micro curriculum), combined with off-policy DDPG and HER-style relabeling; mixture sampling strategies (micro-g: HER + micro goals; micro-sg: HER + desired goals).",
            "generalization_tested": true,
            "generalization_results": "Micro goals allow the policy to generalize from achieved-goal distributions toward the desired-goal distribution, improving learning in hard-distribution cases; explicit cross-environment generalization not reported.",
            "sample_efficiency": "GAN trained for 200 iterations after every 100 DDPG iterations; overall training performed up to 150 epochs (50 cycles/epoch, 40 DDPG iterations/cycle) — specific sample counts to reach specific success rates not provided.",
            "key_findings": "Micro curriculum (GAN-generated intermediate goals) accelerates learning and enables solving tasks where object-target distributions are far apart (push-hard/slide-hard) without resetting to favorable states, but it may not discover off-path sub-goals required for sequencing tasks; tuning α based on policy success on generated goals is important for curriculum progression.",
            "uuid": "e1031.1",
            "source_info": {
                "paper_title": "MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Macro-curr",
            "name_full": "Macro Curriculum (Sub-goal extraction and sequencing)",
            "brief_description": "A curriculum that extracts sub-goals from demonstration trajectories by detecting spikes in the gradient ratio of a dense reward signal and sequences learning between those sub-goals, enabling learning of tasks that require ordered sub-policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Macro-curriculum conditioned DDPG agent (DDPG + sub-goal extractor)",
            "agent_description": "A goal-conditioned DDPG agent that leverages sub-goals extracted from demonstration state trajectories (via dense reward gradient spikes) to decompose long-horizon tasks into sequential sub-policies, with each sub-policy learned using curriculum strategies (micro or other).",
            "agent_type": "simulated robotic agent / hierarchical curriculum",
            "environment_name": "Receptor-PickAndPlace; also used in Push/Slide/PickAndPlace to extract reach sub-goals",
            "environment_description": "Environments where successful completion requires following a particular sequence of sub-tasks (e.g., move object through receptor to activate target then place at target). Complexity comes from sequencing constraints and sparse reward that only triggers after sequence completion.",
            "complexity_measure": "Presence of sequencing requirements (number of sequential sub-goals), sparse rewards that depend on passing through an intermediate site (receptor), and long-horizon nature of tasks.",
            "complexity_level": "high (Receptor-PickAndPlace explicitly high complexity due to sequencing and sparse reward)",
            "variation_measure": "Demonstration trajectories vary start states and goal trajectories; sub-goals are state-dependent and thus vary with start-state / goal sampling distributions.",
            "variation_level": "medium (sub-goals depend on start-state and goal sampling distributions; experiments use varied demonstrations)",
            "performance_metric": "Success rate per episode; episodic reward (0 / -1); ability to activate receptor before placing at target is critical for success.",
            "performance_value": "Qualitative: Macro curriculum combined with micro (MaMiC) is necessary and sufficient to solve Receptor-PickAndPlace reliably; macro alone not sufficient; numeric success rates not provided in text.",
            "complexity_variation_relationship": "Yes — sequencing complexity (high) can defeat micro-only curricula because micro goals are generated along the direct path to the goal and may miss off-path sub-goals; macro extraction of sub-goals from demonstrations provides the alternative paths and decomposes the complex task into learnable chunks, especially under varied initializations.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Sub-goal extraction from demonstration trajectories via dense reward gradient ratio peaks; learn mapping from start states to sub-goals with an MLP; sequence sub-policies learned typically with micro curriculum in-between sub-goals.",
            "generalization_tested": true,
            "generalization_results": "Sub-goals are learned as a function of start state and thus the approach supports goal-parameterized policies across varied starts; demonstrated ability to extract receptor sub-goal across multiple expert trajectories and use it to learn the sequencing required for task completion.",
            "sample_efficiency": "Sub-goal extractor trained with 1000 expert trajectories (1000 training iterations) using a small MLP; authors note that ~200 expert trajectories does not much affect accuracy. Overall RL training schedule as above (150 epochs etc.).",
            "key_findings": "Macro curriculum is essential for tasks that require non-obvious sequencing and off-path sub-goals (e.g., Receptor-PickAndPlace). Micro curricula alone are insufficient for such sequence-dependent tasks because goal generation tends to remain on the direct path and ignores alternative necessary paths revealed by demonstrations.",
            "uuid": "e1031.2",
            "source_info": {
                "paper_title": "MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "HER baseline",
            "name_full": "Hindsight Experience Replay (HER)",
            "brief_description": "An off-policy goal-conditioned replay technique that relabels failed episodes with achieved goals to provide additional learning signal for sparse-reward tasks.",
            "citation_title": "Hindsight experience replay",
            "mention_or_use": "use",
            "agent_name": "HER + DDPG baseline",
            "agent_description": "DDPG actor-critic agent augmented with HER relabeling of transitions to learn from unsuccessful attempts by using achieved goals as training goals.",
            "agent_type": "simulated robotic agent / baseline algorithm",
            "environment_name": "Fetch Push, Slide, PickAndPlace; push-hard/slide-hard and Push-far/Slide-far variants; Receptor-PickAndPlace as comparison",
            "environment_description": "Same Fetch/MuJoCo manipulation tasks described above; used as the principal baseline for sparse-reward multi-goal RL.",
            "complexity_measure": "Task difficulty via sparse rewards, object-target distances, start-state distribution breadth and sequencing requirements.",
            "complexity_level": "varies: performs well on default Push/Slide when object/target distributions overlap; fails on push-hard/slide-hard and struggles on tasks with sequencing or targets always in air",
            "variation_measure": "Randomized target/object initializations and widened start-state distributions in Push-far/Slide-far experiments; HER's performance evaluated under these variations.",
            "variation_level": "medium-to-high (experiments explicitly widen start distributions and randomize goal positions)",
            "performance_metric": "Success rate and episodic reward (0 / -1); plotted learning curves (success vs epochs) in figures.",
            "performance_value": "Qualitative: HER fails to learn in push-hard and slide-hard (unable to reach object), shows extremely high variance on Push-far/Slide-far (sometimes solves, sometimes learns nothing), performs well when object and target distributions overlap. No numeric success-rate values provided in text.",
            "complexity_variation_relationship": "Discussed: HER is sensitive to the relationship between achieved-goal distribution and desired-goal distribution; when these distributions are mismatched (object far from target or gripper far from object), HER can fail or show high variance. MaMiC's curricula mitigate these issues by generating intermediate goals and sequencing.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Off-policy DDPG with HER relabeling; no curriculum augmentation in baseline runs.",
            "generalization_tested": true,
            "generalization_results": "HER generalizes adequately when distributions overlap and starts are favorable; with widened start distributions or non-overlapping object/target distributions, generalization is poor and variability high.",
            "sample_efficiency": "Compared under same training regime as MaMiC (150 epochs, 50 cycles/epoch, 40 DDPG iterations/cycle); HER sometimes converges but with high variance; explicit sample-to-convergence numbers not provided.",
            "key_findings": "HER is an effective baseline for many multi-goal sparse-reward tasks but struggles when the achieved-goal distribution differs significantly from desired goals or when start-state variation is large; curricula that generate intermediate goals or extract sub-goals can substantially improve stability and learning speed.",
            "uuid": "e1031.3",
            "source_info": {
                "paper_title": "MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "DDPG baseline",
            "name_full": "Deep Deterministic Policy Gradients (DDPG)",
            "brief_description": "An off-policy actor-critic algorithm for continuous control used as the base RL method in experiments; when used alone on these sparse tasks, it fails to learn useful policies.",
            "citation_title": "Deep Deterministic Policy Gradients (DDPG)",
            "mention_or_use": "use",
            "agent_name": "DDPG (no HER / no curriculum) baseline",
            "agent_description": "Standard DDPG actor-critic agent trained on the Fetch tasks without HER or curriculum enhancements; served as a lower-bound baseline.",
            "agent_type": "simulated robotic agent / baseline",
            "environment_name": "Fetch Push, Slide, PickAndPlace (default and hard variants)",
            "environment_description": "Same MuJoCo Fetch manipulation tasks with sparse rewards and variable start/goal distributions.",
            "complexity_measure": "Sparse-reward task difficulty, object-target distance, and sequencing needs; DDPG baseline is unable to overcome sparse-reward exploration challenges here.",
            "complexity_level": "high (for sparse tasks used), and DDPG alone fails across tasks",
            "variation_measure": "Randomized initializations used in environments; DDPG evaluated under same variation but showed near-zero success.",
            "variation_level": "medium-to-high (as per experimental setups)",
            "performance_metric": "Success rate per episode",
            "performance_value": "Quantitative statement in paper: DDPG baseline 'fails to solve any of the tasks' with 'success rate of almost 0 across all training epochs.' No exact numeric value given beyond 'almost 0'.",
            "complexity_variation_relationship": "Implicit: DDPG without curricula cannot cope with sparse rewards and environmental variation present in these tasks; curricula and HER are necessary to provide learning signal and stability.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Vanilla DDPG (off-policy actor-critic), no curriculum, no HER in baseline runs.",
            "generalization_tested": false,
            "generalization_results": "Not applicable; baseline fails to learn, so generalization not demonstrated.",
            "sample_efficiency": "Trained under same schedule (150 epochs, 50 cycles/epoch, 40 DDPG iterations/cycle) but with near-zero success; precise interaction counts not provided.",
            "key_findings": "Plain DDPG is insufficient for sparse-reward, long-horizon robotic manipulation tasks with environmental variation as used in the paper; augmentations like HER and curriculum strategies are necessary to obtain learning.",
            "uuid": "e1031.4",
            "source_info": {
                "paper_title": "MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning",
                "publication_date_yy_mm": "2019-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hindsight experience replay",
            "rating": 2,
            "sanitized_title": "hindsight_experience_replay"
        },
        {
            "paper_title": "Continuous control with deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "continuous_control_with_deep_reinforcement_learning"
        },
        {
            "paper_title": "Automatic goal generation for reinforcement learning agents",
            "rating": 2,
            "sanitized_title": "automatic_goal_generation_for_reinforcement_learning_agents"
        },
        {
            "paper_title": "Reverse curriculum generation for reinforcement learning",
            "rating": 2,
            "sanitized_title": "reverse_curriculum_generation_for_reinforcement_learning"
        },
        {
            "paper_title": "Stochastic neural networks for hierarchical reinforcement learning",
            "rating": 1,
            "sanitized_title": "stochastic_neural_networks_for_hierarchical_reinforcement_learning"
        }
    ],
    "cost": 0.015200249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning</p>
<p>Manan Tomar manan.tomar@gmail.com 
Indian Institute of Technology Madras
ChennaiIndia</p>
<p>Akhil Sathuluri akhilsathuluri@gmail.com 
Indian Institute of Technology Madras
ChennaiIndia</p>
<p>Balaraman Ravindran 
Indian Institute of Technology Madras
ChennaiIndia</p>
<p>Robert Bosch Center for Data Science and AI (RBCDSAI)
ChennaiIndia</p>
<p>MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning
Reinforcement Learning; Curriculum Learning
Shaping in humans and animals has been shown to be a powerful tool for learning complex tasks as compared to learning in a randomized fashion. This makes the problem less complex and enables one to solve the easier sub task at hand first. Generating a curriculum for such guided learning involves subjecting the agent to easier goals first, and then gradually increasing their difficulty. This paper takes a similar direction and proposes a dual curriculum scheme for solving robotic manipulation tasks with sparse rewards, called MaMiC. It includes a macro curriculum scheme which divides the task into multiple sub-tasks followed by a micro curriculum scheme which enables the agent to learn between such discovered sub-tasks. We show how combining macro and micro curriculum strategies help in overcoming major exploratory constraints considered in robot manipulation tasks without having to engineer any complex rewards. We also illustrate the meaning of the individual curricula and how they can be used independently based on the task. The performance of such a dual curriculum scheme is analyzed on the Fetch environments.</p>
<p>INTRODUCTION</p>
<p>In recent years, deep Reinforcement Learning has seen a lot of promising results in varied domains such as game-playing [16], [21], and continuous control [12], [20], [9]. Despite these developments, robotic decision making remains a hard problem given minimal context of the task in hand [5]. Robotic learning presents a huge challenge mainly because of the complex dynamics, sparse rewards and exploration issues arising from large continuous state spaces, thus providing a good testbed for reinforcement learning algorithms.</p>
<p>Solving complex tasks requires exploiting the structure of the task efficiently. Each task can be viewed as a combination of much simpler prelearnt skills. Consider the cases of unscrewing a bottle or placing an object in a drawer. All such everyday tasks involve reusing distinct skills or sub-policies in an intelligent manner to achieve the overall objective. To be able to solve such complex tasks it is important that we learn in a organized, meaningful manner rather than learning using data collected in a random fashion. Curriculum learning [3], [23] is a powerful concept that allows us to come up with such training strategies. Starting to learn for simpler tasks and then using the acquired knowledge to learn progressively harder tasks is a natural outcome of formulating a curriculum. A curriculum assists one in overcoming exploratory constraints of the agent by focusing learning over simpler parts of the state space first. Recently, curriculum learning has been used to solve complex robotic tasks (not necessarily manipulation) such as in [7], [17]. However, these approaches make the assumption that the agent can be reset to any desired state, and also make use of expert state action trajectories [17], which are expensive to generate. Unlike such techniques, our method is not restricted by the ability to reset. Moreover, we use state-only demonstration sequences for learning only in specific tasks, and do not use demonstrations at all for the other tasks, thus distinguishing our work from those in the imitation learning sphere. Although learning from only state or observation sequences is a much more difficult method [13], it offers practical benefits in terms of reduced trajectory collection costs and implementation ease, thus fitting our problem domain more accurately.</p>
<p>One way of looking at the problem in hand is to extract sub-goals for a given task, learn sub-policies or skills that achieve these subgoals, and then execute them in the right order. Such a top-down approach allows exploiting the structure of the problem, since the extracted sub-goals define the nature of the solution. Moreover, we also focus on the sequential nature of the problem, i.e. solving to achieve the first sub-goal, then the second sub-goal and so on. This is important as most robotic locomotion or manipulation problems can be recognized in this manner. In our method, the sub-goal extraction and sequencing is managed by the macro scheme, while learning each sub-policy is managed by the micro scheme. In order to achieve this, both of these methods exhibit and use concepts from curriculum learning.</p>
<p>We introduce MaMiC, comprising macro and micro curriculum, which can be applied either individually or in combination. A micro curriculum essentially generates increasingly complex goals for the agent to achieve. For example, in learning to push a block, initial goals will be generated very near to the block and then slowly shifted to the desired location. However, such a scheme is not sufficient if we need to solve tasks which are more complex, such as ones which require the agent to maintain a particular sequence of sub policies. For instance, in order to put an object in a drawer, it is not enough to guide the agent in learning to put the object to the desired location, but also to open the drawer first. It is only when a particular sequence of such sub policies is followed that we refer to the task as completed. A macro curriculum helps in identifying such a sequence and allows the micro scheme to learn in between this sequence. A policy starts from a achieved sub-goal and proceeds to the next sub-goal, evolving in the process, ultimately reaching the actual goal. Two ideas are at the core of this technique, of being able to discover the sub-goals and of learning between the recognized sub-goals. The working of MaMiC is as illustrated in Fig 1. To summarize, the following are the major contributions of the paper:</p>
<p>• We propose a dual curriculum strategy comprising micro and macro schemes, which enables an agent to discover sub-goals and learn a policy which evolves to achieve such sub-goals sequentially, eventually solving the task • We analyze the macro and micro schemes individually, and illustrate how to combine these individual schemes with base reinforcement learning algorithms such as Deep Deterministic Policy Gradients (DDPG) to solve a given task • The performance of the proposed dual curriculum scheme is tested in a Receptor-PickandPlace environment and also in a custom physics environment. • An industrial robot with minimal observations available is considered for training and the learnt policy is deployed onto a physical robot as validation. (see the supplementary videos at https://goo.gl/nKZoCQ)</p>
<p>BACKGROUND AND PRELIMINARIES</p>
<p>Reinforcement Learning (RL), [24], considers the interaction of an agent with a given environment and is modeled by a Markov Decision Process (MDP), defined by the tuple ⟨S, A, P, ρ ′ , r ⟩, where S defines the set of states, A the set of actions, P : S × A → S the transition function, ρ ′ the probability distribution over initial states, and r : S × A → R the reward function. A policy is denoted by π (s): S → P(A), where P(A) defines a probability distribution over actions a ϵ A in a state s ϵ S. The objective is to learn a policy such that the return R t = T i=t γ (i−t ) r (s i , a i ) is maximized, where r (s i , a i ) is the reward function and γ is the discount factor.</p>
<p>Deep Deterministic Policy Gradients (DDPG)</p>
<p>DDPG [12] is an off policy, model free actor critic based reinforcement learning method. The critic is used to estimate the action value function Q(s t , a t ), while the actor refers to the deterministic policy of the agent. The critic is learned by minimizing the standard TD error
δ T D = r t + γQ ′ (s t +1 , π (s t +1 )) − Q(s t , a t )(1)
,where Q ′ refers to a target network [16] which is updated after a fixed number of time steps. The actor is optimized by following the gradient of the critic's estimate of the Q value. Universal Value Function Approximators (UVFA) [19] parameterizes the Q value function by the goal and tries to learn a policy π (s t , д t ): S × G → A dependent on the goal as well. Such a value function is denoted by Q(s, a, д).</p>
<p>Hindsight Experience Replay</p>
<p>Hindsight Experience Replay (HER) was introduced by [1] and works along with an off policy method such as DDPG to accelerate the learning process. The overall idea is to learn from unsuccessful trials as well by parameterizing over goals. HER helps in accelerating learning by substituting some samples with the achieved goal instead of the actual goal. Since the current policy is able to reach these achieved goal, learning the mapping between goals to actions becomes faster. [10] propose using a Generative Adversarial Network (GAN) [8], [14] based goal generator for sampling good goals, which refers to goals which are neither too hard nor too easy for the current policy to achieve. The goals used for training the GAN are labeled based on the return obtained for the specific goal. Goals which lead to a positive return are encouraged while those which lead to a negative return are discouraged.</p>
<p>Goal Generative Adversarial Network</p>
<p>Definitions</p>
<p>The following are the definitions of the terms used throughout the paper:</p>
<p>Desired Goals : These refer to the actual goals received from the environment and correspond to the task being solved.</p>
<p>Achieved Goals : These refer to end of trajectory states achieved by the agent while following the currently learned policy.</p>
<p>HER Goals: These refer to achieved states in a trajectory while following the currently learned policy, randomly sampled as is proposed by HER .</p>
<p>Micro Goals: These refer to goals generated by the goal generator.</p>
<p>Sub Goals: These refer to the sub-goals extracted from demonstrations or assumed to be given by an oracle.</p>
<p>This work assumes that there exists a mapping m(д) : G → S between a goal д ϵ G and a state s ϵ S. The task then is defined by achieving the corresponding goal state s д for a given goal д. Note that if such a mapping exists, a goal can be achieved by achieving more than one state. Many robotic manipulation tasks are designed such that the goal can be represented as an achievable state, and therefore, such an assumption does not add extreme constraints. In such cases, the achieved goal can be the object's position and the desired goal can be the target location. Note that the framework adopted in this work does not limit us to only have Cartesian coordinates of objects for defining an achieved goal.</p>
<p>Assumptions about the environment dilute the generalization of an algorithm and lead to failure in unconstrained settings or real-world deployment. In manipulation tasks these can be alleviated by breaking the task into much simpler tasks with lesser constraints, making it easier for the agent to learn. Below are two such assumptions:</p>
<p>• Resetting the agent to any desired state : In the native reinforcement learning setting, the agent is initialized at particular states based on a start state distribution available only to the environment. However, as mentioned, previous works assume that the agent can be initialized from whichever state is desired. Given this assumption, the agent can start directly from the goal state and thus not learn at all. Such an assumption is extremely limiting as in any practical setting the environment dictates the start state of the agent. The agent should be intelligent enough to reach such desired or favorable states. • Starting from solved or partially solved states : Prior work also mentions another technique for learning sparse reward manipulation tasks which involves starting some training trajectories from solved states and the rest by sampling from the start state distribution. For example, in a pushing task, some trajectories start with the object being placed at the target location.</p>
<p>MICRO CURRICULUM</p>
<p>A micro curriculum tries to alleviate the above-mentioned assumption of being able to start some trajectories from favorable states. As argued above, we believe that starting at a particular state should be based on the environment's choice but not the agent's. We propose replacing all or some transition sample goals with the micro goals which may be generated by any generative modeling technique.</p>
<p>Using an off policy RL algorithm allows us to replace sampled transition goals from the buffer with micro goals. The goals are generated such that they are initially close to the achieved states at the end of each trajectory (i.e. the achieved goal distribution) and slowly shift to being closer to the actual or desired goal distribution of the task in hand. Since this procedure involves learning a mapping between goals and actions, eventually the agent is able to generalize well for the actual goal distribution. We relate this with curriculum learning because the agent initially learns for a goal distribution much simpler to learn i.e. the achieved goal distribution and then continues learning for increasingly difficult goals, leveraging the previously learned skills.</p>
<p>To train the goal generator, we make use of Generative Adversarial Networks or GANs [8] and modify the formulation used by [10]. We incorporate an additional parameter α ϵ [0, 1] which governs the resemblance of the generated distribution to the achieved goal distribution and the actual or desired goal distribution. α = 0 forces the generator to produce goals similar to the currently achieved states, while α = 1 produces goals similar to the actual distribution. The exact objective function is given below.
min D V (D) = E д∼p d at a (д) [(1 − α) (D(д achieved ) − 1) 2 + α (D(д desir ed ) − 1) 2 ] + E z∼p z (z) [D(G(z)) 2 ] (2) min G V (G) = E z∼pz (z) <a href="3">(D(G(z)) − 1) 2 </a>
,where D denotes the discriminator network, G the generator network, and V the GAN value function. p z here is taken as a uniform distribution between 0 and 1 from which the noise vector z is sampled. In all experiments that follow, we choose to update α if the success rate of the currently learned policy for goals generated by the GAN lies above a particular threshold consistently for a few epochs. This essentially tells us that the policy has now mastered achieving the currently generated goals with some degree of confidence and thus the GAN can now shift further towards producing goals resembling the desired distribution.</p>
<p>Algorithm 1 : Micro Curriculum</p>
<p>Given : An off policy RL algorithm A, a goal generator G, a goal sampling strategy S, replay buffer R Initialize A, R, G for n = 1, ..., N episodes do Sample initial state s 0 , goal д DesiredGoal n ← д Generate artificial goal from G, д micr o ← G(z), z ∼ p z for t = 0, ..., T − 1 steps do Compute a t from behavioral policy, a t ← π b (s t , д micr o ) Execute a t , observe next state s t +1 and compute reward
r (s t +1 , д micr o ) Store transition (s t , a t , r t , s t +1 , д micr o ) in R AchievedGoal n ← s T end for Sample a random minibatch of N transitions (s i , a i , r i , s i+1 ) from R
Sample new goals д ′ using S Replace the sampled transitions goals with the new goal д ′ ,
(s t , a t , r t , s t +1 , д ′ )
Recompute reward for replaced goals for i = 1, ..., K iterations do Perform one optimization step for Goal Generator using (AchievedGoal, DesiredGoal) end for for i = 1, ..., M iterations do Perform one optimization step of A end for end for Algorithm 1. describes our method in detail. At each iteration, the goal generator produces a micro goal which is used to condition the behavior policy and collect samples by executing it. For each episode, the end of trajectory state, called as the achieved goal is collected and stored in memory. While training, a mini batch of data is sampled and some or all of the goal samples are relabelled with new ones using the goal sampling strategy (described below). The achieved goals and the desired goals are used to update the goal generator periodically. The desired goals essentially either are the goals corresponding to the task in hand or any of the sub-goals provided by the sub-goal extraction method. Therefore, this allows the micro scheme to be run independently as well as in combination with the macro method. We elaborate more on this in the below section.</p>
<p>Strategy for goal sampling</p>
<p>For replacing goals by sampling new ones, we consider different strategies such as having a mixture of HER goals and micro goals (referred to as micro-g), and having a mixture of HER goals and desired goals (referred to as micro-sg).</p>
<p>Environment Details</p>
<p>• Pushing : This requires a block placed on a table to be pushed by the end-effector of the robot to a given target. • Sliding : In this task, the robot is supposed to hit a puck so that the puck reaches a target location. The target location is given at a position out of the reach of the end-effector, hindering the puck from pushing it continuously towards the target. Instead the agent needs learn to solve the task from a single hit. Overall, We observe that although it is possible to learn a good policy, it is very hard to produce a perfect policy. This can possibly be attributed to the design of the task itself, or the fact that using a very small r thr eshold for such a hard task in calculating the reward. • Pick and Place : This requires the robot agent to pick a box lying on the table and place it at a target location in the air. The gripper is also controlled by the policy in this case, unlike the previous ones. We also do not start any episode with the block already in the robot's gripper, thus making sure that favorable starts are not considered. Specifically for this task, we consider two sampling strategies for the target location. We denote a uniform strategy to sample target location in the air completely randomly without prioritizing the table. A non-uniform strategy is one in which the target is sampled on the table with probability 0.5 and in the air with probability 0.5.</p>
<p>Training Details</p>
<p>Goal Generator.</p>
<p>We train a GAN on the achieved and desired goals data gather after each rollout. The generator network consists of two 128 nodes layers, while the discriminator consists of two 256 nodes layers. We use a learning rate of 0.001, a batch size of 64, and sample from a noise vector z of size 4. We run 200 training iterations of the GAN after every 100 iterations of the DDPG policy.</p>
<p>sub-goal Extractor.</p>
<p>For learning a mapping between start states and sub-goals, we train a 2 layer MLP with 16 nodes each. The input is the start state while the output is the sub-goal i.e. a vector of size 3. The batch size used is 64, and the learning rate is 0.001. We run 1000 training iterations of this extractor for a dataset consisting of 1000 expert trajectory samples. It is observed that having less number of expert trajectories i.e. around 200 does not affect the accuracy by a lot. </p>
<p>Micro -Tasks Considered</p>
<p>We consider variants of the pushing, sliding and pick and place tasks for a 7 DOF Fetch robot simulation [18] as shown in the Fig 3. The sampling strategy S for micro used here comprises HER goals and micro goal samples. We consider three tasks in the Mujoco Micro-sg refers to a goal sampling strategy comprising a mixture of HER samples and Desired goals [26] environment for our experiments as described below. A successful trajectory receives a 0 reward while an unsuccessful one receives −1 reward. For all three tasks, the target and the object are randomly initialized such that they do not lie in the reward threshold r t hr eshold = 0.05, equivalent to 5cm, and therefore the reward received initially is always -1, i.e. we make sure that the agent does not start from a solved state even randomly. We compare our method with the original HER algorithm proposed in [1] which is the state-of-the-art algorithm on these domains. Moreover, we also compare with the original DDPG algorithm as a baseline. However, since DDPG fails to solve any of the tasks considered in this paper independently (success rate of almost 0 across all training epochs), we opt to not show these results explicitly in the plots.</p>
<p>Push-hard and Slide-hard tasks.</p>
<p>We consider harder variants of the pushing and sliding tasks for testing the micro scheme. These tasks are "made hard" by ensuring that the object and the target do not lie in similar distributions initially and are far apart from each other for all episode samples. This makes the task difficult to solve as even if the agent somehow learns to push or slide the object to some nearby target site, the task is still not considered solved.</p>
<p>3.4.2 Pick and Place. The task requires an object to be picked and placed at a target site. The target is never sampled on the table and always in the air. We also do not start any episode with the block already in the robot's gripper, thus making sure that favorable starts are not considered.</p>
<p>Results</p>
<p>We are able to learn optimal policies for all three tasks. For pushhard and slide-hard tasks, HER is unable to even learn to reach the object as shown in Fig 3. This can be attributed to a mismatch in the kind of goals provided to the parameterized policy and the ones on which the agent learns off-policy. On the other hand, following the micro scheme, we are able to gradually start learning to reach and push / slide the object to nearby generated goals and then gain expertise with respect to the target goals. For Pick and Place, since the goal is always in the air and the object always on the table, a similar mismatch is conceivable.</p>
<p>MACRO CURRICULUM</p>
<p>A macro curriculum scheme allows extracting sub-goals by leveraging demonstrated states or observations and sequentially learning the sub-policies for each sub-goal. In the experiments we consider, this implies that learning to achieve the second sub-goal is facilitated by leveraging previous learning of achieving the first sub-goal (learning to push uses already gathered information about learning to reach). We argue that this setting is general enough because each sub-policy itself learns a hard task (the task of reaching) instead of simple "macro" actions (moving the manipulator continuously in a particular direction). This allows representing the final task policy as comprising each sub-policy. Specifically, we consider long horizon tasks and assume that few demonstration state trajectories τ = s 0 , s 1 , ...s t are available for the given tasks. In general, detecting changes in state representation has been shown to be a good method for extracting sub-goals. This is since system dynamics change suddenly around such sub-goals. In our case, the dense reward (eq. 4) computed per time step for a demonstration is used as the signal for sub-goal extraction. We compute the gradient ratio for such a signal and choose the sub-goal as the state for which consistent spikes are observed. Fig 4 shows the plots for such a dense reward signal in the three tasks considered. The intuition for finding a good sub-goal in a typical manipulation task is to observe that there is a sudden change in the dynamics of the system. For example, if the robot is trying to push a block, it can be easily seen that once the robot explores and starts to interact with the block, the policy will differ as the block interaction dynamics also affect the reward now. For demonstration trajectories, we observe that the gradient ratio of the dense reward always results in consistent spikes near the object position, proving that it is a good sub-goal for learning the three tasks mentioned.
r dense = || д achieved − д desir ed || 2(4)
Learning between two such sub-goals can be performed by following a micro curriculum scheme detailed above. The extracted sub-goals form a set of states that are achieved by most of the sampled expert trajectories. Note that these sub-goals are dependent on the start state. This is because we consider learning over varied goals, thus using goal conditioned policies and not over a single goal state. Given a policy π (s t , sд t +1 ) that has learnt to achieve a sub-goal sд t allows the agent to achieve the next sub-goal sд t +1 by leveraging previous information.</p>
<p>Consider the example of robotic ant navigation where to reach the goal state, the ant needs to collect a key which will open the door to the goal state room. The point we make here is that only using a micro scheme will generate goals between the ants start position and the goal position. However, doing so will result in the ant always jamming against the door with no success in opening it. Since the key lies along another path, through which no micro goals are generated, the agent never learns to open the door. This is where observing an expert and using it to learn that sub-goal lies at the key location becomes relevant. Following this, a micro scheme can be used to learn each sub-policy, that of reaching to the key from the start state and that of reaching to the actual goal state from the key location.</p>
<p>Method 1 : Extract sub-goals</p>
<p>Collect state demonstration trajectories τ Compute dense reward obtained at each stage, r dense = (AchievedGoal i -DesiredGoal i ) 2 Compute ratio of gradient of the dense reward, r дr ad for each state in an expert trajectory p ← Normalize r дr ad in [0, 1] sub-goals ← Sample num_subgoals states from each trajectory based on highest probability p for n = 1, ..., N iterations do Train sub-goal extractor F (sub-goals, start_states) for end return F Figure 4: sub-goal Extraction for Fetch Push, Slide and Pick and Place for 3 expert trajectory samples a. Top row : dense reward, a. Middle row : reward gradient, a. Bottom row : ratio of reward gradient. sub-goal for a particular trajectory is taken as the achieved goal at time t, t being obtained by observing the peaks in the ratio of gradient curve. This is done for all such demonstration trajectories and the mapping F is learned over this set of sub-goals and the corresponding start states.</p>
<p>Macro -Tasks Considered</p>
<p>4.1.1 Receptor-PickAndPlace task. We introduce a new task setting called Receptor-PickandPlace which comprises an object placed on a table, a receptor site on the table, and a target located in the air. As shown in Fig 5, the green and red markers represent the receptor and the goal locations respectively. The agent is required to pick and place the object at a target, which gets activated only if the object passes through the receptor site. Therefore, the agent is not rewarded even if the object is successfully placed at the target, if it does not pass from the receptor site. Such a task becomes extremely difficult to solve because of a sequencing behavior involved and a sparse reward available. We show how combining the macro and micro schemes can solve this task, by 1) leveraging demonstration states to extract a sub-goal near the receptor site and 2) using a powerful micro scheme to realize the sequencing of tasks involved, i.e. first moving the block to the receptor and then to the target. r = 0, if receptor on and distance to goal &lt; r thr eshold -1, otherwise</p>
<p>4.1.2 Push-far and Slide-far tasks. We also consider variants of the pushing and sliding tasks in which the start state (the gripper position) is considerably far from the table and varied as opposed to the default case where the gripper always starts from a single state and over the table. </p>
<p>Results</p>
<p>For the Receptor-PickandPlace task, recognizing the receptor as a sub-goal is crucial to learning. There is a significant peak in the dense reward gradient ratio around the receptor location, proving that the sub-goal extraction in the macro scheme is able to leverage demonstrations efficiently. This when combined with a micro scheme is able to learn the sequence of going to the receptor first with the block, thus activating the target, followed by placing it over the target. HER and micro scheme applied individually would fail to learn this task as shown in Fig 5, for different reasons. For HER, the task is too difficult because of the target being quite far and always sampled in the air. With a micro scheme alone, initially we see that the policy learned tries to pick and place the object to targets which are just above the table directly without going over the receptor. However, since the agent is not being rewarded, it quickly diverges to random behavior.</p>
<p>For the Push-far and Slide-far tasks, both MaMiC and HER learn useful policies. Since in these tasks, object and target lie in overlapping distributions, HER is able to perform well as shown in Fig 6. However, please note the extremely high variance in HER, ranging from solving the task in some instances to learning no useful behavior at all in some. This can potentially be attributed to the fact that since the gripper starts at a significantly different part of the state space as the block, learning no longer remains as stable as when the gripper starts over the table and close to the block. MaMiC, on the other hand is able to first learn the reaching sub-task by identifying locations close to the object's position as good sub-goals and then learns to push the object. Please note that MaMiC provides a clear acceleration in this case and is more stable than HER. [11] exploit the idea of starting from states near the goal and then gradually expanding the starting distribution to learn the overall task. This works because the agent slowly starts to learn how to reach states which are close to the goal. [7] build on this concept and propose a scheme for expanding the start state distribution based on the reward received while starting from such states. However, as mentioned, the usual assumption here is that the agent has the ability to reset to any state, which is not general enough. Moreover, the experiments are shown on tasks having single goal states, and therefore the policy is not generalized for a multiple goal domain such as pick and place. It is not at all trivial to extend this idea for goal parameterized policies as well. [23] also propose an automatic curriculum generation scheme, but work on the assumption that the environment is either reversible or resettable. There have been other works such as [15], [22] which propose different methods for extracting sub-goals. On a higher level, given a sub-goal extraction technique and a function which maps goals to states, our method can work on domains other than robotic manipulation as well. A by-product of an evolving policy, as in our method, is that the sub policies can be saved as learnt options ( [25], [2]) and then used for transfer to tasks which define a different meaning but require similar options. Similar ideas have been reported in [6], [4], where the agent learns a set of skills in a pre training procedure. Such skills are later combined with a master policy which allows for efficient exploration. These works mainly build on a bottom-up approach which restricts the meta-policy required to solve complex tasks to comprise only pre-defined or pre-learnt options.</p>
<p>RELATED AND FUTURE WORK</p>
<p>Since the setting of the algorithm is quite general, there are multiple directions for extending this work. The next challenge is to show how such a technique performs on even more longer horizon tasks, perhaps involving multiple objects as well. Working with image based observations can allow for learning richer representations useful in sub-goal extraction. Moreover, collecting state or observation demonstration trajectories is relatively simpler and more intuitive with images. Considering better heuristics for how α is updated to produce goals closer to the DesiredGoal distribution is an important point to improve upon. Another avenue for future work is to incorporate different schemes of sub-goal extraction which exploit domain specific properties.</p>
<p>CONCLUSION</p>
<p>We introduce a dual curriculum scheme for robotic manipulation which aids in exploration in robotic manipulation tasks with very sparse rewards. We show how the micro scheme is a powerful method for generating goals intelligently and can allow solving hard variants of the pushing, sliding and pick and place tasks without resetting to arbitrary states, starting from favorable states or using expert actions. Moreover, through the Receptor-PickandPlace task, we emphasize on the need for a macro scheme combined with micro when a task involves completing sub-tasks sequentially.</p>
<p>Proc. of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), N. Agmon, M. E. Taylor, E. Elkind, M. Veloso (eds.), May 2019, Montreal, Canada © 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved. https://doi.org/doi</p>
<p>Figure 1 :
1A diagram illustrating the working of MaMiC</p>
<p>We run all experiments till 150 epochs on 5 CPU cores. Each epoch consists of 50 cycles. For each cycle 40 training iterations of DDPG are performed. Both the Actor and Critic networks in DDPG are 3 layer MLPs with ReLU non-linearities, 256 nodes each and learning rate as 10 −3 .</p>
<p>Figure 2 :Figure 3 :
23The Fetch Pick and Place, Push and Slide environments Micro curriculum performance compared with HER for a. (left) Fetch Pick and Place b. (middle) Fetch Slide-hard, c.(right) Fetch Push-hard. Micro-g refers to a goal sampling strategy comprising a mixture of HER samples and Micro goals.</p>
<p>Figure 5 :
5Receptor-PickAndPlace task :</p>
<p>Figure 6 :
6MaMiC's performance compared with HER when the agent starts from an widened initial distribution for a. (left) Fetch Push-far b. (right) Fetch Slide-far</p>
<p>OpenAI Pieter Abbeel, and Wojciech Zaremba. 2017. Hindsight experience replay. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mcgrew, Josh Tobin, Advances in Neural Information Processing Systems. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. 2017. Hindsight experience replay. In Advances in Neural Information Processing Systems. 5048-5058.</p>
<p>The Option-Critic Architecture. Pierre-Luc Bacon, Jean Harb, Doina Precup, AAAI. Pierre-Luc Bacon, Jean Harb, and Doina Precup. 2017. The Option-Critic Archi- tecture.. In AAAI. 1726-1734.</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningACMYoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning. ACM, 41-48.</p>
<p>Bruno Da Silva, George Konidaris, Andrew Barto, arXiv:1206.6398Learning parameterized skills. arXiv preprintBruno Da Silva, George Konidaris, and Andrew Barto. 2012. Learning parameter- ized skills. arXiv preprint arXiv:1206.6398 (2012).</p>
<p>A survey on policy search for robotics. Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, Foundations and Trends® in Robotics. 2Marc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. 2013. A survey on policy search for robotics. Foundations and Trends® in Robotics 2, 1-2 (2013), 1-142.</p>
<p>Carlos Florensa, Yan Duan, Pieter Abbeel, arXiv:1704.03012Stochastic neural networks for hierarchical reinforcement learning. arXiv preprintCarlos Florensa, Yan Duan, and Pieter Abbeel. 2017. Stochastic neural networks for hierarchical reinforcement learning. arXiv preprint arXiv:1704.03012 (2017).</p>
<p>Reverse curriculum generation for reinforcement learning. Carlos Florensa, David Held, Markus Wulfmeier, Pieter Abbeel, arXiv:1707.05300arXiv preprintCarlos Florensa, David Held, Markus Wulfmeier, and Pieter Abbeel. 2017. Reverse curriculum generation for reinforcement learning. arXiv preprint arXiv:1707.05300 (2017).</p>
<p>Generative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in neural information processing systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural information processing systems. 2672-2680.</p>
<p>Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. Shixiang Gu, Ethan Holly, Timothy Lillicrap, Sergey Levine, 2017 IEEE International Conference on. IEEE. Robotics and Automation (ICRAShixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. 2017. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In Robotics and Automation (ICRA), 2017 IEEE International Conference on. IEEE, 3389-3396.</p>
<p>Automatic goal generation for reinforcement learning agents. David Held, Xinyang Geng, Carlos Florensa, Pieter Abbeel, arXiv:1705.06366arXiv preprintDavid Held, Xinyang Geng, Carlos Florensa, and Pieter Abbeel. 2017. Automatic goal generation for reinforcement learning agents. arXiv preprint arXiv:1705.06366 (2017).</p>
<p>Skill discovery in continuous reinforcement learning domains using skill chaining. George Konidaris, G Andrew, Barto, Advances in neural information processing systems. George Konidaris and Andrew G Barto. 2009. Skill discovery in continuous reinforcement learning domains using skill chaining. In Advances in neural infor- mation processing systems. 1015-1023.</p>
<p>Continuous control with deep reinforcement learning. Timothy Lillicrap, Jonathan Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra, cs.LG/1509.02971arXiv preprintTimothy Lillicrap, Jonathan Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous control with deep reinforcement learning.(2016). arXiv preprint cs.LG/1509.02971 (2016).</p>
<p>Imitation from observation: Learning to imitate behaviors from raw video via context translation. Yuxuan Liu, Abhishek Gupta, Pieter Abbeel, Sergey Levine, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEEYuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. 2018. Imitation from observation: Learning to imitate behaviors from raw video via context translation. In 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 1118-1125.</p>
<p>Least squares generative adversarial networks. Xudong Mao, Qing Li, Haoran Xie, Y K Raymond, Zhen Lau, Stephen Paul Wang, Smolley, 2017 IEEE International Conference on Computer Vision (ICCV). IEEEXudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. 2017. Least squares generative adversarial networks. In 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2813-2821.</p>
<p>Automatic discovery of subgoals in reinforcement learning using diverse density. Amy Mcgovern, G Andrew, Barto, ICML. 1Amy McGovern and Andrew G Barto. 2001. Automatic discovery of subgoals in reinforcement learning using diverse density. In ICML, Vol. 1. 361-368.</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, G Marc, Alex Bellemare, Martin Graves, Andreas K Riedmiller, Georg Fidjeland, Ostrovski, Nature. 518529Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. Nature 518, 7540 (2015), 529.</p>
<p>Overcoming exploration in reinforcement learning with demonstrations. Ashvin Nair, Bob Mcgrew, Marcin Andrychowicz, Wojciech Zaremba, Pieter Abbeel, arXiv:1709.10089arXiv preprintAshvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. 2017. Overcoming exploration in reinforcement learning with demon- strations. arXiv preprint arXiv:1709.10089 (2017).</p>
<p>Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob Mcgrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, arXiv:1802.09464Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research. arXiv preprintMatthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. 2018. Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research. arXiv preprint arXiv:1802.09464 (2018).</p>
<p>Universal value function approximators. Tom Schaul, Daniel Horgan, Karol Gregor, David Silver, International Conference on Machine Learning. Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. 2015. Universal value function approximators. In International Conference on Machine Learning. 1312-1320.</p>
<p>Trust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, International Conference on Machine Learning. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. Trust region policy optimization. In International Conference on Machine Learning. 1889-1897.</p>
<p>Mastering the game of Go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 529David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel- vam, Marc Lanctot, et al. 2016. Mastering the game of Go with deep neural networks and tree search. nature 529, 7587 (2016), 484-489.</p>
<p>Identifying useful subgoals in reinforcement learning by local graph partitioning. Özgür Şimşek, Alicia P Wolfe, Andrew G Barto, Proceedings of the 22nd international conference on Machine learning. the 22nd international conference on Machine learningACMÖzgür Şimşek, Alicia P Wolfe, and Andrew G Barto. 2005. Identifying useful subgoals in reinforcement learning by local graph partitioning. In Proceedings of the 22nd international conference on Machine learning. ACM, 816-823.</p>
<p>Intrinsic motivation and automatic curricula via asymmetric self-play. Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, Rob Fergus, arXiv:1703.05407arXiv preprintSainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Rob Fergus. 2017. Intrinsic motivation and automatic curricula via asymmetric self-play. arXiv preprint arXiv:1703.05407 (2017).</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT press Cambridge1Richard S Sutton and Andrew G Barto. 1998. Reinforcement learning: An intro- duction. Vol. 1. MIT press Cambridge.</p>
<p>Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Doina Richard S Sutton, Satinder Precup, Singh, Artificial intelligence. 112Richard S Sutton, Doina Precup, and Satinder Singh. 1999. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial intelligence 112, 1-2 (1999), 181-211.</p>
<p>Mujoco: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEE. Emanuel Todorov, Tom Erez, and Yuval Tassa. 2012. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEE, 5026-5033.</p>            </div>
        </div>

    </div>
</body>
</html>
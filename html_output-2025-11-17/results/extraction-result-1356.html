<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1356 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1356</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1356</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-ba62f4dd92ab79375e75bdb47c54f935d097b380</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ba62f4dd92ab79375e75bdb47c54f935d097b380" target="_blank">Learning Graph Search Heuristics</a></p>
                <p><strong>Paper Venue:</strong> LOG IN</p>
                <p><strong>Paper TL;DR:</strong> Experiments show that PHIL reduces the number of explored nodes compared to state-of-the-art methods on benchmark datasets by 58.5\% on average, can be directly applied in diverse graphs ranging from biological networks to road networks, and allows for fast planning in time-critical robotics domains.</p>
                <p><strong>Paper Abstract:</strong> Searching for a path between two nodes in a graph is one of the most well-studied and fundamental problems in computer science. In numerous domains such as robotics, AI, or biology, practitioners develop search heuristics to accelerate their pathfinding algorithms. However, it is a laborious and complex process to hand-design heuristics based on the problem and the structure of a given use case. Here we present PHIL (Path Heuristic with Imitation Learning), a novel neural architecture and a training algorithm for discovering graph search and navigation heuristics from data by leveraging recent advances in imitation learning and graph representation learning. At training time, we aggregate datasets of search trajectories and ground-truth shortest path distances, which we use to train a specialized graph neural network-based heuristic function using backpropagation through steps of the pathfinding process. Our heuristic function learns graph embeddings useful for inferring node distances, runs in constant time independent of graph sizes, and can be easily incorporated in an algorithm such as A* at test time. Experiments show that PHIL reduces the number of explored nodes compared to state-of-the-art methods on benchmark datasets by 58.5\% on average, can be directly applied in diverse graphs ranging from biological networks to road networks, and allows for fast planning in time-critical robotics domains.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1356.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1356.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GridBench-8conn</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>8-connected 200x200 Grid Benchmark Datasets (Bhardwaj et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of 8 grid-based pathfinding benchmark datasets (200x200 grids, 8-connected) with obstacle configurations (gaps, forests, mazes, bugtraps) used to evaluate search heuristics; tasks are start-to-goal shortest-path search minimizing node expansions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>8-connected 200x200 grid benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Synthetic 2D grid navigation tasks with obstacles creating bottlenecks and local minima (variants named Alternating gaps, Single Bugtrap, Shifting gaps, Forest, Bugtrap+Forest, Gaps+Forest, Mazes, Multiple Bugtraps); domain: grid-based path planning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Regular grid topology, 8-neighbour connectivity (each node connects to up to 8 neighbours), sparse locally regular adjacency</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>200 x 200 nodes (40,000 nodes per graph), multiple graphs per dataset (training/validation/test splits reported: e.g., 200 train, 70 val, 100 test)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PHIL (learned heuristic) and baselines (SAIL, BFS, A*, BFWS, Neural A*, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PHIL: a recurrent graph neural network heuristic h_theta with an explored-graph memory z_t trained via AggreVaTe-style imitation learning and truncated BPTT; used inside greedy best-first search expanding fringe nodes with minimal predicted distance; constant-time per evaluated fringe node via bounded 1-hop neighborhood sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Number of expanded nodes before reaching goal (also reported as percent reduction vs baselines and ratios to baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>PHIL reduces number of explored nodes by 58.5% on average compared to SAIL across the 8 grid datasets; per-dataset variations (e.g., up to 82.3% reduction on Gaps+Forest).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Greedy best-first search with a learned, memory-augmented heuristic (planning-based, memory-equipped policy)</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Topology affects performance: bottlenecks, bugtrap-like structures and mazes increase redundant expansions for naive heuristics; PHIL's memory and local neighborhood sampling reduce redundant expansions and help escape local minima. Increased local neighborhood sampling (n) improves performance until a plateau; attention-based GNNs help in more complex topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>PHIL consistently outperforms baselines across grid topologies; relative gains vary by topology (largest gains in datasets with mixed obstacles/bottlenecks such as Gaps+Forest). Attention GNNs (GAT) performed best in the most complex Gaps+Forest; DeeperGCN (softmax) and maximization aggregation performed well on many datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that include a memory embedding of explored graph (recurrent GNN) are better on topologies with bottlenecks and traps; evaluating only newly added fringe nodes (no re-evaluation) yields computational benefits but can limit re-prioritization; attention and larger memory capacity improve policies on complex topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Graph Search Heuristics', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1356.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1356.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gaps+Forest</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaps+Forest grid dataset (one grid variant within benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A challenging grid topology combining gaps and forest-like obstacles producing multiple bottlenecks and complex navigation constraints, used to test escape from local minima and redundancy reduction in search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Gaps+Forest</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Grid-based navigation with dispersed gaps and forest obstacles producing multiple planning bottlenecks and challenging routes; domain: synthetic pathfinding.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>8-connected grid with obstacle-induced irregular connectivity and narrow corridors (bottlenecks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>200 x 200 grid</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PHIL, SAIL, BFWS, A*, BFS, Neural A*</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PHIL uses a recurrent GNN memory and neighborhood sampling (n) during heuristic computation; trained via imitation with oracle cost-to-go and truncated BPTT.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Number of expanded nodes to goal; percent reduction vs baselines</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>PHIL achieves up to 82.3% reduction in search effort vs SAIL on Gaps+Forest (reported as maximal reduction across datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-augmented heuristic-guided greedy search; attention beneficial in this complex topology</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Complexity (multiple bottlenecks) increases need for attention and memory; attention-based GNN (GAT) outperformed others on this dataset, indicating that topologies with multiple bottlenecks benefit from mechanisms that weight neighbor information non-uniformly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Gaps+Forest showed the largest relative improvements for PHIL vs baselines, indicating that learned, memory-based heuristics particularly help in topologies with many bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that incorporate attention and larger effective receptive fields (via neighborhood sampling) perform better; memory capacity and local neighborhood size (n) both contribute to escaping trap-like structures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Graph Search Heuristics', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1356.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1356.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bugtrap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bugtrap grid variants (Single / Multiple Bugtraps)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Grid environments containing trap-like structures (bugtraps) that can mislead naive heuristics into costly local minima and require structured exploration to escape.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Single Bugtrap / Multiple Bugtraps (grid variants)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Grid-based navigation with enclosed trap regions (bugtraps) that can cause large numbers of redundant expansions if the heuristic is misled.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>8-connected grid with enclosed trap structures and local regions of low connectivity relative to surroundings</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>200 x 200 grid</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PHIL and baselines (SAIL, BFWS, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PHIL: recurrent GNN heuristic trained by imitation, evaluates newly added fringe nodes using sampled 1-hop neighborhoods and retains memory embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Number of node expansions</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>PHIL significantly reduces redundant expansions in bugtrap scenarios compared to baselines (per-table per-dataset differences reported, overall average reduction vs SAIL is 58.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-augmented greedy heuristic that can recognize and avoid trap-induced expansion redundancy</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Bugtraps induce local minima for greedy heuristics; PHIL's memory helps detect previously-explored regions and avoid unnecessary exploration, improving efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Datasets with bugtraps show clear benefit from learned heuristics with memory relative to naive heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies require mechanisms to recognize trap regions and prioritize exits; recurrent memory and neighborhood context support this behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Graph Search Heuristics', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1356.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1356.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mazes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maze grid dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Grid-based maze topologies that produce long narrow corridors and dead-ends requiring backtracking, testing heuristic ability to avoid exhaustive exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mazes (grid dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Grid mazes with long corridors and dead-ends; domain: synthetic path planning emphasizing long-range corridor navigation and backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>8-connected grid constrained by maze walls producing many dead-end leaf regions and narrow corridors</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>200 x 200 grid</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PHIL and baseline heuristics (SAIL, BFS, A*, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PHIL uses local sampling of 1-hop neighborhoods and recurrent memory to estimate distance-to-goal for fringe nodes; greedy expansion based on predicted distances.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Number of expanded nodes to goal</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>PHIL outperforms SAIL and other baselines across maze datasets (aggregate improvements reported; exact per-dataset numbers in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-informed planning-based heuristic to reduce backtracking and redundant expansions</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Dead-ends and narrow corridors increase redundant expansions for uninformed search; learned heuristics that incorporate explored-history embeddings reduce redundant exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>PHIL shows reduced redundancy in explored nodes compared to SAIL especially in maze-like topologies where local minima and dead-ends are prevalent.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies benefit from memory to remember explored corridors/entrances; ability to generalize from local features to corridor structure aids efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Graph Search Heuristics', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1356.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1356.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiverseGraphs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Real-world graph groups: Citation, Biological, AST, Road networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of real-world graph domains used to evaluate generalization: citation networks (Cora, PubMed, CiteSeer, Coauthor), biological graphs (OGBG-Molhiv, PPI, Proteins, Enzymes), abstract syntax trees (OGBG-Code2), and road networks (OSMnx Modena, OSMnx New York).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Citation / Biological / AST / Road networks (real-world graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-synthetic graphs from different domains: citation graphs (academic-citation networks), biological molecular/interactome graphs, program AST graphs, and geographic road networks; tasks are shortest-path navigation between node pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Heterogeneous: citation graphs and ASTs typically have moderate average degree and community structure; road networks are spatial and sparse with geographic locality; biological graphs vary (often sparse, varying degree distributions). Average degrees reported per dataset in paper tables (varies).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Varies by dataset: examples given include Cora (2,708 nodes), PubMed (19,717), Coauthor (18k+), OSMnx Modena (~29,324 nodes), OSMnx New York (~54,128 nodes), OGBG datasets with many graphs (e.g., OGBG-Molhiv: 41,127 graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PHIL and baselines (SL, A*, h_euc, BFS, SAIL, BFWS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PHIL: recurrent GNN heuristic with bounded neighborhood sampling and memory embedding; trained via imitation with aggregated trajectories and TBTT; designed to run in O(1) per fringe node w.r.t graph size.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Number of node expansions (often reported as ratio to best baseline or to Euclidean heuristic h_euc)</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Across these diverse real-world graphs, PHIL outperforms the best baseline per dataset by 13.4% on average; excluding OGBG datasets (which discourage structural similarity between train/test), improvement is 19.5% on average. Road networks: PHIL shows substantial reduction (e.g., Modena reported ratio ~0.489 vs baseline h_euc).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Graph-search heuristics learned with local neighborhood information and memory; policy must generalize across structural variability (training on few start-goal pairs requires generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Structural consistency between train and test graphs affects learned heuristic generalization (OGBG splits that force structural difference degrade PHIL's gains); graphs with unique node features (or augmented features) make constant-time heuristics feasible; increasing neighborhood sample size n helps up to a point; memory and model capacity interact with graph complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>PHIL provides consistent improvement across citation, biological, AST and road graphs, but performance gains are reduced when dataset splits purposely introduce structural heterogeneity (OGBG scaffold/project splits).</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies require expressive node/edge features to disambiguate symmetric nodes; when those are absent, either larger neighborhood sampling n or non-constant preprocessing (position-aware features) are needed. Memory capacity and neighborhood size must be tuned per-graph complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Graph Search Heuristics', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1356.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1356.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoomSimple</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Room simple (indoor UAV planning environment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated indoor room environment discretized to a 3D grid (50x50x25) with furniture; used to evaluate PHIL on practical drone path planning and collision avoidance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Room simple (3D indoor UAV environment)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Indoor drone navigation in a simulated furnished room (single table and furniture); environment discretized into a 50x50x25 3D grid with randomly removed 5 sub-graphs to simulate obstacles; domain: UAV collision-free planning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>3D grid connectivity with obstacles; sparse regions due to removed sub-graphs and furniture creating bottlenecks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>50 x 50 x 25 grid (62,500 discretized cells/nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PHIL and baselines (SL, A*, h_euc, BFS, BFWS, S+IL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PHIL: recurrent GNN heuristic with n=4 neighborhood sampling in 3D, trained via imitation learning; used inside greedy best-first search to plan collision-free paths.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Number of expanded nodes relative to Euclidean heuristic (h_euc) and to optimal shortest-path expansions</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>PHIL expands ~0.785 times as many nodes as h_euc baseline (reported ratio = 0.785) and expands only ~0.3% more nodes than the optimal least-possible expansions (shortest path baseline ~0.782).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Planning-based learned heuristic with memory that closely approximates oracle distances (near-optimal expansions)</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Simple indoor topology allows PHIL to approach optimal expansions closely; naive heuristics (h_euc) expand substantially more nodes (~27.8% worse than optimal in simple room).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>PHIL remained close to optimal in both simple and adversarial room topologies, with larger relative degradation for naive heuristics in adversarial settings.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Memory-augmented heuristic enables near-optimal greedy search in structured indoor environments; small neighborhood size (n=4) sufficed in this 3D indoor setting with discretization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Graph Search Heuristics', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1356.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1356.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoomAdversarial</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Room adversarial (indoor UAV planning environment with bottlenecks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated indoor room with adversarial furniture placement creating narrow bottlenecks, discretized to a 3D grid; used to test robustness of learned heuristics under challenging, constrained connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Room adversarial (3D indoor UAV)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Indoor drone planning environment with multiple tables arranged to create a bottleneck; discretized to 50x50x25 grid and with randomized obstacle placements to simulate adversarial planning scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>3D grid with narrow bottlenecks reducing connectivity in critical regions; obstacle-induced sparsity and chokepoints</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>50 x 50 x 25 grid (62,500 nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PHIL and baselines (SL, A*, h_euc, BFS, BFWS, S+IL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PHIL: recurrent GNN heuristic trained with n=4 neighborhood sampling and TBTT; used in greedy best-first search; trained and tested with randomized obstacle sub-graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Number of expanded nodes relative to h_euc and optimal expansions; percent over optimal</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>PHIL expands ~0.895 times as many nodes as h_euc baseline (ratio = 0.895) and expands approx. 4.9% more nodes than the optimal least-possible expansions in adversarial room.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-based learned heuristic combined with greedy best-first search; planning-based policies that learn to avoid bottlenecks perform best</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Bottlenecks amplify the disadvantage of naive heuristics; PHIL's learned memory and local neighborhood features reduce redundancy and approach oracle performance but show modest degradation relative to simple rooms (4.9% above optimal).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>In adversarial room PHIL still outperforms baselines but shows a larger gap to optimal than in simpler room, demonstrating topology sensitivity (bottlenecks increase difficulty).</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies need sufficient memory and local context to handle chokepoints; PHIL's recurrent memory and neighborhood context provide robustness but some performance gap remains in adversarial chokepoint topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Graph Search Heuristics', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1356.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1356.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TopologyAblations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Topology & model-architecture ablations (neighborhood size, memory capacity, GNN choice)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Controlled ablations exploring how graph-local sampling size (n), memory embedding dimensionality (d), and GNN aggregation type affect search performance across different topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Ablation experiments across down-scaled grid datasets (Alternating gaps, Forest, Gaps+Forest)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Downscaled 40x40 grid variants used to measure effects of hyperparameters and architecture choices on search efficiency in different topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Grid graphs with varying obstacle-induced connectivity patterns (bottlenecks, forests, gaps), used to probe sensitivity to local/topological structure.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>40 x 40 grids (downscaled for ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PHIL (variants: PHIL-zero, PHIL-roll) with different GNNs (GAT, MPNN, DeeperGCN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Variants of PHIL that differ in whether rolled-in memory states are stored, neighborhood sampling size n, GRU memory dimensionality d, and choice of GNN aggregation (max, sum, softmax/power, attention).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Ratio of explored nodes relative to optimal (shortest-path expansions) or relative to A*</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Key ablation findings: increasing n (neighbourhood size) improves performance up to n=2 then plateaus; increasing memory d helps up to ~d=32 then may overfit; maximization-based aggregation slightly outperforms others (~1.57% average improvement), and attention (GAT) helps in most complex topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-augmented policies with tuned neighborhood sampling and appropriate GNN aggregation (attention for complex topologies)</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Performance positively correlates with local neighborhood information (n) and memory capacity (d) up to task-dependent thresholds; complex topologies (many bottlenecks) benefit more from attention and larger memory.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Simpler topologies show marginal sensitivity to memory capacity; harder topologies (Gaps+Forest) show large benefits from increased memory up to a point and from attention mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Optimal policy structure depends on topology complexity: simple graphs need less memory/neighbor context, while complex graphs require larger memory, attention-based aggregation, and sufficient local receptive field size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Graph Search Heuristics', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning heuristic search via imitation <em>(Rating: 2)</em></li>
                <li>Best-first width search: Exploration and exploitation in classical planning <em>(Rating: 2)</em></li>
                <li>Open graph benchmark: Datasets for machine learning on graphs <em>(Rating: 2)</em></li>
                <li>Retro*: Learning retrosynthetic planning with neural guided a* search <em>(Rating: 1)</em></li>
                <li>Multi-heuristic a* <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1356",
    "paper_id": "paper-ba62f4dd92ab79375e75bdb47c54f935d097b380",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "GridBench-8conn",
            "name_full": "8-connected 200x200 Grid Benchmark Datasets (Bhardwaj et al.)",
            "brief_description": "A suite of 8 grid-based pathfinding benchmark datasets (200x200 grids, 8-connected) with obstacle configurations (gaps, forests, mazes, bugtraps) used to evaluate search heuristics; tasks are start-to-goal shortest-path search minimizing node expansions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "8-connected 200x200 grid benchmark",
            "environment_description": "Synthetic 2D grid navigation tasks with obstacles creating bottlenecks and local minima (variants named Alternating gaps, Single Bugtrap, Shifting gaps, Forest, Bugtrap+Forest, Gaps+Forest, Mazes, Multiple Bugtraps); domain: grid-based path planning.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Regular grid topology, 8-neighbour connectivity (each node connects to up to 8 neighbours), sparse locally regular adjacency",
            "environment_size": "200 x 200 nodes (40,000 nodes per graph), multiple graphs per dataset (training/validation/test splits reported: e.g., 200 train, 70 val, 100 test)",
            "agent_name": "PHIL (learned heuristic) and baselines (SAIL, BFS, A*, BFWS, Neural A*, etc.)",
            "agent_description": "PHIL: a recurrent graph neural network heuristic h_theta with an explored-graph memory z_t trained via AggreVaTe-style imitation learning and truncated BPTT; used inside greedy best-first search expanding fringe nodes with minimal predicted distance; constant-time per evaluated fringe node via bounded 1-hop neighborhood sampling.",
            "exploration_efficiency_metric": "Number of expanded nodes before reaching goal (also reported as percent reduction vs baselines and ratios to baselines)",
            "exploration_efficiency_value": "PHIL reduces number of explored nodes by 58.5% on average compared to SAIL across the 8 grid datasets; per-dataset variations (e.g., up to 82.3% reduction on Gaps+Forest).",
            "success_rate": null,
            "optimal_policy_type": "Greedy best-first search with a learned, memory-augmented heuristic (planning-based, memory-equipped policy)",
            "topology_performance_relationship": "Topology affects performance: bottlenecks, bugtrap-like structures and mazes increase redundant expansions for naive heuristics; PHIL's memory and local neighborhood sampling reduce redundant expansions and help escape local minima. Increased local neighborhood sampling (n) improves performance until a plateau; attention-based GNNs help in more complex topologies.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "PHIL consistently outperforms baselines across grid topologies; relative gains vary by topology (largest gains in datasets with mixed obstacles/bottlenecks such as Gaps+Forest). Attention GNNs (GAT) performed best in the most complex Gaps+Forest; DeeperGCN (softmax) and maximization aggregation performed well on many datasets.",
            "policy_structure_findings": "Policies that include a memory embedding of explored graph (recurrent GNN) are better on topologies with bottlenecks and traps; evaluating only newly added fringe nodes (no re-evaluation) yields computational benefits but can limit re-prioritization; attention and larger memory capacity improve policies on complex topologies.",
            "uuid": "e1356.0",
            "source_info": {
                "paper_title": "Learning Graph Search Heuristics",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Gaps+Forest",
            "name_full": "Gaps+Forest grid dataset (one grid variant within benchmark)",
            "brief_description": "A challenging grid topology combining gaps and forest-like obstacles producing multiple bottlenecks and complex navigation constraints, used to test escape from local minima and redundancy reduction in search.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Gaps+Forest",
            "environment_description": "Grid-based navigation with dispersed gaps and forest obstacles producing multiple planning bottlenecks and challenging routes; domain: synthetic pathfinding.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "8-connected grid with obstacle-induced irregular connectivity and narrow corridors (bottlenecks)",
            "environment_size": "200 x 200 grid",
            "agent_name": "PHIL, SAIL, BFWS, A*, BFS, Neural A*",
            "agent_description": "PHIL uses a recurrent GNN memory and neighborhood sampling (n) during heuristic computation; trained via imitation with oracle cost-to-go and truncated BPTT.",
            "exploration_efficiency_metric": "Number of expanded nodes to goal; percent reduction vs baselines",
            "exploration_efficiency_value": "PHIL achieves up to 82.3% reduction in search effort vs SAIL on Gaps+Forest (reported as maximal reduction across datasets).",
            "success_rate": null,
            "optimal_policy_type": "Memory-augmented heuristic-guided greedy search; attention beneficial in this complex topology",
            "topology_performance_relationship": "Complexity (multiple bottlenecks) increases need for attention and memory; attention-based GNN (GAT) outperformed others on this dataset, indicating that topologies with multiple bottlenecks benefit from mechanisms that weight neighbor information non-uniformly.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Gaps+Forest showed the largest relative improvements for PHIL vs baselines, indicating that learned, memory-based heuristics particularly help in topologies with many bottlenecks.",
            "policy_structure_findings": "Policies that incorporate attention and larger effective receptive fields (via neighborhood sampling) perform better; memory capacity and local neighborhood size (n) both contribute to escaping trap-like structures.",
            "uuid": "e1356.1",
            "source_info": {
                "paper_title": "Learning Graph Search Heuristics",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Bugtrap",
            "name_full": "Bugtrap grid variants (Single / Multiple Bugtraps)",
            "brief_description": "Grid environments containing trap-like structures (bugtraps) that can mislead naive heuristics into costly local minima and require structured exploration to escape.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Single Bugtrap / Multiple Bugtraps (grid variants)",
            "environment_description": "Grid-based navigation with enclosed trap regions (bugtraps) that can cause large numbers of redundant expansions if the heuristic is misled.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "8-connected grid with enclosed trap structures and local regions of low connectivity relative to surroundings",
            "environment_size": "200 x 200 grid",
            "agent_name": "PHIL and baselines (SAIL, BFWS, etc.)",
            "agent_description": "PHIL: recurrent GNN heuristic trained by imitation, evaluates newly added fringe nodes using sampled 1-hop neighborhoods and retains memory embeddings.",
            "exploration_efficiency_metric": "Number of node expansions",
            "exploration_efficiency_value": "PHIL significantly reduces redundant expansions in bugtrap scenarios compared to baselines (per-table per-dataset differences reported, overall average reduction vs SAIL is 58.5%).",
            "success_rate": null,
            "optimal_policy_type": "Memory-augmented greedy heuristic that can recognize and avoid trap-induced expansion redundancy",
            "topology_performance_relationship": "Bugtraps induce local minima for greedy heuristics; PHIL's memory helps detect previously-explored regions and avoid unnecessary exploration, improving efficiency.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Datasets with bugtraps show clear benefit from learned heuristics with memory relative to naive heuristics.",
            "policy_structure_findings": "Policies require mechanisms to recognize trap regions and prioritize exits; recurrent memory and neighborhood context support this behavior.",
            "uuid": "e1356.2",
            "source_info": {
                "paper_title": "Learning Graph Search Heuristics",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Mazes",
            "name_full": "Maze grid dataset",
            "brief_description": "Grid-based maze topologies that produce long narrow corridors and dead-ends requiring backtracking, testing heuristic ability to avoid exhaustive exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Mazes (grid dataset)",
            "environment_description": "Grid mazes with long corridors and dead-ends; domain: synthetic path planning emphasizing long-range corridor navigation and backtracking.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "8-connected grid constrained by maze walls producing many dead-end leaf regions and narrow corridors",
            "environment_size": "200 x 200 grid",
            "agent_name": "PHIL and baseline heuristics (SAIL, BFS, A*, etc.)",
            "agent_description": "PHIL uses local sampling of 1-hop neighborhoods and recurrent memory to estimate distance-to-goal for fringe nodes; greedy expansion based on predicted distances.",
            "exploration_efficiency_metric": "Number of expanded nodes to goal",
            "exploration_efficiency_value": "PHIL outperforms SAIL and other baselines across maze datasets (aggregate improvements reported; exact per-dataset numbers in paper tables).",
            "success_rate": null,
            "optimal_policy_type": "Memory-informed planning-based heuristic to reduce backtracking and redundant expansions",
            "topology_performance_relationship": "Dead-ends and narrow corridors increase redundant expansions for uninformed search; learned heuristics that incorporate explored-history embeddings reduce redundant exploration.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "PHIL shows reduced redundancy in explored nodes compared to SAIL especially in maze-like topologies where local minima and dead-ends are prevalent.",
            "policy_structure_findings": "Policies benefit from memory to remember explored corridors/entrances; ability to generalize from local features to corridor structure aids efficiency.",
            "uuid": "e1356.3",
            "source_info": {
                "paper_title": "Learning Graph Search Heuristics",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "DiverseGraphs",
            "name_full": "Real-world graph groups: Citation, Biological, AST, Road networks",
            "brief_description": "A set of real-world graph domains used to evaluate generalization: citation networks (Cora, PubMed, CiteSeer, Coauthor), biological graphs (OGBG-Molhiv, PPI, Proteins, Enzymes), abstract syntax trees (OGBG-Code2), and road networks (OSMnx Modena, OSMnx New York).",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Citation / Biological / AST / Road networks (real-world graphs)",
            "environment_description": "Non-synthetic graphs from different domains: citation graphs (academic-citation networks), biological molecular/interactome graphs, program AST graphs, and geographic road networks; tasks are shortest-path navigation between node pairs.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Heterogeneous: citation graphs and ASTs typically have moderate average degree and community structure; road networks are spatial and sparse with geographic locality; biological graphs vary (often sparse, varying degree distributions). Average degrees reported per dataset in paper tables (varies).",
            "environment_size": "Varies by dataset: examples given include Cora (2,708 nodes), PubMed (19,717), Coauthor (18k+), OSMnx Modena (~29,324 nodes), OSMnx New York (~54,128 nodes), OGBG datasets with many graphs (e.g., OGBG-Molhiv: 41,127 graphs).",
            "agent_name": "PHIL and baselines (SL, A*, h_euc, BFS, SAIL, BFWS)",
            "agent_description": "PHIL: recurrent GNN heuristic with bounded neighborhood sampling and memory embedding; trained via imitation with aggregated trajectories and TBTT; designed to run in O(1) per fringe node w.r.t graph size.",
            "exploration_efficiency_metric": "Number of node expansions (often reported as ratio to best baseline or to Euclidean heuristic h_euc)",
            "exploration_efficiency_value": "Across these diverse real-world graphs, PHIL outperforms the best baseline per dataset by 13.4% on average; excluding OGBG datasets (which discourage structural similarity between train/test), improvement is 19.5% on average. Road networks: PHIL shows substantial reduction (e.g., Modena reported ratio ~0.489 vs baseline h_euc).",
            "success_rate": null,
            "optimal_policy_type": "Graph-search heuristics learned with local neighborhood information and memory; policy must generalize across structural variability (training on few start-goal pairs requires generalization).",
            "topology_performance_relationship": "Structural consistency between train and test graphs affects learned heuristic generalization (OGBG splits that force structural difference degrade PHIL's gains); graphs with unique node features (or augmented features) make constant-time heuristics feasible; increasing neighborhood sample size n helps up to a point; memory and model capacity interact with graph complexity.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "PHIL provides consistent improvement across citation, biological, AST and road graphs, but performance gains are reduced when dataset splits purposely introduce structural heterogeneity (OGBG scaffold/project splits).",
            "policy_structure_findings": "Policies require expressive node/edge features to disambiguate symmetric nodes; when those are absent, either larger neighborhood sampling n or non-constant preprocessing (position-aware features) are needed. Memory capacity and neighborhood size must be tuned per-graph complexity.",
            "uuid": "e1356.4",
            "source_info": {
                "paper_title": "Learning Graph Search Heuristics",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "RoomSimple",
            "name_full": "Room simple (indoor UAV planning environment)",
            "brief_description": "A simulated indoor room environment discretized to a 3D grid (50x50x25) with furniture; used to evaluate PHIL on practical drone path planning and collision avoidance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Room simple (3D indoor UAV environment)",
            "environment_description": "Indoor drone navigation in a simulated furnished room (single table and furniture); environment discretized into a 50x50x25 3D grid with randomly removed 5 sub-graphs to simulate obstacles; domain: UAV collision-free planning.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "3D grid connectivity with obstacles; sparse regions due to removed sub-graphs and furniture creating bottlenecks",
            "environment_size": "50 x 50 x 25 grid (62,500 discretized cells/nodes)",
            "agent_name": "PHIL and baselines (SL, A*, h_euc, BFS, BFWS, S+IL)",
            "agent_description": "PHIL: recurrent GNN heuristic with n=4 neighborhood sampling in 3D, trained via imitation learning; used inside greedy best-first search to plan collision-free paths.",
            "exploration_efficiency_metric": "Number of expanded nodes relative to Euclidean heuristic (h_euc) and to optimal shortest-path expansions",
            "exploration_efficiency_value": "PHIL expands ~0.785 times as many nodes as h_euc baseline (reported ratio = 0.785) and expands only ~0.3% more nodes than the optimal least-possible expansions (shortest path baseline ~0.782).",
            "success_rate": null,
            "optimal_policy_type": "Planning-based learned heuristic with memory that closely approximates oracle distances (near-optimal expansions)",
            "topology_performance_relationship": "Simple indoor topology allows PHIL to approach optimal expansions closely; naive heuristics (h_euc) expand substantially more nodes (~27.8% worse than optimal in simple room).",
            "comparison_across_topologies": true,
            "topology_comparison_results": "PHIL remained close to optimal in both simple and adversarial room topologies, with larger relative degradation for naive heuristics in adversarial settings.",
            "policy_structure_findings": "Memory-augmented heuristic enables near-optimal greedy search in structured indoor environments; small neighborhood size (n=4) sufficed in this 3D indoor setting with discretization.",
            "uuid": "e1356.5",
            "source_info": {
                "paper_title": "Learning Graph Search Heuristics",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "RoomAdversarial",
            "name_full": "Room adversarial (indoor UAV planning environment with bottlenecks)",
            "brief_description": "A simulated indoor room with adversarial furniture placement creating narrow bottlenecks, discretized to a 3D grid; used to test robustness of learned heuristics under challenging, constrained connectivity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Room adversarial (3D indoor UAV)",
            "environment_description": "Indoor drone planning environment with multiple tables arranged to create a bottleneck; discretized to 50x50x25 grid and with randomized obstacle placements to simulate adversarial planning scenarios.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "3D grid with narrow bottlenecks reducing connectivity in critical regions; obstacle-induced sparsity and chokepoints",
            "environment_size": "50 x 50 x 25 grid (62,500 nodes)",
            "agent_name": "PHIL and baselines (SL, A*, h_euc, BFS, BFWS, S+IL)",
            "agent_description": "PHIL: recurrent GNN heuristic trained with n=4 neighborhood sampling and TBTT; used in greedy best-first search; trained and tested with randomized obstacle sub-graphs.",
            "exploration_efficiency_metric": "Number of expanded nodes relative to h_euc and optimal expansions; percent over optimal",
            "exploration_efficiency_value": "PHIL expands ~0.895 times as many nodes as h_euc baseline (ratio = 0.895) and expands approx. 4.9% more nodes than the optimal least-possible expansions in adversarial room.",
            "success_rate": null,
            "optimal_policy_type": "Memory-based learned heuristic combined with greedy best-first search; planning-based policies that learn to avoid bottlenecks perform best",
            "topology_performance_relationship": "Bottlenecks amplify the disadvantage of naive heuristics; PHIL's learned memory and local neighborhood features reduce redundancy and approach oracle performance but show modest degradation relative to simple rooms (4.9% above optimal).",
            "comparison_across_topologies": true,
            "topology_comparison_results": "In adversarial room PHIL still outperforms baselines but shows a larger gap to optimal than in simpler room, demonstrating topology sensitivity (bottlenecks increase difficulty).",
            "policy_structure_findings": "Policies need sufficient memory and local context to handle chokepoints; PHIL's recurrent memory and neighborhood context provide robustness but some performance gap remains in adversarial chokepoint topologies.",
            "uuid": "e1356.6",
            "source_info": {
                "paper_title": "Learning Graph Search Heuristics",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "TopologyAblations",
            "name_full": "Topology & model-architecture ablations (neighborhood size, memory capacity, GNN choice)",
            "brief_description": "Controlled ablations exploring how graph-local sampling size (n), memory embedding dimensionality (d), and GNN aggregation type affect search performance across different topologies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Ablation experiments across down-scaled grid datasets (Alternating gaps, Forest, Gaps+Forest)",
            "environment_description": "Downscaled 40x40 grid variants used to measure effects of hyperparameters and architecture choices on search efficiency in different topologies.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Grid graphs with varying obstacle-induced connectivity patterns (bottlenecks, forests, gaps), used to probe sensitivity to local/topological structure.",
            "environment_size": "40 x 40 grids (downscaled for ablation)",
            "agent_name": "PHIL (variants: PHIL-zero, PHIL-roll) with different GNNs (GAT, MPNN, DeeperGCN)",
            "agent_description": "Variants of PHIL that differ in whether rolled-in memory states are stored, neighborhood sampling size n, GRU memory dimensionality d, and choice of GNN aggregation (max, sum, softmax/power, attention).",
            "exploration_efficiency_metric": "Ratio of explored nodes relative to optimal (shortest-path expansions) or relative to A*",
            "exploration_efficiency_value": "Key ablation findings: increasing n (neighbourhood size) improves performance up to n=2 then plateaus; increasing memory d helps up to ~d=32 then may overfit; maximization-based aggregation slightly outperforms others (~1.57% average improvement), and attention (GAT) helps in most complex topologies.",
            "success_rate": null,
            "optimal_policy_type": "Memory-augmented policies with tuned neighborhood sampling and appropriate GNN aggregation (attention for complex topologies)",
            "topology_performance_relationship": "Performance positively correlates with local neighborhood information (n) and memory capacity (d) up to task-dependent thresholds; complex topologies (many bottlenecks) benefit more from attention and larger memory.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Simpler topologies show marginal sensitivity to memory capacity; harder topologies (Gaps+Forest) show large benefits from increased memory up to a point and from attention mechanisms.",
            "policy_structure_findings": "Optimal policy structure depends on topology complexity: simple graphs need less memory/neighbor context, while complex graphs require larger memory, attention-based aggregation, and sufficient local receptive field size.",
            "uuid": "e1356.7",
            "source_info": {
                "paper_title": "Learning Graph Search Heuristics",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning heuristic search via imitation",
            "rating": 2
        },
        {
            "paper_title": "Best-first width search: Exploration and exploitation in classical planning",
            "rating": 2
        },
        {
            "paper_title": "Open graph benchmark: Datasets for machine learning on graphs",
            "rating": 2
        },
        {
            "paper_title": "Retro*: Learning retrosynthetic planning with neural guided a* search",
            "rating": 1
        },
        {
            "paper_title": "Multi-heuristic a*",
            "rating": 1
        }
    ],
    "cost": 0.020062499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning Graph Search Heuristics</h1>
<p>Michal Pndy*<br>University of Cambridge<br>Uwe Leskovec<br>Stanford University</p>
<p>Gabriele Corso
MIT</p>
<p>Pietro Li
University of Cambridge</p>
<h2>Rex Ying</h2>
<p>Yale University</p>
<h2>Abstract</h2>
<p>Searching for a path between two nodes in a graph is one of the most well-studied and fundamental problems in computer science. In numerous domains such as robotics, AI, or biology, practitioners develop search heuristics to accelerate their pathfinding algorithms. However, it is a laborious and complex process to handdesign heuristics based on the problem and the structure of a given use case. Here we present PHIL (Path Heuristic with Imitation Learning), a novel neural architecture and a training algorithm for discovering graph search and navigation heuristics from data by leveraging recent advances in imitation learning and graph representation learning. At training time, we aggregate datasets of search trajectories and ground-truth shortest path distances, which we use to train a specialized graph neural network-based heuristic function using backpropagation through steps of the pathfinding process. Our heuristic function learns graph embeddings useful for inferring node distances, runs in constant time independent of graph sizes, and can be easily incorporated in an algorithm such as A* at test time. Experiments show that PHIL reduces the number of explored nodes compared to state-of-the-art methods on benchmark datasets by $58.5 \%$ on average, can be directly applied in diverse graphs ranging from biological networks to road networks, and allows for fast planning in time-critical robotics domains.</p>
<h2>1 Introduction</h2>
<p>Search heuristics are essential in several domains, including robotics, AI, biology, and chemistry [16]. For example, in robotics, complex robot geometries often yield slow collision checks, and search algorithms are constrained by the robot's onboard computation resources, requiring well-performing search heuristics that visit as few nodes as possible [1, 4]. In AI, domain-specific search heuristics are useful for improving the performance of inference engines operating on knowledge bases [3, 5]. Search heuristics have been previously also developed to reduce search efforts in protein-protein interaction networks [6] and in planning chemical reactions that can synthesize target chemical products [2]. This broad set of applications underlines the importance of good search heuristics that are applicable to a wide range of problems.
The search task can be formulated as a pathfinding problem on a graph, where given a graph, the task is to navigate and find a short feasible path from a start node to a goal node, while in the process visiting as few nodes as possible (Figure 1). The most straightforward approach would be to launch a search algorithm such as breadth-first search (BFS) and iteratively expand the graph from the start node until it reaches the goal node. Since BFS does not harness any prior knowledge about the graph, it usually visits many nodes before reaching the goal, which is expensive in cases such as robotics where visiting nodes is costly. To visit fewer nodes during the search, one may use domain-specific information about the graph via a heuristic function [7], which allows one to define a distance metric on graph nodes to prune directions that seem less promising to explore. Unfortunately, coming up with good search heuristics requires significant domain expertise and manual effort.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The goal is to navigate (find a path) from the start to the goal node. While BFS visits many nodes to find a start-to-goal path (left), one can use a heuristic based on the features of the nodes (e.g., Euclidean distance) on the graph to reduce the search effort (middle). We propose PHIL to learn a tailored search heuristic for a given graph, capable of reducing the number of visited nodes even further by exploiting the inductive biases of the graph (right).</p>
<p>While there has been significant progress in designing search heuristics, it remains a challenging problem. Classical approaches [8, 9] tend to hand-design search heuristics, which requires domain knowledge and a lot of trial and error. To alleviate this problem, there has been significant development in general-purpose search heuristics based on trading-off greedy expansions and novelty-based exploration [10-13] or search problem simplifications [14-16]. These approaches alleviate some of the common pitfalls of goal-directed heuristics, but we demonstrate that if possible, it is useful to learn domain-specific heuristics that can better exploit problem structure.
On the other hand, learning-based methods face a set of different challenges. Firstly, the data distribution is not i.i.d., as newly encountered graph nodes depend on past heuristic values, which means that supervised learning-based methods are not directly applicable. Secondly, heuristics should run fast, with ideally constant time complexity. Otherwise, the overall asymptotic time complexity of the search procedure could be increased. Finally, as the environment (search graph) sizes increase, reinforcement learning-based heuristic learning approaches tend to perform poorly [1]. State-of-theart imitation learning-based methods can learn useful search heuristics [1]; however, these methods still rely on feature-engineering for a specific domain and do not generally guarantee a constant time complexity with respect to graph sizes.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Main components of PHIL: On the left, using a greedy mixture policy induced by the current version of our parameterized heuristic $h_{\theta}$ and an oracle heuristic $h^{<em>}$ (i.e., a heuristic that correctly determines distances between nodes), we roll-out a search trajectory from the start node to the goal node. Each trajectory step contains a set of newly added fringe nodes with bounded random subsets of their 1-hop neighborhoods and their oracle $\left(h^{</em>}\right)$ distances to the goal node. Trajectories are aggregated throughout the training procedure. On the right, we use truncated backpropagation through time on each collected trajectory to train $h_{\theta}$, where $\hat{h}$ is the predicted distance between $x_{2}$ and $x_{g}$, and $z_{2}$ is the updated state of the memory. Here, the memory captures the embedding of the graph visited so far.</p>
<p>In this paper, we propose Path Heuristic with Imitation Learning (PHIL), a framework that extends the recent imitation learning-based heuristic search paradigm with a learnable explored graph memory. This means that PHIL learns a representation that allows it to capture the structure of the so far explored graph, so that it can then better select what node to explore next (Figure 2). We train our approach to predict the node-to-goal distances ( $h^{*}$ in Figure 2) of graph nodes during search. To train our memory module, which captures the explored graph, we use truncated backpropagation through time (TBTT) [17], where we utilize ground-truth node-to-goal distances as a supervision signal at each search step. Our TBTT procedure is embedded within an adaptation of the AggreVaTe imitation learning algorithm [18]. PHIL also includes a specialized graph neural network architecture, which allows us to apply PHIL to diverse graphs from different domains.</p>
<p>We evaluate PHIL on standard benchmark heuristic learning datasets (Section 5.1), diverse graphbased datasets from different domains (Section 5.2), and practical UAV flight use cases (Section 5.3). Experiments demonstrate that PHIL outperforms state-of-the-art heuristic learning methods up to $4 \times$. Further, PHIL performs within $4.9 \%$ of an oracle in indoor drone planning scenarios, which is up to a $21.5 \%$ reduction compared with commonly used approaches. In practice, our contributions enable practitioners to quickly extract useful search heuristics from their graph datasets without any hand-engineering.</p>
<h1>2 Preliminaries</h1>
<p>Graph search. Suppose that we are given an unweighted connected graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ is a set of nodes, and $\mathcal{E}$ a corresponding set of edges. Further suppose that each node $i \in \mathcal{V}$ has corresponding features $x_{i} \in \mathbb{R}^{D_{v}}$, and each edge $(i, j) \in \mathcal{E}$ has features $e_{i j} \in \mathbb{R}^{D_{v}}$. Assume that we are also given a start node $v_{s} \in \mathcal{V}$ and a goal node $v_{g} \in \mathcal{V}$. At any stage of our search algorithm, we can partition the nodes of our graph into three sets as $\mathcal{V}=$ CLOSE $\cup$ OPEN $\cup$ REST, where CLOSE are the nodes already explored, OPEN are candidate nodes for exploration (i.e., all nodes connected to any node in CLOSE, but not yet in CLOSE), and REST is the rest of the graph. Each expansion moves a node from OPEN to CLOSE, and adds the neighbors of the given node from REST to OPEN. We call the set of newly added fringe nodes $\mathcal{V}<em s="s">{n e w}$ at each search step. At the start of the search procedure, CLOSE $=\left{v</em> \in$ CLOSE).
Greedy best-first search. We can perform greedy best-first search using a greedy fringe expansion policy, such that we always expand the node $v \in$ OPEN that minimizes $h\left(v, v_{g}\right)$. Here, $h: \mathcal{V} \times \mathcal{V} \longrightarrow$ $\mathbb{R}$ is a tailored heuristic function for a given use case. In our work, we are interested in learning a function $h$ that predicts shortest path lengths, this way minimizing [CLOSE] in a greedy best-first search regime.
Imitation of perfect heuristics. Partially observable Markov decision processes (POMDPs) are a suitable framework to describe the problem of learning search heuristics [1]. We can have $s=$ (CLOSE, OPEN, REST) as our state, an action $a \in \mathcal{A}$ corresponds to moving a node from OPEN to CLOSE, and the observations $o \in \mathcal{O}$ are the features of newly included nodes in OPEN. Note that one could consider an MDP framework to learn heuristics, but the time complexity of operating on the whole state is in most cases prohibitive. We also define a history $\psi \in \Psi$ as a sequence of observations $\psi=o_{1}, o_{2}, o_{3}, \ldots$. Our work leverages the observation that using a heuristic function during greedy best-first search that correctly determines the length of the shortest path between fringe nodes and the goal node will also yield minimal [CLOSE]. For training, we adopt a perfect heuristic $h^{}\right}$ and we expand the nodes until $v_{g}$ is encountered (i.e., until $v_{g<em>}$, similar to [1], which has full information about $s$ during search. Such oracle can provide ground-truth distances $h^{</em>}\left(s, v, v_{g}\right)$, where $v \in$ OPEN. To conclude, we define a greedy best-first search policy $\pi_{\theta}$ that uses a parameterized heuristic $h_{\theta}$ to expand nodes from OPEN with minimal heuristic values. One could also directly use a POMDP solver for the above-described problem, but this approach is usually infeasible due to the dimensionality of the search state [19].</p>
<h2>3 Related Work</h2>
<p>General purpose heuristic design. There has been significant research in designing general-purpose heuristics for speeding up satisficing planning. The first set of approaches are based on simplifying the search problem for example using landmark heuristics [14, 16]. The next set of approaches aim to include novelty-based exploration in greedy best-first search [10-13]. The latter set of approaches</p>
<p>showed state-of-the-art performance (best-first width search [12, 13], BFWS) in numerous settings. We show that in domains where data is available, it can be more effective to incorporate a learned heuristic into a greedy best-first search procedure.</p>
<p>Learning heuristic search. There have been numerous previous works that attempt to learn search heuristics: Arfaee et al. [20] propose to improve heuristics iteratively, Virseda et al. [21] learn to combine heuristics to estimate graph node distances, Wilt et al. [22] and Garrett et al. [23] propose to learn node rankings, Thayer et al. [24] suggest to infer heuristics during a search, and Kim et al. [25] train a neural network to predict graph node distances. These methods generally do not consider the non-i.i.d. nature of heuristic search. Further, Bhardwaj et al. [1] propose SAIL, where heuristic learning is framed as an imitation learning problem with cost-to-go oracles. The SAIL heuristic uses hand-designed features tailored for obstacle avoidance, with a linear time-complexity in the number of explored grid nodes found to be colliding with an obstacle. Feature-engineering becomes more difficult as we attempt to learn heuristics on diverse graphs such as ones seen in Section 5.2, where we may need expert knowledge. Further, heuristics that do not have a constant time complexity in the size of the graph [1, 26-29] generally scale poorly with graph size and hence have constrained use cases. Recent approaches to learning heuristics include Retro* [2] by Chen et al., where a heuristic is learned in the context of AND-OR search trees for chemical retrosynthetic planning. Our work focuses on a more general graph setting.</p>
<p>There has been significant progress on learning heuristics for NP-hard combinatorial optimization problems [30-32]. Focusing on solving NP-hard problems allows these approaches to design algorithms that are often non-exact and have a relatively large computational budget. This is not the case for methods that focus on polynomial time search, where learning-based methods are bounded by the determinism and time complexity of classical algorithms such as greedy best-first search.</p>
<p>Learning general purpose search. Learning general search policies is a very well-studied research area with a rich set of developments and applications. These include Monte Carlo Tree Search methods [33, 34], implicit planning methods [35-37], and imagination-based planning approaches [38, 39]. Learning search heuristics can be seen as a special case of general purpose search, where the search problem is treated as a partially observable Markov decision process with restricted action evaluation (see Section 4), and with models running in $\mathcal{O}(1)$ to remain competitive time-complexitywise on problems where best-first search performs well. General purpose search methods do not take into account the above-mentioned constraints, which motivates the development of tailored approaches for learning heuristics [1, 2].
Imitation learning. Our approach builds on prior work in imitation learning (IL) with cost-to-go oracles. Cost-to-go oracles have been incorporated in the context of IL in methods such as SEARN [40], AggreVaTe [18], LOLS [41], AggrevaTeD [42], DART [43], and THOR [44]. SAIL [1] presents an AggreVaTe-based algorithm for learning heuristic search. We extend SAIL by incorporating a recurrent $Q$-like function, in which sense our algorithm more closely resembles AggreVaTeD by Sun et al. [42]. While a recurrent policy can be easily incorporated in AggreVaTeD, we cannot use a policy to evaluate actions. This is due to the fact that we would either have to evaluate all actions in a state, which is computationally infeasible, or we would have to give up on taking actions that are not in the most recent version of the search fringe, which would degrade the performance (see Section 4).</p>
<h1>4 Path Heuristic with Imitation Learning</h1>
<p>Training objective. With the aim of minimizing $|\mathrm{CLOSE}|$ after search, our goal is to train a parameterized heuristic function $h_{\theta}: \Psi \times \mathcal{V} \times \mathcal{V} \longrightarrow \mathbb{R}$ to predict ground-truth node distances $h^{*}$ and use $h_{\theta}$ within a greedy best-first policy $\pi_{\theta}$ at test time. More specifically, we assume access to a distribution over graphs $P_{\mathcal{G}}$, a start-goal node distribution $P_{v_{s g}}(\cdot \mid \mathcal{G})$, and a time horizon $T$. Moreover, we assume a joint state-history distribution $s, \psi \sim P_{s}\left(\cdot \mid \mathcal{G}, t, \pi_{\theta}, v_{s}, v_{g}\right)$, where $P_{s}$ represents the probability our search being in state $s$, at time $0 \leq t \leq T$ on graph $\mathcal{G}$ with pathfinding problem $\left(v_{s}, v_{g}\right)$, with a greedy best-first search policy $\pi_{\theta}$ using heuristic $h_{\theta}$. Hence, our goal can be summarized as minimizing the following objective:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">PHIL</span><span class="err"></span><span class="w"> </span><span class="nt">Sequential</span><span class="w"> </span><span class="nt">Heuristic</span><span class="w"> </span><span class="nt">Training</span>
<span class="nt">Obtain</span><span class="w"> </span><span class="nt">hyperparameters</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">beta_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">N</span><span class="o">,</span><span class="w"> </span><span class="nt">m</span><span class="o">,</span><span class="w"> </span><span class="nt">t_</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="nt">Initialize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">emptyset</span><span class="o">,</span><span class="w"> </span><span class="nt">h_</span><span class="p">{</span><span class="err">\theta_{1</span><span class="p">}</span><span class="err">}\</span><span class="o">);</span>
<span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">i</span><span class="o">=</span><span class="nt">1</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">ldots</span><span class="o">,</span><span class="w"> </span><span class="nt">N</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">    </span><span class="nt">Sample</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">G</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="nt">P_</span><span class="p">{</span><span class="err">\mathcal{G</span><span class="p">}</span><span class="err">}\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">Sample</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">v_</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">v_</span><span class="p">{</span><span class="err">g</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="nt">P_</span><span class="p">{</span><span class="err">v_{s</span><span class="w"> </span><span class="err">g</span><span class="p">}</span><span class="err">}\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">Set</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">beta</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">beta_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">*</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">Set</span><span class="w"> </span><span class="nt">mixture</span><span class="w"> </span><span class="nt">policy</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{mix</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="o">(</span><span class="nt">1-</span><span class="err">\</span><span class="nt">beta</span><span class="o">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\theta_{i</span><span class="p">}</span><span class="err">}</span><span class="o">+</span><span class="err">\</span><span class="nt">beta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="err">\</span><span class="nt">pi</span><span class="o">^</span><span class="p">{</span><span class="err">*</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">Collect</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">m</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">trajectories</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tau_</span><span class="p">{</span><span class="err">i</span><span class="w"> </span><span class="err">j</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">as</span><span class="w"> </span><span class="nt">follows</span><span class="o">;</span>
<span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">j</span><span class="o">=</span><span class="nt">1</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">ldots</span><span class="o">,</span><span class="w"> </span><span class="nt">m</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">    </span><span class="nt">Sample</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">U</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">0</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">ldots</span><span class="o">,</span><span class="w"> </span><span class="nt">T-t_</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">Roll-in</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">time</span><span class="w"> </span><span class="nt">steps</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\theta_{i</span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">obtain</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">z_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">new</span><span class="w"> </span><span class="nt">state</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">right</span><span class="o">.</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">CLOSE</span><span class="w"> </span><span class="err">\</span><span class="o">(^</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="nt">OPEN</span><span class="w"> </span><span class="err">\</span><span class="o">(^</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">REST</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="nt">Roll-out</span><span class="w"> </span><span class="nt">trajectory</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tau_</span><span class="p">{</span><span class="err">i</span><span class="w"> </span><span class="err">j</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">as</span><span class="w"> </span><span class="nt">follows</span><span class="o">;</span>
<span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">k</span><span class="o">=</span><span class="nt">1</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">ldots</span><span class="o">,</span><span class="w"> </span><span class="nt">t_</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">Update</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">t+k-1</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">using</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{mix</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">get</span><span class="w"> </span><span class="nt">new</span><span class="w"> </span><span class="nt">state</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">t+k</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">new</span><span class="w"> </span><span class="nt">fringe</span><span class="w"> </span><span class="nt">state</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">OPEN</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">Obtain</span><span class="w"> </span><span class="nt">new</span><span class="w"> </span><span class="nt">fringe</span><span class="w"> </span><span class="nt">nodes</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">V</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{new</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">OPEN</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">backslash</span><span class="w"> </span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">OPEN</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">k-1</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">Update</span><span class="w"> </span><span class="nt">trajectory</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tau_</span><span class="p">{</span><span class="err">i</span><span class="w"> </span><span class="err">j</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">tau_</span><span class="p">{</span><span class="err">i</span><span class="w"> </span><span class="err">j</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cup</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">\left(\mathcal{V</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{new</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="nt">h</span><span class="o">^</span><span class="p">{</span><span class="err">*</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">t+k</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">V</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{new</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="nt">v_</span><span class="p">{</span><span class="err">g</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="err">\}\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">Update</span><span class="w"> </span><span class="nt">dataset</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cup</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">\left(\tau_{i</span><span class="w"> </span><span class="err">j</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">z_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="err">\}\</span><span class="o">)</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cup</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">\left(\tau_{i</span><span class="w"> </span><span class="err">j</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">0</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="err">\}\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">Train</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">h_</span><span class="p">{</span><span class="err">\theta_{i</span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">using</span><span class="w"> </span><span class="nt">TBTT</span><span class="w"> </span><span class="nt">on</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tau</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">get</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">h_</span><span class="p">{</span><span class="err">\theta_{i+1</span><span class="p">}</span><span class="err">}\</span><span class="o">);</span>
<span class="nt">return</span><span class="w"> </span><span class="nt">best</span><span class="w"> </span><span class="nt">performing</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">h_</span><span class="p">{</span><span class="err">\theta_{i</span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">on</span><span class="w"> </span><span class="nt">validation</span><span class="o">;</span>
</code></pre></div>

<p>$$
\mathcal{L}(\theta)=\underset{\substack{\mathcal{G} \sim P_{\mathcal{G}},}\left(\begin{array}{l}
1 \
\left(v_{s}, v_{g}\right) \sim P_{v_{s g}}\end{array}\right.}{\left(v_{s}, v_{g}\right) \sim P_{v_{s g}}} \quad\left[\frac{1}{|\mathrm{OPEN}|} \sum_{v \in \mathrm{OPEN}}\left(h^{*}\left(s, v, v_{g}\right)-h_{\theta}\left(\psi, v, v_{g}\right)\right)^{2}\right]
$$</p>
<p>Before we describe the algorithm that can be used to minimize $\mathcal{L}$, we rewrite $h_{\theta}$ to include a memory digest component $\left(z_{t}\right)$, which represents an embedding of $\psi$ at time step $t$. Hence, $h_{\theta}$ becomes $h_{\theta}: \mathbb{R}^{d} \times \mathcal{O} \times \mathcal{V} \times \mathcal{V} \longrightarrow \mathbb{R}$, where $d$ is the dimensionality of our memory's embedding space. As opposed to previous methods [1], $z_{t}$ allows us to automatically extract relevant features for heuristic computations and concurrently reduce the computational complexity of the heuristic function. Further, as shown in [1], if we would use $h_{\theta}$ to evaluate all actions in a state (i.e., recalculate the heuristic values of all nodes in OPEN), we would need a squared reduction in the number of expanded nodes compared with BFS for PHIL to bring performance benefits over BFS, which however may not be possible on all datasets. Hence, we constrain the heuristic only to evaluate new OPEN nodes which we obtain after moving a node to CLOSE, calling the set of new fringe nodes $\mathcal{V}<em _theta="\theta">{\text {new }}$ after each expansion. In practice, the policy $\pi</em>$.}$ yields an algorithm equivalent to greedy best-first search, with the heuristic function replaced by $h_{\theta</p>
<h1>4.1 Learning algorithm \&amp; architecture</h1>
<p>Imitation learning algorithm. In Algorithm 1, we present the pseudo-code of the IL algorithm used to train our heuristic models (Figure 3). The high-level idea of our algorithm is that we aggregate trajectories of search traces (i.e., sequences of new fringe nodes) and use truncated backpropagation through time to optimize $h_{\theta}$ after each data-collection step. In particular, after sampling a graph $\mathcal{G}$ and a search problem $v_{s}, v_{g}$, we use our greedy learned policy $\pi_{\theta}$ induced by $h_{\theta}$ to roll-in for $t \sim \mathcal{U}\left(0, \ldots, T-t_{\tau}\right)$ expansions, where $T$ is the episode time horizon, and $t_{\tau}$ is the roll-out length. From our roll-in, we obtain a new state $s=\left(\right.$ CLOSE $\left.^{0}, \mathrm{OPEN}^{0}, \mathrm{REST}^{0}\right)$, and an initial memory state $z_{t}$. After our roll-in, we roll-out for $t_{\tau}$ steps using our mixture policy $\pi_{\text {mix }}$, which is obtained by probabilistically blending $\pi_{\theta}$ and the greedy best-first policy induced by the oracle heuristic $\pi^{<em>}$. In a roll-out, we collect sequences of new fringe nodes, together with their ground-truth distances to the goal $v_{g}$, given by $h^{</em>}$. Once the roll-out is complete, we append the obtained trajectory and the initial state for the following optimization using backpropagation through time. Further analysis on the trade-offs between using rolled-in states $z_{t}$ or zeroed-out states for training can be found in the supplementary material.</p>
<p>Note that we could also use supervised learning-based approaches to sample a fixed dataset of ( $v_{s}$, $\left.v_{g}, h^{*}\left(s, v_{s}, v_{g}\right)\right)$ 3-tuples and train a model to predict node distances conditioned on their features. However, our experiments in Section 5 demonstrate that ignoring the non-i.i.d. nature of heuristic search negatively impacts model performance, with supervised learning-based methods performing up to $40 \times$ worse.</p>
<p>Roll-out for $t_{i}$ steps using $\pi_{\text {mix }}$ to collect $\tau$ (green)
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: This figure demonstrates the core idea behind our IL algorithm. We present the roll-in phase on the left-hand side, where our policy is rolled in for $t$ steps to obtain state $s_{t}$ and embedding $z_{t}$. On the right-hand side, we show the trajectory collection and training steps, where we aggregate the trajectory for downstream training (green) and use truncated backpropagation through time on the collected dataset (red).</p>
<p>Recurrent GNN architecture. In each forward pass, $h_{\theta}$ obtains a set of new fringe nodes $\mathcal{V}<em g="g">{\text {new }}$, the goal node $v</em>}$, and the memory $z_{t}$ at time step $t$. We represent each node in $\mathcal{V<em i="i">{\text {new }}$ using its features $x</em>} \in \mathbb{R}^{D_{v}}$, and likewise the goal node $v_{g}$ using its features $x_{g} \in \mathbb{R}^{D_{v}}$. Further, for each $i \in \mathcal{V<em 0="0" _geq="\geq">{\text {new }}$, we uniformly sample an $n \in \mathbb{N}</em>}$ bounded set of nodes present in the 1-hop neighborhood of $i$, calling this set $\mathcal{N<em i="i">{i}$, with $|\mathcal{N}</em>| \leq n$. This sampling step produces a set of neighboring node features,</p>
<p>Algorithm 2: Heuristic func. $\left(h_{\theta}\right)$ forward pass
Obtain $x_{i}, x_{j}\left(j \in \mathcal{N}<em i="i" j="j">{i}\right), e</em>$;
$x_{i} \leftarrow f\left(x_{i}, x_{g}, D_{E U C}\left(x_{i}, x_{g}\right), D_{C O S}\left(x_{i}, x_{g}\right)\right)$;
$x_{j} \leftarrow f\left(x_{j}, x_{g}, D_{E U C}\left(x_{j}, x_{g}\right), D_{C O S}\left(x_{j}, x_{g}\right)\right)$;
$g_{i} \leftarrow \phi\left(x_{i}, \oplus_{j \in \mathcal{N}}, x_{g} z_{t<em i="i">{i}} \gamma\left(x</em>\right)\right)$;
$g_{i}^{\prime}, z_{i, t+1} \leftarrow \operatorname{GRU}\left(g_{i}, z_{t}\right) ;$
$z_{t+1} \leftarrow \overline{z_{i, t+1}} ;$
$\hat{h}}, x_{j}, e_{i j<em i="i">{i} \leftarrow \operatorname{MLP}\left(g</em>\right) ;$
return $\hat{h}}^{\prime}, x_{g<em t_1="t+1">{i}, z</em> ;$
where each $j \in \mathcal{N}<em j="j">{i}$ has features $x</em>$.
$h_{\theta}$ forward pass. Algorithm 2 presents a single forward pass of $h_{\theta}$. The forward pass outputs predicted distances of the new fringe nodes to the goal $\hat{h}} \in \mathbb{R}^{D_{v}}$, and corresponding edge features $e_{i j} \in \mathbb{R}^{D_{v}<em t_1="t+1">{i}$, together with an updated memory digest $z</em>$. In Algorithm 2, $f, \phi, \gamma$, GRU[45], MLP are each parameterised differentiable functions, with $\phi, \gamma$ representing the update and message functions [46] of a graph neural network, respectively.
In our forward pass, using the function $f$, we first project $x_{i}, x_{j}$ into a node embedding space, together with the goal features $x_{g}$, and their Euclidean $\left(D_{E U C}\right)$ and cosine distances $\left(D_{C O S}\right)$. After that, using a 1-layer GNN, we perform a single convolution over each $x_{i}$ and the corresponding neighborhood $\mathcal{N}<em i="i">{i}$, to obtain $g</em>}$. The specific GNN choice is a design decision left to the practitioner, and further analysis of GNN choices can be found in Appendix D. Our graph convolution processing step allows us to easily incorporate edge features and work with variable sizes of $\mathcal{N<em i="i">{i}$. After the graph convolution, we apply the GRU module over each embedding $g</em>}$ to obtain hidden states $z_{i, t+1}$, and new embeddings $g_{i}^{\prime}$. We compute the sample mean of $z_{i, t+1}$ for each node $i \in \mathcal{V<em t_1="t+1">{\text {new }}$ to obtain a new hidden state $z</em>$ using an MLP to compute the distances between the graph nodes.}$, and process $g_{i}^{\prime}$ with $x_{g</p>
<p>The intuitive explanation for using history embeddings is to address the partial observability of the problem. PHIL does not have access to the full graph because of time complexity concerns. The history embeddings provide a mechanism for keeping track of the belief over the full state of the graph during search. Further, the GNN allows for easily incorporating local neighborhoods and edge features. Please refer to Appendix D where we discuss the effects of our design choices.</p>
<p>Permutation invariant $\mathcal{V}<em t="t">{\text {new }}$ embedding. There is a trade-off between processing new fringe nodes in batch, as in Algorithm 2, and processing them sequentially. Namely, when we process the nodes in batch, we do not use the in-batch observations to predict batch node values, which means that $z</em>}$ is slightly outdated. On the other hand, in PHIL, batch processing allows us to compute the heuristic values of all $v \in \mathcal{V<em _new="{new" _text="\text">{\text {new }}$ in parallel on a GPU and preserves the memory's permutation invariance with respect to nodes in $\mathcal{V}</em>}}$. That is, because our observations are nodes \&amp; edges of a graph, the respective observation ordering usually does not contain inductive biases useful for predictions, which means that we can apply a permutation invariant operator such as the mean of all new states $z_{i, t+1}$ to obtain an aggregated updated state. This approach provides additional scalability as we can process values in parallel and PHIL does not have to infer permutation invariance in $\mathcal{V<em _new="{new" _text="\text">{\text {new }}$ from data.
Runtime complexity. Since $\forall i \in \mathcal{V}</em>}}:\left|\mathcal{N<em 1="1">{i}\right| \leq n$, Algorithm 2 together with neighborhood sampling runs in up to $n c</em>}+(n+1) c_{2}$ operations per each node $i \in \mathcal{V<em 1="1">{\text {new }}$, which is $\mathcal{O}(1)$ with respect to the size of the graph. Here, $c</em>}$ is the maximal number of operations associated with evaluating a node, such as performing robot collision checks in dynamically constructed graphs, and $c_{2}$ is the maximal count of total model operations (e.g., $f \&amp; \gamma$ operations) on the node set ${i} \cup \mathcal{N<em 1="1">{i}$. Note that for this analysis, we assume that $c</em>$ may dominate overall complexity, which means the hyperparameter $n$ is helpful for practitioners to tune trade-offs between constant factors and search effort minimization.}$ is bounded. In general, we expect to learn a better search heuristic with increasing $n$, but in some use cases, $c_{1</p>
<h1>5 Experiments</h1>
<p>In our experiments, we evaluate PHIL both on benchmark heuristic learning datasets [1] (Section 5.1) as well on a diverse set of graph datasets (Section 5.2). Finally, we show that PHIL can be applied to efficient planning in the context of drone flight (Section 5.3). Our main goal is to assess how PHIL compares to baseline methods in terms of necessary expansions before the goal node is reached. Please refer to the supplementary material for information about baselines, an ablation study, and additional experiment details.</p>
<h3>5.1 Heuristic search in grids</h3>
<p>In Section 5.1, we evaluate PHIL on 8, $200 \times 200$ 8-connected grid graph-based datasets by Bhardwaj et al. [1]. These datasets present challenging obstacle configurations for naive greedy planning heuristics, especially when $v_{s}$ is in the bottom-left of the grid, and $v_{g}$ in the top-right. Each dataset contains 200 training graphs, 70 validation graphs, and 100 test graphs. Example graphs from each dataset can be found in Table 1.
We train PHIL with a hyperparameter configuration of $T=128$, $t_{\tau}=32, \beta_{0}=0.7, n=8$, and using rolled-in $z_{t}$ states as initial states for training. We use a 3-layer MLP of width 128 with LeakyReLU activations, followed by a DeeperGCN [47] graph convolution with softmax aggregation. Our memory's embedding dimensionality is 64 . The node features are 2D grid coordinates. See Appendix C for an overview of our baselines and datasets. Note that although using positional representations [48] might be useful for this search problem, it is not scalable as the graph size increases (we refer the reader to Appendix A for more details).
Discussion. As we can see in Table 1, PHIL outperforms the best baseline (SAIL) on all datasets, with an average reduction of explored nodes before $v_{g}$ is found of $58.5 \%$. Qualitatively, observing Figure 5, we can attribute these results to PHIL's ability to reduce the redundancy in explored nodes during a search. Further, PHIL is also capable of escaping local minima, which is illustrated in Figure 4. However, note that we occasionally observe failure cases in practice, where PHIL gets stuck in a bug trap-like structure. We discuss possible remedies and opportunities for future work in the supplementary material.
Runtime \&amp; convergence speed. PHIL converges in up to $N=36$ iterations, with $m=1, t_{\tau}=32$ (i.e., after observing less than $N * t_{\tau} * \max \left(\left|\mathcal{V}_{\text {new }}\right|\right) \approx 9,216$ shortest path distances, where we</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Graph Examples</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SAIL</th>
<th style="text-align: center;">SL</th>
<th style="text-align: center;">CEM</th>
<th style="text-align: center;">QL</th>
<th style="text-align: center;">$h_{\text {max }}$</th>
<th style="text-align: center;">$h_{\text {mean }}$</th>
<th style="text-align: center;">A*</th>
<th style="text-align: center;">MHA*</th>
<th style="text-align: center;">BFWS</th>
<th style="text-align: center;">Neural A*</th>
<th style="text-align: center;">PHIL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Alternating gaps</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.039</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.546</td>
<td style="text-align: center;">0.024</td>
</tr>
<tr>
<td style="text-align: center;">Single Bugtrap</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.158</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.057</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.099</td>
<td style="text-align: center;">0.394</td>
</tr>
<tr>
<td style="text-align: center;">Shifting gaps</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.104</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.589</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.804</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.563</td>
</tr>
<tr>
<td style="text-align: center;">Forest</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.043</td>
<td style="text-align: center;">0.048</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">0.041</td>
<td style="text-align: center;">0.043</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.075</td>
<td style="text-align: center;">0.039</td>
<td style="text-align: center;">0.399</td>
</tr>
<tr>
<td style="text-align: center;">Bugtrap+Forest</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.384</td>
<td style="text-align: center;">0.182</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">3.177</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.651</td>
</tr>
<tr>
<td style="text-align: center;">Gaps+Forest</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">0.580</td>
</tr>
<tr>
<td style="text-align: center;">Mazes</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.171</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.095</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">Multiple Bugtraps</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.648</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.331</td>
</tr>
</tbody>
</table>
<p>Table 1: The number of expanded graph nodes of PHIL with respect to SAIL. We can observe that out of all baselines, SAIL performs best. PHIL outperforms SAIL by $58.5 \%$ on average over all datasets, with a maximal search effort reduction of $82.3 \%$ in the Gaps+Forest dataset.
take $\max \left(\left|\mathcal{V}<em _new="{new" _text="\text">{\text {new }}\right|\right)=8$ as the maximal size of $\mathcal{V}</em>$ ). According to figures reported in [1], this is approximately $5 \times$ less data than it takes for SAIL to converge.
}<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: In each image pair of this figure, we provide a qualitative comparison with the SAIL method. In particular, we show comparisons on the Shifting gaps, Gaps+Forest, Mazes, and Forest datasets. We can observe that PHIL (right) learns the appropriate heuristics for the given dataset and makes fewer redundant expansions than SAIL (left).</p>
<h1>5.2 Search in real-life graphs of different structures</h1>
<p>In this experiment, our goal is to demonstrate the general applicability of PHIL to various graphs. We train PHIL on 4 different groups of graph datasets: citation networks, biological networks, abstract syntax trees (ASTs), and road networks. We use the same graph for citation networks and road networks for training and evaluation, and we use 100 random $v_{s}, v_{g}$ pairs for testing. In the case of biological networks and ASTs, we usually have train/validation/test splits of $80 / 10 / 10$, and in the case of the OGB [49] datasets, we use the provided splits.
Similarly as in Section 5.1, our MLP has four layers of width 128 with LeakyReLU activations and we use a DeeperGCN [47] graph convolution with softmax aggregation. The utilized node and edge features are the provided features in each dataset, except for a few minor modifications which are discussed in Appendix A \&amp; Appendix C. We train an MLP of depth 5 and width 256 using supervised learning (SL) for our learning-based baseline method.
Discussion. The results presented in Table 2 suggest that PHIL can learn superior search heuristics compared with baseline methods, outperforming top baselines per dataset in terms of visited nodes</p>
<p>| | Dataset | $|\mathcal{D}|$ | $|\mathcal{V}|$ | $|\mathcal{E}|$ | SL | A* | $h_{\text {euc }}$ | BFS | S+IL | BFWS | PHIL |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Citation Networks | Cora (Sen et al. [50]) | 1 | 2,708 | 5,429 | 2.201 | 2.067 | 1.000 | 4.001 | 6.669 | 1.378 | 0.475 |
| | PubMed (Sen et al. [50])) | 1 | 19,717 | 44,338 | 2.157 | 2.983 | 1.000 | 3.853 | 1.196 | 1.000 | 0.745 |
| | CiteSeer (Sen et al. [50])) | 1 | 3,327 | 4,732 | 1.636 | 1.487 | 1.000 | 2.190 | 1.062 | 0.951 | 0.599 |
| | Coauthor (cs) (Schur et al. [51]) | 1 | 18,333 | 81,894 | 1.571 | 1.069 | 1.000 | 2.820 | 1.941 | 1.026 | 0.835 |
| | Coauthor (physics) (Schur et al. [51]) | 1 | 34,493 | 247,962 | 4.076 | 1.081 | 1.000 | 4.523 | - | 1.012 | 0.964 |
| Biological Networks | OGBG-Molhiv (Hu et al. [49]) | 41,127 | 25.5 | 27.5 | 1.086 | 1.065 | 1.000 | 1.267 | 1.104 | 1.146 | 1.016 |
| | PPf (Zinnik et al. [52]) | 24 | 2,372.67 | 34,113.16 | 0.772 | 0.831 | 1.000 | 5.618 | 1.746 | 3.941 | 0.658 |
| | Proteins (Full) (Morris et al. [53]) | 1,113 | 39.66 | 72.82 | 0.995 | 0.997 | 1.000 | 2.645 | 0.891 | 0.966 | 0.831 |
| | Enzymes (Morris et al. [53]) | 600 | 32.63 | 62.14 | 1.073 | 1.007 | 1.000 | 1.358 | 1.036 | 0.992 | 0.757 |
| ASTs | OGBG-Code2 (Hu et al. [49]) | 452,741 | 125.2 | 124.2 | 1.196 | 1.013 | 1.000 | 1.267 | 1.029 | 0.817 | 1.219 |
| Road Networks | OSMux - Modena (Boeing [54]) | 1 | 29,324 | 38,309 | 2.904 | 3.085 | 1.000 | 3.493 | 1.182 | 0.997 | 0.489 |
| | OSMux - New York (Boeing [54]) | 1 | 54,128 | 89,618 | 39.424 | 36.529 | 1.000 | 63.352 | 1.583 | 1.013 | 0.962 |</p>
<p>Table 2: Comparison of PHIL with baseline approaches on 4 groups of datasets: citation networks, biological networks, abstract syntax trees, and road networks. "-" denotes being out of a 4-day's training time limit. We can observe that, on average across all datasets, PHIL outperforms the best baseline per dataset by $13.4 \%$. Discounting the OGBG datasets, this number becomes $19.5 \%$.
during a search by $13.4 \%$ on average. Two datasets where PHIL fell short compared to other baselines are the OGBG-Molhiv and OGBG-Code2 datasets. The OGBG-Code2 dataset adopts a project split [55] and OGBG-Mohliv adopts a scaffold split [56], both of which ensure that graphs of different structure are present in the training \&amp; test sets. Although PHIL improved upon uninformed search (BFS) in the OGB datasets, structural graph consistency is explicitly discouraged in the above-mentioned OGBG splits. Without the OGBG datasets, PHIL improves on the top baselines per dataset by $19.5 \%$ on average, and upon the Euclidean node feature heuristic ( $h_{\text {euc }}$ ) by $20.4 \%$. Note that we trained PHIL up to $N=60$ iterations, which means that it only encountered a small subset of the pathfinding problems in the single graph setting, which means that PHIL had to generalize to learn useful heuristics. Even in Cora, the $|\mathcal{D}|=1$ dataset with least number of nodes, PHIL observed roughly 6,000 node distances during training, which is less than $0.2 \%$ of total distances in the Cora graph.</p>
<h1>5.3 Planning for drone flight</h1>
<p>In our final experiment, we use PHIL to plan collisionfree paths in a practical drone flight use case within an indoor environment. We built our environment using the CoppeliaSim simulator [57], and the Ivy framework [58]. Figure 6 presents the environment which we refer to as room adversarial in Table 3. For more detail about each environment, please refer to the supplementary material. We discretize the environments into 3D grid graphs of size $50 \times 50 \times 25$, and randomly remove 5 sub-graphs of size $5 \times$ $5 \times 5$ both during training and testing, this way simulating real-life planning scenarios with random obstacles. The hyperparameter configuration and the specific architecture we utilize are equivalent to Section 5.1, but with $n=4$. Likewise, the node features are 3D grid coordinates, and the baselines include supervised learning (SL), $h_{\text {euc }}, \mathrm{A}^{*}$, and BFS, similarly as in Sections 5.1, 5.2. In Table 3 we report the ratio of expanded nodes with respect to $h_{\text {euc }}$.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: This figure illustrates the room adversarial environment with an example planning problem (red) and the expanded graph by PHIL (blue).</p>
<p>Video demo. We provide a video demonstration of PHIL running in room adversarial: https: //cutt.ly/eniu5ax.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">SL</th>
<th style="text-align: center;">A*</th>
<th style="text-align: center;">$h_{\text {euc }}$</th>
<th style="text-align: center;">BFS</th>
<th style="text-align: center;">S+IL</th>
<th style="text-align: center;">BFWS</th>
<th style="text-align: center;">PHIL</th>
<th style="text-align: center;">Shortest path</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Room simple</td>
<td style="text-align: center;">1.124</td>
<td style="text-align: center;">76.052</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">291.888</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">1.286</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.782</td>
</tr>
<tr>
<td style="text-align: center;">Room adversarial</td>
<td style="text-align: center;">2.022</td>
<td style="text-align: center;">67.215</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">238.768</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">1.583</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.853</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of PHIL in the context of planning for indoor UAV flight. PHIL outperforms all baselines in both the room simple and room adversarial environments while remaining close performance-wise to the optimal number of expansions.</p>
<p>Discussion. As we can observe in Table 3, PHIL outperforms all baselines in both environments. Interestingly, PHIL expands only approximately $0.3 \%$ more nodes in the simple room than least possible and $4.9 \%$ more in the adversarial room case. The same figures for the greedy method ( $h_{\text {euc }}$ ) are $27.8 \%$ and $17.2 \%$, respectively. These results indicate that PHIL is capable of learning planning strategies that are close to optimal in both simple and adversarial graphs, while the performance of naive heuristics degrades.</p>
<h1>5.4 Runtime Analysis</h1>
<p>We summarize test run-times of different approaches in Appendix G. PHIL runs $57.9 \%$ faster than BFWS and $32.2 \%$ faster than SAIL, and not much slower than traditional A<em> ( $34.7 \%$ ) and $h_{\text {man }}$ $(18.3 \%)$. Although Neural A</em> is $71.0 \%$ faster than PHIL due to the fact that it casts the whole search process into matrix operations on images, it cannot be employed in a generic search setting.</p>
<h2>6 Conclusion</h2>
<p>We consider the problem of learning to search for feasible paths in graphs efficiently. We propose a model and a training procedure to learn search heuristics that can be easily deployed across diverse graphs, with tuneable trade-off parameters between constant factors and performance. Our results demonstrate that PHIL outperforms current state-of-the-art approaches and can be applied to various graphs with practical use cases.</p>
<h2>References</h2>
<p>[1] Mohak Bhardwaj, Sanjiban Choudhury, and Sebastian Scherer. Learning heuristic search via imitation. In Conference on Robot Learning, 2017. 1, 2, 3, 4, 5, 7, 8, 14, 15, 16, 17, 20, 23
[2] Binghong Chen, Chengtao Li, Hanjun Dai, and Le Song. Retro<em>: Learning retrosynthetic planning with neural guided a</em> search. In ICML, 2020. 1, 4
[3] Martin Gebser, Benjamin Kaufmann, Javier Romero, Ramn Otero, Torsten Schaub, and Philipp Wanko. Domain-specific heuristics in answer set programming. In AAAI, 2013. 1
[4] Thi Thoa Mac, Cosmin Copot, Duc Trung Tran, and Robin De Keyser. Heuristic approaches in robot path planning: A survey. In Robotics and Autonomous Systems, 2016. 1
[5] Abhishek Sharma and Keith M. Goolsbey. Identifying useful inference paths in large commonsense knowledge bases by retrograde analysis. In AAAI, 2017. 1
[6] Cheng-Yu Yeh, Hsiang-Yuan Yeh, Carlos Roberto Arias, and Von-Wun Soo. Pathway detection from protein interaction networks and gene expression data using color-coding methods and a<em> search algorithms. In The Scientific World booktitle, 2012. 1
[7] Judea Pearl. Heuristics: intelligent search strategies for computer problem solving. 1984. 1
[8] Danish Khalidi, Dhaval Gujarathi, and Indranil Saha. T</em>: A heuristic search based path planning algorithm for temporal logic specifications. In ICRA, 2020. 2
[9] Bhargav Adabala and Zlatan Ajanovic. A multi-heuristic search-based motion planning for autonomous parking. In 30th International Conference on Automated Planning and Scheduling: Planning and Robotics Workshop, 2020. 2
[10] Fan Xie, Hootan Nakhost, and Martin Mller. Planning via random walk-driven local search. In Twenty-Second International Conference on Automated Planning and Scheduling, 2012. 2, 3
[11] Fan Xie, Martin Mller, and Robert Holte. Adding local exploration to greedy best-first search in satisficing planning. In AAAI, 2014.
[12] Nir Lipovetzky and Hector Geffner. Best-first width search: Exploration and exploitation in classical planning. In AAAI, 2017. 4, 15
[13] Florent Teichteil-Knigsbuch, Miquel Ramirez, and Nir Lipovetzky. Boundary extension features for width-based planning with simulators on continuous-state domains. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, 2021. 2, 3, 4, 15</p>
<p>[14] Blai Bonet and Hctor Geffner. Planning as heuristic search. pages 5-33. 2, 3
[15] Lin Zhu and Robert Givan. Landmark extraction via planning graph propagation. 2003.
[16] Silvia Richter and Matthias Westphal. The lama planner: Guiding cost-based anytime planning with landmarks. 2010. 2, 3
[17] Ilya Sutskever. Training recurrent neural networks. University of Toronto, Toronto, Canada, 2013. 3
[18] Stephane Ross and J Andrew Bagnell. Reinforcement and imitation learning via interactive no-regret learning. In arXiv preprint arXiv:1406.5979, 2014. 3, 4
[19] Sanjiban Choudhury, Ashish Kapoor, Gireeja Ranade, Sebastian Scherer, and Debadeepta Dey. Adaptive information gathering via imitation learning. 2017. 3
[20] Shahab Jabbari Arfaee, Sandra Zilles, and Robert C Holte. Learning heuristic functions for large state spaces. In Artificial Intelligence, 2011. 4
[21] Jes s Virseda, Daniel Borrajo, and Vidal Alczar. Learning heuristic functions for cost-based planning. In Planning and Learning, 2013. 4
[22] Christopher Makoto Wilt and Wheeler Ruml. Building a heuristic for greedy search. In SOCS, 2015. 4
[23] Caelan Reed Garrett, Leslie Pack Kaelbling, and Toms Lozano-Prez. Learning to rank for synthesizing planning heuristics. In IJCAI, 2016. 4
[24] Jordan Thayer, Austin Dionne, and Wheeler Ruml. Learning inadmissible heuristics during search. In Proceedings of the International Conference on Automated Planning and Scheduling, 2011. 4
[25] Soonkyum Kim and Byungchul An. Learning heuristic a<em>: efficient graph search using neural network. In ICRA, 2020. 4
[26] Yuka Ariki and Takuya Narihira. Fully convolutional search heuristic learning for rapid path planners. In arXiv preprint arXiv:1908.03343, 2019. 4
[27] Ryo Terasawa, Yuka Ariki, Takuya Narihira, Toshimitsu Tsuboi, and Kenichiro Nagasaka. 3d-cnn based heuristic guided task-space planner for faster motion planning. In ICRA, 2020.
[28] Ryo Yonetani, Tatsunori Taniai, Mohammadamin Barekatain, Mai Nishimura, and Asako Kanezaki. Path planning using neural a</em> search. In ICML, 2021. 15
[29] Alberto Archetti, Marco Cannici, and Matteo Matteucci. Neural weighted a*: Learning graph costs and heuristics with differentiable anytime a. 2021. 4
[30] Elias B. Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. In NeurIPS, 2017. 4
[31] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. In NeurIPS, 2018.
[32] Nikolaos Karalias and Andreas Loukas. Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs. In NeurIPS, 2020. 4
[33] David Silver and Joel Veness. Monte-carlo planning in large pomdps. In NeurIPS, 2010. 4
[34] Arthur Guez, Theophane Weber, Ioannis Antonoglou, Karen Simonyan, Oriol Vinyals, Daan Wierstra, Rmi Munos, and David Silver. Learning to search with mctsnets. In ICML, 2018. 4
[35] Andreea Deac, Petar Velikovi, Ognjen Milinkovi, Pierre-Luc Bacon, Jian Tang, and Mladen Nikoli. Xlvin: executed latent value iteration nets. In arXiv preprint arXiv:2010.13146, 2020. 4
[36] Pter Karkus, David Hsu, and Wee Sun Lee. Qmdp-net: Deep learning for planning under partial observability. In NeurIPS, 2017.
[37] Aviv Tamar, Sergey Levine, Pieter Abbeel, Yi Wu, and Garrett Thomas. Value iteration networks. In NeurIPS, 2016. 4
[38] Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing, Sebastien Racanire, David Reichert, Thophane Weber, Daan Wierstra, and Peter Battaglia. Learning model-based planning from scratch. In arXiv preprint arXiv:1707.06170, 2017. 4</p>
<p>[39] Sbastien Racanire, Theophane Weber, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adri Puigdomnech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter W. Battaglia, Demis Hassabis, David Silver, and Daan Wierstra. Imagination-augmented agents for deep reinforcement learning. In NeurIPS, 2017. 4
[40] Hal Daum, John Langford, and Daniel Marcu. Search-based structured prediction. In Machine learning, 2009. 4
[41] Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daum III, and John Langford. Learning to search better than your teacher. In ICML, 2015. 4
[42] Wen Sun, Arun Venkatraman, Geoffrey J. Gordon, Byron Boots, and J. Andrew Bagnell. Deeply aggrevated: differentiable imitation learning for sequential prediction. In ICML, 2017. 4
[43] Michael Laskey, Jonathan Lee, Roy Fox, Anca Dragan, and Ken Goldberg. Dart: Noise injection for robust imitation learning. In Conference on robot learning, 2017. 4
[44] Wen Sun, J. Andrew Bagnell, and Byron Boots. Truncated horizon policy search: combining reinforcement learning \&amp; imitation learning. In $I C L R, 2018.4$
[45] Kyunghyun Cho, Bart van Merrinboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoderdecoder for statistical machine translation. In EMNLP, 2014. 6
[46] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In ICML, 2017. 6, 18
[47] Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train deeper gcns. In arXiv preprint arXiv:2006.07739, 2020. 7, 8, 18
[48] Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In ICML, 2019. 7, 14
[49] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In NeurIPS, 2020. 8, 9
[50] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina EliassiRad. Collective classification in network data. In AI magazine, 2008. 9
[51] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gnnemann. Pitfalls of graph neural network evaluation. In arXiv preprint arXiv:1811.05868, 2018. 9
[52] Marinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer tissue networks. In Bioinformatics, 2017. 9
[53] Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In arXiv preprint arXiv:2007.08663, 2020. 9
[54] Geoff Boeing. Osmnx: New methods for acquiring, constructing, analyzing, and visualizing complex street networks. In Computers, Environment and Urban Systems, 2017. 9
[55] Miltiadis Allamanis. The adverse effects of code duplication in machine learning models of code. In ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, 2019. 9
[56] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. In Chemical science, 2018. 9
[57] E. Rohmer, S. P. N. Singh, and M. Freese. Coppeliasim (formerly v-rep): a versatile and scalable robot simulation framework. In IROS, 2013. 9, 16
[58] Daniel Lenton, Fabio Pardo, Fabian Falck, Stephen James, and Ronald Clark. Ivy: Templated deep learning for inter-framework portability. In arXiv preprint arXiv:2102.02886, 2021. 9, 16
[59] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In ACM SIGKDD, 2016. 14
[60] Stuart Russell and Peter Norvig. Artificial intelligence: a modern approach. 2002. 14</p>
<p>[61] Sandip Aine, Siddharth Swaminathan, Venkatraman Narayanan, Victor Hwang, and Maxim Likhachev. Multi-heuristic a*. In The International booktitle of Robotics Research, 2016. 14, 15
[62] Edo Cohen-Karlik, Avichai Ben David, and Amir Globerson. Regularizing towards permutation invariance in recurrent models. In NeurIPS, 2020. 14
[63] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. In NeurIPS Deep Learning Workshop, 2013. 15
[64] Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the cross-entropy method. In Annals of operations research, 2005. 15
[65] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li, and Yoshua Bengio. Graph attention networks. In ICLR, 2018. 18
[66] Petar Velickovic, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural execution of graph algorithms. In ICLR, 2020. 19
[67] Petar Velickovic. Tikz. https://github.com/PetarV-/TikZ, last accessed on 01/6/21. 21</p>
<h1>A Discussion, Limitations, and Future Work</h1>
<p>Injection from nodes and edges to features. As mentioned in Section 5.2, if multiple graph nodes or edges have the same features, our heuristic learning method is challenging to apply. To ensure that PHIL has a constant time complexity, we bound the number of neighbouring nodes used for graph convolutions around new fringe nodes, and do not perform any graph convolutions on goal nodes. However, if multiple graph nodes have the same features, or they perhaps do not have any features at all, we may need to perform operations that are not constant in the size of the graph, such as sampling anchor nodes as in position-aware GNN [48], or collecting more expressive Node2Vec [59] style features. Since the time complexity of these methods are relatively high (e.g. position-aware GNN's time complexity is $O\left(|V|^{2} \log ^{2}|V|\right)$ while the Dijkstra algorithm only runs in $O(|V| \log |V|+|E|)$ ), we do not use them unless necessary. For use cases where injections from nodes \&amp; edges to features are hard to guarantee, we encourage practitioners to increase $n$ or potentially perform multiple convolutions on fringe and goal nodes.</p>
<p>Restricted fringe evaluation. As explained in Section 4, PHIL only evaluates new fringe nodes which are obtained after expanding a node. In practice, this means that once PHIL assigns a heuristic value to a node, the value is never updated. While this approach is favorable in terms of the timecomplexity of heuristic computations, it does not allow PHIL to re-evaluate potentially promising nodes for expansion, based on its updated belief about the POMDP state. We believe that methods that predict the features of promising nodes to expand combined with locality-sensitive hashing or approaches that incorporate node value uncertainty present promising avenues for future work.</p>
<p>Solutions not necessarily optimal. For a best-first search algorithm to find optimal solutions, the used heuristics needs to be admissible [60]. In our approach, we do not guarantee the trained heuristics to be itadmissible, which means that when combined with best-first search, PHIL would not guarantee optimal final solutions. On the other hand, our approach is concerned with finding satisficing solutions as quickly as possible, which is motivated by possible applications in Section 1. As in [1], our learned heuristics can be easily incorporated into a framework such as multi-heuristic A* [61], where any number of inadmissible heuristics can be used with a single admissible heuristic, and the final solution cost sub-optimality is bounded. An interesting avenue for future work would be to design heuristic learning loss functions that discourage models from over-estimating heuristic values.</p>
<p>Full memory permutation invariance. As noted in Section 4, our memory module is invariant to the permutation of nodes in $\mathcal{V}<em _new="{new" _text="\text">{\text {new }}$. However, due to how the GRU module is applied, we do not guarantee that that the memory is permutation invariant with respect to the sequence in which nodes are expanded, or equivalently the sequence of $\mathcal{V}</em>$ sets. It could be desirable to guarantee such permutation invariance, as the observations are still nodes and edges of a graph, which may not contain sequential inductive biases. Recent work by Cohen et al. [62] shows that a simple regularization trick can help efficiently train permutation invariant RNNs. It would be interesting to explore in which cases does full permutation invariance improve PHIL's performance.}</p>
<p>Directed graphs. As one may notice, most of the examples in this work include graphs that are undirected. The main reason for this is that once we have directed edges in a graph, it may happen that a particular node does not have a path toward the goal, which means that the oracle cost would be effectively undefined. One option for avoiding this issue is adding parallel backward edges during training, ensuring that paths to goals always exist (assuming that the start and goal nodes are parts of the same connected component). This way, PHIL is correspondingly penalised for expanding a node that does not immediately lie on a path to the goal.</p>
<p>Ethical considerations. Search algorithms with heuristics can be used within unethical systems. However, our work is not tailored for any particular use cases, and hence we do not believe that it has clear direct negative consequences.</p>
<h2>B Practical implementation details</h2>
<p>As noted in our abstract, at test time, the heuristic function obtained from PHIL can be directly used as a heuristic in an algorithm such as A* or greedy best-first search. In practice, this means that we</p>
<p>maintain a priority queue of nodes and distances predicted by the PHIL heuristic and greedily expand the nodes which are predicted to be closest to the goal, as seen in Figure 7.</p>
<p>To ensure fast training in Algorithm 1, we maintain two priority queues, one for PHIL and one for the oracle heuristic. On every expansion, we update both the PHIL queue and oracle queue. This way, probabilistically blending the two greedy policies comes down to either popping from the PHIL queue or the oracle queue.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: PHIL is used as a heuristic to predict the distances of each node, similarly as a heuristic function in best-first search. This figure illustrates a single queue 'pop' operation.</p>
<h1>C Training, Baselines, Hyperparameters, and Datasets</h1>
<p>Baselines. In our experiments we use a range of both learning-based and classical baselines. Our baselines include:</p>
<ul>
<li>Neural A<em> [28]: Neural A</em> learns a differentiable version of A* search on two-dimensional grid graphs. It cannot be straightforwardly adapted to general graphs, which is why we use it as a baseline solely in Section 5.1.</li>
<li>SAIL [1]: Imitation learning-based approach for learning search heuristics on grid graphs. SAIL is adapted in Section 5.2 to not include robotics domain specific features presented in [1], as these are not possible to compute for general graphs.</li>
<li>Supervised Learning (SL): An MLP trained to predict distances between nodes conditioned on node features. In Sections 5.2, Sections 5.3 the nodes are sampled from the graph uniformly at random. In Section 5.1, an oracle policy is rolled out, and data points are collected by sampling random actions during the roll-out as in SAIL. This is because in Section 5.1, the start-goal distribution is not uniform.</li>
<li>Deep Q Learning (QL) [63]: A DQN agent trained to explore the graphs such that $\left|\mathcal{V}_{\text {seen }}\right|$ is minimized. The agent receives a negative reward of -1 until the goal node is reached in each episode.</li>
<li>Cross Entropy Method - Evolutionary Strategies (CEM) [64]: Derivative free optimization of $h_{\theta}$ using evolutionary strategies. As explained in [1], the initial population of policies is sampled using a batch size of 40 . Then, each policy is evaluated on 5 graphs and assigned a score based on the number of expanded nodes for the fitness function. After computing the fitness function, $20 \%$ of best-performing policies are selected to be a part of the next population.</li>
<li>Best-first width search (BFWS) [12]: BFWS adapts greedy best-first search with a generic search history-based novelty metric, which allows it to escape search plateaus and explore relevant nodes. We implement BFWS in experiments from Section 5.1 and 5.3. In Section 5.2, we extend BFWS with boundary-extension features [13], which allows us to apply it to continuous feature spaces.</li>
<li>Multi-heuristic A<em> (MHA</em>)[61]: Multi-heuristic A* using the Euclidean, Manhattan, and $d_{o b s}$ heuristics in a round-robin fashion, where $d_{o b s}$ is the distance of the closest uncovered obstacle for a given node. This baseline is only used in Section 5.1, where all of these heuristics are well-defined.</li>
<li>
<p>A<em> search (A</em>): A* search algorithm using a Euclidean heuristic function on the node features.</p>
</li>
<li>
<p>Greedy best-first search ( $h_{\text {man }}, h_{\text {euc }}$ ): Greedy best-first search using Manhattan and Euclidean heuristics, respectively.</p>
</li>
<li>Breadth-first-search (BFS): Vanilla breadth-first-search without any heuristic.</li>
</ul>
<p>Datasets. Here, we provide more details about the datasets used in Section 5.2 and Section 5.3. For more information about datasets used in Section 5.1, we refer the reader to Bhardwaj and Choudhury et al. [1].
In our experiments using diverse graphs, as noted in Section 5.2, we use the node \&amp; edge features provided in each dataset for training and testing. The three exceptions to this are the PPI, OSMnx datasets, and citation networks. As discussed in Appendix A, we added node labels (i.e., 120 dimensional label vectors) as features in the PPI dataset because the default node features were not unique. In the OSMnx networks, we did not use all the provided node and edge features, rather only the geographical node coordinates. We further augmented the OSMnx node features with their degree centrality and included Laplacian, modularity, and Bethe Hessian edge features. For citation networks, we only used the first 128 features to make the search problem more challenging.
In static graphs (such as the OSMnx networks), it may be helpful to augment the graph with expressive features that could be useful for heuristic computations, such as eigenvectors of the graph Laplacian matrix. In non-static graphs, these operations would typically be too expensive to re-compute on each pathfinding attempt. Note that computing more expressive features such as betweenness or percolation centrality have higher time complexities than computing shortest path distances between all pairs of nodes. Due to their time complexity, these features are impractical pre-compute, though they would likely lead to superior search heuristics.
In Section 5.3, we built our environment using the CoppeliaSim simulator [57], and the Ivy framework by Lenton et al. [58]. Figure 8 presents the environments which we refer to as room adversarial and room simple in Table 3. The room simple environment is equivalent to the Ivy drone demo environment - a single room with a table surrounded by chairs in the middle of the room and various furniture close to the walls. The main difference between room simple and room adversarial is that room simple only contains a single table in the middle of the room. In contrast, in room adversarial, three tables span to a wall, this was creating a bottleneck for naive heuristics. Note that the drones are not allowed to fly under furniture to make the environment more challenging.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: We present two CoppeliaSim environments used for our UAV flight experiments. On the left, we have the adversarial room environment, on the right, we present the room simple environment.</p>
<p>Hyperparameters in diverse graphs. With regards to the diverse graph experiments (Section 5.2), there are in-practice three parameters that we can tune for each dataset: $n, t_{\tau} T$. Recall that the hyperparameter $n$ is the maximal size of the 1-hop neighborhood around the new fringe nodes sampled during training. We found that a rule of thumb of setting $n$ to the minimum of 8 and the average node degree in the given graph dataset worked reasonably well. In practice, $n$ should be tuned to optimize the search effort \&amp; wall-clock performance trade-off. In terms of $T$, it is advisable to set it such that the algorithm can reach target nodes during training. Hence, we found that setting $T$ roughly to the largest graph diameter is a suitable choice. In terms of $t_{\tau}$, we train each sequence using the stored old rolled-in states, and for larger graphs ( $&gt;10,000$ nodes), we use $t_{\tau}=128$, while for smaller graphs (between 10, 000 and 5, 000 nodes), we use $t_{\tau}=64$, and otherwise we use a rule of thumb of taking $t_{\tau} \approx T * 0.2$.</p>
<p>Grid graphs \&amp; drone flight hyperparameters. In our grid graph experiments (Section 5.1), we use $T=256, t_{\tau}=32, \beta_{0}=0.7, n=8$. Finally, in our drone flight experiments, we use the same</p>
<p>configuration as in the grid graph experiments. The only differences are that we set $n=4, t_{\tau}=64$, and we randomize start and goal nodes both during training and testing.</p>
<p>Optimization. PHIL is generally trained using the Adam optimizer with a learning rate of 0.001 and a batch size of 32 . Once we sample $t \sim \mathcal{U}\left(0, \ldots, T-t_{\tau}\right)$ for a roll-in, it can happen that the target is reached in less than $t+t_{\tau}$ steps. In such cases, we continue the sequence until we reach $t_{\tau}$ steps or all the graph nodes are explored, in which case we end the episode. Another approach would be to end episodes once the goal node is reached. In practice, we did not find significant performance differences between the two methods.</p>
<h1>D Ablation studies</h1>
<p>For the ablation studies, we use three, 4-connected versions of down-scaled datasets provided in Bhardwaj et al. [1]: Gaps+Forest, Forest, and Alternating gaps. We downscale each dataset $5 \times$, to dimensions $40 \times 40$. While the Gaps+Forest dataset presents a more challenging environment with multiple planning bottlenecks and obstacles, Forest and Alternating gaps are simpler environments with more straightforward heuristics. Figure 9 present example graphs from the down-scaled datasets Gaps+Forest, Forest, and Alternating gaps.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: This figure presents the down-scaled datasets used for the ablation experiments. We have Gaps+Forest (left), Forest (centre), and Alternating gaps (right).</p>
<p>In each ablation experiment, unless stated otherwise, we use a batch size of 8 , the Adam optimizer with a learning rate of 0.01 , and up to 20 roll-in steps. The MLP in PHIL has 3 layers of width 128, LeakyReLU activations, and the memory state has $d=64$ dimensions. 2D grid coordinates are used as node features, similarly as in Section 5.1. We report the number of explored nodes with respect to $\mathrm{A}^{*}$ or with respect to the optimal number of expansions (i.e., the shortest path distances between nodes). All experiments are performed across 3 random seeds, and standard deviations are used for error bars.</p>
<h2>D. 1 Zeroed-out states vs. Rolled-in states</h2>
<p>There is a trade-off between using rolled-in states for downstream training using backpropagation through time (i.e., storing $z_{t}$ after each roll-in) and using zeroed-out states (i.e., storing zeroed-out initial states). Namely, past rolled-in states $z_{t}$ are out-of-distribution for optimized versions of $h_{\theta}$, but PHIL may use these embeddings for inferring initial node distances because $z_{t}$ contains information about the rolled-in graph. On the other hand, zeroed-out states are always in-distribution for $h_{\theta}$, but the algorithm is constrained to start from an initial state of $\overrightarrow{\mathbf{0}}$ in regions where this may never be the case at test time. Our goal is to gain insight into when one method is preferable over to the other in this ablation.</p>
<p>We train two versions of PHIL, one with zeroed-out initial states for TBTT on each trajectory (PHIL-zero), and one with initial states stored from roll-ins of the past versions of PHIL (PHIL-roll). Both versions are trained for $t_{\tau}=0,8,16,32,64$ TBTT steps using the Gaps+Forest graph dataset. Figure 10 illustrates the performance progress across TBTT steps of both of these approaches. Firstly, we observe that up to around $t_{\tau}=16$ steps; both approaches positively benefit from performing more backpropagation steps through time, reinforcing that the memory module brings overall performance benefits. Further, we may see the performance difference of PHIL-roll and PHIL-zero at 0 steps. In this region, PHIL-roll outperforms PHIL-zero by about $3 \%$, which is following the intuition of the rolled-in states containing useful information about the embeddings of rolled-in graphs. Secondly, we can notice that the performance of PHIL-zero plateaus after 16 steps. This plateau suggests that the graph may not contain useful information for inferring node distances beyond 16 backpropagated steps. Finally, the performance of PHIL-roll decreases much steeper than that of PHIL-zero once it</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: This plot illustrates the performance of PHIL with zeroed-out initial states vs. stored rolled-in initial states during training across multiple TBTT steps.
starts deteriorating after 16 steps, which could mean that there is some form of error compounding once PHIL makes predictions from a wrongly initialised state during training.</p>
<p>Conclusion. The main takeaway of this ablation experiment is that if one would like to perform only a few TBTT steps (i.e., low $t_{\tau}$ ), using rolled-in states will likely provide some performance benefits. On the other hand, as $t_{\tau}$ increases, it is preferable to use zeroed-out initial states. Practitioners can determine this choice on a per-problem basis.</p>
<h1>D. 2 Effect of GNN choice</h1>
<p>In this ablation we assess what effects do different graph neural networks have on the performance of PHIL. Namely, we train the PHIL-zero from the previous ablation, using $t_{\tau}=16, n=4$ and five different GNNs: GAT (Velikovi et al. [65]), MPNN (max), MPNN (sum) (Gilmer et al. [46]), DeeperGCN (softmax), and DeeperGCN (power) (Li et al. [47]). We report the ratio of explored nodes with respect to A* in Table 4.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">GNN</th>
<th style="text-align: center;">Alternating gaps</th>
<th style="text-align: center;">Forest</th>
<th style="text-align: center;">Gaps+Forest</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GAT</td>
<td style="text-align: center;">$0.077 \pm 0.0110$</td>
<td style="text-align: center;">$0.065 \pm 0.0013$</td>
<td style="text-align: center;">$\mathbf{0 . 1 5 4} \pm \mathbf{0 . 0 1 1 2}$</td>
</tr>
<tr>
<td style="text-align: center;">MPNN (max)</td>
<td style="text-align: center;">$0.071 \pm 0.0834$</td>
<td style="text-align: center;">$\mathbf{0 . 0 6 4} \pm \mathbf{0 . 0 0 0 4}$</td>
<td style="text-align: center;">$0.158 \pm 0.0143$</td>
</tr>
<tr>
<td style="text-align: center;">MPNN (sum)</td>
<td style="text-align: center;">$0.095 \pm 0.0487$</td>
<td style="text-align: center;">$0.07 \pm 0.0085$</td>
<td style="text-align: center;">$0.187 \pm 0.0135$</td>
</tr>
<tr>
<td style="text-align: center;">DeeperGCN (softmax)</td>
<td style="text-align: center;">$\mathbf{0 . 0 6 9} \pm \mathbf{0 . 0 0 0 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 6 4} \pm \mathbf{0 . 0 0 3 0}$</td>
<td style="text-align: center;">$0.164 \pm 0.0113$</td>
</tr>
<tr>
<td style="text-align: center;">DeeperGCN (power)</td>
<td style="text-align: center;">$0.076 \pm 0.0027$</td>
<td style="text-align: center;">$0.07 \pm 0.0101$</td>
<td style="text-align: center;">$0.19 \pm 0.0037$</td>
</tr>
</tbody>
</table>
<p>Table 4: This table presents the results obtained using different graph convolutions in the three graph datasets from the ablation study. We can observe that maximisation-based aggregation performs better on average, while attention can provide performance benefits in the challenging Gaps+Forest graphs.</p>
<p>In general, we find that maximisation-based aggregation approaches tend to perform better than other approaches by $1.57 \%$ on average. Note that this comparison only includes the MPNN and DeeperGCN GNNs. DeeperGCN (softmax) achieves the best results on both the Alternating gaps and Forest datasets. Further, using DeeperGCN (softmax), PHIL learned the optimal strategy for finding the goal in the Alternating gaps dataset, which is to follow the path along the bottleneck wall, as seen in Figure 11.
GAT outperforms other approaches on the Gaps+Forest dataset, which is also the most complex of the three datasets. This finding suggests that forms of attention
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: This figure illustrates PHIL using the DeeperGCN (softmax) GNN learning the optimal heuristic strategy in the Alternating gaps dataset.</p>
<p>could be useful for learning heuristics in more complex graphs. In Figure 12, we can see a side-by-side comparison of GAT, DeeperGCN (softmax), and MPNN (sum) in the Gaps+Forest graph dataset. We may observe that the GAT-based and DeeperGCNbased heuristics find strategies that are close to optimal, while MPNN (sum) performs a similar strategy, but with slightly more expansions.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: We compare the solutions of PHIL trained with a GAT (left), DeeperGCN (softmax) (middle), and MPNN (sum) (right). Results demonstrate that MPNN (sum) makes several redundant expansions while the other two methods expand paths close to optimally.</p>
<p>Conclusion. The GNN ablation demonstrates that maximisation-based aggregation performs better, with an average reduction of explored nodes by $1.57 \%$. This finding is consistent with Velikovi et al. [66], where GNNs are applied to execute graph algorithms. While the problem solved by Velikovi et al. [66] is different, its nature is similar: train a GNN to select which node to explore next in order to imitate a reference graph algorithm. In PHIL, scores are assigned to nodes rather than nodes being selected, but the downstream operation is node selection. Further, these experiments also suggest that attention can be helpful in more complex graphs, with GAT outperforming other approaches in the Gaps+Forest graphs.</p>
<h1>D. 3 Increasing neighborhood size</h1>
<p>As explained in the approach (Section 4), for each new fringe node we uniformly sample an $n \in \mathbb{N}<em _tau="\tau">{\geq 0}$ bounded neighborhood of nodes, which we then use for graph convolutions. We hypothesized that with increasing $n$, the performance of PHIL will improve. In Figure 13, we validate this hypothesis on Alternating gaps, Forest, and Gaps+Forest datasets, by gradually setting $n=0,1,2,4$. We train PHIL using a DeeperGCN (softmax) graph convolution, $t</em>=16$, and with otherwise the same set of hyperparameters as in the Zeroed-out states vs. Rolled-in states (Appendix D.1) experiment. In Figure 13, we report the explored node ratio of each method with respect to the optimal number of explored nodes, that is, the shortest paths between start \&amp; goal pairs.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: This plot illustrates the performance of PHIL with increasing $n=\left|\mathcal{N}_{i}\right|$ through $0,1,2,4$ for each evaluated fringe node $i$.</p>
<p>Conclusion. In Figure 13, we may observe that on all datasets the performance of PHIL improves with increasing $n=\left|\mathcal{N}_{i}\right|$ until $n=2$, after which it plateaus. These results suggest that PHIL can</p>
<p>benefit from additional nodes sampled from the neighbourhoods of evaluated fringe nodes, even if this sampling is performed uniformly at random.</p>
<h1>D. 4 Increasing memory capacity</h1>
<p>In the final ablation experiment, we consider what effects changing the capacity of memory $(d)$ has on the overall performance of PHIL. We alter $d=1,16,32,64,128,256$ across all datasets. In Figure 14 , the ratio of explored nodes is presented with respect to the shortest path length, which is the optimal baseline.</p>
<p>Focusing on the Gaps+Forest dataset, in Figure 14 we can observe that the performance of PHIL generally improves until about $d=32$ by approximately $40 \%$ with respect to $d=1$, after which it starts getting worse. Hence, we can conclude that additional memory capacity can be helpful for PHIL to learn representations better suited for inferring distances of newly added fringe nodes. Note that we trained PHIL for a fixed number of iterations $(N=36)$ in all cases, which means that the decrease in performance after $d=32$ could also be due to the GRU module having more parameters to optimise, which may take longer to converge. However, it could also easily be that the memory module starts overfitting to samples in the aggregated dataset during training. In the case of the simpler Alternating gaps and Forest datasets, the differences between different amounts of memory capacity are marginal. These findings are supported by approaches such as SAIL [1] achieving good performances in simpler environments. By implication, performance decrease in the Gaps+Forest dataset for larger values of $d$ is more likely due to overfitting than optimisation difficulties.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14: This plot illustrates the performance of PHIL with increasing $d$ through $1,16,32,64,128,256$ on the three ablation datasets.</p>
<p>Conclusion. Additional memory capacity is crucial for PHIL to learn useful representations in more challenging graphs, while the importance of additional memory decreases as the graphs are simpler. However, a certain amount of overfitting is observed for larger values of $d$, which means that $d$ should be tuned on a per problem basis.</p>
<h1>E Architecture</h1>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 15: This figure illustrates the computation performed in a single forward pass of $h_{\theta}$. In the case of this figure, we would have $\left|\mathcal{V}<em i="i">{\text {new }}\right|=n=3$, with $g</em>}$ representing the convolved embeddings, and $\hat{h<em t="t">{i}$ the output heuristic values. Horizontally, we illustrate the update of memory $z</em>$. This figure is adapted from [67].}$ to $z_{t+1</p>
<h2>F Qualitative Examples</h2>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 16: This figure illustrates the road network of Modena, Italy. We contrast the search effort of PHIL (left) and A* (right). We can observe that PHIL expands (shown in green) considerably fewer nodes searching for the goal $v_{g}$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Correspondence to michalpandy@google.com.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
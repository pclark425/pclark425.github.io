<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3985 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3985</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3985</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-d0086b86103a620a86bc918746df0aa642e2a8a3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d0086b86103a620a86bc918746df0aa642e2a8a3" target="_blank">Language Models as Knowledge Bases?</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> An in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models finds that BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge.</p>
                <p><strong>Paper Abstract:</strong> Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as “fill-in-the-blank” cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3985.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3985.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LAMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LAnguage Model Analysis (LAMA) probe</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probe introduced in this paper to measure factual and commonsense knowledge stored in pretrained language models by converting facts into cloze statements and ranking model predictions for masked object tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Ability of a pretrained LM to rank the ground-truth object token highly when presented with a cloze statement expressing a (subject, relation, object) fact (i.e., recall of factual/commonsense knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Convert facts or question-answer pairs into cloze templates (masked-token queries); feed templates to pretrained models without fine-tuning; compute ranks of the ground-truth object among a fixed candidate vocabulary; compare to baselines (frequency, relation-extraction KB, DrQA).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>LAMA is a probe composed of multiple knowledge sources (Google-RE, T-REx, ConceptNet, SQuAD subset) assembled by the authors to test factual and commonsense knowledge via cloze-style queries.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Rank-based metrics: mean Precision@k (P@1, P@10, etc.), P@k curves (mean P@k across facts and relations); Pearson correlations between P@1 and auxiliary statistics (object/subject frequency in training data, log-probability of first prediction, cosine similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Manual creation of cloze templates per relation; manual mapping of a subset of SQuAD questions to cloze form; selection/inspection of example generations. No human labeling at evaluation time for model outputs besides those template/selection steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Probe measures a lower bound on knowledge (sensitive to template wording), restricted to single-token object targets (multi-token answer decoding omitted), uses an intersection vocabulary to normalize comparisons, and can conflate memorization of training sentences with understanding; alignment noise in some sources (T-REx) and N-to-M relations are especially challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Not an LLM-generated scientific theory per se; example use-case: predicting the masked object in 'Dante was born in [MASK]' and ranking 'Florence' among candidate tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BERT-large outperformed other pretrained models on LAMA: e.g., P@1 across T-REx ≈ 32.3% (BERT-large), ConceptNet P@1 ≈ 19.2%, SQuAD subset P@1 ≈ 17.4%; correct object in top-10 for BERT ≈ 60% on T-REx (P@10 ≈ 57.1% reported for cloze SQuAD vs DrQA P@10 63.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3985.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3985.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cloze/masked evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cloze-style (masked-token) evaluation using templates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Querying pretrained LMs with manually defined cloze templates (masking the object token) and measuring whether the model ranks the correct object highly in its softmax over vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Ranking of the ground-truth object token in the LM's predicted token distribution for a masked position in a sentence expressing a fact or question.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>For bidirectional models (e.g., BERT): mask the object token and use the masked position vector to score vocabulary tokens; for unidirectional models use usual next-token probability; unify by restricting candidates to the intersection vocabulary and compute ranks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to facts transformed from Google-RE, T-REx, ConceptNet, and a manually clozified SQuAD subset.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Mean Precision@k (P@1, P@10, P@100), rank distributions across multiple templates/mentions per fact, log-probability of top prediction (LPFP), Pearson correlations between LPFP and P@1.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Manual design of templates that are used to generate cloze queries; selection of aligned sentences where available.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Results depend on template phrasing (sensitivity to query wording); single-token restriction; multi-token generation for bidirectional models omitted due to extra modeling/decoding complexities.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Transforming a KB triple (Dante, born-in, Florence) to 'Dante was born in [MASK]' and checking if 'Florence' ranks highly.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Demonstrated that BERT-large ranks many correct objects highly (e.g., in top-10 roughly 60% for T-REx); but performance varies widely by relation type (strong for 1-to-1 relations, weak for N-to-M relations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3985.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3985.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>P@k</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Precision at k (P@k) rank-based metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Primary quantitative metric used to evaluate whether the ground-truth object appears in the top-k predictions for each cloze query; averaged per relation and across relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Whether the correct object is among the top-k ranked vocabulary tokens for a given cloze query.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>For each test fact, compute a binary 1/0 whether the object appears in top-k; average across facts to obtain mean P@k (e.g., P@1, P@10). Remove other valid objects from training data when ranking to account for multiple valid answers (standard KB-completion protocol).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to Google-RE, T-REx, ConceptNet and SQuAD cloze set within the LAMA probe.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Mean P@1 shown in Table 2 for models and baselines; P@10 and P@100 used in curve analyses (P@k curves). Examples: BERT-large P@1 on T-REx total = 32.3%; BERT-large P@10 on SQuAD ≈ 57.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>No human labeling required at scoring stage; human effort used earlier to build templates/datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>P@k ignores answer confidence calibration beyond ranking and does not capture partial credit for semantically close or multi-token answers; choice of k affects interpretation; multiple correct objects require careful filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>N/A (metric-level), operationalized for cloze queries and reported per-relation and averaged.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used to show that BERT-large frequently ranks correct factual tokens near top (e.g., P@1 and P@10 statistics reported), with clear gaps across relation types and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3985.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3985.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baselines: RE and Freq</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation-Extraction baseline (RE) with oracle/naïve entity linking; Frequency baseline (Freq)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Off-the-shelf relation extraction system (Sorokin & Gurevych, 2017) applied to aligned sentences to build a KB and frequency-based baseline ranking most frequent objects for a relation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Context-aware representations for knowledge base relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Performance of an automatically extracted symbolic KB (RE) and a simple frequency heuristic (Freq) in ranking correct objects for relations, to compare against LM recall.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Run RE on sentences aligned to facts to build a knowledge graph; query the graph for subject and relation and rank candidate objects by extractor confidence. Two RE variants: RE_n (naïve string-match entity linking) and RE_o (oracle entity linking). Freq ranks by frequency of objects in test data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated on the same knowledge sources used by LAMA (Google-RE aligned sentences, T-REx alignments).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Mean P@1 for RE_n and RE_o per corpus and relation (Table 2). Example: RE_o on T-REx total P@1 = 33.8%; RE_n much lower (6.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Oracle entity linking simulates perfect human-level linking for RE_o; otherwise minimal human involvement at evaluation time.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>RE depends on good entity linking and high-quality alignments; RE_o is a generous upper bound (uses oracle linking) and may not reflect practical systems; adding more text to RE did not substantially change performance in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>N/A (baseline system rather than an LLM-generated theory).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BERT-large matched or approached the oracle RE_o in some settings (e.g., BERT-large P@1 close to RE_o on certain relations), suggesting pretrained LMs can rival supervised relation-extraction pipelines for many factual queries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3985.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3985.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline: DrQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DrQA open-domain question answering pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage open-domain QA system (TF–IDF retrieval + neural reading comprehension) used as a supervised baseline for SQuAD-style cloze questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reading wikipedia to answer open-domain questions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Ability to retrieve and extract single-token answers from retrieved Wikipedia articles for open-domain questions (constrained to single-token answers for fair comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Run TF/IDF retrieval to get top-k articles, then apply the neural reader to extract answer tokens; restrict to single-token predictions to match LM evaluation constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated on a 305-question subset of SQuAD (single-token answers) transformed into cloze templates.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Mean P@1 and P@10; comparative P@10 reported in paper: DrQA P@10 = 63.5% vs BERT-large P@10 ≈ 57.1% on the clozified SQuAD subset.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>DrQA is a supervised system trained on labeled QA data; evaluation uses no extra human labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>DrQA benefits from an IR stage and supervised training; constraining DrQA to single-token answers reduces its usual output expressivity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>N/A (baseline comparison for LM recall on QA).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>DrQA outperformed BERT-large on the constrained SQuAD cloze P@10 measure by a modest margin, but the gap was smaller than might be expected given BERT-large had no retrieval or supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3985.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3985.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Datasets: T-REx</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T-REx (subset derived from Wikidata alignments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale set of knowledge triples automatically aligned to Wikipedia sentences (Elsahar et al., 2018); used in LAMA to test many relations (41 relations, up to 1000 facts per relation sampled).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>T-rex: A large scale alignment of natural language with knowledge base triples</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Whether models can recover objects of Wikidata triples from masked Wikipedia sentences aligned to those triples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Mask object tokens in aligned Wikipedia sentences (or manually defined templates) and evaluate LM rankings; compute per-relation P@k and analyze sensitivity across multiple mentions per fact.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>T-REx subset (≈34k facts across 41 relations) used within the LAMA probe.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Per-relation and overall mean P@1 and P@k; correlation analyses between P@1 and statistics like object frequency in training data and log-probability of predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>T-REx alignments are automatic (Elsahar et al. report ≈97.8% accuracy); authors subsampled and manually defined templates for relations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Alignments can be noisy despite high reported accuracy; performance varies strongly by relation type (1-to-1 relations high, N-to-M relations low); possible memorization if models were trained on the same Wikipedia sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Examples: for P19 (birth-place), mask yields candidate ranking where model may place correct city among top tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BERT-large achieved substantially higher P@1 than many other LMs (e.g., BERT-large T-REx total P@1 = 32.3%), with correct object in top-10 ≈ 60% of cases; strong correlation between object frequency in LM training data and P@1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3985.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3985.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Datasets: ConceptNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConceptNet (English subset derived from OMCS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commonsense knowledge base mapping concepts to relations; LAMA uses English ConceptNet triples with single-token objects and corresponding OMCS sentences for cloze querying.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Representing general relational knowledge in conceptnet 5</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Whether pretrained LMs can predict commonsense object concepts masked within OMCS sentences corresponding to ConceptNet triples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Find OMCS sentence that contains subject and object, mask object, query LM, compute rank of correct object (P@k).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>ConceptNet subset (11,458 facts across 16 relations with single-token objects) used in LAMA.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Mean P@1 per relation and overall P@1 (e.g., BERT-large ConceptNet P@1 ≈ 19.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Selection of OMCS sentences and mapping to triples involved deterministic matching; template choice implicit in the sentence used.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No explicit Wikipedia alignment; diversity of commonsense phrasing affects sensitivity; single-token constraint limits coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Queries like 'Birds have [MASK]' expected to yield 'feathers' or similar commonsense tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BERT-large consistently achieved the best performance among tested LMs on ConceptNet and retrieved many commonsense facts at levels comparable to factual knowledge retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3985.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3985.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Datasets: SQuAD-subset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SQuAD subset (manually clozified questions with single-token answers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A manually selected set of 305 context-insensitive SQuAD questions with single-token answers rewritten as cloze statements to test open-domain QA ability of pretrained LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SQuAD: 100,000+ Questions for Machine Comprehension of Text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Whether an unsupervised pretrained LM (no retrieval, no fine-tuning) ranks the correct single-token answer highly given a cloze-style rewrite of a SQuAD question.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Rewrite questions to passive/cloze form (manual), mask answer, query LM; compare to supervised DrQA baseline constrained to single-token answers; compute P@1 and P@10.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>305-cloze-question subset from SQuAD development set used in LAMA.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>P@1 and P@10; reported examples include BERT-large P@1 ≈ 17.4% and P@10 ≈ 57.1% versus DrQA P@10 ≈ 63.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Manual rewriting of questions to cloze templates and manual selection of context-insensitive questions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Lack of IR retrieval stage disadvantages LM compared to DrQA; single-token constraint; manual rewriting may introduce bias; pretrained LMs may have seen the supporting Wikipedia context, conflating memorization with QA ability.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Rewriting 'Who developed the theory of relativity?' as 'The theory of relativity was developed by [MASK]' and checking whether 'Einstein' ranks highly.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BERT-large (unsupervised, no retrieval) achieved P@10 close to DrQA (57.1% vs 63.5%), showing strong zero-shot open-domain QA potential when evaluated with cloze-style templates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3985.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3985.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-involvement (templates & alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human involvement in template creation and dataset mapping</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual steps in the LAMA probe pipeline: defining cloze templates per relation, mapping SQuAD questions to cloze form, and sampling/inspecting examples and generations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Quality and suitability of templates and mappings to elicit the factual object from the LM (affects measured knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Experts manually design templates per relation (e.g., '[S] was born in [O]'); manual mapping of SQuAD questions to cloze sentences; manual selection of example sentences for inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied across LAMA's knowledge sources (Google-RE, T-REx, ConceptNet, SQuAD subset).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Not a numeric metric per se; paper reports sensitivity analyses showing variability by template phrasing and by different mentions of the same fact (rank distributions across 10 mentions per fact).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>High at template creation and SQuAD clozification; also manual inspection of example model generations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Template choices introduce measurement bias — the authors treat the results as a lower bound; manual work does not scale and can favor some models depending on phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Manual rewrite example: 'Who developed the theory of relativity?' -> 'The theory of relativity was developed by [MASK]'.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper finds that alternate templates can produce both better and worse retrieval results for the same fact, demonstrating sensitivity to human-designed phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3985.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3985.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Limitations & challenges</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation limitations and open challenges noted in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concise summary of caveats the authors report about evaluating knowledge in pretrained LMs via the LAMA probe.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>N/A (meta-evaluation of limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Discussion and empirical analyses (e.g., per-relation breakdowns, sensitivity to templates, rank-distribution over multiple mentions).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applies to all LAMA knowledge sources and evaluation procedures described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Empirical observations (e.g., poor performance on N-to-M relations, variability across templates, correlation coefficients) rather than a single new metric.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Noted that manual template creation influences results and that multi-token answer evaluation was intentionally omitted due to complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Template sensitivity (results are a lower bound); restriction to single-token objects omits many facts; multi-token decoding for bidirectional models is an open challenge; possible memorization vs true understanding (models trained on Wikipedia may have seen test sentences); vocabulary intersection can affect difficulty; noisy alignments for some datasets; entity-linking is a bottleneck for RE baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>N/A (limitation-level commentary).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Authors conclude LAMA provides insight but has clear boundaries: it shows BERT-large contains substantial factual/commonsense knowledge, yet extracting multi-token and N-M relation knowledge reliably remains unsolved and sensitive to evaluation design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3985.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e3985.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Example: BERT-large evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-large performance examples reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical evaluation results showing BERT-large's relative strength at recalling factual and commonsense knowledge in cloze settings compared to other LMs and to symbolic baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>P@1 and P@k performance on cloze queries derived from Google-RE, T-REx, ConceptNet, and a SQuAD subset; robustness to template phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Use LAMA probe cloze queries; compute per-relation and aggregated mean P@1 and P@k; analyze rank distributions across multiple mentions and compute Pearson correlations with auxiliary statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated on LAMA's datasets (Google-RE, T-REx, ConceptNet, SQuAD subset).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Selected reported numbers: Google-RE total P@1 = 10.5% (BERT-large), T-REx total P@1 = 32.3%, ConceptNet P@1 = 19.2%, SQuAD subset P@1 = 17.4%; P@10 (SQuAD cloze) ≈ 57.1% for BERT-large.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>No fine-tuning or human labels during model querying; humans authored templates and selected SQuAD subset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Authors note that high performance may partially reflect memorization of Wikipedia contexts BERT saw during training rather than generalizable understanding; sensitivity to phrasing and single-token constraints apply.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Concrete generation examples in Table 3 show BERT-large top predictions and log-probabilities for masked objects (e.g., 'Francesco Bartolomeo Conti was born in [MASK]' yields 'Rome' and 'Florence' among top tokens with associated log probs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BERT-large consistently best among tested LMs and competitive with a KB built by a supervised RE system with oracle linking on many relations, but with notable weaknesses on N-to-M relations and multi-token answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models as Knowledge Bases?', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <em>(Rating: 2)</em></li>
                <li>Reading wikipedia to answer open-domain questions <em>(Rating: 2)</em></li>
                <li>Context-aware representations for knowledge base relation extraction <em>(Rating: 2)</em></li>
                <li>T-rex: A large scale alignment of natural language with knowledge base triples <em>(Rating: 2)</em></li>
                <li>Representing general relational knowledge in conceptnet 5 <em>(Rating: 1)</em></li>
                <li>SQuAD: 100,000+ Questions for Machine Comprehension of Text <em>(Rating: 1)</em></li>
                <li>Language models are unsupervised multitask learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3985",
    "paper_id": "paper-d0086b86103a620a86bc918746df0aa642e2a8a3",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "LAMA",
            "name_full": "LAnguage Model Analysis (LAMA) probe",
            "brief_description": "A probe introduced in this paper to measure factual and commonsense knowledge stored in pretrained language models by converting facts into cloze statements and ranking model predictions for masked object tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Ability of a pretrained LM to rank the ground-truth object token highly when presented with a cloze statement expressing a (subject, relation, object) fact (i.e., recall of factual/commonsense knowledge).",
            "evaluation_methods": "Convert facts or question-answer pairs into cloze templates (masked-token queries); feed templates to pretrained models without fine-tuning; compute ranks of the ground-truth object among a fixed candidate vocabulary; compare to baselines (frequency, relation-extraction KB, DrQA).",
            "benchmark_or_dataset": "LAMA is a probe composed of multiple knowledge sources (Google-RE, T-REx, ConceptNet, SQuAD subset) assembled by the authors to test factual and commonsense knowledge via cloze-style queries.",
            "metrics_reported": "Rank-based metrics: mean Precision@k (P@1, P@10, etc.), P@k curves (mean P@k across facts and relations); Pearson correlations between P@1 and auxiliary statistics (object/subject frequency in training data, log-probability of first prediction, cosine similarity).",
            "human_involvement": "Manual creation of cloze templates per relation; manual mapping of a subset of SQuAD questions to cloze form; selection/inspection of example generations. No human labeling at evaluation time for model outputs besides those template/selection steps.",
            "limitations_or_challenges": "Probe measures a lower bound on knowledge (sensitive to template wording), restricted to single-token object targets (multi-token answer decoding omitted), uses an intersection vocabulary to normalize comparisons, and can conflate memorization of training sentences with understanding; alignment noise in some sources (T-REx) and N-to-M relations are especially challenging.",
            "llm_theory_example": "Not an LLM-generated scientific theory per se; example use-case: predicting the masked object in 'Dante was born in [MASK]' and ranking 'Florence' among candidate tokens.",
            "evaluation_results": "BERT-large outperformed other pretrained models on LAMA: e.g., P@1 across T-REx ≈ 32.3% (BERT-large), ConceptNet P@1 ≈ 19.2%, SQuAD subset P@1 ≈ 17.4%; correct object in top-10 for BERT ≈ 60% on T-REx (P@10 ≈ 57.1% reported for cloze SQuAD vs DrQA P@10 63.5%).",
            "uuid": "e3985.0",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "cloze/masked evaluation",
            "name_full": "Cloze-style (masked-token) evaluation using templates",
            "brief_description": "Querying pretrained LMs with manually defined cloze templates (masking the object token) and measuring whether the model ranks the correct object highly in its softmax over vocabulary.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "Ranking of the ground-truth object token in the LM's predicted token distribution for a masked position in a sentence expressing a fact or question.",
            "evaluation_methods": "For bidirectional models (e.g., BERT): mask the object token and use the masked position vector to score vocabulary tokens; for unidirectional models use usual next-token probability; unify by restricting candidates to the intersection vocabulary and compute ranks.",
            "benchmark_or_dataset": "Applied to facts transformed from Google-RE, T-REx, ConceptNet, and a manually clozified SQuAD subset.",
            "metrics_reported": "Mean Precision@k (P@1, P@10, P@100), rank distributions across multiple templates/mentions per fact, log-probability of top prediction (LPFP), Pearson correlations between LPFP and P@1.",
            "human_involvement": "Manual design of templates that are used to generate cloze queries; selection of aligned sentences where available.",
            "limitations_or_challenges": "Results depend on template phrasing (sensitivity to query wording); single-token restriction; multi-token generation for bidirectional models omitted due to extra modeling/decoding complexities.",
            "llm_theory_example": "Transforming a KB triple (Dante, born-in, Florence) to 'Dante was born in [MASK]' and checking if 'Florence' ranks highly.",
            "evaluation_results": "Demonstrated that BERT-large ranks many correct objects highly (e.g., in top-10 roughly 60% for T-REx); but performance varies widely by relation type (strong for 1-to-1 relations, weak for N-to-M relations).",
            "uuid": "e3985.1",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "P@k",
            "name_full": "Precision at k (P@k) rank-based metric",
            "brief_description": "Primary quantitative metric used to evaluate whether the ground-truth object appears in the top-k predictions for each cloze query; averaged per relation and across relations.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "Whether the correct object is among the top-k ranked vocabulary tokens for a given cloze query.",
            "evaluation_methods": "For each test fact, compute a binary 1/0 whether the object appears in top-k; average across facts to obtain mean P@k (e.g., P@1, P@10). Remove other valid objects from training data when ranking to account for multiple valid answers (standard KB-completion protocol).",
            "benchmark_or_dataset": "Applied to Google-RE, T-REx, ConceptNet and SQuAD cloze set within the LAMA probe.",
            "metrics_reported": "Mean P@1 shown in Table 2 for models and baselines; P@10 and P@100 used in curve analyses (P@k curves). Examples: BERT-large P@1 on T-REx total = 32.3%; BERT-large P@10 on SQuAD ≈ 57.1%.",
            "human_involvement": "No human labeling required at scoring stage; human effort used earlier to build templates/datasets.",
            "limitations_or_challenges": "P@k ignores answer confidence calibration beyond ranking and does not capture partial credit for semantically close or multi-token answers; choice of k affects interpretation; multiple correct objects require careful filtering.",
            "llm_theory_example": "N/A (metric-level), operationalized for cloze queries and reported per-relation and averaged.",
            "evaluation_results": "Used to show that BERT-large frequently ranks correct factual tokens near top (e.g., P@1 and P@10 statistics reported), with clear gaps across relation types and datasets.",
            "uuid": "e3985.2",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Baselines: RE and Freq",
            "name_full": "Relation-Extraction baseline (RE) with oracle/naïve entity linking; Frequency baseline (Freq)",
            "brief_description": "Off-the-shelf relation extraction system (Sorokin & Gurevych, 2017) applied to aligned sentences to build a KB and frequency-based baseline ranking most frequent objects for a relation.",
            "citation_title": "Context-aware representations for knowledge base relation extraction",
            "mention_or_use": "use",
            "evaluation_criteria": "Performance of an automatically extracted symbolic KB (RE) and a simple frequency heuristic (Freq) in ranking correct objects for relations, to compare against LM recall.",
            "evaluation_methods": "Run RE on sentences aligned to facts to build a knowledge graph; query the graph for subject and relation and rank candidate objects by extractor confidence. Two RE variants: RE_n (naïve string-match entity linking) and RE_o (oracle entity linking). Freq ranks by frequency of objects in test data.",
            "benchmark_or_dataset": "Evaluated on the same knowledge sources used by LAMA (Google-RE aligned sentences, T-REx alignments).",
            "metrics_reported": "Mean P@1 for RE_n and RE_o per corpus and relation (Table 2). Example: RE_o on T-REx total P@1 = 33.8%; RE_n much lower (6.1%).",
            "human_involvement": "Oracle entity linking simulates perfect human-level linking for RE_o; otherwise minimal human involvement at evaluation time.",
            "limitations_or_challenges": "RE depends on good entity linking and high-quality alignments; RE_o is a generous upper bound (uses oracle linking) and may not reflect practical systems; adding more text to RE did not substantially change performance in experiments.",
            "llm_theory_example": "N/A (baseline system rather than an LLM-generated theory).",
            "evaluation_results": "BERT-large matched or approached the oracle RE_o in some settings (e.g., BERT-large P@1 close to RE_o on certain relations), suggesting pretrained LMs can rival supervised relation-extraction pipelines for many factual queries.",
            "uuid": "e3985.3",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Baseline: DrQA",
            "name_full": "DrQA open-domain question answering pipeline",
            "brief_description": "A two-stage open-domain QA system (TF–IDF retrieval + neural reading comprehension) used as a supervised baseline for SQuAD-style cloze questions.",
            "citation_title": "Reading wikipedia to answer open-domain questions",
            "mention_or_use": "use",
            "evaluation_criteria": "Ability to retrieve and extract single-token answers from retrieved Wikipedia articles for open-domain questions (constrained to single-token answers for fair comparison).",
            "evaluation_methods": "Run TF/IDF retrieval to get top-k articles, then apply the neural reader to extract answer tokens; restrict to single-token predictions to match LM evaluation constraints.",
            "benchmark_or_dataset": "Evaluated on a 305-question subset of SQuAD (single-token answers) transformed into cloze templates.",
            "metrics_reported": "Mean P@1 and P@10; comparative P@10 reported in paper: DrQA P@10 = 63.5% vs BERT-large P@10 ≈ 57.1% on the clozified SQuAD subset.",
            "human_involvement": "DrQA is a supervised system trained on labeled QA data; evaluation uses no extra human labeling.",
            "limitations_or_challenges": "DrQA benefits from an IR stage and supervised training; constraining DrQA to single-token answers reduces its usual output expressivity.",
            "llm_theory_example": "N/A (baseline comparison for LM recall on QA).",
            "evaluation_results": "DrQA outperformed BERT-large on the constrained SQuAD cloze P@10 measure by a modest margin, but the gap was smaller than might be expected given BERT-large had no retrieval or supervision.",
            "uuid": "e3985.4",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Datasets: T-REx",
            "name_full": "T-REx (subset derived from Wikidata alignments)",
            "brief_description": "A large-scale set of knowledge triples automatically aligned to Wikipedia sentences (Elsahar et al., 2018); used in LAMA to test many relations (41 relations, up to 1000 facts per relation sampled).",
            "citation_title": "T-rex: A large scale alignment of natural language with knowledge base triples",
            "mention_or_use": "use",
            "evaluation_criteria": "Whether models can recover objects of Wikidata triples from masked Wikipedia sentences aligned to those triples.",
            "evaluation_methods": "Mask object tokens in aligned Wikipedia sentences (or manually defined templates) and evaluate LM rankings; compute per-relation P@k and analyze sensitivity across multiple mentions per fact.",
            "benchmark_or_dataset": "T-REx subset (≈34k facts across 41 relations) used within the LAMA probe.",
            "metrics_reported": "Per-relation and overall mean P@1 and P@k; correlation analyses between P@1 and statistics like object frequency in training data and log-probability of predictions.",
            "human_involvement": "T-REx alignments are automatic (Elsahar et al. report ≈97.8% accuracy); authors subsampled and manually defined templates for relations.",
            "limitations_or_challenges": "Alignments can be noisy despite high reported accuracy; performance varies strongly by relation type (1-to-1 relations high, N-to-M relations low); possible memorization if models were trained on the same Wikipedia sentences.",
            "llm_theory_example": "Examples: for P19 (birth-place), mask yields candidate ranking where model may place correct city among top tokens.",
            "evaluation_results": "BERT-large achieved substantially higher P@1 than many other LMs (e.g., BERT-large T-REx total P@1 = 32.3%), with correct object in top-10 ≈ 60% of cases; strong correlation between object frequency in LM training data and P@1.",
            "uuid": "e3985.5",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Datasets: ConceptNet",
            "name_full": "ConceptNet (English subset derived from OMCS)",
            "brief_description": "A commonsense knowledge base mapping concepts to relations; LAMA uses English ConceptNet triples with single-token objects and corresponding OMCS sentences for cloze querying.",
            "citation_title": "Representing general relational knowledge in conceptnet 5",
            "mention_or_use": "use",
            "evaluation_criteria": "Whether pretrained LMs can predict commonsense object concepts masked within OMCS sentences corresponding to ConceptNet triples.",
            "evaluation_methods": "Find OMCS sentence that contains subject and object, mask object, query LM, compute rank of correct object (P@k).",
            "benchmark_or_dataset": "ConceptNet subset (11,458 facts across 16 relations with single-token objects) used in LAMA.",
            "metrics_reported": "Mean P@1 per relation and overall P@1 (e.g., BERT-large ConceptNet P@1 ≈ 19.2%).",
            "human_involvement": "Selection of OMCS sentences and mapping to triples involved deterministic matching; template choice implicit in the sentence used.",
            "limitations_or_challenges": "No explicit Wikipedia alignment; diversity of commonsense phrasing affects sensitivity; single-token constraint limits coverage.",
            "llm_theory_example": "Queries like 'Birds have [MASK]' expected to yield 'feathers' or similar commonsense tokens.",
            "evaluation_results": "BERT-large consistently achieved the best performance among tested LMs on ConceptNet and retrieved many commonsense facts at levels comparable to factual knowledge retrieval.",
            "uuid": "e3985.6",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Datasets: SQuAD-subset",
            "name_full": "SQuAD subset (manually clozified questions with single-token answers)",
            "brief_description": "A manually selected set of 305 context-insensitive SQuAD questions with single-token answers rewritten as cloze statements to test open-domain QA ability of pretrained LMs.",
            "citation_title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
            "mention_or_use": "use",
            "evaluation_criteria": "Whether an unsupervised pretrained LM (no retrieval, no fine-tuning) ranks the correct single-token answer highly given a cloze-style rewrite of a SQuAD question.",
            "evaluation_methods": "Rewrite questions to passive/cloze form (manual), mask answer, query LM; compare to supervised DrQA baseline constrained to single-token answers; compute P@1 and P@10.",
            "benchmark_or_dataset": "305-cloze-question subset from SQuAD development set used in LAMA.",
            "metrics_reported": "P@1 and P@10; reported examples include BERT-large P@1 ≈ 17.4% and P@10 ≈ 57.1% versus DrQA P@10 ≈ 63.5%.",
            "human_involvement": "Manual rewriting of questions to cloze templates and manual selection of context-insensitive questions.",
            "limitations_or_challenges": "Lack of IR retrieval stage disadvantages LM compared to DrQA; single-token constraint; manual rewriting may introduce bias; pretrained LMs may have seen the supporting Wikipedia context, conflating memorization with QA ability.",
            "llm_theory_example": "Rewriting 'Who developed the theory of relativity?' as 'The theory of relativity was developed by [MASK]' and checking whether 'Einstein' ranks highly.",
            "evaluation_results": "BERT-large (unsupervised, no retrieval) achieved P@10 close to DrQA (57.1% vs 63.5%), showing strong zero-shot open-domain QA potential when evaluated with cloze-style templates.",
            "uuid": "e3985.7",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Human-involvement (templates & alignment)",
            "name_full": "Human involvement in template creation and dataset mapping",
            "brief_description": "Manual steps in the LAMA probe pipeline: defining cloze templates per relation, mapping SQuAD questions to cloze form, and sampling/inspecting examples and generations.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "Quality and suitability of templates and mappings to elicit the factual object from the LM (affects measured knowledge).",
            "evaluation_methods": "Experts manually design templates per relation (e.g., '[S] was born in [O]'); manual mapping of SQuAD questions to cloze sentences; manual selection of example sentences for inspection.",
            "benchmark_or_dataset": "Applied across LAMA's knowledge sources (Google-RE, T-REx, ConceptNet, SQuAD subset).",
            "metrics_reported": "Not a numeric metric per se; paper reports sensitivity analyses showing variability by template phrasing and by different mentions of the same fact (rank distributions across 10 mentions per fact).",
            "human_involvement": "High at template creation and SQuAD clozification; also manual inspection of example model generations.",
            "limitations_or_challenges": "Template choices introduce measurement bias — the authors treat the results as a lower bound; manual work does not scale and can favor some models depending on phrasing.",
            "llm_theory_example": "Manual rewrite example: 'Who developed the theory of relativity?' -&gt; 'The theory of relativity was developed by [MASK]'.",
            "evaluation_results": "Paper finds that alternate templates can produce both better and worse retrieval results for the same fact, demonstrating sensitivity to human-designed phrasing.",
            "uuid": "e3985.8",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Limitations & challenges",
            "name_full": "Evaluation limitations and open challenges noted in the paper",
            "brief_description": "Concise summary of caveats the authors report about evaluating knowledge in pretrained LMs via the LAMA probe.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "N/A (meta-evaluation of limitations).",
            "evaluation_methods": "Discussion and empirical analyses (e.g., per-relation breakdowns, sensitivity to templates, rank-distribution over multiple mentions).",
            "benchmark_or_dataset": "Applies to all LAMA knowledge sources and evaluation procedures described in the paper.",
            "metrics_reported": "Empirical observations (e.g., poor performance on N-to-M relations, variability across templates, correlation coefficients) rather than a single new metric.",
            "human_involvement": "Noted that manual template creation influences results and that multi-token answer evaluation was intentionally omitted due to complexity.",
            "limitations_or_challenges": "Template sensitivity (results are a lower bound); restriction to single-token objects omits many facts; multi-token decoding for bidirectional models is an open challenge; possible memorization vs true understanding (models trained on Wikipedia may have seen test sentences); vocabulary intersection can affect difficulty; noisy alignments for some datasets; entity-linking is a bottleneck for RE baselines.",
            "llm_theory_example": "N/A (limitation-level commentary).",
            "evaluation_results": "Authors conclude LAMA provides insight but has clear boundaries: it shows BERT-large contains substantial factual/commonsense knowledge, yet extracting multi-token and N-M relation knowledge reliably remains unsolved and sensitive to evaluation design.",
            "uuid": "e3985.9",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Example: BERT-large evaluation",
            "name_full": "BERT-large performance examples reported in this paper",
            "brief_description": "Empirical evaluation results showing BERT-large's relative strength at recalling factual and commonsense knowledge in cloze settings compared to other LMs and to symbolic baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "P@1 and P@k performance on cloze queries derived from Google-RE, T-REx, ConceptNet, and a SQuAD subset; robustness to template phrasing.",
            "evaluation_methods": "Use LAMA probe cloze queries; compute per-relation and aggregated mean P@1 and P@k; analyze rank distributions across multiple mentions and compute Pearson correlations with auxiliary statistics.",
            "benchmark_or_dataset": "Evaluated on LAMA's datasets (Google-RE, T-REx, ConceptNet, SQuAD subset).",
            "metrics_reported": "Selected reported numbers: Google-RE total P@1 = 10.5% (BERT-large), T-REx total P@1 = 32.3%, ConceptNet P@1 = 19.2%, SQuAD subset P@1 = 17.4%; P@10 (SQuAD cloze) ≈ 57.1% for BERT-large.",
            "human_involvement": "No fine-tuning or human labels during model querying; humans authored templates and selected SQuAD subset.",
            "limitations_or_challenges": "Authors note that high performance may partially reflect memorization of Wikipedia contexts BERT saw during training rather than generalizable understanding; sensitivity to phrasing and single-token constraints apply.",
            "llm_theory_example": "Concrete generation examples in Table 3 show BERT-large top predictions and log-probabilities for masked objects (e.g., 'Francesco Bartolomeo Conti was born in [MASK]' yields 'Rome' and 'Florence' among top tokens with associated log probs).",
            "evaluation_results": "BERT-large consistently best among tested LMs and competitive with a KB built by a supervised RE system with oracle linking on many relations, but with notable weaknesses on N-to-M relations and multi-token answers.",
            "uuid": "e3985.10",
            "source_info": {
                "paper_title": "Language Models as Knowledge Bases?",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "rating": 2
        },
        {
            "paper_title": "Reading wikipedia to answer open-domain questions",
            "rating": 2
        },
        {
            "paper_title": "Context-aware representations for knowledge base relation extraction",
            "rating": 2
        },
        {
            "paper_title": "T-rex: A large scale alignment of natural language with knowledge base triples",
            "rating": 2
        },
        {
            "paper_title": "Representing general relational knowledge in conceptnet 5",
            "rating": 1
        },
        {
            "paper_title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
            "rating": 1
        },
        {
            "paper_title": "Language models are unsupervised multitask learners",
            "rating": 1
        }
    ],
    "cost": 0.017458249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Language Models as Knowledge Bases?</h1>
<p>Fabio Petroni ${ }^{1}$ Tim Rocktäschel ${ }^{1,2}$ Patrick Lewis ${ }^{1,2}$ Anton Bakhtin ${ }^{1}$<br>Yuxiang Wu ${ }^{1,2}$ Alexander H. Miller ${ }^{1}$ Sebastian Riedel ${ }^{1,2}$<br>${ }^{1}$ Facebook AI Research<br>${ }^{2}$ University College London<br>{fabiopetroni, rockt, plewis, yolo, yuxiangwu, ahm, sriedel}@fb.com</p>
<h4>Abstract</h4>
<p>Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-theart pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https: //github.com/facebookresearch/LAMA.</p>
<h2>1 Introduction</h2>
<p>Recently, pretrained high-capacity language models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018a) have become increasingly important in NLP. They are optimised to either predict the next word in a sequence or some masked word anywhere in a given sequence (e.g. "Dante was born in [Mask] in the year 1265."). The parameters of these models appear to store
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Querying knowledge bases (KB) and language models (LM) for factual knowledge.
vast amounts of linguistic knowledge (Peters et al., 2018b; Goldberg, 2019; Tenney et al., 2019) useful for downstream tasks. This knowledge is usually accessed either by conditioning on latent context representations produced by the original model or by using the original model weights to initialize a task-specific model which is then further fine-tuned. This type of knowledge transfer is crucial for current state-of-the-art results on a wide range of tasks.</p>
<p>In contrast, knowledge bases are effective solutions for accessing annotated gold-standard relational data by enabling queries such as (Dante, born-in, $\mathbf{X}$ ). However, in practice we often need to extract relational data from text or other modalities to populate these knowledge bases. This requires complex NLP pipelines involving entity extraction, coreference resolution, entity linking and relation extraction (Surdeanu and Ji, 2014)components that often need supervised data and fixed schemas. Moreover, errors can easily propagate and accumulate throughout the pipeline. Instead, we could attempt to query neural language models for relational data by asking them to fill in masked tokens in sequences like "Dante was born</p>
<p>in [Mask]", as illustrated in Figure 1. In this setting, language models come with various attractive properties: they require no schema engineering, do not need human annotations, and they support an open set of queries.</p>
<p>Given the above qualities of language models as potential representations of relational knowledge, we are interested in the relational knowledge already present in pretrained off-the-shelf language models such as ELMo and BERT. How much relational knowledge do they store? How does this differ for different types of knowledge such as facts about entities, common sense, and general question answering? How does their performance without fine-tuning compare to symbolic knowledge bases automatically extracted from text? Beyond gathering a better general understanding of these models, we believe that answers to these questions can help us design better unsupervised knowledge representations that could transfer factual and commonsense knowledge reliably to downstream tasks such as commonsense (visual) question answering (Zellers et al., 2018; Talmor et al., 2019) or reinforcement learning (Branavan et al., 2011; Chevalier-Boisvert et al., 2018; Bahdanau et al., 2019; Luketina et al., 2019).</p>
<p>For the purpose of answering the above questions we introduce the LAMA (LAnguage Model Analysis) probe, consisting of a set of knowledge sources, each comprised of a set of facts. We define that a pretrained language model knows a fact (subject, relation, object) such as (Dante, born-in, Florence) if it can successfully predict masked objects in cloze sentences such as "Dante was born in $\qquad$ " expressing that fact. We test for a variety of types of knowledge: relations between entities stored in Wikidata, common sense relations between concepts from ConceptNet, and knowledge necessary to answer natural language questions in SQuAD. In the latter case we manually map a subset of SQuAD questions to cloze sentences.</p>
<p>Our investigation reveals that (i) the largest BERT model from Devlin et al. (2018b) (BERT-large) captures (accurate) relational knowledge comparable to that of a knowledge base extracted with an off-the-shelf relation extractor and an oracle-based entity linker from a corpus known to express the relevant knowledge, (ii) factual knowledge can be recovered surprisingly well from pretrained language mod-
els, however, for some relations (particularly $N$-to- $M$ relations) performance is very poor, (iii) BERT-large consistently outperforms other language models in recovering factual and commonsense knowledge while at the same time being more robust to the phrasing of a query, and (iv) BERT-large achieves remarkable results for open-domain QA, reaching $57.1 \%$ precision@10 compared to $63.5 \%$ of a knowledge base constructed using a task-specific supervised relation extraction system.</p>
<h2>2 Background</h2>
<p>In this section we provide background on language models. Statistics for the models that we include in our investigation are summarized in Table 1.</p>
<h3>2.1 Unidirectional Language Models</h3>
<p>Given an input sequence of tokens $\mathbf{w}=$ $\left[w_{1}, w_{2}, \ldots, w_{N}\right]$, unidirectional language models commonly assign a probability $p(\mathbf{w})$ to the sequence by factorizing it as follows</p>
<p>$$
p(\mathbf{w})=\prod_{t} p\left(w_{t} \mid w_{t-1}, \ldots, w_{1}\right)
$$</p>
<p>A common way to estimate this probability is using neural language models (Mikolov and Zweig, 2012; Melis et al., 2017; Bengio et al., 2003) with</p>
<p>$$
p\left(w_{t} \mid w_{t-1}, \ldots, w_{1}\right)=\operatorname{softmax}\left(\mathbf{W} \mathbf{h}_{t}+\mathbf{b}\right)
$$</p>
<p>where $\mathbf{h}<em t="t">{t} \in \mathbb{R}^{k}$ is the output vector of a neural network at position $t$ and $\mathbf{W} \in \mathbb{R}^{|\mathcal{V}| \times k}$ is a learned parameter matrix that maps $\mathbf{h}</em>$ given the word history, e.g., by using a multi-layer perceptron (Bengio et al., 2003; Mikolov and Zweig, 2012), convolutional layers (Dauphin et al., 2017), recurrent neural networks (Zaremba et al., 2014; Merity et al., 2016; Melis et al., 2017) or self-attention mechanisms (Radford et al., 2018; Dai et al., 2019; Radford et al., 2019).
fairseq-fconv: Instead of commonly used recurrent neural networks, Dauphin et al. (2017) use multiple layers of gated convolutions. We use the pretrained model in the fairseq ${ }^{1}$ library in our study. It has been trained on the WikiText-103 corpus introduced by Merity et al. (2016).}$ to unnormalized scores for every word in the vocabulary $\mathcal{V}$. Various neural language models then mainly differ in how they compute $\mathbf{h}_{t</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Base Model</th>
<th style="text-align: center;">#Parameters</th>
<th style="text-align: center;">Training Corpus</th>
<th style="text-align: center;">Corpus Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">fairseq-fconv (Dauphin et al., 2017)</td>
<td style="text-align: center;">ConvNet</td>
<td style="text-align: center;">324 M</td>
<td style="text-align: center;">WikiText-103</td>
<td style="text-align: center;">103 M Words</td>
</tr>
<tr>
<td style="text-align: left;">Transformer-XL (large) (Dai et al., 2019)</td>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">257 M</td>
<td style="text-align: center;">WikiText-103</td>
<td style="text-align: center;">103 M Words</td>
</tr>
<tr>
<td style="text-align: left;">ELMo (original) (Peters et al., 2018a)</td>
<td style="text-align: center;">BiLSTM</td>
<td style="text-align: center;">93.6 M</td>
<td style="text-align: center;">Google Billion Word</td>
<td style="text-align: center;">800 M Words</td>
</tr>
<tr>
<td style="text-align: left;">ELMo 5.5B (Peters et al., 2018a)</td>
<td style="text-align: center;">BiLSTM</td>
<td style="text-align: center;">93.6 M</td>
<td style="text-align: center;">Wikipedia (en) \&amp; WMT 2008-2012</td>
<td style="text-align: center;">5.5 B Words</td>
</tr>
<tr>
<td style="text-align: left;">BERT (base) (Devlin et al., 2018a)</td>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">110 M</td>
<td style="text-align: center;">Wikipedia (en) \&amp; BookCorpus</td>
<td style="text-align: center;">3.3 B Words</td>
</tr>
<tr>
<td style="text-align: left;">BERT (large) (Devlin et al., 2018a)</td>
<td style="text-align: center;">Transformer</td>
<td style="text-align: center;">340 M</td>
<td style="text-align: center;">Wikipedia (en) \&amp; BookCorpus</td>
<td style="text-align: center;">3.3 B Words</td>
</tr>
</tbody>
</table>
<p>Table 1: Language models considered in this study.</p>
<p>Transformer-XL: Dai et al. (2019) introduce a large-scale language model based on the Transformer (Vaswani et al., 2017). Transformer-XL can take into account a longer history by caching previous outputs and by using relative instead of absolute positional encoding. It achieves a test perplexity of 18.3 on the WikiText-103 corpus.</p>
<h3>2.2 Bidirectional "Language Models"</h3>
<p>So far, we have looked at language models that predict the next word given a history of words. However, in many downstream applications we mostly care about having access to contextual representations of words, i.e., word representations that are a function of the entire context of a unit of text such as a sentence or paragraph, and not only conditioned on previous words. Formally, given an input sequence $\mathbf{w}=\left[w_{1}, w_{2}, \ldots, w_{N}\right]$ and a position $1 \leq i \leq N$, we want to estimate $p\left(w_{i}\right)=p\left(w_{i} \mid w_{1}, \ldots, w_{i-1}, w_{i+1}, \ldots, w_{N}\right)$ using the left and right context of that word.
ELMo: To estimate this probability, Peters et al. (2018a) propose running a forward and backward LSTM (Hochreiter and Schmidhuber, 1997), resulting in $\overline{\mathbf{h}}<em i="i">{i}$ and $\overline{\mathbf{h}}</em>$ which consequently are used to calculate a forward and backward language model log-likelihood. Their model, ELMo, uses multiple layers of LSTMs and it has been pretrained on the Google Billion Word dataset. Another version of the model, ELMo 5.5B, has been trained on the English Wikipedia and monolingual news crawl data from WMT 2008-2012.
BERT: Instead of a standard language model objective, Devlin et al. (2018a) propose to sample positions in the input sequence randomly and to learn to fill the word at the masked position. To this end, they employ a Transformer architecture and train it on the BookCorpus (Zhu et al., 2015) as well as a crawl of English Wikipedia. In addi-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tion to this pseudo language model objective, they use an auxiliary binary classification objective to predict whether a particular sentence follows the given sequence of words.</p>
<h2>3 Related Work</h2>
<p>Many studies have investigated pretrained word representations, sentence representations, and language models. Existing work focuses on understanding linguistic and semantic properties of word representations or how well pretrained sentence representations and language models transfer linguistic knowledge to downstream tasks. In contrast, our investigation seeks to answer to what extent pretrained language models store factual and commonsense knowledge by comparing them with symbolic knowledge bases populated by traditional relation extraction approaches.</p>
<p>Baroni et al. (2014) present a systematic comparative analysis between neural word representation methods and more traditional count-based distributional semantic methods on lexical semantics tasks like semantic relatedness and concept categorization. They find that neural word representations outperform count-based distributional methods on the majority of the considered tasks. Hill et al. (2015) investigate to what degree word representations capture semantic meaning as measured by similarity between word pairs.</p>
<p>Marvin and Linzen (2018) assess the grammaticality of pretrained language models. Their dataset consists of sentence pairs with a grammatical and an ungrammatical sentence. While a good language model should assign higher probability to the grammatical sentence, they find that LSTMs do not learn syntax well.</p>
<p>Another line of work investigates the ability of pretrained sentence and language models to transfer knowledge to downstream natural language understanding tasks (Wang et al., 2018). While such an analysis sheds light on the transfer-learning</p>
<p>abilities of pretrained models for understanding short pieces of text, it provides little insight into whether these models can compete with traditional approaches to representing knowledge like symbolic knowledge bases.</p>
<p>More recently, McCoy et al. (2019) found that for natural language inference, a model based on BERT learns to rely heavily on fallible syntactic heuristics instead of a deeper understanding of the natural language input. Peters et al. (2018b) found that lower layers in ELMo specialize on local syntactic relationships, while higher layers can learn to model long-range relationships. Similarly, Goldberg (2019) found that BERT captures English syntactic phenomena remarkably well. Tenney et al. (2019) investigate to what extent language models encode sentence structure for different syntactic and semantic phenomena and found that they excel for the former but only provide small improvements for tasks that fall into the latter category. While this provides insights into the linguistic knowledge of language models, it does not provide insights into their factual and commonsense knowledge.</p>
<p>Radford et al. (2018) introduce a pretrained language model based on the Transformer which they termed generative pretraining (GPTv1). The first version of GPT (Radford et al., 2018) has been trained on the Book Corpus (Zhu et al., 2015) containing 7000 books. The closest to our investigation is the work by Radford et al. (2019) which introduces GPTv2 and investigates how well their language model does zero-shot transfer to a range of downstream tasks. They find that GPTv2 achieves an $F_{1}$ of 55 for answering questions in CoQA (Reddy et al., 2018) and $4.1 \%$ accuracy on the Natural Questions dataset (Kwiatkowski et al., 2019), in both cases without making use of annotated question-answer pairs or an information retrieval step. While these results are encouraging and hint at the ability of very large pretrained language models to memorize factual knowledge, the large GPTv2 model has not been made public and the publicly available small version achieves less than $1 \%$ on Natural Questions ( 5.3 times worse than the large model). Thus, we decided to not include GPTv2 in our study. Similarly, we do not include GPTv1 in this study as it uses a limited lower-cased vocabulary, making it incompatible to the way we assess the other language models.</p>
<h2>4 The LAMA Probe</h2>
<p>We introduce the LAMA (LAnguage Model Analysis) probe to test the factual and commonsense knowledge in language models. It provides a set of knowledge sources which are composed of a corpus of facts. Facts are either subject-relationobject triples or question-answer pairs. Each fact is converted into a cloze statement which is used to query the language model for a missing token. We evaluate each model based on how highly it ranks the ground truth token against every other word in a fixed candidate vocabulary. This is similar to ranking-based metrics from the knowledge base completion literature (Bordes et al., 2013; Nickel et al., 2016). Our assumption is that models which rank ground truth tokens high for these cloze statements have more factual knowledge. We discuss each step in detail next and provide considerations on the probe below.</p>
<h3>4.1 Knowledge Sources</h3>
<p>To assess the different language models in Section 2, we cover a variety of sources of factual and commonsense knowledge. For each source, we describe the origin of fact triples (or questionanswer pairs), how we transform them into cloze templates, and to what extent aligned texts exist in Wikipedia that are known to express a particular fact. We use the latter information in supervised baselines that extract knowledge representations directly from the aligned text.</p>
<h3>4.1.1 Google-RE</h3>
<p>The Google-RE corpus ${ }^{3}$ contains $\sim 60 \mathrm{~K}$ facts manually extracted from Wikipedia. It covers five relations but we consider only three of them, namely "place of birth", "date of birth" and "place of death". We exclude the other two because they contain mainly multi-tokens objects that are not supported in our evaluation. We manually define a template for each considered relation, e.g., "[S] was born in [O]" for "place of birth". Each fact in the Google-RE dataset is, by design, manually aligned to a short piece of Wikipedia text supporting it.</p>
<h3>4.1.2 T-REx</h3>
<p>The T-REx knowledge source is a subset of Wikidata triples. It is derived from the T-REx</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>dataset (Elsahar et al., 2018) and is much larger than Google-RE with a broader set of relations. We consider 41 Wikidata relations and subsample at most 1000 facts per relation. As with the Google-RE corpus, we manually define a template for each relation (see Table 3 for some examples). In contrast to the Google-RE knowledge source, T-REx facts were automatically aligned to Wikipedia and hence this alignment can be noisy. However, Elsahar et al. (2018) report an accuracy of $97.8 \%$ for the alignment technique over a test set.</p>
<h3>4.1.3 ConceptNet</h3>
<p>ConceptNet (Speer and Havasi, 2012) is a multilingual knowledge base, initially built on top of Open Mind Common Sense (OMCS) sentences. OMCS represents commonsense relationships between words and/or phrases. We consider facts from the English part of ConceptNet that have single-token objects covering 16 relations. For these ConceptNet triples, we find the OMCS sentence that contains both the subject and the object. We then mask the object within the sentence and use the sentence as template for querying language models. If there are several sentences for a triple, we pick one at random. Note that for this knowledge source there is no explicit alignment of facts to Wikipedia sentences.</p>
<h3>4.1.4 SQuAD</h3>
<p>SQuAD (Rajpurkar et al., 2016) is a popular question answering dataset. We select a subset of 305 context-insensitive questions from the SQuAD development set with single token answers. We manually create cloze-style questions from these questions, e.g., rewriting "Who developed the theory of relativity?" as "The theory of relativity was developed by $\qquad$ ". For each question and answer pair, we know that the corresponding fact is expressed in Wikipedia since this is how SQuAD was created.</p>
<h3>4.2 Models</h3>
<p>We consider the following pretrained casesensitive language models in our study (see Table 1): fairseq-fconv (Fs), Transformer-XL large (Txl), ELMo original (Eb), ELMo 5.5B (E5B), BERT-base $(B b)$ and BERT-large $(B l)$. We use the natural way of generating tokens for each model by following the definition of the training objective function.</p>
<p>Assume we want to compute the generation for the token at position $t$. For unidirectional language models, we use the network output $\left(\mathbf{h}<em t-1="t-1">{t-1}\right)$ just before the token to produce the output layer softmax. For ELMo we consider the output just before $\overrightarrow{\left(\mathbf{h}</em>}\right)}$ for the forward direction and just after $\left(\overrightarrow{\mathbf{h}<em t="t">{t+1}\right)$ for the backward direction. Following the loss definition in (Peters et al., 2018a), we average forward and backward probabilities from the corresponding softmax layers. For BERT, we mask the token at position $t$, and we feed the output vector corresponding to the masked token $\left(\mathbf{h}</em>$ case-sensitive tokens).}\right)$ into the softmax layer. To allow a fair comparison, we let models generate over a unified vocabulary, which is the intersection of the vocabularies for all considered models ( $\sim 21 \mathrm{~K</p>
<h3>4.3 Baselines</h3>
<p>To compare language models to canonical ways of using off-the-shelf systems for extracting symbolic knowledge and answering questions, we consider the following baselines.
Freq: For a subject and relation pair, this baseline ranks words based on how frequently they appear as objects for the given relation in the test data. It indicates the upper bound performance of a model that always predicts the same objects for a particular relation.
RE: For the relation-based knowledge sources, we consider the pretrained Relation Extraction (RE) model of Sorokin and Gurevych (2017). This model was trained on a subcorpus of Wikipedia annotated with Wikidata relations. It extracts relation triples from a given sentence using an LSTMbased encoder and an attention mechanism. Based on the alignment information from the knowledge sources, we provide the relation extractor with the sentences known to express the test facts. Using these datasets, RE constructs a knowledge graph of triples. At test time, we query this graph by finding the subject entity and then rank all objects in the correct relation based on the confidence scores returned by RE. We consider two versions of this procedure that differ in how the entity linking is implemented: $\mathbf{R E}<em o="o">{n}$ makes use of a naïve entity linking solution based on exact string matching, while $\mathbf{R E}</em>\right)$ from that sen-}$ uses an oracle for entity linking in addition to string matching. In other words, assume we query for the object $o$ of a test subjectrelation fact $(s, r, o)$ expressed in a sentence $x$. If RE has extracted any triple $\left(s^{\prime}, r, o^{\prime</p>
<p>tence $x, s^{\prime}$ will be linked to $s$ and $o^{\prime}$ to $o$. In practice, this means RE can return the correct solution $o$ if any relation instance of the right type was extracted from $x$, regardless of whether it has a wrong subject or object.
DrQA: Chen et al. (2017) introduce DrQA, a popular system for open-domain question answering. DrQA predicts answers to natural language questions using a two step pipeline. First, a TF/IDF information retrieval step is used to find relevant articles from a large store of documents (e.g. Wikipedia). On the retrieved top $k$ articles, a neural reading comprehension model then extracts answers. To avoid giving the language models a competitive advantage, we constrain the predictions of DrQA to single-token answers.</p>
<h3>4.4 Metrics</h3>
<p>We consider rank-based metrics and compute results per relation along with mean values across all relations. To account for multiple valid objects for a subject-relation pair (i.e., for N-M relations), we follow Bordes et al. (2013) and remove from the candidates when ranking at test time all other valid objects in the training data other than the one we test. We use the mean precision at $\mathrm{k}(P @ k)$. For a given fact, this value is 1 if the object is ranked among the top k results, and 0 otherwise.</p>
<h3>4.5 Considerations</h3>
<p>There are several important design decisions we made when creating the LAMA probe. Below we give more detailed justifications for these decisions.</p>
<p>Manually Defined Templates For each relation we manually define a template that queries for the object slot in that relation. One can expect that the choice of templates has an impact on the results, and this is indeed the case: for some relations we find both worse and better ways to query for the same information (with respect to a given model) by using an alternate template. We argue that this means we are measuring a lower bound for what language models know. We make this argument by analogy with traditional knowledge bases: they only have a single way of querying knowledge for a specific relation, namely by using the relation id of that relation, and this way is used to measure their accuracy. For example, if the relation ID is works-For and the user asks for is-working-for, the accuracy of the KG would
be 0 .
Single Token We only consider single token objects as our prediction targets. The reason we include this limitation is that multi-token decoding adds a number of additional tuneable parameters (beam size, candidate scoring weights, length normalization, n-gram repetition penalties, etc.) that obscure the knowledge we are trying to measure. Moreover, well-calibrated multi-token generation is still an active research area, particularly for bidirectional models (see e.g. Welleck et al. (2019)).</p>
<p>Object Slots We choose to only query object slots in triples, as opposed to subject or relation slots. By including reverse relations (e.g. contains and contained-by) we can also query subject slots. We do not query relation slots for two reasons. First, surface form realisations of relations will span several tokens, and as we discussed above, this poses a technical challenge that is not in the scope of this work. Second, even if we could easily predict multi-token phrases, relations can generally be expressed with many different wordings, making it unclear what the gold standard pattern for a relation should be, and how to measure accuracy in this context.</p>
<p>Intersection of Vocabularies The models that we considered are trained with different vocabularies. For instance, ELMo uses a list of $\sim 800 \mathrm{~K}$ tokens while BERT considers only $\sim 30 \mathrm{~K}$ tokens. The size of the vocabulary can influence the performance of a model for the LAMA probe. Specifically, the larger the vocabulary the harder it would be to rank the gold token at the top. For this reason we considered a common vocabulary of $\sim 21 \mathrm{~K}$ case-sensitive tokens that are obtained from the intersection of the vocabularies for all considered models. To allow a fair comparison, we let every model rank only tokens in this joint vocabulary.</p>
<h2>5 Results</h2>
<p>We summarize the main results in Table 2, which shows the mean precision at one ( $\mathrm{P} @ 1$ ) for the different models across the set of corpora considered. In the remainder of this section, we discuss the results for each corpus in detail.</p>
<p>Google-RE We query the LMs using a standard cloze template for each relation. The base and large versions of BERT both outperform all other models by a substantial margin. Furthermore, they</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Corpus</th>
<th style="text-align: center;">Relation</th>
<th style="text-align: center;">Statistics</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Baselines</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">KB</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">#Facts</td>
<td style="text-align: center;">#Rel</td>
<td style="text-align: center;">Freq</td>
<td style="text-align: center;">DrQA</td>
<td style="text-align: center;">$\mathrm{RE}_{n}$</td>
<td style="text-align: center;">$\mathrm{RE}_{o}$</td>
<td style="text-align: center;">Fs</td>
<td style="text-align: center;">Txl</td>
<td style="text-align: center;">Eb</td>
<td style="text-align: center;">E5B</td>
<td style="text-align: center;">Bb</td>
<td style="text-align: center;">Bl</td>
</tr>
<tr>
<td style="text-align: center;">Google-RE</td>
<td style="text-align: center;">birth-place</td>
<td style="text-align: center;">2937</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">16.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">birth-date</td>
<td style="text-align: center;">1825</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">death-place</td>
<td style="text-align: center;">765</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">14.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">5527</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">10.5</td>
</tr>
<tr>
<td style="text-align: center;">T-REx</td>
<td style="text-align: center;">1-1</td>
<td style="text-align: center;">937</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">74.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$N-1$</td>
<td style="text-align: center;">20006</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">23.85</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">34.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$N-M$</td>
<td style="text-align: center;">13096</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">21.95</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">24.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">34039</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">22.03</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">32.3</td>
</tr>
<tr>
<td style="text-align: center;">ConceptNet</td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">11458</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">19.2</td>
</tr>
<tr>
<td style="text-align: center;">SQuAD</td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">305</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">17.4</td>
</tr>
</tbody>
</table>
<p>Table 2: Mean precision at one (P@1) for a frequency baseline (Freq), DrQA, a relation extraction with naïve entity linking $\left(\mathrm{RE}<em o="o">{n}\right)$, oracle entity linking $\left(\mathrm{RE}</em>\right)$, fairseq-fconv (Fs), Transformer-XL large (Txl), ELMo original (Eb), ELMo 5.5B (E5B), BERT-base (Bb) and BERT-large (Bl) across the set of evaluation corpora.
obtain a 2.2 and 2.9 respective average accuracy improvement over the oracle-based RE baseline. This is particularly surprising given that with the gold-aligned Google-RE source we know for certain that the oracle RE baseline has seen at least one sentence expressing each test fact. Moreover, the RE baseline was given substantial help through an entity linking oracle.</p>
<p>It is worth pointing out that while BERT-large does better, this does not mean it does so for the right reasons. Although the aligned Google-RE sentences are likely in its training set (as they are part of Wikipedia and BERT has been trained on Wikipedia), it might not "understand" them to produce these results. Instead, it could have learned associations of objects with subjects from co-occurrence patterns.</p>
<p>T-REx The knowledge source derived from Google-RE contains relatively few facts and only three relations. Hence, we perform experiments on the larger set of facts and relations in T-REx. We find that results are generally consistent with Google-RE. Again, the performance of BERT in retrieving factual knowledge are close to the performance obtained by automatically building a knowledge base with an off-the-shelf relation extraction system and oracle-based entity linking. Broken down by relation type, the performance of BERT is very high for 1-to-1 relations (e.g., capital of) and low for N-to-M relations.</p>
<p>Note that a downstream model could learn to make use of knowledge in the output representations of a language model even if the correct answer is not ranked first but high enough (i.e. a hint
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Mean P@k curve for T-REx varying k. Base10 log scale for X axis.
about the correct answer can be extracted from the output representation). Figure 2 shows the mean P@k curves for the considered models. For BERT, the correct object is ranked among the top ten in around $60 \%$ of the cases and among the top 100 in $80 \%$ of the cases.</p>
<p>To further investigate why BERT achieves such strong results, we compute the Pearson correlation coefficient between the $P @ 1$ and a set of metrics that we report in Figure 3. We notice, for instance, that the number of times an object is mentioned in the training data positively correlates with performance while the same is not true for the subject of a relation. Furthermore, the log probability of a prediction is strongly positively correlated with P@1. Thus, when BERT has a high confidence in its prediction, it is often correct. Performance is also positively correlated with the cosine similarity between subject and object vectors, and</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Pearson correlation coefficient for the P@1 of the BERT-large model on T-REx and a set of metrics: SM and OM refer to the number of times a subject and an object are mentioned in the BERT training corpus<sup>5</sup> respectively; LPFP is the log probability score associated with the first prediction; SOCS is the cosine similarity between subject and object vectors (we use spaCy<sup>5</sup>); ST and SWP are the number of tokens in the subject with a standard tokenization and the BERT WordPiece tokenization respectively.</p>
<p>slightly with the number of tokens in the subject.</p>
<p>Table 3 shows randomly picked examples for the generation of BERT-large for cloze template queries. We find that BERT-large generally predicts objects of the correct type, even when the predicted object itself is not correct.</p>
<p>To understand how the performance of a pretrained language model varies with different ways of querying for a particular fact, we analyze a maximum of 100 random facts per relation for which we randomly select 10 aligned sentences in Wikipedia from T-REx.<sup>6</sup> In each of the sentences, we mask the object of the fact, and ask the model to predict it. For several of our language models this also tests their ability to memorize and recall sentences from the training data since as the models have been trained on Wikipedia (see Table 1).</p>
<p>Figure 4 shows the average distribution of the rank for ten queries per fact. The two BERT models and ELMo 5.5B exhibit the lowest variability while ranking the correct object close to the top on average. Surprisingly, the performance of ELMo original is not far from BERT, even though this model did not see Wikipedia during training. Fairseq-fconv and Transformer-XL experi-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Average rank distribution for 10 different mentions of 100 random facts per relation in T-REx. ELMo 5.5B and both variants of BERT are least sensitive to the framing of the query but also are the most likely to have seen the query sentence during training.</p>
<p>ence a higher variability in their predictions. Note that BERT and ELMo 5.5B have been trained on a larger portion of Wikipedia than fairseq-fconv and Transformer-XL and may have seen more sentences containing the test queries during training.</p>
<p><strong>ConceptNet</strong> The results on the ConceptNet corpus are in line with those reported for retrieving factual knowledge in Google-RE and T-REx. The BERT-large model consistently achieves the best performance, and it is able to retrieve commonsense knowledge at a similar level to factual knowledge. The lower half of Table 3 shows generations by BERT-large for randomly sampled examples. Some of the concepts generated by the language models are surprisingly reasonable in addition to being syntactically correct.</p>
<p><strong>SQuAD</strong> Next we evaluate our system on open-domain cloze-style question answering and compare against the supervised DrQA model. Table 2 shows a performance gap between BERT-large and the DrQA open-domain QA system on our cloze SQuAD task. Again, note that the pretrained language model is completely unsupervised, it is not fine-tuned, and it has no access to a dedicated information retrieval system. Moreover, when comparing DrQA and BERT-large in terms of P@10, we find that gap is remarkably small (57.1 for BERT-large and 63.5 for DrQA).</p>
<h2>6 Discussion and Conclusion</h2>
<p>We presented a systematic analysis of the factual and commonsense knowledge in publicly available pretrained language models <em>as is</em> and found</p>
<p><sup>5</sup>The original training corpus is not available, we created our version using the same sources.</p>
<p><sup>5</sup>https://spacy.io</p>
<p><sup>6</sup>We exclude all facts with less than 10 alignments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relation</th>
<th style="text-align: center;">Query</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">Generation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P19</td>
<td style="text-align: center;">Francesco Bartolomeo Conti was born in $\qquad$</td>
<td style="text-align: center;">Florence</td>
<td style="text-align: center;">Rome (-1.6), Florence (-4.8), Naples (-1.9), Milan (-2.4), Bologna (-2.5)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P20</td>
<td style="text-align: center;">Adolphe Adam died in $\qquad$</td>
<td style="text-align: center;">Paris</td>
<td style="text-align: center;">Paris (-4.3), London (-3.5), Vienna (-3.6), Berlin (-3.8), Brussels (-4.0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P279</td>
<td style="text-align: center;">English building is a subclass of $\qquad$</td>
<td style="text-align: center;">dog</td>
<td style="text-align: center;">dogs (-0.1), breeds (-2.2), dog (-2.4), cattle (-4.3), sheep (-4.5)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P37</td>
<td style="text-align: center;">The official language of Mauritius is $\qquad$</td>
<td style="text-align: center;">English</td>
<td style="text-align: center;">English (-4.6), French (-0.9), Arabic (-6.2), Tamil (-6.7), Malayalam (-7.0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P413</td>
<td style="text-align: center;">Patrick Obreya plays in $\qquad$ position.</td>
<td style="text-align: center;">midfielder</td>
<td style="text-align: center;">centre (-2.0), center (-2.2), midfielder (-2.6), forward (-2.4), midfield (-2.7)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P138</td>
<td style="text-align: center;">Hamburg Airport is named after $\qquad$</td>
<td style="text-align: center;">Hamburg</td>
<td style="text-align: center;">Hess (-3.0), Hermann (-3.1), Schmidt (-3.1), Hamburg (-5.8), Ludwig (-3.3)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P364</td>
<td style="text-align: center;">The original language of Mon oncle Benjamin is $\qquad$</td>
<td style="text-align: center;">French</td>
<td style="text-align: center;">French (-0.2), Breton (-3.3), English (-3.8), Dutch (-4.2), German (-4.9)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P54</td>
<td style="text-align: center;">Dani Alves plays with $\qquad$</td>
<td style="text-align: center;">Barcelona</td>
<td style="text-align: center;">Santos (-2.4), Porto (-2.5), Sporting (-3.1), Brazil (-3.3), Portugal (-3.7)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P106</td>
<td style="text-align: center;">Paul Toungui is a $\qquad$ by profession .</td>
<td style="text-align: center;">politician</td>
<td style="text-align: center;">lawyer (-1.1), journalist (-2.4), teacher (-2.7), doctor (-3.0), physician (-3.7)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P527</td>
<td style="text-align: center;">Sodium sulfide consists of $\qquad$</td>
<td style="text-align: center;">sodium</td>
<td style="text-align: center;">water (-1.2), suffer (-1.7), sodium (-2.3), zinc (-2.8), salt (-2.9)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P102</td>
<td style="text-align: center;">Gordon Scholes is a member of the $\qquad$ political party.</td>
<td style="text-align: center;">Labor</td>
<td style="text-align: center;">Labour (-1.3), Conservative (-1.4), Green (-2.4), Liberal (-2.9), Labor (-2.9)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P530</td>
<td style="text-align: center;">Kenya maintains diplomatic relations with $\qquad$</td>
<td style="text-align: center;">Uganda</td>
<td style="text-align: center;">India (-3.0), Uganda (-3.2), Tanzania (-3.5), China (-3.8), Pakistan (-3.6)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P176</td>
<td style="text-align: center;">iPod Touch is produced by $\qquad$</td>
<td style="text-align: center;">Apple</td>
<td style="text-align: center;">Apple (-1.6), Nokia (-3.7), Sony (-2.6), Samsung (-2.6), Intel (-3.1)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P30</td>
<td style="text-align: center;">Bailey Peninsula is located in $\qquad$</td>
<td style="text-align: center;">Antarctica</td>
<td style="text-align: center;">Antarctica (-1.4), Bermuda (-2.2), Newfoundland (-2.5), Alaska (-2.7), Canada (-3.1)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P178</td>
<td style="text-align: center;">IDK is developed by $\qquad$</td>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">IBM (-2.4), Intel (-2.3), Microsoft (-2.5), HP (-1.4), Nokia (-3.3)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P1412</td>
<td style="text-align: center;">Carl III used to communicate in $\qquad$</td>
<td style="text-align: center;">Swedish</td>
<td style="text-align: center;">German (-1.6), Latin (-1.9), French (-2.4), English (-3.0), Spanish (-3.0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P17</td>
<td style="text-align: center;">Sunshine Coast, British Columbia is located in $\qquad$</td>
<td style="text-align: center;">Canada</td>
<td style="text-align: center;">Canada (-1.2), Alberta (-2.4), Yukon (-2.8), Labrador (-3.4), Victoria (-3.4)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P39</td>
<td style="text-align: center;">Pope Clement VII has the position of $\qquad$</td>
<td style="text-align: center;">pope</td>
<td style="text-align: center;">cardinal (-2.4), Pope (-2.5), pope (-2.6), President (-3.1), Chancellor (-3.2)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P284</td>
<td style="text-align: center;">Joe Cocker is represented by music label $\qquad$</td>
<td style="text-align: center;">Capitol</td>
<td style="text-align: center;">EMI (-2.6), BMG (-2.4), Universal (-2.8), Capitol (-3.2), Columbia (-3.3)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P276</td>
<td style="text-align: center;">London Jazz Festival is located in $\qquad$</td>
<td style="text-align: center;">London</td>
<td style="text-align: center;">London (-0.3), Greenwich (-3.2), Chelsea (-4.0), Condon (-4.6), Stratford (-4.4)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P127</td>
<td style="text-align: center;">Border TV is owned by $\qquad$</td>
<td style="text-align: center;">ITV</td>
<td style="text-align: center;">Sky (-3.1), ITV (-4.3), Global (-3.4), Frontier (-4.1), Disney (-4.3)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P103</td>
<td style="text-align: center;">The native language of Mammoorty is $\qquad$</td>
<td style="text-align: center;">Malayalam</td>
<td style="text-align: center;">Malayalam (-4.2), Tamil (-2.1), Telugu (-4.8), English (-5.2), Hindi (-5.6)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P495</td>
<td style="text-align: center;">The Sharon Cunera Show was created in $\qquad$</td>
<td style="text-align: center;">Philippines</td>
<td style="text-align: center;">Manila (-3.2), Philippines (-3.6), February (-3.7), December (-3.8), Argentina (-4.0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Adj.ocation</td>
<td style="text-align: center;">You are likely to find a overflow in a $\qquad$</td>
<td style="text-align: center;">drain</td>
<td style="text-align: center;">sewer (-3.1), canal (-3.2), toilet (-3.3), stream (-3.6), drain (-3.6)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CapableOf</td>
<td style="text-align: center;">Kuvens can $\qquad$</td>
<td style="text-align: center;">fly</td>
<td style="text-align: center;">fly (-1.3), fight (-1.6), kill (-2.2), die (-3.2), hunt (-3.4)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CausesDesire</td>
<td style="text-align: center;">Joke would make you want to $\qquad$</td>
<td style="text-align: center;">laugh</td>
<td style="text-align: center;">cry (-1.7), die (-1.7), laugh (-2.0), vomit (-2.6), scream (-2.6)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Causes</td>
<td style="text-align: center;">Sometimes virus causes $\qquad$</td>
<td style="text-align: center;">infection</td>
<td style="text-align: center;">disease (-1.2), cancer (-2.0), infection (-2.6), plague (-2.3), fever (-3.4)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HasA</td>
<td style="text-align: center;">Birds have $\qquad$</td>
<td style="text-align: center;">feathers</td>
<td style="text-align: center;">wings (-1.8), nests (-3.1), feathers (-3.2), died (-3.7), eggs (-3.9)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HasPretrogniste</td>
<td style="text-align: center;">Typing requires $\qquad$</td>
<td style="text-align: center;">speed</td>
<td style="text-align: center;">patience (-3.5), precision (-3.6), registration (-3.8), accuracy (-4.0), speed (-4.1)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HasProperty</td>
<td style="text-align: center;">Time is $\qquad$</td>
<td style="text-align: center;">finite</td>
<td style="text-align: center;">short (-1.7), passing (-1.8), precious (-2.9), irrelevant (-3.2), gone (-4.0)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MotivatedByGoal</td>
<td style="text-align: center;">You would celebrate because you are $\qquad$</td>
<td style="text-align: center;">alive</td>
<td style="text-align: center;">happy (-2.4), human (-3.3), alive (-4.0), young (-3.6), free (-3.9)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ReservesAction</td>
<td style="text-align: center;">Skills can be $\qquad$</td>
<td style="text-align: center;">taught</td>
<td style="text-align: center;">acquired (-2.5), useful (-2.5), learned (-2.8), combined (-3.9), varied (-3.9)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UsedFor</td>
<td style="text-align: center;">A pond is for $\qquad$</td>
<td style="text-align: center;">fish</td>
<td style="text-align: center;">swimming (-1.3), fishing (-1.4), bathing (-2.0), fish (-2.8), recreation (-3.1)</td>
</tr>
</tbody>
</table>
<p>Table 3: Examples of generation for BERT-large. The last column reports the top five tokens generated together with the associated log probability (in square brackets).
that BERT-large is able to recall such knowledge better than its competitors and at a level remarkably competitive with non-neural and supervised alternatives. Note that we did not compare the ability of the corresponding architectures and objectives to capture knowledge in a given body of text but rather focused on the knowledge present in the weights of existing pretrained models that are being used as starting points for many researchers' work. Understanding which aspects of data our commonly-used models and learning algorithms are capturing is a crucial field of research and this paper complements the many studies focused on the learned linguistic properties of the data.</p>
<p>We found that it is non-trivial to extract a knowledge base from text that performs on par to directly using pretrained BERT-large. This is despite providing our relation extraction baseline with only data that is likely expressing target facts, thus reducing potential for false negatives, as well as using a generous entity-linking oracle. We suspected BERT might have an advantage due to the larger amount of data it has processed, so we added Wikitext-103 as additional data to the relation extraction system and observed no significant change in performance. This suggests that while relation extraction performance might be difficult to improve with more data, language mod-
els trained on ever growing corpora might become a viable alternative to traditional knowledge bases extracted from text in the future.</p>
<p>In addition to testing future pretrained language models using the LAMA probe, we are interested in quantifying the variance of recalling factual knowledge with respect to varying natural language templates. Moreover, assessing multi-token answers remains an open challenge for our evaluation setup.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank the reviewers for their thoughtful comments and efforts towards improving our manuscript. In addition, we would like to acknowledge three frameworks that were used in our experiments: AllenNLP ${ }^{7}$, Fairseq ${ }^{8}$ and the Hugging Face PyTorch-Transformers ${ }^{9}$ library.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Pushmeet Kohli, and Edward Grefenstette. 2019. Learning to understand goal specifications by</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>modelling reward. In International Conference on Learning Representations (ICLR).</p>
<p>Marco Baroni, Georgiana Dinu, and Germán Kruszewski. 2014. Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 1: Long Papers, pages 238-247.</p>
<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137-1155.</p>
<p>Antoine Bordes, Nicolas Usunier, Alberto GarcíaDurán, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multirelational data. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 27872795.
S. R. K. Branavan, David Silver, and Regina Barzilay. 2011. Learning to win by reading manuals in a monte-carlo framework. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 268-277.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. CoRR, abs/1704.00051.</p>
<p>Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. 2018. Babyai: First steps towards grounded language learning with a human in the loop. CoRR, abs/1810.08272.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. CoRR, abs/1901.02860.</p>
<p>Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017. Language modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 933-941.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018a. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018b. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs]. ArXiv: 1810.04805.</p>
<p>Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. T-rex: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018).</p>
<p>Yoav Goldberg. 2019. Assessing bert's syntactic abilities. CoRR, abs/1901.05287.</p>
<p>Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. Computational Linguistics, 41(4):665-695.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735-1780.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Rhinehart, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, et al. 2019. Natural questions: a benchmark for question answering research.</p>
<p>Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, and Tim Rocktäschel. 2019. A Survey of Reinforcement Learning Informed by Natural Language. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, August 10-16 2019, Macao, China.</p>
<p>Rebecca Marvin and Tal Linzen. 2018. Targeted syntactic evaluation of language models. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 1192-1202.
R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference.</p>
<p>Gábor Melis, Chris Dyer, and Phil Blunsom. 2017. On the state of the art of evaluation in neural language models. CoRR, abs/1707.05589.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. CoRR, abs/1609.07843.</p>
<p>Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. In 2012 IEEE Spoken Language Technology Workshop (SLT), Miami, FL, USA, December 2-5, 2012, pages 234-239.</p>
<p>Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. 2016. A review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11-33.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 2227-2237.</p>
<p>Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 1499-1509.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Siva Reddy, Danqi Chen, and Christopher D. Manning. 2018. Coqa: A conversational question answering challenge. CoRR, abs/1808.07042.</p>
<p>Daniil Sorokin and Iryna Gurevych. 2017. Contextaware representations for knowledge base relation extraction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 1784-1789.</p>
<p>Robert Speer and Catherine Havasi. 2012. Representing general relational knowledge in conceptnet 5. In LREC, pages 3679-3686.</p>
<p>Mihai Surdeanu and Heng Ji. 2014. Overview of the English Slot Filling Track at the TAC2014 Knowledge Base Population Evaluation. page 15.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4149-4158.</p>
<p>Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? probing for sentence structure in contextualized word representations. In International Conference on Learning Representations.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 6000-6010.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP@EMNLP 2018, Brussels, Belgium, November 1, 2018, pages 353-355.</p>
<p>Sean Welleck, Kianté Brantley, Hal Daumé III, and Kyunghyun Cho. 2019. Non-monotonic sequential text generation. arXiv preprint arXiv:1902.02192.</p>
<p>Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization. CoRR, abs/1409.2329.</p>
<p>Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2018. From recognition to cognition: Visual commonsense reasoning. CoRR, abs/1811.10830.</p>
<p>Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 1927.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/allenai/allennlp
${ }^{8}$ https://github.com/pytorch/fairseq
${ }^{9}$ https://github.com/huggingface/
pytorch-transformers&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
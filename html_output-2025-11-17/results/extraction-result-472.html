<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-472 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-472</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-472</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-67870215</p>
                <p><strong>Paper Title:</strong> Automatic Acquisition of Annotated Training Corpora for Test-Code Generation</p>
                <p><strong>Paper Abstract:</strong> : Open software repositories make large amounts of source code publicly available. Potentially, this source code could be used as training data to develop new, machine learning-based programming tools. For many applications, however, raw code scraped from online repositories does not constitute an adequate training dataset. Building on the recent and rapid improvements in machine translation (MT), one possibly very interesting application is code generation from natural language descriptions. One of the bottlenecks in developing these MT-inspired systems is the acquisition of parallel text-code corpora required for training code-generative models. This paper addresses the problem of automatically synthetizing parallel text-code corpora in the software testing domain. Our approach is based on the observation that self-documentation through descriptive method names is widely adopted in test automation, in particular for unit testing. Therefore, we propose synthesizing parallel corpora comprised of parsed test function names serving as code descriptions, aligned with the corresponding function bodies. We present the results of applying one of the state-of-the-art MT methods on such a generated dataset. Our experiments show that a neural MT model trained on our dataset can generate syntactically correct and semantically relevant short Java functions from quasi-natural language descriptions of functionality.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e472.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e472.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q&A noise / informal NL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Noise and informality in natural language descriptions mined from Q&A websites (e.g., StackOverflow titles)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Natural language descriptions mined from Q&A sites often contain irrelevant, informal, or underspecified titles and queries that do not faithfully describe the paired code snippet, causing a mismatch between NL description and code implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Systematically Mined Question-Code Dataset from Stack Overflow</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Big-code Q&A-derived text-code corpora pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipelines that mine question titles/queries and matching answer code snippets from Q&A sites (StackOverflow) to build text-code parallel corpora for training text↔code models.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Q&A question titles and search-engine queries</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Code snippets from answers (often partial/incomplete)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous/irrelevant natural language (high noise)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Question titles and queries can be informal, context-dependent, or unrelated to the code provided; as a result the NL text does not reliably describe the code snippet's semantics or intent.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>dataset acquisition / annotation (source of NL descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual annotation/inspection and reporting in related work; training a classifier on human-annotated seed data to detect irrelevant titles</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Indirectly measured via downstream model performance (BLEU) and by manual curation statistics; e.g., cleaned corpora used in cited work led to models with BLEU scores reported in those studies</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Causes lower model performance and noisy supervision; cited examples: neural models trained on noisy Q&A-derived corpora achieved low BLEU (e.g., BLEU=11 and BLEU~14 reported for some Q&A-derived corpora), indicating poor alignment between NL and code.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as 'very high' level of noise in Q&A-derived datasets; large-scale Q&A corpora require extensive cleaning to be useful.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>User-generated, informal text; titles are short, contextual, and often do not contain a standalone semantic description; code snippets are context-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Manual annotation of seed data and semi-supervised classifiers to filter irrelevant pairs; engineered heuristics; curated subsets (human review).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partial improvement: filtered/curated datasets used in cited work produced higher-quality corpora and improved downstream metrics (examples: cleaned C#/SQL corpora trained models reported BLEU=20.5 and 18.4 after filtering).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / program synthesis (text-code corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e472.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e472.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Incomplete/non-compilable code snippets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Incomplete or non-compilable code fragments in mined corpora (Q&A-derived snippets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Code snippets harvested from informal sources are often partial or non-compilable, creating a semantic and structural mismatch between the NL description and a runnable code implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Systematically Mined Question-Code Dataset from Stack Overflow</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Q&A-to-corpus mining pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Systems that pair user queries/titles with code snippets from Q&A sites to form text-code training pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Q&A titles / search queries</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Partial code snippets extracted from answers</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete implementation / missing context</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Snippets lack surrounding context (imports, class definitions, helper functions) or are deliberately minimal, so the code does not map cleanly to the NL description as a standalone, executable unit.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>dataset content (code completeness) and preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual inspection; engineered syntactic/structural heuristics (e.g., code-validity checks) as described in related work</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Filtered corpus sizes and downstream performance (BLEU) reported before/after cleaning; manual annotation of subset to estimate proportion of usable snippets (examples in related work show multi-step filtering and annotation)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Lower quality supervision for models, leading to poorer code generation performance (referenced low BLEU scores for corpora with many incomplete snippets).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common in Q&A-mined datasets; motivating the multi-stage cleaning efforts reported in prior studies.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Nature of Q&A answers (showing only the essential fragment) and lack of explicit API/class scaffolding in snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use structural heuristics to determine syntactic validity; manual annotation to build classifiers; discard non-standalone snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Improved downstream performance in cited works after filtering; reported BLEU improvements in curated/filtered subsets (e.g., models trained on curated subsets outperform fully noisy sets).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e472.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e472.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quasi-NL vs true NL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quasi-natural language descriptions produced from self-documenting function/class names</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Function and class names split into words create a controlled/quasi-natural language that is consistent and compact but is not equivalent to free-form natural language, producing a mismatch with expectations of pure NLP-trained models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-documenting-code-derived corpus and NMT-based test-code generator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A pipeline that extracts JUnit test function and class names (split into words) as NL descriptions and aligns them with their Java test function bodies to create training data for sequence-to-sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>Parsed function and class names (quasi-natural language / controlled natural language)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Java unit test function bodies (Junit)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>controlled/quasi-NL vs free-form NL (expressiveness mismatch)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Descriptions are compact, follow naming conventions (camelCase split), and lack full natural-language syntax; they may omit context that a free-form description would include, which could limit generalization to arbitrary NL inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training data / source-language representation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Conceptual analysis and empirical evaluation: the paper observes this mismatch and explicitly discusses it; qualitative review of generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Indirect: downstream BLEU and exact-match statistics when training NMT on these quasi-NL corpora (paper reports BLEU and % exact matches per dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Positive in this domain: the controlled nature yields consistent patterns that help the model (high BLEU for rx=82.44), but it may limit applicability to unrestricted natural language inputs and generalization across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>This is the central design choice of the paper (applies to test suites with self-documenting names); prevalence depends on codebase adherence to 'clean code' naming conventions.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Design choice / software engineering practice (developers using meaningful, convention-following names) producing non-freeform NL.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Treat the NL as a controlled language (acceptable for test-generation tasks); future work recommended to test generalization to free-form NL or richer annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective for unit-test generation in the experiments: models trained on these corpora achieved BLEU scores (rx=82.44, sf=58.00, multi-min10=50.50) and non-trivial exact-match rates (rx: 47.9% exact matches on test set).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>software engineering / machine learning for code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e472.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e472.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU mismatch for code</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limitations and misalignment of BLEU metric when used to evaluate generated source code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BLEU, developed for natural language MT, may not faithfully reflect semantic or functional correctness of generated code, producing a discrepancy between metric-reported quality and true code utility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NMT-based code generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Model evaluation pipeline using BLEU as the principal automatic metric to assess similarity between generated code and ground-truth code sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>function/class name descriptions (quasi-NL) used as inputs</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>generated Java test function bodies</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>evaluation metric mismatch / inadequate metric</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>BLEU compares n-gram overlap and may not capture syntactic correctness, compilation status, functional equivalence, or the practical usefulness of generated code; there are no empirical studies validating BLEU for source code.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation / metrics</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Critical discussion and qualitative manual inspection of generated outputs; noting literature gap (no empirical validation of BLEU for code).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Paper reports BLEU scores and complements them with qualitative, example-based manual analysis and exact-match counts to better contextualize results.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Could mislead about model quality; authors therefore supplement BLEU with manual review and exact-match percentages (e.g., rx BLEU=82.44 with 47.9% exact matches; sf BLEU=58 with 8% exact matches).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>BLEU is widely used in related work as reported in the paper, but the paper cautions against overreliance on it for code tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>BLEU's origin as an NLP metric that measures surface n-gram overlap rather than semantic/functional equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Complement BLEU with qualitative manual review, exact-match statistics, and planning human evaluation of generated code; call for future human-feedback-based evaluation (Test Recommendation Engine and collaboration with test teams).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Manual review provided additional insight and revealed substantive differences between datasets (e.g., rx high exact-match fraction vs sf and multi lower rates), demonstrating that BLEU alone is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine translation applied to source code / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e472.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e472.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vocabulary / rare-token gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vocabulary mismatch and rare-token handling between NL and PL vocabularies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large, project-specific programming language vocabularies introduce many rare tokens (API names, identifiers) that create gaps between training language models and realistic code generation; rare tokens can negatively affect model performance if not handled.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>corpus preprocessing and vocabulary selection pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Preprocessing step that tokenizes code and builds vocabularies for both NL (parsed names) and PL (code tokens), with options to limit PL vocabulary by frequency thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>class and method names (quasi-NL)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Java test function tokens (PL)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>vocabulary mismatch / rare-token sparsity</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Programming APIs and project-specific identifiers create a long tail of rare tokens; mapping rare tokens to an unknown token reduces vocabulary size but may discard important semantics needed to generate correct code.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>preprocessing / tokenization / vocabulary construction</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical: datasets show very large PL vocabularies; authors experimented with limiting PL vocabulary for multi-repo corpus and observed changes in BLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Compare BLEU across versions with differing vocabulary cutoffs (paper reports multi-min5 BLEU=48.85 and multi-min10 BLEU=50.50), and report vocabulary sizes (e.g., PL vocab sizes listed in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Vocabulary pruning had non-trivial effects: multi-min10 slightly outperformed multi-min5 in BLEU despite higher unknown token rate, indicating non-obvious interactions between vocabulary size and model performance in code domain.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Universal in Big Code datasets due to diverse APIs; explicit statistics provided in paper (large PL vocabularies for project corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Heterogeneity of APIs and identifiers across projects and scarcity of many tokens in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Map rare tokens to an unknown token, experiment with frequency thresholds, and keep full vocabularies for project-specific corpora when feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Mixed: in this paper, raising the minimum occurrence threshold (n=10) produced a small BLEU increase (multi-min10 BLEU=50.50 vs multi-min5 BLEU=48.85), but authors caution that hyperparameter tuning was limited and results are not definitive.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning for code / corpus preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e472.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e472.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Naming inconsistency / meaningless names</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meaningless or inconsistent function/method names in code (e.g., test1(), test2()) causing poor NL descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Some codebases contain non-descriptive, autogenerated, or placeholder function names that fail to provide useful NL descriptions for aligning with code, producing mismatches in text-code corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>self-documenting-code-based corpus synthesis pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The extraction and preprocessing pipeline that converts camelCase/snake_case class and method names into space-separated quasi-NL descriptions paired with tokenized function bodies.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>parsed method and class names</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Java test function bodies</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>meaningless or inconsistent naming (insufficient NL description)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Some test functions use meaningless names (e.g., test1()) that do not carry semantic content; such pairs produce poor or misleading NL descriptions for the associated code body.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>dataset acquisition / NL description quality</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Heuristic filtering implemented in preprocessing (simple heuristics to exclude meaningless names); manual inspection during dataset assembly.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Counts of excluded functions (not explicitly quantified numerically in paper) and downstream model performance after exclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Including meaningless names would add noisy pairs and degrade model learning; the authors removed such functions to improve corpus quality.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Some occurrences across repositories; authors applied heuristics to exclude them but did not provide exact prevalence statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Developers using placeholder names or lacking descriptive naming practices in some code.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Apply heuristics to exclude functions with meaningless names during dataset creation (e.g., pattern matching for 'test1', 'test2').</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Qualitatively effective as part of preprocessing; no explicit quantitative before/after numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>software engineering / dataset curation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e472.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e472.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cross-project generalization gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lack of evidence for effective cross-project transfer / use of information across repositories in multi-repository models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models trained on multi-repository corpora did not show clear evidence of leveraging cross-project regularities; correct generation often relied on examples from the same parent project, indicating a generalization gap across projects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>multi-repository NMT training pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An aggregated corpus constructed from parsed test cases across >700 repositories, used to train a generic NMT model for test-code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>parsed class and method names (quasi-NL) aggregated across many projects</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Java unit test function bodies from many projects</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>domain shift / lack of cross-project generalization</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although the multi-repo model produced correct outputs for some test examples, analysis showed correct predictions were typically supported by examples from the same project rather than by cross-project learning, suggesting limited transfer across different codebases.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model generalization / training data heterogeneity</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Post-hoc analysis of fully matched outputs: tracing which training examples the model appeared to exploit (authors inspected matched outputs and their training-origin projects).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Exact-match counts and inspection: multi model had 2,908/30,039 (9.7%) exact matches; authors examined origins and found evidence that correct predictions were often based on examples from same parent project.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Indicates that multi-repo generic models may require more within-project examples to be effective; generic model BLEU was lower than project-specific BLEUs (multi BLEU ~48.85–50.50 vs rx=82.44, sf=58.00).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in this study's empirical analysis of multi-repo dataset; prevalence depends on inter-project similarity and amount of per-project data.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>High heterogeneity of APIs, naming conventions, and domain-specific tokens across projects; insufficient cross-project shared patterns for the model to exploit.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Two suggested use cases: (1) train project-specific models for mature projects to leverage intra-project regularities; (2) accept generic model will be less accurate and require editing; future work to explore vocabulary and domain adaptation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Project-specific models demonstrated substantially better BLEU and exact-match behavior in experiments (rx and sf outperform multi), indicating that project specialization is an effective mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning for code / transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to mine aligned code and natural language pairs from stack overflow <em>(Rating: 2)</em></li>
                <li>A Systematically Mined Question-Code Dataset from Stack Overflow <em>(Rating: 2)</em></li>
                <li>A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation <em>(Rating: 2)</em></li>
                <li>Learning to generate pseudo-code from source code using statistical machine translation <em>(Rating: 1)</em></li>
                <li>Latent Predictor Networks for Code Generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-472",
    "paper_id": "paper-67870215",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Q&A noise / informal NL",
            "name_full": "Noise and informality in natural language descriptions mined from Q&A websites (e.g., StackOverflow titles)",
            "brief_description": "Natural language descriptions mined from Q&A sites often contain irrelevant, informal, or underspecified titles and queries that do not faithfully describe the paired code snippet, causing a mismatch between NL description and code implementation.",
            "citation_title": "A Systematically Mined Question-Code Dataset from Stack Overflow",
            "mention_or_use": "mention",
            "system_name": "Big-code Q&A-derived text-code corpora pipeline",
            "system_description": "Pipelines that mine question titles/queries and matching answer code snippets from Q&A sites (StackOverflow) to build text-code parallel corpora for training text↔code models.",
            "nl_description_type": "Q&A question titles and search-engine queries",
            "code_implementation_type": "Code snippets from answers (often partial/incomplete)",
            "gap_type": "ambiguous/irrelevant natural language (high noise)",
            "gap_description": "Question titles and queries can be informal, context-dependent, or unrelated to the code provided; as a result the NL text does not reliably describe the code snippet's semantics or intent.",
            "gap_location": "dataset acquisition / annotation (source of NL descriptions)",
            "detection_method": "Manual annotation/inspection and reporting in related work; training a classifier on human-annotated seed data to detect irrelevant titles",
            "measurement_method": "Indirectly measured via downstream model performance (BLEU) and by manual curation statistics; e.g., cleaned corpora used in cited work led to models with BLEU scores reported in those studies",
            "impact_on_results": "Causes lower model performance and noisy supervision; cited examples: neural models trained on noisy Q&A-derived corpora achieved low BLEU (e.g., BLEU=11 and BLEU~14 reported for some Q&A-derived corpora), indicating poor alignment between NL and code.",
            "frequency_or_prevalence": "Described as 'very high' level of noise in Q&A-derived datasets; large-scale Q&A corpora require extensive cleaning to be useful.",
            "root_cause": "User-generated, informal text; titles are short, contextual, and often do not contain a standalone semantic description; code snippets are context-dependent.",
            "mitigation_approach": "Manual annotation of seed data and semi-supervised classifiers to filter irrelevant pairs; engineered heuristics; curated subsets (human review).",
            "mitigation_effectiveness": "Partial improvement: filtered/curated datasets used in cited work produced higher-quality corpora and improved downstream metrics (examples: cleaned C#/SQL corpora trained models reported BLEU=20.5 and 18.4 after filtering).",
            "domain_or_field": "machine learning / program synthesis (text-code corpora)",
            "reproducibility_impact": true,
            "uuid": "e472.0"
        },
        {
            "name_short": "Incomplete/non-compilable code snippets",
            "name_full": "Incomplete or non-compilable code fragments in mined corpora (Q&A-derived snippets)",
            "brief_description": "Code snippets harvested from informal sources are often partial or non-compilable, creating a semantic and structural mismatch between the NL description and a runnable code implementation.",
            "citation_title": "A Systematically Mined Question-Code Dataset from Stack Overflow",
            "mention_or_use": "mention",
            "system_name": "Q&A-to-corpus mining pipelines",
            "system_description": "Systems that pair user queries/titles with code snippets from Q&A sites to form text-code training pairs.",
            "nl_description_type": "Q&A titles / search queries",
            "code_implementation_type": "Partial code snippets extracted from answers",
            "gap_type": "incomplete implementation / missing context",
            "gap_description": "Snippets lack surrounding context (imports, class definitions, helper functions) or are deliberately minimal, so the code does not map cleanly to the NL description as a standalone, executable unit.",
            "gap_location": "dataset content (code completeness) and preprocessing",
            "detection_method": "Manual inspection; engineered syntactic/structural heuristics (e.g., code-validity checks) as described in related work",
            "measurement_method": "Filtered corpus sizes and downstream performance (BLEU) reported before/after cleaning; manual annotation of subset to estimate proportion of usable snippets (examples in related work show multi-step filtering and annotation)",
            "impact_on_results": "Lower quality supervision for models, leading to poorer code generation performance (referenced low BLEU scores for corpora with many incomplete snippets).",
            "frequency_or_prevalence": "Common in Q&A-mined datasets; motivating the multi-stage cleaning efforts reported in prior studies.",
            "root_cause": "Nature of Q&A answers (showing only the essential fragment) and lack of explicit API/class scaffolding in snippets.",
            "mitigation_approach": "Use structural heuristics to determine syntactic validity; manual annotation to build classifiers; discard non-standalone snippets.",
            "mitigation_effectiveness": "Improved downstream performance in cited works after filtering; reported BLEU improvements in curated/filtered subsets (e.g., models trained on curated subsets outperform fully noisy sets).",
            "domain_or_field": "machine learning / program synthesis",
            "reproducibility_impact": true,
            "uuid": "e472.1"
        },
        {
            "name_short": "Quasi-NL vs true NL",
            "name_full": "Quasi-natural language descriptions produced from self-documenting function/class names",
            "brief_description": "Function and class names split into words create a controlled/quasi-natural language that is consistent and compact but is not equivalent to free-form natural language, producing a mismatch with expectations of pure NLP-trained models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Self-documenting-code-derived corpus and NMT-based test-code generator",
            "system_description": "A pipeline that extracts JUnit test function and class names (split into words) as NL descriptions and aligns them with their Java test function bodies to create training data for sequence-to-sequence models.",
            "nl_description_type": "Parsed function and class names (quasi-natural language / controlled natural language)",
            "code_implementation_type": "Java unit test function bodies (Junit)",
            "gap_type": "controlled/quasi-NL vs free-form NL (expressiveness mismatch)",
            "gap_description": "Descriptions are compact, follow naming conventions (camelCase split), and lack full natural-language syntax; they may omit context that a free-form description would include, which could limit generalization to arbitrary NL inputs.",
            "gap_location": "training data / source-language representation",
            "detection_method": "Conceptual analysis and empirical evaluation: the paper observes this mismatch and explicitly discusses it; qualitative review of generated outputs.",
            "measurement_method": "Indirect: downstream BLEU and exact-match statistics when training NMT on these quasi-NL corpora (paper reports BLEU and % exact matches per dataset).",
            "impact_on_results": "Positive in this domain: the controlled nature yields consistent patterns that help the model (high BLEU for rx=82.44), but it may limit applicability to unrestricted natural language inputs and generalization across domains.",
            "frequency_or_prevalence": "This is the central design choice of the paper (applies to test suites with self-documenting names); prevalence depends on codebase adherence to 'clean code' naming conventions.",
            "root_cause": "Design choice / software engineering practice (developers using meaningful, convention-following names) producing non-freeform NL.",
            "mitigation_approach": "Treat the NL as a controlled language (acceptable for test-generation tasks); future work recommended to test generalization to free-form NL or richer annotations.",
            "mitigation_effectiveness": "Effective for unit-test generation in the experiments: models trained on these corpora achieved BLEU scores (rx=82.44, sf=58.00, multi-min10=50.50) and non-trivial exact-match rates (rx: 47.9% exact matches on test set).",
            "domain_or_field": "software engineering / machine learning for code",
            "reproducibility_impact": true,
            "uuid": "e472.2"
        },
        {
            "name_short": "BLEU mismatch for code",
            "name_full": "Limitations and misalignment of BLEU metric when used to evaluate generated source code",
            "brief_description": "BLEU, developed for natural language MT, may not faithfully reflect semantic or functional correctness of generated code, producing a discrepancy between metric-reported quality and true code utility.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "NMT-based code generation evaluation",
            "system_description": "Model evaluation pipeline using BLEU as the principal automatic metric to assess similarity between generated code and ground-truth code sequences.",
            "nl_description_type": "function/class name descriptions (quasi-NL) used as inputs",
            "code_implementation_type": "generated Java test function bodies",
            "gap_type": "evaluation metric mismatch / inadequate metric",
            "gap_description": "BLEU compares n-gram overlap and may not capture syntactic correctness, compilation status, functional equivalence, or the practical usefulness of generated code; there are no empirical studies validating BLEU for source code.",
            "gap_location": "evaluation / metrics",
            "detection_method": "Critical discussion and qualitative manual inspection of generated outputs; noting literature gap (no empirical validation of BLEU for code).",
            "measurement_method": "Paper reports BLEU scores and complements them with qualitative, example-based manual analysis and exact-match counts to better contextualize results.",
            "impact_on_results": "Could mislead about model quality; authors therefore supplement BLEU with manual review and exact-match percentages (e.g., rx BLEU=82.44 with 47.9% exact matches; sf BLEU=58 with 8% exact matches).",
            "frequency_or_prevalence": "BLEU is widely used in related work as reported in the paper, but the paper cautions against overreliance on it for code tasks.",
            "root_cause": "BLEU's origin as an NLP metric that measures surface n-gram overlap rather than semantic/functional equivalence.",
            "mitigation_approach": "Complement BLEU with qualitative manual review, exact-match statistics, and planning human evaluation of generated code; call for future human-feedback-based evaluation (Test Recommendation Engine and collaboration with test teams).",
            "mitigation_effectiveness": "Manual review provided additional insight and revealed substantive differences between datasets (e.g., rx high exact-match fraction vs sf and multi lower rates), demonstrating that BLEU alone is insufficient.",
            "domain_or_field": "machine translation applied to source code / evaluation",
            "reproducibility_impact": true,
            "uuid": "e472.3"
        },
        {
            "name_short": "Vocabulary / rare-token gap",
            "name_full": "Vocabulary mismatch and rare-token handling between NL and PL vocabularies",
            "brief_description": "Large, project-specific programming language vocabularies introduce many rare tokens (API names, identifiers) that create gaps between training language models and realistic code generation; rare tokens can negatively affect model performance if not handled.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "corpus preprocessing and vocabulary selection pipeline",
            "system_description": "Preprocessing step that tokenizes code and builds vocabularies for both NL (parsed names) and PL (code tokens), with options to limit PL vocabulary by frequency thresholds.",
            "nl_description_type": "class and method names (quasi-NL)",
            "code_implementation_type": "Java test function tokens (PL)",
            "gap_type": "vocabulary mismatch / rare-token sparsity",
            "gap_description": "Programming APIs and project-specific identifiers create a long tail of rare tokens; mapping rare tokens to an unknown token reduces vocabulary size but may discard important semantics needed to generate correct code.",
            "gap_location": "preprocessing / tokenization / vocabulary construction",
            "detection_method": "Empirical: datasets show very large PL vocabularies; authors experimented with limiting PL vocabulary for multi-repo corpus and observed changes in BLEU.",
            "measurement_method": "Compare BLEU across versions with differing vocabulary cutoffs (paper reports multi-min5 BLEU=48.85 and multi-min10 BLEU=50.50), and report vocabulary sizes (e.g., PL vocab sizes listed in Table 2).",
            "impact_on_results": "Vocabulary pruning had non-trivial effects: multi-min10 slightly outperformed multi-min5 in BLEU despite higher unknown token rate, indicating non-obvious interactions between vocabulary size and model performance in code domain.",
            "frequency_or_prevalence": "Universal in Big Code datasets due to diverse APIs; explicit statistics provided in paper (large PL vocabularies for project corpora).",
            "root_cause": "Heterogeneity of APIs and identifiers across projects and scarcity of many tokens in training data.",
            "mitigation_approach": "Map rare tokens to an unknown token, experiment with frequency thresholds, and keep full vocabularies for project-specific corpora when feasible.",
            "mitigation_effectiveness": "Mixed: in this paper, raising the minimum occurrence threshold (n=10) produced a small BLEU increase (multi-min10 BLEU=50.50 vs multi-min5 BLEU=48.85), but authors caution that hyperparameter tuning was limited and results are not definitive.",
            "domain_or_field": "machine learning for code / corpus preprocessing",
            "reproducibility_impact": true,
            "uuid": "e472.4"
        },
        {
            "name_short": "Naming inconsistency / meaningless names",
            "name_full": "Meaningless or inconsistent function/method names in code (e.g., test1(), test2()) causing poor NL descriptions",
            "brief_description": "Some codebases contain non-descriptive, autogenerated, or placeholder function names that fail to provide useful NL descriptions for aligning with code, producing mismatches in text-code corpora.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "self-documenting-code-based corpus synthesis pipeline",
            "system_description": "The extraction and preprocessing pipeline that converts camelCase/snake_case class and method names into space-separated quasi-NL descriptions paired with tokenized function bodies.",
            "nl_description_type": "parsed method and class names",
            "code_implementation_type": "Java test function bodies",
            "gap_type": "meaningless or inconsistent naming (insufficient NL description)",
            "gap_description": "Some test functions use meaningless names (e.g., test1()) that do not carry semantic content; such pairs produce poor or misleading NL descriptions for the associated code body.",
            "gap_location": "dataset acquisition / NL description quality",
            "detection_method": "Heuristic filtering implemented in preprocessing (simple heuristics to exclude meaningless names); manual inspection during dataset assembly.",
            "measurement_method": "Counts of excluded functions (not explicitly quantified numerically in paper) and downstream model performance after exclusion.",
            "impact_on_results": "Including meaningless names would add noisy pairs and degrade model learning; the authors removed such functions to improve corpus quality.",
            "frequency_or_prevalence": "Some occurrences across repositories; authors applied heuristics to exclude them but did not provide exact prevalence statistics.",
            "root_cause": "Developers using placeholder names or lacking descriptive naming practices in some code.",
            "mitigation_approach": "Apply heuristics to exclude functions with meaningless names during dataset creation (e.g., pattern matching for 'test1', 'test2').",
            "mitigation_effectiveness": "Qualitatively effective as part of preprocessing; no explicit quantitative before/after numbers provided.",
            "domain_or_field": "software engineering / dataset curation",
            "reproducibility_impact": true,
            "uuid": "e472.5"
        },
        {
            "name_short": "Cross-project generalization gap",
            "name_full": "Lack of evidence for effective cross-project transfer / use of information across repositories in multi-repository models",
            "brief_description": "Models trained on multi-repository corpora did not show clear evidence of leveraging cross-project regularities; correct generation often relied on examples from the same parent project, indicating a generalization gap across projects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "multi-repository NMT training pipeline",
            "system_description": "An aggregated corpus constructed from parsed test cases across &gt;700 repositories, used to train a generic NMT model for test-code generation.",
            "nl_description_type": "parsed class and method names (quasi-NL) aggregated across many projects",
            "code_implementation_type": "Java unit test function bodies from many projects",
            "gap_type": "domain shift / lack of cross-project generalization",
            "gap_description": "Although the multi-repo model produced correct outputs for some test examples, analysis showed correct predictions were typically supported by examples from the same project rather than by cross-project learning, suggesting limited transfer across different codebases.",
            "gap_location": "model generalization / training data heterogeneity",
            "detection_method": "Post-hoc analysis of fully matched outputs: tracing which training examples the model appeared to exploit (authors inspected matched outputs and their training-origin projects).",
            "measurement_method": "Exact-match counts and inspection: multi model had 2,908/30,039 (9.7%) exact matches; authors examined origins and found evidence that correct predictions were often based on examples from same parent project.",
            "impact_on_results": "Indicates that multi-repo generic models may require more within-project examples to be effective; generic model BLEU was lower than project-specific BLEUs (multi BLEU ~48.85–50.50 vs rx=82.44, sf=58.00).",
            "frequency_or_prevalence": "Observed in this study's empirical analysis of multi-repo dataset; prevalence depends on inter-project similarity and amount of per-project data.",
            "root_cause": "High heterogeneity of APIs, naming conventions, and domain-specific tokens across projects; insufficient cross-project shared patterns for the model to exploit.",
            "mitigation_approach": "Two suggested use cases: (1) train project-specific models for mature projects to leverage intra-project regularities; (2) accept generic model will be less accurate and require editing; future work to explore vocabulary and domain adaptation strategies.",
            "mitigation_effectiveness": "Project-specific models demonstrated substantially better BLEU and exact-match behavior in experiments (rx and sf outperform multi), indicating that project specialization is an effective mitigation.",
            "domain_or_field": "machine learning for code / transfer learning",
            "reproducibility_impact": true,
            "uuid": "e472.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to mine aligned code and natural language pairs from stack overflow",
            "rating": 2,
            "sanitized_title": "learning_to_mine_aligned_code_and_natural_language_pairs_from_stack_overflow"
        },
        {
            "paper_title": "A Systematically Mined Question-Code Dataset from Stack Overflow",
            "rating": 2,
            "sanitized_title": "a_systematically_mined_questioncode_dataset_from_stack_overflow"
        },
        {
            "paper_title": "A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation",
            "rating": 2,
            "sanitized_title": "a_parallel_corpus_of_python_functions_and_documentation_strings_for_automated_code_documentation_and_code_generation"
        },
        {
            "paper_title": "Learning to generate pseudo-code from source code using statistical machine translation",
            "rating": 1,
            "sanitized_title": "learning_to_generate_pseudocode_from_source_code_using_statistical_machine_translation"
        },
        {
            "paper_title": "Latent Predictor Networks for Code Generation",
            "rating": 1,
            "sanitized_title": "latent_predictor_networks_for_code_generation"
        }
    ],
    "cost": 0.01507725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automatic Acquisition of Annotated Training Corpora for Test-Code Generation
17 February 2019</p>
<p>Magdalena Kacmajor magdalena.kacmajor@ie.ibm.com 0000-0001-6843-9050
Innovation Exchange
IBM Ireland
Dublin 4Ireland</p>
<p>John D Kelleher john.d.kelleher@dit.ie 0000-0001-6462-3248
ADAPT Centre &amp; ICE Research Institute
Technological University Dublin
Dublin 2D08 X622Ireland</p>
<p>Automatic Acquisition of Annotated Training Corpora for Test-Code Generation
17 February 201920E7A6FEA5434EF8AC186AAA73E4026510.3390/info10020066Received: 21 January 2019; Accepted: 13 February 2019test automationcode generationneural machine translationnaturalness of softwarestatistical semantics
Open software repositories make large amounts of source code publicly available.Potentially, this source code could be used as training data to develop new, machine learning-based programming tools.For many applications, however, raw code scraped from online repositories does not constitute an adequate training dataset.Building on the recent and rapid improvements in machine translation (MT), one possibly very interesting application is code generation from natural language descriptions.One of the bottlenecks in developing these MT-inspired systems is the acquisition of parallel text-code corpora required for training code-generative models.This paper addresses the problem of automatically synthetizing parallel text-code corpora in the software testing domain.Our approach is based on the observation that self-documentation through descriptive method names is widely adopted in test automation, in particular for unit testing.Therefore, we propose synthesizing parallel corpora comprised of parsed test function names serving as code descriptions, aligned with the corresponding function bodies.We present the results of applying one of the state-of-the-art MT methods on such a generated dataset.Our experiments show that a neural MT model trained on our dataset can generate syntactically correct and semantically relevant short Java functions from quasi-natural language descriptions of functionality.</p>
<p>Introduction</p>
<p>As digitization spreads into all areas of business and social life, the pressure on software development organizations is growing.The sheer amount of code being created, and the increasing complexity of software systems, fuels the need for new methods and tools to support the software development process.</p>
<p>A widely adopted framework addressing the challenges of the modern software delivery lifecycle is the DevOps model [1], which is founded on the principles of continuous integration, continuous delivery, and continuous testing.Both the wisdom of the crowd and academic evidence [2] speak for the efficiency of DevOps practice, but adopting DevOps brings its own challenges, including a significant increase in the volume and frequency of testing.In fact, on a large-scale project it is not feasible to implement DevOps without test automation-and writing automated test cases is timeand resource-consuming.Not surprisingly, automated test case generation methods are being actively studied.In general, to generate unit test cases, existing approaches use information extracted from other software artifacts, such as code under test, specification models, or execution logs [3].</p>
<p>State-of-the-art test generation tools can significantly improve test coverage; however, it has been shown that their fault detection potential is problematic: many faulty code portions are never executed, or are executed in such a way that defects are not detected [4].These tools stem from the tradition of research on code analysis and code generation that is concerned with formal semantics and structural information about the code.Such research takes advantage of the formality, consistency, and unequivocalness of programming languages-that is, the properties that distinguish source code from natural languages.A more recent research trend switches the focus to statistical semantics.This exciting alternative can be now fully explored thanks to the much-increased availability of source-code resources stored in online open source repositories.It has been argued that large source-code corpora exhibit similar statistical properties to those of natural language corpora [5], and indeed, statistical language models developed for Natural Language Processing (NLP) have proved to be efficient when applied to programming languages [6].</p>
<p>However, even billions of lines of code scraped from online repositories are not sufficient to satisfy the training requirements for some types of tasks.Many applications-such as code generation from natural language descriptions, code search by natural language queries, or automated code documentation-require joint processing of natural languages and programming languages.This means that the source-code corpora used for training these systems need to be appropriately annotated with natural language descriptions.The main challenge here is the acquisition of fine-grained natural language annotations that accurately and consistently describe the semantics of code.</p>
<p>As a concrete example, statistical translation models used in NLP require training on parallel corpora-also called bi-texts-in which many sentences written in a source language are aligned with their translations in a target language.A schematic example of a parallel corpus with aligned sentences in natural languages is presented in Figure 1a.To apply statistical translation models to source code-that is, to train a model capable of mapping textual sequences to sequences of source code-it is necessary to obtain a text-code parallel corpus in which a large number of code units are aligned with their descriptions in natural language (Figure 1b).Aligned corpora for statistical machine translation (MT) of two natural languages (bi-text datasets) can be gathered from existing collections of translated texts.However, obtaining a parallel corpus for a natural language coupled with a programming language (a text-code dataset) is much less straightforward.</p>
<p>The question of what should be the nature and level of detail of the natural language descriptions provided in such a corpus does not have a definite answer and requires more investigation.</p>
<p>Nonetheless, it seems reasonable to assume that the practical value of a text-code corpora depends on the following properties:</p>
<p>1.</p>
<p>Size, and the potential to scale in size in the future, is of particular importance for deep learning models which require large amounts of training data.</p>
<p>2.</p>
<p>Acquisition cost, in terms of human effort and the complexity of the procedure.</p>
<p>3.</p>
<p>Level of noise in the acquired data.</p>
<p>4.</p>
<p>Granularity of the natural language descriptions.</p>
<p>Several recent studies have proposed more or less sophisticated methods of obtaining text-code corpora (see Section 2).The proposed methods vary in terms of the properties listed above, but regardless their practical value, none of them are applicable to the testing domain.The main contribution of this paper is a novel method of automatically synthetizing large text-code datasets containing textual descriptions of single testing tasks, each matched with the code implementing that task.Moreover, in this paper we demonstrate that machine learning models trained on our datasets can generate complete, compilable, and semantically relevant automated test cases based on quasi-natural language descriptions of testing tasks.These results were obtained using a neural MT model [7] designed for learning from bi-text corpora, in which the degree of equivalence between source and target languages is very high.We find that this off-the-shelf neural MT architecture performs well on our code-text corpora, which suggests that the quasi-natural language descriptions obtained using our approach are precise and consistent enough to allow direct translation to code.</p>
<p>There are two aspects of the potential implications of the presented work.First, from the perspective of the testing community, we present an efficient, inexpensive, and scalable method for annotating test code with textual descriptions.The availability of such annotated datasets can accelerate the application of the latest advances in machine learning to the testing domain.Second, from the perspective of research on applying statistical models to source code, our datasets may provide better insight into the desired characteristics of text and code sequences in a training corpus.Understanding what type of annotations works well or what is the optimal translation unit for the source code may be valuable for researchers concerned with synthesizing text-code datasets.</p>
<p>The remainder of this article is organized as follows.Section 2 provides an overview of existing solutions for text-code corpora acquisition.In Section 3 we provide the rationale of our approach and explains how it works.Section 4 describes in detail the procedure of synthesizing training corpora used in our experiments.Section 5 presents the experimental setup and the results of training a neural MT model on a text-code dataset generated using our method.In Section 6 we discuss the results, and Section 7 concludes the paper.</p>
<p>Related Work</p>
<p>In this section, we do not attempt to show the full range of techniques of matching natural language descriptions to code that have been proposed throughout the literature.Rather, our aim is to investigate which approaches can yield datasets that meet the training needs of statistical text-code language models.Thus, the focus of this review is on studies which are explicitly concerned with applying language models to source code, and which provide some evidence for the performance of text-code language models trained on the proposed datasets.</p>
<p>The performance of the many of the models covered in this review, and indeed the models we present later in the paper, are evaluated using BLEU [8].BLEU is a de facto standard measure for MT.BLEU compares machine output with human-authored ground-truth translation, and scores how close they are to each other, on a scale from 0 to 100, with 100 indicating a hypothetically perfect translation.In the context of source-code generation from text input, BLEU is calculated by comparing the output of the model to the source-code ground truth from the corpus.</p>
<p>Perhaps the most straightforward solution to creating a dataset of aligned text and code is reported in [9], where a software engineer was hired to manually annotate 18,000 Python statements with pseudo-code.This approach is neither scalable nor cheap, but the study provides interesting insights.The dataset was used to train a phrase-based and a tree-to-string SMT models to generate pseudo-code from source code.The tree-to-string model outperformed the phrase-based model by a large margin (BLEU score of 54 compared to 25), suggesting that correct code-to-text mapping necessitates parsing program structure, and even line-by-line, noise-less descriptions are not sufficient to support a plain phrase-based translation model.</p>
<p>For the work reported in [10] two text-code datasets, one containing Java and the other Python code, were created.In both datasets the code units were aligned with descriptions that combine structured and unstructured text.These datasets were used to train a neural model which generated code from a mix of natural language and structured inputs.The model achieved an impressive performance of 65.6 BLEU scores.Furthermore, the authors trained two neural translation models as baselines, one augmented with their structured attention mechanism.The augmented translation model outperformed the plain translation model on both datasets (BLEU scores of 50 and 44 compared to 34 and 29).</p>
<p>The remaining papers included in this review use a Big Data approach.This research follows two main directions: one toward exploiting Big Code (primarily GitHub (https://github.com)),and the other toward mining programming-related Q&amp;A websites, primarily StackOverflow (https: //stackoverflow.com).</p>
<p>The Big Code route involves scraping API documentation (Javadoc, Python docstrings) from online source-code repositories, and using it as natural language description of code fragments.The research reported in [11] created a massive parallel corpus of over 7.5 million API sequences annotated with excerpts from Javadoc.These API sequences are not raw code sequence, they are rather parsed representations of general API usage.Consequently, a neural MT model trained on this corpus would not generate code, instead given some description of required functionality it would produce hints on the APIs that can be used.This model was augmented with information on API importance, and achieved BLEU score of 54.</p>
<p>Another corpus exploiting API documentation [12] consists of over 150,000 Python function bodies annotated with docstrings.The authors used a back-translation approach [13] to extend this corpus with a synthetic dataset of 160,000 entries.The performance of a (non-augmented) neural translation model trained on the extended corpus was low (BLEU score of 11).</p>
<p>The second route-assembling datasets from user queries matched to code fragments mined from Q&amp;A websites-has recently attracted a lot of attention.In [14], two training corpora were created from C# snippets extracted from responses to StackOverflow and Dot Net Perls questions, and matched with the titles of these questions.Furthermore, general-purpose engine queries that produced clicks to the questions were added as alternative natural language descriptions.A bi-modal source-code language model trained on the resulting dataset was evaluated in terms of retrieval capability.The model had much better performance when retrieving natural language descriptions (with code snippets as queries) compared with retrieving code snippets, with NL descriptions as queries (Mean Reciprocal Rank of 0.44 as compared to 1.18).</p>
<p>Datasets collected from Q&amp;A websites are large, and have the potential to grow as new questions and answers are added to the websites, but the level of noise in the data is very high.Queries can have irrelevant or very informal titles, and the code snippets are often incomplete and non-compilable.This problem was partly addressed in [14], by applying simple heuristics, but other researchers deemed this approach insufficient and proposed extracting quality datasets from noisy Q&amp;A corpora by applying machine learning models, trained on human-annotated seed datasets, as filters.For example, in one study after collecting C# and SQL snippets produced in response to questions posted on StackOverflow and paired with the titles of these questions, the authors manually annotated a small subset of data and trained a semi-supervised classifier to filter out titles that were irrelevant to the corresponding code snippet [15].The resulting cleaned corpora (containing over 66,000 pairs for C# and 32,000 of SQL) were used to training a neural MT model for code summarization (that is, for generating text from code, not code from text), which achieved BLEU scores of 20.5 (C#) and 18.4 (SQL).</p>
<p>Systematic mining of question-code datasets retrieved from Stack Overflow was the main focus of two other studies.In [16], user queries matched with Python and SQL code snippets were subject to a series of cleaning steps.First, a logistic regression classifier (with human-engineered features) was trained to select questions of type "how-to-do-it", in which the user provides a scenario and asks how to implement it.Next, a subset of over 5000 question-code pairs was manually annotated by hired students who judged whether a snippet constitutes a standalone solution to the corresponding question.A novel model, called the Bi-View Hierarchical Neural Network, was trained on the annotated data and used to select over 147,500 Python and 119,500 SQL question-code pairs, to be included in the final dataset (https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset).</p>
<p>Another complex method to mine high-quality aligned data from Stack Overflow was described in [17].First, the authors manually engineered a set of code structure features needed to determine the syntactic validity of a code snippet.Second, a subset of collected StackOverflow posts was manually annotated, using a carefully designed annotation procedure, to label specific elements in each post (intent, context, snippet), and to filter out non "how-to-do-it" questions.Next, a neural translation model was trained to learn "correspondence features"-that is, to learn the probability of the intent given a snippet, and the probability of the snippet given an intent.Finally, the calculated probabilities for each intent and snippet were fed to a logistic regression classifier as the correspondence features, together with the manually engineered structural features.After training, the classifier was used to create an automatically mined training corpus containing almost 600,000 intent-snippet pairs (https://conala-corpus.github.io/).Given the small set of annotated data used for training the correspondence features, the authors acknowledged existing threats to validity, and provided an additional dataset of 3,000 examples manually curated by annotators.A baseline neural MT model trained on 100,000 examples from the automatically mined corpus combined with the curated data achieved BLEU score of 14.26.The performance of the model trained on curated data only was even lower (BLEU score 10.58).</p>
<p>Text-Code Corpora Acquisition from Self-Documenting Code</p>
<p>In Section 2 we outlined two main approaches to the large-scale acquisition of annotated source code: collecting developer-defined descriptions extracted from API documentation, and collecting user-defined descriptions, extracted from users' questions posted on Q&amp;A websites and matched with code snippets posted as answers.Neither of these approaches is applicable to the testing domain.Javadoc or docstring only exist for the code that is a part of a public API, and this type of documentation is not available for test automation code.Data collected from Q&amp;A websites contains code snippets that can help in solving concrete programming issues but not in writing specific test cases.Only a small fraction of questions on Stack Overflow are related to testing, and dedicated websites on software quality are far behind in terms of popularity (For example, Software Quality Assurance &amp; Testing website (https://sqa.stackexchange.com/)stores 8500 questions, as compared to 17,000,000 at Stack Overflow).Furthermore, the performance of machine learning models trained on the existing large-scale text-code datasets it low.</p>
<p>In our approach we take advantage of a programming routine known as self-documenting code.Although research on applying statistical language models to source code is relatively new, software developers have long been aware that source code is written for two recipients: one is the machine, and the other-a software developer who will be reviewing, extending, or maintaining the code.The need to secure the interests of the second recipient has been embodied in Clean Code paradigm [18]-a well-known set of best practices focused on making programming code readable and easy to understand for humans.One of the key Clean Code principles is to create variable identifiers and functions names that are meaningful and reveal programmer's intent.To that end, the developer uses multiple words to formulate an adequate description of a function, and then squeezes all the words into the function name, using some convention that helps the reader to recognize individual words (such as camelCase in Java, or snake_case in Python).Code comments are discouraged, as they carry high risk of being outdated and are often redundant; instead, it is recommended that the code should be self-explanatory.</p>
<p>The method we propose is based on the observation that self-documentation through descriptive method names is widely adopted in test automation, in particular unit testing.Figure 2 shows real-life examples drawn from the spring-framework, an open source Java project stored on GitHub (https://github.com/spring-projects/spring-framework).Each of these self-documenting test function names is a concise summary of a specific testing task, written in a quasi-natural language, and observing a consistent naming convention.The body of each function is the implementation of that task in a programming language.Thus, a parallel text-code corpus can be formed from function names split into individual words and aligned with function bodies.</p>
<p>Methodology</p>
<p>The text-code dataset creation method we propose exploits the self-documenting code and can, in principle, be applied to test cases written in any high-level programming language, provided that the code has been written with the consideration for readability.The generic language-independent procedure can be summarized as follows:</p>
<p>1.</p>
<p>Collect automated test cases from source-code repository.Depending on the use case, datasets can be assembled from data within a single repository (for training custom, project-or organization-specific models) or from multiple repositories (for training generic models).</p>
<p>2.</p>
<p>For each collected test case: On a lower level, however, preprocessing details become language-specific.All the examples presented in this article are based on code written in Java-the second most popular programming language on GitHub (https://blog.github.com/2018-11-15-state-of-the-octoverse-top-programminglanguages/).Furthermore, all the test cases we used for dataset synthesis were created with Junit (https://junit.org/)-anopen source unit testing framework for Java.</p>
<p>In the experiments we use three datasets synthesized from open source code stored on GitHub.Two of them (labeled sf and rx) were each extracted from a single large Java repository, and the third one (multi) was assembled from test code pulled from over 700 repositories.In the following paragraphs we first describe how we acquired data from a single repository, and then describe the procedure used to assemble the multi-repository corpus.</p>
<p>Processing Data from A Dingle Depository</p>
<p>To create the project-specific datasets, sf and rx, we chose two actively maintained GitHub repositories: the spring-framework and RxJava (https://github.com/ReactiveX/RxJava).This choice was guided by two criteria: the size of the repository and the popularity of the repository, as indicated by the number of stars assigned by users and the number of created forks (repository copies created by members of the GitHub community).We assume that the popularity of a repository is correlated with the quality of code.Table 1 summarizes the properties of the two selected repositories.We crawl each repository to retrieve all test class files, identified as those containing the @Test annotation string.We process every test class using the JavaParser library (http://javaparser.org/) to extract and parse the test functions.Each test function is an automated test case.A parsed test function is represented as a JSON object whose properties comprise function name, function body, parent class name, and some metadata for identification.Any inline comments are separated out from the function body and rejected.The result of this step is a JSON array containing all parsed test functions from the repository.The next step is the actual synthesis of the corpus.From each JSON file, we take the class name and the function name, which are both camel case strings, convert them into space-separated strings, and prepend with the special tokens #class and #method, respectively.The quasi-natural language description is created by concatenating the two resulting strings.The programming language sequence is created by simply tokenizing the function body.We assume that tokens such as parentheses, punctuation marks or mathematical symbols should be treated as individual words, and parse the code by surrounding each such character with white-spaces.</p>
<p>Figure 3 shows schematically how the quasi-natural language sentence and the corresponding programming language sequence are derived from a single unit test case.When adding sequence pairs to the corpus, we apply two filters:</p>
<p>1.</p>
<p>Duplicates filter.Any repeating sequence pairs are skipped, although we allow repeating code sequences, as long as they have different natural language descriptions.2.</p>
<p>Length filter.Any sequence pair with a programming language sequence beyond a predefined length (see Table 2) is rejected.This filter has dual purpose.First, it removes code sequences whose size drastically deviates from the average.For example, before applying the length filter, sequences of the length up to 300 tokens comprised over 97% of test cases extracted from RxJava repository.The sizes of the remaining 3% ranged from 300 to more than 1000.Second, it mitigates the disproportion between the size of the quasi-natural language sequences and programming language sequences.</p>
<p>Collecting Data from Multiple Repositories</p>
<p>The data used to create the multi-repository corpus was collected using the github3 (https: //pypi.org/project/github3.py/) library-a Python wrapper for the GitHub API.The search query included two parameters: language:java and stars:&gt;1000.We were not concerned with the size of individual repositories.The query returned almost 2000 repositories.Over 700 of them contained Junit test cases which we used for dataset creation.</p>
<p>The repositories were processed one by one according to the procedure described in Section 4.1.In addition to applying the duplicate and length filters to each individual repository, we also checked for and removed any duplicates across the repositories.We also applied some simple heuristics to exclude functions with meaningless names, such as test1(), test2() etc.</p>
<p>Table 2 lists the metadata for the three datasets used in the experiments.The source-code vocabulary sizes are very large.This is not surprising, given that each programming API brings in new source-code tokens.For the rx and sf corpora we kept the full PL vocabularies.For the multi-repository dataset, we limited the PL vocabulary size by discarding all tokens that occurred less than n times in the corpus.In the dataset, words excluded from the vocabulary were mapped to a special unknown token.We created two versions of the multi-repository dataset, one with n = 5 and the other with n = 10.* The size after rare tokens have been mapped to the unknown token.</p>
<p>Experiments</p>
<p>The purpose of the experiments reported in this section is to investigate whether test code annotated with descriptions extracted from meaningful function names provides good quality data for training statistical language models.To do this, we use the corpora described in Section 4 to train a well-known neural translation model to generate test code from quasi-natural language descriptions.</p>
<p>Neural translation models are an example of end-to-end learning: these models take a source language sentence as an input, encode it into a dense vector (inter-lingual) representation, and decode this representation to generate the translation target language sentence.The model used in our study belongs to the class of sequence-to-sequence models [19], and is a TensorFlow implementation of the attention-based architecture proposed in [7] (https://github.com/tensorflow/nmt/).</p>
<p>We trained three models, one on each of the datasets.In the preliminary phase we performed trial runs to establish a set of reasonable hyperparameters, and then kept them constant throughout the experiments.We used 2-layered LSTMs for both the encoder and the decoder, with the Bahdanau attention mechanism [20].For the optimizer we used Adam with a starting learning rate of 0.001.Regarding regularization, we set dropout probability to 0.2, as recommended for LSTMs [21].</p>
<p>Each model was evaluated on a test set that was randomly sampled from the dataset the model was trained on.These test sets were sampled prior to training and were held-out from the training process.In case of the multi-repository corpus, the test set is the concatenation of test sets sampled from each contributing repository.The results are presented in Table 3. BLEU scores on code generation are not directly comparable with scores on natural language translation due the differences in the length of the translated strings and the inherent differences in the (natural vs. programming) language structures.Moreover, it is well known in the MT community that BLEU scores are not necessarily a true reflection of translation quality.These caveats aside, however, the results from our experiments are promising and indicate that our approach of generating test case source code from parsed class and function names is feasible (BLEU score obtained on multi-min10 is somewhat higher than the score obtained on multi-min5, which may seem surprising, given that in the context of natural language translations, the higher number of unknown words has been shown to have a negative impact on performance [20,22].However, since the aim of our experiments was to confirm the usefulness of the parallel corpora built from self-documenting code (rather than the evaluation of a specific machine learning model), we put limited effort into hyperparameters optimization, and it is possible that a more extensive search of the hyperparameter space would provide slightly different results.Investigating the impact of the vocabulary size in the context of programming language would be useful, but remains out the scope of this paper).In the following section we present a qualitative, example-based analysis of the performance of our models with a view to better understanding and contextualizing the results of training an NMT model on the three datasets extracted from self-documenting code.</p>
<p>Discussion</p>
<p>Acknowledging the limitations of BLEU metrics as a tool for evaluating the quality of generated code, we carefully reviewed the outputs of our models and analyzed them in the context of the examples available in the training corpora.In this section, we discuss the findings from this qualitative analysis of the three datasets.</p>
<p>Dataset rx</p>
<p>The unusually high performance of the model trained on the rx dataset becomes less surprising after a closer look into the RxJava project test code.RxJava features several base classes exhibiting standardized behaviors that can be-and are-tested by analogous test functions.For instance, a large body of analogous test code exists for the base classes Observable and Flowable.Examples of highly equivalent functions testing these two classes are shown in Figure 4.Each test function is presented in the parsed format, as a text-code pair.The parts forming textual descriptions are marked in italics.The differences between equivalent function versions (a, b) are highlighted in bold.</p>
<p>The replacement patterns emphasized in Figure 4 (Observable vs. Flowable, PublishSubject vs. PublishProcessor, TestObserver vs. TestSubscriber, BackpressureStrategy and blockingGet vs. the absence of these patterns) are highly repeatable throughout the corpus, making the learning task much easier.In fact, the model was able to fully generate the unseen long function body shown in 3a, based on the NL description (#class flowable delay subscription other test #method test complete triggers subscription) which was a part of our test set.</p>
<p>After completed training of the rx model, we reviewed all the code generated against NL descriptions from the test set.Out of 2014 examples in the test set, 965 (47.9%) generated sequences that fully matched unseen ground-truth source code.Clearly, the repeatable patterns occurring in analogous test functions strongly contributed to this result, but the model was also able to exploit other regularities in the training data.</p>
<p>In the case of short code sequences, it is relatively easy to track back at least some of the pieces of information used by the model to generate test functions.Table 4 presents the analysis of four correctly predicted test functions.The top test-code pair (bold font) in each section of the table is pulled from the test set.The remaining text-code pairs (plain font) are the pulled from the trainset.For the rx dataset, we selected a function (rxe) that-in contrast to the patterns depicted in Figure 4</p>
<p>Table 4.</p>
<p>Examples of correctly predicted unseen functions, together with seen instances that provided necessary information.Each row in the table contains the following data: Lbl (label)-created by concatenating the dataset label (rx, sf, m for multi) with a suffix indicating whether the text-code pair comes from the test set (suffix e) or the trainset (suffix t). ; Text-the quasi-natural description (input sequence); Code-the test code (output sequence).Text-code pairs coming from the test set (rxe, sfe, me) are additionally highlighted in bold.</p>
<p>Lbl</p>
<p>Dataset sf</p>
<p>The phenomenon of analogous functions is not observed in the test code pulled from spring-framework, which probably explains why the NMT model trained on the sf dataset achieved less spectacular performance than the one trained on the rx dataset.Nevertheless, the BLEU score of 58 obtained by the sf model indicates that consistent naming conventions are useful for learning mappings between textual descriptions and code sequences.As we did previously with the rx dataset, we analyzed the examples generated by the model from the descriptions in the test set.Table 4 shows an interesting example of a generated function (sfe), in which the model was able to correctly predict mathematical symbols corresponding to the string ge (greater-than-or-equal-to) based on the seen examples gt, lt, le (greater-than, less-than and less-than-or-equal-to).</p>
<p>The total number of model predictions that fully matched the ground-truth was 224 out of 2759 examples in the test set (8%).</p>
<p>Dataset Multi</p>
<p>The BLEU score obtained by the model trained on multi-repository dataset was significantly lower that than the score of the sf model, but the percentage of fully matched predictions was actually slightly higher (9.7%, or 2,908 out of 30,039).On analyzing these fully matched outputs of the model trained on the multi-repository dataset, we did not find evidence that the model learned to use information across projects originating from different repositories.For example, the correct predictions produced for the two functions from the multi-repository test set shown in Table 4 (labeled me) were produced based on examples originating from the same project as the generated sequence (i.e., the same parent project).However, it seems that generating a correct code sequence does not require a large parent project.The parent projects of the two examples in Although the model trained on the multi dataset performed worse than the project-specific models, it is worth remembering that the vanilla neural translation models trained on the parallel corpora extracted from API documentation [12] or Stack Overflow queries [17] achieved BLEU scores of 11 and 14, respectively (It is not possible to make direct comparisons with the results obtained using models dedicated for code processing task (exploiting structural/syntactical information about the code, building Abstract Syntax Tree representations etc.).The comparisons of the potential of training corpora only make sense for experiments that used a similar ML model.From the studies discussed in Section 2, four papers report the results of training a pure NMT model.However, the results reported in [11,15] do not refer to code generation task (the first paper focuses on generating parsed API usage examples, and the second is concerned with code summarization (generating text from code))).The relatively high score of the model trained on data assembled from 700 GitHub repositories seems to confirm that the principle of self-documenting code is popular among test automation developers and can be relied upon as a source of training data.</p>
<p>Conclusions and Future Work</p>
<p>In this paper, we have presented a method that exploits the availability of source code in open software repositories to automatically construct an aligned text-code dataset.This method leverages self-documenting code-the software engineering practice of using meaningful class and function names to describe the functionality of program modules.Furthermore, we have demonstrated how datasets constructed in this way can be used to train (MT-inspired) text to source-code generation systems.Moreover, we have shown that it is feasible to use this machine translation-based code generation approach for automatic test case generation.</p>
<p>A key differentiation between our approach and the methods discussed in Section 2 is that the textual descriptions in our parallel corpora are not expressed in true natural language.In NLP, a lack of the naturalness is often considered a weakness, but in the context of the software testing domain, the quasi-natural language nature of the text in our generated datasets does not affect usability, because this language has been devised by the software developer community.This form of communication has a lot in common with Controlled Natural Languages [23]: it uses simplified syntax but preserves most of the natural properties of its base language and can be intuitively understood.As elaborated on in Section 6, this compliance by software developers with naming conventions results in consistent repeatable patterns within the generated datasets which make the learning task feasible.Furthermore, unlike previous attempts to leverage developer-defined descriptions of code [11,12], our approach can be applied to generating code that is not exposed as a public API and therefore lacks inline documentation.Admittedly, escaping one limitation (the restriction to code sourced from public APIs) comes at the cost of another limitation (the restriction to self-documenting code only).</p>
<p>We have demonstrated the feasibility of our approach within the software testing domain.Specifically, our experiments have been on the generation of unit test cases, which can be described in a single quasi-natural language sentence and which have relatively short code bodies.As a result, one open question that remains is whether our approach can be generalized to other (potentially more complex) code domains.This is a question we will address in future work.However, even with this potential limitation the current results are very worthwhile because the demand for automated tests in the modern software development cycle is very high, and we believe it is important to fill the gap in the availability of training data for developing test automation tools, even if the approach is not universal.The fact that existing neural translation models trained on this type of data can achieve satisfactory performance is evidence of the high quality of the text-code parallel corpora synthesized from class and function names.Indeed, we believe that the performance of these initial systems is at a level that permits the immediate application of our approach in the area of software engineering.</p>
<p>That said, the evaluation of the true value of the generated code requires far more effort.As pointed out in Section 5, a BLEU score is an approximate indicator of the quality of translation of natural languages.We have not identified any empirical studies investigating the applicability of BLEU to source code, and therefore the results reported in this and other papers need to be treated with caution.</p>
<p>A more reliable evaluation would involve retrieving feedback from human users.Our current efforts are focused on developing a Test Recommendation Engine trained on the corpora extracted from self-documenting code.The Engine, which is a part of a Horizon 2020 project (https://elastest.eu/), will be released publicly, and we have plans for the collaboration with several industry test teams to gather their feedback.</p>
<p>We envisage two use cases benefiting from automated test generation.The first one involves training a project-specific model, similar to the ones trained on the rx and sf datasets.The tools built using such a specialized model are likely to produce accurate test cases for the new code written as developers add or modify features in an already mature project.The second use case involves a generic model trained on multiple repositories.In this case, the predictions are likely to be less accurate (and so require some editing) but the model would still be of value for test teams working on new projects with a minimal codebase.</p>
<p>Figure 1 .
1
Figure 1.Schematic examples of parallel corpora: (a) for NLP translation (source English sentences aligned with target Vietnamese sentences); (b) for joint processing of natural and programming languages.</p>
<p>Figure 2 .
2
Figure 2. Examples of long, descriptive function names from the spring-framework project.</p>
<p>into individual words, according to the adopted naming convention.(b) Tokenize the function body, preserving punctuation marks.(c) Add the split function name to the corpus as a source sequence.Add the tokenized function body to the corpus as the corresponding target sequence.</p>
<p>Figure 3 .
3
Figure 3. Extracting a text-code sequence pair from a test case.</p>
<p>-did not have an analogous counterpart in the trainset.Training examples (rxt) reveal how the model learned the code sequence representing the string timeout in the description.</p>
<p>Figure 4 .
4
Figure 4. Examples of functions written to validate analogous behavior of Observable (1a, 2a, 3a) and Flowable (1b, 2b, 3b).</p>
<p>Table 1 .
1
Properties of the repositories used for creating project-specific datasets, as of January 2019.
RepositorySize (MB)StarsForksspring-framework11525,920 15,546RxJava7237,1785340</p>
<p>Table 2 .
2
Properties of datasets used in the experiments described in Section 5. NL refers to natural language and PL-to programming language.
rxsfmulti-min5 multi-min10Corpus size10,069 13,792297,728297,728Trainset size805511,033267,689267,689Testset size2014275930,03930,039Vocabulary size (NL)1067267918,80518,805Vocabulary size (PL)303218,42270,417 <em>31,738 </em>Sequence length limit300200150150Avg. sequence length (NL)9111111Avg sequence length (PL)94764545</p>
<p>Table 3 .
3
BLEU scores from training on three datasets.
Training Corpus BLEUrx82.44sf58.00multi-min548.85multi-min1050.50</p>
<p>Table 4 are google/guava (https://github.com/google/guava)and dropwizard/metrics (https://github.com/dropwizard/metrics),represented in the trainset by 288 and 355 examples, respectively.</p>
<p>Acknowledgments: Magdalena Kacmajor would like to include special thanks to Micael Gallego Carrillo from Universidad Rey Juan Carlos in Madrid, who first brought her attention to self-documenting code as a possible source of training data.Funding: This work was partly funded by the EC project ElasTest, 731535 (H2020-ICT-2016-2017/H2020-ICT-2016-1, Research and Innovation Action).The work was also supported by the ADAPT Centre which is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional Development Fund.Author Contributions: Conceptualization, M.K. and J.D.K.; methodology, M.K.; software, M.K.; investigation, M.K; Writing-original draft preparation, M.K.; Writing-review and editing, J.D.K.; Supervision, J.D.K.Conflicts of Interest:The authors declare no conflict of interest.AbbreviationsThe
Understanding DevOps &amp; bridging the gap from continuous integration to continuous delivery. M Virmani, 10.1109/INTECH.2015.7173368Proceedings of the 5th International Conference on the Innovative Computing Technology. the 5th International Conference on the Innovative Computing TechnologyGalicia, SpainINTECH2015. May 2015</p>
<p>Improve software quality through practicing DevOps. P Perera, R Silva, I Perera, 10.1109/ICTER.2017.8257807Proceedings of the 2017 Seventeenth International Conference on Advances in ICT for Emerging Regions (ICTer). the 2017 Seventeenth International Conference on Advances in ICT for Emerging Regions (ICTer)Colombo, Sri Lanka6-9 September 2017</p>
<p>An orchestrated survey of methodologies for automated software test case generation. S Anand, E K Burke, T Y Chen, J Clark, M B Cohen, W Grieskamp, M Harman, M J Harrold, P Mcminn, A Bertolino, 10.1016/j.jss.2013.02.061J. Syst. Softw. 862013. 1978-2001</p>
<p>Do automatically generated unit tests find real faults? An empirical study of effectiveness and challenges (t). S Shamshiri, R Just, J M Rojas, G Fraser, P Mcminn, A Arcuri, 10.1109/ASE.2015.86Proceedings of the 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE). the 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)Lincoln, NE, USANovember 2015</p>
<p>On the naturalness of software. A Hindle, E T Barr, M Gabel, Z Su, P Devanbu, 10.1145/2902362Commun. ACM. 2016</p>
<p>A Survey of Machine Learning for Big Code and Naturalness. M Allamanis, E T Barr, P T Devanbu, C A Sutton, 10.1145/3212695ACM Comput. Surv. 512018</p>
<p>Effective Approaches to Attention-based Neural Machine Translation. T Luong, H Pham, C D Manning, 10.18653/v1/D15-1166Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalSeptember 2015</p>
<p>A Method for Automatic Evaluation of Machine Translation. K Papineni, S Roukos, T Ward, W J Zhu, Bleu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, PA, USAJuly 2002</p>
<p>Learning to generate pseudo-code from source code using statistical machine translation (T). Y Oda, H Fudaba, G Neubig, H Hata, S Sakti, T Toda, S Nakamura, 10.1109/ASE.2015.36Proceedings of the 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE). the 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)Lincoln, NE, USANovember 2015</p>
<p>Latent Predictor Networks for Code Generation. W Ling, P Blunsom, E Grefenstette, K M Hermann, T Kočiský, F Wang, A Senior, 10.18653/v1/P16-1057Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAugust 2016</p>
<p>X Gu, H Zhang, D Zhang, S Kim, Deep, Learning, 10.1145/2950290.2950334Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software EngineeringSeattle, WA, USANovember 2016</p>
<p>A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation. A V Miceli Barone, R Sennrich, Proceedings of the Eighth International Joint Conference on Natural Language Processing. the Eighth International Joint Conference on Natural Language ProcessingTaipei, Taiwan27 November-1 December 2017</p>
<p>Improving Neural Machine Translation Models with Monolingual Data. R Sennrich, B Haddow, A Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAugust 2016</p>
<p>Bimodal Modelling of Source Code and Natural Language. M Allamanis, D Tarlow, A D Gordon, Y Wei, Proceedings of the 32nd International Conference on International Conference on Machine Learning. the 32nd International Conference on International Conference on Machine LearningLille, France6-11 July 2015</p>
<p>Summarizing Source Code using a Neural Attention Model. S Iyer, I Konstas, A Cheung, L Zettlemoyer, 10.18653/v1/P16-1195Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAugust 2016</p>
<p>A Systematically Mined Question-Code Dataset from Stack Overflow. Z Yao, D S Weld, W P Chen, H Sun, Staqc, 10.1145/3178876.3186081Proceedings of the 2018 World Wide Web Conference. the 2018 World Wide Web ConferenceLyon, FranceApril 2018</p>
<p>Learning to mine aligned code and natural language pairs from stack overflow. P Yin, B Deng, E Chen, B Vasilescu, G Neubig, 10.1145/3196398.3196408Proceedings of the 15th International Conference on Mining Software Repositories. the 15th International Conference on Mining Software RepositoriesGothenburg, SwedenMay 2018</p>
<p>Clean Code: A Handbook of Agile Software Craftsmanship. R C Martin, 2008UpperSaddle River, NJ, USA1st ed.</p>
<p>Sequence to Sequence Learning with Neural Networks. I Sutskever, O Vinyals, Q V Le, Proceedings of the 27th International Conference on Neural Information Processing Systems. the 27th International Conference on Neural Information Processing SystemsMontreal, QC, Canada8-13 December 2014</p>
<p>Neural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, arXiv:1409.04732014. 15 February 2019</p>
<p>Recurrent Neural Network Regularization. W Zaremba, I Sutskever, O Vinyals, arXiv:1409.23292014. 2014. 15 February 2019</p>
<p>On the Properties of Neural Machine Translation: Encoder-Decoder Approaches. K Cho, B Van Merrienboer, D Bahdanau, Y Bengio, Proceedings of the SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. the SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical TranslationDoha, Qatar25 October 2014</p>
<p>This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license. T Kuhn, 10.1162/COLI_a_00168Comput. Linguist. 402014A Survey and Classification of Controlled Natural Languages</p>            </div>
        </div>

    </div>
</body>
</html>
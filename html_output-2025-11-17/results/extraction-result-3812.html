<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3812 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3812</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3812</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-90.html">extraction-schema-90</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <p><strong>Paper ID:</strong> paper-595c1f3e364a9dc12a31b6c355efea52f02c1ec5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/595c1f3e364a9dc12a31b6c355efea52f02c1ec5" target="_blank">SymbolicGPT: A Generative Transformer Model for Symbolic Regression</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work presents SymbolicGPT, a novel transformer-based language model for symbolic regression that exploits the advantages of probabilistic language models like GPT, including strength in performance and flexibility.</p>
                <p><strong>Paper Abstract:</strong> Symbolic regression is the task of identifying a mathematical expression that best fits a provided dataset of input and output values. Due to the richness of the space of mathematical expressions, symbolic regression is generally a challenging problem. While conventional approaches based on genetic evolution algorithms have been used for decades, deep learning-based methods are relatively new and an active research area. In this work, we present SymbolicGPT, a novel transformer-based language model for symbolic regression. This model exploits the advantages of probabilistic language models like GPT, including strength in performance and flexibility. Through comprehensive experiments, we show that our model performs strongly compared to competing models with respect to the accuracy, running time, and data efficiency.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3812.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3812.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SymbolicGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SymbolicGPT (Generative Transformer model for Symbolic Regression)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-style transformer conditioned on an order-invariant T-net embedding of point clouds that generates symbolic equation 'skeletons' for symbolic regression; constants are learned post-hoc with numerical optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SymbolicGPT (GPT-style transformer conditioned on T-net embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A custom generative transformer (GPT-style) with l = 8 transformer blocks, character-level tokenization, trainable token and positional embeddings, T-net dataset embedding of dimension e = 512, maximum output length 200 tokens, and top-k sampling with k = 40. The model is trained end-to-end to map order-invariant vector embeddings of point-cloud datasets to symbolic-equation skeletons; constant values are masked during training and later optimized with BFGS.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>symbolic regression / equation discovery (generate symbolic expressions from numerical datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>symbolic mathematics / general scientific modeling (multivariate symbolic regression)</td>
                        </tr>
                        <tr>
                            <td><strong>input_data_type</strong></td>
                            <td>numerical datasets: point clouds of (x,y) datapoints represented as an n x (d+1) matrix; T-net provides order-invariant embeddings (learnable normalization + MLPs + global max-pool).</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>One-time offline pretraining: (1) generate large synthetic dataset of input-output pairs by sampling parse trees (max depth k=4) with a predefined operator set and random constants; (2) convert each dataset to an order-invariant embedding via a T-net; (3) train a GPT-style transformer to produce tokenized equation skeletons with constant placeholders (<C>); (4) at inference, decode skeleton via top-k sampling and replace <C> tokens by optimizing constants with BFGS (SciPy).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>symbolic expressions / equation skeletons (strings) with constant placeholders that are later instantiated with numeric constants</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Synthetic parse-tree dataset generated by the authors: for each experiment setting (d = 1,2,3 or general d in 1..5) they sampled 10,000 training instances, 1,000 validation, 1,000 test; equations constructed from a balanced parse-tree template (max depth k=4) using operator set P = {id, add, mul, sin, pow, cos, sqrt, exp, div, sub, log}, constants sampled from [-2.1,2.1] with insertion probability r=0.5. Inputs x sampled from specified intervals (train: [-3,3]^d; test: ([-5,-3] ∪ [3,5])^d).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Normalized mean squared error (MSE_N) defined as (1/n) Σ (y_i - ŷ_i)^2 / ||y + ε||_2; reported cumulative distributions of log MSE_N across 1000 test instances and average runtime per instance (seconds) with standard deviation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SymbolicGPT achieved lower MSE_N on more test cases than Deep Symbolic Regression (DSR), genetic programming (GP and GP Max), and MLP baselines; produced more highly accurate reconstructions (more cases with log MSE_N < -10). Reported mean runtimes per instance (same hardware) were much lower than other symbolic methods (example: General setting SymbolicGPT 5.0 ± 12.0 s vs GP 48.0 ± 26.7 s, GP Max 84.8 ± 25.8 s, DSR 78.8 ± 42.8 s). SymbolicGPT also showed greater data-efficiency: e.g., with 50 datapoints it outperformed other methods given many more points.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Constants are not predicted directly by the language model and require separate numerical optimization (BFGS), which can fail or be sensitive to initialization; training was performed on synthetic, randomly generated parse-tree equations restricted to a predefined operator set and maximum nesting depth (k=4), so generalization to real-world, noisy, or more complex scientific laws is untested; model size (parameter count) and pretraining dataset scale beyond described synthetic sets are not reported; stochastic decoding (top-k sampling) can produce variable outputs; no experiments reported on extracting laws from textual scholarly corpora (paper focuses on numeric datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Direct empirical comparison against Deep Symbolic Regression (DSR), two GP configurations (GP, GP Max), and a Multilayer Perceptron (MLP) baseline: SymbolicGPT is substantially faster at inference (one-time offline training) and attains lower error on the synthetic testbeds; DSR can be effective for simple equations but is computationally expensive (especially constant optimization); GP Max (larger population) can achieve competitive accuracy but with much larger runtime; MLP is a non-symbolic baseline and does not yield interpretable formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_counterexamples</strong></td>
                            <td>The paper does not present explicit hand-curated counterexamples where SymbolicGPT produces incorrect scientific laws from real scholarly data; implicit failure modes include inability to exactly recover constants solely from the language model (necessitating BFGS), potential failures on expressions outside the training grammar/operator set or deeper nesting than k=4, and untested robustness to noisy/heterogeneous real-world data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SymbolicGPT: A Generative Transformer Model for Symbolic Regression', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep learning for symbolic mathematics <em>(Rating: 2)</em></li>
                <li>A seq2seq approach to symbolic regression <em>(Rating: 2)</em></li>
                <li>Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients <em>(Rating: 2)</em></li>
                <li>Distilling free-form natural laws from experimental data <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3812",
    "paper_id": "paper-595c1f3e364a9dc12a31b6c355efea52f02c1ec5",
    "extraction_schema_id": "extraction-schema-90",
    "extracted_data": [
        {
            "name_short": "SymbolicGPT",
            "name_full": "SymbolicGPT (Generative Transformer model for Symbolic Regression)",
            "brief_description": "A GPT-style transformer conditioned on an order-invariant T-net embedding of point clouds that generates symbolic equation 'skeletons' for symbolic regression; constants are learned post-hoc with numerical optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SymbolicGPT (GPT-style transformer conditioned on T-net embeddings)",
            "model_description": "A custom generative transformer (GPT-style) with l = 8 transformer blocks, character-level tokenization, trainable token and positional embeddings, T-net dataset embedding of dimension e = 512, maximum output length 200 tokens, and top-k sampling with k = 40. The model is trained end-to-end to map order-invariant vector embeddings of point-cloud datasets to symbolic-equation skeletons; constant values are masked during training and later optimized with BFGS.",
            "task_type": "symbolic regression / equation discovery (generate symbolic expressions from numerical datasets)",
            "domain": "symbolic mathematics / general scientific modeling (multivariate symbolic regression)",
            "input_data_type": "numerical datasets: point clouds of (x,y) datapoints represented as an n x (d+1) matrix; T-net provides order-invariant embeddings (learnable normalization + MLPs + global max-pool).",
            "method_description": "One-time offline pretraining: (1) generate large synthetic dataset of input-output pairs by sampling parse trees (max depth k=4) with a predefined operator set and random constants; (2) convert each dataset to an order-invariant embedding via a T-net; (3) train a GPT-style transformer to produce tokenized equation skeletons with constant placeholders (&lt;C&gt;); (4) at inference, decode skeleton via top-k sampling and replace &lt;C&gt; tokens by optimizing constants with BFGS (SciPy).",
            "output_type": "symbolic expressions / equation skeletons (strings) with constant placeholders that are later instantiated with numeric constants",
            "benchmark_or_dataset": "Synthetic parse-tree dataset generated by the authors: for each experiment setting (d = 1,2,3 or general d in 1..5) they sampled 10,000 training instances, 1,000 validation, 1,000 test; equations constructed from a balanced parse-tree template (max depth k=4) using operator set P = {id, add, mul, sin, pow, cos, sqrt, exp, div, sub, log}, constants sampled from [-2.1,2.1] with insertion probability r=0.5. Inputs x sampled from specified intervals (train: [-3,3]^d; test: ([-5,-3] ∪ [3,5])^d).",
            "evaluation_metrics": "Normalized mean squared error (MSE_N) defined as (1/n) Σ (y_i - ŷ_i)^2 / ||y + ε||_2; reported cumulative distributions of log MSE_N across 1000 test instances and average runtime per instance (seconds) with standard deviation.",
            "results_summary": "SymbolicGPT achieved lower MSE_N on more test cases than Deep Symbolic Regression (DSR), genetic programming (GP and GP Max), and MLP baselines; produced more highly accurate reconstructions (more cases with log MSE_N &lt; -10). Reported mean runtimes per instance (same hardware) were much lower than other symbolic methods (example: General setting SymbolicGPT 5.0 ± 12.0 s vs GP 48.0 ± 26.7 s, GP Max 84.8 ± 25.8 s, DSR 78.8 ± 42.8 s). SymbolicGPT also showed greater data-efficiency: e.g., with 50 datapoints it outperformed other methods given many more points.",
            "limitations_or_challenges": "Constants are not predicted directly by the language model and require separate numerical optimization (BFGS), which can fail or be sensitive to initialization; training was performed on synthetic, randomly generated parse-tree equations restricted to a predefined operator set and maximum nesting depth (k=4), so generalization to real-world, noisy, or more complex scientific laws is untested; model size (parameter count) and pretraining dataset scale beyond described synthetic sets are not reported; stochastic decoding (top-k sampling) can produce variable outputs; no experiments reported on extracting laws from textual scholarly corpora (paper focuses on numeric datasets).",
            "comparison_to_other_methods": "Direct empirical comparison against Deep Symbolic Regression (DSR), two GP configurations (GP, GP Max), and a Multilayer Perceptron (MLP) baseline: SymbolicGPT is substantially faster at inference (one-time offline training) and attains lower error on the synthetic testbeds; DSR can be effective for simple equations but is computationally expensive (especially constant optimization); GP Max (larger population) can achieve competitive accuracy but with much larger runtime; MLP is a non-symbolic baseline and does not yield interpretable formulas.",
            "notable_counterexamples": "The paper does not present explicit hand-curated counterexamples where SymbolicGPT produces incorrect scientific laws from real scholarly data; implicit failure modes include inability to exactly recover constants solely from the language model (necessitating BFGS), potential failures on expressions outside the training grammar/operator set or deeper nesting than k=4, and untested robustness to noisy/heterogeneous real-world data.",
            "uuid": "e3812.0",
            "source_info": {
                "paper_title": "SymbolicGPT: A Generative Transformer Model for Symbolic Regression",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep learning for symbolic mathematics",
            "rating": 2
        },
        {
            "paper_title": "A seq2seq approach to symbolic regression",
            "rating": 2
        },
        {
            "paper_title": "Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients",
            "rating": 2
        },
        {
            "paper_title": "Distilling free-form natural laws from experimental data",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        }
    ],
    "cost": 0.00995675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SymbolicGPT: A Generative Transformer Model for Symbolic Regression</h1>
<p>Mojtaba Valipour *<br>University of Waterloo<br>mojtaba.valipour@uwaterloo.ca</p>
<p>Bowen You ${ }^{\dagger}$<br>University of Waterloo<br>byyou@uwaterloo.ca</p>
<p>Maysum Panju ${ }^{\dagger}$<br>University of Waterloo<br>mhpanju@uwaterloo.ca</p>
<p>Ali Ghodsi ${ }^{* \dagger}$<br>University of Waterloo<br>ali.ghodsi@uwaterloo.ca</p>
<h4>Abstract</h4>
<p>Symbolic regression is the task of identifying a mathematical expression that best fits a provided dataset of input and output values. Due to the richness of the space of mathematical expressions, symbolic regression is generally a challenging problem. While conventional approaches based on genetic evolution algorithms have been used for decades, deep learning-based methods are relatively new and an active research area. In this work, we present SymbolicGPT, a novel transformer-based language model for symbolic regression ${ }^{3}$. This model exploits the advantages of probabilistic language models like GPT, including strength in performance and flexibility. Through comprehensive experiments, we show that our model performs strongly compared to competing models with respect to the accuracy, running time, and data efficiency.</p>
<h2>1 Introduction</h2>
<p>Deep learning and neural networks have earned an esteemed reputation for being capable tools for solving a wide variety of problems over countless application domains. Notably, deep language models have made an enormous impact in the field of linguistics and natural language processing. With the advances in technology like Generative Pre-trained Transformers, or GPT [17], the scope of problems now accessible to automated methods continues to grow. It is particularly interesting when language models are used for tasks that, at first glance, do not seem to have any relationship with language at all.</p>
<p>Symbolic regression, the problem of finding a mathematical equation to fit a set of data, is one such task. The objective of symbolic regression is to obtain a closed-form symbolic mathematical expression to describe the relationship between specified predictor and response variables, where the mathematical expression is allowed to be flexible without being restricted to a particular structure or family. More precisely, the goal in symbolic regression is to recover a mathematical function $f$ in terms of the input variables $\boldsymbol{x}=\left[x_{1} \ldots x_{d}\right]^{\top}$, given a set of datapoint vectors of the form $D=\left{\left(\boldsymbol{x}<em i="i">{i}, y</em>\right)\right}<em i="i">{i=1}^{n}$, such that $f\left(\boldsymbol{x}</em>$.
By not imposing any structural constraints on the shape of the desired equation, symbolic regression is a much more difficult task compared to other kinds of regression, such as linear regression or multinomial regression, as the search space of candidate expressions is so much larger.}\right)=y_{i}$ for all $i$. Here, $x_{1}, \ldots, x_{d}, y_{i}$ are scalars and $\boldsymbol{x}_{i} \in \mathbb{R}^{d</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The most common approach for symbolic regression is based on genetic programming, where numerous candidate parse trees are generated, evaluated, combined, and mutated in an evolutionary way until a tree is produced that models an expression that fits the dataset up to a required accuracy level. In essence, it is a search strategy over the vast space of mathematical expressions, seeking the formula that would optimize an objective function.</p>
<p>In this typical framework, which applies not only to genetic methods but also many deep-learningbased approaches to symbolic regression, the goal is to identify a mathematical expression that fits most optimally given a single input dataset. This dataset is the basis over which all the training occurs. Consequently, when presented with any new dataset (as a fresh instance of the task of symbolic regression), the entire training procedure must begin again from scratch.</p>
<p>In this work, we explore an alternative approach to symbolic regression by considering it as a task in language modelling. Symbolic mathematics behaves as a language in its own right, with well-formed mathematical expressions treated as valid "sentences" in this language. It is natural, therefore, to consider using deep language models to address tasks involving symbolic mathematics.</p>
<p>We can frame the regression problem as an exercise in captioning. Each instance takes input in the form of a cloud of points in $\mathbb{R}^{d+1}$, with each point consisting of $d$ components corresponding to $\boldsymbol{x}$ and a single component for the associated $y$ value. The instance returns a statement in the language of symbolic mathematics to describe the point set. By training a model to correctly "caption" datasets with the equations underlying them, we will have a system for performing symbolic regression quickly and accurately.</p>
<p>Based on this idea, we present SymbolicGPT, a method that makes use of deep language models for symbolic regression. SymbolicGPT employs a framework that represents a major shift from the way symbolic regression is conventionally performed. We move the task of symbolic regression from being a strictly quantitative problem into a language one. Effectively, we propose a system that not only learns the language of symbolic mathematics, but also the underlying relationship between point clouds and mathematical expressions that define them.</p>
<p>As part of SymbolicGPT, we make use of a T-net model [16] to represent the input point cloud in an order-invariant way. This allows us to obtain vector embeddings of the entire input dataset for symbolic regression instances without depending on the number of points in the dataset or the order in which they are given.</p>
<p>A major advantage of SymbolicGPT is that we are no longer training a model to learn an equation for an individual dataset in each instance of symbolic regression. Instead, we train a single language model once, and use that trained model to rapidly solve instances of symbolic regression as individual captioning tasks. We will show that SymbolicGPT not only presents a running time speed boost of an order of magnitude or more, but also provides competent performance in accurately reconstructing mathematical equations to fit numerical datasets, presenting a new frontier for language models and a novel direction for approaching symbolic regression.</p>
<h1>2 Related Work</h1>
<p>Traditionally, the problem of symbolic regression has been tackled with methods based on genetic algorithms [12, 1, 20, 13, 22]. In this framework, the task is seen as a search space optimization problem where symbolic expressions are candidates and the expression with the greatest fitness, or fitting accuracy on the training data, is obtained through a process of mutation and evolution. Although this approach has shown success in practice, it is computationally expensive, highly randomized, requires instance-based training, and struggles with learning equations containing many variables and constants.</p>
<p>More recently, newer approaches to symbolic regression have arisen that make use of neural networks. The EQL (Equation Learner) model [11, 19] is an example of performing symbolic regression by training a neural network that represents a symbolic expression. This method, and others based on it $[5,8]$, take advantage of advances in deep learning as an alternative to genetic approaches. However, they still approach symbolic regression as an instance-based problem, training a model from scratch given every new input dataset for a regression task.</p>
<p>A recent study [2] presents a novel, language-based method for handling symbolic regression as a machine translation task, similar to the approach used by [10] for performing symbolic integration and solving differential equations. Given an input dataset, the algorithm treats the input as a text string and passes it through a trained sequence-to-sequence LSTM to produce an output text string that is parsed as the desired symbolic expression. Although this method overcomes the cost of per-instance training, its interpretation of the input dataset as a textual string limits its usability, as the input data must follow specific constraints, such as fitting a 1-dimensional mesh of fixed size. Consequently, this method can only be used in one-dimensional space. However, in most problems, more than one variable is involved and we need to find a multivariate function. In this work, we propose a method that removes such limitations on the structure of input data. This can be applied easily to symbolic regression problems in high-dimensional spaces and when many variables are involved.</p>
<p>Another active area of research is to use deep reinforcement learning methods to tackle this problem [8, 15]. The method presented by Petersen et. al. [15] uses a hybrid approach between traditional genetic algorithms and deep learning methods. Here, the authors use deep RNNs to generate samples of candidate skeletons. As an example, if the function was $f(x)=x^{2}+1$, the corresponding skeleton would be $C_{1} x^{2}+C_{2}$. As in [9], numerical optimization is then used to optimize for the constants of each candidate skeleton. A reinforcement learning algorithm is applied to train the RNN to generate better skeletons at every iteration. However, this method still relies on the iterative nature of traditional genetic algorithms as well as numerical optimization. This results in a computationally intensive process in order to generate a prediction for each equation.</p>
<h1>3 Method</h1>
<p>Our model for symbolic regression, SymbolicGPT, consists of three main stages: obtaining an order-invariant embedding of the input dataset using a T-net [16], obtaining a skeleton equation using a GPT language model [18], and optimizing constant values to fill in the equation skeleton. In addition to discussing each of these steps, we also present the method for generating our equation datasets.</p>
<h3>3.1 Equation Generation</h3>
<p>To train our language model, we need a large dataset of solved instances of symbolic regression. This dataset is a collection of input-label pairs where each input is in the form of a numerical dataset, itself a set of input and output pairs ${\langle\boldsymbol{x}, y\rangle}$, and the corresponding label is a string encoding the symbolic expression governing the relationship between variables in the numerical dataset.</p>
<p>In order to ensure that the language model is able to generalize to unseen equations, having good training data is key. It is necessary to train the model over a wide, diverse set of training equations in order to prevent the language model from overfitting.</p>
<p>There are a number of different ways to randomly sample symbolic mathematical expressions. One approach, as used in [3], is to consider symbolic expressions as constructed by rules in a context-free grammar, and randomly sampling from rules until reaching a string containing only terminal values. Another approach, taken in [10], uses parse-tree representations of symbolic formulas, presenting a method that samples uniformly from all trees of $n$ nodes and then filling in nodes with valid operators or variable values.</p>
<p>For our training dataset, we use an approach similar to the latter, where we start with a blank parse tree and then "decorate" the nodes with choices of operators and variables. In contrast with [10], we do not constrain our parse trees by the number of nodes, but by the number of levels. This enables more control over the maximum level of complexity in the equations used in our training set, as the number of levels in the parse tree corresponds to the number of potential function nestings, a measure of how complex an equation can be.</p>
<p>We begin by fixing $k$, the maximum number of levels in the parse tree for the equations we wish to encounter in our training set. We also begin with a pre-specified number of variables, $d$, and a pre-selected set of operators, $P=\left{u_{1}, \ldots, u_{m}\right}$, that are allowed to appear in any training equation. Then, for each data-equation pair in our training set, we generate a perfectly balanced binary tree of depth $k$, having $2^{k-1}-1$ internal nodes and $2^{k-1}$ leaf nodes. These nodes originally start off empty to form the template of a symbolic expression.</p>
<p>The template is filled in by randomly selecting valid choices to occupy each node in the parse tree. For leaf nodes, each node is randomly assigned with a variable from the set $\left{x_{1}, \ldots, x_{d}\right}$. For interior nodes, operators from the set $P$ are randomly chosen. Once filled in, the parse tree can naturally be interpreted as a symbolic expression. For nodes filled in by binary operators, both of their child nodes are used as input; in the case of unary operators, only the left child is used as input, and the right child is ignored. Importantly, the unary operator "id( $\langle$ )", which returns its input argument unchanged, is included in $P$, which effectively allows for equations with shallower or unbalanced parse trees to still be represented using this template.</p>
<p>Additionally, to ensure that the equations generated are not all too complex, we introduce "terminal" nodes in which children of the terminal nodes are discarded. This ensures that we obtain a diverse set of equations within the training set.</p>
<p>As a final step for the equation generation procedure, constants are incorporated into the equation by inserting them at nodes in the parse tree. Given a specified value $r \in[0,1]$ and constant bounds $c_{\text {min }}$ and $c_{\text {max }}$, for each node in the tree, a random real-valued constant is selected between $c_{\text {min }}$ and $c_{\text {max }}$ and, with probability $r$, is inserted as a multiplicative factor the subtree rooted at that node. Similarly, a second random constant is selected between $c_{\text {min }}$ and $c_{\text {max }}$ and, with probability $r$, is inserted as an additive bias to the subtree rooted at that node. By varying the constant ratio $r$, the equations can be customized to include many constants, few constants, or none at all.</p>
<p>Once an equation is generated, an input dataset for symbolic regression can be produced by evaluating the symbolic expression at $n$ different vectors $\boldsymbol{x}$ randomly sampled from some region of interest in $\mathbb{R}^{d}$. The label value for the symbolic regression instance would be the symbolic expression. This process can be repeated many times to construct the training set by which our SymbolicGPT model will learn how to perform symbolic regression.</p>
<h1>3.2 Order-Invariant Embedding</h1>
<p>Once the training set of input data and output equations is generated, it is used to train our model for translating numerical datasets into equation strings.
The first step is to convert the input dataset $D=\left{\left(\boldsymbol{x}<em i="i">{i}, y</em>\right)\right}<em D="D">{i=1}^{n} \subset \mathbb{R}^{d+1}$ into a single vector embedding $\boldsymbol{w}</em>$. For the conversion to be useful, it must have two properties. First, it should not strictly depend on the number of points in the input dataset, $n$. In practice, the datasets provided as input to a symbolic regression solver may have varying sizes, and we do not want our method to be restricted to cases with a fixed number of input points.} \in \mathbb{R}^{e</p>
<p>Second, the conversion method should not be sensitive to the order in which the points of the dataset are given. The input to a symbolic regression instance is a collection of datapoints, rather than a sequence, and the optimal symbolic expression to fit the dataset should not depend on the order in which the points are listed. Thus, the vector embedding of the dataset should be similarly order-invariant.</p>
<p>Our approach for converting the dataset $D$ into a vector embedding is to use a T-net, a kind of deep network that makes use of a global max-pooling layer to provide order-invariance over its arbitrarily-sized input [16]. The T-net takes as input the dataset $D$, consisting of $n$ datapoints over $d$ variables, represented in matrix format as $X \in \mathbb{R}^{n \times(d+1)}$, where $n$ can be any number and $d$, the number of allowable variables, is fixed in advance. Any symbolic regression instance with fewer than $d$ variables can be padded with 0 values, bringing the total number of variables up to $d$.</p>
<p>The matrix $X$ is first normalized using a learnable normalization layer in order to regulate extreme values from the input. The normalized input points are then passed through three stages of MLP networks. Within each stage, each of the $n$ rows of $X$ are passed individually, albeit in parallel, through a single fully connected layer, where weights are shared between the networks for all points for that stage. The first stage results in $n$ points encoded in $e$-dimensional space; the second stage takes them into $2 e$ dimensions, and the output after the third stage are $n$ points having $4 e$ dimensions each.</p>
<p>The next layer in the T-net is a global max pool, which reduces the $n \times 4 e$ output of the previous step down to a $1 \times 4 e$-dimensional vector. The max-pooling eliminates the dependence on both $n$ and the order of the input points, achieving both goals needed for our vector embedding. Finally, the output of the global max-pool is passed through two more fully connected layers, resulting in a single output</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The architecture of SymbolicGPT. The left box illustrates the structure of our order-invariant T-net for obtaining a vector representation of the input dataset, and the right box shows the structure of the GPT language model for producing symbolic equation skeletons.</p>
<p>vector <strong>w<sub>D</sub></strong>, an <em>e</em>-dimensional embedding of the input dataset. The overall structure of the T-net is shown in the left part of Figure 1.</p>
<h3>3.3 Generative Model Architecture</h3>
<p>The main component of SymbolicGPT is the deep network for producing symbolic equations, as implemented using a GPT language model [17, 18, 4]. This framework takes in two pieces of input: the order-invariant embedding of the point cloud <strong>w<sub>D</sub></strong> as produced by the T-net, representing the input dataset, and a sequence of tokens, <em>X<sub>(eq)</sub></em>, used to initialize the output formula string. In the typical regression case where no information is provided about the output symbolic expression in advance, this token sequence would be the singleton Start-of-Sequence token <em><SO5></em>, although in general it can be any desired prefix of the output equation. The input token sequence is tokenized at a character level and encoded as the matrix <em>W<sub>t</sub></em> using a trainable embedding as part of the GPT model.</p>
<p>The first step in the GPT model is to combine the two inputs <strong>w<sub>D</sub></strong> and <em>W<sub>t</sub></em> together, along with the positional embedding matrix <em>W<sub>p</sub></em>. Based on empirical support, we chose to obtain the combined embedding by taking the sum <em>W<sub>p</sub></em> + <em>W<sub>D</sub></em> + <em>X<sub>eq</sub>W<sub>t</sub></em>, where <em>W<sub>D</sub></em> is the dataset representation vector <strong>w<sub>D</sub></strong> expanded to fit a matrix matching the dimensions of the other embeddings.</p>
<p>The combined vector is then passed through <em>l</em> = 8 successive transformer blocks, using the standard format of GPT models [18]. Each transformer block is a sequential combination of a masked multi-headed self-attention layer and a pointwise feed-forward network, with all blocks feeding into a central residual pathway, similar to ResNets [7].</p>
<p>After <em>l</em> layers of the transformer block, the resulting output vector <strong>h</strong> is passed through a final decoder in the form of a linear projection into a vanilla softmax classifier. The projection uses the transposed token embedding matrix <em>W<sub>t</sub><sup>⊤</sup></em> to map the hidden state vector back into the space of tokens for symbolic expressions. The result of the softmax is a probability vector over tokens in the symbolic equation, which can be sampled to produce the best equation to describe the input dataset. We use top-<em>k</em> sampling with <em>k</em> = 40 for our experiments.</p>
<p>Although the symbolic equation used to generate the data can contain constant values, we do not train the GPT model to recover these values exactly. Instead, constant values in the equation are masked by <em><C></em> tokens during the training phase, and the output of the GPT model is a "skeleton equation" which leaves these constant placeholders in the output string. This is because it is unnecessary to burden the language model with the additional task of learning precise constant values, as this can be easily handled as a separate step.</p>
<h1>3.4 Learning Constants</h1>
<p>Once the GPT model predicts a skeleton equation, we learn values of constants to decorate the skeleton as a post-processing step. This division of tasks is a common approach for string-based regression methods $[9,2]$.
To learning the values of constants in the symbolic equation, we employ BFGS optimization, similar to [2], using an implementation from SciPy [21]. The learned constant values then replace the $\langle C\rangle$ placeholder tokens in the skeleton equation, resulting in the final symbolic expression to represent the given symbolic regression task.</p>
<h3>3.5 Evaluation Metric</h3>
<p>Normally, regression tasks use the mean square error as a metric for measuring the predictive accuracy of an equation. For data following equations with large values, however, this can be problematic, as the residuals can grow very large even when the predicted equation is very close to the true underlying one. To resolve this issue normalize by $|\boldsymbol{y}+\epsilon|<em 2="2">{2}$ where $\epsilon$ is used to avoid division by zero and $|\cdot|</em>$, is given by}$ is the Euclidean norm. Then the normalized mean square error, $M S E_{N</p>
<p>$$
M S E_{N}(\boldsymbol{y}, \hat{\boldsymbol{y}})=\frac{1}{n} \sum_{i=1}^{n} \frac{\left(y_{i}-\hat{y}<em 2="2">{i}\right)^{2}}{|\boldsymbol{y}+\epsilon|</em>
$$}</p>
<h3>3.6 Strengths and Advantages</h3>
<p>Our method exhibits the following strengths and advantages.</p>
<h3>3.6.1 One-Time Training</h3>
<p>In contrast with most approaches for symbolic regression, our method does not start training from scratch given every new problem instance. All of the model training is performed as a one-time procedure that takes place before the GPT transformer is ever used. Thus, SymbolicGPT enjoys all of the benefits of allowing a pretrained model, similar to popular frameworks like BERT [6], which can make use of massive neural networks because the model can be trained offline in advance.
After the model is trained, every instance of symbolic regression can be solved rapidly as a problem in inference. The running time is dependent only on the initial step of reading in the input dataset, obtaining the T-Net embedding, and the final step of optimizing constant values.</p>
<h3>3.6.2 GPT Technology</h3>
<p>Our approach to symbolic regression is based on a probabilistic language model as implemented by GPT. As state-of-the-art language models continue to evolve, our method is expected to organically improve accordingly, with no extra effort in design or implementation, by simply replacing the GPT architecture with any newer and more powerful alternative.</p>
<h3>3.6.3 Scalability</h3>
<p>Our approach addresses two of the main problems with traditional methods. First, our model is able to scale to multiple variables. Iterative methods that choose the best candidate equation at each iteration struggle as the dimension $d$ of the inputs increase since the search space of functions grow exponentially with respect to $d$. By passing in the data points directly as inputs, the model is able to infer the dimension and produce equations accordingly. Second, our model is able to scale in terms of the speed in which we generate predictions. Existing methods that train from scratch for each regression instance are slow compared to our model. These methods incrementally update their model based on computing many candidate equations. For string-based methods, the constants within these candidate equations would need to be optimized as well. This results in a bottleneck in terms of the number of constant optimizations that need to be performed to perform inference. SymbolicGPT only performs this constant optimization once which results in significantly faster inference times. We show empirically that SymbolicGPT produces superior results using significantly less computation time in Section 4.</p>
<p>4 Experiments and Results</p>
<p>To test our model, we implemented SymbolicGPT and trained it in a number of different settings, which we detail below. In all cases, we trained SymbolicGPT over 4 epochs using a batch size of 64. The embedding size for the T-net vector representation is $e=512$, and the maximum equation output length was capped at 200 tokens.</p>
<p>Training and inference for the SymbolicGPT model were performed using an Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz with a single NVIDIA GeForce RTX 2080 11 GB GPU and 32.0 GB Ram. It is noteworthy that our performance scores were achieved using only a single GPU, and scaling up is expected to improve training and inference times even further.</p>
<p>Our experimental framework consists of a large-scale comparison test where we test our model on 1000 different, randomly generated instances of symbolic regression and evaluate performance based on $M S E_{N}$. We repeat this test on four different settings, based on the choice of the dimension $d$ : datasets with one input variable, two variables, three variables, and a random selection between one and five variables. This last setting will be referred to as the “general” experiment.</p>
<p>In each experimental setting, SymbolicGPT was trained using 10,000 randomly generated symbolic regression instances belonging to the associated dimensional configuration, each consisting of an input dataset and an equation label. A further 1000 dataset-equation pairs were generated and used for validation, and 1000 new dataset-equation pairs were generated for a test set he training and validation datasets used values of $\boldsymbol{x}\in[-3.0,3.0]^{d}$, and test datasets took values of $\boldsymbol{x}\in([-5,-3] \cup[3,5])^{d}$. The one, two, and three variables datasets contained 30, 200, and 500 points, respectively. The number of points in the general dataset with $d \in{1,2, \ldots, 5}$ was a randomly selected integer between 10 and 200.</p>
<p>The parse tree templates, as described in Section 3.1, contained a maximum depth of $k=4$ levels and allowable operators coming from the set</p>
<p>$$
P={\operatorname{id}(\cdot), \operatorname{add}(\cdot, \cdot), \operatorname{mul}(\cdot, \cdot), \sin (\cdot), \operatorname{pow}(\cdot, \cdot), \cos (\cdot), \operatorname{sqrt}(\cdot), \operatorname{exp}(\cdot), \operatorname{div}(\cdot, \cdot), \operatorname{sub}(\cdot, \cdot), \log (\cdot)}
$$</p>
<p>Constant values selected from the interval $[-2.1,2.1]$ were randomly inserted using a constant ratio $r=0.5$.</p>
<p>We compared our methods with three existing models for nonlinear regression:</p>
<ol>
<li>Deep Symbolic Regression (DSR): We use the method in [15] to represent the most recent developments in deep learning methods for symbolic regression. The DSR algorithm can be very effective for simple equations, but includes a constant optimization step that is extremely expensive for larger configurations, computationally. In order to complete experiments within a reasonable running time, we limited the population size to be 1000 and trained for a maximum of 10 epochs.</li>
<li>Genetic Programming (GP): We chose to use Python's GPLearn package to represent genetic evolution algorithms for symbolic regression. We use two models with different configurations for this experiment. We refer to GP as the model with a population size of 1000 and 10 generations. GP Max is the model with a population size of 5000 and 20 generations.</li>
<li>Neural Network (MLP): We use a standard Multilayer Perceptron to act as a non-symbolic, nonlinear regressor to use as a baseline for comparison, as implemented in the Python package Scikit-Learn [14].</li>
</ol>
<p>For each method, we evaluated its performance on 1000 test instances of symbolic regression in each of the four experiment settings, using $M S E_{N}$ as the fitness metric. We summarized the results in the cumulative distribution plots of Figure 2, showing the proportion of the test cases that attained error less than any given threshold value. Methods corresponding to curves positioned higher in the plot achieved higher accuracy on more test equations, and hence are better regressors. However, the most important region of the plot is the far left side, as the number of test cases that achieved the lowest possible error is an indication of how often the method would find a highly accurate fitting equation. Some visualized examples of predictions generated by SymbolicGPT are presented in Figure 4.</p>
<p>To give an indication of the speedy performance of SymbolicGPT, we measured and compared the average running time to solve an instance of symbolic regression in the general experiment setting.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Cumulative $\log M S E_{N}$ over all methods and experiments. Each curve shows the proportion of test cases that attained an error score less than every given threshold. SymbolicGPT finds better fitting equations for more test cases than DSR and finds more highly accurate equations (with $\log M S E_{N}&lt;-10$ ) than any other method tested.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Experiment</th>
<th style="text-align: left;">GP</th>
<th style="text-align: left;">GP Max</th>
<th style="text-align: left;">DSR</th>
<th style="text-align: left;">SymbolicGPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">General</td>
<td style="text-align: left;">$48.0 \pm 26.7$</td>
<td style="text-align: left;">$84.8 \pm 25.8$</td>
<td style="text-align: left;">$78.8 \pm 42.8$</td>
<td style="text-align: left;">$\mathbf{5 . 0} \pm 12.0$</td>
</tr>
<tr>
<td style="text-align: left;">One variable</td>
<td style="text-align: left;">$44.6 \pm 33.0$</td>
<td style="text-align: left;">$82.1 \pm 32.1$</td>
<td style="text-align: left;">$15.1 \pm 2.1$</td>
<td style="text-align: left;">$\mathbf{1 . 1} \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: left;">Two variables</td>
<td style="text-align: left;">$47.3 \pm 29.4$</td>
<td style="text-align: left;">$100.8 \pm 31.9$</td>
<td style="text-align: left;">$76.7 \pm 39.8$</td>
<td style="text-align: left;">$\mathbf{3 . 5} \pm 9.0$</td>
</tr>
<tr>
<td style="text-align: left;">Three variables</td>
<td style="text-align: left;">$60.0 \pm 32.9$</td>
<td style="text-align: left;">$109.5 \pm 32.4$</td>
<td style="text-align: left;">$73.3 \pm 56.4$</td>
<td style="text-align: left;">$\mathbf{1 0 . 3} \pm 26.2$</td>
</tr>
</tbody>
</table>
<p>Table 1: Average running times (in seconds) for an instance of symbolic regression during each of the four experiments.</p>
<p>The mean running times, along with standard deviations, are shown in Table 1. In order to make a fair comparison between running times, all experiments were performed using the same computer specifications. We excluded the MLP regressor from this experiment in order to compare between strictly symbolic regression methods. The results show that SymbolicGPT requires significantly less time to solve an instance of symbolic regression compared with other methods, often by an order of magnitude or more, due to most of the computation being shifted to the offline step of setting up the pre-trained model.
We also ran each of the four regression algorithms on symbolic regression instances with the varying number of input datapoints, in order to gauge the data efficiency of each of the methods. The results of this experiment are shown in Figure 3. As the plot indicates, all methods improve performance (by reducing regression error) as more training points are provided; however, SymbolicGPT consistently achieves lower error scores than all other methods, regardless of how many data points are available in the symbolic regression instances. In particular, SymbolicGPT, when given datasets of just 50 points, achieves lower regression error than the other algorithms do on instances with up to 500 points. This is in spite of the fact that the SymbolicGPT model was trained only on datasets of 500 points each: the robustness to differently sized input dataset instances is possibly a consequence of the order-invariant T-embeddings used in the model.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The effect of the number of points on the performance of the model.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Graphical representations of selected equations of one input variable. The solid blue curves are the graphs of the true underlying equations; the orange dotted curves are the predicted functions as generated by SymbolicGPT.</p>
<h1>5 Conclusions</h1>
<p>In this work, we have presented a method that pushes the boundaries of language models and approaches the problem of symbolic regression from a new and powerful direction. We have employed language models in a novel way and with a novel approach, combining them with symbolic mathematics and order-invariant representations of point clouds. Our approach eliminates the per-instance computation expense of most regression methods, and resolves the input restrictions imposed by other language-based regression models. Moreover, our method is fast, scalable, and performs competently on several kinds of symbolic regression problems when compared with existing approaches.</p>
<h2>References</h2>
<p>[1] D. A. Augusto and H. J. Barbosa. Symbolic regression via genetic programming. In Neural Networks, 2000. Proceedings. Sixth Brazilian Symposium on, pages 173-178. IEEE, 2000.
[2] L. Biggio, T. Bendinelli, A. Lucchi, and G. Parascandolo. A seq2seq approach to symbolic regression. In Learning Meets Combinatorial Algorithms at NeurIPS2020, 2020.
[3] J. Brence, L. Todorovski, and S. Džeroski. Probabilistic grammars for equation discovery. Knowledge-Based Systems, page 107077, 2021.
[4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
[5] G. Chen. Learning symbolic expressions via gumbel-max equation learner network. arXiv preprint arXiv:2012.06921, 2020.
[6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[7] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016.
[8] S. Kim, P. Y. Lu, S. Mukherjee, M. Gilbert, L. Jing, V. Čeperić, and M. Soljačić. Integration of neural network-based symbolic regression in deep learning for scientific discovery. IEEE Transactions on Neural Networks and Learning Systems, 2020.
[9] M. Kommenda, B. Burlacu, G. Kronberger, and M. Affenzeller. Parameter identification for symbolic regression using nonlinear least squares. Genetic Programming and Evolvable Machines, pages 1-31, 2019.
[10] G. Lample and F. Charton. Deep learning for symbolic mathematics. International Conference on Learning Representations, 2020.
[11] G. S. Martius and C. Lampert. Extrapolation and learning equations. In 5th International Conference on Learning Representations, ICLR 2017-Workshop Track Proceedings, 2017.
[12] B. McKay, M. J. Willis, and G. W. Barton. Using a tree structured genetic algorithm to perform symbolic regression. In Genetic Algorithms in Engineering Systems: Innovations and Applications, 1995. GALESIA. First International Conference on (Conf. Publ. No. 414), pages 487-492. IET, 1995.
[13] A. Murari, E. Peluso, M. Gelfusa, I. Lupelli, M. Lungaroni, and P. Gaudio. Symbolic regression via genetic programming for data driven derivation of confinement scaling laws without any assumption on their mathematical form. Plasma Physics and Controlled Fusion, 57(1):014008, 2014.</p>
<p>[14] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and Édouard Duchesnay. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12(85):2825-2830, 2011. URL http://jmlr.org/papers/ v12/pedregosa11a.html.
[15] B. K. Petersen, M. L. Larma, T. N. Mundhenk, C. P. Santiago, S. K. Kim, and J. T. Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=m5Qsh0kBQG.
[16] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 652-660, 2017.
[17] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pre-training. preprint, 2018.
[18] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[19] S. Sahoo, C. Lampert, and G. Martius. Learning equations for extrapolation and control. In International Conference on Machine Learning, pages 4442-4450. PMLR, 2018.
[20] M. Schmidt and H. Lipson. Distilling free-form natural laws from experimental data. Science, 324(5923):81-85, 2009.
[21] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, İ. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261-272, 2020. doi: 10.1038/s41592-019-0686-2.
[22] Y. Wang, N. Wagner, and J. M. Rondinelli. Symbolic regression in materials science. MRS Communications, 9(3):793-805, 2019.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>David R. Cheriton School of Computer Science
${ }^{\dagger}$ Department of Statistics and Actuarial Science
${ }^{\dagger}$ Code and results available at https://git.uwaterloo.ca/data-analytics-lab/symbolicgpt2</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
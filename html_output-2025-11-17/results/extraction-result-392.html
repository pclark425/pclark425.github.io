<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-392 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-392</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-392</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-267782428</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.14623v1.pdf" target="_blank">RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation</a></p>
                <p><strong>Paper Abstract:</strong> Rapid progress in high-level task planning and code generation for open-world robot manipulation has been witnessed in Embodied AI. However, previous studies put much effort into general common sense reasoning and task planning capabilities of large-scale language or multi-modal models, relatively little effort on ensuring the deployability of generated code on real robots, and other fundamental components of autonomous robot systems including robot perception, motion planning, and control. To bridge this ``ideal-to-real'' gap, this paper presents \textbf{RobotScript}, a platform for 1) a deployable robot manipulation pipeline powered by code generation; and 2) a code generation benchmark for robot manipulation tasks in free-form natural language. The RobotScript platform addresses this gap by emphasizing the unified interface with both simulation and real robots, based on abstraction from the Robot Operating System (ROS), ensuring syntax compliance and simulation validation with Gazebo. We demonstrate the adaptability of our code generation framework across multiple robot embodiments, including the Franka and UR5 robot arms, and multiple grippers. Additionally, our benchmark assesses reasoning abilities for physical space and constraints, highlighting the differences between GPT-3.5, GPT-4, and Gemini in handling complex physical interactions. Finally, we present a thorough evaluation on the whole system, exploring how each module in the pipeline: code generation, perception, motion planning, and even object geometric properties, impact the overall performance of the system.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e392.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e392.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model from OpenAI used in this work to generate executable Python robot-control code via few-shot prompts, hierarchical decomposition, and chain-of-thought comments; evaluated within the RoboScript pipeline for embodied manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLM accessed via OpenAI API; in RoboScript used with a structured system prompt containing API doc-strings, 10 few-shot code examples, hierarchical sub-agents for function synthesis, and Chain-of-Thought (CoT) comments to decompose tasks into procedural code; not provided raw sensory streams but receives textual scene descriptions and perception-tool outputs via API calls embedded in generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RoboScript benchmark (robotic code generation for free-form manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a scene (RGBD imagery integrated into 3D reconstructions or textual scene descriptions) and a free-form natural language instruction, the model must generate executable Python scripts that call perception tools and motion-planning APIs (ROS/MoveIt) to produce collision-free end-effector trajectories, grasps, and interactions (e.g., open drawer, pick-and-place, place into receptacle) in simulation and on real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation; household tasks; multi-step planning; instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>in-context few-shot prompts (example code), system prompt (API doc-strings), tool outputs provided at runtime (perception APIs / 3D reconstructions / bounding boxes), pretraining of the LLM (implicit knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot prompting with in-context examples, hierarchical sub-agents (for on-demand function generation), Chain-of-Thought (CoT) comments, and code-generation (the model emits Python that calls perception/motion APIs)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Procedural knowledge is represented as generated executable code (sequence of API calls and control flow). Spatial knowledge is accessed and represented by the model via object-centric API arguments: 3D bounding boxes, SE(3) end-effector poses, plane normals, joint axes, and place/grasp poses (i.e., explicit numeric spatial parameters returned by perception tools). Object-relational knowledge is represented symbolically by object names and properties accessible through the perception tools (e.g., get_3d_bbox, get_object_joint_info, parse_place_pose). The model's world knowledge remains primarily implicit in weights, but operationalized via calls to explicit perception and planning data structures (TSDF/meshes/Octomap) that are exposed to generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Grammar correctness (script syntactic validity), Semantic correctness (code logically implements instruction), Finished steps ratio (average completion rate of defined sub-steps), Finished whole task (task success rate), and execution success rate under different perception/planning settings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Under ground-truth perception settings (Gazebo GT 3D data provided to planning tools) GPT-4 achieved 100% grammar correctness and 100% semantic correctness in code generation (as reported). Execution success rates depend on downstream perception and motion modules; exact per-task execution numbers are not exhaustively enumerated in paper text beyond ablation numbers below.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Succeeded at high-level task decomposition, procedural sequencing, calling appropriate perception and motion APIs, composing multi-step plans (e.g., open drawer → re-detect → place), and computing which object properties to request (object-centric reasoning about which API to call). CoT and hierarchical prompting improved generation of multi-step procedural code.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>When failures occurred they were primarily downstream (not code-generation) and due to noisy/no-GT perception, incorrect grasp poses, or motion-planning collisions; GPT-4 produced correct code but could not compensate for inaccurate perception (e.g., enlarged bounding boxes causing self-collision in MoveIt). The model is weak at producing low-level, numerically precise geometric algorithms (e.g., deriving robust analytic grasp transforms) and occasionally relies on assumptions that break with noisy perception.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared qualitatively to GPT-3.5-turbo and Gemini-pro in the same RoboScript pipeline; GPT-4 outperformed both on code-generation grammar and semantics (paper reports GPT-4 reaches 100% correctness while the others are lower).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablations in the pipeline show that using real perception vs GT perception reduces execution success (pick-and-place success dropped by 10.1% and open-drawer tasks by 6.0% when using 3D bounding boxes from 2D grounding and cross-view matching instead of GT). Using Octomap/reconstructed meshes (no GT planning scene) caused larger drops (approx. 14.0% and 15.0%). These show GPT-4's generated plans are sensitive to the fidelity of spatial/perception inputs even when code is correct.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 reliably encodes procedural knowledge as executable code and can utilize explicit spatial/object-relational information when exposed via object-centric perception APIs and example-driven prompts. However, the model itself does not internalize precise geometric representations; instead it delegates spatial computations to perception APIs and motion planners. Thus, the LLM functions best as a high-level planner/program synthesizer and requires explicit numeric spatial inputs or robust perception modules to realize physical actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e392.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e392.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier OpenAI transformer LLM evaluated in RoboScript for code generation; used with the same prompting stack but shows weaker code-generation semantics compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLM used with the RoboScript system prompt, few-shot code examples, and CoT prompting to synthesize robot-control Python code. Operates on textual scene descriptions or perception-tool outputs rather than raw sensor streams.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RoboScript benchmark (robotic code generation for free-form manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same RoboScript tasks as above: generate executable python to perform multi-step manipulation (pick/place, open drawers/doors, place in receptacles) using perception outputs and MoveIt planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation; household tasks; multi-step planning; instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>few-shot in-context examples, system prompt with API doc-strings, implicit pretraining knowledge, and tool outputs at runtime</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot prompting, chain-of-thought comments, hierarchical function-generation sub-agents, code generation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Procedural knowledge emerges as synthesized Python code calling object-centric perception and motion APIs; spatial knowledge is used when explicit spatial parameters (3D bboxes, poses, joint axes) are provided by perception tools; the LLM itself does not directly represent raw sensor data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Grammar correctness, Semantic correctness, Finished steps ratio, Finished whole task (execution success under GT and perception settings)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Quantitative task-level numbers are not fully enumerated in the provided text, but GPT-3.5 performed worse than GPT-4 on code-generation grammar and semantics; produced many grammatically correct snippets but had more logical/code failures than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Capable of producing procedural code and calling the right APIs for many tasks; handled high-level decomposition and function-call ordering in many few-shot cases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>More prone than GPT-4 to hallucinated function arguments, wrong variable types, logical fallacies in generated code, and failures on longer-horizon or more complex tasks; downstream execution failures when perception noise or geometric constraints were challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly to GPT-4 (superior) and Gemini-pro (GPT-3.5 outperforms Gemini-pro in code quality in this study).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Same pipeline ablations apply: execution performance drops when perception is noisy (≈10–15% drops depending on setup). GPT-3.5's weaker code semantics exacerbate failures when perception is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5 encodes procedural and some object-relational reasoning enough to generate working controller code in many cases, but less reliably than GPT-4; like GPT-4 it depends on explicit spatial data fed via perception APIs and cannot by itself overcome noisy or missing geometric inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e392.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e392.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-pro (Gemini family multimodal model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal LLM evaluated in RoboScript for code generation; in this study it produced lower-quality code outputs, with frequent format/hallucination issues and lower attention to system prompt constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal foundation model (referred to as Gemini-pro in the paper) evaluated for code generation under the RoboScript prompting regimen; required extra prompt engineering to try to constrain output format but still underperformed other LLMs in this pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RoboScript benchmark (robotic code generation for free-form manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same RoboScript tasks: generate executable robot-control Python calling perception and motion APIs for manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation; household tasks; multi-step planning; instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>system prompt, few-shot examples (in-context), and perception-tool outputs when the generated code queries them</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot prompting, additional instruction prompts (attempted), code generation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Like other LLMs, procedural knowledge is produced as code; spatial/object knowledge must be accessed via perception APIs and is represented as explicit numeric parameters in code. The model did not reliably follow the constrained system-prompted API docstrings in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Grammar correctness, Semantic correctness, Finished steps ratio, Finished whole task</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Quantitative numbers not provided in the excerpt; the paper reports Gemini-pro performed worse than GPT-3.5 and GPT-4 in code-generation quality, with near-failure rate without extra prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Occasionally produces plausible high-level plans and API sequences when prompted carefully.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Frequently produced incorrect output format, hallucinated functions/assumptions (e.g., wrong naming conventions for objects), and ignored system-prompt constraints — leading to high failure rates for executable code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperforms GPT-3.5-turbo and GPT-4 on the RoboScript code-generation tasks in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not separately ablated beyond general pipeline tests; overall system-level ablations (perception GT vs real perception) still apply: noisy perception reduces execution success regardless of model.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gemini-pro in this evaluation was less reliable at following structured system prompts and generating syntactically and semantically correct control code; this highlights that robustness to constrained program-generation prompts is critical for LLM-driven embodied planning when the model does not have direct sensory grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e392.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e392.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboScript</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboScript pipeline and benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified ROS/Gazebo/MoveIt-based platform and benchmark that evaluates LLMs generating executable robot-control code for free-form manipulation tasks, integrating perception (2D grounding, TSDF-based 3D reconstruction, AnyGrasp/GIGA grasping, GAMMA joint prediction), motion planning, and control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (benchmark/platform)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a neural model — a system that supplies LLMs with: system prompts that include API doc-strings, perception tools that convert RGBD to 3D representations (TSDF → point clouds/meshes/Octomap), object-centric perception APIs (get_3d_bbox, get_object_joint_info, get_plane_normal, parse_place_pose, parse_adaptive_shape_grasp_pose), and motion-planning wrappers (MoveIt/OMPL) so the LLM generates Python programs which are executed in simulation or real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RoboScript benchmark (free-form manipulation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Eight free-form manipulation tasks of varying difficulty (simple: place apple in bowl; moderate: open second-top drawer and place boxes; hard: clear table into basket, swap positions) executed in Gazebo or on real robots; tasks require object selection, spatial placement, articulated-object manipulation, and multi-step, perception-in-the-loop planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation; household tasks; multi-step planning; instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>perception pipeline (RGBD → TSDF → 3D bboxes/meshes/point clouds), LLM in-context examples & system prompt, explicit API tool outputs, and optionally ground-truth simulation data (GT planning scene)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>code generation (LLM synthesizes Python that calls perception and motion APIs), in-context few-shot prompting, chain-of-thought comments, hierarchical sub-agents for function synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Spatial: explicit numeric forms (3D bounding boxes [x_min,y_min,z_min,x_max,y_max,z_max], SE(3) poses, plane normals, TSDF-derived meshes, Octomap) exposed to generated code; Procedural: code sequences and subroutines (grasp → move_to_pose → attach_object → parse_place_pose → follow_path); Object-relational: object-centric API design (object names and properties as first-class entities), kinematic joint info (GAMMA predictions) and affordances via AnyGrasp/GIGA outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Grammar correctness, Semantic correctness, Finished steps ratio, Finished whole task (execution success), task-specific success rates under GT vs perception-based settings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported: GPT-4 code generation reached 100% grammar and semantic correctness under ground-truth perception; ablation: enabling real perception (3D bboxes from 2D grounding) reduced pick-and-place success by 10.1% and open-drawer tasks by 6.0% (relative to GT detection & planning); using Octomap/reconstructed meshes instead of GT planning scene reduced performance by ~14.0% and ~15.0% respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>When perception is high-fidelity, LLM-generated code successfully sequences multi-step manipulation and uses perception outputs (3D bboxes, joint axes, grasp poses) to generate appropriate motion plans; object-centric API design allows the model to request and combine spatial/object-relational information effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Execution failures stem more from imperfect perception and grasp/motion modules than from high-level planning: noisy 3D instance detection can produce inflated bounding boxes that cause MoveIt to detect self-collisions and reject plans; thin two-finger grippers lead to unstable grasps for certain shapes in constrained spaces; LLMs sometimes hallucinate API usage or make incorrect assumptions about object naming/contents (Gemini example for empty-plate check). LLMs are weak at producing numerically robust geometric algorithms themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Benchmarked LLMs include GPT-4 (best), GPT-3.5-turbo (middle), Gemini-pro (worst). Perception baselines include GT detection & GT planning scene vs perception-built Octomap/meshes. Motion-planning algorithms include RRT* and others via OMPL (RRT* primarily used).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Perception ablations: (1) GT Detection & Planning (best execution performance); (2) GT Planning Scene with perception-derived object instances (medium); (3) No GT (full perception) degraded performance: pick-and-place -10.1% (3D bbox perception) and -14.0% (Octomap) relative to GT; open-drawer -6.0% and -15.0% respectively. Also ablated grasp pose estimators (AnyGrasp, GIGA, heuristic central lift) and demonstrated object-shape-dependent failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RoboScript demonstrates that LLMs can represent procedural knowledge as executable programs that orchestrate perception and planning but rely on explicit spatial information exposed through perception APIs rather than encoding precise metric spatial maps internally. Object-centric tool APIs are effective intermediaries: they let LLMs reason symbolically about objects and request required spatial parameters (3D bboxes, joint axes, plane normals) needed for motion planning. The main bottlenecks for embodied execution are perception noise, grasp stability, and motion-planning sensitivity, not the LLM's high-level planning when perception is GT-quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Code as policies: Language model programs for embodied control. <em>(Rating: 2)</em></li>
                <li>Palm-e: An embodied multimodal language model. <em>(Rating: 2)</em></li>
                <li>Do as i can and not as i say: Grounding language in robotic affordances. <em>(Rating: 2)</em></li>
                <li>Grounded decoding: Guiding text generation with grounded models for robot control. <em>(Rating: 2)</em></li>
                <li>Voxposer: Composable 3d value maps for robotic manipulation with language models. <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-392",
    "paper_id": "paper-267782428",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A state-of-the-art large language model from OpenAI used in this work to generate executable Python robot-control code via few-shot prompts, hierarchical decomposition, and chain-of-thought comments; evaluated within the RoboScript pipeline for embodied manipulation tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_description": "Autoregressive transformer LLM accessed via OpenAI API; in RoboScript used with a structured system prompt containing API doc-strings, 10 few-shot code examples, hierarchical sub-agents for function synthesis, and Chain-of-Thought (CoT) comments to decompose tasks into procedural code; not provided raw sensory streams but receives textual scene descriptions and perception-tool outputs via API calls embedded in generated code.",
            "task_name": "RoboScript benchmark (robotic code generation for free-form manipulation)",
            "task_description": "Given a scene (RGBD imagery integrated into 3D reconstructions or textual scene descriptions) and a free-form natural language instruction, the model must generate executable Python scripts that call perception tools and motion-planning APIs (ROS/MoveIt) to produce collision-free end-effector trajectories, grasps, and interactions (e.g., open drawer, pick-and-place, place into receptacle) in simulation and on real robots.",
            "task_type": "object manipulation; household tasks; multi-step planning; instruction following",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "in-context few-shot prompts (example code), system prompt (API doc-strings), tool outputs provided at runtime (perception APIs / 3D reconstructions / bounding boxes), pretraining of the LLM (implicit knowledge)",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot prompting with in-context examples, hierarchical sub-agents (for on-demand function generation), Chain-of-Thought (CoT) comments, and code-generation (the model emits Python that calls perception/motion APIs)",
            "knowledge_representation": "Procedural knowledge is represented as generated executable code (sequence of API calls and control flow). Spatial knowledge is accessed and represented by the model via object-centric API arguments: 3D bounding boxes, SE(3) end-effector poses, plane normals, joint axes, and place/grasp poses (i.e., explicit numeric spatial parameters returned by perception tools). Object-relational knowledge is represented symbolically by object names and properties accessible through the perception tools (e.g., get_3d_bbox, get_object_joint_info, parse_place_pose). The model's world knowledge remains primarily implicit in weights, but operationalized via calls to explicit perception and planning data structures (TSDF/meshes/Octomap) that are exposed to generated code.",
            "performance_metric": "Grammar correctness (script syntactic validity), Semantic correctness (code logically implements instruction), Finished steps ratio (average completion rate of defined sub-steps), Finished whole task (task success rate), and execution success rate under different perception/planning settings",
            "performance_result": "Under ground-truth perception settings (Gazebo GT 3D data provided to planning tools) GPT-4 achieved 100% grammar correctness and 100% semantic correctness in code generation (as reported). Execution success rates depend on downstream perception and motion modules; exact per-task execution numbers are not exhaustively enumerated in paper text beyond ablation numbers below.",
            "success_patterns": "Succeeded at high-level task decomposition, procedural sequencing, calling appropriate perception and motion APIs, composing multi-step plans (e.g., open drawer → re-detect → place), and computing which object properties to request (object-centric reasoning about which API to call). CoT and hierarchical prompting improved generation of multi-step procedural code.",
            "failure_patterns": "When failures occurred they were primarily downstream (not code-generation) and due to noisy/no-GT perception, incorrect grasp poses, or motion-planning collisions; GPT-4 produced correct code but could not compensate for inaccurate perception (e.g., enlarged bounding boxes causing self-collision in MoveIt). The model is weak at producing low-level, numerically precise geometric algorithms (e.g., deriving robust analytic grasp transforms) and occasionally relies on assumptions that break with noisy perception.",
            "baseline_comparison": "Compared qualitatively to GPT-3.5-turbo and Gemini-pro in the same RoboScript pipeline; GPT-4 outperformed both on code-generation grammar and semantics (paper reports GPT-4 reaches 100% correctness while the others are lower).",
            "ablation_results": "Ablations in the pipeline show that using real perception vs GT perception reduces execution success (pick-and-place success dropped by 10.1% and open-drawer tasks by 6.0% when using 3D bounding boxes from 2D grounding and cross-view matching instead of GT). Using Octomap/reconstructed meshes (no GT planning scene) caused larger drops (approx. 14.0% and 15.0%). These show GPT-4's generated plans are sensitive to the fidelity of spatial/perception inputs even when code is correct.",
            "key_findings": "GPT-4 reliably encodes procedural knowledge as executable code and can utilize explicit spatial/object-relational information when exposed via object-centric perception APIs and example-driven prompts. However, the model itself does not internalize precise geometric representations; instead it delegates spatial computations to perception APIs and motion planners. Thus, the LLM functions best as a high-level planner/program synthesizer and requires explicit numeric spatial inputs or robust perception modules to realize physical actions.",
            "uuid": "e392.0",
            "source_info": {
                "paper_title": "RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-turbo",
            "brief_description": "An earlier OpenAI transformer LLM evaluated in RoboScript for code generation; used with the same prompting stack but shows weaker code-generation semantics compared to GPT-4.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": null,
            "model_description": "Autoregressive transformer LLM used with the RoboScript system prompt, few-shot code examples, and CoT prompting to synthesize robot-control Python code. Operates on textual scene descriptions or perception-tool outputs rather than raw sensor streams.",
            "task_name": "RoboScript benchmark (robotic code generation for free-form manipulation)",
            "task_description": "Same RoboScript tasks as above: generate executable python to perform multi-step manipulation (pick/place, open drawers/doors, place in receptacles) using perception outputs and MoveIt planning.",
            "task_type": "object manipulation; household tasks; multi-step planning; instruction following",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "few-shot in-context examples, system prompt with API doc-strings, implicit pretraining knowledge, and tool outputs at runtime",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot prompting, chain-of-thought comments, hierarchical function-generation sub-agents, code generation",
            "knowledge_representation": "Procedural knowledge emerges as synthesized Python code calling object-centric perception and motion APIs; spatial knowledge is used when explicit spatial parameters (3D bboxes, poses, joint axes) are provided by perception tools; the LLM itself does not directly represent raw sensor data.",
            "performance_metric": "Grammar correctness, Semantic correctness, Finished steps ratio, Finished whole task (execution success under GT and perception settings)",
            "performance_result": "Quantitative task-level numbers are not fully enumerated in the provided text, but GPT-3.5 performed worse than GPT-4 on code-generation grammar and semantics; produced many grammatically correct snippets but had more logical/code failures than GPT-4.",
            "success_patterns": "Capable of producing procedural code and calling the right APIs for many tasks; handled high-level decomposition and function-call ordering in many few-shot cases.",
            "failure_patterns": "More prone than GPT-4 to hallucinated function arguments, wrong variable types, logical fallacies in generated code, and failures on longer-horizon or more complex tasks; downstream execution failures when perception noise or geometric constraints were challenging.",
            "baseline_comparison": "Compared directly to GPT-4 (superior) and Gemini-pro (GPT-3.5 outperforms Gemini-pro in code quality in this study).",
            "ablation_results": "Same pipeline ablations apply: execution performance drops when perception is noisy (≈10–15% drops depending on setup). GPT-3.5's weaker code semantics exacerbate failures when perception is imperfect.",
            "key_findings": "GPT-3.5 encodes procedural and some object-relational reasoning enough to generate working controller code in many cases, but less reliably than GPT-4; like GPT-4 it depends on explicit spatial data fed via perception APIs and cannot by itself overcome noisy or missing geometric inputs.",
            "uuid": "e392.1",
            "source_info": {
                "paper_title": "RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Gemini-pro",
            "name_full": "Gemini-pro (Gemini family multimodal model)",
            "brief_description": "A multimodal LLM evaluated in RoboScript for code generation; in this study it produced lower-quality code outputs, with frequent format/hallucination issues and lower attention to system prompt constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini-pro",
            "model_size": null,
            "model_description": "A multimodal foundation model (referred to as Gemini-pro in the paper) evaluated for code generation under the RoboScript prompting regimen; required extra prompt engineering to try to constrain output format but still underperformed other LLMs in this pipeline.",
            "task_name": "RoboScript benchmark (robotic code generation for free-form manipulation)",
            "task_description": "Same RoboScript tasks: generate executable robot-control Python calling perception and motion APIs for manipulation tasks.",
            "task_type": "object manipulation; household tasks; multi-step planning; instruction following",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "system prompt, few-shot examples (in-context), and perception-tool outputs when the generated code queries them",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot prompting, additional instruction prompts (attempted), code generation",
            "knowledge_representation": "Like other LLMs, procedural knowledge is produced as code; spatial/object knowledge must be accessed via perception APIs and is represented as explicit numeric parameters in code. The model did not reliably follow the constrained system-prompted API docstrings in this work.",
            "performance_metric": "Grammar correctness, Semantic correctness, Finished steps ratio, Finished whole task",
            "performance_result": "Quantitative numbers not provided in the excerpt; the paper reports Gemini-pro performed worse than GPT-3.5 and GPT-4 in code-generation quality, with near-failure rate without extra prompt engineering.",
            "success_patterns": "Occasionally produces plausible high-level plans and API sequences when prompted carefully.",
            "failure_patterns": "Frequently produced incorrect output format, hallucinated functions/assumptions (e.g., wrong naming conventions for objects), and ignored system-prompt constraints — leading to high failure rates for executable code generation.",
            "baseline_comparison": "Underperforms GPT-3.5-turbo and GPT-4 on the RoboScript code-generation tasks in this study.",
            "ablation_results": "Not separately ablated beyond general pipeline tests; overall system-level ablations (perception GT vs real perception) still apply: noisy perception reduces execution success regardless of model.",
            "key_findings": "Gemini-pro in this evaluation was less reliable at following structured system prompts and generating syntactically and semantically correct control code; this highlights that robustness to constrained program-generation prompts is critical for LLM-driven embodied planning when the model does not have direct sensory grounding.",
            "uuid": "e392.2",
            "source_info": {
                "paper_title": "RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RoboScript",
            "name_full": "RoboScript pipeline and benchmark",
            "brief_description": "A unified ROS/Gazebo/MoveIt-based platform and benchmark that evaluates LLMs generating executable robot-control code for free-form manipulation tasks, integrating perception (2D grounding, TSDF-based 3D reconstruction, AnyGrasp/GIGA grasping, GAMMA joint prediction), motion planning, and control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (benchmark/platform)",
            "model_size": null,
            "model_description": "Not a neural model — a system that supplies LLMs with: system prompts that include API doc-strings, perception tools that convert RGBD to 3D representations (TSDF → point clouds/meshes/Octomap), object-centric perception APIs (get_3d_bbox, get_object_joint_info, get_plane_normal, parse_place_pose, parse_adaptive_shape_grasp_pose), and motion-planning wrappers (MoveIt/OMPL) so the LLM generates Python programs which are executed in simulation or real robots.",
            "task_name": "RoboScript benchmark (free-form manipulation tasks)",
            "task_description": "Eight free-form manipulation tasks of varying difficulty (simple: place apple in bowl; moderate: open second-top drawer and place boxes; hard: clear table into basket, swap positions) executed in Gazebo or on real robots; tasks require object selection, spatial placement, articulated-object manipulation, and multi-step, perception-in-the-loop planning.",
            "task_type": "object manipulation; household tasks; multi-step planning; instruction following",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "perception pipeline (RGBD → TSDF → 3D bboxes/meshes/point clouds), LLM in-context examples & system prompt, explicit API tool outputs, and optionally ground-truth simulation data (GT planning scene)",
            "has_direct_sensory_input": true,
            "elicitation_method": "code generation (LLM synthesizes Python that calls perception and motion APIs), in-context few-shot prompting, chain-of-thought comments, hierarchical sub-agents for function synthesis",
            "knowledge_representation": "Spatial: explicit numeric forms (3D bounding boxes [x_min,y_min,z_min,x_max,y_max,z_max], SE(3) poses, plane normals, TSDF-derived meshes, Octomap) exposed to generated code; Procedural: code sequences and subroutines (grasp → move_to_pose → attach_object → parse_place_pose → follow_path); Object-relational: object-centric API design (object names and properties as first-class entities), kinematic joint info (GAMMA predictions) and affordances via AnyGrasp/GIGA outputs.",
            "performance_metric": "Grammar correctness, Semantic correctness, Finished steps ratio, Finished whole task (execution success), task-specific success rates under GT vs perception-based settings",
            "performance_result": "Reported: GPT-4 code generation reached 100% grammar and semantic correctness under ground-truth perception; ablation: enabling real perception (3D bboxes from 2D grounding) reduced pick-and-place success by 10.1% and open-drawer tasks by 6.0% (relative to GT detection & planning); using Octomap/reconstructed meshes instead of GT planning scene reduced performance by ~14.0% and ~15.0% respectively.",
            "success_patterns": "When perception is high-fidelity, LLM-generated code successfully sequences multi-step manipulation and uses perception outputs (3D bboxes, joint axes, grasp poses) to generate appropriate motion plans; object-centric API design allows the model to request and combine spatial/object-relational information effectively.",
            "failure_patterns": "Execution failures stem more from imperfect perception and grasp/motion modules than from high-level planning: noisy 3D instance detection can produce inflated bounding boxes that cause MoveIt to detect self-collisions and reject plans; thin two-finger grippers lead to unstable grasps for certain shapes in constrained spaces; LLMs sometimes hallucinate API usage or make incorrect assumptions about object naming/contents (Gemini example for empty-plate check). LLMs are weak at producing numerically robust geometric algorithms themselves.",
            "baseline_comparison": "Benchmarked LLMs include GPT-4 (best), GPT-3.5-turbo (middle), Gemini-pro (worst). Perception baselines include GT detection & GT planning scene vs perception-built Octomap/meshes. Motion-planning algorithms include RRT* and others via OMPL (RRT* primarily used).",
            "ablation_results": "Perception ablations: (1) GT Detection & Planning (best execution performance); (2) GT Planning Scene with perception-derived object instances (medium); (3) No GT (full perception) degraded performance: pick-and-place -10.1% (3D bbox perception) and -14.0% (Octomap) relative to GT; open-drawer -6.0% and -15.0% respectively. Also ablated grasp pose estimators (AnyGrasp, GIGA, heuristic central lift) and demonstrated object-shape-dependent failure modes.",
            "key_findings": "RoboScript demonstrates that LLMs can represent procedural knowledge as executable programs that orchestrate perception and planning but rely on explicit spatial information exposed through perception APIs rather than encoding precise metric spatial maps internally. Object-centric tool APIs are effective intermediaries: they let LLMs reason symbolically about objects and request required spatial parameters (3D bboxes, joint axes, plane normals) needed for motion planning. The main bottlenecks for embodied execution are perception noise, grasp stability, and motion-planning sensitivity, not the LLM's high-level planning when perception is GT-quality.",
            "uuid": "e392.3",
            "source_info": {
                "paper_title": "RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Code as policies: Language model programs for embodied control.",
            "rating": 2,
            "sanitized_title": "code_as_policies_language_model_programs_for_embodied_control"
        },
        {
            "paper_title": "Palm-e: An embodied multimodal language model.",
            "rating": 2,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "Do as i can and not as i say: Grounding language in robotic affordances.",
            "rating": 2,
            "sanitized_title": "do_as_i_can_and_not_as_i_say_grounding_language_in_robotic_affordances"
        },
        {
            "paper_title": "Grounded decoding: Guiding text generation with grounded models for robot control.",
            "rating": 2,
            "sanitized_title": "grounded_decoding_guiding_text_generation_with_grounded_models_for_robot_control"
        },
        {
            "paper_title": "Voxposer: Composable 3d value maps for robotic manipulation with language models.",
            "rating": 2,
            "sanitized_title": "voxposer_composable_3d_value_maps_for_robotic_manipulation_with_language_models"
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models.",
            "rating": 2,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        }
    ],
    "cost": 0.01723875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation
22 Feb 2024</p>
<p>Junting Chen 
Yao Mu 
Qiaojun Yu 
Tsinghua University
5 UC Berkeley 6</p>
<p>Tianming Wei 
Tsinghua University
5 UC Berkeley 6</p>
<p>Silang Wu 
Zhecheng Yuan 
Zhixuan Liang 
Shanghai Jiao Tong University</p>
<p>Chao Yang 
Kaipeng Zhang 
Wenqi Shao 
Yu Qiao 
Huazhe Xu 
Mingyu Ding 
Ping Luo 
Shanghai Ai Opengvlab 
Laboratory 
Eth Zurich </p>
<p>The University of Hong Kong</p>
<p>RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation
22 Feb 20241A21B436DD5E4B0F57B82075C076C89DarXiv:2402.14623v1[cs.RO]
Rapid progress in high-level task planning and code generation for open-world robot manipulation has been witnessed in Embodied AI.However, previous studies put much effort into general common sense reasoning and task planning capabilities of large-scale language or multi-modal models, relatively little effort on ensuring the deployability of generated code on real robots, and other fundamental components of autonomous robot systems including robot perception, motion planning, and control.To bridge this "ideal-to-real" gap, this paper presents RobotScript, a platform for 1) a deployable robot manipulation pipeline powered by code generation; and 2) a code generation benchmark for robot manipulation tasks in free-form natural language.The RobotScript platform addresses this gap by emphasizing the unified interface with both simulation and real robots, based on abstraction from the Robot Operating System (ROS), ensuring syntax compliance and simulation validation with Gazebo.We demonstrate the adaptability of our code generation framework across multiple robot embodiments, including the Franka and UR5 robot arms, and multiple grippers.Additionally, our benchmark assesses reasoning abilities for physical space and constraints, highlighting the differences between GPT-3.5, GPT-4, and Gemini in handling complex physical interactions.Finally, we present a thorough evaluation on the whole system, exploring how each module in the pipeline: code generation, perception, motion planning, and even object geometric properties, impact the overall performance of the system.</p>
<p>I. INTRODUCTION</p>
<p>Leveraging pre-trained language models for robotic applications is an active research area, with many works focusing on planning and reasoning [24,72,8,49,25,51,23,47,52,57,37,10,26,13,71,64,40,45,28,58,65,59].To enable language models to perceive physical environments, textual scene descriptions [25,72,51] or perception tools [36] are provided.Integrating large language models (LLMs) with robots has significantly advanced robotics, enhancing decision-making and control through improved language understanding and task execution.However, current benchmarks focus more on high-level semantic understanding, often This work was completed during the internship at the OpenGV Lab, Shanghai AI Laboratory by Yao  overlooking the nuances of lower-level control and physical constraints in robotic manipulation.</p>
<p>To address this gap, we introduce the RobotScript Benchmark, which maps human language instructions to robot motion planning and provides evaluation within physically realistic contexts.Going beyond traditional semantic parsing, it incorporates nuances of physical interactions, workspace constraints, and object properties that are critical for realworld robotic applications.By assessing both conceptual and embodied intelligence, RobotScript combines the rich semantic and pragmatic understanding of large language models with the fine-grained motor control needed in dynamic, real-world settings.The designed system provides a complete pipeline from 2D image detection to 3D scene modeling, grasp pose prediction, and finally, motion planning.This enables the robot to both understand high-level natural language commands and autonomously leverage various perception tools and planning algorithms.As a result, it can generate low-level motion control, without the need for supervised information.</p>
<p>The RobotScript Benchmark revolves around three core components: 1) ROS-based Code Generation Testing: The proposed benchmark facilitates testing and validation of generated code within the Robot Operating System (ROS) framework, enabling direct deployment on physical robots.It supports connectivity with various sensors, actuators, and large neural network models, ensuring the syntactic validity of generated code and simulation-based testing in Gazebo.To demonstrate the versatility across robotic platforms, we evaluated our framework on different robot embodiments, assessing the impact of mechanical designs and gripper types on task performance.Historically, the incompatibility between the ROS and conda environments has impeded the deployment and testing of AI models on robotic hardware.By leveraging recent advancements in Robostack [17], which is a conda-compatible ROS infrastructure, our benchmark enables seamless model implementation within any conda environment.2) Perception-in-the-loop Benchmark Pipeline: In order to make our tests more closely resemble real robot Description Format) data.The system utilizes various perception tools, such as grasp detection, 2D grounding, 3D perception, and joint prediction, to interpret the input data.These tools are integrated with motion planning tools that include arm planning, gripper force control, and solving place pose with inverse kinematics (IK).The Robot Operating System (ROS) serves as the middleware to provide abstraction to sensor drivers, controllers, and robot definitions across real robots and the simulation.The framework controls multiple real robots and their counterparts in the simulation with a unified code generation pipeline.This modular approach enables flexibility in robotic applications and adaptability to new code generation methods or robot architectures, from simple tasks to complex manipulations.</p>
<p>scenarios, in our benchmark, the inputs of both the robot planner and controller are based on the results from robot perception.We provide multiple tools, such as AnyGrasp [16], to predict a grasp pose from a segmented object point cloud which is constructed by multi-view 3D fusion, and then used as the input to the grasp motion planning API.Basing the motion planning on the perceived grasp pose, rather than ground truth, introduces realism and potential errors that would occur in a live system.However, we also supply the original ground truth RGBD images and 3D bounding box data to mitigate excessive noise from potential perception failures.This pipeline is directly deployable to real robots by using sensor RGBD images and predicted bounding boxes.3) Physical Space and Constraint Reasoning: Our benchmark introduces a comprehensive testbed that evaluates reasoning capabilities regarding physical space and constraints.These tests involve understanding complex interactions among various objects, such as calculating feasible transitional positions without causing interference with other objects.The benchmark underscores the challenges in constraint satisfaction by highlighting the performance differences between GPT-3.5 and GPT-4 in these scenarios.</p>
<p>To summarize, the contributions of this paper include:</p>
<p>• Comprehensive Integration of LLMs with Robotics: RoboScript effectively bridges the gap between high-level semantic understanding, as well as the practical nuances of lower-level control and physical constraints in robotic manipulation.It provides an autonomous manipulation pipeline covering task interpretation, object detection, pose estimation, grasp planning, and motion planning.• In-depth Ablation Study on System Modules: We provide an ablation study on each system module, deeply analyzing the impact of individual module errors.This enhances the benchmark's relevance and practicality in real-world robotic applications.• Assessment of LLM Reasoning for Physical Interactions:</p>
<p>Our benchmark assesses reasoning abilities for physical space and constraints, highlighting the differences between GPT-3.5, GPT-4, and Gemini in handling complex physical interactions.</p>
<p>II. RELATED WORK A. Large Language Models for Robotics</p>
<p>Recent research has delved into the integration of large language models (LLMs) with embodied AI tasks, focusing on planning, reasoning, and control [46,5].A common approach involves feeding LLMs with perceptual inputs, such as scene descriptions [25,72] or visual information [13,43], and implementing action capabilities through libraries of motion primitives [36].Despite these promising developments, effectively representing intricate spatial relationships continues to be a formidable challenge for LLMs.Several works have demonstrated the potential of LLMs in composing sequential policies for embodied domains including VoxPoser [27] for robotic manipulation, SayCan [1] for instruction following, Palm-E [14] for dexterous manipulation, RT-2 [4] for mobile manipulation, EmbodiedGPT [41] for motion planning, Voyager [58] and Smallville [44] for embodied gaming, VisProg [20] for program synthesis, TAPA [62] and SayPlan [48] for task planning.While significant strides have been taken, fully closing the perception-action loop in embodied agents remains a dynamic and ongoing area of research.Key challenges encompass representing complex spatial relationships and long-term planning, enhancing sample efficiency in learning, and seamlessly integrating perception, reasoning, and control within a unified framework.Further exploration of benchmark tasks, and physical knowledge representation will propel LLMs towards achieving generalized embodied intelligence.</p>
<p>B. Benchmarks for Robotic Code Generation</p>
<p>Ravens [73] was originally proposed as a simulation benchmark for reinforcement learning of robotic manipulation skills.It was later extended by CLIPort [50] to include language conditioning, allowing for the study of languageconditioned policy learning and generalization to new instructions.Building on the Ravens benchmark, RoboCodeGen [36] was developed to test language models to test language models on robotics programming problems.RoboGen [60] further expanded this by encouraging the use of libraries like NumPy and allowing for the definition of helper functions.These benchmarks have provided valuable platforms for evaluating the performance of various Large Language Models (LLMs) (e.g., RoboCodeGen supports evaluation on GPT-3, InstructGPT and Codex) in generating code for robotic tasks.</p>
<p>C. Motion Planning</p>
<p>In the realm of robot motion planning, the Open Motion Planning Library (OMPL) [53] is a key open-source framework that offers a range of advanced algorithms for navigating complex environments.MoveIt [9] was developed to integrate OMPL's capabilities with the ROS ecosystem, providing a comprehensive platform that includes SOTA motion planning, manipulation, 3D perception, kinematics, and control.This integration greatly improves the applicability and efficiency of robot motion planning within the ROS ecosystem.</p>
<p>D. Grasp Pose Detection</p>
<p>Vision-guided grasp pose detection is a crucial area in robotics research, transitioning from 4 degrees of freedom (DOF) top-down grasping methods [19,70,74] to more complex 6 DOF grasping methods [3,31].The state-of-the-art 6 DOF grasp pose detection model, AnyGrasp [16], achieves a success rate in object grasping comparable to human performance by studying the geometric features of objects.This has paved the way for further work on specified object grasping [38,32] or articulated object manipulation [69].</p>
<p>III. ROBOSCRIPT PIPELINE</p>
<p>In this section, we introduce the RoboScript pipeline, which is designed to deploy and evaluate Large Language Models (LLMs) in generating code to control robotic arms for completing embodied AI tasks as instructed by humans.Given RGBD images from the sensor and human instructions as input, the pipeline generates Python code that directly manipulates a robotic arm.By leveraging the capabilities of infrastructure including 1) the highly versatile communication protocols and message interface provided by the Robot Operating System (ROS), 2) the ROS-integrated Gazebo simulation suite, and 3) the versatile motion planning framework MoveIt, the pipeline creates a unified interface to both the simulation and real robots.This abstraction allows our pipeline to have a snippet of generated code tested safely in a simulated environment and then executed in reality.</p>
<p>A. Pipeline Overview</p>
<p>As described in the introduction, our framework provides a set of Perception Tools and Motion Planning Tools.Each time a task query is sent to the system, the LLM generates a Python script by using In-context Learning (ICL) [11].This means that the few-shot example codes are integrated into the prompt as demonstrations.Then, the code is executed to finish the task, as shown in Fig. 2.</p>
<p>A generated Python script mainly consists of three parts: 1) Chain-of-Thought (CoT) [61] Comments help LLMs decompose long-time-horizon and complex tasks.2) Perception Tools process the raw sensor input of multiview RGB-Depth images to 3D representations and spatial arguments.When the "detect_obejects" function is called, the system perception pipeline is triggered, building 3D representations and implicitly sending them to the Moveit planning scene.This behavior is designed to support fundamental functionalities of robotic system like obstacle-aware path planning.Other perception utils are then called to get spatial arguments like positions and orientations for motion planning.3) Motion Planning Tools take the output of perception utils as input arguments, generate a collision-free path plan, and further call low-level controllers to control the motion.For example, when opening a drawer attached to a cabinet, the robot needs to pull the drawer handle with a Cartesian path outward.The path should be parallel to the prismatic joint between the drawer and the cabinet.The motion plan generates the path based on the predicted prismatic joint direction from the perception tool "get_object_joint_info".You can find the details of all our tools API, their explanation, their arguments, and returns definitions in Appendix VI-B.</p>
<p>It is also worth noting that there could be multiple rounds of perception → motion planning in the code script, especially when the task requires multiple rounds of robot-environment interaction.For instance, consider a fundamental task for (future) household robots: storing table containers in a cabinet.The robot might first detect the environment and find a closed cabinet, with no direct motion plan to place a container into the drawer.In this case, the robot needs to first interact with the environment by opening the drawer and retracting its arm.It then needs to re-detect the scene to update its plan.</p>
<p>B. Code Generation</p>
<p>The full prompt for code generation involves three sections: 1) System Prompt: A system prompt is a pre-defined and query-irrelevant input that sets the context and general instructions for the LLM to respond.It helps structure the output of long texts.GPTs and Llama-series models are trained with special data to increase the model's attention on system messages wrapped in special tokens [42,56].We provide all format-related instructions, e.g., "You should only generate Python code and comments." in this section.Since LLMs generally pay better attention to the system prompt, especially when reading long text, we also include descriptions/doc-strings of perception and motion planning tools into the system prompt.2) Few-shot Examples: To limit the LLM output format and help reduce hallucination, we provide 10 rounds of task query: master script code dialogues so that LLMs can learn from demonstrations how to respond to a task query input.3) Task Query: The task query consists of two sentences.</p>
<p>The first sentence describes what objects are present in the current environment.The second sentence provides the task instruction.For example, "objects=[apple, bowl].Please place the apple and place it into the bowl" is a valid task query.For LLMs, the user needs to input the scene description.For Large Multimodal Models (LMMs), the scene description could also be generated by querying the LMM with an RGB image and another pre-defined query prompt.Similar to Liang et al. [36], RoboScript also employs a hierarchical agent architecture to facilitate modular code generation.At the top is the main agent responsible for analyzing the problem and producing the overall workflow.As the main agent generates code, a syntax parser operates in parallel, continuously verifying the syntactic correctness of each function call.When the parser identifies a function that is not in loaded variables, it triggers a specialized sub-agent to generate the required function from the function name and description.The sub-agent then produces Python function definitions as requested, using a similar three-section prompt as model input.</p>
<p>However, even for the state-of-the-art GPT-4 model, this hierarchical decomposition is not sufficient when generating code for tasks with long time-horizons or tasks involving complex reasoning processes.Chain-of-Thought [61] (CoT), a prompting method, is adopted to teach the LLM to explicitly print its thinking process step by step, as shown in the "CoT Comments" part of Fig. 2. By leveraging the power of contextual attention in auto-regressive models, CoT significantly enhances the reasoning capability of LLMs.Many following works and variants [66,67] have once again validated its advantages.Viewing our pipeline as a baseline for future embodied control code generation architectures, we accept the original CoT prompting and find that CoT already performs well on the benchmark tasks.</p>
<p>C. Perception Pipeline</p>
<p>In this section, we introduce the perception pipeline, a key component of the Roboscript system.As mentioned in the overview, "detect_objects" function, which serves as the foundation and prerequisite of all the other perception tools, triggers the perception pipeline of the system, conducts 2D visual grounding, builds 3D representations, and updates the Moveit planning space with reconstructed meshes.The LLM can then choose the perception tools API to retrieve the result of 3D representations or further process it for more advanced information.For example, "get_3d_bbox" simply returns the stored object instance 3D bounding box, while "get_plane_normal" returns the surface normal vector of the plane closest to the given position.This design is motivated by the observation that 1) 3D reconstruction and object 3D bounding boxes, with relatively fixed algorithms to calculate, are shared as the foundation data of most other perception tools, and 2) Moveit's planning scene, which is the data infrastructure of motion planning tools, is organized based on objects' meshes and an Octomap [22] for collisionfree planning.Therefore, we encapsulate these low-level perception processes in a high-level API, This approach effectively minimizes code length and reduces redundancy, also relieving the difficulties encountered when generating lengthy texts with language models.</p>
<p>For clarity, we define all the mathematical notations in the perception pipeline as follows: Generally, we assume the robot system is equipped with k ≥ 1 RGB-Depth cameras, generating images I 0 , . . ., I k−1 .I i denotes the input RGBD image from camera i, I i rgb ∈ R H×W ×3 denotes the input RGB image of height H and width W , and I i depth ∈ R H×W denotes the depth image.For camera i, denote the camera intrinsic matrix as K i ∈ R 3×3 .The rotation and translation of the camera in the world frame are represented as R i ∈ R 3×3 and t i ∈ R 3 , respectively.A 2D bounding box on i-th RGB image 3)) transformation denoted as T ∈ R 4×4 .Since we build the whole system on top of Sucan et al. [53], we have chosen to omit the details of joint motion planning and focus solely on the end-effector pose in code generation.Thus, in the rest of this paper, we use the term "motion trajectory" to indicate a sequence of waypoints of end-effector poses.We denote a motion trajectory as M = {T 0 , . . ., T n }, where T i ∈ R 4×4 represents an endeffector pose.
I i rgb with label o j is denoted as b ij 2D = [x min , y min , x max , y max ]. And a 3D bounding box of object o j is denoted as b j 3D = [x min , y min , z min , x max , y max , z max ]. A 3D point cloud is denoted as P = {p 0 , . . . , p n }, where p i = [x i , y i , z i ] ∈ R 3 represents a point in space. A grasp pose is a 3D- homogeneous (i.e. SE(
1) 2D Grounding: By leveraging the capability of the open-set text-to-image grounding model GLIP [35], the pipeline first detects a list of task-relevant objects, reasoned by the LLMs with scene description and task instruction, on each RGB image I i rgb .GLIP generates a list of 2D bounding boxes b ij 2D on each RGB image I i rgb of object label o j , given its text description:
b ij 2D = GLIP(I i rgb , Description(o j )), i ∈ {0, . . . , k − 1}.(1)
The open-set reasoning capabilities of LLMs, combined with the open-set grounding capabilities of large 2D vision models, empower our system with open-set intelligence capability.Limited by data and training paradigms, 3D models are relatively more constrained to a specific domain or even a dataset.This is the main reason we choose to use a 2D grounding model and manually propagate semantic information to 3D representations, instead of using a 3D grounding model.</p>
<p>2) 3D Reconstruction: In an indoor environment with a lot of objects and clutter, varying viewpoints are helpful to circumvent the obstruction issues that can occur with a single view.The multi-view RGBD images I 0 , . . ., I k−1 are firstly integrated into a Truncated Signed Distance Function (TSDF) volume, using a TSDF fusion pipeline proposed by Zhou and Koltun [75] and implemented in Open3D [76].TSDF fusion is often preferred over cloud fusion from depth maps in real-time reconstruction pipelines due to its simplicity, faster speed, and ease of parallelization, as discussed in [54].This process can be formulated as:
T SDF (x) = k−1 i=0 F (I i , x),(2)
where F (I i , x) denotes the fusion process of image I i at position x, and is the integration operation over all views.The volumetric representation is further converted to different representations at request: (a) point cloud for grasp detection model AnyGrasp [16], joint prediction model GAMMA, and plane detection model [2]; (b) mesh for Moveit planning scene; (c) uniform voxel grids for GIGA [31].The marching Cube [39] algorithm is used in the surface extraction process to compute point cloud and mesh.</p>
<p>3) Cross-view Bounding Box Matching: With the TSDF volume and 2D grounding results, which are independent 2D object bounding boxes across images of different camera views, the perception pipeline continues to get a list of 3D object instances by (a) matching 2D bounding boxes across different views and (b) filtering the point cloud within matched 2D bounding boxes.</p>
<p>To match the 2D bounding boxes b ij 2D on each RGB image I i rgb of object label o j , we propose a simple heuristic algorithm based on view frustum filtering and greedy matching.Firstly, the algorithm takes a list of 2D bounding boxes for each camera view, as well as camera intrinsic and extrinsic parameters, as input.It checks which points are in the 2D bounding box with a frustum filter.Then, a 2D bounding box b ij 2D can be represented by the binary vector v ij 2D ∈ {0, 1} N , indicating the projection of which points out of all N points are within the bounding box.We use the one-dimension Intersection Over Union (IOU) of two binary vectors as the  matching score s between two bounding boxes:
s(b ij 2D , b kl 2D ) = IOU(v ij 2D , v kl 2D ) = N n=1 min(v ij 2D [n], v kl 2D [n]) N n=1 max(v ij 2D [n], v kl 2D [n]) ,(3)
where v ij 2D and v kl 2D represent the binary vectors of the bounding boxes b ij 2D , b kl 2D from views i and k.This heuristic is predicated on the premise that the greater the number of points shared by two bounding boxes across views, the higher the likelihood of these bounding boxes pertaining to the same object.An demonstration of cross-view bounding box matching score computation is shown in Fig. 3.</p>
<p>With cross-view pairwise matching scores, the problem can be formulated as a quadratic assignment problem (QAP), which is known to be an NP-hard problem [29].We use a greedy algorithm approach to match bounding boxes across different views by maximizing the Intersection over Union (IoU) of these one-hot vectors, subject to the constraint that each bounding box can be matched at most once.The matched bounding boxes are then output as a list of tuples, with each tuple containing the indices of matched bounding boxes across views or "−1" if no match is found.It's important to note that there could be label discrepancies or failures across different camera views.We unify the object label of all matched 2D bounding boxes by max voting.</p>
<p>4) 3D Instance Segmentation: With matched 2D bounding boxes for object  In this section, we have delved into the workings of the RoboScript pipeline from receiving task instruction to execution, and explored its key components, code generation and perception.Since the Motion Planning is mainly done by Moveit, we do not repeat it here due to space constraints and will turn our attention to the discussion of our benchmark from the perspective of language model evaluation.
B * j 2D = {b ij 2D }, i ∈ {0, • • • , k − 1},</p>
<p>IV. ROBOSCRIPT BENCHMARK</p>
<p>Built on top of the infrastructure of Gazebo, MoveIt, and ROS, our pipeline generates executable code both in simulation and real robots, which enables us to evaluate the capability of language models to generate realistic deployable code, as well as to study the limitations and strengths of different language models in a real robotics system.For this purpose, we present our RoboScript benchmark.Compared with other benchmarks or test environments for general robot manipulation with natural language instructions [73,36,30], our benchmark highlights realistic object manipulation, complex spatial reasoning with articulated objects, and a smaller gap with the real robotic system.</p>
<p>A. Environment Setup</p>
<p>The foundation of the RoboScript benchmark lies in its carefully designed base environment, which includes the following key elements:</p>
<p>• Robot Arm: Our platform supports testing for 2 different robot arms: the Franka arm and the UR5 robot arm.The Franka arm is a lightweight and flexible 7-axis robot designed for research and manufacturing automation [21].The UR5 is a versatile 6-axis collaborative industrial robot known for its flexibility, precision, and safety [34].</p>
<p>• Furniture and Objects: To simulate real-world scenarios, we have incorporated 10 cabinets with drawers and doors as parts from the storageFurniture category in the PartNet-Mobility dataset [63].Both the drawers and doors are articulated objects that require a deep understanding of physical constraints to manipulate successfully.• Diverse Containers: For object interaction and manipulation tasks, we populate the environment with diverse containers.These containers are collected from the Google Scanned Objects 1k dataset [12], representing some of the largest indoor object meshes sourced from real scans.The inclusion of these containers adds a layer of realism and complexity to the tasks.• Pickable Objects: The benchmark includes 57 pickable objects, carefully selected from the YCB dataset [7].This dataset encompasses a wide array of objects, including both household and kitchen items, ensuring a diverse set of objects for the robot to interact with.</p>
<p>B. Free-form Manipulation Tasks</p>
<p>As shown in Table I and Fig. 4, our benchmark comprises eight carefully designed tasks featuring four distinct challenges that test the reasoning capabilities of LLMs.The manipulation tasks outlined in the benchmark table are cleverly designed to assess various levels of cognitive processing in language models.The simplest task involves placing an apple into a bowl, testing basic category understanding, object selection, and spatial awareness-suitable for an 'Easy' difficulty level.As the tasks become more nuanced, we see 'Moderate' challenges like nesting objects (a mug into a drawer) or categorizing and repositioning items (fruits onto a plate, or swapping an apple and a banana), which require a blend of categories and spatial reasoning.A task such as placing a brown cup onto an empty plate introduces property reasoning, adding a layer of complexity.The 'Hard' tasks are the most intricate: clearing a table by moving all objects into a basket demands advanced spatial and property reasoning while placing a white mug into a container of the same color tests the model's ability to comprehend and apply abstract properties like color coordination.These tasks are thoughtfully curated to evaluate the model's understanding from simple object recognition to complex, multi-step reasoning.</p>
<p>C. Perception and Motion Planning Tools</p>
<p>1) Grasp Pose Estimation: We provide 4 types of grasp pose estimation methods.a. Central Lift Grasp: As a heuristic-based method provided in the HomeRobot project [68], the system executes a top-down grasp by aligning the gripper directly above the object's geometric center.b.Horizontal Grasping: In this approach, the gripper is oriented horizontally, making it suitable for objects aligned with a plane perpendicular to the tabletop.c.GIGA Grasp Pose Prediction [31]: GIGA presents a neural networkbased approach for 6-DOF grasp pose prediction.This method utilizes generative models and offers enhanced flexibility.However, it may have limitations in constrained tasks.d.AnyGrasp [16] Grasp Pose Prediction: AnyGrasp, a state-of-art neural network-based grasping model, is adept at generating dense 6-DOF grasp poses.It is trained on the comprehensive GraspNet dataset [15], which encompasses over 1 billion grasp labels, ensuring a robust and versatile grasping capability.</p>
<p>2) Kinematic Modeling of Articulated Objects: We utilize the GAMMA [69] to predict articulated objects' kinematic structures.GAMMA effectively processes the object's point clouds, segments them into rigid parts, and accurately estimates the associated kinematic parameters.More specifically, GAMMA predicts offsets for each point towards part centroids, thereby grouping them into distinct articulated components.Additionally, the model conducts regression on per-point projections onto joint axes, facilitating a voting process for determining axis origins.Simultaneously, it regresses per-point joint directions, aggregating votes to establish precise axis directions.GAMMA demonstrates the capability to generically model both revolute and prismatic joint parameters by clustering adjusted and projected points, entirely independent of the articulated object's category.</p>
<p>3) Motion Planning Module: Our planning module is built upon the Open Motion Planning Library (OMPL) platform [53] integrated within the MoveIt platform, which enables performance benchmarking across 12 different motion planning algorithms including RRT<em> [33], PRM</em> [18] and so on.For this work, we primarily utilized the RRT* algorithm due to its efficient exploration and optimality guarantees.The planning module samples configuration space to find a collision-free path to connect the start and goal states.The output trajectory is checked for dynamic feasibility before being passed to the control module for execution.</p>
<p>In the benchmark, we offer two modes for running the MoveIt planning scene, which is the data structure used to store objects and obstacles for collision checking during planning.Users can either 1) enable using ground truth planning scenes, with perfect model meshes of Gazebo world loaded into the MoveIt planning scene and updated automatically; or 2) enable the planning scene sensor plugin to construct and update an Octomap from depth images in the real-time.The meshes of objects to be contacted or taken as receptacles are loaded at request by LLMs when calling "detect_objects" function.The visualization of MoveIt planning scene under the two options is shown in Fig. 5.</p>
<p>V. EXPERIMENTS</p>
<p>In this section, we first present the LLMs evaluation results on the RoboScript pipeline, discussing the strengths and limitations of LLMs for realistic control code generation in subsection V-A.We then conduct an in-depth study of how the physics properties of objects influence grasp and arm motion, and subsequently, the final success rate of tasks in subsection V-B.Lastly, in subsection V-C, we present an ablation study of the perception pipeline, demonstrating how perception could potentially bottleneck the entire model.We also present our findings on how language models and vision models collaborate can make the pipeline work efficiently, and what happens otherwise.</p>
<p>A. Benchmarking LLMs on RoboScript</p>
<p>In this subsection, we present the evaluation results of three popular LLMs on the RoboScript benchmark.Table II shows the evaluation result under the ground truth perception settings, where the perception pipeline is turned off and the ground truth 3D perception from the Gazebo simulation is always used.The results are the average over 10 repeated trials.There are four metrics presented in the results: 1) Grammar correctness, which indicates whether the generated script has correct grammar; 2) Semantic correctness, which indicates whether the generated script can faithfully complete the task as per the language instruction without logical errors.This is determined by the success of any of the 10 trials.However, we also manually check each code snippet that fails all trials, to eliminate randomness in the reported results; 3) Finished steps ratio, which indicates the average success rate of all pre-defined steps of a task.For example, task hard_0 involves picking two fruits on a plate, which comprises two steps: moving the first fruit and moving the second fruit; 4) Finished whole task, which indicates the success rate of the entire task.We further collect all failed snippets in Appendix VI-C and analyze the failure reasons.</p>
<p>From the perspective of evaluating the code generation ability of LLMs, the first two metrics are more meaningful, as the task execution success rate depends more on other factors, including grasp pose detection, motion planning, etc. From Table II, we can see that GPT-4 outperforms the other two LLMs by a clear margin.GPT-4 reaches amazingly 100% correctness on both grammar and semantics.Surprisingly, despite our pre-defined prompt introduced in section III-B, Gemini-pro performs even worse than GPT-3.5-turbo, in terms of code generation quality.This result was observed even after we provided Gemini-pro with an additional instruction prompt:</p>
<p>Please pay attention to the format of examples in chat history and imitate it.Do NOT generate common texts or code to import any Python libraries.ONLY generate comments or detailed planning starting with # and Python code.</p>
<p>Without this prompt, Gemini-pro fails almost all tasks due to the random output format.Through the case study, we found that Gemini-pro exhibits more hallucinations compared to GPT-3.5 or GPT-4, and pays less attention to instructions in the system prompt.In the Gemini technical report [55], there is no mention of techniques similar to system message tokens as used in Llama-2 and GPTs.This could be related to the issue, but no definitive conclusion can be drawn.</p>
<p>B. Grasp and Motion Plan: Impact of Object Shapes</p>
<p>In this section, we shift our focus from the code generation and reasoning capabilities of LLMs to lower-level factors: the environment objects and how their shapes affect the manipulation task.Through the construction of the benchmarking world and the selection of appropriate objects for different tasks, we find that object shapes dramatically impact the success of object manipulation for a gripper with certain mechanical structures.To better study this impact, we isolate the errors in LLM reasoning and code generation by selecting two fundamental manipulation tasks in daily scenarios: 1) object pick and place on a table (Simple_0); 2) placing objects into a closed cabinet drawer (Moderate_1).We execute verified scripts based on code generated by GPT-4 and only change the drawers and objects.</p>
<p>First, we exhibit the results of table object pick and place in Fig. 6.Among our object models, 21 come from YCB [6] and 25 come from Google Scanned Objects [12].</p>
<p>Secondly, we select the objects with the top success rates in tabletop pick and place task (Fig. 6), and test their success rates in "open a drawer, pick and place an object into the drawer" task.We present results in Table III and IV.</p>
<p>A striking observation is that some round and cylindrical objects, like the brown_medication_bottle and grey_medication_bottle from Google Scanned Objects and orange and baseball from YCB dataset perform well in Table II: Evaluation result of LLMs on the RoboScript benchmark.There are four metrics presented in the results: 1) grammar correctness, which indicates whether the generated script has correct grammar; 2) semantic correctness, which indicates whether the generated script can faithfully complete the task as per the language instruction without logical errors.3) finished steps ratio, which indicates the average success rate of all steps of a task.4) finished whole task, which indicates the success rate of the entire task.desktop pick and place task but struggle when placed in the drawer.Through careful investigation, we found that when using a thin two-fingered mechanical gripper to grasp cylindrical and spherical objects, the grasp is always in an unstable mechanical state.The end of the gripper could not always maintain a Cartesian path when moving in a complex constrained space.And sudden accelerations and rotations would cause this mechanical state to go out of control.</p>
<p>C. Perception Matters: Pipeline Ablation Study</p>
<p>In this section, we present the results of the pipeline ablation study, mostly focusing on how different perception modules affect the overall instruction-following manipulation pipeline.The perception pipeline generates 3D object instances that can be utilized by perception or motion planning tools.Thus, we study how 1) 3D object grounding and 2) input data of MoveIt planning scene affect the pipeline.To minimize the influence of object shape and grasp detection on the experiments, we selected the 10 objects from Google Scan Objects that had the highest grasp success rate in the last experiment.For each object model, each task repeats for 10 times and we report the success rate out of these 10 trials.</p>
<p>Following the settings in last subsection, we test and report the system performance on 1) Pick and Place in Fig. 7; 2) Open Drawer and Place Object tasks in Fig. 8.By enabling the real perception pipeline, using 3D bounding boxes from 2D grounding and cross-view matching rather than ground truth in Gazebo, the success rate of pick and place dropped by 10.1%, and the success rate of task open drawer and place object dropped by 6.0%.In comparison, using 3D representations of the Octomap and reconstructed mesh from perception makes the performance drop by 14.0% and 15.0% on the two tasks separately.The result demonstrates that the noise in the perception pipeline has a larger impact on motion planning than perception tools, mostly the grasp pose detection models in our tasks.This could hint that the noise in Octomap construction is large, affecting the motion planning accuracy.However, we also find that MoveIt motion planning is sensitive to the error in detections, as demonstrated in Fig. 9.In this case, the bounding box of the target object is enlarged, resulting in an incorrect 3D instance bounding box that encompasses the Franka robot.The robot arm's mesh will cause unsolvable self-collisions, rendering all states illegal for motion planning.Thus, any motion request will be rejected.</p>
<p>D. Real Robot Deployment</p>
<p>To further support our claim that RoboScript is a unified code generation pipeline for both simulation testing and real robot deployment, we installed RoboScript onto two sets of robot systems: the Franka Panda and the UR5 with Robotiq 85 gripper.As shown in Fig. 10, this deployment demonstrated RoboScript's ability to seamlessly integrate with existing robotic frameworks, ensuring that tasks such as object manipulation and precision placement are executed with high fidelity across different hardware configurations.Our findings indicate that RoboScript not only simplifies the transition from simulation to real-world application but also enhances the robots' adaptability to diverse tasks without requiring extensive programming effort.This dual deployment validates RoboScript's role as a pivotal technology in bridging the gap between simulation-based development and practical, real-world robotic automation.</p>
<p>VI. CONCLUSION</p>
<p>This paper presents RoboScript that facilitates the deployment and evaluation of large language models for generating executable robot control code.By integrating with simulation and real-world robotics frameworks like ROS, Gazebo, and MoveIt, RoboScript bridges high-level reasoning and lowlevel control.The comprehensive pipeline encompasses perception, planning, grasp detection, and motion control achieving an end-to-end system from language interpretation to plan execution.Experiments highlight the GPT model's reasoning in complex spatial, physical, and property reasoning manipulation tasks.Ablation studies further illustrate the impact of factors like object geometry, perception accuracy, and planning robustness on real-world deployment.Overall, RoboScript advances the integration of language model intelligence with robots for autonomous manipulation.</p>
<p>B. Tool APIs design</p>
<p>This section gives a detailed description of all the tools API that language models have access to.</p>
<p>The Perception Tools are shown in Code Block 1.The perception tools provide geometric information as function arguments for the motion planning tools, including object positions, object 3D bounding box, articulated joint information (moving direction or rotational axis), and plane surface normals.Perception tools are organized as object properties using object names as arguments and function names to specify the property type.With this object-centric design, LLMs only need to reason which object properties are involved in a specific task.Through our observation, LLMs are good at handling high-level task decomposition and context-consistent function calling rather than lowlevel geometric understanding or mathematical-consistent algorithm generation.This design makes the best use of LLM's strengths and avoids its weaknesses.</p>
<p>These perception tools provide spatial parameters for trajectory generation and pick-and-place actions.For example, to generate a trajectory to open a door, the LLM needs to call the "get_object_joint_info" function to get its door joint axis, then to calculate the distance from the handle to the door joint, and further generate an arc path around this joint, with radius same as the distance from the door joint to handle, as demonstrated in "generate_arc_path_around_joint."Then, the robot can call "follow_path" to execute this trajectory.</p>
<p>The Motion Planning Tools are shown in Code Block 2. The motion planning tools are mostly re-wrappers of the Moveit Python move group interface.They hide system functional code under the hood, such as ROS synchronization and failure recovery behavior.Two API tools worth mentioning are:</p>
<p>• grasp.This tool executes a grasp motion with a given grasp pose: It first moves to a pre-grasp pose, as did in [31] [16], which is a pose retracted from the grasp pose on the approaching direction of the gripper.Assume the target grasp pose to be P = (R|t), and the approaching direction is [0, 0, 1] under identity rotation, then the pre-grasp pose is
P ′ = (R|t − R • [0, 0,</p>
<p>C. Failed code case study</p>
<p>LLMs generate control code with a failure rate and all sorts of failure patterns, including grammar failures and logical fallacies.The grammar failures can be: 1) Hallucinated function named arguments, 2) wrong variable type and operation, 3) garbage code (usually when code is too long or the generation process goes out of control), etc.However, these are the basic code-generation capabilities.They are wellstudied in a lot of general LLM code generation evaluation papers.Here, we discuss several failure cases due to the limitation of LLMs' reasoning capabilities on embodied tasks.</p>
<p>1) GPT-3.5-turbo:This approach ensures task diversity and complexity.We have already generated 100 randomly arranged tabletop environments and 4500+ grammatically correct code snippets and constructed into (task query, environment, generated code) tuples for some private projects.The models are selected from YCB or Google Scanned Objects.A collage of pickable objects and container objects is given in Figure11 and Figure12.Some of the auto-generated worlds are shown in Figure13.To better visualize its heuristic arrangments, we show the top view.</p>
<p>E. Implementation of ROBOSCRIPT</p>
<p>LLMs and Prompting.We follow prompting structure by Liang et al. [36], which recursively calls LLMs using their own generated code, where each language model program (LMP) is responsible for a unique functionality (e.g., processing perception calls).We use GPT-4 [43] from OpenAI API.For each LMP, we include 10 example queries and corresponding responses as part of the prompt.</p>
<p>Motion Planning and control.We use MoveIt-integrated Open Motion Planning Library (OMPL), which is a collection of state-of-the-art sampling-based motion planning algorithms.Specifically, we use RRT (Rapidly-exploring Random Trees) and PRM (Probabilistic Roadmaps) by default for trajectory path sampling.</p>
<p>For the robot control, by default, we use JointStateController in the ros control package, taking as input the current joint states and joint goal states.It runs a PID controller at the backend at 50 Hz.The PID parameters are tuned and saved to individual Moveit robot configurations separately, mostly provided by each robot manufacturer.</p>
<p>Dynamics Model.We use the known robot dynamics model in all tasks, where it is used in motion planning for the end-effector to follow the waypoints.For the majority of our considered tasks where the "entity of interest" is the robot, no environment dynamics model is used (i.e., the scene is assumed to be static), but we replan at every step to account for the latest observation.For tasks in which the "entity of interest" is an object, we study only a planar pushing model parametrized by contact point, push direction, and push distance.We use a heuristic-based dynamics model that translates an input point cloud along the push direction by the push distance.</p>
<p>Mu and Junting Chen.Mingyu Ding and Ping Luo are the co-corresponding authors.* Equal contribution.† Corresponding Authors.</p>
<h1>Figure 1 :</h1>
<p>1
Figure1: Framework of RoboScript .The input layer contains sensor input, human instruction, and robotic URDF (Unified Robot Description Format) data.The system utilizes various perception tools, such as grasp detection, 2D grounding, 3D perception, and joint prediction, to interpret the input data.These tools are integrated with motion planning tools that include arm planning, gripper force control, and solving place pose with inverse kinematics (IK).The Robot Operating System (ROS) serves as the middleware to provide abstraction to sensor drivers, controllers, and robot definitions across real robots and the simulation.The framework controls multiple real robots and their counterparts in the simulation with a unified code generation pipeline.This modular approach enables flexibility in robotic applications and adaptability to new code generation methods or robot architectures, from simple tasks to complex manipulations.</p>
<p>All zeros binary vectorAll ones binary vector</p>
<p>Figure 3 :
3
Figure 3: Cross-view bounding box matching score computation.The matching score between two bounding boxes is calculated by the Intersection over Union (IOU) of points presence binary vector.</p>
<p>Figure 4 :
4
Figure 4: Benchmark tabletop environment and tasks.</p>
<p>Figure 5 :
5
Figure 5: MoveIt planning scene.There are two modes for running MoveIt planning scene in the benchmark.MoveIt can either load ground truth mesh data from Gazebo, or build an Octomap from sensors in real time and load reconstructed meshes from the perception pipeline.planning scene</p>
<p>(a) Google Scanned Objects f o a m b r ic k o r a n g e p e a c h r a c q u e t b a ll b a s e b a ll p lu m t o m a t o s o u p c a n le m o n m u g b a n a n a b o w l t e n n is b a ll p o t t e d m e a t c a n a p p le p e a r m a s t e r c h e f c a n s o f t b a ll b le a c h c le a n s e r m u s t a r d b o t t le s k il le t li d w in d e x b o t t</p>
<p>Figure 6 :
6
Figure 6: Results of pick and place task.They present the success rates of pick and place tasks for all object models in our model base, including 21 YCB objects and 25 Google Scanned objects.</p>
<p>Nd o g f i g u r e g r e y m e d i c a t i o n b o t t l e p a n d a t o y f i g u r e t o y b u s w h i t e a n d b r o w n b o x w o o d b l o cFigure 7 :
7
Figure 7: Perception ablation study: pick and place.The figure presents the success rate of task pick and place under three ablation settings: 1) GT Detection &amp; Planning, indicating the perception tools and motion planning tools use ground truth 3D representations from Gazebo; 2) GT Planning Scene, indicating the system only uses ground truth object meshes in motion planning tools from MoveIt; 3) No GT, indicating the system does not use ground truth data, all from perception pipeline.</p>
<p>Nd o g f i g u r e g r e y m e d i c a t i o n b o t t l e p a n d a t o y f i g u r e t o y b u s w h i t e a n d b r o w n b o x w o o d b l o cFigure 8 :Figure 9 :
89
Figure 8: Perception ablation study: open drawer and place objects.The figure presents the success rate of task pick and place open the drawer and place objects under three ablation settings: 1) GT Detection &amp; Planning, indicating the perception tools and motion planning tools use ground truth 3D representations from Gazebo; 2) GT Planning Scene, indicating the system only uses ground truth object meshes in motion planning tools from MoveIt; 3) GT Detection, indicating the system only uses ground truth 3D object instances as the result of perception pipeline.</p>
<p>Figure 10 : 2 Function 4 for function F in C do 5 if F is not defined then 6 C 7 C
1024567
Figure 10: Deployment on the real robot: open the drawer with the Franka robot.scope of instructions/environments, and improving sim-toreal transfer sample efficiency to further leverage the potential of AI-powered systems.</p>
<p>→</p>
<p>object_names.append(obj)return object_namesD.Task Data Generation and DatasetOne of the distinctive features of the RoboScript platform is the capability of task data generation automatically.The platform can generate task data of (task query, environment, generated code) to train custom embodied code generation models.This generation involves three steps: 1) Randomly select candidate containers and pickable objects, sample an valid arrangement, and save to the environment definition file.2) Collect table top arrangement information, convert to text description ("Apple is in the grey bowl.Banana is on the white plate.").Send the environment description to LLM, to prompt the LLM to suggest what tasks can be done for this environment.3) For each task query, generate a normal code snippet to execute the task.</p>
<p>Figure 11 :
11
Figure 11: Pickable objects selected from Google Scanned Objects.</p>
<p>Figure 12 :
12
Figure 12: Container objects selected from Google Scanned Objects.</p>
<p>Figure 13 :
13
Figure 13: Auto-generated Worlds.We show the top view of the table and saved GT detection results of objects.</p>
<p>Data Flow Chain of Code CoT Comments Perception Motion Planning "place the apple into the footed bowl" # Detailed planning: # Step 0: Reason task-specific objects： [apple, footed bowl] # Step 1: Grasp the apple # Step 2: Move the apple to the footed bowl # Step 3: Release the apple # Reason task-specific objects：[apple, footed bowl] objects</p>
<p>= ['apple', 'footed bowl']</p>
<p>detect_objects(object_list=objects) # Grasp the apple open_gripper()
grasp(grasp_pose)grasp_pose=parse_adaptive_shape_grasp_pose('apple_0')</p>
<p>close_gripper() attach_object('apple_0') # Move the apple to the footed bowl parse_place_pose(object_name='apple_0', receptacle_name='footed bowl_0') move_to_pose(place_pose) object detection grasp pose estimation motion planningFigure 2 :
2
Pipeline of RoboScript.RoboScript uses Perception and Motion Planning Tools, activated by a task query, to generate a Python script with the LLM.This script includes comments, processes images into 3D models, and plans safe robot movements.Perception Tools identify objects and spatial details for planning.Motion Planning Tools then create a collision-free path for actions.</p>
<p>obtaining the objects' 3D instance segmentations is straightforward.Simply filter the point cloud within the matched bounding boxes across different views, and we can get the 3D instance segmentation of the object.Note that the shape of B * j 2D is not fixed since there could be detection failures in random camera views.
place all fruits on the table into the white plateplace the apple into the footed bowlplace the brown cup into an empty plateclear the table by moving all table objects into the basketplace the mug into the second top drawermove the white mug into a container with same color</p>
<p>Table I :
I
Free-form manipulation tasks in the RoboScript benchmark.The benchmark comprises eight carefully designed tasks of three difficulty levels, featuring four distinct challenges that test the reasoning capabilities of LLMs.
Task IDDifficulties in Language Model ReasoningTask InstructionCategory UnderstandingObject SelectionSpatial RelationshipProperty ReasoningSimple_0Place the apple into the footed bowlSimple_1✓Place the brown cup into an empty plateModerate_0✓✓✓Open the second top cabinet drawer and place all boxes into itModerate_1✓Move the white mug into a container with the same colorModerate_2✓Place the mug into the second top drawerHard_0✓✓Place all fruits on the table into the white plate.Hard_1✓Exchange the position of the apple and the banana on the tableHard_2✓Clear the table by moving all table objects into the basket</p>
<p>Table III :
III
Success rate of task opening drawer_0 and placing Google Scanned Objects.We observe two cylindrical objects have significant performance drop compared to pick and place task.
objectopen_drawerplace_objectwood_block10.8grey_medication_bottle0.90.4white_and_brown_box10.6panda_toy_figure10.9brown_medication_bottle10.2blue_medication_bottle0.90.7brown_ceramic_cup0.90.5toy_bus10.8Nintendo_Mario_toy_figure0.90.8dog_figure10.5</p>
<p>Table IV :
IV
Success rate of task opening drawer_1 and placing YCB objects.We observe that two sphere objects have a significant performance drop compared to the pick and place task.
objectopen_drawerplace_objectplum0.90.4tomato_soup_can0.80.6lemon10.3orange10mug0.90.7baseball0.60.1lemon10.3tennis_ball10.4banana10.3</p>
<p>0.1]).So this API plans and executes the path with two end-effector sub-goals {P ′ , P}. • parse_place_pose.Although it may seem like a perception tool at first glance, this tool relies on the object bounding boxes, object mesh, and Moveit Inverse Kinematics solver to get the valid placing pose.Specifically, it first predicts a candidate object place center position by a heuristic algorithm given container bounding box and object bounding box and then solves a valid orientation for the gripper with Moviet IK.</p>
<p>Extra instruction in prompt: you need to find empty plate by checking if there is any object in the plate's 2D region on the table This script has the wrong assumption that if the min and max values of a plate's bounding box are zero, it is empty, which is obviously not correct.#Returnsthe pose of an object in the world frame.Returns: pose: Pose get_3d_bbox, # Returns the 3D bounding box of an object in the world frame.Args: object_name: str.Returns: bbox: np.array[x_min, y_min, z_min, x_max, y_max, z_max]This method involves a vertical lifting action.The gripper closes at the center of the object and is not suitable for elongated objects and is not suitable for the objects with openings, as the gripper's width is really small.It is optimal for handling spherical and cuboid objects without any opening that are not ideal for off-center grasping.object_name: str, description: Optional(str) in ['top', 'center'], Returns: grasp_pose: Pose # Get the joint info of an object closest to a given position.Args: obj_name: str, name of the object; position: np.ndarray, select the joint closest to this position; type: str, allowed type of the joint, choice in ["any", "revolute", "prismatic"].Returns: joint_info: dict, joint info of the object.{"joint_position":[x,y,z],"joint_axis":[rx,ry,rz],"type":str} # Get the plane normal of an object closest to the given position.Args: obj_name: name of the object position: np.ndarray, select the plane closest to this position.Returns: np.ndarray, normal vector [x,y,z] Perception tools API in system prompt during code generation # Detaches an object from the robot gripper in the planning space.Call this function right after opening the gripper.Args: object_id: str.# Move the gripper in the given direction in a straight line by certain distance.Note that you can use the surface normal and the joint axis.Args: axis: np.array, the move direction vector ; distance: float.# Get the gripper pose.No args.Returns: pose: Pose grasp, # Executes a grasp motion at the grasp_pose.Args: grasp_pose: Pose parse_place_pose, # Predict the place pose for an object relative to a receptacle.Args: object_name: str, receptacle_name: Optional(str), position: Optional(np.array)[x,y,z], .Returns: place_pose: Pose Motion Planning tools API in system prompt during code generation • hard_4, clear the table by moving all table objects into the basket Although it might look good initially, the script skips the table and basket.However, the function did return cabinet, which the script does not need to pick.The master code is to construct a list of objects, only including objects that need to be picked and placed, rather than this exclusion logic.# Iterate through each object on the table for object_name in object_names: # Skip the table and the basket if object_name == 'table' or object_name == 'basket_with_fabric_liner': hard_3, Place the brown cup into an empty plate: Extra instruction in prompt: you need to find empty plate by checking if there is any object in the plate's 2D region on the table This task requires LLM to understand what is an "empty plate" with spatial reasoning process.However, Gemini generated a weird function to check if an object is in a plate by assuming objects have a name format plate_apple, which does not hold obviously.This failure can be attributed to Gemini's lack of attention to instruction hallucination.
from perception_utils import (get_object_center_position, # Returns the position of an object in the world frame. Returns: position: np.array[x,y,z]→get_object_name_list,# Returns a list of names of objects present in the sceneparse_adaptive_shape_grasp_pose, # Parse adaptive grasp pose for objects. Args: object_name: str,→preferred_position: Optional(np.ndarray); preferred gripper tip point position; preferred_approach_direction:→Optional(np.ndarray), preferred gripper approach direction; preferred_plane_normal: Optional(np.ndarray),→preferred gripper plane normal direction. Returns: grasp_pose: Poseparse_central_lift_grasp_pose, # →→→→detect_objects,# Detect and update task-relevant objects' status in the environment. Call this→function before interaction with environment objects. Args: object_list: Optional(List[str]), objects to→detect.get_object_joint_info,→→→get_plane_normal,→→)Code Block 1: →open_gripper,# Open the gripper. No args.close_gripper,# Close the gripper. No args.move_to_pose,# Move the gripper to pose. Args: pose: Pose# Find an empty plate on the table empty_plate = None table_objects = get_object_name_list() move_in_direction, →for object_name in table_objects: continueif object_name.endswith('_plate'):bbox = get_3d_bbox(object_name) # Grasp the objectif bbox[2] == 0 and bbox[5] == 0: # Check if open_gripper()the plate is empty by checking its height grasp_pose = →→(z-axis) parse_adaptive_shape_grasp_pose(object_name)empty_plate = object_namebreak
• hard_3, place the brown cup into an empty plate → get_object_pose, from motion_utils import ( attach_object, # Attaches an object to the robot gripper in the planning space.Call this function right after closing the gripper.Args: object_id: str.→ detach_object, → → generate_arc_path_around_joint, # Generate a rotational gripper path of poses around the revolute joint.Args: current_pose: Pose, current pose of the gripper; joint_axis: np.array, the joint axis of the revolute joint; joint_position: np.array, the joint position of the revolute joint; n: int, number of waypoints; angle: float, angle of rotation in degree.Returns: path: List[Pose] → → → follow_path, # Move the gripper to follow a path of poses.Args: path: List[Pose] get_gripper_pose, → ) Code Block 2: → ...2) Gemini:• def get_object_names_in_region(plate, region='table'): → """ Get a list of object names in a given region.Args: plate (str): The name of the plate.region (str): The name of the region.Returns: list: A list of object names in the region.""" object_names = [] for obj in get_object_name_list(): # Check if the object is in the given region if region == 'table': if is_object_on_table(obj, plate): object_names.append(obj)elif region == 'receptacle': if get_receptacle_name_from_object_name(obj) == plate:</p>
<p>ACKNOWLEDGMENTSWe would like to express our sincere appreciation to Professor Yufeng Yue Mr. Guangyan Chen from the Beijing Institute of Technology for their great help in code deployment onto real robots, and their valuable feedback for the user-friendly development of the project.Equally, our gratitude extends to Professor Cewu Lu and Dr. Wenhai Liu from Shanghai Jiao Tong University, for their great support in helping integrate the Anygrasp model into our system.
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Alex Hausman, Daniel Herzog, Jasmine Ho, Julian Hsu, Brian Ibarz, Alex Ichter, Eric Irpan, Rosario Jang, Kyle Jauregui Ruano, Sally Jeffrey, Nikhil Jesmonth, Ryan Joshi, Dmitry Julian, Yuheng Kalashnikov, Kuang-Huei Kuang, Sergey Lee, Yao Levine, Linda Lu, Carolina Luu, Peter Parada, Jornell Pastor, Kanishka Quiambao, Jarek Rao, Diego Rettinghouse, Pierre Reyes, Nicolas Sermanet, Clayton Sievers, Alexander Tan, Vincent Toshev, Fei Vanhoucke, Ted Xia, Peng Xiao, Sichun Xu, Mengyuan Xu, Andy Yan, Zeng, arXivpreprintarXiv:2204.01691Do as i can and not as i say: Grounding language in robotic affordances. 2022</p>
<p>A robust statistics approach for plane detection in unorganized point clouds. M C Abner, Manuel M Araújo, Oliveira, Pattern Recognition. 10051071152020</p>
<p>Volumetric grasping network: Real-time 6 dof grasp detection in clutter. Michel Breyer, Jen , Jen Chung, Lionel Ott, Roland Siegwart, Juan Nieto, PMLR, 2021. 3Conference on Robot Learning. </p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>The ycb object and model set: Towards common benchmarks for manipulation research. Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, Aaron M Dollar, 2015 international conference on advanced robotics (ICAR). IEEE2015</p>
<p>Berk Calli, Aaron Walsman, Arjun Singh, Siddhartha Srinivasa, arXiv:1502.03143Pieter Abbeel, and Aaron M Dollar. Benchmarking in manipulation research: The ycb object and model set and benchmarking protocols. 2015arXiv preprint</p>
<p>Open-vocabulary queryable scene representations for real world planning. Brian Chen, Fei Xia, Ben Ichter, Karthik Rao, Kaiyu Gopalakrishnan, Aviv Michael S Ryoo, Dan Stone, Kappler, arXiv:2209.098742022arXiv preprint</p>
<p>Reducing the barrier to entry of complex robotic software: a moveit! case study. D M Coleman, Ioan Alexandru Sucan, Sachin Chitta, Nikolaus Correll, ArXiv, abs/1404.37852014</p>
<p>Task and motion planning with large language models for object rearrangement. Yiming Ding, Xuran Zhang, Chris Paxton, Shiwen Zhang, arXiv:2303.062472023arXiv preprint</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>Google scanned objects: A high-quality dataset of 3d scanned household items. Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B Mchugh, Vincent Vanhoucke, 2022 International Conference on Robotics and Automation (ICRA). IEEE20227</p>
<p>Palm-e: An embodied multimodal language model. Daniel Driess, Fei Xia, Mohammad Sadegh Sajjadi, Corey Lynch, Ankur Chowdhery, Ben Ichter, Azalia Mirza Wahid, Jonathan Tompson, Quoc Vuong, Tony Yu, arXiv:2303.0337820231arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Yevgen Huang, Pierre Chebotar, Daniel Sermanet, Sergey Duckworth, Vincent Levine, Karol Vanhoucke, Marc Hausman, Klaus Toussaint, Andy Greff, Igor Zeng, Pete Mordatch, Florence, arXivpreprintarXiv:2303.033782023</p>
<p>Graspnet-1billion: A large-scale benchmark for general object grasping. Chengkun Hao Fang, Yifeng Wang, Cewu Gao, Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. Chenxi Hao-Shu Fang, Hongjie Wang, Minghao Fang, Jirong Gou, Hengxu Liu, Wenhai Yan, Yichen Liu, Cewu Xie, Lu, IEEE Transactions on Robotics. 7152023. 2, 3, 5</p>
<p>A robostack tutorial: Using the robot operating system alongside the conda and jupyter data science ecosystems. IEEE Robotics and Automation Magazine. Tobias Fischer, Wolf Vollprecht, Silvio Traversaro, Sean Yen, Carlos Herrero, Michael Milford, 10.1109/MRA.2021.3128367.12021</p>
<p>Prm path planning optimization algorithm research. Li Gang, Jingfang Wang, Wseas Transactions on Systems and control. 1172016</p>
<p>Object discovery and grasp detection with a shared convolutional neural network. Di Guo, Tao Kong, Fuchun Sun, Huaping Liu, 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE2016</p>
<p>Visual programming: Compositional visual reasoning without training. Tanmay Gupta, Aniruddha Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>The franka emika robot: A reference platform for robotics research and education. Sami Haddadin, Sven Parusel, Lars Johannsmeier, Saskia Golz, Simon Gabl, Florian Walch, Mohamadreza Sabaghian, Christoph Jähne, Lukas Hausperger, Simon Haddadin, IEEE Robotics &amp; Automation Magazine. 2922022</p>
<p>Octomap: An efficient probabilistic 3d mapping framework based on octrees. Armin Hornung, Kai M Wurm, Maren Bennewitz, Cyrill Stachniss, Wolfram Burgard, Autonomous robots. 3452013</p>
<p>Chih-Yao Huang, Oier Mees, Andrei Zeng, Wolfram Burgard, arXiv:2210.05714Visual language maps for robot navigation. 2022arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wendy Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. 2022</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wendy Huang, Fei Xia, Tete Xiao, Hei Chan, Percy Liang, Pete Florence, Animesh Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXivpreprintarXiv:2207.0560820221</p>
<p>Grounded decoding: Guiding text generation with grounded models for robot control. Wendy Huang, Fei Xia, Dhruv Shah, Daniel Driess, Andy Zeng, Yicheng Lu, Peter Florence, Igor Mordatch, Sergey Levine, Keno Hausman, arXiv:2303.008552023arXiv preprint</p>
<p>Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023arXiv preprint</p>
<p>Visually-grounded planning without vision: Language models infer detailed plans from high-level instructions. Jansen Peter Adriaan, arXiv:2009.142592020arXiv preprint</p>
<p>Roml: A robust feature correspondence approach for matching objects in a set of images. Kui Jia, Tsung-Han Chan, Zinan Zeng, Shenghua Gao, Gang Wang, Tianzhu Zhang, Yi Ma, International Journal of Computer Vision. 11762016</p>
<p>Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan, Vima, arXiv, 2022. 6General robot manipulation with multimodal prompts. </p>
<p>Zhenyu Jiang, Yifeng Zhu, Maxwell Svetlik, Kuan Fang, Yuke Zhu, arXiv:2104.01542Synergies between affordance and geometry: 6-dof grasp detection via implicit representations. 2021. 3, 5715arXiv preprint</p>
<p>Robo-abc: Affordance generalization beyond categories via semantic correspondence for robot manipulation. Yuanchen Ju, Kaizhe Hu, Guowei Zhang, Gu Zhang, Mingrun Jiang, Huazhe Xu, arXiv:2401.074872024arXiv preprint</p>
<p>Sampling-based algorithms for optimal motion planning. Sertac Karaman, Emilio Frazzoli, International Journal of Robotics Research. 3072011</p>
<p>Kinematic and dynamic modelling of ur5 manipulator. Saba Parham M Kebria, Hamid Al-Wais, Saeid Abdi, Nahavandi, 2016 IEEE international conference on systems, man, and cybernetics (SMC). IEEE2016</p>
<p>Grounded languageimage pre-training. Liunian Harold Li, * , Pengchuan Zhang, * , Haotian Zhang, * , Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, Jianfeng Gao, CVPR. 2022</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023. 1, 2, 3, 4, 6, 17</p>
<p>Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, Jeannette Bohg, arXiv:2303.12153Text2motion: From natural language instructions to feasible plans. 2023arXiv preprint</p>
<p>Peiqi Liu, Yaswanth Orru, Chris Paxton, arXiv:2401.12202Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. Ok-robot: What really matters in integrating open-knowledge models for robotics. 2024arXiv preprint</p>
<p>Marching cubes: A high resolution 3d surface construction algorithm. Harvey E William E Lorensen, Cline, Seminal graphics: pioneering efforts that shaped the field. 1998</p>
<p>Yujia Lu, Peng Lu, Zhengyu Chen, Wenchao Zhu, Eric Xi, William Wang, Wang Yang, arXiv:2305.01795Multimodal procedural planning via dual text-image prompting. 2023arXiv preprint</p>
<p>Embodiedgpt: Vision-language pretraining via embodied chain of thought. Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo, arXiv:2305.150212023arXiv preprint</p>
<p>. ArXiv, abs/2303.08774OpenAI. Gpt-4 technical report. 42023</p>
<p>Openai, arXivGpt-4 technical report. 2023218</p>
<p>Sung Joon, Park, C Joseph, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023arXiv preprint</p>
<p>Pretrained language models as visual planners for human assistance. Devansh Patel, Hamid Eghbalzadeh, Naman Kamra, Utkarsh Michael L Iuzzolino, Raghavendra Jain, Desai, arXiv:2304.091792023arXiv preprint</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Planning with large language models via corrective re-prompting. Shrayash Raman, Vinay Cohen, Edward Rosen, Isma Idrees, Dan Paulius, Stefanie Tellex, arXiv:2211.099352022arXiv preprint</p>
<p>Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, Niko Suenderhauf, arXiv:2307.061352023arXiv preprint</p>
<p>Lm-nav: Robotic navigation with large pretrained models of language, vision, and action. Devinik Shah, Barret Osinski, Brett Ichter, Sergey Levine, arXiv:2207.044292022arXiv preprint</p>
<p>Cliport: What and where pathways for robotic manipulation. Mohit Shridhar, Luca Manuelli, Dieter Fox, PMLR, 2022. 3Conference on Robot Learning. </p>
<p>Ishan Singh, Viktors Blukis, Arsalan Mousavian, Animesh Goyal, Denis Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, arXiv:2209.11302Progprompt: Generating situated robot task plans using large language models. 2022arXiv preprint</p>
<p>Llmplanner: Fewshot grounded planning for embodied agents with large language models. Chenghao Song, Jingyun Wu, Christopher Washington, Brent M Sadler, Wen-Lin Chao, Yu Su, arXiv:2212.040882022arXiv preprint</p>
<p>The open motion planning library. Mark Ioan A Sucan, Lydia E Moll, Kavraki, IEEE Robotics &amp; Automation Magazine. 19472012</p>
<p>Neuralrecon: Real-time coherent 3d reconstruction from monocular video. Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, Hujun Bao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Chatgpt for robotics: Design principles and model abilities. Sai Vemprala, Rogerio Bonatti, Arthur Bucker, Ashish Kapoor, MSR-TR- 2023-8February 2023Microsoft ResearchTechnical Report</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.16291202313arXiv preprint</p>
<p>Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shridhar, Chen Bao, Yuzhe Qin, Bailin Wang, Huazhe Xu, Xiaolong Wang, Gensim, arXiv:2310.01361Generating robotic simulation tasks via large language models. 2023arXiv preprint</p>
<p>Robogen: Towards unleashing infinite data for automated robot learning via generative simulation. Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, Chuang Gan, arXiv:2311.014552023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022354</p>
<p>Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan, arXiv:2307.01848Embodied task planning with large language models. 2023arXiv preprint</p>
<p>Sapien: A simulated partbased interactive environment. Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Yujing Xie, Chun Yu, Tiancheng Zhu, Jiaze Bai, Zihan Gong, Harold Soh, arXiv:2302.05128Translating natural language to planning goals with large-language models. 2023arXiv preprint</p>
<p>Pave the way to grasp anything: Transferring foundation models for universal pick-place robots. Jiange Yang, Wenhui Tan, Chuhao Jin, Bei Liu, Jianlong Fu, Ruihua Song, Limin Wang, arXiv:2306.057162023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Beyond chain-ofthought, effective graph-of-thought reasoning in large language models. Yao Yao, Zuchao Li, Hai Zhao, arXiv:2305.165822023arXiv preprint</p>
<p>Manolis Savva, Angel Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, and Chris Paxton. Homerobot: Open vocab mobile manipulation. Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alex William Clegg, John Turner, Zsolt Kira, 2023</p>
<p>Gamma: Generalizable articulation modeling and manipulation for articulated objects. Qiaojun Yu, Junbo Wang, Wenhai Liu, Ce Hao, Liu Liu, Lin Shao, Weiming Wang, Cewu Lu, arXiv:2309.16264202337arXiv preprint</p>
<p>A novel vision-based grasping method under occlusion for manipulating robotic system. Yingying Yu, Zhiqiang Cao, Shuang Liang, Wenjie Geng, Junzhi Yu, IEEE Sensors Journal. 20182020</p>
<p>Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. Haojian Yuan, Chenyi Zhang, Hong Wang, Fengda Xie, Peng Cai, Haoye Dong, Zijian Lu, arXiv:2303.165632023arXiv preprint</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. Aishan Zeng, Alvin Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Ashish Purohit, Vikas Michael S Ryoo, Joonseok Sindhwani, Vincent Lee, Vanhoucke, arXiv:2204.0059820221arXiv preprint</p>
<p>Transporter networks: Rearranging the visual world for robotic manipulation. Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, Conference on Robot Learning. PMLR, 2021. 3, 6</p>
<p>A multi-task convolutional neural network for autonomous robotic grasping in object stacking scenes. Hanbo Zhang, Xuguang Lan, Site Bai, Lipeng Wan, Chenjie Yang, Nanning Zheng, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2019</p>
<p>Dense scene reconstruction with points of interest. Qian-Yi Zhou, Vladlen Koltun, ACM Transactions on Graphics (ToG). 3242013</p>
<p>Qian-Yi Zhou, Jaesik Park, Vladlen Koltun, arXiv:1801.09847Open3d: A modern library for 3d data processing. 2018arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
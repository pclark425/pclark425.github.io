<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2018 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2018</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2018</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-48.html">extraction-schema-48</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <p><strong>Paper ID:</strong> paper-279305768</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.09251v2.pdf" target="_blank">Extrapolation by Association: Length Generalization Transfer in Transformers</a></p>
                <p><strong>Paper Abstract:</strong> Transformer language models have demonstrated impressive generalization capabilities in natural language domains, yet we lack a fine-grained understanding of how such generalization arises. In this paper, we investigate length generalization--the ability to extrapolate from shorter to longer inputs--through the lens of \textit{task association}. We find that length generalization can be \textit{transferred} across related tasks. That is, training a model with a longer and related auxiliary task can lead it to generalize to unseen and longer inputs from some other target task. We demonstrate this length generalization transfer across diverse algorithmic tasks, including arithmetic operations, string transformations, and maze navigation. Our results show that transformer models can inherit generalization capabilities from similar tasks when trained jointly. Moreover, we observe similar transfer effects in pretrained language models, suggesting that pretraining equips models with reusable computational scaffolding that facilitates extrapolation in downstream settings. Finally, we provide initial mechanistic evidence that length generalization transfer correlates with the re-use of the same attention heads between the tasks. Together, our findings deepen our understanding of how transformers generalize to out-of-distribution inputs and highlight the compositional reuse of inductive structure across tasks.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2018.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2018.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>From-scratch Transformer (arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>6-layer 6-head Transformer (Llama-style, RoPE) — Arithmetic length transfer experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>From-scratch transformer models (6 layers, 6 heads, Rotary Positional Embeddings) are trained in multitask setups to study length generalization transfer on arithmetic tasks (reverse add as main task, with auxiliary tasks no-carry, carry-only, reverse-subtract). Co-training with longer, structurally related auxiliary tasks enables extrapolation of the main task to longer lengths that the main task alone fails to reach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (Llama-like, from-scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer following Llama-style architecture with 6 layers and 6 attention heads, using Rotary Positional Embeddings (RoPE); trained from scratch on synthetic multitask datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard transformer decoder architecture with multi-head attention and RoPE positional encoding</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical/arithmetic (algorithmic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>reverse add (main) + auxiliary: no carry, carry only, reverse subtract</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reverse-addition: compute the sum of two integers with input/output digit sequences reversed. Auxiliary tasks include no-carry (digit-wise sum mod 10), carry-only (binary mask of carry positions), and reverse-subtract (digit-wise subtraction in reverse order). These tasks are algorithmic and structurally related, enabling study of reuse of computational circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>function composition / algorithmic step composition (digit-wise operations and carry propagation)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>length extrapolation split: train main task on shorter lengths (e.g., max 16 digits) and auxiliary task on longer lengths (e.g., up to 32); evaluate on longer (OOD) lengths</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>standard supervised multitask training; tasks sampled uniformly per iteration; all data generated on-the-fly</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Within-training-length accuracy is high and stabilizes quickly (exact numbers not reported; evaluated by exact-match on 1024 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>When trained alone, reverse add fails to generalize beyond its training length (near-zero extrapolation). When co-trained with structurally related auxiliary tasks trained to longer lengths, the main task frequently extrapolates up to the auxiliary task's length (qualitative: 'often matching the auxiliary task's generalization range'). Performance in the transfer range is seed-dependent and variable.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Defined in-paper as normalized average difference between main and auxiliary accuracy across evaluation lengths; co-training reduces the gap substantially compared to training main task alone, with strongest transfer when auxiliary/main length ratio is between 0.5 and 2. (No single numeric gap reported.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Baselines: (1) main task trained alone (fails to extrapolate beyond training length); (2) main task co-trained with longer, related auxiliary tasks (successful transfer); (3) main task co-trained with unrelated control auxiliary (copy-first-op) — no transfer. Comparisons reported as accuracy-vs-length plots across seeds (exact numeric curves provided in figures but not tabulated).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td>Shows transfer of length-generalization ability from auxiliary tasks to main task: co-training with longer, related auxiliary tasks enables the main task to extrapolate to lengths unseen in its own training set. Transfer is stronger when tasks are structurally aligned and lengths are within a moderate ratio (0.5–2).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Length generalization that is absent when training a task alone can emerge via multitask training with a related, longer auxiliary task; transfer often allows the main task to match the auxiliary task's generalization range; the effect is task-alignment dependent and seed-sensitive (unstable across initializations).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Transfer is unstable across random seeds: in the 'expected transfer' range (lengths seen by auxiliary but not main) accuracy fluctuates substantially during training; fully OOD lengths beyond auxiliary remain low. Overfitting to large length differences (very large aux/main ratio) reduces transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Successful transfer requires structural alignment between tasks (algorithmic similarity), moderate auxiliary/main length ratios (0.5–2), and co-training rather than mere exposure to longer inputs; RoPE positional encoding further helps (see separate result).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2018.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2018.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>From-scratch Transformer (string)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>6-layer 6-head Transformer (Llama-style, RoPE) — String manipulation length transfer experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>From-scratch transformers trained on string manipulation tasks (string-copy, reverse, capitalize, MQAR, capitalize-reverse) show that co-training with related auxiliary tasks at longer lengths produces length generalization transfer, whereas training main tasks alone fails to extrapolate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (Llama-like, from-scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architecture as arithmetic experiments: 6-layer, 6-head Transformer decoder with RoPE positional encodings; trained from scratch on synthetic string tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard transformer decoder with multi-head attention and RoPE</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>procedural/sequential (string manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>string copy, MQAR, reverse, capitalize, capitalize-reverse (main and auxiliary pairings)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks involve deterministic string transformations: copying, retrieving following character for repeated query substrings (MQAR), reversing character order, flipping letter case, and composed operations (capitalize-reverse). Main tasks are trained at shorter lengths, auxiliary tasks at longer lengths to test length extrapolation transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>composition of string operations (sequence of primitive operations like reverse and case-flip)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>length extrapolation split: train on shorter strings for main tasks and longer strings for auxiliary tasks; evaluate on longer strings</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>supervised multitask co-training with uniform task sampling; in-control experiments include unrelated auxiliary tasks (e.g., copy with reverse as control)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>High accuracy within training-length range (no exact numbers provided; evaluated by exact-match).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Trained alone, main string tasks fail to generalize beyond training length; co-training with structurally aligned auxiliary tasks yields substantial extrapolation to longer strings (qualitatively large improvement shown in plots). Transfer is weaker or absent with unrelated control auxiliaries.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Co-training reduces the generalization gap between main and auxiliary tasks compared to training main alone; exact numeric gap values not tabulated but shown via plotted curves.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Appendix shows that co-training with both relevant auxiliaries outperforms co-training with only one; choice of auxiliary matters (more relevant auxiliary yields stronger transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Baselines: main-only training (no extrapolation), co-training with related auxiliary tasks (successful transfer), co-training with unrelated control tasks (no transfer). Additional ablation: co-training with only one auxiliary vs both auxiliaries (transfer weaker with single auxiliary).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td>Multitask co-training transfers length-generalization from auxiliary string tasks to main string tasks when tasks are structurally aligned; pretrained models show similar effects (see SmolLM entry).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Length generalization transfer is not limited to arithmetic; string manipulation tasks also exhibit transfer when auxiliary tasks are structurally related. Using multiple (relevant) auxiliaries improves transfer compared to a single auxiliary.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Transfer fails with unrelated auxiliary tasks (control pairs) and degrades when only less-relevant auxiliaries are used; seed variability observed but not quantified numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Requires structural alignment between string tasks, sufficient auxiliary length coverage, and (empirically) co-training with multiple relevant auxiliaries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2018.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2018.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>From-scratch Transformer (maze)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>6-layer 6-head Transformer (Llama-style, RoPE) — Maze navigation length transfer experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>From-scratch transformers trained on maze navigation tasks (DFS trace and shortest path) exhibit bidirectional length generalization transfer: co-training enables each traversal task to generalize to larger mazes than when trained alone, with task relatedness and representation choices controlled to avoid token novelty confounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (Llama-like, from-scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>6-layer, 6-head Transformer decoder with RoPE; mazes serialized as adjacency lists with unique node tokens. Training varied node-count (length) while fixing grid size to avoid unseen tokens at test.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard transformer decoder with multi-head attention and RoPE</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>graph/traversal (procedural/sequential)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>DFS trace (main/aux) and Shortest Path (main/aux)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maze is a spanning-tree partial graph generated via Wilson's algorithm; length defined as number of nodes included. DFS trace requires simulating a depth-first traversal including backtracking; shortest path requires outputting the optimal path between start and goal. Main tasks are trained at shorter node counts and auxiliary at longer node counts to test length extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>sequential traversal and search strategy composition (backtracking, lookahead at branch points)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>length extrapolation split: train on mazes with fewer nodes for main task and more nodes for auxiliary task; evaluate on larger node counts</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>supervised multitask co-training; control for token novelty by fixing grid vocabulary and varying number of nodes generated</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Within-distribution (training node counts) accuracy improves and is stable (measured by exact-match path outputs), exact percentages not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Co-training DFS trace with shortest path leads DFS trace to generalize to higher node counts than when trained alone; conversely, DFS trace helps shortest path generalize. The magnitude of improvement is shown in plots (no tabulated numeric values).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Co-training reduces the generalization gap relative to single-task training; no absolute numeric gap reported. Transfer effectiveness varies by task difficulty (shortest path is harder and benefits from DFS as auxiliary).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Baselines: single-task training for DFS trace or shortest path (poorer extrapolation) versus co-training pairs (improved extrapolation); control experiments ensure that mere exposure to longer inputs is insufficient (token novelty and unrelated auxiliaries controlled).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td>Bidirectional transfer observed: learning a related traversal strategy at larger lengths improves the other task's ability to extrapolate; transfer depends on task alignment and representation choices.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Traversal-related tasks can share inductive scaffolding such that learning one at longer lengths yields extrapolation in the other; shortest-path (more lookahead-required) benefits from DFS training and vice versa.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Short fully out-of-distribution cases (beyond auxiliary maximum length) still show low accuracy; some instability across seeds observed but less characterized numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Shared algorithmic structure between tasks (e.g., traversal strategies), controlled serialization to avoid unseen tokens, and auxiliary lengths within a moderate ratio to main lengths.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2018.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2018.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SmolLM-360M Finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SmolLM-360M (pretrained) — Finetuning experiments showing length generalization transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finetuning various pretraining checkpoints of SmolLM-360M on synthetic arithmetic and maze tasks shows that progress in natural language pretraining steadily improves downstream length generalization transfer; zero-shot on reverse-add is near-zero before finetuning, confirming downstream learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SmolLM-360M</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A publicly released pretrained language model (SmolLM) of approximately 360M parameters; checkpoints from pretraining (160K to 2.56M steps) are finetuned on synthetic tasks to probe transfer of length-generalization capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>360M</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard pretrained transformer (details per SmolLM release); used with finetuning and access to intermediate checkpoints</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mathematical/arithmetic and graph/maze (algorithmic downstream tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>reverse add; shortest path; other arithmetic and string groups (finetuning from multiple pretraining checkpoints)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Downstream synthetic tasks (reverse-addition and maze solving) are used to evaluate how pretraining checkpoints influence the model's ability to extrapolate to longer inputs after finetuning. Pretraining dataset includes natural language and code, providing long-range structure exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>algorithmic reasoning and length extrapolation; tests reuse of pretraining-induced inductive biases</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>length extrapolation: finetune on shorter lengths for downstream tasks and evaluate on longer lengths</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>finetuning from multiple checkpoints; same supervised training procedure for downstream tasks as from-scratch experiments</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Before finetuning, zero-shot accuracy on reverse add is near-zero; after finetuning, in-distribution accuracy on training lengths improves rapidly (exact numbers not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Trend: downstream extrapolation to longer inputs improves steadily with pretraining progress (checkpoints later in training yield better length generalization transfer after finetuning). Specific quantitative increases are shown in plotted curves (no single numeric summary reported).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Generalization gap decreases with more pretraining (later checkpoints), indicating pretraining reduces the difference between in-distribution and OOD length performance; exact gap values not tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Baselines: finetuning from earlier pretraining checkpoints vs later checkpoints; from-scratch training; also control task pairs (e.g., reverse add with copy-first-op) that do not transfer even when finetuned from pretrained checkpoints. Results show pretrained finetuning mirrors from-scratch co-training findings but with pretraining improving transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Only one model size (360M) evaluated at multiple pretraining checkpoints; no multi-size scaling study reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td>Natural language pretraining instills reusable inductive biases that facilitate length generalization transfer on downstream synthetic tasks; extent of transfer improves monotonically with pretraining progress in the checkpoints evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining on diverse natural-language and code corpora provides computational scaffolding that aids extrapolation in downstream algorithmic tasks; finetuning from later checkpoints yields better OOD length generalization than from earlier checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Pretraining alone without finetuning does not solve the synthetic tasks (zero-shot near-zero); finetuned models still show limited performance beyond auxiliary-task maximum lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Pretraining progress (later checkpoints) and finetuning on task-specific data; task structural alignment with inductive biases learned during pretraining appears helpful even when input tokens/formats differ (e.g., maze node tokens introduced at finetuning).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2018.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2018.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoPE vs NoPE Positional Encoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rotary Positional Encoding (RoPE) vs No Positional Encoding (NoPE) — effect on length generalization transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experimental comparison shows that models using RoPE consistently enable stronger length generalization transfer from auxiliary to main tasks than equivalent models with NoPE (no positional encoding). This contrasts with some prior reports favoring NoPE for length extrapolation in other settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (RoPE vs NoPE variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same 6-layer 6-head transformer architecture, ablated to either include Rotary Positional Embeddings (RoPE) or remove positional encodings (NoPE) to evaluate effect on transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>positional encoding variation (RoPE versus none) as primary architectural ablation</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>algorithmic tasks across arithmetic, string, and maze domains</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>various main/auxiliary task groups used elsewhere in the paper (reverse add groups, string groups, maze groups)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same length-extrapolation multitask experimental setup; compare whether inclusion of RoPE vs NoPE affects the ability of auxiliary-task training to transfer length generalization to the main task.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>algorithmic composition / length extrapolation</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>length extrapolation split (main short, auxiliary long)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>standard supervised multitask training; architectural ablation: RoPE vs NoPE</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Not separately tabulated per encoding; within-distribution learning occurs under both encodings but reported transfer differs.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>RoPE models consistently exhibit stronger transfer and smaller generalization gaps compared to NoPE models across multiple task groups; NoPE yields notably weaker transfer in most tasks (plots show RoPE > NoPE).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>RoPE produces lower generalization gaps across several task groups compared to NoPE (plotted comparison shown in paper; no single aggregate numeric provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Baseline is identical architecture with NoPE; comparison shows RoPE consistently outperforms NoPE for length generalization transfer in the experiments performed.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Direct architectural ablation between positional encoding strategies (RoPE vs NoPE) with identical model sizes and training; RoPE leads to stronger transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td>Rotary positional encodings (RoPE) facilitate length generalization transfer better than removing positional encodings, in the tasks and architectures tested.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Contrary to some prior results that favored NoPE for length extrapolation, in the multitask transfer setting RoPE consistently enables stronger transfer from auxiliary to main tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Using RoPE positional encodings in standard transformer architectures supports transfer of length-generalization ability across related tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2018.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2018.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Circuit-sharing / Attention reuse analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention-similarity and head ablation analyses linking circuit sharing to length generalization transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mechanistic analyses show that successful length generalization transfer correlates with reuse of attention circuitry across tasks: attention-matrix differences and attention-head mean-ablation-map differences (6x6 maps) tend to decrease when transfer occurs, particularly in arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (6x6 heads/layers) used in above experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same 6-layer, 6-head Transformer; internal activations inspected across checkpoints to compute attention similarity and head-importance maps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>multi-head attention; analysis uses activation patching / mean-ablating head outputs to build head-importance maps</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>algorithmic tasks (arithmetic, string, maze)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>reverse add & reverse subtract (arithmetic example), other task pairs examined in Appendix</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Investigate whether internal attention mechanisms are shared between pairs of tasks that exhibit length generalization transfer by measuring: (1) attention matrix difference (sum of absolute differences across attention matrices per head), and (2) mean-ablation map difference (average absolute difference between 6x6 head-importance matrices obtained by mean-ablating each head). Correlate these metrics with the generalization gap across checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>analysis of circuit reuse underlying algorithmic composition</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>analyze training checkpoints from multitask runs; compute metrics and correlate with generalization performance</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Used as dependent variable correlated against attention similarity metrics; reported to align with decreases in attention-difference metrics (especially for arithmetic tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Control pair (reverse add & copy-first-op) shows no correlation between attention-similarity metrics and generalization (i.e., no transfer and no circuit sharing), reinforcing that observed correlations are specific to successful-transfer pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td>When length generalization transfer occurs, attention-matrix differences and head-importance map differences between tasks decline (greater similarity), suggesting reuse of the same attention heads/circuits across tasks; correlation strongest in arithmetic tasks, ablation-map metric more informative for string tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Initial mechanistic evidence indicates that length generalization transfer correlates with sharing of internal attention-based computation (reused heads), measured via attention difference and mean-ablation maps; arithmetic tasks show strong correlation at both matrix and head-importance levels, while string tasks show clearer signal in head-importance similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>For control/unrelated task pairs, similarity metrics do not correlate with generalization, supporting that metric-generalization correlations are not incidental.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Transfer tends to coincide with models converging to shared attention circuitry across tasks as training progresses; activation-patching / mean-ablation techniques reveal which heads are important and whether importance aligns across tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformers can achieve length generalization but not robustly <em>(Rating: 2)</em></li>
                <li>Improving length-generalization in transformers via task hinting <em>(Rating: 2)</em></li>
                <li>Teaching arithmetic to small transformers <em>(Rating: 2)</em></li>
                <li>The impact of positional encoding on length generalization in transformers <em>(Rating: 2)</em></li>
                <li>From interpolation to extrapolation: Complete length generalization for arithmetic transformers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2018",
    "paper_id": "paper-279305768",
    "extraction_schema_id": "extraction-schema-48",
    "extracted_data": [
        {
            "name_short": "From-scratch Transformer (arithmetic)",
            "name_full": "6-layer 6-head Transformer (Llama-style, RoPE) — Arithmetic length transfer experiments",
            "brief_description": "From-scratch transformer models (6 layers, 6 heads, Rotary Positional Embeddings) are trained in multitask setups to study length generalization transfer on arithmetic tasks (reverse add as main task, with auxiliary tasks no-carry, carry-only, reverse-subtract). Co-training with longer, structurally related auxiliary tasks enables extrapolation of the main task to longer lengths that the main task alone fails to reach.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (Llama-like, from-scratch)",
            "model_description": "Decoder-only Transformer following Llama-style architecture with 6 layers and 6 attention heads, using Rotary Positional Embeddings (RoPE); trained from scratch on synthetic multitask datasets.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "standard transformer decoder architecture with multi-head attention and RoPE positional encoding",
            "task_domain": "mathematical/arithmetic (algorithmic)",
            "task_name": "reverse add (main) + auxiliary: no carry, carry only, reverse subtract",
            "task_description": "Reverse-addition: compute the sum of two integers with input/output digit sequences reversed. Auxiliary tasks include no-carry (digit-wise sum mod 10), carry-only (binary mask of carry positions), and reverse-subtract (digit-wise subtraction in reverse order). These tasks are algorithmic and structurally related, enabling study of reuse of computational circuits.",
            "compositional_depth": null,
            "composition_type": "function composition / algorithmic step composition (digit-wise operations and carry propagation)",
            "split_type": "length extrapolation split: train main task on shorter lengths (e.g., max 16 digits) and auxiliary task on longer lengths (e.g., up to 32); evaluate on longer (OOD) lengths",
            "training_strategy": "standard supervised multitask training; tasks sampled uniformly per iteration; all data generated on-the-fly",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Within-training-length accuracy is high and stabilizes quickly (exact numbers not reported; evaluated by exact-match on 1024 examples).",
            "compositional_performance": "When trained alone, reverse add fails to generalize beyond its training length (near-zero extrapolation). When co-trained with structurally related auxiliary tasks trained to longer lengths, the main task frequently extrapolates up to the auxiliary task's length (qualitative: 'often matching the auxiliary task's generalization range'). Performance in the transfer range is seed-dependent and variable.",
            "generalization_gap": "Defined in-paper as normalized average difference between main and auxiliary accuracy across evaluation lengths; co-training reduces the gap substantially compared to training main task alone, with strongest transfer when auxiliary/main length ratio is between 0.5 and 2. (No single numeric gap reported.)",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Baselines: (1) main task trained alone (fails to extrapolate beyond training length); (2) main task co-trained with longer, related auxiliary tasks (successful transfer); (3) main task co-trained with unrelated control auxiliary (copy-first-op) — no transfer. Comparisons reported as accuracy-vs-length plots across seeds (exact numeric curves provided in figures but not tabulated).",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": "Shows transfer of length-generalization ability from auxiliary tasks to main task: co-training with longer, related auxiliary tasks enables the main task to extrapolate to lengths unseen in its own training set. Transfer is stronger when tasks are structurally aligned and lengths are within a moderate ratio (0.5–2).",
            "key_findings": "Length generalization that is absent when training a task alone can emerge via multitask training with a related, longer auxiliary task; transfer often allows the main task to match the auxiliary task's generalization range; the effect is task-alignment dependent and seed-sensitive (unstable across initializations).",
            "failure_analysis": "Transfer is unstable across random seeds: in the 'expected transfer' range (lengths seen by auxiliary but not main) accuracy fluctuates substantially during training; fully OOD lengths beyond auxiliary remain low. Overfitting to large length differences (very large aux/main ratio) reduces transfer.",
            "success_conditions": "Successful transfer requires structural alignment between tasks (algorithmic similarity), moderate auxiliary/main length ratios (0.5–2), and co-training rather than mere exposure to longer inputs; RoPE positional encoding further helps (see separate result).",
            "uuid": "e2018.0"
        },
        {
            "name_short": "From-scratch Transformer (string)",
            "name_full": "6-layer 6-head Transformer (Llama-style, RoPE) — String manipulation length transfer experiments",
            "brief_description": "From-scratch transformers trained on string manipulation tasks (string-copy, reverse, capitalize, MQAR, capitalize-reverse) show that co-training with related auxiliary tasks at longer lengths produces length generalization transfer, whereas training main tasks alone fails to extrapolate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (Llama-like, from-scratch)",
            "model_description": "Same architecture as arithmetic experiments: 6-layer, 6-head Transformer decoder with RoPE positional encodings; trained from scratch on synthetic string tasks.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "standard transformer decoder with multi-head attention and RoPE",
            "task_domain": "procedural/sequential (string manipulation)",
            "task_name": "string copy, MQAR, reverse, capitalize, capitalize-reverse (main and auxiliary pairings)",
            "task_description": "Tasks involve deterministic string transformations: copying, retrieving following character for repeated query substrings (MQAR), reversing character order, flipping letter case, and composed operations (capitalize-reverse). Main tasks are trained at shorter lengths, auxiliary tasks at longer lengths to test length extrapolation transfer.",
            "compositional_depth": null,
            "composition_type": "composition of string operations (sequence of primitive operations like reverse and case-flip)",
            "split_type": "length extrapolation split: train on shorter strings for main tasks and longer strings for auxiliary tasks; evaluate on longer strings",
            "training_strategy": "supervised multitask co-training with uniform task sampling; in-control experiments include unrelated auxiliary tasks (e.g., copy with reverse as control)",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "High accuracy within training-length range (no exact numbers provided; evaluated by exact-match).",
            "compositional_performance": "Trained alone, main string tasks fail to generalize beyond training length; co-training with structurally aligned auxiliary tasks yields substantial extrapolation to longer strings (qualitatively large improvement shown in plots). Transfer is weaker or absent with unrelated control auxiliaries.",
            "generalization_gap": "Co-training reduces the generalization gap between main and auxiliary tasks compared to training main alone; exact numeric gap values not tabulated but shown via plotted curves.",
            "performance_by_depth": null,
            "performance_by_composition_type": "Appendix shows that co-training with both relevant auxiliaries outperforms co-training with only one; choice of auxiliary matters (more relevant auxiliary yields stronger transfer).",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Baselines: main-only training (no extrapolation), co-training with related auxiliary tasks (successful transfer), co-training with unrelated control tasks (no transfer). Additional ablation: co-training with only one auxiliary vs both auxiliaries (transfer weaker with single auxiliary).",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": "Multitask co-training transfers length-generalization from auxiliary string tasks to main string tasks when tasks are structurally aligned; pretrained models show similar effects (see SmolLM entry).",
            "key_findings": "Length generalization transfer is not limited to arithmetic; string manipulation tasks also exhibit transfer when auxiliary tasks are structurally related. Using multiple (relevant) auxiliaries improves transfer compared to a single auxiliary.",
            "failure_analysis": "Transfer fails with unrelated auxiliary tasks (control pairs) and degrades when only less-relevant auxiliaries are used; seed variability observed but not quantified numerically.",
            "success_conditions": "Requires structural alignment between string tasks, sufficient auxiliary length coverage, and (empirically) co-training with multiple relevant auxiliaries.",
            "uuid": "e2018.1"
        },
        {
            "name_short": "From-scratch Transformer (maze)",
            "name_full": "6-layer 6-head Transformer (Llama-style, RoPE) — Maze navigation length transfer experiments",
            "brief_description": "From-scratch transformers trained on maze navigation tasks (DFS trace and shortest path) exhibit bidirectional length generalization transfer: co-training enables each traversal task to generalize to larger mazes than when trained alone, with task relatedness and representation choices controlled to avoid token novelty confounds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (Llama-like, from-scratch)",
            "model_description": "6-layer, 6-head Transformer decoder with RoPE; mazes serialized as adjacency lists with unique node tokens. Training varied node-count (length) while fixing grid size to avoid unseen tokens at test.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "standard transformer decoder with multi-head attention and RoPE",
            "task_domain": "graph/traversal (procedural/sequential)",
            "task_name": "DFS trace (main/aux) and Shortest Path (main/aux)",
            "task_description": "Maze is a spanning-tree partial graph generated via Wilson's algorithm; length defined as number of nodes included. DFS trace requires simulating a depth-first traversal including backtracking; shortest path requires outputting the optimal path between start and goal. Main tasks are trained at shorter node counts and auxiliary at longer node counts to test length extrapolation.",
            "compositional_depth": null,
            "composition_type": "sequential traversal and search strategy composition (backtracking, lookahead at branch points)",
            "split_type": "length extrapolation split: train on mazes with fewer nodes for main task and more nodes for auxiliary task; evaluate on larger node counts",
            "training_strategy": "supervised multitask co-training; control for token novelty by fixing grid vocabulary and varying number of nodes generated",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Within-distribution (training node counts) accuracy improves and is stable (measured by exact-match path outputs), exact percentages not provided.",
            "compositional_performance": "Co-training DFS trace with shortest path leads DFS trace to generalize to higher node counts than when trained alone; conversely, DFS trace helps shortest path generalize. The magnitude of improvement is shown in plots (no tabulated numeric values).",
            "generalization_gap": "Co-training reduces the generalization gap relative to single-task training; no absolute numeric gap reported. Transfer effectiveness varies by task difficulty (shortest path is harder and benefits from DFS as auxiliary).",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Baselines: single-task training for DFS trace or shortest path (poorer extrapolation) versus co-training pairs (improved extrapolation); control experiments ensure that mere exposure to longer inputs is insufficient (token novelty and unrelated auxiliaries controlled).",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": "Bidirectional transfer observed: learning a related traversal strategy at larger lengths improves the other task's ability to extrapolate; transfer depends on task alignment and representation choices.",
            "key_findings": "Traversal-related tasks can share inductive scaffolding such that learning one at longer lengths yields extrapolation in the other; shortest-path (more lookahead-required) benefits from DFS training and vice versa.",
            "failure_analysis": "Short fully out-of-distribution cases (beyond auxiliary maximum length) still show low accuracy; some instability across seeds observed but less characterized numerically.",
            "success_conditions": "Shared algorithmic structure between tasks (e.g., traversal strategies), controlled serialization to avoid unseen tokens, and auxiliary lengths within a moderate ratio to main lengths.",
            "uuid": "e2018.2"
        },
        {
            "name_short": "SmolLM-360M Finetuning",
            "name_full": "SmolLM-360M (pretrained) — Finetuning experiments showing length generalization transfer",
            "brief_description": "Finetuning various pretraining checkpoints of SmolLM-360M on synthetic arithmetic and maze tasks shows that progress in natural language pretraining steadily improves downstream length generalization transfer; zero-shot on reverse-add is near-zero before finetuning, confirming downstream learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SmolLM-360M",
            "model_description": "A publicly released pretrained language model (SmolLM) of approximately 360M parameters; checkpoints from pretraining (160K to 2.56M steps) are finetuned on synthetic tasks to probe transfer of length-generalization capabilities.",
            "model_size": "360M",
            "is_pretrained": true,
            "architectural_features": "standard pretrained transformer (details per SmolLM release); used with finetuning and access to intermediate checkpoints",
            "task_domain": "mathematical/arithmetic and graph/maze (algorithmic downstream tasks)",
            "task_name": "reverse add; shortest path; other arithmetic and string groups (finetuning from multiple pretraining checkpoints)",
            "task_description": "Downstream synthetic tasks (reverse-addition and maze solving) are used to evaluate how pretraining checkpoints influence the model's ability to extrapolate to longer inputs after finetuning. Pretraining dataset includes natural language and code, providing long-range structure exposure.",
            "compositional_depth": null,
            "composition_type": "algorithmic reasoning and length extrapolation; tests reuse of pretraining-induced inductive biases",
            "split_type": "length extrapolation: finetune on shorter lengths for downstream tasks and evaluate on longer lengths",
            "training_strategy": "finetuning from multiple checkpoints; same supervised training procedure for downstream tasks as from-scratch experiments",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Before finetuning, zero-shot accuracy on reverse add is near-zero; after finetuning, in-distribution accuracy on training lengths improves rapidly (exact numbers not provided).",
            "compositional_performance": "Trend: downstream extrapolation to longer inputs improves steadily with pretraining progress (checkpoints later in training yield better length generalization transfer after finetuning). Specific quantitative increases are shown in plotted curves (no single numeric summary reported).",
            "generalization_gap": "Generalization gap decreases with more pretraining (later checkpoints), indicating pretraining reduces the difference between in-distribution and OOD length performance; exact gap values not tabulated.",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Baselines: finetuning from earlier pretraining checkpoints vs later checkpoints; from-scratch training; also control task pairs (e.g., reverse add with copy-first-op) that do not transfer even when finetuned from pretrained checkpoints. Results show pretrained finetuning mirrors from-scratch co-training findings but with pretraining improving transferability.",
            "architectural_comparison": null,
            "scale_effects": "Only one model size (360M) evaluated at multiple pretraining checkpoints; no multi-size scaling study reported.",
            "transfer_results": "Natural language pretraining instills reusable inductive biases that facilitate length generalization transfer on downstream synthetic tasks; extent of transfer improves monotonically with pretraining progress in the checkpoints evaluated.",
            "key_findings": "Pretraining on diverse natural-language and code corpora provides computational scaffolding that aids extrapolation in downstream algorithmic tasks; finetuning from later checkpoints yields better OOD length generalization than from earlier checkpoints.",
            "failure_analysis": "Pretraining alone without finetuning does not solve the synthetic tasks (zero-shot near-zero); finetuned models still show limited performance beyond auxiliary-task maximum lengths.",
            "success_conditions": "Pretraining progress (later checkpoints) and finetuning on task-specific data; task structural alignment with inductive biases learned during pretraining appears helpful even when input tokens/formats differ (e.g., maze node tokens introduced at finetuning).",
            "uuid": "e2018.3"
        },
        {
            "name_short": "RoPE vs NoPE Positional Encoding",
            "name_full": "Rotary Positional Encoding (RoPE) vs No Positional Encoding (NoPE) — effect on length generalization transfer",
            "brief_description": "Experimental comparison shows that models using RoPE consistently enable stronger length generalization transfer from auxiliary to main tasks than equivalent models with NoPE (no positional encoding). This contrasts with some prior reports favoring NoPE for length extrapolation in other settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (RoPE vs NoPE variants)",
            "model_description": "Same 6-layer 6-head transformer architecture, ablated to either include Rotary Positional Embeddings (RoPE) or remove positional encodings (NoPE) to evaluate effect on transfer.",
            "model_size": null,
            "is_pretrained": false,
            "architectural_features": "positional encoding variation (RoPE versus none) as primary architectural ablation",
            "task_domain": "algorithmic tasks across arithmetic, string, and maze domains",
            "task_name": "various main/auxiliary task groups used elsewhere in the paper (reverse add groups, string groups, maze groups)",
            "task_description": "Same length-extrapolation multitask experimental setup; compare whether inclusion of RoPE vs NoPE affects the ability of auxiliary-task training to transfer length generalization to the main task.",
            "compositional_depth": null,
            "composition_type": "algorithmic composition / length extrapolation",
            "split_type": "length extrapolation split (main short, auxiliary long)",
            "training_strategy": "standard supervised multitask training; architectural ablation: RoPE vs NoPE",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": "Not separately tabulated per encoding; within-distribution learning occurs under both encodings but reported transfer differs.",
            "compositional_performance": "RoPE models consistently exhibit stronger transfer and smaller generalization gaps compared to NoPE models across multiple task groups; NoPE yields notably weaker transfer in most tasks (plots show RoPE &gt; NoPE).",
            "generalization_gap": "RoPE produces lower generalization gaps across several task groups compared to NoPE (plotted comparison shown in paper; no single aggregate numeric provided).",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Baseline is identical architecture with NoPE; comparison shows RoPE consistently outperforms NoPE for length generalization transfer in the experiments performed.",
            "architectural_comparison": "Direct architectural ablation between positional encoding strategies (RoPE vs NoPE) with identical model sizes and training; RoPE leads to stronger transfer.",
            "scale_effects": null,
            "transfer_results": "Rotary positional encodings (RoPE) facilitate length generalization transfer better than removing positional encodings, in the tasks and architectures tested.",
            "key_findings": "Contrary to some prior results that favored NoPE for length extrapolation, in the multitask transfer setting RoPE consistently enables stronger transfer from auxiliary to main tasks.",
            "failure_analysis": null,
            "success_conditions": "Using RoPE positional encodings in standard transformer architectures supports transfer of length-generalization ability across related tasks.",
            "uuid": "e2018.4"
        },
        {
            "name_short": "Circuit-sharing / Attention reuse analysis",
            "name_full": "Attention-similarity and head ablation analyses linking circuit sharing to length generalization transfer",
            "brief_description": "Mechanistic analyses show that successful length generalization transfer correlates with reuse of attention circuitry across tasks: attention-matrix differences and attention-head mean-ablation-map differences (6x6 maps) tend to decrease when transfer occurs, particularly in arithmetic tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (6x6 heads/layers) used in above experiments",
            "model_description": "Same 6-layer, 6-head Transformer; internal activations inspected across checkpoints to compute attention similarity and head-importance maps.",
            "model_size": null,
            "is_pretrained": null,
            "architectural_features": "multi-head attention; analysis uses activation patching / mean-ablating head outputs to build head-importance maps",
            "task_domain": "algorithmic tasks (arithmetic, string, maze)",
            "task_name": "reverse add & reverse subtract (arithmetic example), other task pairs examined in Appendix",
            "task_description": "Investigate whether internal attention mechanisms are shared between pairs of tasks that exhibit length generalization transfer by measuring: (1) attention matrix difference (sum of absolute differences across attention matrices per head), and (2) mean-ablation map difference (average absolute difference between 6x6 head-importance matrices obtained by mean-ablating each head). Correlate these metrics with the generalization gap across checkpoints.",
            "compositional_depth": null,
            "composition_type": "analysis of circuit reuse underlying algorithmic composition",
            "split_type": null,
            "training_strategy": "analyze training checkpoints from multitask runs; compute metrics and correlate with generalization performance",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": null,
            "generalization_gap": "Used as dependent variable correlated against attention similarity metrics; reported to align with decreases in attention-difference metrics (especially for arithmetic tasks).",
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Control pair (reverse add & copy-first-op) shows no correlation between attention-similarity metrics and generalization (i.e., no transfer and no circuit sharing), reinforcing that observed correlations are specific to successful-transfer pairs.",
            "architectural_comparison": null,
            "scale_effects": null,
            "transfer_results": "When length generalization transfer occurs, attention-matrix differences and head-importance map differences between tasks decline (greater similarity), suggesting reuse of the same attention heads/circuits across tasks; correlation strongest in arithmetic tasks, ablation-map metric more informative for string tasks.",
            "key_findings": "Initial mechanistic evidence indicates that length generalization transfer correlates with sharing of internal attention-based computation (reused heads), measured via attention difference and mean-ablation maps; arithmetic tasks show strong correlation at both matrix and head-importance levels, while string tasks show clearer signal in head-importance similarity.",
            "failure_analysis": "For control/unrelated task pairs, similarity metrics do not correlate with generalization, supporting that metric-generalization correlations are not incidental.",
            "success_conditions": "Transfer tends to coincide with models converging to shared attention circuitry across tasks as training progresses; activation-patching / mean-ablation techniques reveal which heads are important and whether importance aligns across tasks.",
            "uuid": "e2018.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformers can achieve length generalization but not robustly",
            "rating": 2
        },
        {
            "paper_title": "Improving length-generalization in transformers via task hinting",
            "rating": 2
        },
        {
            "paper_title": "Teaching arithmetic to small transformers",
            "rating": 2
        },
        {
            "paper_title": "The impact of positional encoding on length generalization in transformers",
            "rating": 2
        },
        {
            "paper_title": "From interpolation to extrapolation: Complete length generalization for arithmetic transformers",
            "rating": 1
        }
    ],
    "cost": 0.01712725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Extrapolation by Association: Length Generalization Transfer in Transformers
4 Aug 2025</p>
<p>Ziyang Cai 
Nayoung Lee 
Avi Schwarzschild 
Samet Oymak 
Dimitris Papailiopoulos 
Acc Len 
Aux Acc 
Len Main 
Main Task </p>
<p>University of Wisconsin-Madison</p>
<p>University of Wisconsin-Madison</p>
<p>Carnegie Mellon University</p>
<p>University of Michigan</p>
<p>University of Wisconsin-Madison Microsoft Research</p>
<p>Extrapolation by Association: Length Generalization Transfer in Transformers
4 Aug 2025C1188544F7EFC8544EFFA565DDD79F86arXiv:2506.09251v2[cs.CL]Carry Only2050465+7829548=00000011 No Carry2050465+7829548=98799030 Reverse Subtract2050465+7829548=5878182-Auxiliary Task
Transformer language models have demonstrated impressive generalization capabilities in natural language domains, yet we lack a fine-grained understanding of how such generalization arises.In this paper, we investigate length generalization-the ability to extrapolate from shorter to longer inputs-through the lens of task association.We find that length generalization can be transferred across related tasks.That is, training a model with a longer and related auxiliary task can lead it to generalize to unseen and longer inputs from some other target task.We demonstrate this length generalization transfer across diverse algorithmic tasks, including arithmetic operations, string transformations, and maze navigation.Our results show that transformer models can inherit generalization capabilities from similar tasks when trained jointly.Moreover, we observe similar transfer effects in pretrained language models, suggesting that pretraining equips models with reusable computational scaffolding that facilitates extrapolation in downstream settings.Finally, we provide initial mechanistic evidence that length generalization transfer correlates with the re-use of the same attention heads between the tasks.Together, our findings deepen our understanding of how transformers generalize to out-of-distribution inputs and highlight the compositional reuse of inductive structure across tasks.</p>
<p>Introduction</p>
<p>A central theme of transformer language models is their ability to generalize.By scaling up data and model size, large language models develop emergent abilities that exceed expectations [Wei et al., 2022].They can also transfer knowledge across domains and tasks [OpenAI, 2024, Brown et al., 2020, Sanh et al., 2022].While it is widely believed that language models are not simply parroting or memorizing their training data, we still lack a fine-grained understanding of how language models apply skills learned during training to potentially unseen problems.</p>
<p>The out-of-distribution (OOD) generalization capabilities of language models have garnered much attention in the literature [Anil et al., 2022, Zhang et al., 2024, Yang et al., 2024].In this work, we study a canonical example of OOD generalization, length generalization, which is the ability to generalize from shorter to longer inputs [Zhou et al., 2023].There is a long line of work focusing on improving length generalization of arithmetic tasks in transformers, which has spurred innovations in positional encoding schemes and transformer architecture [Cho et al., 2024, McLeish et al., 2024].Closely related is the concept of compositional generalization, where the model combines previously learned skills to solve new problems [Yang et al., 2024, Xu et al., 2024].</p>
<p>Figure 1: Trained separately, each task fails to generalize to longer inputs.When trained jointly, the main task inherits the generalization range of the auxiliary task.</p>
<p>To showcase the length generalization transfer capabilities in transformers, we choose three distinct groups of synthetic tasks.The tasks in each group are related such that they represent similar algorithmic procedures.Within each group, we train multiple tasks together, and crucially, we train an "auxiliary task" at a longer length and a "main task" at a shorter length.Using this setup, we observe that the shorter main task generalizes to the length of the longer auxiliary task when trained together.See Figure 2 for the tasks and respective lengths used in each experiment.</p>
<p>Contributions</p>
<ol>
<li>We present the phenomenon of length generalization transfer, in which transformer models trained on related tasks exhibit extrapolation behavior not present when trained on the target task alone, providing new insights on the effect of multitask training on length generalization.2. We show that the same phenomenon replicates in pretrained language models, and that natural language pretraining transfers length generalization capabilities to synthetic downstream tasks.3. We provide mechanistic evidence that transfer correlates with shared internal computation-specifically, the reuse of attention heads across tasks.</li>
</ol>
<p>Related Works</p>
<p>Length Generalization.Length generalization concerns extrapolating to longer sequence lengths than those seen during training [Dubois et al., 2019, Hupkes et al., 2020, Newman et al., 2020, Anil et al., 2022].Previous approaches include architectural modifications such as specialized positional embeddings [Press et al., 2021, Li et al., 2023, Ruoss et al., 2023, Kazemnejad et al., 2024, Sabbaghi et al., 2024, Cho et al., 2024, Zhou et al., 2024, McLeish et al., 2024], looping [Fan et al., 2024], novel attention mechanisms [Duan et al., 2023, Li et al., 2025], and input format augmentation [Zhou et al., 2023[Zhou et al., , 2024]].Beyond arithmetic, Yehudai et al. [2021] studies length generalization in graph tasks.In contrast, our work examines a novel mechanism from which length generalization emerges: transfer from related tasks.Finally, closely related to our work, "task hinting" [Awasthi and Gupta, 2023] trains sorting and increment-by-one tasks with simpler auxiliary tasks, showing improvements in length generalization performance.</p>
<p>Compositional Capabilities.To explain emergent capabilities in language models, many works study compositional generalization to understand whether transformers can gain abilities beyond those in the training set.Yu et al. [2023], Zhao et al. [2025] and Hosseini et al. [2024] design benchmarks testing the ability to combine learned skills to solve compositional math problems.Ahuja and Mansouri [2024] derive provable guarantees for length and compositional generalization conditioned on training set diversity.Some works use synthetic tasks to probe compositional generalization.Ramesh et al. show transformers achieve compositional generalization on unseen combinations using a series of bijections and permutations applied to strings, while Abedsoltan et al. [2025] show similar results on families of parity functions.</p>
<p>For the specific task of reverse addition, works like Quirke and Barez [2023] and Quirke et al. [2025] identify computational circuits responsible for compositional subtasks and show transferability of such circuits to the related task of subtraction.</p>
<p>Experimental Settings</p>
<p>Models.For from-scratch experiments, we use transformer models with 6 heads and 6 layers, following the Llama architecture [AI@Meta, 2024], which uses Rotary Positional Embeddings (RoPE) [Su et al., 2023] for position encoding.For experiments with pretrained models, we use SmolLM [Allal et al., 2024], which provides access to intermediate checkpoints during pretraining, allowing us to investigate how length generalization transfer evolves over time.</p>
<p>Tasks.We evaluate length generalization transfer across three categories of algorithmic problems: arithmetic, string manipulation, and maze solving.Our tasks include:</p>
<p>• Arithmetic Tasks reverse add -Compute the sum of two integers, presented in reversed order.</p>
<p>no carry -Compute digit-wise sums mod 10, without carry propagation.</p>
<p>carry only -Output a binary mask indicating carry positions during addition.</p>
<p>reverse subtract -Compute the reversed digit-wise difference between two numbers.</p>
<p>n × 3 CoT multiply -Multiply an n-digit number by 3, with chain-of-thought steps.• String Manipulation Tasks string copy -Return the input string unchanged.</p>
<p>-MQAR (Multi-Query Associative Recall) [Arora et al., 2023] -Given a repeated query substring, retrieve the next character following each occurrence.capitalize -Flip the case of all alphabetic characters (lower ↔ upper).</p>
<p>reverse -Reverse the character order of the input string.</p>
<p>capitalize-reverse -Apply both reversal and case-flipping to the input string.</p>
<p>• Maze Tasks -DFS trace -Simulate a depth-first search from a start node to a goal node in a maze.</p>
<p>shortest path -Return the optimal (shortest) path between a start and goal node.</p>
<p>Task Groups.We construct task groups by pairing a main task, trained on short sequence lengths, with one or more auxiliary tasks, trained on longer sequences.The main goal is to evaluate whether training on a related auxiliary task improves the main task's ability to generalize to longer inputs, despite never seeing such lengths during training.The list of task groups are:  32), reverse (32) DFS trace (32) shortest path (64)</p>
<p>Data sampling and Task Length.Since we train under a multi-task setting, at each iteration, a task is sampled uniformly at random from a predefined task group.For the selected task, an individual training example is constructed based on a single governing parameter: length, which determines the size or complexity of the problem instance.The length of each example is sampled uniformly from a specified range for that task.All training data is generated on-the-fly during training.</p>
<p>Since the notion of length varies across task types, we define length for each task as:</p>
<p>• Addition Tasks: the maximum number of digits in both operands.</p>
<p>• String Tasks: the number of characters in the input string.</p>
<p>• Maze Tasks: the number of nodes in the input maze graph.See Section 4.3 for further details.Training and Evaluation.Each example consists of an input-output pair.We use a loss mask to train only on output tokens (and for MQAR, only on answer characters).At test time, we evaluate using exact match accuracy on a fixed test set of 1024 examples.For each configuration, we report results across 5 random initialization seeds but the dataset is kept the same.Full experimental configurations and hyperparameter details are provided in Appendix B.</p>
<p>Length Generalization Transfer in Algorithmic Tasks</p>
<p>In this section, we demonstrate that while length generalization is often difficult for algorithmic tasks, it can emerge through transfer when the model is co-trained on longer auxiliary tasks.Figure 2 illustrates the three categories of tasks we study-arithmetic operations, string transformations, and maze navigation.</p>
<p>Arithmetic Tasks</p>
<p>Reverse addition has become a popular synthetic task for studying length generalization [Lee et al., 2023, Shen et al., 2023, Zhou et al., 2023, 2024, Cho et al., 2024, McLeish et al., 2024, Lee et al., 2025] in Transformers.The task involves calculating the sum of two randomly sampled integers, and length generalization in this task involves training on examples up to some fixed length, and generalizing on test data beyond the training lengths.Here, we adopt the reverse add format proposed by Lee et al. [2023], where the operands and the sum are reversed for faster learning.For the auxiliary tasks, we consider (1) reverse subtract , which computes the difference between two operands, (2) no carry , which computes the digit-wise sum mod 10, ignoring the carries, and</p>
<p>(3) carry only , which computes the locations where a carry happens in the addition.As shown in Figure 3, models trained only on reverse add (Figure 3d) struggle to generalize beyond the training length.However, when co-trained with longer auxiliary tasks (Figures 3a,3b,3c), the model successfully extrapolates, often matching the auxiliary task's generalization range.This provides empirical evidence that length generalization can transfer across tasks.</p>
<p>It is worth noting that the generalization behavior is not entirely robust: different random seeds yield noticeably different outcomes, suggesting unstable training dynamics.We discuss this instability further in Section 6.2.</p>
<p>String Tasks</p>
<p>We now turn to string operations, where we observe similar transfer effects on two task groups.</p>
<p>The tasks include: string copy , which returns the input unchanged; MQAR (Multi-Query Associative Recall) [Arora et al., 2023], where the model retrieves the next character given a random substring; reverse , which reverses character order; capitalize , which inverts letter case; and capitalize-reverse , combining case inversion and reversal.</p>
<p>Figure 4 shows that when trained on main tasks alone (Figures 4b, 4d), the model does not generalize beyond the training range.On the other hand,Adding training with auxiliary tasks enables substantial extrapolation (as shown in Figures 4a and 4c).</p>
<p>Maze Tasks</p>
<p>Lastly, we examine maze-solving tasks as a testbed for length generalization transfer.We define a maze as a spanning tree over a square grid, generated using Wilson's algorithm [Wilson, 1996], which A challenge in defining length generalization for mazes is that increasing grid size introduces unseen node tokens at test time.To avoid this, we fix the grid size and instead vary the number of nodes included in the spanning tree.Specifically, we define the input length as the total number of nodes in the maze graph and generate partial mazes by stopping Wilson's algorithm early.For example, to construct a 32-node maze on an 8 × 8 grid, we run the algorithm until 32 nodes are added.The resulting maze may not span the full grid but remains a valid traversal problem. Figure 5 illustrates such partial mazes with 16, 32, and 64 nodes.</p>
<p>Figure 5: 8 × 8 mazes with number of nodes equal to 16, 32, and 64.We define length generalization as the ability to generalize to mazes with a higher number of nodes.</p>
<p>We consider two maze tasks: (1) shortest path , where the model outputs the shortest path from start to end node, and (2) DFS trace , where the model simulates a depth-first search traversal (including backtracking).Shortest path is harder to learn perfectly, as it requires "lookahead" at branch points, while DFS trace allows exploration and backtracking.Figure 6 shows that in the multi-task setting, the addition of shortest path helps DFS trace generalize to higher lengths.The opposite is true as well: DFS trace helps shortest path generalize to higher lengths, which is shown in Figure 7.</p>
<p>Transfer with Swapped Main and Auxiliary Tasks</p>
<p>We consider another maze task group where we the main and auxiliary tasks are reversed relative to Section 4.3.In this case, the main task is shortest path , and the auxiliary task is DFS trace .As shown in Figure 7, co-training with the auxiliary task again improves length generalization performance.While shortest path is more difficult than DFS trace , the model benefits from learning a related traversal strategy.</p>
<p>Control Tasks</p>
<p>To verify that length generalization transfer does not arise from merely seeing longer inputs, we further test arithmetic tasks and string operations with control auxiliary tasks.or arithmetic, we use copy-first-op , which follows the addition format but simply copies the first operand.For string operations, we pair string copy with reverse .As expected, length generalization transfer is not observed with unrelated task (Figure 8).</p>
<p>Length Generalization Transfer from Pretraining</p>
<p>Remarkably, we find that natural language pretraining can serve as an effective form of implicit auxiliary task that enhances length generalization in synthetic tasks.tasks.SmolLM is released by Huggingface and pretrained on a diverse corpus containing natural language and programming data, which includes long-range structures and dependencies.</p>
<p>Before finetuning, we verify that the model does not already solve these tasks.For reverse add , a zero-shot evaluation using prompt-based input results in near-zero accuracy, confirming that the model has not learned this task during pretraining.For the maze task, all node tokens are newly introduced during finetuning, meaning the entire input format is unseen by the pretrained model.</p>
<p>We then finetune models from multiple publicly available checkpoints, taken throughout the pretraining process (from step 160K to 2.56M), and evaluate their length generalization performance on out-of-distribution inputs.As shown in Figure 9, we observe a clear trend: generalization to longer inputs improves steadily with pretraining progress, for both arithmetic and maze-solving tasks.This suggests that natural language pretraining instills reusable inductive biases that transfer to novel tasks-even when those tasks have little structural resemblance to natural language.We speculate the extent of generalization transfer from pretrained models may not be limited to length generalization, but could extend to other forms of out-of-distribution generalization such as compositional reasoning, distributional shifts, and task complexity.Future work could explore whether similar transfer effects exist for other generalization challenges.</p>
<p>Additionally, we confirm that length generalization transfer is not limited to small models trained from scratch, but also emerges in finetuned pretrained models.Additional results across other task groups are provided in Appendix A.2.</p>
<p>Ablations</p>
<p>In this section, we present several complementary analyses to better understand the conditions under which transfer occurs.We examine the effect of varying the length configurations of the main and auxiliary tasks and also provide an initial mechanistic explanation of the transfer phenomenon based on circuit sharing between tasks.Additional analyses, including the instability of training dynamics (Section 6.2) and the effect of positional encodings (Section 6.3) are included in the Appendix.</p>
<p>Varying Main and Auxiliary Task Lengths</p>
<p>In our previous experiments, we fixed the main task length to 16 and the auxiliary task length to 32.</p>
<p>A natural question is: does length generalization transfer persist across other main-auxiliary length configurations?To investigate this, we define the generalization gap (Figure 10), a scalar between 0 and 1 that quantifies the discrepancy in performance between the main and auxiliary tasks across a range of evaluation lengths.A smaller generalization gap indicates stronger transfer, with a value of 0 implying perfect alignment between the main and auxiliary generalization curves.</p>
<p>First we fix the task group reverse add , no carry and carry only .Then, we systematically vary the training lengths of both main and auxiliary tasks across the range {4, 8, 16, . . ., 256} and compute the average generalization gap over three random seeds.As shown in Figure 10, we find that the transfer effect is most effective when the ratio between the auxiliary and main lengths is between 0.5 and 2. The intuitive explanation is that, when the difference between task length is too high, the model will overfit to the task length difference and therefore do not exhibit length transfer.The transfer effect is strongest when the ratio between auxiliary and main lengths is between 0.5 and 2, as shown by the dark diagonal band.As shown in Figures 3, 4, and 6, not all random seeds exhibit successful length generalization transfer.In our experiments with 5 different seeds per task group, we observe considerable variability in length generalization transfer performance.The variability is entirely due to different model initializations, since we keep the dataset the same between runs.To better illustrate this instability, we visualize training dynamics in Figure 11.</p>
<p>Unstable Training Dynamics in Length Generalization Transfer</p>
<p>The plots show training curves for the reverse add main task when co-trained with no carry and carry only auxiliary tasks.During evaluation, we sweep over input lengths from 1 to 36, which is classified into three regimes:</p>
<p>• In-distribution (length 1-16): These inputs fall within the training range for the main task.</p>
<p>Accuracy in this regime improves quickly and remains stable.</p>
<p>• Expected transfer range (length 17-32): These inputs are unseen by the main task but seen by the auxiliary tasks.Performance in this range is highly variable and sensitive to training dynamics.</p>
<p>• Fully OOD (length &gt;32): These inputs are unseen by both the main and auxiliary tasks.As expected, accuracy in this regime remains low.</p>
<p>Rotary Position Encoding Encourages Length Generalization Transfer</p>
<p>In length generalization literature, NoPE (No Positional Encoding) has often been favored for its length generalization performance on various tasks.However, in practice, many modern transformer models adopt RoPE (Rotary Positional Encoding), motivated by its strong empirical performance in long-context settings [Peng et al., 2023, Ding et al., 2024] and its ability to induce meaningful position-sensitive attention patterns [Barbero et al., 2024].</p>
<p>To compare the two encoding strategies in the context of length generalization transfer, we re-run our main experiments using the same model architecture but remove the RoPE component.As shown in Figure 13, RoPE consistently outperforms NoPE in enabling generalization transfer from auxiliary to main tasks.This finding is orthogonal to the previous result [Kazemnejad et al., 2024] that NoPE is better suited for length generalization and potentially explains the superior performance of RoPE in real-world models and tasks.</p>
<p>Evidence of Circuit Sharing Between Tasks</p>
<p>In Section 4.4, we established that mere exposure to longer inputs is insufficient for length generalization transfer-structural alignment between tasks is crucial.In this section, we present additional evidence that length generalization transfer coincides with the sharing of internal mechanisms between tasks.Specifically, we study whether transformer models reuse similar attention circuits across tasks when length generalization transfer occurs.We focus on two measures of similarity between task mechanisms:</p>
<p>• Attention matrix difference: the sum of entry-wise absolute differences between the attention matrices at each head between the two tasks.</p>
<p>• Attention head mean-ablation map difference: for each attention head, we measure the drop in task accuracy after replacing its output with the mean activation across the batch.This yields a 6 × 6 matrix (corresponding to 6 layers and 6 heads of the model) per task, representing the importance of each head.We then compute the average absolute difference between the two matrices to assess divergence in head importance.</p>
<p>The methodology used in our head ablation studies is known as activation patching.Prior works such as Wang et al. [2022], Cammarata et al. [2021], Olsson et al. [2022] have developed this technique to uncover the underlying computational circuits in language models Higher values for these metrics indicate more divergent computational mechanisms between tasks.We track how these metrics evolve over training and compare with the generalization gap (defined in Figure 10).As seen in Figure 11, different checkpoints yield varying levels of generalization.We find that, in most cases, the attention similarity metrics correlate with the generalization gap.This suggests that when generalization improves, the internal attention patterns become more aligned across tasks, thus, more sharing of computation mechanisms.Complete results are shown in Appendix A.3.2.</p>
<p>Generalization Gap</p>
<p>RoPE Model NoPE Model</p>
<p>Figure 13: Comparison of generalization gap across several task groups shows that NoPE leads to significantly weaker transfer compared to RoPE.</p>
<p>Limitations</p>
<p>While our work demonstrates length generalization transfer across a range of synthetic tasks, several important limitations remain.First, our study does not provide a formal theoretical framework for understanding when and why transfer occurs.Without a principled understanding of the underlying mechanisms, predicting or optimizing transfer remains challenging.Second, our experiments are limited to relatively simple algorithmic domains with well-defined length parameters and deterministic solution paths.While this setup allows for controlled comparisons, it is unclear whether similar transfer effects would hold in settings that involve hierarchical reasoning, abstract problem-solving, or tasks requiring integration of multiple skills simultaneously.Addressing these limitations is a 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0 0.0022 0.0024 0.0026 0.0028 Attention difference 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0 0.00 0.05 0.10 0.15 0.20 Head ablation map difference 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0  promising direction for future work and could further illuminate the generalization capabilities of transformer models in more realistic settings.</p>
<p>Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou.Transformers can achieve length generalization but not robustly.arXiv preprint arXiv:2402.09371,2024.</p>
<p>A Additional Results</p>
<p>A.1 Additional Results on Arithmetic and String Tasks</p>
<p>For task groups with two auxiliary tasksreverse add with no carry and carry only , and capitalize-reverse with capitalize and reverse -we additionally evaluate the effect of training with only one of the auxiliary tasks.As shown in Figure 15, length generalization transfer performance consistently declines when only a single auxiliary task is used, compared to co-training with both.Notably, the choice of auxiliary task matters: models trained with the more relevant auxiliary ( no carry or reverse ) stronger generalization than those trained with less relevant ones ( carry only or capitalize ).These results reinforce the importance of task alignment for successful transfer.</p>
<p>A.2 Finetuning from Pretrained Models</p>
<p>We replicate our length generalization transfer experiments using a pretrained language model, SmolLM-360M, where we observe similar patterns of length generalization transfer as in the fromscratch setting.Figure 16 presents results across three arithmetic task groups and one string manipulation group.As with our earlier experiments, co-training with structurally related auxiliary tasks facilitates generalization beyond the training length.Notably, we also confirm that control task pairs-such as reverse add with copy-first-op -do not lead to successful transfer.Orthogonal to the length generalization transfer, results show that SmolLM-360M exhibits strong inherent generalization in copying tasks (16c, 16d).showing transfer from capitalize and reverse to capitalize-reverse .Overall, the transfer effect persists in the pretrained model.</p>
<p>A.3 Additional Results on Circuit Sharing</p>
<p>To supplement our findings in Section 6.4, we present additional results analyzing how shared attention mechanisms evolve across training checkpoints and correlate with length generalization performance.We break this down into two parts: (1) qualitative examples of head ablation maps, and</p>
<p>(2) full correlation results between circuit similarity and generalization gap.</p>
<p>A.3.1 Example Attention Head Ablation Map</p>
<p>We first visualize the attention-head mean-ablation maps for a pair of related tasksreverse add and reverse subtract -across four training checkpoints (Figure 17).Each 6 × 6 matrix represents the importance of each attention head: the value at position (i, j) indicates the drop in accuracy when head i in layer j is replaced with the mean activation across the batch.These matrices reflect the reliance of each task on specific attention heads.If two tasks reuse the same attention circuitry, their ablation maps should be similar.The scalar quantity used in subsequent comparisons is the average absolute difference between two such matrices.Similar ablation maps suggest that both tasks rely on overlapping computational circuits.</p>
<p>A.3.2 Full Results for Circuit Sharing</p>
<p>We now compare how two metrics (attention matrix difference and ablation map difference) track with the generalization gap across training checkpoints.</p>
<p>String Tasks.As shown in Figure 18, the attention matrix difference does not correlate well with generalization in string task groups.However, the ablation map difference shows a clearer trend: as generalization improves, ablation similarity increases.This suggests that shared head usage, rather than raw attention weights, better reflects functional similarity across tasks.</p>
<p>For maze-based tasks, we serialize graphs using an adjacency list format with unique node tokens, followed by a query specifying the start and end node.A detailed example is shown in Figure 20.From-scratch models are trained with a higher learning rate and larger batch sizes, while pretrained models (SmolLM-360M) use lower learning rates and shorter training schedules.All models are optimized using AdamW with a learning rate schedule that includes a warm-up phase, a constant phase, and a cosine decay</p>
<p>B.3.2 Computational Resources</p>
<p>For all experiments the paper, we run on a single machine with two NVIDIA GeForce RTX 3090 graphics cards.For all experiment settings, each individual training run is at most 2 hours.The total estimate of compute used, in terms of hours on the 2-GPU machine, is around 300 hours.</p>
<p>Figure 2 :
2
Figure2: Overview of the tasks used in our length generalization transfer experiments, spanning three domains: arithmetic, string manipulation, and maze solving.Each group consists of a main task trained on shorter sequences and one or more auxiliary tasks trained on longer ones.We study whether generalization to longer inputs can be transferred from the auxiliary to the main task.</p>
<p>Figure 3 :
3
Figure 3: Length generalization results for addition-related task groups.The main task is reverse add , with performance shown when trained with different auxiliary tasks.Each model is trained with 5 random seeds; best-performing runs are shown in bold.The dashed vertical line indicates the maximum training length for each task.When trained alone (d), the model fails to generalize beyond training length.Co-training with related auxiliary tasks (a-c) enables extrapolation to longer inputs.</p>
<p>Figure 4: Performance plots for string tasks.When trained alone (b, d), models fail to generalize beyond their training range.Co-training with auxiliary tasks (a, c) enables substantial length extrapolation.</p>
<p>Figure 6: Performance plots for maze tasks.Co-training DFS trace with shortest path (a) enables generalization to longer lengths compared to training on DFS trace alone (b).</p>
<p>Figure 7: Length generalization results for maze task group with reversed task roles.Cotraining shortest path with DFS trace (a) leads to improved generalization over training on shortest path alone (b).</p>
<p>Figure 8 :
8
Figure 8: Control tasks for (a) addition and (b) string operations.These unrelated task pairs fail to produce length generalization transfer, confirming that task relatedness is crucial.</p>
<p>Figure 9 :
9
Figure 9: Finetuning at different SmolLM-360M checkpoints reveals that length generalization transfer improves with more natural language pretraining.</p>
<p>Figure 10 :
10
Figure 10: (a) The generalization gap is defined as the average difference in accuracy between the main and auxiliary tasks across evaluation lengths, normalized to the range [0, 1].A lower value indicates better transfer.(b) Generalization gap across different combinations of main ( reverse add ) and auxiliary ( no carry &amp; carry only ) training lengths.The transfer effect is strongest when the ratio between auxiliary and main lengths is between 0.5 and 2, as shown by the dark diagonal band.</p>
<p>Figure 11: Training curves for the reverse add when co-trained with no carry and carry only .Accuracy in the transfer region (length 17-32) fluctuates significantly, illustrating unstable training dynamics in length generalization transfer.</p>
<p>Figure 12 :
12
Figure 12: Length generalization transfer results using NoPE model, under the same task settings.The transfer effect is notably weaker in most tasks.</p>
<p>Figure 14 :
14
Figure 14: We use the arithmetic task group reverse add &amp; reverse subtraction for the analysis.Evolution of generalization gap, attention matrix difference, and attention head meanablation map difference across checkpoints.All three metrics closely align, suggesting successful length generalization transfer accompanies shared attention mechanisms between tasks.</p>
<p>Figure15: Additional results for arithmetic and string (copy) task groups.Each row shows performance on the main task (A) when co-trained with: both auxiliary tasks (left), only one of the auxiliary task (middle &amp; right).Performance degrades when training with only one auxiliary task, especially when the auxiliary is less structurally aligned with the main task.</p>
<p>Figure 16 :
16
Figure16: Length generalization transfer with the pretrained model SmolLM-360M.(a-c): Arithmetic task groups.In (a) and (b), we observe successful transfer from auxiliary to main tasks, mirroring results from from-scratch training.In (c), no transfer occurs when using the control task copy-first-op , confirming the importance of task relevance.(d): String manipulation task, showing transfer from capitalize and reverse to capitalize-reverse .Overall, the transfer effect persists in the pretrained model.</p>
<p>Figure 17 :
17
Figure17: Mean-ablation maps for reverse add and reverse subtract across training checkpoints.Each (i, j) entry indicates the accuracy drop after mean-ablating head i in layer j.Similar ablation maps suggest that both tasks rely on overlapping computational circuits.</p>
<p>Figure 20 :
20
Figure 20: Detailed example of maze data format.Each node is a random number selected from n × n nodes in the grid.</p>
<p>[Allal et al., 2024]finetune various checkpoints of SmolLM-360M[Allal et al., 2024]on reverse add and shortest path
1.001.00Accuracy0.25 0.50 0.75Copy First Op (main) Reverse Add (aux)Accuracy0.25 0.50 0.75Copy String (main) Reverse String (aux)0.00102030 Length40500.00102030 Length4050(a) reverse add &amp; copy-first-op
(b) string copy &amp; reverse</p>
<p>Table 2 :
2
Examples of algorithmic tasks used in our experiments.
Task NameInputOutputonly carry82050465+23782955=010010111no carry82050465+23782955=057323100reverse82050465+23782955=067333211reverse subtract82050465+23782955=692674000n × 3 CoT multiply60844671*502=030422880+0000000000=03042288+00216982530=0325817163copy stringfVOBA1fR=fVOBA1fRMulti-Query Associative RecallfVOBA1fR=fVOB;OBA1;string reversefVOBA1fR=Rf1ABOVfcapitalizefVOBA1fR=Fvoba1Frcapitalize-reverse fVOBA1fR=rF1abovFShortest Path</p>
<p>Table 3 lists the hyperparameters used for training across different task domains and model types.</p>
<p>Table 3 :
3
Hyperparameters for training
TaskBatch Size LR Iterations Warmup Iter Decay IterArithmetic Tasks String Tasks1024 10241e-3 1e-320000 50002000 5005000 1000Maze Tasks Arithmetic Tasks (SmolLM) String Tasks (SmolLM)256 128 1281e-3 5e-5 5e-520000 2500 10002000 250 1005000 500 500Maze Tasks (SmolLM)2565e-52500250500
5 0 0 1 0 0 0 1 5 0 0 2 0 0 0 2 5 0 0 3 0 0 0 3 5 0 0 4 0 0 0 4 5 0 0 5 0 0 0 0.012 0.014 0.016 0.018 Attention difference 5 0 0 1 0 0 0 1 5 0 0 2 0 0 0 2 5 0 0 3 0 0 0 3 5 0 0 4 0 0 0 4 5 0 0 5 0 0 0 0.000 0.025 0.050 0.075 Head ablation map difference 5 0 0 1 0 0 0 1 5 0 0 2 0 0 0 2 5 0 0 3 0 0 0 3 5 0 0 4 0 0 0 4 5 0 0 5 0 0 0 Arithmetic Tasks.In contrast, for arithmetic task pairs, both metrics strongly correlate with the generalization gap (Figure19).This suggests that arithmetic tasks not only share similar head usage but also similar attention patterns at the matrix level.Control Tasks.As a sanity check, we analyze a task pair with no expected transfer ( reverse add and copy-first-op ).As expected, neither metric correlates with generalization performance, reinforcing that our observed patterns are not incidental.2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0 0.010 0.011 Attention difference 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0 0.02 0.04 0.06 Head ablation map difference 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0 Checkpoint 0.00 0.25 0.50 0.75 1.00 Generalization Gap (a) A: reverse add , B: copy-first-op 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0 0.0022 0.0024 0.0026 0.0028 Attention difference 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0 0.00 0.05 0.10 0.15 0.20 Head ablation map difference 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0 Checkpoint 0.00 0.25 0.50 0.75 1.00 Generalization Gap (b) A: reverse add , B: no carry 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0 0.0035 0.0040 0.0045 Attention difference 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0 0.00 0.05 0.10 0.15 0.20 Head ablation map difference 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0B Experiment DetailsB.2 Data Formats and Data SamplingWe provide examples of each task in Table2.For all arithmetic tasks, both the inputs and outputs are written in reverse digit order.For the n × 3 CoT multiply task, the output includes intermediate steps where the first operand is multiplied by each digit of the second operand.
Task generalization with autoregressive compositional structure: Can learning from d tasks generalize to d t tasks. Huaqing Zhang, Kaiyue Wen, Hongzhou Lin, Jingzhao Zhang, Mikhail Belkin, arXiv:2502.08991adjacency list Out: All paths in DFS References Amirhesam Abedsoltan. 2025arXiv preprint</p>
<p>On provable length and compositional generalization. Kartik Ahuja, Amin Mansouri, 2024</p>
<p>Llama 3 model card. A I , Meta , 2024</p>
<p>Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Leandro Von Werra, Thomas Wolf, Smollmblazingly fast and remarkably powerful. 2024</p>
<p>Exploring length generalization in large language models. Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, Behnam Neyshabur, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Zoology: Measuring and improving recall in efficient language models. Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, Christopher Ré, 2023</p>
<p>Improving length-generalization in transformers via task hinting. Pranjal Awasthi, Anupam Gupta, 2023</p>
<p>Christos Perivolaropoulos, Razvan Pascanu, and Petar Velivckovi'c. Round and round we go! what makes rotary positional encodings useful?. Federico Barbero, Alex Vitvitskyi, ArXiv, abs/2410.062052024</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, 2020</p>
<p>Curve circuits. Distill. Nick Cammarata, Gabriel Goh, Shan Carter, Chelsea Voss, Ludwig Schubert, Chris Olah, 10.23915/distill.00024.0062021</p>
<p>Position coupling: Improving length generalization of arithmetic transformers using task structure. Hanseul Cho, Jaeyoung Cha, Pranjal Awasthi, Srinadh Bhojanapalli, Anupam Gupta, Chulhee Yun, 2024</p>
<p>Longrope: Extending llm context window beyond 2 million tokens. Yiran Ding, L Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, Mao Yang, ArXiv, abs/2402.137532024</p>
<p>From interpolation to extrapolation: Complete length generalization for arithmetic transformers. Shaoxiong Duan, Yining Shi, Wei Xu, arXiv:2310.119842023arXiv preprint</p>
<p>Location attention for extrapolation to longer sequences. Yann Dubois, Gautier Dagan, Dieuwke Hupkes, Elia Bruni, arXiv:1911.038722019arXiv preprint</p>
<p>Looped transformers for length generalization. Ying Fan, Yilun Du, Kannan Ramchandran, Kangwook Lee, arXiv:2409.156472024arXiv preprint</p>
<p>Not all llm reasoners are created equal. Arian Hosseini, Alessandro Sordoni, Daniel Toyama, Aaron Courville, Rishabh Agarwal, arXiv:2410.017482024arXiv preprint</p>
<p>Compositionality decomposed: How do neural networks generalise. Dieuwke Hupkes, Verna Dankers, Mathijs Mul, Elia Bruni, Journal of Artificial Intelligence Research. 672020</p>
<p>The impact of positional encoding on length generalization in transformers. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, Siva Reddy, Advances in Neural Information Processing Systems. 202436</p>
<p>Teaching arithmetic to small transformers. Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, Dimitris Papailiopoulos, arXiv:2307.033812023arXiv preprint</p>
<p>Selfimproving transformers overcome easy-to-hard and length generalization challenges. Nayoung Lee, Ziyang Cai, Avi Schwarzschild, Kangwook Lee, Dimitris Papailiopoulos, 2025</p>
<p>On the power of convolutionaugmented transformer. Mingchen Li, Xuechen Zhang, Yixiao Huang, Samet Oymak, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Functional interpolation for relative positions improves long context transformers. Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, Srinadh Bhojanapalli, arXiv:2310.044182023arXiv preprint</p>
<p>Transformers can do arithmetic with the right embeddings. Sean Mcleish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Bhavya Brian R Bartoldson, Abhinav Kailkhura, Jonas Bhatele, Avi Geiping, Schwarzschild, arXiv:2405.173992024arXiv preprint</p>
<p>The eos decision and length extrapolation. Benjamin Newman, John Hewitt, Percy Liang, Christopher D Manning, arXiv:2010.071742020arXiv preprint</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, -context learning and induction heads. Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam Mccandlish, Chris Olah, 2022</p>
<p>OpenAI. Gpt-4 technical report. 2024</p>
<p>Yarn: Efficient context window extension of large language models. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, Enrico Shippole, ArXiv, abs/2309.000712023</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. Ofir Press, Noah A Smith, Mike Lewis, arXiv:2108.124092021arXiv preprint</p>
<p>Understanding addition in transformers. Philip Quirke, Fazl Barez, arXiv:2310.131212023arXiv preprint</p>
<p>Arithmetic in transformers explained. Philip Quirke, Clement Neo, Fazl Barez, 2025</p>
<p>Compositional capabilities of autoregressive transformers: A study on synthetic, interpretable tasks. Rahul Ramesh, Ekdeep Singh Lubana, Mikail Khona, Robert P Dick, Hidenori Tanaka, Forty-first International Conference on Machine Learning. </p>
<p>Randomized positional encodings boost length generalization of transformers. Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane Legg, Joel Veness, arXiv:2305.168432023arXiv preprint</p>
<p>Explicitly encoding structural symmetry is key to length generalization in arithmetic tasks. Mahdi Sabbaghi, George Pappas, Hamed Hassani, Surbhi Goel, arXiv:2406.018952024arXiv preprint</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Trishala Wang, Jos Neeraj, Abheesht Rozen, Andrea Sharma, Thibault Santilli, Jason Fevry, Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M Rush, International Conference on Learning Representations. 2022</p>
<p>Positional description matters for transformers arithmetic. Ruoqi Shen, Sébastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, Yi Zhang, arXiv:2311.147372023arXiv preprint</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu, 2023</p>
<p>Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, 2022</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, ArXiv, abs/2206.07682Emergent abilities of large language models. 2022</p>
<p>Generating random spanning trees more quickly than the cover time. David Bruce, Wilson , 10.1145/237814.237880Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing, STOC '96. the Twenty-Eighth Annual ACM Symposium on Theory of Computing, STOC '96New York, NY, USAAssociation for Computing Machinery1996</p>
<p>Do large language models have compositional ability? an investigation into limitations and scalability. Zhuoyan Xu, Zhenmei Shi, Yingyu Liang, 2024</p>
<p>Exploring compositional generalization of large language models. Haoran Yang, Hongyuan Lu, Wai Lam, Deng Cai, 10.18653/v1/2024.naacl-srw.3Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Yang , Trista ) Cao, Isabel Papadimitriou, Anaelia Ovalle, Marcos Zampieri, Francis Ferraro, Swabha Swayamdipta, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 20244Student Research Workshop)</p>
<p>From local structures to size generalization in graph neural networks. Gilad Yehudai, Ethan Fetaya, Eli Meirom, Gal Chechik, Haggai Maron, International Conference on Machine Learning. PMLR2021</p>
<p>Skill-mix: a flexible and expandable family of evaluations for ai models. Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, Sanjeev Arora, 2023</p>
<p>On the out-of-distribution generalization of multimodal large language models. Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, Peng Cui, 2024</p>
<p>Can models learn skill composition from examples?. Haoyu Zhao, Simran Kaur, Dingli Yu, Anirudh Goyal, Sanjeev Arora, 2025</p>
<p>What algorithms can transformers learn? a study in length generalization. Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, Preetum Nakkiran, arXiv:2310.1602810]:[0][92023arXiv preprint13], [2]:[4], [1]:[11], [7]:[5], [13]:[8][10], [5]:[11][7][14][15], [12]:[8][6], [9]:[10][14], [8]:[12][13], [6]:[12] ?[12]&gt;[2]? [12][8][13] [10][9][14] [5][15][4][2</p>            </div>
        </div>

    </div>
</body>
</html>
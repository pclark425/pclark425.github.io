<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-563 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-563</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-563</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-a2d7201d1077bbb02ca53da4018e55d19226d8ea</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a2d7201d1077bbb02ca53da4018e55d19226d8ea" target="_blank">MotionTransformer: Transferring Neural Inertial Tracking between Domains</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This work proposes MotionTransformer - a novel framework that extracts domain-invariant features of raw sequences from arbitrary domains, and transforms to new domains without any paired data to overcome the challenges of domain adaptation on long sensory sequences.</p>
                <p><strong>Paper Abstract:</strong> Inertial information processing plays a pivotal role in egomotion awareness for mobile agents, as inertial measurements are entirely egocentric and not environment dependent. However, they are affected greatly by changes in sensor placement/orientation or motion dynamics, and it is infeasible to collect labelled data from every domain. To overcome the challenges of domain adaptation on long sensory sequences, we propose MotionTransformer - a novel framework that extracts domain-invariant features of raw sequences from arbitrary domains, and transforms to new domains without any paired data. Through the experiments, we demonstrate that it is able to efficiently and effectively convert the raw sequence from a new unlabelled target domain into an accurate inertial trajectory, benefiting from the motion knowledge transferred from the labelled source domain. We also conduct real-world experiments to show our framework can reconstruct physically meaningful trajectories from raw IMU measurements obtained with a standard mobile phone in various attachments.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e563.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e563.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MotionTransformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MotionTransformer (Generative Adversarial Sequence Domain Transformation for IMU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that adapts GAN-based domain adaptation to continuous inertial time-series by learning a shared encoder to extract domain-invariant sequence representations and RNN generators/decoders to synthesize target-domain IMU streams and predict odometry polar vectors without paired labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>GAN-based sequence domain adaptation for inertial tracking</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>A computational method that transfers knowledge from a labeled source motion domain to an unlabeled target motion domain by (1) encoding raw IMU subsequences into a shared, domain-invariant latent representation z using an RNN encoder conditioned on a domain vector θ; (2) generating synthetic IMU sequences in either domain from z using RNN generators (G_{S→T}, G_{T→S}) trained adversarially against domain discriminators; (3) reconstructing sequences with an RNN decoder from noisy z to regularize representation learning; and (4) predicting polar-odometry labels (Δl, Δψ) from z with a shared predictor. The training objective combines GAN adversarial loss, MSE reconstruction loss, supervised prediction loss on source labels, cycle-consistency (L1) and perceptual-consistency (L2 on encoder outputs) losses to enforce content preservation across domain transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data analysis technique (unsupervised domain adaptation for sequential data)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>Generative adversarial domain adaptation techniques (image-to-image / sequence generation literature; GANs and CycleGAN/SeqGAN traditions)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>Inertial sensing / inertial odometry (continuous IMU time-series for smartphone attachments)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Multiple architecture and objective modifications were made to apply GAN-domain-adaptation ideas to continuous IMU sequences: convolutional generators/discriminators common in images were replaced with RNN/LSTM-based generators and discriminators to model temporal continuity; the generator is decomposed into an encoder that extracts domain-invariant representations and a generator conditioned on that representation (rather than direct autoregressive conditional decoding from entire source sequence); the decoder uses additive Gaussian noise on latent z (instead of full VAE/DAE) to regularize reconstruction; cycle-consistency and a novel perceptual-consistency loss (f_enc invariance) are applied at the latent level; a shared predictor is trained to produce polar vector labels from z using supervised source labels and from generated target data; domain vectors θ are concatenated to inputs at each timestep to isolate domain-specific features; hyperparameters (λ1..λ4) balance losses.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - quantitatively effective: MotionTransformer substantially reduced label prediction MSE compared to source-only transfer (Hand→Pocket MSE 0.119 vs source-only 0.718; Hand→Bag 0.123 vs 1.129; Hand→Trolley 0.098 vs 0.273). Performance approaches supervised target-only training (e.g. supervised target MSEs 0.045, 0.010, 0.006 respectively) and produced physically meaningful inertial trajectories in unlabelled target domains as evaluated against high-precision Vicon ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Challenges included: IMU data are continuous, real-valued and temporally correlated (not discrete tokens like NLP nor spatial patterns like images); large domain shifts caused by different device placements/orientations and projection of gravity; absence of paired/timesynchronized sequences between source and target; high measurement noise and drift characteristics of low-cost MEMS IMUs; necessity to preserve time alignment of generated sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Facilitators were: the assumption that underlying odometry statistics (agent motion) are similar across attachments; availability of labeled source-domain sequences (from Vicon) to supervise prediction; RNN/LSTM architectures appropriate for sequences; cycle- and perceptual-consistency losses that enforce content preservation; integration of a physical understanding (polar vector target) which structures the prediction task.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Required components/conditions included: labeled source-domain IMU sequences with Vicon ground truth for supervision; high-frequency IMU sampling (100 Hz) segmented into fixed-length windows (200 frames / 2 s); RNN/LSTM-based generator/discriminator/encoder/decoder/predictor implementations; sufficient compute for adversarial training and hyperparameter tuning; and choice of loss-weight hyperparameters (λ1..λ4).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Authors state the framework is likely suitable for any continuous, sequential domain transformation task that has an underlying physical model; empirically demonstrated across multiple phone-attachment domains (hand, pocket, bag, trolley), suggesting good generalizability across similar inertial-domain shifts, but applicability to very different modalities would require appropriate sequence modeling adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and technical/algorithmic knowledge (computational method + some theoretical principles about domain-invariant representations)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MotionTransformer: Transferring Neural Inertial Tracking between Domains', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e563.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e563.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADDA (LSTM-adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial Discriminative Domain Adaptation (adapted with LSTM layers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discriminative adversarial domain adaptation method originally for image features that was adapted in this paper by replacing convolutional components with LSTM layers to operate on continuous IMU sequences as a comparative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adversarial Discriminative Domain Adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>ADDA adapted to sequential IMU data</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>A discriminative adversarial domain adaptation technique that trains an encoder for target-domain inputs to match the distribution of source-domain features by adversarially training a discriminator to distinguish source vs target features; in this paper the original convolutional encoder/discriminator architecture was replaced by LSTM-based networks to process time-series IMU data and the fused features were used to predict odometry labels.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / domain adaptation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>Image-based adversarial domain adaptation (computer vision)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>Inertial sensing / IMU sequence domain</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Replaced convolutional generator/discriminator blocks (common in image ADDA implementations) with LSTM layers to model temporal dependence in continuous IMU sequences; otherwise followed the ADDA adversarial alignment principle. The reconstruction and perceptual losses used in MotionTransformer were reduced/cut down when using this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful - improved over source-only but worse than MotionTransformer: quantitative MSEs reported (Hand→Pocket 0.471, Hand→Bag 0.631, Hand→Trolley 0.216) which are lower than training-on-source-only but higher than MotionTransformer and CyCADA-with-LSTM.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>ADDA's discriminative alignment focuses on feature confusion and may not preserve sequence content needed for odometry labels; original design for spatial image features required adaptation to temporal sequences and did not fully close domain gaps for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of LSTM replacements that can model temporal dependencies; the underlying adversarial alignment principle is transferable.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Need to implement LSTM-based encoder and discriminator, and have source labels for supervised prediction in source domain; reduced reconstruction/perceptual regularization compared to generative approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Adaptation of discriminative adversarial alignment is generalizable to other temporal sequence tasks, but may require stronger content-preserving constraints for tasks where labels depend on fine-grained temporal structure.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and algorithmic knowledge (adaptation of adversarial alignment to sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MotionTransformer: Transferring Neural Inertial Tracking between Domains', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e563.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e563.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CyCADA (LSTM-adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CyCADA / Cycle-GAN style adversarial domain adaptation (adapted with LSTM layers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cycle-consistent adversarial domain adaptation / Cycle-GAN approach, adapted here by replacing convolutional networks with LSTM layers to generate synthetic IMU target sequences from source sequences and vice versa as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CyCADA: Cycle-Consistent Adversarial Domain Adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Cycle-consistent adversarial sequence generation for IMU</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>A generative adversarial domain adaptation method that trains bidirectional generators to translate data between source and target domains while enforcing cycle-consistency (reconstruction when mapping A→B→A) so that content is preserved; adapted here with LSTM-based generators and discriminators to synthesize continuous IMU sequences and used to fine-tune the predictor on synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / generative domain adaptation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>Image-to-image translation / Cycle-GAN literature (computer vision)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>Inertial sensing / IMU sequence translation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Convolutional Cycle-GAN architectures replaced by LSTM-based generator and discriminator networks to handle continuous, temporally correlated IMU signals. Reconstruction and perceptual loss terms were scaled down relative to the full MotionTransformer experiments for baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful - better than ADDA and source-only but worse than MotionTransformer: reported MSEs (Hand→Pocket 0.237, Hand→Bag 0.455, Hand→Trolley 0.182).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Original Cycle-GAN designs assume spatial image structure and discrete pixels; applying cycle-consistency to long continuous sequences requires careful temporal modeling and additional latent/content-preserving constraints to ensure label-relevant information is maintained.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Cycle-consistency provides strong content preservation which helps when adapting to sequences; straightforward substitution with LSTMs provided temporal modeling capability.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>LSTM-based generators/discriminators, sufficient training data for adversarial mapping, and capacity to enforce cycle and perceptual losses to retain semantic content relevant for odometry.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Cycle-consistent adversarial translation is broadly applicable to sequence-to-sequence domain shifts if architectures and losses are adapted appropriately to preserve temporal semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and algorithmic knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MotionTransformer: Transferring Neural Inertial Tracking between Domains', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e563.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e563.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicon labelling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicon Motion Capture labeling of IMU sequences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of a Vicon optical motion-capture system to record high-precision ground-truth full-pose (location and orientation) that is synchronized with smartphone IMU sequences to produce labeled training data for supervised components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ViconMotion Capture Systems: Viconn.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Optical motion capture ground-truth labeling for IMU data</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>An experimental measurement procedure where a participant carries the IMU-equipped device with Vicon markers while walking; the Vicon system records high-precision 6-DoF poses (≈0.01 m position, 0.1° orientation) at high frequency which are segmented and aligned to 100 Hz IMU readings to generate subsequence labels (location and heading displacements) used to supervise model training.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>measurement technique / experimental protocol</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>Motion-capture / biomechanics / robotics experiment measurement</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>Supervised training for inertial odometry models (machine learning on IMU data)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application (measurement technique applied to enable labeling in a different experimental setup)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Standard Vicon capture was used to track the device carrying participants; data were segmented into 2s windows (200 frames) and converted into polar-vector labels (Δl, Δψ) aligned with IMU subsequences; no novel modification to Vicon itself reported, but integration pipeline produced labelled subsequences suitable for ML training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - provided high-quality labels enabling supervised source training and quantitative evaluation; however authors note it is infeasible to collect such labeled data for every domain/attachment, motivating unsupervised transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Requires specialized equipment, marker placement and controlled capture environment; impractical to scale to every phone placement/orientation in real-world usage.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Vicon's high accuracy and full-pose outputs provide reliable ground truth for both position and orientation, enabling precise construction of polar-vector labels for training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to a motion-capture facility, instrumentation of experimental device with markers, controlled capture environment and synchronization between IMU and Vicon timestamps.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High-quality motion-capture labeling generalizes to other inertial-sensor ML tasks but is limited by practical costs and feasibility for large-scale or in-the-wild data collection.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical skills and explicit procedural steps (experimental measurement protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MotionTransformer: Transferring Neural Inertial Tracking between Domains', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e563.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e563.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid physics + RNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid physical inertial model combined with RNN-based polar-vector prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid methodological approach that combines Newtonian inertial integration principles (orientation integration and double integration of transformed accelerations) with RNN-based supervised prediction of polar odometry vectors for stable trajectory reconstruction from noisy IMU data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Physics-informed segmentation and RNN prediction for inertial odometry</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Instead of performing unstable continuous strapdown integration on noisy MEMS IMU data, the sequence is segmented into fixed-length windows; an RNN encoder/predictor is trained to map each subsequence of accelerations and angular rates to a polar vector label (Δl, Δψ) representing displacement magnitude and heading change; these window-level outputs are chained via dead reckoning equations to reconstruct trajectories. This leverages physical insight (polar parametrization and navigation-frame composition) to structure the supervised learning target and reduce drift amplification.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method combined with measurement/physical model (hybrid modelling)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>Classical inertial navigation / physics (SINS, Newtonian mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>Machine-learning-based inertial odometry for consumer IMUs</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>hybrid approach combining with existing methods</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Rather than integrating raw IMU signals continuously (which is unstable for low-cost MEMS), the approach segments sequences into windows and trains RNNs to directly predict window-level polar odometry outputs; these predictions are then composed according to the physical dead-reckoning equations to form trajectories. This breaks the continuous integration and reduces error accumulation; the physical model defines the output space (Δl, Δψ) used for supervised learning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - this hybrid approach underlies IONet and is effective: it produces accurate trajectories when trained and when domain adaptation is available; it enabled the MotionTransformer to reconstruct physically meaningful paths in unlabelled domains when prediction quality was sufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Underlying physical integration still vulnerable to drift if predictions are inaccurate; requires labeled windows for supervised training (hence need for domain adaptation when labels absent); small prediction errors still accumulate over time.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Combining physical structure (polar outputs and dead-reckoning composition) with data-driven RNN predictions reduces sensitivity to MEMS noise and yields interpretable outputs that are straightforward to chain into trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Choice of subsequence length (here 2 s / 200 frames), synchronized high-frequency IMU and ground truth for supervised training, and appropriate RNN architectures for temporal modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Generalizable to other inertial odometry problems and other sensors where physical composition laws can be combined with learned local estimators; authors note earlier IONet work and current transfer experiments validate this across attachments.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles combined with explicit procedural steps (physics-informed ML, algorithmic knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MotionTransformer: Transferring Neural Inertial Tracking between Domains', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CyCADA: Cycle-Consistent Adversarial Domain Adaptation <em>(Rating: 2)</em></li>
                <li>Adversarial Discriminative Domain Adaptation <em>(Rating: 2)</em></li>
                <li>SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient <em>(Rating: 2)</em></li>
                <li>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks <em>(Rating: 1)</em></li>
                <li>IONet: Learning to Cure the Curse of Drift in Inertial Odometry <em>(Rating: 2)</em></li>
                <li>OxIOD: The Dataset for Deep Inertial Odometry <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-563",
    "paper_id": "paper-a2d7201d1077bbb02ca53da4018e55d19226d8ea",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "MotionTransformer",
            "name_full": "MotionTransformer (Generative Adversarial Sequence Domain Transformation for IMU)",
            "brief_description": "A framework that adapts GAN-based domain adaptation to continuous inertial time-series by learning a shared encoder to extract domain-invariant sequence representations and RNN generators/decoders to synthesize target-domain IMU streams and predict odometry polar vectors without paired labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "GAN-based sequence domain adaptation for inertial tracking",
            "procedure_description": "A computational method that transfers knowledge from a labeled source motion domain to an unlabeled target motion domain by (1) encoding raw IMU subsequences into a shared, domain-invariant latent representation z using an RNN encoder conditioned on a domain vector θ; (2) generating synthetic IMU sequences in either domain from z using RNN generators (G_{S→T}, G_{T→S}) trained adversarially against domain discriminators; (3) reconstructing sequences with an RNN decoder from noisy z to regularize representation learning; and (4) predicting polar-odometry labels (Δl, Δψ) from z with a shared predictor. The training objective combines GAN adversarial loss, MSE reconstruction loss, supervised prediction loss on source labels, cycle-consistency (L1) and perceptual-consistency (L2 on encoder outputs) losses to enforce content preservation across domain transformations.",
            "procedure_type": "computational method / data analysis technique (unsupervised domain adaptation for sequential data)",
            "source_domain": "Generative adversarial domain adaptation techniques (image-to-image / sequence generation literature; GANs and CycleGAN/SeqGAN traditions)",
            "target_domain": "Inertial sensing / inertial odometry (continuous IMU time-series for smartphone attachments)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Multiple architecture and objective modifications were made to apply GAN-domain-adaptation ideas to continuous IMU sequences: convolutional generators/discriminators common in images were replaced with RNN/LSTM-based generators and discriminators to model temporal continuity; the generator is decomposed into an encoder that extracts domain-invariant representations and a generator conditioned on that representation (rather than direct autoregressive conditional decoding from entire source sequence); the decoder uses additive Gaussian noise on latent z (instead of full VAE/DAE) to regularize reconstruction; cycle-consistency and a novel perceptual-consistency loss (f_enc invariance) are applied at the latent level; a shared predictor is trained to produce polar vector labels from z using supervised source labels and from generated target data; domain vectors θ are concatenated to inputs at each timestep to isolate domain-specific features; hyperparameters (λ1..λ4) balance losses.",
            "transfer_success": "successful - quantitatively effective: MotionTransformer substantially reduced label prediction MSE compared to source-only transfer (Hand→Pocket MSE 0.119 vs source-only 0.718; Hand→Bag 0.123 vs 1.129; Hand→Trolley 0.098 vs 0.273). Performance approaches supervised target-only training (e.g. supervised target MSEs 0.045, 0.010, 0.006 respectively) and produced physically meaningful inertial trajectories in unlabelled target domains as evaluated against high-precision Vicon ground truth.",
            "barriers_encountered": "Challenges included: IMU data are continuous, real-valued and temporally correlated (not discrete tokens like NLP nor spatial patterns like images); large domain shifts caused by different device placements/orientations and projection of gravity; absence of paired/timesynchronized sequences between source and target; high measurement noise and drift characteristics of low-cost MEMS IMUs; necessity to preserve time alignment of generated sequences.",
            "facilitating_factors": "Facilitators were: the assumption that underlying odometry statistics (agent motion) are similar across attachments; availability of labeled source-domain sequences (from Vicon) to supervise prediction; RNN/LSTM architectures appropriate for sequences; cycle- and perceptual-consistency losses that enforce content preservation; integration of a physical understanding (polar vector target) which structures the prediction task.",
            "contextual_requirements": "Required components/conditions included: labeled source-domain IMU sequences with Vicon ground truth for supervision; high-frequency IMU sampling (100 Hz) segmented into fixed-length windows (200 frames / 2 s); RNN/LSTM-based generator/discriminator/encoder/decoder/predictor implementations; sufficient compute for adversarial training and hyperparameter tuning; and choice of loss-weight hyperparameters (λ1..λ4).",
            "generalizability": "Authors state the framework is likely suitable for any continuous, sequential domain transformation task that has an underlying physical model; empirically demonstrated across multiple phone-attachment domains (hand, pocket, bag, trolley), suggesting good generalizability across similar inertial-domain shifts, but applicability to very different modalities would require appropriate sequence modeling adaptations.",
            "knowledge_type": "explicit procedural steps and technical/algorithmic knowledge (computational method + some theoretical principles about domain-invariant representations)",
            "uuid": "e563.0",
            "source_info": {
                "paper_title": "MotionTransformer: Transferring Neural Inertial Tracking between Domains",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "ADDA (LSTM-adapted)",
            "name_full": "Adversarial Discriminative Domain Adaptation (adapted with LSTM layers)",
            "brief_description": "A discriminative adversarial domain adaptation method originally for image features that was adapted in this paper by replacing convolutional components with LSTM layers to operate on continuous IMU sequences as a comparative baseline.",
            "citation_title": "Adversarial Discriminative Domain Adaptation",
            "mention_or_use": "use",
            "procedure_name": "ADDA adapted to sequential IMU data",
            "procedure_description": "A discriminative adversarial domain adaptation technique that trains an encoder for target-domain inputs to match the distribution of source-domain features by adversarially training a discriminator to distinguish source vs target features; in this paper the original convolutional encoder/discriminator architecture was replaced by LSTM-based networks to process time-series IMU data and the fused features were used to predict odometry labels.",
            "procedure_type": "computational method / domain adaptation baseline",
            "source_domain": "Image-based adversarial domain adaptation (computer vision)",
            "target_domain": "Inertial sensing / IMU sequence domain",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Replaced convolutional generator/discriminator blocks (common in image ADDA implementations) with LSTM layers to model temporal dependence in continuous IMU sequences; otherwise followed the ADDA adversarial alignment principle. The reconstruction and perceptual losses used in MotionTransformer were reduced/cut down when using this baseline.",
            "transfer_success": "partially successful - improved over source-only but worse than MotionTransformer: quantitative MSEs reported (Hand→Pocket 0.471, Hand→Bag 0.631, Hand→Trolley 0.216) which are lower than training-on-source-only but higher than MotionTransformer and CyCADA-with-LSTM.",
            "barriers_encountered": "ADDA's discriminative alignment focuses on feature confusion and may not preserve sequence content needed for odometry labels; original design for spatial image features required adaptation to temporal sequences and did not fully close domain gaps for this task.",
            "facilitating_factors": "Availability of LSTM replacements that can model temporal dependencies; the underlying adversarial alignment principle is transferable.",
            "contextual_requirements": "Need to implement LSTM-based encoder and discriminator, and have source labels for supervised prediction in source domain; reduced reconstruction/perceptual regularization compared to generative approaches.",
            "generalizability": "Adaptation of discriminative adversarial alignment is generalizable to other temporal sequence tasks, but may require stronger content-preserving constraints for tasks where labels depend on fine-grained temporal structure.",
            "knowledge_type": "explicit procedural steps and algorithmic knowledge (adaptation of adversarial alignment to sequences)",
            "uuid": "e563.1",
            "source_info": {
                "paper_title": "MotionTransformer: Transferring Neural Inertial Tracking between Domains",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "CyCADA (LSTM-adapted)",
            "name_full": "CyCADA / Cycle-GAN style adversarial domain adaptation (adapted with LSTM layers)",
            "brief_description": "A cycle-consistent adversarial domain adaptation / Cycle-GAN approach, adapted here by replacing convolutional networks with LSTM layers to generate synthetic IMU target sequences from source sequences and vice versa as a baseline.",
            "citation_title": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation",
            "mention_or_use": "use",
            "procedure_name": "Cycle-consistent adversarial sequence generation for IMU",
            "procedure_description": "A generative adversarial domain adaptation method that trains bidirectional generators to translate data between source and target domains while enforcing cycle-consistency (reconstruction when mapping A→B→A) so that content is preserved; adapted here with LSTM-based generators and discriminators to synthesize continuous IMU sequences and used to fine-tune the predictor on synthetic data.",
            "procedure_type": "computational method / generative domain adaptation baseline",
            "source_domain": "Image-to-image translation / Cycle-GAN literature (computer vision)",
            "target_domain": "Inertial sensing / IMU sequence translation",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Convolutional Cycle-GAN architectures replaced by LSTM-based generator and discriminator networks to handle continuous, temporally correlated IMU signals. Reconstruction and perceptual loss terms were scaled down relative to the full MotionTransformer experiments for baseline comparison.",
            "transfer_success": "partially successful - better than ADDA and source-only but worse than MotionTransformer: reported MSEs (Hand→Pocket 0.237, Hand→Bag 0.455, Hand→Trolley 0.182).",
            "barriers_encountered": "Original Cycle-GAN designs assume spatial image structure and discrete pixels; applying cycle-consistency to long continuous sequences requires careful temporal modeling and additional latent/content-preserving constraints to ensure label-relevant information is maintained.",
            "facilitating_factors": "Cycle-consistency provides strong content preservation which helps when adapting to sequences; straightforward substitution with LSTMs provided temporal modeling capability.",
            "contextual_requirements": "LSTM-based generators/discriminators, sufficient training data for adversarial mapping, and capacity to enforce cycle and perceptual losses to retain semantic content relevant for odometry.",
            "generalizability": "Cycle-consistent adversarial translation is broadly applicable to sequence-to-sequence domain shifts if architectures and losses are adapted appropriately to preserve temporal semantics.",
            "knowledge_type": "explicit procedural steps and algorithmic knowledge",
            "uuid": "e563.2",
            "source_info": {
                "paper_title": "MotionTransformer: Transferring Neural Inertial Tracking between Domains",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "Vicon labelling",
            "name_full": "Vicon Motion Capture labeling of IMU sequences",
            "brief_description": "Use of a Vicon optical motion-capture system to record high-precision ground-truth full-pose (location and orientation) that is synchronized with smartphone IMU sequences to produce labeled training data for supervised components.",
            "citation_title": "ViconMotion Capture Systems: Viconn.",
            "mention_or_use": "use",
            "procedure_name": "Optical motion capture ground-truth labeling for IMU data",
            "procedure_description": "An experimental measurement procedure where a participant carries the IMU-equipped device with Vicon markers while walking; the Vicon system records high-precision 6-DoF poses (≈0.01 m position, 0.1° orientation) at high frequency which are segmented and aligned to 100 Hz IMU readings to generate subsequence labels (location and heading displacements) used to supervise model training.",
            "procedure_type": "measurement technique / experimental protocol",
            "source_domain": "Motion-capture / biomechanics / robotics experiment measurement",
            "target_domain": "Supervised training for inertial odometry models (machine learning on IMU data)",
            "transfer_type": "direct application (measurement technique applied to enable labeling in a different experimental setup)",
            "modifications_made": "Standard Vicon capture was used to track the device carrying participants; data were segmented into 2s windows (200 frames) and converted into polar-vector labels (Δl, Δψ) aligned with IMU subsequences; no novel modification to Vicon itself reported, but integration pipeline produced labelled subsequences suitable for ML training.",
            "transfer_success": "successful - provided high-quality labels enabling supervised source training and quantitative evaluation; however authors note it is infeasible to collect such labeled data for every domain/attachment, motivating unsupervised transfer.",
            "barriers_encountered": "Requires specialized equipment, marker placement and controlled capture environment; impractical to scale to every phone placement/orientation in real-world usage.",
            "facilitating_factors": "Vicon's high accuracy and full-pose outputs provide reliable ground truth for both position and orientation, enabling precise construction of polar-vector labels for training and evaluation.",
            "contextual_requirements": "Access to a motion-capture facility, instrumentation of experimental device with markers, controlled capture environment and synchronization between IMU and Vicon timestamps.",
            "generalizability": "High-quality motion-capture labeling generalizes to other inertial-sensor ML tasks but is limited by practical costs and feasibility for large-scale or in-the-wild data collection.",
            "knowledge_type": "instrumental/technical skills and explicit procedural steps (experimental measurement protocol)",
            "uuid": "e563.3",
            "source_info": {
                "paper_title": "MotionTransformer: Transferring Neural Inertial Tracking between Domains",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "Hybrid physics + RNN",
            "name_full": "Hybrid physical inertial model combined with RNN-based polar-vector prediction",
            "brief_description": "A hybrid methodological approach that combines Newtonian inertial integration principles (orientation integration and double integration of transformed accelerations) with RNN-based supervised prediction of polar odometry vectors for stable trajectory reconstruction from noisy IMU data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Physics-informed segmentation and RNN prediction for inertial odometry",
            "procedure_description": "Instead of performing unstable continuous strapdown integration on noisy MEMS IMU data, the sequence is segmented into fixed-length windows; an RNN encoder/predictor is trained to map each subsequence of accelerations and angular rates to a polar vector label (Δl, Δψ) representing displacement magnitude and heading change; these window-level outputs are chained via dead reckoning equations to reconstruct trajectories. This leverages physical insight (polar parametrization and navigation-frame composition) to structure the supervised learning target and reduce drift amplification.",
            "procedure_type": "computational method combined with measurement/physical model (hybrid modelling)",
            "source_domain": "Classical inertial navigation / physics (SINS, Newtonian mechanics)",
            "target_domain": "Machine-learning-based inertial odometry for consumer IMUs",
            "transfer_type": "hybrid approach combining with existing methods",
            "modifications_made": "Rather than integrating raw IMU signals continuously (which is unstable for low-cost MEMS), the approach segments sequences into windows and trains RNNs to directly predict window-level polar odometry outputs; these predictions are then composed according to the physical dead-reckoning equations to form trajectories. This breaks the continuous integration and reduces error accumulation; the physical model defines the output space (Δl, Δψ) used for supervised learning.",
            "transfer_success": "successful - this hybrid approach underlies IONet and is effective: it produces accurate trajectories when trained and when domain adaptation is available; it enabled the MotionTransformer to reconstruct physically meaningful paths in unlabelled domains when prediction quality was sufficient.",
            "barriers_encountered": "Underlying physical integration still vulnerable to drift if predictions are inaccurate; requires labeled windows for supervised training (hence need for domain adaptation when labels absent); small prediction errors still accumulate over time.",
            "facilitating_factors": "Combining physical structure (polar outputs and dead-reckoning composition) with data-driven RNN predictions reduces sensitivity to MEMS noise and yields interpretable outputs that are straightforward to chain into trajectories.",
            "contextual_requirements": "Choice of subsequence length (here 2 s / 200 frames), synchronized high-frequency IMU and ground truth for supervised training, and appropriate RNN architectures for temporal modeling.",
            "generalizability": "Generalizable to other inertial odometry problems and other sensors where physical composition laws can be combined with learned local estimators; authors note earlier IONet work and current transfer experiments validate this across attachments.",
            "knowledge_type": "theoretical principles combined with explicit procedural steps (physics-informed ML, algorithmic knowledge)",
            "uuid": "e563.4",
            "source_info": {
                "paper_title": "MotionTransformer: Transferring Neural Inertial Tracking between Domains",
                "publication_date_yy_mm": "2019-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation",
            "rating": 2
        },
        {
            "paper_title": "Adversarial Discriminative Domain Adaptation",
            "rating": 2
        },
        {
            "paper_title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient",
            "rating": 2
        },
        {
            "paper_title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
            "rating": 1
        },
        {
            "paper_title": "IONet: Learning to Cure the Curse of Drift in Inertial Odometry",
            "rating": 2
        },
        {
            "paper_title": "OxIOD: The Dataset for Deep Inertial Odometry",
            "rating": 2
        }
    ],
    "cost": 0.014093499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MotionTransformer: Transferring Neural Inertial Tracking between Domains</h1>
<p>Changhao Chen, ${ }^{1}$ Yishu Miao, ${ }^{1}$ Chris Xiaoxuan Lu, ${ }^{1}$ Linhai Xie, ${ }^{1}$ Phil Blunsom, ${ }^{1,2}$ Andrew Markham, ${ }^{1}$ Niki Trigoni ${ }^{1}$<br>${ }^{1}$ Department of Computer Science, University of Oxford<br>${ }^{2}$ DeepMind<br>firstname.lastname@cs.ox.ac.uk</p>
<h4>Abstract</h4>
<p>Inertial information processing plays a pivotal role in egomotion awareness for mobile agents, as inertial measurements are entirely egocentric and not environment dependent. However, they are affected greatly by changes in sensor placement/orientation or motion dynamics, and it is infeasible to collect labelled data from every domain. To overcome the challenges of domain adaptation on long sensory sequences, we propose MotionTransformer - a novel framework that extracts domain-invariant features of raw sequences from arbitrary domains, and transforms to new domains without any paired data. Through the experiments, we demonstrate that it is able to efficiently and effectively convert the raw sequence from a new unlabelled target domain into an accurate inertial trajectory, benefiting from the motion knowledge transferred from the labelled source domain. We also conduct real-world experiments to show our framework can reconstruct physically meaningful trajectories from raw IMU measurements obtained with a standard mobile phone in various attachments.</p>
<h2>Introduction</h2>
<p>Egomotion awareness plays a vital role in developing perception, cognition, and motor control for mobile agents through their own sensory experiences (Agrawal, Carreira, and Malik 2015). Inertial information processing, a typical egomotion awareness process operating in the human vestibular system (Cullen 2012) contributes to a wide range of daily activities. Modern micro-electro-mechanical (MEMS) inertial measurements units (IMUs) are analogously able to sense angular and linear accelerations - they are small, cheap, energy efficient and widely employed in smartphones, robots and drones. Unlike other commonly used sensor modalities, such as GPS, radio and vision, inertial measurements are completely egocentric and as such are far less environment dependent e.g. they work equally well in an unlit underground tunnel as in open spaces. Developing accurate inertial tracking is thus of key importance for robot/pedestrian navigation and for selfmotion estimation (Harle 2013). In the emergency scenarios, it can help track firefighters and other first-responders to enhance the safety and efficiency of their operations without the need of any bespoke positioning infrastructure. However,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the task of turning inertial measurements into pose and odometry estimates is hugely complicated by the fact that different placements (e.g. carrying a smartphone in a pocket or in the hand) and orientations lead to significantly different inertial data in the sensor frame. It is clearly infeasible to collect labelled data from every possible attachment, as this requires specialized motion capture systems e.g. VICON and a high degree of effort. In this paper, therefore, we propose a robust generative adversarial network for sequence domain transformation which is able to directly learn inertial tracking in unlabelled domains without using any paired sequences.</p>
<p>Prevailing inertial tracking methods, e.g. Strapdown Inertial Navigation System (SINS) (Savage 1998) and Pedestrian Dead Reckoning (PDR) (Xiao et al. 2015), are mostly based on delicate handcrafted models. These model-based approaches can obtain plausible achievements in general scenarios, but their lack of generalisation ability yields poor performance in the complex real world applications. Recent work in neural inertial tracking (Chen et al. 2018a) has demonstrated that deep neural networks are capable of extracting high level motion representations (displacement and heading angle) from raw IMU sequence data, and providing accurate trajectories. However, the data-driven method that requires substantial labeled data for training, and a model trained on a single domain-specific dataset may not generalise well to new domains (Tzeng et al. 2017). As shown in Figure 1, the uncertainties of phone placements, the corresponding motion dynamics, and the projection of gravity significantly alter the inertial measurements acquired from different domains (sensor frames) while the actual trajectories in the navigation frame are identical.</p>
<p>We note that it is possible to train end-to-end deep neural networks when presented with large amounts of labelled data. The question becomes, how can we generalize to an arbitrary attachment in the absence of labels or a paired/timesynchronized sequence? Although from the observation the raw inertial data for each domain is very different, and the resulting odometry trajectories are also unrelated to one another, the underlying statistical distribution of odometry pose updates, if derived from a common agent (e.g. human motion), must be similar. Our intuition is to decompose the raw inertial data into a domain-invariant semantic representation, learning to discard the domain-specific motion sequence transformation.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Phone was placed in (a) hand, (b) pocket, (c) bag and (d) trolley. Compared to firmly holding in the hand, the IMU experiences slight swings in pocket. The angular and linear accelerations in the navigation frame are projected on different axes in each sensor frame, and the gravity is mainly projected onto the y axis rather than z axis. These variations are termed motion domain shifts as the sensor frames are different yet the inertial data in the navigation frame is invariant, which impose huge challenges on transferring a learned inertial tracking system to a new domain.</p>
<p>To overcome the challenges of generalising inertial tracking across different motion domains, we propose the MotionTransformer framework with Generative Adversarial Networks (GAN) for sensory sequence domain transformation. Its key novelty is in using a shared encoder to transform raw inertial sequences into a domain-invariant hidden representation, without the use of any paired data. Different from many GAN-based sequence generation models applied in the field of natural language processing (Yu et al. 2017), where the sequences consist of discrete symbols or words (e.g. dialogue generation, poem generation and unsupervised machine translation) (Li et al. 2017), our model is focused on transferring continuous long time series sensory data. It is worth mentioning that instead of using the conditional autoregressive decoder that takes whole source sequences as input and generates variant-length sequences (generally applied in sequence-to-sequence generation models), our framework is able to take the advantage of time consistency and directly produces the outputs in target domains aligned to the inputs in source domains in every time step.</p>
<p>MotionTransformer dramatically reduces the effort in converting raw inertial data to an accurate trajectory, as no labelled or even paired data is required to achieve motion transformation in new domains. Through extensive, real-world experiments, we demonstrate that the framework is able to efficiently and effectively transform an arbitrary domain into an accurate inertial trajectory, benefiting from the knowledge transferred from the labelled source domain. This work addresses a challenging problem in inertial egomotion inference.</p>
<h2>Model</h2>
<p>Instead of directly predicting the trajectories conditioned on IMU outputs, we incorporate the neural model with a physical model for better inertial tracking inference. Here we introduce the physical model for inertial tracking and the MotionTransformer for sequence domain adaptation respectively.</p>
<h3>Inertial Tracking Physical Model</h3>
<p>The physical model, derived from Newtonian Mechanics, integrates the angular rates of the sensor frame ${\mathbf{w}<em i="1">{i}}</em>$ (w $}^{N<em i="i">{i} \in \mathbb{R}^3$ and $N$ is the length of the whole sequence) measured by the three-axis gyroscope into orientation attitudes. While the linear accelerations of the sensor frame ${\mathbf{a}</em>}<em i="i">{i=1}^N$ (a $</em>} \in \mathbb{R}^3$) measured by the three-axis accelerometer are transformed to the navigation frame and doubly integrated to give the position displacement, which discards the impact of the constant acceleration of gravity. This physical model is hard to implement directly on low-cost IMUs, because even a small measurement error will be exaggerated exponentially through the integration. Recent deep-learning based inertial tracking (Chen et al. 2018a) breaks the continuous integration by segmenting the sequence of inertial measurements ${(\mathbf{a<em i="i">{i}, \mathbf{w}</em>)}<em i="i">{i=1}^N$ into subsequences. We denote a subsequence as $\mathbf{x} = {(\mathbf{a}</em>}, \mathbf{w<em i="1">{i})}</em> = (\Delta l, \Delta \psi)$, which represents the heading and location displacement:}^n$, whose length is $n$. By taking into subsequences as inputs, a recurrent neural network (RNN) is leveraged to periodically predict the polar vector $\mathbf{y</p>
<p>$$(\Delta l, \Delta \psi) = \text{RNN}({(\mathbf{a}<em i="1">i, \mathbf{w}_i)}</em>$$}^n \tag{1</p>
<p>Based on the predicted $(\Delta l, \Delta \psi)$, we are able to easily construct the trajectories. However, it requires a large labelled dataset to build an end-to-end inertial tracking system, and it is infeasible to label data for every possible domain due to the motion dynamics and unpredictability of device placements. Therefore, we introduce the MotionTransformer framework in next subsection which is able to exploit the unlabelled sensory measurements in new domains and carry out accurate inertial tracking.</p>
<h2>MotionTransformer Framework</h2>
<p>As Figure. 2 illustrates, our framework consists of encoder, generator, decoder and predictor modules. Assume a scenario of two domains: a source domain and a target domain, where the source domain has labelled sequences $\left(\mathbf{x}^{S}, \mathbf{y}^{S}\right) \in \mathbb{D}^{S}$ ( $\mathbf{y}^{S}$ is the sequence label - the polar vector of $\mathbf{x}^{S}$ ), and the target domain only has unlabelled sequences $\mathbf{x}^{T} \in \mathbb{D}^{T}$. Note that the sequences $\mathbf{x}^{S}$ and $\mathbf{x}^{T}$ are not aligned. The objectives of MotionTransformer Framework are three-fold: 1) extracting domain-invariant representations $\mathbf{z}$ shared across domains; 2) generating $\tilde{\mathbf{x}}^{T}$ in the the target domain conditioned on $\mathbf{x}^{S} ; 3$ ) predicting sequence labels $\mathbf{y}^{T}$ in the target domain.</p>
<p>Sequence Encoder To extract the domain-invariant hidden representations $\mathbf{z}$ of sensory sequences across different domains, a RNN encoder is employed together with a specific domain vector $\theta$ :</p>
<p>$$
\mathbf{z}=f_{e n c}(\mathbf{x}, \theta)
$$</p>
<p>where $\mathbf{z}<em i="i">{i}$ is aligned to $\mathbf{x}</em>$ are shared across all the domains.}$ at every $i$ th time step, and $\theta$ remains the same across all the time steps. For different domains, we apply different domain vectors $\theta$ that attempt to isolate domain-specific features, while the parameters of $f_{\text {enc }</p>
<p>GAN Generator Having the domain-invariant representations $\mathbf{z}$, a RNN generator $f_{g e n}^{T}(\mathbf{z})$ can be directly built to generate synthetic sequences $\tilde{\mathbf{x}}^{T}$ in the target domain from $\mathbf{x}^{S}$. By combining it with the encoder, we derive the sequence transformation model $G_{S \rightarrow T}=f_{g e n}^{T} \circ f_{\text {enc }}$ as:</p>
<p>$$
\tilde{\mathbf{x}}^{T}=G_{S \rightarrow T}\left(\mathbf{x}^{S}, \theta^{S}\right)=f_{g e n}^{T}\left(f_{e n c}\left(\mathbf{x}^{S}, \theta^{S}\right)\right)
$$</p>
<p>Likewise, we construct an inverse mapping $G_{T \rightarrow S}=f_{g e n}^{S} \circ$ $f_{\text {enc }}$ for generating $\tilde{\mathbf{x}}^{S}$ from $\mathbf{x}^{T}$ :</p>
<p>$$
\tilde{\mathbf{x}}^{S}=G_{T \rightarrow S}\left(\mathbf{x}^{T}, \theta^{T}\right)=f_{g e n}^{S}\left(f_{e n c}\left(\mathbf{x}^{T}, \theta^{T}\right)\right)
$$</p>
<p>$G_{S \rightarrow T}$ is trained against a target domain discriminator $D^{T}$ (discriminators are omitted in Figure [2]) in the framework of GAN, and vice versa for $G_{T \rightarrow S}$. Unlike conventional GAN models, we decompose the generator by the domain-invariant representation $\mathbf{z}$. Intuitively, this architecture encourages the encoder $f_{\text {enc }}$ to capture domain-invariant features that generate sensory sequences in different domains, as the encoding function $f_{\text {enc }}$ are shared by both $G_{S \rightarrow T}$ and $G_{T \rightarrow S}$.</p>
<p>Reconstruction Decoder In addition to the GAN generator, we introduce a RNN decoder $f_{d e c}$ to reconstruct the sequences $\tilde{\mathbf{x}}$ conditioned on $\mathbf{z}$. This is aimed at reinforcing the learning of domain-invariant features when jointly learned with the GAN generator. Instead of using the conventional Denoising Autoencoders (DAE) (Vincent et al. 2010) and Variational Autoencoders (VAE) (Kingma and Welling 2014), we only introduce an additive noise to the hidden representations $\tilde{\mathbf{z}}=\mathbf{z}+\epsilon$, where $\epsilon \sim N\left(0, I^{2}\right)$, in order to simplify the learning of the autoencoder component. Similar to the sequence encoder $f_{\text {enc }}$, the decoder is shared across all the domains and the domain vector $\theta$ is concatenated with inputs at every time step:</p>
<p>$$
\tilde{\mathbf{x}}=f_{d e c}(\tilde{\mathbf{z}}, \theta)=f_{d e c}\left(f_{e n c}(\mathbf{x}, \theta)+\epsilon, \theta\right)
$$</p>
<p>Polar Vector Predictor Since the source domain has labels (polar vectors) aligned to every sensory sequence, it is straightforward to learn a predictor for carrying out inertial tracking by supervised learning the labels $\mathbf{y}^{S}$. However, this is not the ultimate objective of this paper, instead we aim at transferring the knowledge learned in the labelled source domain to the unlabelled target domain. Hence, with the help of the sequence encoder $f_{\text {enc }}$, we construct a predictor $f_{\text {pred }}$ also shared by both the source domain and the target domain. In this case, though there exists no paired data $\left(\mathbf{x}^{T}, \mathbf{y}^{T}\right)$ for supervised learning in the target domain, we can still predict $\mathbf{y}^{T}$ by:</p>
<p>$$
\mathbf{y}^{T}=f_{p r e d}\left(f_{e n c}\left(\mathbf{x}^{T}, \theta^{T}\right)\right)
$$</p>
<h2>Inference</h2>
<p>This section introduces the learning method for jointly training the modules of our MotionTransformer, including GAN loss $\mathcal{L}<em A="A" E="E">{G}$, reconstruction loss $\mathcal{L}</em>}$, prediction loss $\mathcal{L<em _cycle="{cycle" _text="\text">{\text {pred }}$, cycle-consistency $\mathcal{L}</em>}}$ and perceptual consistency $\mathcal{L<em _text="\text" _total="{total">{\text {percep }}$ :
$\mathcal{L}</em>}}=\mathcal{L<em 1="1">{G A N}+\lambda</em>} \mathcal{L<em 2="2">{A E}+\lambda</em>} \mathcal{L<em 3="3">{\text {pred }}+\lambda</em>} \mathcal{L<em 4="4">{\text {cycle }}+\lambda</em>} \mathcal{L<em 1="1">{\text {percep }}$
where $\lambda</em>$ are the hyper-parameters used as the trade-off for the optimization process.}, \lambda_{2}, \lambda_{3}$, and $\lambda_{4</p>
<p>GAN Loss GAN generator is one of the most important component in our framework, which is responsible of producing sensory sequences in the unlabelled target domain. Here, following the general GAN framework, we construct a discriminator $D^{T}$ for the corresponding target domain and learn to discriminate the generated data $\tilde{\mathbf{x}}$ from the real one $\mathbf{x}$. The GAN loss for the target domain generator can be defined as:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _mathbf_x="\mathbf{x">{G^{T}}= &amp; \mathbb{E}</em>\right)\right]+ \
&amp; \mathbb{E}}^{T} \sim p\left(\mathbf{x}^{T}\right)}\left[\log D^{T}\left(\mathbf{x}^{T<em S="S" T="T" _rightarrow="\rightarrow">{\mathbf{x}^{S} \sim p\left(\mathbf{x}^{S}\right)}\left[\log \left(1-D^{T}\left(G</em>\right)\right)\right]\right.
\end{aligned}
$$}\left(\mathbf{x}^{S}, \theta^{S</p>
<p>Similarly, the GAN loss for the source domain generator is:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _mathbf_x="\mathbf{x">{G^{S}}= &amp; \mathbb{E}</em>\right)\right]+ \
&amp; \mathbb{E}}^{S} \sim p\left(\mathbf{x}^{S}\right)}\left[\log D^{S}\left(\mathbf{x}^{S<em S="S" T="T" _rightarrow="\rightarrow">{\mathbf{x}^{T} \sim p\left(\mathbf{x}^{T}\right)}\left[\log \left(1-D^{S}\left(G</em>\right)\right)\right]\right.
\end{aligned}
$$}\left(\mathbf{x}^{T}, \theta^{T</p>
<p>Then, we combine these two losses into the final GAN loss $\mathcal{L}<em G_S="G^{S">{G A N}=\mathcal{L}</em>$.}}+\mathcal{L}_{G^{T}</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Architecture of Proposed MotionTransformer: including the source domain sequence <strong>Encoder</strong> (extracting common features across different domains), the target domain sequence <strong>Generator</strong> (generating sensory stream in the target domain), the sequence reconstruction <strong>Decoder</strong> (reconstructing the sequence for learning better representations) and the polar vector <strong>Predictor</strong> (producing consistent trajectory for inertial navigation). The GAN discriminators and the source domain <strong>Generator</strong> are omitted from this figure.</p>
<h3>Reconstruction Loss</h3>
<p>Considering the inputs are the continuous real-valued data, the MSE loss is chosen to optimize the autoencoder loss for source and target domain data respectively:</p>
<p>$$\mathcal{L}<em _mathbf_x="\mathbf{x">{AE} = \mathbb{E}</em>|}^{S} \sim p(\mathbf{x}^{S}), \mathbf{x}^{T} \sim p(\mathbf{x}^{T})} [|\hat{\mathbf{x}}^{S} - \mathbf{x}^{S<em 2="2">{2} + |\hat{\mathbf{x}}^{T} - \mathbf{x}^{T}|</em>$$}] \tag{10</p>
<h3>Prediction Loss</h3>
<p>In addition to the original paired data (x^{S}, y^{S}) in the source domain, we are able to make use of the generated ones $\hat{\mathbf{x}}^{T} = f_{gen}^{T} (f_{enc}(\mathbf{x}^{S}, \theta^{S}))$ produced by the GAN generator in the target domain as well, since the domain-invariant representations can be directly applied for the prediction no matter which domain the sequences are from. Hence, a joint regression loss can be constructed for learning the predictor:</p>
<p>$$\mathcal{L}<em _mathbf_x="(\mathbf{x">{pred} = \mathbb{E}</em>))|}^{S}, \mathbf{y}^{S}) \sim p(\mathbf{x}^{S}, \mathbf{y}^{S})} [|\mathbf{y}^{S} - f_{pred}(f_{enc}(\mathbf{x}^{S}, \theta^{S<em pred="pred">{2} + |\mathbf{y}^{S} - f</em>$$}(f_{enc}(\hat{\mathbf{x}}^{T}, \theta^T))|_2] \tag{11</p>
<p>Although the adversarial training is unable to produce exact the same sequences as the ones generated in the target domain, the labels in the source domain encourages the sequence encoder to preserve the prominent features for prediction, so that the domain-invariant representations will be further regularised by the labels in the source domain.</p>
<h3>Cycle Consistency Regularisation</h3>
<p>In order to improve the sensory sequence generation, we apply the cycle-consistency regularisation to ensure the sequences generated to the target domain from the source domain can be mapped back without losing too much content information. As demonstrated by (Kim et al. 2017; Zhu et al. 2017; Yi et al. 2017), this bidirectional architecture encourages the GAN to generate data in meaningful direction by punishing the optimizer with the L-1 consistency loss defined as:</p>
<p>$$\mathcal{L}<em x_S="x^{S">{cycle} = \mathbb{E}</em>^S \right|} \sim p(x^{S})} \left| G_{T \rightarrow S} (G_{S \rightarrow T} (\mathbf{x}^{S}, \theta^S), \theta^T) - \mathbf{x<em x_T="x^{T">1 + \mathbb{E}</em>$$} \sim p(x^{T})} \left| G_{S \rightarrow T} (G_{T \rightarrow S} (\mathbf{x}^{T}, \theta^T), \theta^S) - \mathbf{x}^T \right|_1 \tag{12</p>
<h3>Perceptual Consistency Regularisation</h3>
<p>To further regularise the learning of domain-invariant representations, we propose the perceptual consistency regularisation. Inspired by the <em>f</em> constancy (Taigman, Polyak, and Wolf 2017), we employ the encoder <em>f_{enc}</em> as the perceptual function to enforce the semantic representation constant after being transformed into another domain by generators. For example, in source domains, the hidden representation <strong>z</strong><sup>S</sup> = <em>f_{enc}</em> (x<sup>S</sup>, θ<sup>S</sup>) extracted by the encoder conditioned on the source domain vector, will be encouraged invariant under <em>G_{S→T}</em> by minimizing a L-2 distance between the original hidden representation <strong>z</strong><sup>S</sup> and the hidden representation <strong>ẑ</strong><sup>S</sup> = <em>f_{enc}</em> (x̂<sup>T</sup>, θ<sup>T</sup>) extracted from the generated synthetic target domain data <strong>x̂</strong><sup>T</sup> = <em>G_{S→T}</em> (<em>x</em>^{S</sup>, θ<sup>S</sup>). Similar perceptual constraint can be applied for target domain.</p>
<p>$$\mathcal{L}<em x_S="x^{S">{percep} = \mathbb{E}</em>, \theta^S), \theta^T) |} \sim X^{S}} | f_{enc}(x^{S}, \theta^S) - f_{enc}(G_{S \rightarrow T}(x^{S<em x_T="x^{T">2 + \mathbb{E}</em>$$} \sim X^{T}} | f_{enc}(x^T, \theta^T) - f_{enc}(G_{T \rightarrow S}(x^T, \theta^T), \theta^S) |_2 \tag{13</p>
<h1>Experiments</h1>
<h2>Inertial Tracking Dataset</h2>
<p>A commercial-off-the-shelf smartphone, the iPhone 7Plus, is employed to collect inertial measurement data of pedestrian random walking. The smartphone was attached in four</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Heading displacement estimation from training in (a) source domain, (b) target domain and (c) MotionTransformer, and location displacement estimation from training in (d) source domain, (e) target domain and (f) MotionTransformer</p>
<p>Different poses: handheld, pocket, handbag and trolley, each of which represents a domain that has dramatically distinct motion pattern with others.</p>
<p>We use an optical motion capture system (Vicon) (Vicon 2017) to record the ground truth. The Vicon system is able to provide high-precision full pose reference (0.01 m for location, 0.1 degree for orientation), and tracks our participants carrying the experimental device attached with Vicon markers. The 100 Hz sensor readings are then segmented into sequences with corresponding labels, e.g., location and heading attitude displacement provided by Vicon system. These source-domain labels are used for MotionTransformer training while the target-domain labels are used for MotionTransformer evaluation only. The length of each sequence is 200 frames (2 seconds), including three linear accelerations and three angular rates per frame. In summary, the dataset<sup>1</sup> (Chen et al. 2018b) used in this work contains around 45K, 53 K, 36K and 29K sequences for handheld, pocket, bag, trolley domains respectively. Among them, 4K sequences were selected as validation data in each domain, and the rest was taken as training set. In our training phase, we set the hyper-parameters λ<sub>1</sub> = 0.01, λ<sub>2</sub> = 100, λ<sub>3</sub> = 0.1, and λ<sub>4</sub> = 1.</p>
<h3>Transferring Across Motion Domains</h3>
<p>We evaluate our model on unsupervised motion domain transfer tasks. The source domain is the inertial data collected in the handheld attachment, while the target domains are those collected in the attachments of pocket, handbag and trolley. We test the our framework with the real target data.</p>
<p>Its generalization performance is evaluated by comparing the label prediction (polar vector) with the ground-truth data captured by Vicon system. We compare with source-only, where we use the trained source predictor to predict data directly in the target domain and with target-only where we train the target dataset with target labels (40K) to show the performance of fully supervised learning. Figure 3 presents the predicted location and heading displacement in pocket domain for the three different techniques. It can be seen that source-only is unable to follow either delta heading or delta location accurately, whereas MotionTransformer achieves a level of performance close to the fully supervised target-only, especially for delta heading.</p>
<p>Table 1 presents the <em>quantitative analysis</em> with a metric of mean square error of the label prediction against the ground truth. Compared with using a model trained on source data only, our proposed unsupervised sequence domain adaptation technique helps to dramatically decrease the validation loss (almost 6 times, 9 times, and 3 times in pocket, bag and trolley domains). Two popular baselines are compared with MotionTransformer: i) adversarial discriminative domain adaptation (ADDA) (Tzeng et al. 2017) and ii) cycle-consistent adversarial domain adaptation (CyCADA) (Hoffman et al. 2017). ADDA is a discriminative model, forcing the feature fusion of two domains by distinguishing the features after the encoder. CyCADA is a generative model, using standard Cycle-GAN framework to generate synthetic target domain data and fine-tune the predictor through synthetic data. Because they both aim to process images rather than continuous sequential data, we replace their convolutional generator and discriminator with the same LSTM layers as described in our frameworks. Moreover, we cut down the reconstruction loss and percep-</p>
<p><sup>1</sup>Dataset can be found at http://deepio.cs.ox.ac.uk</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Inertial tracking trajectories of (a) Pocket (b) Trolley (c) Handbag, comparing our proposed unsupervised MotionTransformer with Ground Truth and Supervised Learning.</p>
<p>Table 1: Unsupervised Transfer Across Motion Domains</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Hand → Pocket</th>
<th>Hand → Bag</th>
<th>Hand → Trolley</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training on Source, Testing on Target</td>
<td>0.718</td>
<td>1.129</td>
<td>0.273</td>
</tr>
<tr>
<td>ADDA with LSTM</td>
<td>0.471</td>
<td>0.631</td>
<td>0.216</td>
</tr>
<tr>
<td>CyCADA with LSTM</td>
<td>0.237</td>
<td>0.455</td>
<td>0.182</td>
</tr>
<tr>
<td>MotionTransformer w/o Reconstruction</td>
<td>0.313</td>
<td>0.335</td>
<td>0.113</td>
</tr>
<tr>
<td>MotionTransformer w/o Perceptual Loss</td>
<td>0.315</td>
<td>0.202</td>
<td>0.140</td>
</tr>
<tr>
<td>MotionTransformer</td>
<td>0.119</td>
<td>0.123</td>
<td>0.098</td>
</tr>
<tr>
<td>Semi-Supervised MotionTransformer (1K)</td>
<td>0.113</td>
<td>0.075</td>
<td>0.047</td>
</tr>
<tr>
<td>Train on Target, Test on Target (40K)</td>
<td>0.045</td>
<td>0.010</td>
<td>0.006</td>
</tr>
</tbody>
</table>
<p>tual loss in our framework respectively to show their impact. As shown in Table 1, our MotionTransformer still achieves competitive performance compared to these baselines.</p>
<p>Lastly, we also evaluate our framework on a semi-supervised domain transfer task, with 1K labelled target domain sequences to train the predictor. As shown in Table 1, the sparse labels in target domains help decrease the prediction error, especially in the bag and trolley domains.</p>
<h3>Inertial Tracking in Unlabelled Domains</h3>
<p>We argue that the predicted label from our domain transformation framework is capable of solving a downstream task—inertial odometry tracking. In an inertial tracking task, the precision of the predicted label determines the localization accuracy, as the current location (<em>x</em><em n="n">{n}, <em>y</em></em>) is calculated by using an initial location (<em>x</em><em 0="0">{0}, <em>y</em></em>) and heading, and chaining the results of previous windows via Equation 14. This dead reckoning (DR) technique, also called path integration, can be widely found in animal navigation (McNaughton et al. 2006), which enables animals to use inertial cues (e.g., steps and turns) to track themselves in the absence of vision. The errors in path integration will accumulate and cause unavoidable drifts in trajectory estimation, which imposes a requirement for accurate motion domain transformation. Without domain adaptation, if the model trained on source domain is directly applied to data from target domains, it will not produce any trajectory representing the self-motion. When using only inertial information, traditional model-based navigation algorithms perform poorly in unlabelled scenarios, as SINS will collapse due to the high measurements' noises, and PDR will be influenced by incorrect step detection or device orientation.</p>
<p>$$\begin{cases} x_n = x_0 + \Delta \text{lcos}(\psi_0 + \Delta \psi) \ y_n = y_0 + \Delta \text{lsin}(\psi_0 + \Delta \psi) \end{cases} \tag{14}$$</p>
<p>We show that the inertial tracking trajectory can be recovered from the labels predicted by our domain adaptation framework in <em>unlabelled</em> domains. The participant walked with the device placed in the pocket, the handbag and on the trolley. The inertial data during test walking trajectory was not included in training dataset, and collected in different days. Figure 4 illustrates that our proposed model succeeds in generating physically meaningful trajectories, close to the ground truth captured by Vicon system. It proves that exploiting the raw sensory stream and transforming to a common latent distribution can extract meaningful semantic features that help solve downstream tasks.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Visualization of extracted representations in the source and target domains. It can be seen that the MotionTransformer leads to a more consistent latent representation compared with the disjoint representations of the normal encoder and the ADDA encoder.</p>
<h3>Interpreting the Sequence Encoder</h3>
<p>The role of the sequence encoder is evaluated by the t-SNE projection to show its ability to map the raw data from two domains to an identical semantic space. We compare it with two other baselines: a domain-specific encoder, which is only employed in the source domain (it is not shared across domains); an ADDA encoder, which is learned jointly with the predictor by adversarial training to force the fusion of representations extracted by the encoder. The t-SNE projection is shown in Figure 5, and all of the models apply the same parameters (Perplexity=10, step=5000). As can be seen, the data points of domain-specific encoder are distinctly separated into two folds. ADDA attempts to fuse the points from two domains but it turns out points are still clearly separated. By contrast, the encoder of our MotionTransformer is able to better scatter the points dispersively in the semantic space, which removes the domain shifts and benefit target label prediction.</p>
<h3>Related Work</h3>
<p><strong>Domain Adaptation</strong> Our work is most related to domain adaptation techniques, which aim to align the learned representation across source and target domains by minimizing maximum mean discrepancy loss (Long et al. 2015) or adversarial loss (Ganin et al. 2016; Tzeng et al. 2017). Recent adversarial approaches have been achieved the state of art results in multiple tasks, for example, sleep stages prediction (Zhao et al. 2017), healthcare data prediction (Purushotham et al. 2017) and image-to-image translation (Liu, Breuel, and Kautz 2017). Prior art can be categorized into two main groups: the discriminative adversarial models seek to align the embedding representation between target and source domains to encourage domain confusion (Ganin et al. 2016; Tzeng et al. 2015; 2017); the generative adversarial models aim to employ generated data for training the prediction networks and meanwhile fooling the discriminator (Shrivastava et al. 2017; Hoffman et al. 2017; Liu, Breuel, and Kautz 2017). Here, we utilize the generative adversarial networks (GAN) to generate sensory sequence data from invariant features extracted by an identical encoder.</p>
<p><strong>Inertial Navigation Systems</strong> Early inertial navigation systems were developed as the core components in control and navigation systems for missiles, submarines, and spacecraft, relying on expensive, heavy and high-precision inertial measurement units (Savage 1998). The traditional strapdown inertial navigation algorithms are hard to realize on low-cost MEMS inertial sensors, because the high measurement noises cause exponential error propagation via open integration, and the inertial output collapses within seconds (Harle 2013). To mitigate the unbounded error drift, one solution is to combine cameras with inertial sensors as realized in visual inertial odometry (Li and Mourikis 2013). Another solution is to detect steps and update the trajectory with estimated step length and heading through pedestrian dead reckoning (Xiao et al. 2015). These model based approaches exploit the context information to reduce the inertial systems drift, for example, via zero-velocity update (Nilsson et al. 2012; Chen et al. 2016), floor map (Xiao et al. 2014), electronic magnetic field (Lu et al. 2018), but their assumptions are too strong. As a consequence their performance is variable in complex real-world conditions: visual-inertial odometry assumes cameras have feature-rich, well illuminated scenes without occlusion, and PDR assumes the user's personal walking model and the phone placement are prior knowledge (Brajdic and Harle 2013). Recent deep learning based inertial tracking (Chen et al. 2018a) can learn direct location transforms from raw inertial data, and construct continuous accurate trajectories for indoor users, but still suffers from serious domain shifts and generalization problems. Our work aims to unsupervised learn the inertial tracking in unlabeled new domains, effectively increasing its generalization ability and flexibility in real usages.</p>
<h3>Conclusion and Discussion</h3>
<p>Motion transformation between different domains is a challenging task, which typically requires the use of labeled data for training. In the presented framework, by transforming target domains to a consistent, invariant representation, a physically meaningful trajectory can be well reconstructed. Intuitively, our technique is learning how to transform data from an arbitrary sensor domain θ to a common latent representation. Analogously, this is equivalent to learning how to translate any sensor frame to the navigation frame, without any labels in the target domain. Although MotionTransformer has been shown to work on IMU data, the broad framework is likely to be suitable for any continuous, sequential domain</p>
<p>transformation task where there is an underlying physical model.</p>
<h2>Acknowledgements</h2>
<p>We thank all the reviewers and ACs. This work is funded by the National Institute of Standards and Technology (NIST) Grant No. 70NANB17H185. We also hope to thank Prof. Xiaoping Hu, Prof. Xiaofeng He, and Prof. Lilian Zhang at National University of Defense Technology, China for their useful assistance and valuable discussion, who are supported by the National Natural Science Foundation of China (Grants Nos. 61773394, 61573371, 61503403).</p>
<h2>References</h2>
<p>Agrawal, P.; Carreira, J.; and Malik, J. 2015. Learning to see by moving. In ICCV, volume 11-18, 37-45.
Brajdic, A., and Harle, R. 2013. Walk detection and step counting on unconstrained smartphones. In Ubicomp.
Chen, C.; Chen, Z.; Pan, X.; and Hu, X. 2016. Assessment of zerovelocity detectors for pedestrian navigation system using mimu. In 2016 IEEE Chinese Guidance, Navigation and Control Conference (CGNCC), 128-132.
Chen, C.; Lu, X.; Markham, A.; and Trigoni, N. 2018a. IONet: Learning to Cure the Curse of Drift in Inertial Odometry. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI).
Chen, C.; Zhao, P.; Lu, C. X.; Wang, W.; Markham, A.; and Trigoni, N. 2018b. OxIOD: The Dataset for Deep Inertial Odometry. arXiv 1809.07491.</p>
<p>Cullen, K. E. 2012. The vestibular system: Multimodal integration and encoding of self-motion for motor control. Trends in Neurosciences 35(3):185-196.
Ganin, Y.; Ustinova, E.; Ajakan, H.; Germain, P.; Larochelle, H.; Laviolette, F.; Marchand, M.; and Lempitsky, V. 2016. Domainadversarial training of neural networks. Journal of Machine Learning Research 17:1-35.
Harle, R. 2013. A Survey of Indoor Inertial Positioning Systems for Pedestrians. IEEE Communications Surveys and Tutorials 15(3):1281-1293.
Hoffman, J.; Tzeng, E.; Park, T.; Zhu, J.-Y.; Isola, P.; Saenko, K.; Efros, A. A.; and Darrell, T. 2017. CyCADA: Cycle-Consistent Adversarial Domain Adaptation. In ICML, 1-12.
Kim, T.; Cha, M.; Kim, H.; Lee, J. K.; and Kim, J. 2017. Learning to Discover Cross-Domain Relations with Generative Adversarial Networks. In ICML.
Kingma, D. P., and Welling, M. 2014. Auto-Encoding Variational Bayes. In $I C L R, 1-14$.
Li, M., and Mourikis, A. I. 2013. High-precision, consistent EKF-based visual-inertial odometry. The International Journal of Robotics Research 32(6):690-711.
Li, J.; Monroe, W.; Shi, T.; Jean, S.; Ritter, A.; and Jurafsky, D. 2017. Adversarial Learning for Neural Dialogue Generation. In EMNLP.
Liu, M.-Y.; Breuel, T.; and Kautz, J. 2017. Unsupervised Image-toImage Translation Networks. In NIPS.
Long, M.; Cao, Y.; Wang, J.; and Jordan, M. I. 2015. Learning Transferable Features with Deep Adaptation Networks. In ICML, volume 37, 1-20.</p>
<p>Lu, C. X.; Li, Y.; Zhao, P.; Chen, C.; Xie, L.; Wen, H.; Tan, R.; and Trigoni, N. 2018. Simultaneous localization and mapping with power network electromagnetic field. In Proceedings of the 24th Annual International Conference on Mobile Computing and Networking, MobiCom '18, 607-622. New York, NY, USA: ACM.
McNaughton, B. L.; Battaglia, F. P.; Jensen, O.; Moser, E. I.; and Moser, M. B. 2006. Path integration and the neural basis of the 'cognitive map'. Nature Reviews Neuroscience 7(8):663-678.
Nilsson, J. O.; Skog, I.; Händel, P.; and Hari, K. V. S. 2012. Footmounted INS for everybody - An open-source embedded implementation. In IEEE PLANS, Position Location and Navigation Symposium, 140-145.
Purushotham, S.; Carvalho, W.; Nilanon, T.; and Liu, Y. 2017. Variational Recurrent Adversarial Deep Domain Adaptation. In $I C L R, 1-11$.
Savage, P. G. 1998. Strapdown Inertial Navigation Integration Algorithm Design Part 1: Attitude Algorithms. Journal of Guidance, Control, and Dynamics 21(1):19-28.
Shrivastava, A.; Pfister, T.; Tuzel, O.; Susskind, J.; Wang, W.; and Webb, R. 2017. Learning from Simulated and Unsupervised Images through Adversarial Training. In CVPR.
Taigman, Y.; Polyak, A.; and Wolf, L. 2017. Unsupervised CrossDomain Image Generation. In $I C L R, 1-15$.
Tzeng, E.; Hoffman, J.; Darrell, T.; Saenko, K.; and Lowell, U. 2015. Simultaneous Deep Transfer Across Domains and Tasks. In ICCV.
Tzeng, E.; Hoffman, J.; Saenko, K.; and Darrell, T. 2017. Adversarial Discriminative Domain Adaptation. In CVPR.
Vicon. 2017. ViconMotion Capture Systems: Viconn.
Vincent, P.; Larochelle, H.; Lajoie, I.; Bengio, Y.; and Manzagol, P.-A. 2010. Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion Pierre-Antoine Manzagol. Journal of Machine Learning Research 11:3371-3408.
Xiao, Z.; Wen, H.; Markham, A.; and Trigoni, N. 2014. Lightweight map matching for indoor localization using conditional random fields. In International Conference on Information Processing in Sensor Networks (IPSN), 131-142.
Xiao, Z.; Wen, H.; Markham, A.; and Trigoni, N. 2015. Robust indoor positioning with lifelong learning. IEEE Journal on Selected Areas in Communications 33(11):2287-2301.
Yi, Z.; Zhang, H.; Tan, P.; and Gong, M. 2017. DualGAN: Unsupervised Dual Learning for Image-to-Image Translation. In ICCV, 2868-2876.
Yu, L.; Zhang, W.; Wang, J.; and Yu, Y. 2017. SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient. In AAAI, 28522858.</p>
<p>Zhao, M.; Yue, S.; Katabi, D.; Jaakkola, T. S.; and Bianchi, M. T. 2017. Learning Sleep Stages from Radio Signals: A Conditional Adversarial Architecture. ICML 70:4100-4109.
Zhu, J.-y.; Park, T.; Efros, A. A.; Ai, B.; and Berkeley, U. C. 2017. Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. In ICCV.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Copyright (c) 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
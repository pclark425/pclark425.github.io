<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1019 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1019</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1019</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-247779091</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2203.15172v1.pdf" target="_blank">Assessing evolutionary terrain generation methods for curriculum reinforcement learning</a></p>
                <p><strong>Paper Abstract:</strong> Curriculum learning allows complex tasks to be mastered via incremental progression over 'stepping stone' goals towards a final desired behaviour. Typical implementations learn locomotion policies for challenging environments through gradual complexification of a terrain mesh generated through a parameterised noise function. To date, researchers have predominantly generated terrains from a limited range of noise functions, and the effect of the generator on the learning process is underrepresented in the literature. We compare popular noise-based terrain generators to two indirect encodings, CPPN and GAN. To allow direct comparison between both direct and indirect representations, we assess the impact of a range of representation-agnostic MAP-Elites feature descriptors that compute metrics directly from the generated terrain meshes. Next, performance and coverage are assessed when training a humanoid robot in a physics simulator using the PPO algorithm. Results describe key differences between the generators that inform their use in curriculum learning, and present a range of useful feature descriptors for uptake by the community.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1019.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1019.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bipedal Walker (sim)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulated Bipedal Walker Humanoid</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated bipedal humanoid trained with Proximal Policy Optimization (PPO) starting from a base policy on flat ground; used to evaluate curricula of procedurally/evolutionarily generated terrains (MAP-Elites archives) and study effects of environment complexity and variation on learning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Simulated Bipedal Walker (humanoid)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simulated humanoid locomotion agent trained with reinforcement learning: Proximal Policy Optimization (PPO) implementation from SpinningUp, initialized from a base policy trained on flat ground; map traversal / curriculum selection uses the IT&E-style Gaussian Process (GP) method.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Evolved Terrain Curricula (MAP-Elites archives of terrains from Perlin, Diamond-Square, Worley, GAN, CPPN, and Combined)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments are 256x256 heightmap-derived terrain meshes converted to simulation scenes. Terrains vary in geometric features (e.g., roughness, topographic position index (TPI), terrain ruggedness index (TRI)), and are generated by five generators (Perlin noise, Diamond Square noise, Worley noise, a DCGAN trained on real terrain patches, and CPPN) and combinations thereof. Complexity arises from terrain difficulty (how far the walker can traverse) and local geometric heterogeneity; variation arises from different generator types, generator parameterisations, and MAP-Elites archive diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Terrain difficulty = average distance travelled on the terrain (best 5 of 20 attempts with random joint initialisations), normalized to [0,1]; fitness = 1.0 - difficulty (so lower difficulty => higher fitness). Feature-based complexity descriptors used: Terrain Ruggedness Index (TRI), Topographic Position Index (TPI), and Roughness (max local difference); euclidean distance in feature-space from (0,0) used as a proxy for increasing difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Quantitative range used in paper: difficulty examples ‚âà 0.1 (easy), ‚âà 0.5 (medium), ‚âà 0.9 (hard); highest-difficulty terrains are those with large euclidean feature distance in the MAP-Elites grid.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation measured by generator type and MAP-Elites coverage: five generator sources (Perlin, Diamond-Square, Worley, GAN, CPPN) plus a Combined map; MAP-Elites discretisation of 50 bins per descriptor; coverage percent of total map per generator (Table 1) and the number/diversity of unique terrains in archives. GAN latent vector size = 50; noise/generator genome parameter ranges also provide variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Reported coverage percentages (Roughness/TPI map): CPPN 23.16% (high variation/coverage), GAN 2.48% (low), Perlin 3.92%, Worley 2.88%, Diamond Square 3.12%; Combined 23.64%. Thus CPPN/Combined = high variation, GAN = low variation, noise generators = medium-low variation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Primary: 'hardest terrain successfully traversed' measured by euclidean feature distance (from (0,0)) and by terrain difficulty/traversal distance; secondary: learning speed measured as number of training epochs required to reach terrain milestones (Table 2). Also fitness used during evolution = 1 - difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: CPPN and Combined maps achieve the highest final difficulty and outperform other generators in hardest-terrain reached (Figure 7). Quantitative epoch milestones (from Table 2): CPPN reaches 5% at 40 epochs, 10% at 80, 15% at 120, 20% at 240, 25% at 960, 30% at 4720, 35% at 7000, 40% at 9880, 45% at 13280 epochs (50% not reached within 30k); Combined reaches 5% at 80, 10% at 120, 15% at 160, 20% at 320, 25% at 440, 30% at 3720, 35% at 7280, 40% at 10280, 45% at 15040, 50% at 29000 epochs. GAN and Diamond-Square reach only low-to-moderate milestones (GAN: 5% at 40, 10% at 80, 15% at 1000 epochs; Diamond Square: 5% at 80, 10% at 160, 15% at 1280). Perlin learns markedly faster than Worley despite similar coverage (qualitative; Perlin achieves designated milestones earlier in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes ‚Äî the paper explicitly analyses trade-offs and relationships: indirect representations (CPPN) produce much higher coverage of the feature map and can generate both more diverse and more difficult terrains (high complexity, high variation), whereas GAN (also indirect but trained on real data) produces limited coverage (low variation) and is constrained to terrains similar to training data. Direct noise generators (Perlin, Worley, Diamond-Square) occupy a middle ground. Importantly, combining generators (Combined map) yields faster achievement of difficulty milestones than CPPN alone: starting on higher-fitness (easier) noise terrains then transitioning to CPPN terrains accelerates learning, indicating a trade-off where initial low-complexity/easier-but-varied terrains help speed learning toward high-complexity terrains. Also, a simple single-feature curriculum (increasing roughness) learns slower and reaches lower final difficulty than the MAP-Elites map-traversal curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>CPPN (high complexity & high variation) ‚Äî achieves the highest final traversability milestones among single generators; epoch milestones (examples) ‚Äî 25% map distance in 960 epochs, 40% map distance by 9880 epochs, 45% by 13280 epochs (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>GAN (low variation, limited complexity): reaches modest milestones only (e.g., 5% in 40 epochs, 10% in 80 epochs, 15% in 1000 epochs) and did not reach higher milestones within experimental budget.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning implemented via MAP-Elites archives of terrains (representation-agnostic feature descriptors) combined with an IT&E-style Gaussian Process (GP) map traversal to select terrains; PPO for policy updates. The algorithm removes terrains estimated to be easier than the current training terrain to incrementally increase difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Measured in training epochs (PPO epochs). Maximum training budget per trial was 30,000 epochs. Milestone epoch counts are reported in Table 2 (examples: CPPN: 25% in 960 epochs; Combined: 50% in 29,000 epochs). Learning was evaluated averaged over 10 trials with standard-error bands.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Representation-agnostic feature descriptors (TPI, Roughness, TRI) correlate with traversability and allow comparison of direct and indirect terrain generators in MAP-Elites curricula. 2) CPPN (indirect) yields the largest archive coverage and can generate the most difficult terrains; GAN (indirect, data-trained) has the least coverage. 3) Perlin noise produced terrains that enabled faster early learning than Worley noise despite similar coverage, suggesting generator regularity affects policy generalisability. 4) A Combined multi-generator map attained difficulty milestones faster than a CPPN-only map, indicating benefit from mixing generator types (start on easier noise terrains then transition to CPPN). 5) The MAP-Elites map-based curriculum with GP traversal outperformed a standard single-feature incrementally-increasing curriculum in learning speed and final achievable difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing evolutionary terrain generation methods for curriculum reinforcement learning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Robots that can adapt like animals <em>(Rating: 2)</em></li>
                <li>Learning quadrupedal locomotion over challenging terrain <em>(Rating: 2)</em></li>
                <li>Automatic goal generation for reinforcement learning agents <em>(Rating: 2)</em></li>
                <li>Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions <em>(Rating: 2)</em></li>
                <li>Guided Curriculum Learning for Walking Over Complex Terrain <em>(Rating: 1)</em></li>
                <li>Increasing generality in machine learning through procedural content generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1019",
    "paper_id": "paper-247779091",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Bipedal Walker (sim)",
            "name_full": "Simulated Bipedal Walker Humanoid",
            "brief_description": "A simulated bipedal humanoid trained with Proximal Policy Optimization (PPO) starting from a base policy on flat ground; used to evaluate curricula of procedurally/evolutionarily generated terrains (MAP-Elites archives) and study effects of environment complexity and variation on learning performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Simulated Bipedal Walker (humanoid)",
            "agent_description": "A simulated humanoid locomotion agent trained with reinforcement learning: Proximal Policy Optimization (PPO) implementation from SpinningUp, initialized from a base policy trained on flat ground; map traversal / curriculum selection uses the IT&E-style Gaussian Process (GP) method.",
            "agent_type": "simulated agent",
            "environment_name": "Evolved Terrain Curricula (MAP-Elites archives of terrains from Perlin, Diamond-Square, Worley, GAN, CPPN, and Combined)",
            "environment_description": "Environments are 256x256 heightmap-derived terrain meshes converted to simulation scenes. Terrains vary in geometric features (e.g., roughness, topographic position index (TPI), terrain ruggedness index (TRI)), and are generated by five generators (Perlin noise, Diamond Square noise, Worley noise, a DCGAN trained on real terrain patches, and CPPN) and combinations thereof. Complexity arises from terrain difficulty (how far the walker can traverse) and local geometric heterogeneity; variation arises from different generator types, generator parameterisations, and MAP-Elites archive diversity.",
            "complexity_measure": "Terrain difficulty = average distance travelled on the terrain (best 5 of 20 attempts with random joint initialisations), normalized to [0,1]; fitness = 1.0 - difficulty (so lower difficulty =&gt; higher fitness). Feature-based complexity descriptors used: Terrain Ruggedness Index (TRI), Topographic Position Index (TPI), and Roughness (max local difference); euclidean distance in feature-space from (0,0) used as a proxy for increasing difficulty.",
            "complexity_level": "Quantitative range used in paper: difficulty examples ‚âà 0.1 (easy), ‚âà 0.5 (medium), ‚âà 0.9 (hard); highest-difficulty terrains are those with large euclidean feature distance in the MAP-Elites grid.",
            "variation_measure": "Variation measured by generator type and MAP-Elites coverage: five generator sources (Perlin, Diamond-Square, Worley, GAN, CPPN) plus a Combined map; MAP-Elites discretisation of 50 bins per descriptor; coverage percent of total map per generator (Table 1) and the number/diversity of unique terrains in archives. GAN latent vector size = 50; noise/generator genome parameter ranges also provide variation.",
            "variation_level": "Reported coverage percentages (Roughness/TPI map): CPPN 23.16% (high variation/coverage), GAN 2.48% (low), Perlin 3.92%, Worley 2.88%, Diamond Square 3.12%; Combined 23.64%. Thus CPPN/Combined = high variation, GAN = low variation, noise generators = medium-low variation.",
            "performance_metric": "Primary: 'hardest terrain successfully traversed' measured by euclidean feature distance (from (0,0)) and by terrain difficulty/traversal distance; secondary: learning speed measured as number of training epochs required to reach terrain milestones (Table 2). Also fitness used during evolution = 1 - difficulty.",
            "performance_value": "Qualitative: CPPN and Combined maps achieve the highest final difficulty and outperform other generators in hardest-terrain reached (Figure 7). Quantitative epoch milestones (from Table 2): CPPN reaches 5% at 40 epochs, 10% at 80, 15% at 120, 20% at 240, 25% at 960, 30% at 4720, 35% at 7000, 40% at 9880, 45% at 13280 epochs (50% not reached within 30k); Combined reaches 5% at 80, 10% at 120, 15% at 160, 20% at 320, 25% at 440, 30% at 3720, 35% at 7280, 40% at 10280, 45% at 15040, 50% at 29000 epochs. GAN and Diamond-Square reach only low-to-moderate milestones (GAN: 5% at 40, 10% at 80, 15% at 1000 epochs; Diamond Square: 5% at 80, 10% at 160, 15% at 1280). Perlin learns markedly faster than Worley despite similar coverage (qualitative; Perlin achieves designated milestones earlier in Table 2).",
            "complexity_variation_relationship": "Yes ‚Äî the paper explicitly analyses trade-offs and relationships: indirect representations (CPPN) produce much higher coverage of the feature map and can generate both more diverse and more difficult terrains (high complexity, high variation), whereas GAN (also indirect but trained on real data) produces limited coverage (low variation) and is constrained to terrains similar to training data. Direct noise generators (Perlin, Worley, Diamond-Square) occupy a middle ground. Importantly, combining generators (Combined map) yields faster achievement of difficulty milestones than CPPN alone: starting on higher-fitness (easier) noise terrains then transitioning to CPPN terrains accelerates learning, indicating a trade-off where initial low-complexity/easier-but-varied terrains help speed learning toward high-complexity terrains. Also, a simple single-feature curriculum (increasing roughness) learns slower and reaches lower final difficulty than the MAP-Elites map-traversal curriculum.",
            "high_complexity_low_variation_performance": "null",
            "low_complexity_high_variation_performance": "null",
            "high_complexity_high_variation_performance": "CPPN (high complexity & high variation) ‚Äî achieves the highest final traversability milestones among single generators; epoch milestones (examples) ‚Äî 25% map distance in 960 epochs, 40% map distance by 9880 epochs, 45% by 13280 epochs (Table 2).",
            "low_complexity_low_variation_performance": "GAN (low variation, limited complexity): reaches modest milestones only (e.g., 5% in 40 epochs, 10% in 80 epochs, 15% in 1000 epochs) and did not reach higher milestones within experimental budget.",
            "training_strategy": "Curriculum learning implemented via MAP-Elites archives of terrains (representation-agnostic feature descriptors) combined with an IT&E-style Gaussian Process (GP) map traversal to select terrains; PPO for policy updates. The algorithm removes terrains estimated to be easier than the current training terrain to incrementally increase difficulty.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Measured in training epochs (PPO epochs). Maximum training budget per trial was 30,000 epochs. Milestone epoch counts are reported in Table 2 (examples: CPPN: 25% in 960 epochs; Combined: 50% in 29,000 epochs). Learning was evaluated averaged over 10 trials with standard-error bands.",
            "key_findings": "1) Representation-agnostic feature descriptors (TPI, Roughness, TRI) correlate with traversability and allow comparison of direct and indirect terrain generators in MAP-Elites curricula. 2) CPPN (indirect) yields the largest archive coverage and can generate the most difficult terrains; GAN (indirect, data-trained) has the least coverage. 3) Perlin noise produced terrains that enabled faster early learning than Worley noise despite similar coverage, suggesting generator regularity affects policy generalisability. 4) A Combined multi-generator map attained difficulty milestones faster than a CPPN-only map, indicating benefit from mixing generator types (start on easier noise terrains then transition to CPPN). 5) The MAP-Elites map-based curriculum with GP traversal outperformed a standard single-feature incrementally-increasing curriculum in learning speed and final achievable difficulty.",
            "uuid": "e1019.0",
            "source_info": {
                "paper_title": "Assessing evolutionary terrain generation methods for curriculum reinforcement learning",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Robots that can adapt like animals",
            "rating": 2,
            "sanitized_title": "robots_that_can_adapt_like_animals"
        },
        {
            "paper_title": "Learning quadrupedal locomotion over challenging terrain",
            "rating": 2,
            "sanitized_title": "learning_quadrupedal_locomotion_over_challenging_terrain"
        },
        {
            "paper_title": "Automatic goal generation for reinforcement learning agents",
            "rating": 2,
            "sanitized_title": "automatic_goal_generation_for_reinforcement_learning_agents"
        },
        {
            "paper_title": "Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions",
            "rating": 2,
            "sanitized_title": "paired_openended_trailblazer_poet_endlessly_generating_increasingly_complex_and_diverse_learning_environments_and_their_solutions"
        },
        {
            "paper_title": "Guided Curriculum Learning for Walking Over Complex Terrain",
            "rating": 1,
            "sanitized_title": "guided_curriculum_learning_for_walking_over_complex_terrain"
        },
        {
            "paper_title": "Increasing generality in machine learning through procedural content generation",
            "rating": 1,
            "sanitized_title": "increasing_generality_in_machine_learning_through_procedural_content_generation"
        }
    ],
    "cost": 0.01144575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Assessing Evolutionary Terrain Generation Methods for Curriculum Reinforcement Learning
29 Mar 2022</p>
<p>David Howard david.howard@csiro.au 
Josh Kannemeyer 
Davide Dolcetti 
Humphrey Munn 
Nicole Robinson </p>
<p>BrisbaneQueenslandAustralia</p>
<p>Monash University Melbourne
VictoriaAustralia</p>
<p>Queensland University of Technology Brisbane
QueenslandAustralia</p>
<p>University of Queensland Brisbane
QueenslandAustralia</p>
<p>Monash University Melbourne
VictoriaAustralia</p>
<p>GECCO '22
July 9-132022BostonUSA</p>
<p>Assessing Evolutionary Terrain Generation Methods for Curriculum Reinforcement Learning
29 Mar 2022CA3A165F1B4CC8B1ED125ADE61577B9710.1145/nnnnnnn.nnnnnnnarXiv:2203.15172v1[cs.NE]reinforcement learningprocedural content generationCPPNGANrepresentationsquality-diversitycurriculum learning
Figure 1: System overview: (a) Evolved terrain representations populate (b) 2D pixel maps and are subsequently turned into (c) terrain meshes which fill a (d) MAP-Elites archive.A bipedal walker is subsequently trained via PPO (e) and its learning performance is used to assess the impact of the various generators.</p>
<p>INTRODUCTION</p>
<p>Curriculum Learning (CL) [5,24,34] is a powerful reinforcement learning technique that engenders powerful behaviours by gradually increasing the difficulty of the task to be solved.This incremental approach has numerous benefits including performance and generalisation ability.To maximise the benefit of a curriculum, it is important to have a training environment that increases in complexity proportional to the learning ability of the agent, and contains relevant features to encourage learning.Diverse training examples, presented to the learner in an appropriate order [15], have resulted in the generation of capable, complex behaviours for simulated robots [34,37], and aided sim2real transfer of those policies to real robot deployments [21].</p>
<p>A typical setup for learning locomotion skills (a popular focus area) involves the creation of a set of terrains of varying difficulty through some terrain generator, typically a parameterised noise function.The terrains are then presented to the agent, generally in order of difficulty, and the agent learns by solving simpler examples to eventually reach a given target behaviour.Noise functions are popular candidate terrain generators, as increasing terrain difficulty can be simply realised by increasing values of the noise parameters.Different generators produce terrains with different geometric features, which has an affect on learning.</p>
<p>To date, the impact of the chosen terrain generator has not been explored, and this is the focus of our paper.We address the effects of terrain generator on the learning process in two steps.In step 1, we select a varied range of popular terrain generators from the literature, which includes both direct and indirect encodings.Because the difficulty of an indirectly-coded terrain cannot be ascertained a priori, we categorise the resulting terrain mesh according to a set of representation-agnostic features selected from related literature.The selected features become feature descriptors in MAP-Elites, providing a diverse set of high-quality terrains.Terrains are evolved to fill the archive and generate one curriculum per generator type.In step two, each curriculum is used to train a bipedal walker, and results compare reachable terrain difficulty and learning speed.Our approach is illustrated in Figure 1.</p>
<p>We present two main original contributions; (i) the selection of appropriate representation-agnostic feature descriptors, including coverage analysis of curricula evolved using those features, and (ii) analysis of the effects of generator type on learning performance and rate.Results show the identification of suitable feature descriptors for representation-agnostic terrain-based curriculum learning, and suggest key differences between the generators, particularly between direct and indirect representations in terms of map coverage.We provide evidence that a multi-generator approach may be beneficial to the generation of curricula that promote rapid learning.</p>
<p>The remainder of the paper is organised as follows; Section 2 presents pertinent background research.Section 3 describes our methodology.Section 4 presents experiments and results, and Section 5 provides a discussion.</p>
<p>BACKGROUND</p>
<p>Reinforcement Learning (RL) [20] is one of the most commonly used methods to train agent behaviours, and is similar conceptually to an evolutionary algorithm which learns by continually interacting with (and being rewarded by) an environment.In this study we use Proximal Policy Optimisation (PPO) [32] due to its ubiquity as a benchmark method to train agents in complex environments.PPO is a policy gradient method that alternates between sampling data through the agent's interaction with the environment and optimising an objective function using stochastic gradient descent.PPOs main advantage over other policy gradient techniques is the use of multiple epochs of mini-batch updates rather than performing one gradient update per data sample.</p>
<p>Curriculum learning [5] is a form of incremental learning [17], designed to address some common challenges with RL [24].Firstly, it allows for agents to solve very hard problems by progressively building competency in easier versions of the problem [6].Secondly, it provides a wealth of learning experiences to the agent which can greatly assist with generalisation [16,19,25].Lee et al. [21] demonstrate both of these benefits using a particle filter to update parameters of a noise function-based terrain generator, allowing complex terrains to be navigated by a real quadruped after simulated learning.</p>
<p>A guided curriculum approach [34] incrementally removed supporting forces from a bipedal walker's body before increasing terrain complexity and diversity.Accelerated learning of complex environments is also evidenced with hexapod robots [28].Miras demonstrated that balancing behaviour could be achieved by training on a tilted plain rather than a flat plain with objects on it, even though balance was not incorporated into the fitness function of the robot [22].Huizinga [18] found that ordering of sub-tasks towards learning a difficult task is challenging, and instead proposed the Combinatorial Multi-Objective Evolutionary Algorithm (CMOEA) to simultaneously explore all orderings.This is an effective CL method when subtasks can be clearly defined (e.g.jumping, walking).Xie [37] demonstrated the critical role of a curriculum to train three bipedal agents to walk on stepping-stone scenarios, with final terrain complexity and learning rate being superior for curriculum approaches compared to non-curriculum RL.Akkaya [3] presents a CL approach using automatic domain randomization (ADR), demonstrating vastly improved sim2real transfer compared to a non-curriculum baseline.ADR automatically expands the randomisation range parameterising a distribution over environments.Florensa [12] showed that using a curriculum generating network it is possible to train an agent to 'perform a wide set of tasks without requiring any prior knowledge of its environment'.Open-ended curricula [38] can learn on terrains that are continually generated during agent learning [35].</p>
<p>Procedural Content Generation (PCG) originated in computer graphics and video game design.PCG can create large volumes of high-quality content with controllable randomness, including digital objects, landscapes, levels, textures and 3D models.PCG has been readily adopted by the machine learning community [30] to create a wealth of training data.Rich and diverse environments encourage agent learning and improve robustness [15], with works showing that curriculum learning can train several quadrupedal robot policies in parallel to walk over uneven terrains in simulation [31].Terrain generators have been previously evolved [14,26], however these works focus only on a single representation and do not evaluate terrains in an agent-based learning context.Also in an evolutionary context, quality-diversity algorithms [27] present an effective method for storing and traversing a curriculum, shown in both robotics and game-playing contexts [9] [4].In particular,the use of aligned feature dimensions within MAP-Elites [23] can provide direction to a (stochastic) traversal algorithm.</p>
<p>Overall, we see that curriculum learning is a powerful technique relevant to both evolutionary algorithms and reinforcement learning.The literature abounds with a smorgasbord of both direct and indirectly-represented generators, including various noise functions as well as CPPNs and GANs.Additionally, we see many different feature dimensions being used to store the curriculum and permit traversal.However, we identify a significant literature gap in that these different selections of generators and features have not yet been compared.In this study we compare popular generators and a set of selected feature descriptors for an evolved MAP-Elites based curriculum.We attempt to inform the selection of appropriate terrain generators and feature dimensions for fellow researchers in the field.We also demonstrate a curriculum learning approach that simultaneously supports both direct and indirect representations, opening up future work in mixed-generator curricula.</p>
<p>METHODOLOGY</p>
<p>We follow a three-stage process: (i) decide on features, (ii) evolve curricula (iii) learn on curricula.To allow for the isolated study of generators, we omit other tasks that typically comprise a curriculum (steps, jumps, etc.) and focus purely on locomotion in rough terrains.</p>
<p>Generators</p>
<p>We select 5 popular and diverse generators from the literature: 3 direct (Perlin noise, Diamond Square Noise, Worley Noise) and two indirect (a GAN, and a CPPN).Each generator mapped to a 256x256 resolution pixel map which is converted to a heightmap and then to a solid terrain mesh for simulation.</p>
<p>Perlin Noise. Perlin noise was applied to fractal Brownian Motion (fBM). The genome was parameterised into scale [1-100],</p>
<p>octaves [1][2][3][4][5][6][7][8][9], persistence [0.1-0.9],lacunarity [1][2][3] and a random seed [0-100].</p>
<p>3.1.2Diamond Square Noise.We use a version of the diamond square noise algorithm [13].The four corner points of the heightmap grid are initialised with a random value between -1 and 1 making a square.The following two steps alternate until all the values of the heightmap are assigned:</p>
<p>Diamond step: for each square in the grid, assign the value of the midpoint of that square to be a random percentage P of the average of the four corner points, plus a random value R between -1 and 1.</p>
<p>Square step: for each diamond in the grid, assign the value of the midpoint of the diamond to be a random percentage P of the average of the four corner points, plus a random value R between -1 and 1.</p>
<p>After each iteration of these steps, the range of the random value R is reduced using the formula:
‚àí1 (ùë†ùë°ùëíùëùùë† * ùê∑ + 1) ‚â§ ùëÖ ‚â§ 1 (ùë†ùë°ùëíùëùùë† * ùê∑ + 1)
With D ‚àà [1,10], and steps corresponding to the number of iterations of the above two steps performed.P is computed as:
ùëç ‚â§ ùëÉ ‚â§ 100 ‚àí ùëç, ùëç ‚àà [0, 50]
The genome is represented by a random seed, the four initial corner values, percentile (Z), and level (D).</p>
<p>Worley Noise.</p>
<p>Worley Noise is typically used to generate procedural textures, and is implemented following [36].The genome for the Worley terrain generator consists of a seed for randomisation, the number of feature points N ‚àà [2, 400] and the index of the point sorted by ascending distance to the current point D ‚àà [0,   4 ].3.1.4GAN.The Deep Convolutional GAN (DCGAN) model was trained on real terrain heightmap data with identical resolution to the humanoid in simulation.The original data was in pointcloud format obtained from the OpenTopography website.The ground plane was semantically segmented, and the resulting points interpolated into multiple heightmap patches at the desired resolution.This provided a large amount of high resolution data (30000 256x256 patches) which was used to train the model.The latent vector fed into the generator contained 50 values which formed the genome of the GAN generator.</p>
<p>3.1.5CPPN.The CPPN terrain generator has 2 inputs corresponding to row and column of the heightmap, and one output for the height value.All remaining CPPN settings were taken from the PicBreeder paper [33].</p>
<p>Features</p>
<p>We explore a range of generic terrain descriptors for use as MAP-Elites feature descriptors that allow both direct and indirect generators to be compared.Features were aligned to terrain traversability to provide continuity to the final generated curricula, with enough variation to create diversity in the population.Features originate from a range of non-RL, non-evolutionary fields, mainly from surveying where they are used to categorise real terrains.We use:</p>
<p>(1) Terrain Ruggedness Index (TRI) [29] -measures the average difference between a pixel and its 8 neighbouring pixels.(2) Topographic Position Index (TPI) [10] -measures the difference between each pixel and the mean of its 8 neighbouring pixels (3) Roughness [11] -measures the maximum difference between a pixel and its 8 neighbouring pixels (4) Traversability Estimation Model implemented in [7] -A trained model that predicts the traversability of terrains by outputting a traversability map.This model had been trained on different types of terrains to the terrain generators used in this paper.The orientation input to the model was set to 0 as this was the direction the humanoid traversed.</p>
<p>Once generated, a terrain mesh is tagged with values for each feature descriptor.Settings for kernel size are determined in Section 4. Roughness, TPI and TRI use a stride length of 2. The average of the output from each feature was taken as the overall measure.Initial experimentation found that an archive discretisation of 50 bins per descriptor presented meaningful but achievable differences between terrains in neighbouring cells.</p>
<p>Evolutionary Algorithm</p>
<p>We evolve curriculums for each generator, and compare different pairs of feature dimensions.Per treatment, we run MAP-Elites for 5000 generations, with 100 random initial genomes.Per generation, 20 new terrains were generated by randomly selecting from the current members of the archive, mutating, generating feature values, and adding back into the archive as in Figure 2.For noise emitters, each gene in the genome had a mutation probability of 0.35, and mutation altered the allele by ¬± a value taken from a normal distribution with covariance set to 10% of its range.CPPN mutation followed NEAT [33].The GAN's latent space was stored in 50 variables, which with P=0.07 were mutated by a value taken from a normal distribution with covariance set to 10% of its range (heuristically determined).</p>
<p>Fitness is set to 1.0 -the difficulty of the generated terrain.Difficulty is assessed using the base policy of the Bipedal Walker that is trained on flat ground [8] and simulated in PyBullet.Difficulty of a terrain is calculated as the average distance travelled out of the best 5 of 20 attempts with random joint initialisations, normalised between 0 and 1.The 5 best were taken as some initialisations were too extreme and caused a large amount of noise in the fitness estimation.When replacing individuals in a MAP-Elites cell, we kept the lowest difficulty terrain and deleted the higher difficulty one.This generated a smoother curriculum in terms of fitness, whilst also removing many impossible terrains.A second step removed the remaining impossible terrains, following Algorithm 1. Representative terrains for two generators (CPPN and Perlin noise) that achieve difficulties of ‚âà0.1, ‚âà 0.5, and ‚âà 0.9 are shown in Figure 3.</p>
<p>EXPERIMENTATION 4.1 Experiment 1: Features and Curriculum</p>
<p>Initial experimentation compared feature descriptors with different kernel sizes to measure their alignment to terrain difficulty (fitness).This alignment was ideal for constraining the solution search space to produce a strong traversability gradient in the MAP-Elites grid.Without a strong gradient, creating an effective curriculum becomes difficult.Roughness, TPI and TRI had similar results with TPI being most correlated.Results are shown in Figure 4. We used TPI in learning experiments as it was most aligned, and use Roughness since TPI and TRI (k = 30) were too similar (r = 0.94/0.95) to produce sufficient map diversity.A kernel size of 30 was chosen as it had consistently high correlations to terrain difficulty.These features could then be used for arbitrary meshes with confidence that they were meaningful metrics.The traversability model ("Traversability") had the weakest correlation to difficulty so this was not used further.The coverage of each terrain generator is shown in Figure 5. Pairwise feature comparisons are shown in Figure 6.All coverage maps show a strong gradient in terrain traversability, with it declining as the map feature values increase.This suggests the chosen features are very representative of traversability.Some variation exists between the different features, but the most significant basis of variation is in the euclidean distance between feature values and (0,0).Each terrain generator presents a characteristic shape and amount of coverage on the map.Table 1 shows the comparative coverage between generators, with CPPN having significantly higher coverage, and able to create more difficult and more diverse terrains than the other generators.Interestingly the other indirect emitter, GAN, presents the lowest coverage of all.The GAN was trained on real terrains and therefore can only output similar meshes.The CPPN is unrestricted in this manner.Perlin noise shows the strongest diagonal mapping to the feature dimensions and presents the best direct encoding, followed by Worley noise.Diamond-square noise can be seen to struggle to generate reasonable values for TPI.Feature ranges were identical between generator coverage maps for fair comparison.While the total terrains in each map vary significantly based on coverage, adjusting feature ranges to account for this does not help learning as terrains with comparable features are similarly traversable and are skipped.</p>
<p>Experiment 2: Bipedal Walker Learning</p>
<p>In the second experiment we carry out a typical learning experiment on the evolved curricula.We trained the walker from the base policy using the PyTorch PPO implementation from SpinningUp [2] with default parameters with a maximum of 30,000 epochs.The base policy provides the prior for IT&amp;E.Traversal of the map is the same Gaussian Process (GP) technique as described by Cully et al. [1], using the same recommended parameters:  = 0.4,  = 0.9,  = 0.05,  2  = 0.001, and the same covariance function (Matern kernel).PPO trains for 40 epochs at a time until the humanoid reaches the 90% traversability threshold (90% of maximum fitness achieved) or when there had been 100 traversability evaluations on the terrain.Once this condition is met, the GP model finds the highest fitness (easiest) terrain from evaluations with the updated PPO model.However, unlike the original IT&amp;E, our algorithm removes terrains that are estimated by the model to be easier than the terrain just trained on at each update.This ensures that the algorithm incrementally increases difficulty.This also avoids the problem of the curriculum visiting previously easy terrains that may appear hard after learning on complex environments which require a significant change in policy.The learning process ends when all terrains are used.To assess the performance of each terrain generator's curriculum map, the hardest terrain successfully traversed at each training epoch was recorded.This is shown in Figure 7, and includes training using a combined map with the highest fitness terrain chosen when grid squares overlap between generators.Hardest terrains are measured in terms of how large their feature values are, which is the euclidean feature distance from (0,0).CPPN and Combined significantly outperform the other four generators with very similar large coverage profiles.CPPN and combined are similar in performance.Diamond Square Noise and the GAN perform similarly, and have similar coverage maps.Worley and Perlin Noise also obtain a similar final difficulty, and have similarly shaped coverage maps although Perlin covers more of the archive.Learning speed is another key determinant of generator performance -Fig.7. We measure this by designating terrains at a certain Euclidean distance from the top-right corner of the map, and recording the iteration at which the walker learns one of those terrains.Table 2 shows some interesting results.First, the Perlin curriculum learns much faster than the Worley curriculum, despite covering comparable archive area.The high regularity of Perlin Noise may account for this with policies that are more generalisable between Perlin Noise terrains.Second, although CPPN and combined maps achieve comparable difficulty, a combined map that starts on higher-fitness noise terrains before transitioning to CPPN terrains achieves difficulty milestones much faster than the single CPPN generator.The result suggests that combining different generators in a single curriculum may be beneficial, however investigating is out of scope for the current study.Finally, the effect of the map creation and map-traversal algorithm on performance against classic curriculum learning with a single feature (Figure 7(b)).CPPN terrains from the map were sorted by their roughness, and every fifth terrain was used, similar to standard CL by incrementally increasing a difficulty parameter.This trained much slower and did not succeed on terrains as difficult as in the map-based algorithm.</p>
<p>DISCUSSION</p>
<p>In this paper we investigated the utility of different feature descriptors for map-based curricula.Using a ground truth of how hard a terrain was for a pre-trained bipedal walker, we showed how several feature descriptors taken from the literature allow for the assessment of meaningful difficulty purely from a terrain mesh.Using pairwise combinations of features as MAP-Elites feature dimensions, we then showed the effect of feature dimension selection on archive coverage and fill pattern.</p>
<p>In a second step, we used our selected feature descriptors to build a MAP-Elites-based curriculum for different terrain generation algorithms, including direct and indirect representation.Using our representation-agnostic feature descriptors, we could directly compare archive properties.We show that CPPNs present the best coverage, and make the most difficult terrains learnable by the agent.GANs were most limited in terms of coverage, with noise generators taking up the middle ground.Perlin and Worley noise in particular presented useful coverage patterns.Interestingly, Perlin noise allowed for much faster learning (achievement of terrain milestones) than Worley.Even more interestingly, we see that a combined map presents significantly faster learning than a pure CPPN map, as it learns first on fitter (easier) terrains (where cells are shared with multiple generators) before transitioning to pure CPPN terrains that the other generators cannot reach.</p>
<p>Future work will investigate this, as well as exploring the effects of hyperparameter tuning (all parameters were set to default), as well as the generalisability of these findings to other high-impact problems.However, purely in terms of learning about how agentbased curriculum RL works, this paper presents several important contributions for the research community.Future work will also apply the methods presented to the training of real robots, where the effect of generator types and the map-based curriculum may be able to contribute in narrowing the sim2real gap in addition to increasing learning speed.Moreover, game-playing agents could benefit from this curriculum technique.Further exploration of feature descriptors should be conducted in different curriculum contexts to measure their influence on policy performance.We hope this will lead to more terrain generator methods used in curriculum learning, as well as more principled selection of generator.</p>
<p>Figure 2 :
2
Figure 2: Overview of library generation.Features must be calculate in simulation before the terrain can be added to the library.</p>
<p>Algorithm 1 3 ‚ä≤Figure 3 :
133
Figure 3: Representative pixel maps with approximate difficulties of 0.1, 0.5, 0.9 respectively (L-R) for (a)-(c) an indirect representation (CPPN) and (d)-(f) a direct representation (Perlin noise).</p>
<p>Figure 4 :
4
Figure 4: Heatmaps showing Pearson correlation of all considered terrain characterisation features including ground truth: terrain difficulty, for (a) CPPN and (b) Perlin noise.</p>
<p>Figure 5 :
5
Figure 5: Feature coverage of each generator after each experiment.</p>
<p>Figure 6 :
6
Figure 6: MAP-Elites archive coverage for three combinations of features: TPI/TRI (row 1), roughness/TRI (row 2), roughness/TPI (row 3) for Diamond Square noise (a), Perlin noise (b), Worley noise (c), GAN (d) and CPPN (e) terrain generators after evolution.Colour corresponds to fitness, measured as the average performance of the base policy humanoid traversing each terrain.Dark colours represent fitter behaviours.</p>
<p>Figure 7 :
7
Figure 7: Illustrative performance comparison between (a) the different terrain generators, and (b) CPPN terrains between the proposed map-based curriculum algorithm and incrementally increasing roughness with standard CL, showing the most difficult terrain completed at the current training iteration.Plots finish when 30k epochs is reached or IT&amp;E has excluded all terrains.Averaged over 10 trials, with confidence shown for ¬±1 standard error.</p>
<p>Table 1 :
1
Coverage percent of total map for feature combinations with each terrain generator; maps from Figure5.
Terrain Generator Roughness/TPI Roughness/TRI TPI/TRICPPN23.16%15.52%14.92%GAN2.48%2.48%1.48%Perlin Noise3.92%3.92%2.80%Worley Noise2.88%1.84%2.12%Diamond Square3.12%1.36%1.88%Combined23.64%15.56%15.56%</p>
<p>Table 2 :
2
Showing learning speed: the number of training epochs required to learn a designated terrain a given % away from the maximum possible map distance (70.71).
Terrain Generator 5% 10% 15% 20% 25%30%35%40%45%50%CPPN40 80120 2409604720 7000 9880 13280-GAN40 80 1000-------Perlin Noise40 401603209608680----Worley Noise40 80480 2640 4160 14080----Diamond Square80 160 1280-------Combined80 120 160320440 3720 7280 10280 15040 29000</p>
<p>Robots that can adapt like animals. D , Tarapore A Cully, J Clune, J Mouret, 10.1038/nature14422Nature. 2015. 2015</p>
<p>Spinning Up in Deep Reinforcement Learning. Joshua Achiam, 2018. 2018</p>
<p>Solving rubik's cube with a robot hand. Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob Mc-Grew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, arXiv:1910.071132019. 2019arXiv preprint</p>
<p>Interactive Constrained MAP-Elites: Analysis and Evaluation of the Expressiveness of the Feature Dimensions. Alberto Alvarez, Jose Maria, Maria Font Fernandez, Steve Dahlskog, Julian Togelius, 10.1109/TG.2020.3046133IEEE Transactions on Games. 2020. 2020</p>
<p>Curriculum learning. Yoshua Bengio, J√©r√¥me Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>The Utility of Evolving Simulated Robot Morphology Increases with Task Complexity for Object Manipulation. Josh Bongard, 10.1162/artl.2010.Bongard.024Conference Name: Artificial Life. 16</p>
<p>Learning Ground Traversability From Simulations. R , Omar Chavez-Garcia, J√©r√¥me Guzzi, M Luca, Alessandro Gambardella, Giusti, 10.1109/LRA.2018.2801794IEEE Robotics and Automation Letters. 32018. 2018</p>
<p>Pybullet, a python module for physics simulation in robotics, games and machine learning. Erwin Coumans, Yunfei Bai, 2017</p>
<p>Robots that can adapt like animals. Antoine Cully, Jeff Clune, Danesh Tarapore, Jean-Baptiste Mouret, 10.1038/nature14422Nature. 5212015. May 2015</p>
<p>Application of the topographic position index to heterogeneous landscapes. Jeroen De Reu, Jean Bourgeois, Machteld Bats, Ann Zwertvaegher, Vanessa Gelorini, Philippe De Smedt, Wei Chu, Marc Antrop, Philippe De Maeyer, Peter Finke, Marc Van Meirvenne, Jacques Verniers, Philippe Cromb√©, 10.1016/j.geomorph.2012.12.015Geomorphology. 1862013. 2013</p>
<p>DEM resolution dependencies of terrain attributes across a landscape. Y Deng, J P Wilson, B O Bauer, 10.1080/13658810600894364International Journal of Geographical Information Science. 212007. 2007</p>
<p>Automatic goal generation for reinforcement learning agents. Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel, International conference on machine learning. PMLR2018</p>
<p>Computer Rendering of Stochastic Models. Alain Fournier, Don Fussell, Loren Carpenter, 10.1145/358523.358553Commun. ACM. 2561982. June 1982</p>
<p>Breeding Terrains with Genetic Terrain Programming: The Evolution of Terrain Generators. Miguel Frade, Francisco Vega, Carlos Cotta, 10.1155/2009/125714Int. J. Computer Games Technology. 122009. 2009. 2009</p>
<p>Nicolas Heess, T B Dhruva, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, S M Ali Eslami, Martin Riedmiller, David Silver, arXiv:1707.02286[cs.AI]Emergence of Locomotion Behaviours in Rich Environments. 2017</p>
<p>Sebastian Hofer, Kostas Bekris, Ankur Handa, Juan Camilo Gamboa, Melissa Mozifian, Florian Golemo, Chris Atkeson, Dieter Fox, Ken Goldberg, John Leonard, Karen Liu, Jan Peters, Shuran Song, Peter Welinder, Martha White, Sim2Real in Robotics and Automation: Applications and Challenges. 2021. 202118</p>
<p>Evolving spiking networks for turbulencetolerant quadrotor control. David Howard, Alberto Elfes, The Fourteenth International Conference on the Synthesis and Simulation of Living Systems. MIT Press201414</p>
<p>Evolving Multimodal Robot Behavior via Many Stepping Stones with the Combinatorial Multi-Objective Evolutionary Algorithm. Joost Huizinga, Jeff Clune, arXiv:1807.03392[cs.NE]2019</p>
<p>Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia Chernova, Dhruv Batra, Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance? IEEE robotics and automation letters. 2020. 20205</p>
<p>Reinforcement learning in robotics: A survey. Jens Kober, Andrew Bagnell, Jan Peters, The International Journal of Robotics Research. 32112013. 2013</p>
<p>Learning quadrupedal locomotion over challenging terrain. Joonho Lee, Jemin Hwangbo, Vladlen Lorenz Wellhausen, Marco Koltun, Hutter, 10.1126/scirobotics.abc5986Science Robotics. 5472020. 2020</p>
<p>Effects of environmental conditions on evolved robot morphologies and behavior. Karine Miras, A E Eiben, 10.1145/3321707.3321811Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation ConferencePrague Czech RepublicACM2019-07-13</p>
<p>Jean-Baptiste Mouret, Jeff Clune, arXiv:1504.04909[cs.AI]Illuminating search spaces by mapping elites. 2015</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, Peter Stone, arXiv:2003.049602020. 2020arXiv preprint</p>
<p>Alex Nichol, Vicki Pfau, Christopher Hesse, Oleg Klimov, John Schulman, arXiv:1804.03720[cs.LG]Gotta Learn Fast: A New Benchmark for Generalization in RL. 2018</p>
<p>Terrain generation using genetic algorithms. Teongjoo Ong, Ryan Saunders, John Keyser, John Leggett, 10.1145/1068009.10682412005</p>
<p>Quality diversity: A new frontier for evolutionary computation. Justin K Pugh, Lisa B Soros, Kenneth O Stanley, Frontiers in Robotics and AI. 3402016. 2016</p>
<p>Sim-to-real: Six-legged Robot Control with Deep Reinforcement Learning and Curriculum Learning. Yue Bangyu Qin, Yi Gao, Bai, 10.1109/ICRAE48301.2019.90438222019 4th International Conference on Robotics and Automation Engineering (ICRAE). 2019</p>
<p>A Terrain Ruggedness Index that Quantifies Topographic Heterogeneity. Shawn Riley, Stephen Degloria, S D Elliot, Internation Journal of Science. 51999. 01 1999</p>
<p>Increasing generality in machine learning through procedural content generation. Sebastian Risi, Julian Togelius, Nature Machine Intelligence. 22020. 2020</p>
<p>Nikita Rudin, David Hoeller, Philipp Reist, Marco Hutter, arXiv:2109.11978[cs.RO]arXiv:2109.11978 [cs.RO]Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning. 2021. 2021</p>
<p>. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, Proximal Policy Optimization Algorithms. 2017. 07 2017</p>
<p>Picbreeder: A Case Study in Collaborative Evolutionary Exploration of Design Space. Jimmy Secretan, Nicholas Beato, B D' David, Adelein Ambrosio, Adam Rodriguez, Jeremiah T Campbell, Kenneth O Folsom-Kovarik, Stanley, Evolutionary computation. 192011. 2011</p>
<p>Guided Curriculum Learning for Walking Over Complex Terrain. Brendan Tidd, Nicolas Hudson, Akansel Cosgun, 2020. 2020</p>
<p>Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions. Rui Wang, Joel Lehman, Jeff Clune, Kenneth O Stanley, 2019. 2019</p>
<p>A Cellular Texture Basis Function. Steven Worley, 10.1145/237170.237267Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH '96). the 23rd Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH '96)New York, NY, USAAssociation for Computing Machinery1996</p>
<p>Zhaoming Xie, Hung Yu Ling, Nam Hee Kim, Michiel Van De Panne, arXiv:2005.04323[cs.GR]ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills. 2020</p>
<p>Open-Ended Learning Strategies for Learning Complex Locomotion Skills. Fangqin Zhou, Joaquin Vanschoren, Fifth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
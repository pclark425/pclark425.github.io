<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-839 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-839</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-839</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-259243960</p>
                <p><strong>Paper Title:</strong> ToolQA: A Dataset for LLM Question Answering with External Tools</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning. To overcome these challenges, external tools can be used to enhance LLMs’ question-answering abilities. However, current evaluation methods do not distinguish between questions that can be answered using LLMs’ internal knowledge and those that require external information through tool use. To address this issue, we introduce a new dataset called ToolQA, which is designed to faithfully evaluate LLMs’ ability to use external tools for question answering. Our development of ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions. Importantly, we strive to minimize the overlap between our benchmark data and LLMs’ pre-training data, enabling a more precise evaluation of LLMs’ tool-use reasoning abilities. We conducted an in-depth diagnosis of existing tool-use LLMs to highlight their strengths, weaknesses, and potential improvements. Our findings set a new benchmark for evaluating LLMs and suggest new directions for future advancements. Our data and code are freely available for the broader scientific community on GitHub 2 .</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e839.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e839.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI gpt-3.5-turbo as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source instruction-following LLM used as a baseline (no external tools) and as a controller in some experiments; evaluated on ToolQA both with plain prompting and chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer LLM accessed via API (used as black-box); in experiments used with plain prompting and chain-of-thought prompting; sometimes used as the language model behind tool-use agents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>ToolQA question answering (internal-knowledge baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>≈5% success rate on easy ToolQA questions; ≈2% on hard ToolQA questions (reported as very low success when relying only on internal knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolQA question answering with tool access (when used as controller in tool-augmented setups)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step tool-augmented QA</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>black-box transformer LLM; instruction-tuned; used with prompting only (no model-architectural change reported)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting / tool-augmentation (when used in ReAct or similar)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Used as a baseline (prompt-only) showing very low success on ToolQA; can be wrapped by tool-use prompting (e.g., ReAct) to access tools, though exact tool-augmented ChatGPT results were treated as reference and may vary with API versions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>When relying only on internal knowledge ChatGPT performs ≈5% (easy) / ≈2% (hard); tool-augmented methods in the paper improve success substantially for easy questions (paper-level best up to ≈43.1% on easy) though specific ChatGPT+tool figures are given as reference and may vary with API versions.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>ToolQA questions are deliberately drawn from external corpora not present in model pretraining, so ChatGPT (without tool access) cannot answer them; gap arises because internal parametric knowledge is insufficient for temporally/spatially/context-sensitive questions and multi-step data operations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolQA: A Dataset for LLM Question Answering with External Tools', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e839.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e839.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2 (vanilla)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 (13B and 70B variants used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source foundation models evaluated as vanilla (no tools) baselines and with tool-augmentation (in separate experiments) to measure gap between parametric QA and interactive tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LLM family; experiments used 13B and 70B parameter variants as vanilla baselines and in tool-augmented setups (in-context prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B; 70B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>ToolQA question answering (internal-knowledge baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Near-zero success rates on ToolQA (vanilla LLaMA-2 results reported as near-zero across tasks, indicating little overlap with pretraining data)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolQA with tool-augmentation (ReAct prompt applied to LLaMA-2 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step tool-augmented QA</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Tool-augmented LLaMA-2 (ReAct, 13B) reported example: 31.0% on Coffee-easy and 6.2% on Coffee-hard (Table 5); overall tool-augmentation substantially outperforms vanilla (exact numbers vary by domain).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard transformer LM; no built-in tool interface in the base model; tool use enabled via prompting (in-context tool descriptions and few-shot tool-level demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context learning for tool use experiments (no additional fine-tuning reported for these experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy / tool-augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Provide tool descriptions and tool-level demonstrations in context (few-shot), enabling the LLM to call tools (LoadDB, SQLInterpreter, PythonInterpreter, etc.) via in-context programmatic interfaces; applied ReAct prompting (iterative reasoning+tool calls) to LLaMA-2.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Tool augmentation raised performance from near-zero (vanilla) to substantially higher success rates (example: ReAct(LLaMA-2,13B) achieved 31.0% on Coffee-easy vs near-zero for vanilla), demonstrating a large gap closed by tool access but still imperfect, especially on hard compositional questions.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Vanilla LLaMA-2 lacks access to up-to-date / corpus-specific facts and multi-step data operations; tool-augmentation helps by providing external retrieval, computation and code execution, but failures persist due to argument errors, tool composition/planning, and context-length/formatting issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolQA: A Dataset for LLM Question Answering with External Tools', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e839.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e839.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon (vanilla)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon (40B) open large language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open large language model (40B) evaluated as a vanilla baseline on ToolQA showing poor performance without tool access.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Falcon</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LLM (40B parameters in the experiment) used as a vanilla baseline (no tool access) for ToolQA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>ToolQA question answering (internal-knowledge baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Near-zero success rates on ToolQA as a vanilla model (reported to underperform tool-augmented counterparts)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>standard transformer LM; no tool interface used in these experiments</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Falcon (vanilla) cannot access external reference corpora required by ToolQA; lack of nonparametric access and multi-step tool composition causes near-zero performance on these out-of-distribution, temporally/context-dependent questions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolQA: A Dataset for LLM Question Answering with External Tools', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e839.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e839.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (prompting strategy that elicits step-by-step reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that asks the model to produce intermediate reasoning steps (e.g., 'Let's think step by step') intended to improve complex reasoning in LLMs; evaluated here as a baseline (applied to ChatGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Chain-of-Thought prompting (applied to ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt engineering technique to elicit chain-of-thought style reasoning traces from LLMs; not an architectural change to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>ToolQA question answering via internal reasoning (prompt-only)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Very poor success rates (<10% overall) on ToolQA when used without external tools (reported similar to ChatGPT baseline: ≈5% easy, ≈2% hard)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>prompting-only (elicits internal chain-of-thought), no external tool interface</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompt-only / in-context elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Chain-of-thought prompting attempts to improve internal multi-step reasoning but does not add external tool access; thus it provides little benefit when external corpus/tool access is required.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Minimal: CoT gave very low success on ToolQA (similar to ChatGPT baseline), demonstrating that internal chain-of-thought reasoning alone cannot substitute for external tool access for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>CoT improves internal reasoning but cannot provide missing external facts or perform multi-step data retrieval/computation from external corpora, so it fails on ToolQA items that require tool access.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolQA: A Dataset for LLM Question Answering with External Tools', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e839.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e839.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chameleon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chameleon (plug-and-play compositional reasoning with LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent tool-use method that treats the LLM as a controller to select and sequence multiple modular tools; evaluated on ToolQA and improves over vanilla LLMs but below the best-performing methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Chameleon</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM-as-policy approach: uses tool descriptions and integrates human-induced orderings of tools in-context to plan sequences of tool invocations (controller-style), but in the implementation used it cannot take execution feedback iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>ToolQA question answering via tool composition</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolQA with multi-tool composition (controller selects tool sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / planning / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported modest success: ≈10.6% on easy questions and ≈1.9% on hard questions (paper reports Chameleon achieves slightly better performance than vanilla but much worse than ReAct).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>controller-style policy over modular tools; leverages tool descriptions and predefined tool orderings in context; does not ingest execution trace feedback in the form used in these experiments</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>in-context prompting with tool descriptions and tool-level demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural/agent design via controller prompting (prompting strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Provide a policy model prompt that maps a question + modular tool set to a sequence of modules to execute; relies on in-context orderings and tool descriptions to compose tool chains.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improves performance over vanilla LLMs (example: ≈10.6% easy vs near-zero vanilla), but limited by inability to incorporate execution feedback so often suffers infeasible actions or omitted arguments.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Chameleon's lack of iterative feedback integration and reliance on human-provided ordering limits recovery from execution errors and handling of complex compositions; planning-only without feedback insufficient for many hard ToolQA questions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolQA: A Dataset for LLM Question Answering with External Tools', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e839.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e839.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Synergizing reasoning and acting in language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/agent framework that interleaves natural-language reasoning traces with tool calls and uses observations from tool execution to inform subsequent actions; evaluated in multiple variants (GPT-3 and GPT-3.5) and is the best-performing method in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ReAct (applied with GPT-3 and GPT-3.5 controllers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting framework that produces alternating 'Thought/Action/Observation' steps: the LLM emits reasoning and tool calls; the agent executes the tools and returns observations which inform the next step (iterative refine-and-act loop).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>ToolQA question answering via iterative tool use</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ToolQA with iterative reasoning + tool calls</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / iterative multi-step reasoning / planning with feedback</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Best-performing tool-augmented approach in the paper: tool-augmented best overall performance reported at ≈43.13% success on easy questions (paper-level best) and ≈8.24% on hard questions; ReAct variants achieved the paper's top results (ReAct (GPT-3.5) reported average hard ≈8.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>iterative reasoning + action loop; uses execution trace observations to condition subsequent actions; explicitly interleaves chain-of-thought-like reasoning with tool calls</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context learning with tool-level demonstrations (few-shot); two variants evaluated using GPT-3 and GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting/agent design (iterative feedback integration)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Provide a ReAct-style prompt template that instructs the model to produce thoughts, issue tool calls, receive observations, and then reason further, enabling iterative refinement of tool-call sequences and correction of earlier mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Substantially improved interactive performance over prompt-only baselines and over Chameleon (which lacks execution feedback). ReAct is the top method: reported paper-level bests include ≈43.13% on easy ToolQA questions and ≈8.24% on hard questions; ReAct (GPT-3.5) outperforms many alternatives especially on hard questions due to better code/solution innovation, though it can also hallucinate.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>ReAct reduces some gap by using execution feedback, but remaining gap is attributed to (1) argument errors (wrong tool arguments), (2) incorrect data source selection, (3) hallucination/over-innovation beyond available observations, (4) context-length limitations and low-quality retrieval; these limit performance especially on hard compositional questions requiring multiple tools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolQA: A Dataset for LLM Question Answering with External Tools', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e839.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e839.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tool-augmentation (general intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tool-augmentation: enabling LLMs to query external tools (retrieval, code interpreters, calculators, DB ops)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of interventions that augment LLMs with external tools (text retrieval, database ops, SQL/Python interpreters, calculators, graph loaders) to supply non-parametric knowledge and computation; evaluated centrally in this paper as the main approach to close the QA-vs-interactive gap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Tool-augmentation (retrieval + code + calculators + DB + graph tools)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Augmentation stack composed of 13 tool types (AgendaRetriever, SciREXRetriever, DatabaseLoader, DataFilter, GetValue, WolframAlpha/Calculator, GraphLoader, NeighbourChecker, NodeChecker, EdgeChecker, SQLInterpreter, PythonInterpreter, Finish) that LLMs can call via prompting interfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>ToolQA question answering requiring external tool access</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool use for question answering over external corpora (ToolQA)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step reasoning / planning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Paper-level summary: vanilla (internal-only) methods achieve ≈5% (easy) / ≈2% (hard); tool-augmented methods improve to up to ≈43.15% (easy) and ≈8.2% (hard) depending on agent and prompt; specific agents (Chameleon, ReAct) show varied improvements (Chameleon ≈10.6% easy, 1.9% hard; ReAct best).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>external tool interfaces for retrieval, DB ops, computation, code execution, graph queries; can be accessed via prompting/in-context controllers like Chameleon or ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>primarily prompting/in-context demonstrations in experiments; authors suggest fine-tuning on tool-use corpora as future direction</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>hybrid: architectural (tool interfaces) + prompting strategy (tool-level demonstrations); proposed training intervention (fine-tuning on tool-use sequences) as future work</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Provide explicit tool APIs and tool-level demonstrations in-context; instantiate operators and programmatic tool chains to create ground-truth answers; recommend collecting tool-use traces to fine-tune open-source LLMs to further close the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Tool access yields large gains over internal-only baselines for easy questions (from ≈5% to up to ≈43% success) and modest gains on hard compositional questions (up to ≈8.2%); however, performance remains far from perfect due to planning, argument, and hallucination errors.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Remaining gap arises because tool-augmented performance depends on accurate tool argument generation, correct data source selection, handling of long interaction contexts, and preventing hallucinations when inventing actions not supported by execution traces; current prompting approaches do not fully solve planning/compositionality and feedback integration challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolQA: A Dataset for LLM Question Answering with External Tools', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Chameleon: Plug-and-play compositional reasoning with large language models <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>ToolBench <em>(Rating: 1)</em></li>
                <li>API-Bank <em>(Rating: 1)</em></li>
                <li>PAL: Programaided Language Models <em>(Rating: 1)</em></li>
                <li>Code4Struct <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-839",
    "paper_id": "paper-259243960",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "ChatGPT (gpt-3.5-turbo)",
            "name_full": "ChatGPT (OpenAI gpt-3.5-turbo as used in experiments)",
            "brief_description": "Closed-source instruction-following LLM used as a baseline (no external tools) and as a controller in some experiments; evaluated on ToolQA both with plain prompting and chain-of-thought prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "Instruction-tuned transformer LLM accessed via API (used as black-box); in experiments used with plain prompting and chain-of-thought prompting; sometimes used as the language model behind tool-use agents.",
            "model_size": null,
            "qa_task_name": "ToolQA question answering (internal-knowledge baseline)",
            "qa_performance": "≈5% success rate on easy ToolQA questions; ≈2% on hard ToolQA questions (reported as very low success when relying only on internal knowledge)",
            "interactive_task_name": "ToolQA question answering with tool access (when used as controller in tool-augmented setups)",
            "interactive_task_type": "tool use / multi-step tool-augmented QA",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "black-box transformer LLM; instruction-tuned; used with prompting only (no model-architectural change reported)",
            "training_method": null,
            "intervention_type": "prompting / tool-augmentation (when used in ReAct or similar)",
            "intervention_description": "Used as a baseline (prompt-only) showing very low success on ToolQA; can be wrapped by tool-use prompting (e.g., ReAct) to access tools, though exact tool-augmented ChatGPT results were treated as reference and may vary with API versions.",
            "intervention_effect": "When relying only on internal knowledge ChatGPT performs ≈5% (easy) / ≈2% (hard); tool-augmented methods in the paper improve success substantially for easy questions (paper-level best up to ≈43.1% on easy) though specific ChatGPT+tool figures are given as reference and may vary with API versions.",
            "hypothesized_cause_of_gap": "ToolQA questions are deliberately drawn from external corpora not present in model pretraining, so ChatGPT (without tool access) cannot answer them; gap arises because internal parametric knowledge is insufficient for temporally/spatially/context-sensitive questions and multi-step data operations.",
            "uuid": "e839.0",
            "source_info": {
                "paper_title": "ToolQA: A Dataset for LLM Question Answering with External Tools",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "LLaMA-2 (vanilla)",
            "name_full": "LLaMA-2 (13B and 70B variants used in experiments)",
            "brief_description": "Open-source foundation models evaluated as vanilla (no tools) baselines and with tool-augmentation (in separate experiments) to measure gap between parametric QA and interactive tool use.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "LLaMA-2",
            "model_description": "Open-source transformer LLM family; experiments used 13B and 70B parameter variants as vanilla baselines and in tool-augmented setups (in-context prompting).",
            "model_size": "13B; 70B",
            "qa_task_name": "ToolQA question answering (internal-knowledge baseline)",
            "qa_performance": "Near-zero success rates on ToolQA (vanilla LLaMA-2 results reported as near-zero across tasks, indicating little overlap with pretraining data)",
            "interactive_task_name": "ToolQA with tool-augmentation (ReAct prompt applied to LLaMA-2 in experiments)",
            "interactive_task_type": "tool use / multi-step tool-augmented QA",
            "interactive_performance": "Tool-augmented LLaMA-2 (ReAct, 13B) reported example: 31.0% on Coffee-easy and 6.2% on Coffee-hard (Table 5); overall tool-augmentation substantially outperforms vanilla (exact numbers vary by domain).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "standard transformer LM; no built-in tool interface in the base model; tool use enabled via prompting (in-context tool descriptions and few-shot tool-level demonstrations)",
            "training_method": "prompting / in-context learning for tool use experiments (no additional fine-tuning reported for these experiments)",
            "intervention_type": "prompting strategy / tool-augmentation",
            "intervention_description": "Provide tool descriptions and tool-level demonstrations in context (few-shot), enabling the LLM to call tools (LoadDB, SQLInterpreter, PythonInterpreter, etc.) via in-context programmatic interfaces; applied ReAct prompting (iterative reasoning+tool calls) to LLaMA-2.",
            "intervention_effect": "Tool augmentation raised performance from near-zero (vanilla) to substantially higher success rates (example: ReAct(LLaMA-2,13B) achieved 31.0% on Coffee-easy vs near-zero for vanilla), demonstrating a large gap closed by tool access but still imperfect, especially on hard compositional questions.",
            "hypothesized_cause_of_gap": "Vanilla LLaMA-2 lacks access to up-to-date / corpus-specific facts and multi-step data operations; tool-augmentation helps by providing external retrieval, computation and code execution, but failures persist due to argument errors, tool composition/planning, and context-length/formatting issues.",
            "uuid": "e839.1",
            "source_info": {
                "paper_title": "ToolQA: A Dataset for LLM Question Answering with External Tools",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Falcon (vanilla)",
            "name_full": "Falcon (40B) open large language model",
            "brief_description": "An open large language model (40B) evaluated as a vanilla baseline on ToolQA showing poor performance without tool access.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Falcon",
            "model_description": "Open-source transformer LLM (40B parameters in the experiment) used as a vanilla baseline (no tool access) for ToolQA.",
            "model_size": "40B",
            "qa_task_name": "ToolQA question answering (internal-knowledge baseline)",
            "qa_performance": "Near-zero success rates on ToolQA as a vanilla model (reported to underperform tool-augmented counterparts)",
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "standard transformer LM; no tool interface used in these experiments",
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Falcon (vanilla) cannot access external reference corpora required by ToolQA; lack of nonparametric access and multi-step tool composition causes near-zero performance on these out-of-distribution, temporally/context-dependent questions.",
            "uuid": "e839.2",
            "source_info": {
                "paper_title": "ToolQA: A Dataset for LLM Question Answering with External Tools",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-Thought prompting (prompting strategy that elicits step-by-step reasoning)",
            "brief_description": "A prompting technique that asks the model to produce intermediate reasoning steps (e.g., 'Let's think step by step') intended to improve complex reasoning in LLMs; evaluated here as a baseline (applied to ChatGPT).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Chain-of-Thought prompting (applied to ChatGPT)",
            "model_description": "Prompt engineering technique to elicit chain-of-thought style reasoning traces from LLMs; not an architectural change to the model.",
            "model_size": null,
            "qa_task_name": "ToolQA question answering via internal reasoning (prompt-only)",
            "qa_performance": "Very poor success rates (&lt;10% overall) on ToolQA when used without external tools (reported similar to ChatGPT baseline: ≈5% easy, ≈2% hard)",
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "prompting-only (elicits internal chain-of-thought), no external tool interface",
            "training_method": "prompt-only / in-context elicitation",
            "intervention_type": "prompting strategy",
            "intervention_description": "Chain-of-thought prompting attempts to improve internal multi-step reasoning but does not add external tool access; thus it provides little benefit when external corpus/tool access is required.",
            "intervention_effect": "Minimal: CoT gave very low success on ToolQA (similar to ChatGPT baseline), demonstrating that internal chain-of-thought reasoning alone cannot substitute for external tool access for these tasks.",
            "hypothesized_cause_of_gap": "CoT improves internal reasoning but cannot provide missing external facts or perform multi-step data retrieval/computation from external corpora, so it fails on ToolQA items that require tool access.",
            "uuid": "e839.3",
            "source_info": {
                "paper_title": "ToolQA: A Dataset for LLM Question Answering with External Tools",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Chameleon",
            "name_full": "Chameleon (plug-and-play compositional reasoning with LLMs)",
            "brief_description": "A recent tool-use method that treats the LLM as a controller to select and sequence multiple modular tools; evaluated on ToolQA and improves over vanilla LLMs but below the best-performing methods.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Chameleon",
            "model_description": "LLM-as-policy approach: uses tool descriptions and integrates human-induced orderings of tools in-context to plan sequences of tool invocations (controller-style), but in the implementation used it cannot take execution feedback iteratively.",
            "model_size": null,
            "qa_task_name": "ToolQA question answering via tool composition",
            "qa_performance": null,
            "interactive_task_name": "ToolQA with multi-tool composition (controller selects tool sequence)",
            "interactive_task_type": "tool use / planning / multi-step reasoning",
            "interactive_performance": "Reported modest success: ≈10.6% on easy questions and ≈1.9% on hard questions (paper reports Chameleon achieves slightly better performance than vanilla but much worse than ReAct).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "controller-style policy over modular tools; leverages tool descriptions and predefined tool orderings in context; does not ingest execution trace feedback in the form used in these experiments",
            "training_method": "in-context prompting with tool descriptions and tool-level demonstrations",
            "intervention_type": "architectural/agent design via controller prompting (prompting strategy)",
            "intervention_description": "Provide a policy model prompt that maps a question + modular tool set to a sequence of modules to execute; relies on in-context orderings and tool descriptions to compose tool chains.",
            "intervention_effect": "Improves performance over vanilla LLMs (example: ≈10.6% easy vs near-zero vanilla), but limited by inability to incorporate execution feedback so often suffers infeasible actions or omitted arguments.",
            "hypothesized_cause_of_gap": "Chameleon's lack of iterative feedback integration and reliance on human-provided ordering limits recovery from execution errors and handling of complex compositions; planning-only without feedback insufficient for many hard ToolQA questions.",
            "uuid": "e839.4",
            "source_info": {
                "paper_title": "ToolQA: A Dataset for LLM Question Answering with External Tools",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Synergizing reasoning and acting in language models)",
            "brief_description": "A prompting/agent framework that interleaves natural-language reasoning traces with tool calls and uses observations from tool execution to inform subsequent actions; evaluated in multiple variants (GPT-3 and GPT-3.5) and is the best-performing method in this paper's experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "ReAct (applied with GPT-3 and GPT-3.5 controllers)",
            "model_description": "Prompting framework that produces alternating 'Thought/Action/Observation' steps: the LLM emits reasoning and tool calls; the agent executes the tools and returns observations which inform the next step (iterative refine-and-act loop).",
            "model_size": null,
            "qa_task_name": "ToolQA question answering via iterative tool use",
            "qa_performance": null,
            "interactive_task_name": "ToolQA with iterative reasoning + tool calls",
            "interactive_task_type": "tool use / iterative multi-step reasoning / planning with feedback",
            "interactive_performance": "Best-performing tool-augmented approach in the paper: tool-augmented best overall performance reported at ≈43.13% success on easy questions (paper-level best) and ≈8.24% on hard questions; ReAct variants achieved the paper's top results (ReAct (GPT-3.5) reported average hard ≈8.2%).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "iterative reasoning + action loop; uses execution trace observations to condition subsequent actions; explicitly interleaves chain-of-thought-like reasoning with tool calls",
            "training_method": "prompting / in-context learning with tool-level demonstrations (few-shot); two variants evaluated using GPT-3 and GPT-3.5",
            "intervention_type": "prompting/agent design (iterative feedback integration)",
            "intervention_description": "Provide a ReAct-style prompt template that instructs the model to produce thoughts, issue tool calls, receive observations, and then reason further, enabling iterative refinement of tool-call sequences and correction of earlier mistakes.",
            "intervention_effect": "Substantially improved interactive performance over prompt-only baselines and over Chameleon (which lacks execution feedback). ReAct is the top method: reported paper-level bests include ≈43.13% on easy ToolQA questions and ≈8.24% on hard questions; ReAct (GPT-3.5) outperforms many alternatives especially on hard questions due to better code/solution innovation, though it can also hallucinate.",
            "hypothesized_cause_of_gap": "ReAct reduces some gap by using execution feedback, but remaining gap is attributed to (1) argument errors (wrong tool arguments), (2) incorrect data source selection, (3) hallucination/over-innovation beyond available observations, (4) context-length limitations and low-quality retrieval; these limit performance especially on hard compositional questions requiring multiple tools.",
            "uuid": "e839.5",
            "source_info": {
                "paper_title": "ToolQA: A Dataset for LLM Question Answering with External Tools",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Tool-augmentation (general intervention)",
            "name_full": "Tool-augmentation: enabling LLMs to query external tools (retrieval, code interpreters, calculators, DB ops)",
            "brief_description": "A class of interventions that augment LLMs with external tools (text retrieval, database ops, SQL/Python interpreters, calculators, graph loaders) to supply non-parametric knowledge and computation; evaluated centrally in this paper as the main approach to close the QA-vs-interactive gap.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Tool-augmentation (retrieval + code + calculators + DB + graph tools)",
            "model_description": "Augmentation stack composed of 13 tool types (AgendaRetriever, SciREXRetriever, DatabaseLoader, DataFilter, GetValue, WolframAlpha/Calculator, GraphLoader, NeighbourChecker, NodeChecker, EdgeChecker, SQLInterpreter, PythonInterpreter, Finish) that LLMs can call via prompting interfaces.",
            "model_size": null,
            "qa_task_name": "ToolQA question answering requiring external tool access",
            "qa_performance": null,
            "interactive_task_name": "Tool use for question answering over external corpora (ToolQA)",
            "interactive_task_type": "tool use / multi-step reasoning / planning",
            "interactive_performance": "Paper-level summary: vanilla (internal-only) methods achieve ≈5% (easy) / ≈2% (hard); tool-augmented methods improve to up to ≈43.15% (easy) and ≈8.2% (hard) depending on agent and prompt; specific agents (Chameleon, ReAct) show varied improvements (Chameleon ≈10.6% easy, 1.9% hard; ReAct best).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "external tool interfaces for retrieval, DB ops, computation, code execution, graph queries; can be accessed via prompting/in-context controllers like Chameleon or ReAct",
            "training_method": "primarily prompting/in-context demonstrations in experiments; authors suggest fine-tuning on tool-use corpora as future direction",
            "intervention_type": "hybrid: architectural (tool interfaces) + prompting strategy (tool-level demonstrations); proposed training intervention (fine-tuning on tool-use sequences) as future work",
            "intervention_description": "Provide explicit tool APIs and tool-level demonstrations in-context; instantiate operators and programmatic tool chains to create ground-truth answers; recommend collecting tool-use traces to fine-tune open-source LLMs to further close the gap.",
            "intervention_effect": "Tool access yields large gains over internal-only baselines for easy questions (from ≈5% to up to ≈43% success) and modest gains on hard compositional questions (up to ≈8.2%); however, performance remains far from perfect due to planning, argument, and hallucination errors.",
            "hypothesized_cause_of_gap": "Remaining gap arises because tool-augmented performance depends on accurate tool argument generation, correct data source selection, handling of long interaction contexts, and preventing hallucinations when inventing actions not supported by execution traces; current prompting approaches do not fully solve planning/compositionality and feedback integration challenges.",
            "uuid": "e839.6",
            "source_info": {
                "paper_title": "ToolQA: A Dataset for LLM Question Answering with External Tools",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "rating": 2,
            "sanitized_title": "chameleon_plugandplay_compositional_reasoning_with_large_language_models"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "ToolBench",
            "rating": 1
        },
        {
            "paper_title": "API-Bank",
            "rating": 1
        },
        {
            "paper_title": "PAL: Programaided Language Models",
            "rating": 1,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "Code4Struct",
            "rating": 1,
            "sanitized_title": "code4struct"
        }
    ],
    "cost": 0.019691499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ToolQA: A Dataset for LLM Question Answering with External Tools</p>
<p>Yuchen Zhuang yczhuang@gatech.edu 
College of Computing
Georgia Institute of Technology
AtlantaGA</p>
<p>Yue Yu yueyu@gatech.edu 
College of Computing
Georgia Institute of Technology
AtlantaGA</p>
<p>Kuan Wang kuanwang@gatech.edu 
College of Computing
Georgia Institute of Technology
AtlantaGA</p>
<p>Haotian Sun haotian.sun@gatech.edu 
College of Computing
Georgia Institute of Technology
AtlantaGA</p>
<p>Chao Zhang chaozhang@gatech.edu 
College of Computing
Georgia Institute of Technology
AtlantaGA</p>
<p>ToolQA: A Dataset for LLM Question Answering with External Tools
1DC251ECCBA4F00BA6B1D931CAFD03B8
Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning.To overcome these challenges, external tools can be used to enhance LLMs' question-answering abilities.However, current evaluation methods do not distinguish between questions that can be answered using LLMs' internal knowledge and those that require external information through tool use.To address this issue, we introduce a new dataset called ToolQA, which is designed to faithfully evaluate LLMs' ability to use external tools for question answering.Our development of ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions.Importantly, we strive to minimize the overlap between our benchmark data and LLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use reasoning abilities.We conducted an in-depth diagnosis of existing tool-use LLMs to highlight their strengths, weaknesses, and potential improvements.Our findings set a new benchmark for evaluating LLMs and suggest new directions for future advancements.Our data and code are freely available for the broader scientific community on GitHub 2 .</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated superior performance in a myriad of NLP tasks [4,9,43,42,55,61].These models have captured vast amounts of knowledge from enormous and diverse corpora during pre-training.After instruction fine-tuning [10,44,2], they have demonstrated impressive capabilities in information-seeking question answering [65,26].Despite their remarkable performance, LLMs face several challenges.For example, they are susceptible to hallucinationsgenerating plausible yet ungrounded information-which can mislead users and affect content integrity [66,19,5].Additionally, they exhibit weaknesses in numerical reasoning, an essential skill in numerous real-life applications [14,36,41,28,51,13].These limitations highlight the need for techniques that can enhance LLMs' question-answering abilities.</p>
<p>Recent research has shown that these issues can be mitigated by augmenting LLMs with external tools, such as retrieval augmentation [58,17], math tools [56,76,32], and code interpreters [13,63].For example, a Wolfram math plugin can enhance numerical reasoning [68], and a verified database can mitigate hallucinations by providing up-to-date fact-checked knowledge [49].However, existing evaluation methodologies struggle to distinguish whether the model is simply recalling pre-trained information or truly utilizing external tools for problem-solving [37].This challenge arises, in part, because the external data used for evaluation may have already been exposed to LLMs during the pre-training phase [53].This exposure can lead to a biased evaluation of LLMs' tool-use abilities, as the models could just use their ingrained knowledge and their reasoning abilities, bypassing the use of external tools.As a result, these evaluations cannot accurately reflect the true competency of the  models.We need a fair and explicit way to check if LLMs are really good at problem-solving with tools or if they are just using their memorized information.</p>
<p>To fill this gap, we introduce ToolQA, a question answering (QA) benchmark to evaluate LLMs' ability in using external tools for answering questions.ToolQA comprises data from 8 domains and defines 13 types of tools to acquire information from external reference corpora.Each instance in ToolQA consists of a question, an answer, reference corpora, and a list of available tools.ToolQA is unique in that all its questions can be answered only by using appropriate tools to obtain information from the reference corpus.This minimizes the possibility of LLMs answering questions by merely recalling their internal knowledge, and allows for faithfully evaluating LLMs' abilities in using tools.</p>
<p>ToolQA is curated with an automated three-phase process: (1) The first phase, Reference Data Collection, involves gathering various types of public corpora including text, tables, and graphs from different domains.These corpora have no overlap with the LLM pre-training data and will serve as reference corpora for tool-based question answering.(2) The second phase is Human-guided Question Generation with LLMs.In this phase, we generate questions that can only be answered by using tools over the reference corpora.Our approach is a template-based question generation process, which includes human-guided template generation, template validation, and question instantiation with tool attributes.(3) The third phase is Programmatic Answer Generation.This phase produces accurate answers for the generated questions.To ensure answer correctness, we implement operators corresponding to the tools and obtain answers from the reference corpora programmatically.Our three-phase procedure ensures that we generate questions that can only be answered using external knowledge, along with their precise answers.Additionally, the process is highly efficient and requires minimal human labeling efforts.</p>
<p>We conducted experiments using both standard LLMs and tool-augmented LLMs to answer questions in ToolQA.Our findings indicate that ChatGPT and Chain-of-thoughts prompting [65], which rely solely on their internal knowledge, have low success rates of approximately 5% for easy questions and 2% for hard questions.In contrast, tool-augmented LLMs such as Chameleon [32] and ReAct [76] perform better by leveraging external tools.For easy questions, the best performance achieved by tool-augmented LLMs is 43.15%, while for hard questions, the best performance drops to 8.2%.</p>
<p>Our results and error analysis demonstrate that ToolQA is a challenging benchmark for existing tool-augmented LLM methods, especially for its hard questions that require more complex reasoning about tool composition.</p>
<p>2 Related Work</p>
<p>Knowledge-Augmented LLMs</p>
<p>Several prior works aim to enhance LLMs with explicit external knowledge.Specifically, one line of research focus on retrieval-augmented language models [58,3,17,27,30,80,34,72], where they use sparse [54] or dense retrieval [22,16] to extract relevant knowledge from the corpus.These works mainly focus on leveraging free text, without considering multiple types of tools for task solving.On the other hand, Program-of-Thought [6], PAL [13], MathPrompt [15], and   Code4Struct [63] apply code-based tools to enhance LLMs' abilities in question answering with a focus on tabular and math-related tasks.Several additional works [56,32,57] expand the scope of tool utilization by incorporating different types of basic tools (e.g.calculator, calendar, machine translation) to solve complex reasoning tasks.To synergize different functional tools together for problem-solving, LLMs must have advanced planning and memory capabilities.In terms of planning, current methods either enable LLMs to autonomously break down complex tasks into intermediate reasoning steps [45,24,47,50,65,75,31,76], or encourage LLMs to self-reflect the previous decisions with environmental feedback [39,76,35,8].Memory capabilities, on the other hand, provide LLMs with opportunities to learn and adapt based on past experiences, whether successes or failures [62].In addition, several works have extended this line of learning paradigm to other modalities [73,69] and other domains [20].Concurrent to our work, there are also several studies [70,37] that investigate the parametric and nonparametric knowledge from LLMs.A detailed comparison between existing tool-use LLMs can be found in Appendix A.</p>
<p>Benchmarks on Tool-Augmented LLMs</p>
<p>Earlier tool-augmented LLMs primarily assess single tool usage based on downstream task performance across existing benchmarks.For example, there are works that study how text retrievers augment LLMs' performance on open-domain question-answering [21,74], fact-checking [60], and timely information benchmarks [7,23,78,12].Besides, the mathematical reasoning abilities of external calculators and Python interpreters are evaluated using computation-intensive QA datasets [11,33].However, these evaluation benchmarks may not faithfully reflect the extent to which models leverage external tools, as some questions could still be correctly answered solely using the internal knowledge of the LLMs.ToolQA attempts to mitigate these issues by selecting data from out-of-scope sources that have not been memorized by LLMs.Concurrent with our work, there are several recent benchmarks for evaluating LLMs' ability in using multiple tools for solving challenging tasks, including API-Bank [29], APIBench [48], and ToolBench [52,71].They mainly focus on constructing high-quality tool chains for LLM fine-tuning and evaluating API call trace accuracy against a fixed ground truth trace.In contrast, ToolQA is unique in that it focuses on the open-ended use of tools for question-answering, rather than benchmarking the intermediate process of tool use.Specifically, ToolQA creates tool-based question-answer pairs and assesses whether LLMs can arrive at the correct answer, regardless of the tool chains used.</p>
<p>3 ToolQA Dataset</p>
<p>Dataset Details</p>
<p>We curate the ToolQA benchmark to evaluate LLMs' capability in leveraging external tools for question answering.ToolQA consists of data from 8 distinct domains, each instance being a tuple -(question, answer, reference corpora, and tools).The reference corpora are external knowledge sources that can be queried, which can be a text corpus, a tabular database, or a graph.To enable obtaining information from the reference corpora, we have developed 13 tools for text retrieval, database operations, code interpretation, mathematical computations, and more.The questions are designed to simulate real-world information-seeking inquiries.However, they cannot be answered directly with LLMs' internal knowledge, but instead require LLMs to obtain information from the reference corpora via tool use.Table 1 shows the detailed statistics of ToolQA.</p>
<p>To reduce human efforts in generating faithful question-answer pairs to evaluate LLMs' tool-use capabilities, we propose an automatic three-phase process (Figure 2): (1) We first select data from public sources that are unmemorized by LLMs during Reference Data Collection;</p>
<p>(2) We adopt Human-Guided Question Generation to steer LLMs to generate valid questions according to predefined templates; (3) We produce accurate answers for the generated questions with Programmatic Answer Generation.We detail the three-phase generation process in the following.</p>
<p>Reference Data and Tools</p>
<p>To evaluate LLMs' ability in using external tools for question answering, it is crucial to ensure that they cannot directly answer the questions with their internal knowledge.To this end, we collect reference corpora that meet the following criteria (Figure 2(a)): 1) The reference corpora should ideally not overlap with the LLM's pre-training data; 2) The reference corpora should contain context-sensitive facts for generating questions that cannot be directly answered solely based on LLMs' internal knowledge and reasoning abilities; 3) LLMs should be able to obtain all the necessary information from the reference corpora to correctly answer the questions.</p>
<p>Based on these criteria, we define 6 contextual dimensions: temporal, spatial, social, scientific, mathematical, and personal.We collect reference corpora that can yield context-specific questions along one or more of the 6 dimensions.Specifically: 1) Along the temporal dimension, we collect the Flights and Coffee corpora, which contain the latest information that is out of the temporal scope of the LLM's pre-training data.2) Along the spatial dimension, we collect Yelp and Airbnb, which are two non-text corpora that can yield questions with spatial contexts.3) Along the mathematical dimension, we collect the questions from GSM8K that ChatGPT cannot answer correctly with its own mathematical reasoning ability; 4) SciREX emphasizes detailed model performances from the scientific domain [18], where GPT family models can easily hallucinate [42].5) To incorporate personal data and avoid privacy issues, we synthesize the personal Agenda corpus with ChatGPT with virtual names and events.6) In addition, we also select data from the most recent DBLP database and create graphs between authors and papers, where social relational knowledge cannot be understood by LLMs currently.Further details can be found in Appendix B.</p>
<p>To obtain information from these reference corpora, we design 13 tools that are available to the LLMs (Table 2).These tools are designed as follows:  • Text: AgendaRetriever and SciREXRetreiver are text retrieval tools.They can retrieve relevant information to a given query from the (synthesized) personal agenda corpus and scientific corpus.• Database: Database Loader loads data from the local tabular Database.Data Filter can filter the database according to a set of conditions, each of which is composed of a column name, a relation, and a pre-determined value (e.g., "Date=2022-10-15").Get Value returns all the values under a certain column in the database.• Math: Calculator is a mathematical tool that treats the input string as a formula and calculates the corresponding result.We use the WolframAlpha API portal as the calculator 3 , which can perform both simple computations (e.g., add, subtraction, multiplication) and complicated operations (e.g., averaging, finding maximum values).• Graph: Graph Loader loads the graph from local files for future operations.Neighbour Checker lists all the neighbors of the query node in the graph.Node Checker and Edge Checker return the detailed attribute information of the query node and edge, respectively.• Code: The SQL Interpreter and the Python Interpreter are responsible for interpreting and executing SQL commands and Python code, respectively.They can receive and transform data from other tools, serving as bridges between different tools and the LLM.• System: Finish parses the feedback from execution and returns the answer to finish the task.</p>
<p>Human-Guided Question Generation</p>
<p>The question generation phase aims to generate questions that can be answered by using the available tools over the reference corpora.There are two straightforward strategies to generate questions: 1) letting human experts come up with questions about reference corpora, or 2) relying solely on LLMs to generate questions about the reference corpora.However, both strategies have their drawbacks.While human experts can produce high-quality questions, the entire process is labor-intensive, timeconsuming, and hard to scale.Depending solely on LLMs may generate unanswerable questions or hallucinate information that does not exist in the reference data.Besides, some of the LLM-generated questions are too easy and can be directly answered with only LLMs' internal knowledge.</p>
<p>To address these challenges, we propose a human-guided LLM generation approach that uses question templates to bridge human guidance and automatic LLM generation [67,79].We first ask ChatGPT to generate candidate question templates from reference data, using prompts such as "Generate diverse and challenging template questions that users may have based on the given information.".To obtain diverse questions, we generate around 50 template questions for each external data source.We then perform manual validation to select the templates that cannot be answered with LLMs' internal knowledge but become answerable with the reference corpora.We go through all question templates and eliminate those that meet either of the following conditions: (1) Template questions that vanilla ChatGPT can answer based on its internal knowledge with a success rate of over 50% (e.g., "What is the distance between LAX and SFO?", where distance information can be memorized by ChatGPT);</p>
<p>(2) Template questions posing queries about information not present in the external data (e.g., "What is the average price from LAX to SFO?", where price information is missing from the flight data).After examining all the templates, we selected 117 most representative and diverse question templates for the entire ToolQA dataset.</p>
<p>After the high-quality question templates are manually selected, we sample values from the reference data to automatically fill into the templates to generate concrete questions.For example, given the template "Did the flight from {Origin} to {Dest} on {Date} get canceled or diverted?",we can sample the values "LAX", "MDW", "01/09/22" from the reference Flight tabular data and fill into the template to form a question: "Did the flight from LAX to MDW on 01/09/22 get canceled or diverted?"Depending on the difficulty of the questions, we classify them into two classes -easy and hard.Easy questions primarily focus on extracting a single piece of information from external knowledge, thus requiring fewer tools to involve in the solution.Conversely, hard questions require complex operations (e.g., average) and reasoning (e.g., comparison) over multiple information pieces drawn from the reference corpora, requiring more tools and complex reasoning among them.We provide a comprehensive list of both easy and hard question templates in Appendix C and D.</p>
<p>Programmatic Answer Generation</p>
<p>Our final step is to create accurate answers for the generated questions.To guarantee the validity of these responses, we implement 1) operators, which are functions corresponding to the predefined tools; and 2) tool chains, which are schemas for composing different operators for different question templates.For each question, as we know the true arguments filled into the question template, we can run the tool chains with the corresponding arguments to programmatically extract answers from the reference data.This process enables automatic generation correct answers to questions, even for those questions that involve multi-step reasoning.Figure 2(c) demonstrates this generation process.When answering a generated question with sampled values "Did the flight from LAX to MDW on 01/09/22 get canceled or diverted?",we write Python codes to implement the operators over the reference data, including database loader, data filter, and get-value function.Then, the programmatic pipeline runs a tool chain of these operators to automatically generate the correct answer (details in Appendix E).</p>
<p>Experiments</p>
<p>Baselines</p>
<p>We evaluate the performance of the following methods on ToolQA, covering both standard LLMs and tool-augmented LLMs: (1) LLaMA-2 [61] and Falcon [1] are state-of-the-art open-sourced large language models.We directly feed the questions into two versions (13B and 70B) of LLaMA-2 and Falcon (40B) to obtain the predictions; (2) ChatGPT [43]: We directly feed the questions into OpenAI's ChatGPT model (gpt-3.5-turbo)and obtain its response as the final answer.( 3) CoT [65,26]: We use chain-of-thoughts prompting for ChatGPT, adding the prompt "Let's think step by step:" after the question to leverage LLMs' reasoning ability for question answering.(4) Chameleon [32] is a recent method that uses LLMs as a controller to use multiple tools for solving subtasks and has shown promising results in reasoning and QA tasks.When running Chameleon on ToolQA, we set the tool pool to our defined tools in § 3.1.( 5) ReAct [76] integrates reasoning with tool use by prompting LLMs to generate interleaved verbal reasoning traces and tool calls.This integration has been shown effective in enhancing LLMs' problem-solving capabilities.We instantiate two versions of ReAct using gpt-3.5-turboand text-davinci-003.</p>
<p>Different from the existing works that mainly provide task-level few-shot exemplars, we provide tool-level demonstrations.We used 8 demonstrations about how to use tools for QA, ensuring that each tool in the pool is covered at least once by the demonstrations.Such tool-level demonstrations provide a concise tutorial to the LLMs for tool use, covering all tool uses with the LLM context limit.Details about the demonstrations and our prompts are included in Appendix F. To assess the performance of methods on the ToolQA benchmark, we normalize both the ground-truth answers and the model predictions to ensure uniformity in format.Success rates are then computed based on the exact match between these normalized answers.We use a series of rules for normalization:</p>
<p>(1) We normalize different time string formats, (e.g., converting "18:06" and "1806.0" to "1806".);</p>
<p>(2) For price-related questions, we normalize the units by removing price units (e.g., USD, $); (3) We remove all the punctuations from both the model predictions and ground-truth answers; (4) We normalize the article usage (e.g., a, an, the) via removing all articles from both the model predictions and ground-truth answers; (5) We normalize the white spaces by trimming multiple spaces into single space.As most of the predictions and answers are numerical values or entities, these normalization rules address most of the false negative cases during matching.We evaluate the model's ability against the generated question-answer pairs in an open-ended manner, focusing on whether the model can arrive at the correct answer, regardless of the used tool chains.</p>
<p>Results</p>
<p>Internal Knowledge vs. External Knowledge.From the results in Tables 3 and 4, the vanilla open-sourced LLMs and ChatGPT underperform their tool-augmented counterparts on both easy and hard questions.This is expected, as vanilla LLMs lack access to external information for question   answering.Additionally, the vanilla LLMs show near-zero performance on different tasks, indicating that there is little overlap between the benchmark data and the LLMs' internal knowledge.</p>
<p>Comparing Different Tool-Use LLMs.Tables 3 and 4 show the results of different methods on the easy and hard questions.ChatGPT and CoT achieve very poor success rates (&lt; 10) on both easy and hard questions across different tasks.This is expected as the questions in ToolQA cannot be answered solely based on LLMs' internal knowledge and reasoning.Chameleon achieves slightly better performance, with 10.6% and 1.9% success rates on easy and hard questions, respectively.This is because Chameleon incorporates tool descriptions and integrates human-induced orderings of these tools in its context, enabling it to comprehend and compose different tools for QA.However, Chameleon cannot take feedback from the execution trace, thus often suffering from infeasible actions or omitted arguments in its generated plans.ReAct is the best-performing model.It can use observations in the execution trace to generate its next action, allowing it to iteratively refine its tool use chain and obtain better success rates.</p>
<p>Easy vs. Hard Questions.Comparing Tables 3 and 4, we observe that all the baselines perform much worse on hard questions.The best method achieves an average success rate of 43.13% on easy questions, while that number drops to 8.24% on hard questions.As mentioned in § 3, the hard questions in ToolQA require more tool calls and more complicated compositions.Current toolaugmented LLMs struggle with answering such hard questions, which requires further development of techniques to improve their ability to reason about the task and generate plans for tool use.</p>
<p>GPT-3 vs. GPT3.5. 4 Comparing the different versions of ReAct, we observe that the ReAct (GPT-3) outperforms ReAct (GPT-3.5) on easy questions, yet it shows inferior performance on hard questions.Our hypothesis is that for easy questions, it is more important to learn and follow the format of the tool calls in the context, which GPT-3 is stronger at.For hard questions, the better reasoning and code understanding abilities of GPT-3.5 enables it to come up with "innovative" solutions that never appear in the context, leading to higher success rates.An example can be referred to in § 5.3.</p>
<p>Result Analysis and Discussion</p>
<p>We analyze the drawbacks and possible improvements of existing tool-augmented LLMs, taking the best-performed ReAct (GPT-3.5)model on the hard questions of ToolQA as an example.</p>
<p>Main Error Type I: Argument Errors</p>
<p>By performing comprehensive error analysis, we found that the most common error type when asking LLMs to use tools for QA is argument error -LLMs calling the tools with wrong arguments.For ReAct, this error type makes 44.56% and 48.23% out of the 377 and 436 error cases on easy and hard questions respectively, as shown in Figure 3(a).Interestingly, ReAct shows different argument error patterns on easy and hard questions.On easy questions, it tends to make more mistakes on database-related tools.For example, the model commits a total of 120 errors when calling LoadDB, FilterDB, and GetValue tools for easy questions, while this number reduces to 95 for hard questions.On the other hand, when dealing with code-related tools (e.g., SQLInterpreter and PythonInterpreter), ReAct makes nearly 10x more errors for hard questions than for easy ones.This phenomenon is likely because the solution logic for hard questions is often more complex and cannot be fully inferred from the context alone.Consequently, the LLMs tend to rely on their understanding of code and programming concepts to tackle these intricate questions.In contrast, for easy questions, the LLMs tend to follow the patterns provided in the context, attempting to combine different database operations to arrive at a solution.</p>
<p>Main Error Type II: Incorrect Data Source</p>
<p>We have conducted an investigation into the data sources preferred by LLMs when answering questions.We found that LLMs also have difficulties in identifying the proper reference corpora answer the questions.This behavior is graphically represented as a confusion matrix in Figure 3(b).Upon examining the figure, it is apparent that for target reference corpora like Flight, Coffee, Airbnb, and Yelp that contain temporal information, LLMs are more likely to query the Agenda corpus for answering questions.Similarly, given that the SciREX knowledge corpora and DBLP graph are both in the scientific domain, LLMs tend to be confused about which source to query when answering scientific questions.</p>
<p>Main Error Type III: Innovation and Hallucination</p>
<p>For in-context tool-augmented LLMs, it is typical to include descriptions and use-case examples of each tool in the prompt.However, as the problem complexity increases with the number of tools, it becomes challenging to encompass all possible instances of compositional tool use as few-shot exemplars.Consequently, it is vital for LLMs to uncover logical relationships among different tools, which have never been encompassed in the human-provided exemplars, to solve challenging tasksa process we refer to as "innovation."However, these innovative behaviors are a double-edged sword as they are often accompanied by hallucinations.Figure 4 illustrates this phenomenon with a case study, where LLMs answer hard questions with reference Coffee data.Given the context length constraint, the few-shot exemplar only showcases the basic usage of database operations and the SQL interpreter.For the hard question in Figure 4(a), ReAct (GPT-3) strictly follows the operations displayed in the context, leading to failure.On the contrary, ReAct (GPT-3.5)innovatively identifies the SQL interpreter as a possible alternative to database operations, especially when the latter fails repeatedly.However, such innovations can oftentimes lead to hallucinations.As shown in Figure 4(b), when answering another hard question from the Coffee data, ReAct (GPT-3.5)opts to hallucinate certain observations (highlighted in yellow) that are non-existent in the feedback from tool execution.</p>
<p>Other Error Types</p>
<p>We manually go through and count all the errors made by the ReAct (GPT-3.5)model and show the errors on both easy and hard questions in Figure 5.In addition to the aforementioned 3 main error types, there are 4 error types that frequently occur:</p>
<p>Coffee-Hard</p>
<p>Question: What was the average coffee price from 2019-10-31 to 2021-07-23?Correct Answer: 120.9</p>
<p>ReAct (GPT-3)</p>
<p>Thought 1: This is a question related to coffee.We need to load the coffee database.Action 1: LoadDB[coffee] Observation 1: We have successfully loaded the coffee database...   • Infeasible Actions: The execution of tool calls are infeasible in the environment, often involving new tools that do not exist in the pre-defined tool pool.• Too Long Context: The encoding of interaction history, observations, and tool-use plans exceed the length limitation of GPT family models, resulting in runtime errors; • Mis-understanding: The LLMs cannot understand the observations obtained from external interaction and fail to determine the next steps or generate answers; • Low-Quality Retrieval: This error occurs when the retrieval model fails to extract the relevant information from text corpora, indicating insufficient external knowledge for LLMs to answer questions accurately.</p>
<p>Comparing these error types on easy and hard questions, we find that the overall distribution is similar, though there is a slightly higher rate of hallucination and long-context errors when answering hard questions.This can be attributed to the complexity of hard questions, which often require composing more tools for question answering.</p>
<p>Potential Misuse of ToolQA</p>
<p>We summarize the potential misuse of the ToolQA dataset and Tool-augmented LLMs as follows:</p>
<p>• If LLMs can be trained or prompted to use external tools, they could be prompted to use tools that extract personal information, propagate malware, or provide misleading information; • There is potential for future LLMs to over-rely on these external tools, sacrificing their intrinsic reasoning abilities.This can make them less versatile in situations where tool use is not feasible; • As LLMs are prompted to interact with more external systems, the security risks can increase.Malicious actors might find ways to exploit the interactions between LLMs and the external tools they leverage.</p>
<p>Comparing Open-Source LLMs with ChatGPT</p>
<p>Table 5 shows the comparison of vanilla LLaMA-2, tool-augmented LLaMA-2, and tool-augmented ChatGPT.There is a significant performance gap between vanilla LLaMA-2 and tool-augmented LLaMA-2, which is consistent with what we have observed on closed-source LLMs in Section 4.2.</p>
<p>In terms of the ability to use external tools for answering the questions, we found that LLaMA-2 is indeed lagging behind ChatGPT.The prompts tailored for tool-augmented LLMs tend to be complicated and lengthy, containing tool descriptions, few-shot examples, and interaction history with the environment.Such long contexts make it difficult for LLaMA-2 to understand complex instructions hidden inside.</p>
<p>Conclusion and Recommendation</p>
<p>We have developed ToolQA, a dataset that assesses the ability of Large Language Models (LLMs) in using external tools for solving complex problems.ToolQA is curated by an automated three-phase process, including reference data collection, template-based question generation, and programmatic answer generation.This pipeline is general and can be expanded to incorporate external knowledge corpora in different domains.We tested both standard LLMs and tool-augmented LLMs on ToolQA.</p>
<p>Our experiments showed that even the strongest model achieved limited performance on the hard questions of ToolQA.Our analysis found that current tool-augmented LLMs tend to make errors such as incorrect tool calls and using incorrect data sources.These issues could be potentially addressed by fine-tuning using a collection of tool-use corpora with open-source LLMs.In the future, we plan to collect high-quality tool-use sequences to fine-tune open-source LLMs, and subsequently evaluating their performance on ToolQA.</p>
<p>It is important to note that the reported performance of closed-source LLMs, such as ChatGPT, is based on specific versions of these models.As these closed-source models undergo further development and updates, the results may change accordingly.We advocate two strategies to mitigate the issue: (1) We plan to continuously include the performance of newly released models in our repository, with the collective efforts of both our team and peer researchers of the community.We intend to integrate results from different versions of such product APIs and document the identifiers of each version (e.g., 0314, 0613, etc.) to promote reproducibility.(2) The reported performance of closed-source LLMs should be regarded primarily as a reference, and making comparisons with them is optional.Instead, we encourage the community to use the performance of open-source LLMs as the primary baseline when employing the ToolQA benchmark.This will promote reproducibility and consistency of the evaluation on ToolQA over time.</p>
<p>• Agenda is our own synthetic dataset to model the real-world personal agenda data.To avoid the privacy issue, we first create names, events, and dates with ChatGPT and then randomly compose them to form 10000 different records.To create a pure-text personal agenda corpus, we feed each of the records into ChatGPT, containing generated agenda for virtual characters.More Details can be seen in Appendix B.2.</p>
<p>B.2 Generation Details of Agenda Dataset</p>
<p>As mentioned in § 3.2, personal or private data serves as a significant external knowledge source.There exist applications that have been designed with plugins and external tools specifically querying this type of data, such as AI personal assistants on daily agenda.Nevertheless, we recognize that this data often intersects with sensitive areas, and hence, privacy concerns are paramount.To address these issues, we automatically synthesize a personal agenda corpus.This not only ensures that the large language models (LLMs) have not been previously exposed to the data but also eliminates any possibility of them inadvertently memorizing the information within their internal knowledge.</p>
<p>In the synthetically generated personal agenda corpus, each entry follows the pattern: "NAME performs EVENT at TIME on DATE", incorporating key elements such as names, events, dates, and time slots.</p>
<p>To begin, we employ ChatGPT to virtually generate these elements.More precisely, we create 100 unique names, 10000 distinctive events each associated with corresponding time slots within a day, and span all possible dates from 01/01/2022 through 12/31/2022.Following this, we commence the random assembly of these generated elements to formulate personal agenda entries.For every eventtime pair generated, we randomly select from the pool of 100 names and possible dates to construct each record.This process yields a total of 9,494 unique personal agenda entries.To transform this corpus into an accessible external database for model querying, we transcribe each record into a comprehensible natural language description.Prompts designed for agenda data generation are listed in Appendix F.2.</p>
<p>C Easy Question Templates C.1 Flights</p>
<p>We design the following 10 templates:</p>
<p>• What was the departure time of the {CARRIER}{NUMBER} flight from {ORIGIN} to {DEST} on {ORIGIN}?</p>
<p>• Was the flight {CARRIER}{NUMBER} from {ORIGIN} to {DEST} cancelled on {ORIGIN}?</p>
<p>• What is the flight number of the {AIRLINE} flight from {ORIGIN} to {DEST} on {ORIGIN}?</p>
<p>• How long was the different between the CRS-recorded departure time and actual departure time of the {CARRIER}{NUMBER} flight from {ORIGIN} to {DEST} on {ORIGIN}?• How long did {CARRIER}{NUMBER} delay when arrival on {DEST}?• How many extra minutes did the {CARRIER}{NUMBER} flight take from {ORIGIN} to {DEST} on {ORIGIN}?• What was the local arrival time of the {CARRIER}{NUMBER} flight from {ORIGIN} to {DEST} on {ORIGIN}?• What was the CRS-recorded arrival time of the {CARRIER}{NUMBER} flight from {ORIGIN} to {DEST} on {ORIGIN}?• How long was the flight {CARRIER}{NUMBER} from {ORIGIN} to {DEST} on {ORIGIN}?• How many minutes did the {CARRIER}{NUMBER} flight take to taxi in on {DATE}?</p>
<p>C.2 Coffee</p>
<p>We design the following 8 templates:</p>
<p>• What was the daily coffee price opening on {DATE}?</p>
<p>• What was the lowest coffee price on {DATE}?</p>
<p>• What was the highest coffee price on {DATE}?</p>
<p>• What was the daily coffee price closing on {DATE}?</p>
<p>• What was the trading volume of coffee on {DATE}?</p>
<p>C.7 DBLP</p>
<p>We design the following 10 templates for easy questions on DBLP dataset:</p>
<p>• Who are the authors of {TITLE}?</p>
<p>• What organization is {AUTHOR} from?• How many pages is {TITLE}?</p>
<p>• How many papers did {TITLE} cite in the DBLP citation network?• How many papers did papers in the DBLP citation network cite {TITLE}?</p>
<p>• How many collaborators does {AUTHOR} have in the DBLP citation network?• How many papers did {AUTHOR} and {AUTHOR} write together in the DBLP citation network?• What papers did {AUTHOR} write in the DBLP citation network?• How many papers did {AUTHOR} write in the DBLP citation network?• What venue did {AUTHOR} and {AUTHOR} collaborate most in the DBLP citation network?</p>
<p>C.8 GSM8K</p>
<p>The questions are randomly sampled from the ChatGPT errors in GSM8K dataset without following some templates.Thus, we cannot offer any question templates for GSM8K.</p>
<p>D Hard Question Templates D.1 Flights</p>
<p>• What percentage of the flights from {ORIGIN} were delayed on {FLIGHTDATE}?</p>
<p>• What is the average delay time of all the flights that departed from {ORIGIN} on {FLIGHTDATE}?</p>
<p>• How many flights were diverted on {FLIGHTDATE}?</p>
<p>• How many flights with a distance greater than 500 miles on {FLIGHTDATE}?</p>
<p>• What is the average airtime of the flights from {ORIGIN} to {DEST} host by {AIRLINE}?</p>
<p>• How many flights from {ORIGIN} to {DEST} host by {AIRLINE}?</p>
<p>• What is the average flight time of {CARRIER}{NUMBER}?</p>
<p>• What is the fastest flight from {ORIGIN} to {DEST} on {FLIGHTDATE}?</p>
<p>• What is the average speed of {CARRIER}{NUMBER} from {ORIGIN} to {DEST}?</p>
<p>• What is the total number of flights operated by {AIRLINE} on {FLIGHTDATE}?</p>
<p>D.2 Coffee</p>
<p>• What was the highest coffee price from {START-DATE} to {END-DATE}?</p>
<p>• What was the lowest coffee price from {START-DATE} to {END-DATE}?</p>
<p>• What was the average coffee price from {START-DATE} to {END-DATE}?</p>
<p>• How much did the coffee price change from {START-DATE} to {END-DATE}?</p>
<p>• What was the percentage change in coffee price on {DATE} compared to the previous day?• On which date from {START-DATE} to {END-DATE} was the difference between the highest and lowest coffee prices the greatest?• What was the average daily volume of coffee traded from {START-DATE} to {END-DATE}?</p>
<p>• On which date from {START-DATE} to {END-DATE} did the coffee price have the highest increase compared to the previous day?• How many times from {START-DATE} to {END-DATE} did the coffee price increase compared to the previous day?• What was the percentage increase in coffee price from {START-DATE} to {END-DATE}?</p>
<p>• What was the coffee price range from {START-DATE} to {END-DATE}?</p>
<p>E Code Examples of Programmatic Answer Generation</p>
<p>Below is an example of programmatic answer generation.The example code is answering the question of "What percentage of the flights from {ORIGIN} were delayed on {FLIGHTDATE}?".More details of the programmatic answers can be seen in the public code.All experiments are conducted on CPU: Intel(R) Core(TM) i7-5930K CPU @ 3.50GHz and GPU: NVIDIA GeForce RTX A5000 GPUs using python 3.8, Huggingface 4.6.0 and Pytorch 1.10.We keep the parameter top_p = 1.0 and temperature t = 1.0 for calling ChatGPT APIs [43] for the question generation part.</p>
<p>F.2 Prompts</p>
<p>F.2.1 Prompts for Agenda Data Generation</p>
<p>The prompts used for virtual name generation:</p>
<p><Agenda_Name_Gen> Prompt You are an AI assistant to answer questions.Can you list 100 English Names?</p>
<p>The prompts used for virtual events generation: <Agenda_Events _Gen> Prompt You are an AI assistant for text generation.Generate 100 detailed agenda events, including the event, start time, end time, and location.Please make the events as diverse as possible and make sure these events can happen in real life.Make sure the location is a detailed name that may exist in real life.Make sure the dates are selected from 2022/01/01 to 2023/01/01.</p>
<p>Example: Doctor's appointment -9:00 AM -11:00 AM -ABC Medical Center Yoga class -10:30 AM -11:30 AM -Yoga Studio Downtown Generate 100 more detailed agendas that do not conflict with the previous ones.</p>
<p>The prompts used to convert the agenda records into natural language descriptions: <Agenda_Gen> Prompt Please use natural language to describe the event in the agenda with the following information:</p>
<p>F.2.2 Prompts for Methods</p>
<p>The prompts used in ReAct [76]:  Question: Where did Stephen's Opera performance take place?Thought 1: The question is asking the location of Stephen's Opera performance from agenda.Action 1: RetrieveAgenda[Stephen's Opera performance] Observation 1: On January 29, 2022, there will be an opera performance at the Lyric Opera House, featuring Stephen.The show will start at 7:00 PM and end at 9:00 PM.It promises to be a wonderful evening of beautiful music and powerful performances in a stunning venue.Come and experience the magic of opera at its finest!Thought 2: The event happened in Lyric Opera.The prompts used in Chameleon [32]:</p>
<p><Chameleon> Prompt You need to act as a policy model, that given a question and a modular set, determines the sequence of modules that can be executed sequentially can solve the question.</p>
<p>The modules are defined as follows:</p>
<p>-Calculate[formula]: This module calculates a given formula and returns the result.It takes in a mathematical formula and returns the calculated result.Normally, we only consider using "Calculate" when the question involves mathematical computations.</p>
<p>-RetrieveAgenda[keyword]: This module retrieves an agenda related to a specific keyword and returns it.It takes in a keyword and returns the corresponding agenda.Normally, we only consider using "RetrieveAgenda" when the question is about specific actions or tasks related to a topic.</p>
<p>-RetrieveScirex[keyword]: This module retrieves paragraphs from machine learning papers related to the specified keyword and returns them.It takes in a keyword and returns the relevant paragraphs.Normally, we only consider using "RetrieveScirex" when the question involves understanding specific concepts in machine learning.</p>
<p>-LoadDB[DBName]: This module loads a database specified by the database name and returns the loaded database.It takes in a database name and returns the corresponding database.The DBName can be one of the following: flights/ coffee/airbnb/yelp.Normally, we only consider using "LoadDB" when the question requires data from a specific structured dataset.</p>
<p>-FilterDB[column_name, relation, value]: This module filters a database by a specified column name, relation, and value, and then returns the filtered database.It takes in a column name, a relation, and a value, and returns the filtered database.Normally, we only consider using "FilterDB" when the question requires a specific subset of data from a structured dataset.</p>
<p>-GetValue[column_name]: This module returns the value of a specified column in a database.It takes in a column name and returns its value.Normally, we only consider using "GetValue" when the question requires a specific piece of data from a structured dataset.</p>
<p>-LoadGraph[GraphName]: This module loads a graph specified by the graph name and returns the loaded graph.It takes in a graph name and returns the corresponding graph.Normally, we only consider using "LoadGraph" when the question involves understanding or navigating specific graph structures.</p>
<p>-NeighbourCheck[GraphName, Node]: This module lists the neighbors of a specified node in a graph and returns the neighbors.It takes in a graph name and a node, and returns the node's neighbors.Normally, we only consider using "NeighbourCheck" when the question involves understanding relationships in a graph structure.</p>
<p>-NodeCheck[GraphName, Node]: This module returns the detailed attribute information of a specified node in a graph.It takes in a graph name and a node, and returns the node's attributes.Normally, we only consider using "NodeCheck" when the question requires information about a specific entity in a graph.</p>
<p>-EdgeCheck[GraphName, Node1, Node2]: This module returns the detailed attribute information of the edge between two specified nodes in a graph.It takes in a graph name and two nodes, and returns the attributes of the edge between them.Normally, we only consider using "EdgeCheck" when the question involves understanding the relationship between two entities in a graph.</p>
<p>-SQLInterpreter[SQL]: This module interprets a SQL query and returns the result.It takes in a SQL query and returns the result of the query.Normally, we only consider using "SQLInterpreter" when the question requires data manipulation and extraction from a structured dataset.</p>
<p>-PythonInterpreter[Python]: This module interprets Python code and returns the result.It takes in Python code and returns the result of the code execution.Normally, we only consider using "PythonInterpreter" when the question requires complex computations or custom data manipulation.</p>
<p>-Finish[answer]: This module returns the final answer and finishes the task.This module is the final module in the sequence that encapsulates the result of all previous modules.</p>
<p>O</p>
<p>'Neal was drafted by the Orlando Magic with the first overall pick in the 1992 NBA draft.He quickly became one of the best centers in the league… Kobe Bryant was drafted by the Charlotte Hornets with the 13th pick of the 1996 draft, but his draft rights were immediately traded to the Los Angeles Lakers… Jordan joined the Bulls in 1984 as the third overall draft pick and quickly emerged as a league star, entertaining crowds with his prolific scoring… Pre-train Corpus Question: What team did Kobe Bryant start his NBA career with?… Retrieve Kobe Bryant was drafted by the Charlotte Hornets with the 13th pick of the 1996 draft, but his draft rights were immediately traded to the Los Angeles Lakers… LLM with implicit knowledge Reasoning with retrieval Directly input Using tools or only memorizing?Los Angeles Lakers Los Angeles Lakers</p>
<p>Figure 1 :
1
Figure 1: Pre-trained on vast range of corpus, LLMs possess extensive knowledge, which may overlap with evaluation data.This overlap poses a significant challenge to current evaluation methods, as it becomes difficult to discern whether the model is merely recalling pre-trained information or genuinely employing external tools for problem-solving.</p>
<p>Templates: • Did the flight from {Origin} to {Dest} on {Date} get cancelled or diverted?(External Knowledge) • What was the flight distance for the flight from {Origin} to {Dest} on {Date}?(Internal Knowledge) • Which product on {FlightNumber} has the highest price?(Not Mentioned) ... ... def question_gen(table_row): Origin = table_row["Origin"] Dest = table_row["Dest"] FlightDate = table_row["FlightDate"] ... return question, answer</p>
<p>Figure 2 :
2
Figure 2: ToolQA, aiming to faithfully evaluate LLMs' abilities to use external tools, curates data through three phases: (a) Reference Data Collection; (b) Human-Guided Question Generation; and (c) Programmatic Answer Generation.</p>
<p>Confusion matrix of questions from different resources in ToolQA.</p>
<p>Figure 3 :
3
Figure 3: Analysis of incorrect tool calls and incorrect data sources made by ReAct on ToolQA.</p>
<p>Thought 2 : 2 : 2 :Thought 3 : 3 :Thought 3 : 3 :Thought 4 : 4 : 4 :Thought 5 : 5 : 94999999999989 Thought 6 : 6 : 9 ] 6 : 5 ReAct 6 : 6 : 6 :Thought 7 : 7 :Thought 8 : 8 : 75 Thought 9 : 9 :Figure 4 :
2223333444559499999999998966965666778875994
Figure 4: An example of innovation and hallucination when answering hard questions on Coffee data.Actions and observations shrouded in pink are incorrect, whereas those in green are correct.Terms highlighted in yellow signify hallucinations produced by ReAct (GPT-3.5).</p>
<p>Figure 5 :
5
Figure 5: Error analysis of ReAct on ToolQA.</p>
<p>D. 7 DBLP
7
We design the following 10 templates for hard questions on DBLP dataset:• What keywords does {AUTHOR} focus on most in the DBLP citation network?• How many people does {AUTHOR-1} need to know at least to know {AUTHOR-2} in the DBLP citation network?• How many common collaborators does {AUTHOR-1} have with {AUTHOR-2}?• Which is the most cited paper written by {AUTHOR} in the DBLP citation network?• Which collaborator does {AUTHOR} have the most citations with in the DBLP citation network?• Which venue does {AUTHOR} publish the most papers in the DBLP citation network?• How many accumulated citations do papers collaborated by {AUTHOR-1} and {AUTHOR-2} have in the DBLP citation network?• How many papers in all do {AUTHOR} and his/her collaborators have in the DBLP citation network?• Who collaborated with {AUTHOR} most in the DBLP citation network?• What institutions participated in the study of {TITLE} in the DBLP citation network?</p>
<p>def solution(data, flightdate, origin): num_total =len(data.loc[(data["FlightDate"]==flightdate) &amp; (data["Origin"] == origin)]) num_cancelled =len(data.loc[(new_data["FlightDate"]==flightdate) &amp; (data["Origin"] ==origin) &amp; (data["Cancelled"] ==True)]) if num_cancelled &gt;0: question ="What percentage of the flights from {} were delayed on {}?".format(origin, flightdate) answer ="{:.1f}".format(num_cancelled/num_total *100)+"%" F Additional Implementation Details F.1 Implementation Details</p>
<p>Name: NAME Date: DATE Event: EVENT Start Time: START-TIME End Time: END-TIME Location: LOCATION</p>
<p>many extra minutes did the DL1575 flight take from ATL to MCO on 2022-01-12?Thought 1: This is a question related to flights.We need to load the flights database.Action 1: LoadDB[flights] Observation 1: We have successfully loaded the flights database, including the following columns: FlightDate, Airline, Origin, Dest, Cancelled, Diverted, CRSDepTime, DepTime, DepDelayMinutes, DepDelay, ArrTime, ArrDelayMinutes, AirTime, CRSElapsedTime, ActualElapsedTime, Distance, Year, Quarter, Month, DayOfWeek, Marketing_Airline_Network, Operated_or_Branded_Code_Share_Partners, DayofMonth, DOT_ID_Marketing_Airline, IATA_Code_Marketing_Airline, Flight_Number_Marketing_Airline, Operating_Airline, DOT_ID_Operating_Airline, IATA_Code_Operating_Airline, Tail_Number, Flight_Number_Operating_Airline, OriginAirportID, OriginAirportSeqID, OriginCityMarketID, OriginCityName, OriginState, OriginStateFips, OriginStateName, OriginWac, DestAirportID, DestAirportSeqID, DestCityMarketID, DestCityName, DestState, DestStateFips, DestStateName, DestWac, DepDel15, DepartureDelayGroups, DepTimeBlk, TaxiOut, WheelsOff, WheelsOn, TaxiIn, CRSArrTime, ArrDelay, ArrDel15, ArrivalDelayGroups, ArrTimeBlk, DistanceGroup, DivAirportLandings.Thought 2: We need to filter the information related to the query.Action 2: FilterDB[Flight_Number_Marketing_Airline=1575, FlightDate=2022-01-12, Origin=ATL, Dest=MCO] Observation 2: We have successfully filtered the data (1 row).Thought 3: We then need to know the departure delayed time.Action 3: GetValue[DepDelay] Observation 3: -7.0 Thought 4: We then need to know the arrival delayed time.Action 4: GetValue[ArrDelay] Observation 4: -17.0 Thought 5: To compute the extra minutes, we need to subtract the departure delayed time from the arrival delayed time.Action 5: Calculate[(-17)-(-7)] Observation 5: -10</p>
<p>Action 2 :
2
Finish[Lyric Opera] Question: What was the trading volume of coffee on 2000-01-14?Thought 1: Maybe I can write a SQL query to solve this problem.Action 1: SQLInterpreter(SELECT Volume FROM coffee.coffee_dataWHERE Date = '2000-01-14'; Observation 1: Volume: 10115 Thought 2: The volume of coffee on 2000-01-14 is 10115.Action 2: Finish[10115]</p>
<p>Below are some examples that map the problem to the modules.Question: How many extra minutes did the DL1575 flight take from ATL to MCO on 2022-01-12?Modules: ["LoadDB[flights]", "FilterDB[Flight_Number_Marketing_Airline=1575, FlightDate=2022-01-12, Origin=ATL, Dest=MCO]", "GetValue[DepDelay]", "GetValue[ArrDelay]", "Calculate[(-17)-(-7)]", "Finish[-10]"] Question: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of wednesday?Modules: ["PythonInterpreter[# solution in Python:\n\ndef solution():\n # Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of wednesday?\n</p>
<p>Table 1 :
1
Dataset Statistics of ToolQA.
ContextTopicExternal KnowledgeEasyHardFormatSize# Templates # Questions # Templates # QuestionsTemporalFlight CoffeeTabular Database 4078318 Tabular Database 574610 8100 10010 13100 130SpatialYelp AirbnbTabular Database Tabular Database150346 10259911 10100 10010 10100 100Mathematical GSM8K Professional Ability--100--SocialDBLPGraph5533201010010100ScientificSciREX Pure-Text Corpus43811004100PersonalAgenda Pure-Text Corpus1000051005100SUM---5580062730</p>
<p>Table 2 :
2
Different tools in ToolQA.
Tool Types# Tools ToolsText Tools2Agenda Retriever, SciREX RetrieverDatabase Tools3Database Loader, Data Filter, Get ValueMath Tools1WolframAlpha CalculatorGraph Tools4Graph Loader, Neighbour Checker, Node Checker, Edge CheckerCode Tools2Python Interpreter, SQL InterpreterSystem Tools1Finish</p>
<p>Table 3 :
3
Success rates on easy questions.</p>
<p>Table 4 :
4
Success rate on hard questions.
LLM CategoryModelsFlight Coffee Agenda Yelp Airbnb DBLP SciREX AverageLLaMA-2 (13B) 1.00.00.04.01.05.01.01.7Open-Sourced LLMsFalcon (40B)1.00.00.04.01.06.01.01.9LLaMA-2 (70B) 1.00.00.04.01.04.03.01.9Closed-Sourced LLMsChatGPT CoT2.0 0.02.3 0.81.0 0.00.0 1.02.0 0.04.0 3.03.0 5.02.0 1.4Chameleon3.02.30.00.00.08.00.01.9Tool-Augmented LLMsReAct (GPT-3)3.010.80.03.00.019.00.05.1ReAct (GPT-3.5) 5.017.77.08.07.05.08.08.2</p>
<p>Table 5 :
5
Performance of tool-augmented open-sourced LLM.
MethodsCoffee-Easy Coffee-HardLLaMA-2 (13B)2.00.0ReAct (LLaMA-2, 13B)31.06.2ReAct (GPT-3.5)81.017.7</p>
<p>Thought 6 :
6
[33]r calculation, we know that the answer is -10.Action 6: Finish[-10]Question: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of wednesday?Thought 1: Maybe I can write a python program to solve this problem.After execution of Python Code, we know that the answer is 33.Action 2: Finish[33]Question: What is the corresponding Mean_IoU score of the FRRN method on Cityscapes dataset for Semantic_Segmentation task?Thought 1: The question is asking some performance information about Mean_IoU score of the FRRN method on Cityscapes dataset for Semantic_Segmentation task, we need to retrieve some useful information from the scirex database.Action 1: RetrieveScirex[Mean_IoU score of the FRRN method on Cityscapes dataset for Semantic_Segmentation task] Observation 1: The other stream undergoes a sequence of pooling operations to obtain robust features for recognition .The two streams are coupled at the full image resolution using residuals .Without additional processing steps and without pretraining , our approach achieves an intersection -over -union score of 71.8 % on the Cityscapes dataset .Thought 2: As the approach achieves an intersection -over -union score of 71.8 % on the Cityscapes dataset, the answer should be 71.8%.The number of common papers of Chao Zhang and Weihong Lin is 1.
Thought 3: The number of collaborators of Chao Zhang is 6.Action 3: Finish[6]Question: How many papers does Chao Zhang and Weihong Lin have in common in theDBLP graph?Thought 1: The question is asking the number of common papers of two persons, weneed load the DBLP graph.Action 1: PythonInterpreter[# solution in Python: Action 1: LoadGraph[dblp]def solution(): Observation 1: DBLP graph is loaded.# Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, Thought 2: The question is asking the common papers of two persons, we need to# he lost 2 more. How many golf balls did he have at the end of wednesday? check the edges between them from the PaperNet in DBLP graph.golf_balls_initial = 58 Action 2: EdgeCheck[PaperNet, Chao Zhang, Weihong Lin]golf_balls_lost_tuesday = 23 Observation 2: {'weight': 1, 'papers': ['HRFormer: High-Resolution Visiongolf_balls_lost_wednesday = 2 Transformer for Dense Predict.'], 'n_citation': [95]}golf_balls_left = golf_balls_initial -golf_balls_lost_tuesday -\ Thought 3:golf_balls_lost_wednesdayresult = golf_balls_leftreturn result]Observation 1: 33Thought 2: Action 2: Finish[71.8%]Question: When was the paper Learning the Principle of Least Action withReinforcement Learning. published?Thought 1: The question is asking some basic information of a dblp paper. We needto load the DBLP graph.Action 1: LoadGraph[dblp]Observation 1: DBLP graph is loaded.Thought 2: The question is asking the published date of a paper, we need to checkthe node from the PaperNet in DBLP graph.Action 2: NodeCheck[PaperNet, Learning the Principle of Least Action withReinforcement Learning.]Observation 2: {'title': 'Learning the Principle of Least Action with ReinforcementLearning.', 'year': 2021, 'venue': {'raw': 'AAAI Spring Symposium -MLPS'},'n_citation': 0, 'keywords': [], 'doc_type': 'Conference', 'page_start': '','page_end': ''}Thought 3: The published date of the paper is 2021.Action 3: Finish[2021]Question: How many collaborators does Chao Zhang have in the DBLP graph?Thought 1: The question is asking the number of collaborators of a person, weneed load the DBLP graph.Action 1: LoadGraph[dblp]Observation 1: DBLP graph is loaded.Thought 2: The question is asking the collaborators of a person, we need tocheck the neighboring nodes from the AuthorNet in DBLP graph.Action 2: NeighbourCheck[AuthorNet, Chao Zhang]Observation 2: ['YUHUI YUAN', 'Rao Fu', 'Lang Huang', 'Weihong Lin', 'X Chen','Jingdong Wang']
https://products.wolframalpha.com/api
 was not included in the evaluation as we have no access to its API.
https://www.kaggle.com/datasets/robikscube/flight-delay-dataset-20182022?select= Combined_Flights_2022.csv
https://www.kaggle.com/datasets/psycon/daily-coffee-price
https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset?select=yelp_academic_ dataset_business.json
https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata
https://www.aminer.org/citation
https://github.com/openai/grade-school-math
https://github.com/allenai/SciREX
Acknowledgments and Disclosure of FundingThis work was supported in part by NSF (IIS2008334, IIS-2106961, CAREER IIS-2144338), ONR (MURI N00014-17-1-2656), IDEaS Cyberinfrastructure Resources, and Microsoft Accelerate Foundation Models Research Program.A Additional Related WorksMethodsTool Numbers Tool Categories # Tool/Task Reasoning Instruction Type TaskSingle-Tool MethodsCoT[65]1 -1 Generation Prompting QA Lila[38]1 math/code 1 Generation Prompting MathQA Program-of-Thought[6]1 code 1 Generation Prompting TabQA Code4Struct[63]1 code 1 Generation Prompting Event Extraction PAL[13]1 code 1 Generation Prompting MathQA MathPrompt[15]1 code 1 Generation Prompting MathQA ToolFormer[56]5  We list the state-of-the-art related works in tool-augmented LLMs in Table6.All of them can be categorized into two groups: (1) single-tool methods, that focus on making a single API call perfect in the solution;(2) multi-tool methods, that emphasize more on studying how to compose different tools together to solve a challenging problem.ToolQA is more suitable for the evaluation of the second category to test the inherent logical reasoning behind different tools.Additionally, there exist other notable contributions[64,25,59]within the realm of decision-making that specifically emphasize the planning capabilities of expansive language models.These endeavors can be regarded as methods affiliated with tools, wherein the actions within generated plans are analogous to distinct tools utilized for specific purposes.B Data Sources B.1 Different Data Source Introduction• Flight Status (2022-2023) 5 contains almost all flight information of airlines between 2022 and 2023, which is too contemporary for LLMs' internal knowledge.• Daily Coffee Price (2000-2022) 6 contains the daily price of coffee, ranging from 2000 to 2022, where the information is too contemporary and detailed for LLMs' internal knowledge.• Yelp Business Data 7 is a subset of Yelp's business data across 8 metropolitan areas in the USA and Canada, where the information is too detailed for LLMs' internal knowledge.• Airbnb Open Data 8 is a subset of Airbnb activities in New York, where the information is too detailed for LLMs' internal knowledge.• DBLP Citation Network (V14) 9 constructs the graph based on the records after 2020.The author-author and paper-paper relations are formulated as two separate graphs.• GSM8k 10 is a dataset of 8.5K high-quality linguistically diverse grade school math word problems.We sample the questions from the error cases made by ChatGPT on the original dataset to make sure that the questions cannot be easily handled with its internal knowledge.• SciREX 11 is a challenging dataset for document-level information extraction based on a collection of full-length machine-learning scientific papers.• What was the percentage change in coffee price on {DATE}, based on the difference between the opening and closing prices?• Was {DATE} a bearish or bullish day for coffee price?• What was the range of coffee price on {DATE}, based on the difference between the high and low prices?C.3 YelpWe design the following 11 templates for the Yelp dataset:• What is the address of {NAME} in the area of postal code {POSTAL-CODE}?• What city is {NAME} located in {STATE}?• What state is {NAME} located in?• What is the postal code of {NAME} in the area with postal code {POSTAL-CODE}, {CITY}, {STATE}?• What is the star rating of {NAME} in the area with postal code {POSTAL-CODE}, {CITY}, {STATE}?• How many reviews does {NAME} receive in the area with postal code • What are the hours of operation for {NAME} in the area with postal code {POSTAL-CODE}, {CITY}, {STATE}?• What categories does {NAME} belong to, in the area with postal code {POSTAL-CODE}, {CITY}, {STATE}?• What are the coordinates of {NAME} in the area with postal code {POSTAL-CODE}, {CITY}, {STATE}?C.4 AirbnbWe design the following 10 templates for easy questions on Airbnb dataset:• What is the host's name for {NAME} in {NEIGHBOURHOOD}?• How many days are {NAME} (id: {ID}) available during a year (365 days)?• What is the room type of {NAME} (id: {ID}) in {NEIGHBOURHOOD}?• What is the price of {NAME} (id: {ID}) in {NEIGHBOURHOOD}?• What is the minimum number of nights for {NAME} (id: {ID}) in {NEIGHBOURHOOD}?• When did {NAME} (id: {ID}) in {NEIGHBOURHOOD} constructed?• How many reviews does {NAME} (id: {ID}) in {NEIGHBOURHOOD} have?• What is the last review date for {NAME} (id: {ID}) in {NEIGHBOURHOOD}?• What is the review rate number for {NAME} (id: {ID}) in {NEIGHBOURHOOD}?• What is the average number of reviews per month for {NAME} (id: {ID}) in {NEIGHBOURHOOD}?C.5 SciREXWe design the following 1 templates for easy questions on SciREX dataset:• What is the corresponding {METRIC} score of the {METHOD} method on {DATASET} dataset for {TASK} task?C.6 AgendaWe design the following 5 templates for easy questions on Agenda dataset:• What did {NAME} do from {START-TIME} to {END-TIME} on {DATE}?• Where did {EVENT} that {NAME} attended take place on {DATE}?• When did {NAME} attend {EVENT} on {DATE}?• How long did {NAME} attend {EVENT} on {DATE}?• Who attended {EVENT} between {START-TIME} and {END-TIME} on {DATE} in {LOCATION}?D.3 YelpWe design the following 10 templates for hard questions in Yelp Dataset.• How many {CATEGORY} businesses are there in {CITY}, {STATE}?• How many businesses are there in {POSTALCODE} area of {CITY}, {STATE}?• Which {CATEGORY} business has the highest star rating in {CITY}, {STATE}?• Which {CATEGORY} business has the highest review count in {CITY}, {STATE}?"• What is the average review counts of businesses within a 5-mile radius from {NAME}?• Which is the nearest {CATEGORY} business to {NAME}?• Can you recommend a {CATEGORY} business with the highest star rating within a 5-mile radius of {ADDRESS}?• How many businesses are not open currently in {CITY}?• What is the average star rating of {CATEGORY} businesses in {CITY}?• Which region has most bussinesses in {CITY}, {STATE}?D.4 AirbnbWe design the following 10 templates for hard questions on Airbnb dataset.• What is the total price at least if you want to stay at {NAME} in {NEIGHBOURHOOD} for {NUMBER} nights?• How many airbnbs are there in {NEIGHBOURHOOD}?• What is the average price of airbnbs in {NEIGHBOURHOOD}?• What is the average review rates within 5 miles from {NAME} in {NEIGHBOURHOOD}?• How much proporion of airbnbs in {NEIGHBOURHOOD} have a flexible cancellation policy?• How much does it cost per night to stay at the most expensive entire home/apt in {NEIGHBOURHOOD}?• How many airbnbs are there in {NEIGHBOURHOOD} that have a review rate higher than 4? • Can you recommend me a hotel room with the lowest price in {NEIGHBOURHOOD}?• Can you recommend me a private room with the highest review rate that can host at least 2 people in {NEIGHBOURHOOD}?• Can you recommend a shared room with the lowest price within 10 miles from {LONGITUDE} longitude and {LATITUDE} latitude?D.5 SciREXWe design the following 4 templates for hard questions on SciREX dataset:• What is the corresponding {METRIC} score of the {METHOD} method on {DATASET} dataset for {TASK} task?• On which dataset does the {METHOD} method achieve the highest {METRIC} score for {TASK} task?• Which method achieves the highest {METRIC} score on {DATASET} dataset for {TASK} task?• On what metrics is the {METHOD} method evaluated on {DATASET} dataset for {TASK} task?• Which datasets is {METHOD} method evaluated on for {TASK} task?D.6 AgendaWe design the following 5 templates for hard questions on Agenda dataset:• How many events happen on {DATE} in the agendaG Key Information of ToolQA G.1 Dataset DocumentationsThe dataset is provided in jsonl format.Each task corresponds to two files: easy and hard (e.g., "flight-easy.jsonl"and "flight-hard.jsonl",etc.).Each data point contains the following fields:• qid: the unique identifier for the question-answer pair;• question: the question to query;• answer: the corresponding ground-truth answer to question.G.2 Intended UsesToolQA is intended for researchers in machine learning and related fields to innovate novel methods for tool-augmented large language models (LLMs).We also aim to help developers to test their plugins on our dataset.G.3 Hosting and Maintenance PlanToolQA codebase is hosted and version-tracked via GitHub.It will be permanently available under the link https://github.com/night-chen/ToolQA.The download link of all the datasets can be found in the GitHub repository.
Falcon-40B: an open large language model with state-of-the-art performance. E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Debbah, E Goffinet, D Heslow, J Launay, Q Malartic, B Noune, B Pannier, G Penedo, 2023</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Dassarma, D Drain, S Fort, D Ganguli, T Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>Improving language models by retrieving from trillions of tokens. S Borgeaud, A Mensch, J Hoffmann, T Cai, E Rutherford, K Millican, G B Van Den Driessche, J.-B Lespiau, B Damoc, A Clark, International conference on machine learning. PMLR2022</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. W Chen, X Ma, X Wang, W W Cohen, 2022</p>
<p>A dataset for answering time-sensitive questions. W Chen, X Wang, W Y Wang, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Teaching large language models to self-debug. X Chen, M Lin, N Schärli, D Zhou, arXiv:2304.051282023arXiv preprint</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Scaling instruction-finetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, E Li, X Wang, M Dehghani, S Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Timeaware language models as temporal knowledge bases. B Dhingra, J R Cole, J M Eisenschlos, D Gillick, J Eisenstein, W W Cohen, Transactions of the Association for Computational Linguistics. 102022</p>
<p>L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, arXiv:2211.10435Pal: Programaided language models. 2022arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, 2021NeurIPS</p>
<p>S Imani, L Du, H Shrivastava, arXiv:2303.05398Mathprompter: Mathematical reasoning using large language models. 2023arXiv preprint</p>
<p>G Izacard, M Caron, L Hosseini, S Riedel, P Bojanowski, A Joulin, E Grave, arXiv:2112.09118Towards unsupervised dense information retrieval with contrastive learning. 2021arXiv preprint</p>
<p>Few-shot learning with retrieval augmented language models. G Izacard, P Lewis, M Lomeli, L Hosseini, F Petroni, T Schick, J Dwivedi-Yu, A Joulin, S Riedel, E Grave, arXiv:2208.032992022arXiv preprint</p>
<p>SciREX: A challenge dataset for documentlevel information extraction. S Jain, M Van Zuylen, H Hajishirzi, I Beltagy, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Computing Surveys. 55122023</p>
<p>Genegpt: Augmenting large language models with domain tools for improved access to biomedical information. Q Jin, Y Yang, Q Chen, Z Lu, 2023ArXiv</p>
<p>TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. M Joshi, E Choi, D Weld, L Zettlemoyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20171</p>
<p>Dense passage retrieval for open-domain question answering. V Karpukhin, B Oguz, S Min, P Lewis, L Wu, S Edunov, D Chen, W.-T Yih, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>J Kasai, K Sakaguchi, Y Takahashi, R L Bras, A Asai, X Yu, D Radev, N A Smith, Y Choi, K Inui, arXiv:2207.13332Realtime qa: What's the answer right now?. 2022arXiv preprint</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. T Khot, H Trivedi, M Finlayson, Y Fu, K Richardson, P Clark, A Sabharwal, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Language models can solve computer tasks. G Kim, P Baldi, S Mcaleer, 2023</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in Neural Information Processing Systems. A H Oh, A Agarwal, D Belgrave, K Cho, 2022</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W -T. Yih, T Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Solving quantitative reasoning problems with language models. A Lewkowycz, A Andreassen, D Dohan, E Dyer, H Michalewski, V Ramasesh, A Slone, C Anil, I Schlag, T Gutman-Solo, arXiv:2206.148582022arXiv preprint</p>
<p>Api-bank: A benchmark for tool-augmented llms. M Li, F Song, B Yu, H Yu, Z Li, F Huang, Y Li, 2023</p>
<p>Unsupervised cross-task generalization via retrieval augmentation. B Y Lin, K Tan, C S Miller, B Tian, X Ren, Advances in Neural Information Processing Systems. 2022</p>
<p>B Liu, Y Jiang, X Zhang, Q Liu, S Zhang, J Biswas, P Stone, Llm+p: Empowering large language models with optimal planning proficiency. 2023</p>
<p>P Lu, B Peng, H Cheng, M Galley, K.-W Chang, Y N Wu, S.-C Zhu, J Gao, arXiv:2304.09842Chameleon: Plug-and-play compositional reasoning with large language models. 2023arXiv preprint</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. P Lu, L Qiu, K.-W Chang, Y N Wu, S.-C Zhu, T Rajpurohit, P Clark, A Kalyan, arXiv:2209.146102022arXiv preprint</p>
<p>Reacc: A retrievalaugmented code completion framework. S Lu, N Duan, H Han, D Guo, S.-W Hwang, A Svyatkovskiy, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Text and patterns: For effective chain of thought, it takes two to tango. A Madaan, A Yazdanbakhsh, arXiv:2209.076862022arXiv preprint</p>
<p>When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. A Mallen, A Asai, V Zhong, R Das, H Hajishirzi, D Khashabi, arXiv:2212.105112022arXiv preprint</p>
<p>S Mishra, M Finlayson, P Lu, L Tang, S Welleck, C Baral, T Rajpurohit, O Tafjord, A Sabharwal, P Clark, arXiv:2210.17517A unified benchmark for mathematical reasoning. 2022arXiv preprint</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. S Mishra, D Khashabi, C Baral, H Hajishirzi, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. 2021arXiv preprint</p>
<p>Investigating the limitations of transformers with simple arithmetic tasks. R Nogueira, Z Jiang, J Lin, arXiv:2102.130192021arXiv preprint</p>
<p>. OpenAI. Gpt-4 technical report. arXiv. 2023</p>
<p>. OpenAI. Introducing chatgpt. 2023</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>B Paranjape, S Lundberg, S Singh, H Hajishirzi, L Zettlemoyer, M T Ribeiro, arXiv:2303.09014Automatic multi-step reasoning and tool-use for large language models. Art2023arXiv preprint</p>
<p>A Parisi, Y Zhao, N Fiedel, arXiv:2205.12255Talm: Tool augmented language models. 2022arXiv preprint</p>
<p>Is a question decomposition unit all we need?. P Patel, S Mishra, M Parmar, C Baral, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>S G Patil, T Zhang, X Wang, J E Gonzalez, arXiv:2305.15334Gorilla: Large language model connected with massive apis. 2023arXiv preprint</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. B Peng, M Galley, P He, H Cheng, Y Xie, Y Hu, Q Huang, L Liden, Z Yu, W Chen, arXiv:2302.128132023arXiv preprint</p>
<p>Measuring and narrowing the compositionality gap in language models. O Press, M Zhang, S Min, L Schmidt, N A Smith, M Lewis, 2023</p>
<p>Limitations of language models in arithmetic and symbolic induction. J Qian, H Wang, Z Li, S Li, X Yan, arXiv:2208.050512022arXiv preprint</p>
<p>Tool learning with foundation models. Y Qin, S Hu, Y Lin, W Chen, N Ding, G Cui, Z Zeng, Y Huang, C Xiao, C Han, Y R Fung, Y Su, H Wang, C Qian, R Tian, K Zhu, S Liang, X Shen, B Xu, Z Zhang, Y Ye, B Li, Z Tang, J Yi, Y Zhu, Z Dai, L Yan, X Cong, Y Lu, W Zhao, Y Huang, J Yan, X Han, X Sun, D Li, J Phang, C Yang, T Wu, H Ji, Z Liu, M Sun, 2023</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>The probabilistic relevance framework: Bm25 and beyond. S Robertson, H Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>T L Scao, A Fan, C Akiki, E Pavlick, S Ilić, D Hesslow, R Castagné, A S Luccioni, F Yvon, M Gallé, arXiv:2211.05100A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, Toolformer, arXiv:2302.04761Language models can teach themselves to use tools. 2023arXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Y Shen, K Song, X Tan, D Li, W Lu, Y Zhuang, arXiv:2303.175802023arXiv preprint</p>
<p>W Shi, S Min, M Yasunaga, M Seo, R James, M Lewis, L Zettlemoyer, W.-T Yih, arXiv:2301.12652Replug: Retrieval-augmented black-box language models. 2023arXiv preprint</p>
<p>Adaplanner: Adaptive planning from feedback with language models. H Sun, Y Zhuang, L Kong, B Dai, C Zhang, 2023</p>
<p>FEVER: a large-scale dataset for fact extraction and VERification. J Thorne, A Vlachos, C Christodoulopoulos, A Mittal, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational LinguisticsJune 20181</p>
<p>. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, B Fuller, C Gao, V Goswami, N Goyal, A Hartshorn, S Hosseini, R Hou, H Inan, M Kardas, V Kerkez, M Khabsa, I Kloumann, A Korenev, P S Koura, M.-A Lachaux, T Lavril, J Lee, D Liskovich, Y Lu, Y Mao, X Martinet, T Mihaylov, P Mishra, I Molybog, Y Nie, A Poulton, J Reizenstein, R Rungta, K Saladi, A Schelten, R Silva, E M Smith, R Subramanian, X E Tan, B Tang, R Taylor, A Williams, J X Kuan, P Xu, Z Yan, I Zarov, Y Zhang, A Fan, M Kambadur, S Narang, A Rodriguez, R Stojnic, S Edunov, T Scialom, 2023Llama 2: Open foundation and fine-tuned chat models</p>
<p>Augmenting language models with long-term memory. W Wang, L Dong, H Cheng, X Liu, X Yan, J Gao, F Wei, 2023</p>
<p>Code4struct: Code generation for few-shot structured prediction from natural language. X Wang, S Li, H Ji, arXiv:2210.128102022arXiv preprint</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Z Wang, S Cai, A Liu, X Ma, Y Liang, 2023</p>
<p>Chainof-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, arXiv2022</p>
<p>Ethical and social risks of harm from language models. L Weidinger, J Mellor, M Rauh, C Griffin, J Uesato, P.-S Huang, M Cheng, M Glaese, B Balle, A Kasirzadeh, arXiv:2112.043592021arXiv preprint</p>
<p>Reframing human-ai collaboration for generating free-text explanations. S Wiegreffe, J Hessel, S Swayamdipta, M Riedl, Y Choi, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Wolfram|Alpha as the Way to Bring Computational Knowledge Superpowers to ChatGPT. S Wolfram, 2023Stephen Wolfram Writings</p>
<p>C Wu, S Yin, W Qi, X Wang, Z Tang, N Duan, arXiv:2303.04671Visual chatgpt: Talking, drawing and editing with visual foundation models. 2023arXiv preprint</p>
<p>J Xie, K Zhang, J Chen, R Lou, Y Su, arXiv:2305.13300Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts. 2023arXiv preprint</p>
<p>On the tool manipulation capability of open-source large language models. Q Xu, F Hong, B Li, C Hu, Z Chen, J Zhang, arXiv:2305.165042023arXiv preprint</p>
<p>Weakly-supervised scientific document classification via retrieval-augmented multi-stage training. R Xu, Y Yu, J C Ho, C Yang, arXiv:2306.071932023arXiv preprint</p>
<p>Z Yang, L Li, J Wang, K Lin, E Azarnasab, F Ahmed, Z Liu, C Liu, M Zeng, L Wang, arXiv:2303.11381Mm-react: Prompting chatgpt for multimodal reasoning and action. 2023arXiv preprint</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Z Yang, P Qi, S Zhang, Y Bengio, W Cohen, R Salakhutdinov, C D Manning, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOct.-Nov. 2018</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K R Narasimhan, Y Cao, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. J Zhang, 2023</p>
<p>SituatedQA: Incorporating extra-linguistic contexts into QA. M Zhang, E Choi, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNov. 2021Online and Punta Cana</p>
<p>R Zhang, Y Yu, P Shetty, L Song, C Zhang, arXiv:2203.09735Prboost: Prompt-based rule discovery and boosting for interactive weakly-supervised learning. 2022arXiv preprint</p>
<p>ReSel: Nary relation extraction from scientific text and tables by learning to retrieve and select. Y Zhuang, Y Li, J Zhang, Y Yu, Y Mou, X Chen, L Song, C Zhang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>ToolQA is a community-driven and open-source initiative. We are committed and have resources to maintain and actively develop ToolQA in the future. We plan to grow ToolQA to include more tasks, tools, and more baseline methods. 12Licensing We license our work using Apache 2.0 12 . All the datasets will be publicly released through the aforementioned GitHub link. G.5 Statement The authors will bear all responsibility in case of violation of rights</p>            </div>
        </div>

    </div>
</body>
</html>
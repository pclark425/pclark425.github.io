<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4559 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4559</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4559</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-48c83799530dc523ee01e6c1c40ad577d5c10a16</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/48c83799530dc523ee01e6c1c40ad577d5c10a16" target="_blank">DiscoveryBench: Towards Data-Driven Discovery with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The first comprehensive benchmark that formalizes the multi-step process of data-driven discovery is presented, which illustrates the challenges in autonomous data-driven discovery and serves as a valuable resource for the community to make progress.</p>
                <p><strong>Paper Abstract:</strong> Can the rapid advances in code generation, function calling, and data analysis using large language models (LLMs) help automate the search and verification of hypotheses purely from a set of provided datasets? To evaluate this question, we present DiscoveryBench, the first comprehensive benchmark that formalizes the multi-step process of data-driven discovery. The benchmark is designed to systematically assess current model capabilities in discovery tasks and provide a useful resource for improving them. Our benchmark contains 264 tasks collected across 6 diverse domains, such as sociology and engineering, by manually deriving discovery workflows from published papers to approximate the real-world challenges faced by researchers, where each task is defined by a dataset, its metadata, and a discovery goal in natural language. We additionally provide 903 synthetic tasks to conduct controlled evaluations across task complexity. Furthermore, our structured formalism of data-driven discovery enables a facet-based evaluation that provides useful insights into different failure modes. We evaluate several popular LLM-based reasoning frameworks using both open and closed LLMs as baselines on DiscoveryBench and find that even the best system scores only 25%. Our benchmark, thus, illustrates the challenges in autonomous data-driven discovery and serves as a valuable resource for the community to make progress.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4559.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4559.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HMS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis Match Score (HMS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A composite, outcome-based metric that quantifies alignment between an LLM-generated hypothesis and a gold (human) hypothesis by decomposing hypotheses into context, variables, and relationship sub-hypotheses and combining context F1, variable F1, and relationship accuracy into a single score (0–100).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Hypothesis Match Score (HMS)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>HMS first uses a GPT-4 based evaluator to decompose both the gold and predicted natural-language hypotheses into sub-hypotheses, each annotated with three dimensions: context, variables, and relationship. It then matches predicted sub-hypotheses to gold sub-hypotheses based on context equivalence (determined by the evaluator). For matched pairs it computes: ctx_F1 (F1 over matched contexts), var_F1 (F1 over variable sets per paired sub-hypotheses), and rel_acc (relationship accuracy using a 3-level heuristic). HMS is computed as ctx_F1 multiplied by the average, over matched sub-hypotheses, of (var_F1 * rel_acc), yielding a score in [0,100]. This produces a single numeric measure of hypothesis alignment suitable for benchmarking discovery agents.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Three explicit hypothesis dimensions: Context (ctx_F1), Variables (var_F1), and Relationships (rel_acc). Matching is first on context equivalence; variable alignment uses F1 (with fuzzy matching/paraphrase tolerance); relationship alignment uses a specificity-based accuracy heuristic (100 exact, 50 broader-but-encompassing, 0 otherwise). The final HMS combines context alignment and averaged variable×relation alignments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, GPT-4-0125-preview (GPT-4p), Llama-3-70B (agents evaluated); evaluator: gpt-4-preview-0125</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B (Llama-3-70B); closed-model sizes for GPT-4 variants not specified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multi-domain (sociology, biology, humanities, economics, engineering, meta-science) — data-driven discovery tasks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Data-driven hypotheses: empirical/associational and causal/statistical relationships expressed as declarative hypotheses (including hierarchical semantic hypothesis trees)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported as HMS values (0–100). In DB-REAL, best non-oracle agents achieved HMS in the mid-teens (CodeGen: GPT-4o 15.5, GPT-4p 16.3, Llama-3 12.1; ReAct similar), while Reflexion (Oracle) improved to GPT-4o 24.5, GPT-4p 19.5, Llama-3 22.5. NoDataGuess baselines (guessing without data) scored 0.0 (GPT-4o), 4.7 (GPT-4p), 11.5 (Llama-3). DB-SYNTH showed similar patterns with lower absolute scores; Reflexion (Oracle) peaked at ~15.7 (GPT-4o) and 23.2 (Llama-3). Overall peak system performance ~25%. The paper also reports that higher ctx_F1 correlates with higher var_F1×rel_acc, and that performance falls sharply with increasing workflow length/semantic-tree height.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: primary evaluation is model-based and automated using a GPT-4 evaluator to decompose, match, and score hypotheses. Human role: creation/replication of gold hypotheses and workflows (DB-REAL golds are human-derived from published papers), but the scoring is automatic.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>No explicit numeric validation reported (e.g., no reported correlation between GPT-4 evaluator judgments and independent human raters). The evaluator pipeline is described in detail (prompts, decomposition, matching), but the paper does not report inter-annotator agreement or calibration against human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on an LLM (GPT-4-preview) as an automated judge, introducing potential evaluator bias and circularity; relationship scoring is heuristic (100/50/0) and coarse-grained; decomposition and fuzzy matching depend on prompt-engineered judgments; no reported human calibration of the evaluator; multiple valid discovery paths (outcome-based approach) may mask workflow correctness; complex or domain-specific relationships can be poorly captured by the evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>DiscoveryBench (this paper): DB-REAL (264 real tasks collected from published papers across 6 domains) and DB-SYNTH (903 synthetic tasks across 48 domains). Evaluation uses HMS computed via the GPT-4 evaluator and gold hypotheses from DB-REAL / DB-SYNTH.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4559.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4559.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-based Model Evaluator (gpt-4-preview-0125)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based automated evaluation framework using a GPT-4 variant to decompose hypotheses into sub-hypotheses, judge context equivalence, extract variable sets, and compare relationships, serving as the backbone of the HMS computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>GPT-4-based automated hypothesis decomposition and matching</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>The evaluator executes a set of structured prompts to (1) decompose a natural-language hypothesis into sub-hypotheses annotated with context, variables, and relations (Listing 1), (2) decide context equivalence between a predicted and gold sub-hypothesis (Listing 2), (3) extract and compare variable sets using fuzzy matching to compute var_F1 (Listing 3), and (4) compare relationship specificity to assign rel_acc categories (Listing 4). The outputs of these components feed into HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Decomposition correctness, context equivalence (binary match via prompt), variable overlap measured by F1 with fuzzy matching, relation similarity judged into categories (exact, broader-encapsulating, different).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-preview-0125 (evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied across all DiscoveryBench domains (multi-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Applies to data-driven hypotheses expressed in natural language (associational/causal/statistical statements and hierarchical hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not a scored system by itself; used to produce HMS for agents. The paper reports agent HMS results derived from this evaluator (see HMS entry).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: fully automated LLM-based judging pipeline. Gold hypotheses are human-derived, but the judging decisions are performed by GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>No explicit validation reported comparing evaluator judgments to human raters or inter-rater reliability; the paper provides the exact evaluator prompts to enable reproduction but does not report external validation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Risk of evaluator-model bias; possible conflation between model capabilities and evaluation judgments; lack of reported human calibration; sensitivity to prompt phrasing and tokenizer/truncation issues; evaluator may mis-handle highly technical or domain-specific phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used to evaluate outputs on DiscoveryBench (DB-REAL and DB-SYNTH) via the evaluation CLI described in the repository.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4559.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4559.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ctx/var F1 + rel_acc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context F1, Variable F1, and Relationship Accuracy (rel_acc) components</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The component metrics that form HMS: ctx_F1 measures alignment of contexts between predicted and gold sub-hypotheses; var_F1 measures set overlap for variables using fuzzy matching; rel_acc measures relationship alignment with a three-level specificity heuristic (100/50/0).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Context F1 / Variable F1 / Relationship Accuracy (component metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>ctx_F1: computed as an F1-score over matched sub-hypothesis contexts (treating each context as a unit). var_F1: for each matched sub-hypothesis pair, the evaluator extracts variable sets (with fuzzy/paraphrase matching) and computes F1 between predicted and gold variable sets. rel_acc: relationship alignment scored heuristically: 100 if exact match, 50 if predicted relationship is broader but encompasses the gold relationship, and 0 otherwise. These are combined (averaged var_F1×rel_acc across matches, weighted by ctx_F1) to produce HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Contextual alignment, variable overlap, relationship specificity and containment.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with evaluator gpt-4-preview-0125; applied to agent outputs from GPT-4o, GPT-4p, Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B for Llama-3; others unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multi-domain (Benchmark applied across sociology, biology, humanities, economics, engineering, meta-science)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluates data-driven hypotheses expressed at varying specificity (including piecewise and hierarchical relationships)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Component-level analysis shows context accuracy (ctx_F1) is predictive: higher ctx_F1 correlates with higher combined var_F1×rel_acc. The paper reports scatter plots showing these trends, and notes many failures are due to incorrect context identification. No standalone numeric thresholds beyond those integrated into HMS are proposed.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated computation (via GPT-4 evaluator) of these sub-metrics; golds are human-authored but scoring is automated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>No independent validation reported for these sub-metrics against human raters; methodology and prompts are published to enable reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Variable matching relies on fuzzy matching/paraphrase heuristics which can be brittle; rel_acc is coarse (only three outcomes); ctx_F1 treats context as single units which may under-represent partial/contextual overlaps; all are sensitive to evaluator judgments and prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied within DiscoveryBench to compute HMS for DB-REAL and DB-SYNTH tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4559.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4559.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NoDataGuess</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NoDataGuess baseline (memorization / prior-knowledge baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that asks the model to predict the hypothesis using only dataset descriptions and the goal (no data access), used to measure whether models memorize or reproduce published (human) hypotheses and to calibrate evaluation against memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>NoDataGuess memorization baseline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>The LLM is given only the dataset metadata (description, column descriptions) and the discovery goal, but not the actual dataset rows, and asked to produce a hypothesis. The resulting hypothesis is scored with HMS against the gold (human) hypothesis. This isolates how much of performance could be explained by memorization of published results or by prior knowledge rather than data-driven discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>HMS compared between NoDataGuess and data-driven agents; low NoDataGuess scores indicate agents need the data to match gold hypotheses; higher NoDataGuess suggests memorization or strong prior knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to same agent LLMs: GPT-4o, GPT-4-0125-preview, Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B for Llama-3 (others unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multi-domain across DiscoveryBench</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Empirical/data-derived hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>NoDataGuess scored 0.0 (GPT-4o), 4.7 (GPT-4p), and 11.5 (Llama-3) on DB-REAL, indicating limited memorization for many tasks but nontrivial prior-knowledge for some models; demonstrates that data access materially affects performance for many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: same GPT-4-based evaluator (HMS) used to score NoDataGuess outputs against human gold hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Baseline is an internal experimental control; no external validation beyond comparing HMS to other agents reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Does not fully disentangle memorization from strong world knowledge or reasonable guesswork; performance depends on how descriptive the metadata is; evaluator bias still applies.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used as a baseline within DiscoveryBench (DB-REAL).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4559.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4559.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion (Oracle feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (Oracle) iterative feedback loop using HMS as oracle signal</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation-driven iterative improvement method where an agent receives the computed HMS as an 'oracle' evaluation signal and generates reflections to refine subsequent trials, improving hypothesis generation across multiple attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Reflexion (Oracle) with HMS feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Extension of the CodeGen agent: after an agent trial produces a hypothesis and HMS<1, the agent is provided the oracle HMS score and asked to 'reflect' and revise its approach, running up to three trials or until solved. This treats HMS as a reward/feedback signal to guide iterative hypothesis improvement and measures improvement in final HMS compared to single-shot agents.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Improvement in HMS across trials; ability to use feedback to correct context, variable selection, and relationship formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied with GPT-4o, GPT-4-0125-preview, Llama-3-70B as underlying agents</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B for Llama-3; others unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multi-domain (DiscoveryBench tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Data-driven hypotheses (iterative refinement of generated hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reflexion (Oracle) substantially improved over single-shot CodeGen: e.g., DB-REAL Reflexion (Oracle) HMS: GPT-4o 24.5 (vs CodeGen 15.5), GPT-4p 19.5 (vs CodeGen 16.3), Llama-3 22.5 (vs CodeGen 12.1). Indicates that direct evaluative feedback (even oracle) can meaningfully improve LLM discovery performance.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated iterative process using HMS as the automated 'oracle' feedback; gold hypotheses are human-authored but the feedback signal is computed automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical comparison of HMS before and after reflection trials on benchmark tasks; no external human validation of the reflection outputs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Uses an 'oracle' (HMS) unavailable in real deployment (requires gold hypothesis) — so improvement may overstate real-world gains; dependence on evaluator correctness means feedback can be misleading; three-trial cap is arbitrary; not validated with human-in-the-loop feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Demonstrated on DiscoveryBench (both DB-REAL and DB-SYNTH) as an experimental technique to improve HMS.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiscoveryBench: Towards Data-Driven Discovery with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Data-driven discovery with large generative models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data <em>(Rating: 2)</em></li>
                <li>Gptscore: Evaluate as you desire <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4559",
    "paper_id": "paper-48c83799530dc523ee01e6c1c40ad577d5c10a16",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "HMS",
            "name_full": "Hypothesis Match Score (HMS)",
            "brief_description": "A composite, outcome-based metric that quantifies alignment between an LLM-generated hypothesis and a gold (human) hypothesis by decomposing hypotheses into context, variables, and relationship sub-hypotheses and combining context F1, variable F1, and relationship accuracy into a single score (0–100).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Hypothesis Match Score (HMS)",
            "evaluation_method_description": "HMS first uses a GPT-4 based evaluator to decompose both the gold and predicted natural-language hypotheses into sub-hypotheses, each annotated with three dimensions: context, variables, and relationship. It then matches predicted sub-hypotheses to gold sub-hypotheses based on context equivalence (determined by the evaluator). For matched pairs it computes: ctx_F1 (F1 over matched contexts), var_F1 (F1 over variable sets per paired sub-hypotheses), and rel_acc (relationship accuracy using a 3-level heuristic). HMS is computed as ctx_F1 multiplied by the average, over matched sub-hypotheses, of (var_F1 * rel_acc), yielding a score in [0,100]. This produces a single numeric measure of hypothesis alignment suitable for benchmarking discovery agents.",
            "evaluation_criteria": "Three explicit hypothesis dimensions: Context (ctx_F1), Variables (var_F1), and Relationships (rel_acc). Matching is first on context equivalence; variable alignment uses F1 (with fuzzy matching/paraphrase tolerance); relationship alignment uses a specificity-based accuracy heuristic (100 exact, 50 broader-but-encompassing, 0 otherwise). The final HMS combines context alignment and averaged variable×relation alignments.",
            "model_name": "GPT-4o, GPT-4-0125-preview (GPT-4p), Llama-3-70B (agents evaluated); evaluator: gpt-4-preview-0125",
            "model_size": "70B (Llama-3-70B); closed-model sizes for GPT-4 variants not specified",
            "scientific_domain": "Multi-domain (sociology, biology, humanities, economics, engineering, meta-science) — data-driven discovery tasks",
            "theory_type": "Data-driven hypotheses: empirical/associational and causal/statistical relationships expressed as declarative hypotheses (including hierarchical semantic hypothesis trees)",
            "human_comparison": true,
            "evaluation_results": "Reported as HMS values (0–100). In DB-REAL, best non-oracle agents achieved HMS in the mid-teens (CodeGen: GPT-4o 15.5, GPT-4p 16.3, Llama-3 12.1; ReAct similar), while Reflexion (Oracle) improved to GPT-4o 24.5, GPT-4p 19.5, Llama-3 22.5. NoDataGuess baselines (guessing without data) scored 0.0 (GPT-4o), 4.7 (GPT-4p), 11.5 (Llama-3). DB-SYNTH showed similar patterns with lower absolute scores; Reflexion (Oracle) peaked at ~15.7 (GPT-4o) and 23.2 (Llama-3). Overall peak system performance ~25%. The paper also reports that higher ctx_F1 correlates with higher var_F1×rel_acc, and that performance falls sharply with increasing workflow length/semantic-tree height.",
            "automated_vs_human_evaluation": "Automated: primary evaluation is model-based and automated using a GPT-4 evaluator to decompose, match, and score hypotheses. Human role: creation/replication of gold hypotheses and workflows (DB-REAL golds are human-derived from published papers), but the scoring is automatic.",
            "validation_method": "No explicit numeric validation reported (e.g., no reported correlation between GPT-4 evaluator judgments and independent human raters). The evaluator pipeline is described in detail (prompts, decomposition, matching), but the paper does not report inter-annotator agreement or calibration against human judgments.",
            "limitations_challenges": "Relies on an LLM (GPT-4-preview) as an automated judge, introducing potential evaluator bias and circularity; relationship scoring is heuristic (100/50/0) and coarse-grained; decomposition and fuzzy matching depend on prompt-engineered judgments; no reported human calibration of the evaluator; multiple valid discovery paths (outcome-based approach) may mask workflow correctness; complex or domain-specific relationships can be poorly captured by the evaluator.",
            "benchmark_dataset": "DiscoveryBench (this paper): DB-REAL (264 real tasks collected from published papers across 6 domains) and DB-SYNTH (903 synthetic tasks across 48 domains). Evaluation uses HMS computed via the GPT-4 evaluator and gold hypotheses from DB-REAL / DB-SYNTH.",
            "uuid": "e4559.0",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 Evaluator",
            "name_full": "GPT-4-based Model Evaluator (gpt-4-preview-0125)",
            "brief_description": "A model-based automated evaluation framework using a GPT-4 variant to decompose hypotheses into sub-hypotheses, judge context equivalence, extract variable sets, and compare relationships, serving as the backbone of the HMS computation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "GPT-4-based automated hypothesis decomposition and matching",
            "evaluation_method_description": "The evaluator executes a set of structured prompts to (1) decompose a natural-language hypothesis into sub-hypotheses annotated with context, variables, and relations (Listing 1), (2) decide context equivalence between a predicted and gold sub-hypothesis (Listing 2), (3) extract and compare variable sets using fuzzy matching to compute var_F1 (Listing 3), and (4) compare relationship specificity to assign rel_acc categories (Listing 4). The outputs of these components feed into HMS.",
            "evaluation_criteria": "Decomposition correctness, context equivalence (binary match via prompt), variable overlap measured by F1 with fuzzy matching, relation similarity judged into categories (exact, broader-encapsulating, different).",
            "model_name": "gpt-4-preview-0125 (evaluator)",
            "model_size": null,
            "scientific_domain": "Applied across all DiscoveryBench domains (multi-domain)",
            "theory_type": "Applies to data-driven hypotheses expressed in natural language (associational/causal/statistical statements and hierarchical hypotheses)",
            "human_comparison": null,
            "evaluation_results": "Not a scored system by itself; used to produce HMS for agents. The paper reports agent HMS results derived from this evaluator (see HMS entry).",
            "automated_vs_human_evaluation": "Automated: fully automated LLM-based judging pipeline. Gold hypotheses are human-derived, but the judging decisions are performed by GPT-4.",
            "validation_method": "No explicit validation reported comparing evaluator judgments to human raters or inter-rater reliability; the paper provides the exact evaluator prompts to enable reproduction but does not report external validation metrics.",
            "limitations_challenges": "Risk of evaluator-model bias; possible conflation between model capabilities and evaluation judgments; lack of reported human calibration; sensitivity to prompt phrasing and tokenizer/truncation issues; evaluator may mis-handle highly technical or domain-specific phrasing.",
            "benchmark_dataset": "Used to evaluate outputs on DiscoveryBench (DB-REAL and DB-SYNTH) via the evaluation CLI described in the repository.",
            "uuid": "e4559.1",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "ctx/var F1 + rel_acc",
            "name_full": "Context F1, Variable F1, and Relationship Accuracy (rel_acc) components",
            "brief_description": "The component metrics that form HMS: ctx_F1 measures alignment of contexts between predicted and gold sub-hypotheses; var_F1 measures set overlap for variables using fuzzy matching; rel_acc measures relationship alignment with a three-level specificity heuristic (100/50/0).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Context F1 / Variable F1 / Relationship Accuracy (component metrics)",
            "evaluation_method_description": "ctx_F1: computed as an F1-score over matched sub-hypothesis contexts (treating each context as a unit). var_F1: for each matched sub-hypothesis pair, the evaluator extracts variable sets (with fuzzy/paraphrase matching) and computes F1 between predicted and gold variable sets. rel_acc: relationship alignment scored heuristically: 100 if exact match, 50 if predicted relationship is broader but encompasses the gold relationship, and 0 otherwise. These are combined (averaged var_F1×rel_acc across matches, weighted by ctx_F1) to produce HMS.",
            "evaluation_criteria": "Contextual alignment, variable overlap, relationship specificity and containment.",
            "model_name": "Used with evaluator gpt-4-preview-0125; applied to agent outputs from GPT-4o, GPT-4p, Llama-3-70B",
            "model_size": "70B for Llama-3; others unspecified",
            "scientific_domain": "Multi-domain (Benchmark applied across sociology, biology, humanities, economics, engineering, meta-science)",
            "theory_type": "Evaluates data-driven hypotheses expressed at varying specificity (including piecewise and hierarchical relationships)",
            "human_comparison": true,
            "evaluation_results": "Component-level analysis shows context accuracy (ctx_F1) is predictive: higher ctx_F1 correlates with higher combined var_F1×rel_acc. The paper reports scatter plots showing these trends, and notes many failures are due to incorrect context identification. No standalone numeric thresholds beyond those integrated into HMS are proposed.",
            "automated_vs_human_evaluation": "Automated computation (via GPT-4 evaluator) of these sub-metrics; golds are human-authored but scoring is automated.",
            "validation_method": "No independent validation reported for these sub-metrics against human raters; methodology and prompts are published to enable reproducibility.",
            "limitations_challenges": "Variable matching relies on fuzzy matching/paraphrase heuristics which can be brittle; rel_acc is coarse (only three outcomes); ctx_F1 treats context as single units which may under-represent partial/contextual overlaps; all are sensitive to evaluator judgments and prompt design.",
            "benchmark_dataset": "Applied within DiscoveryBench to compute HMS for DB-REAL and DB-SYNTH tasks.",
            "uuid": "e4559.2",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "NoDataGuess",
            "name_full": "NoDataGuess baseline (memorization / prior-knowledge baseline)",
            "brief_description": "A baseline that asks the model to predict the hypothesis using only dataset descriptions and the goal (no data access), used to measure whether models memorize or reproduce published (human) hypotheses and to calibrate evaluation against memorization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "NoDataGuess memorization baseline",
            "evaluation_method_description": "The LLM is given only the dataset metadata (description, column descriptions) and the discovery goal, but not the actual dataset rows, and asked to produce a hypothesis. The resulting hypothesis is scored with HMS against the gold (human) hypothesis. This isolates how much of performance could be explained by memorization of published results or by prior knowledge rather than data-driven discovery.",
            "evaluation_criteria": "HMS compared between NoDataGuess and data-driven agents; low NoDataGuess scores indicate agents need the data to match gold hypotheses; higher NoDataGuess suggests memorization or strong prior knowledge.",
            "model_name": "Applied to same agent LLMs: GPT-4o, GPT-4-0125-preview, Llama-3-70B",
            "model_size": "70B for Llama-3 (others unspecified)",
            "scientific_domain": "Multi-domain across DiscoveryBench",
            "theory_type": "Empirical/data-derived hypotheses",
            "human_comparison": true,
            "evaluation_results": "NoDataGuess scored 0.0 (GPT-4o), 4.7 (GPT-4p), and 11.5 (Llama-3) on DB-REAL, indicating limited memorization for many tasks but nontrivial prior-knowledge for some models; demonstrates that data access materially affects performance for many tasks.",
            "automated_vs_human_evaluation": "Automated: same GPT-4-based evaluator (HMS) used to score NoDataGuess outputs against human gold hypotheses.",
            "validation_method": "Baseline is an internal experimental control; no external validation beyond comparing HMS to other agents reported.",
            "limitations_challenges": "Does not fully disentangle memorization from strong world knowledge or reasonable guesswork; performance depends on how descriptive the metadata is; evaluator bias still applies.",
            "benchmark_dataset": "Used as a baseline within DiscoveryBench (DB-REAL).",
            "uuid": "e4559.3",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Reflexion (Oracle feedback)",
            "name_full": "Reflexion (Oracle) iterative feedback loop using HMS as oracle signal",
            "brief_description": "An evaluation-driven iterative improvement method where an agent receives the computed HMS as an 'oracle' evaluation signal and generates reflections to refine subsequent trials, improving hypothesis generation across multiple attempts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Reflexion (Oracle) with HMS feedback",
            "evaluation_method_description": "Extension of the CodeGen agent: after an agent trial produces a hypothesis and HMS&lt;1, the agent is provided the oracle HMS score and asked to 'reflect' and revise its approach, running up to three trials or until solved. This treats HMS as a reward/feedback signal to guide iterative hypothesis improvement and measures improvement in final HMS compared to single-shot agents.",
            "evaluation_criteria": "Improvement in HMS across trials; ability to use feedback to correct context, variable selection, and relationship formulation.",
            "model_name": "Applied with GPT-4o, GPT-4-0125-preview, Llama-3-70B as underlying agents",
            "model_size": "70B for Llama-3; others unspecified",
            "scientific_domain": "Multi-domain (DiscoveryBench tasks)",
            "theory_type": "Data-driven hypotheses (iterative refinement of generated hypotheses)",
            "human_comparison": true,
            "evaluation_results": "Reflexion (Oracle) substantially improved over single-shot CodeGen: e.g., DB-REAL Reflexion (Oracle) HMS: GPT-4o 24.5 (vs CodeGen 15.5), GPT-4p 19.5 (vs CodeGen 16.3), Llama-3 22.5 (vs CodeGen 12.1). Indicates that direct evaluative feedback (even oracle) can meaningfully improve LLM discovery performance.",
            "automated_vs_human_evaluation": "Automated iterative process using HMS as the automated 'oracle' feedback; gold hypotheses are human-authored but the feedback signal is computed automatically.",
            "validation_method": "Empirical comparison of HMS before and after reflection trials on benchmark tasks; no external human validation of the reflection outputs reported.",
            "limitations_challenges": "Uses an 'oracle' (HMS) unavailable in real deployment (requires gold hypothesis) — so improvement may overstate real-world gains; dependence on evaluator correctness means feedback can be misleading; three-trial cap is arbitrary; not validated with human-in-the-loop feedback.",
            "benchmark_dataset": "Demonstrated on DiscoveryBench (both DB-REAL and DB-SYNTH) as an experimental technique to improve HMS.",
            "uuid": "e4559.4",
            "source_info": {
                "paper_title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Data-driven discovery with large generative models",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data",
            "rating": 2
        },
        {
            "paper_title": "Gptscore: Evaluate as you desire",
            "rating": 1
        }
    ],
    "cost": 0.015698,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DiscoveryBench: Towards Data-Driven Discovery with Large Language Models</h1>
<p>Bodhisattwa Prasad Majumder ${ }^{<em> 1}$ Harshit Surana ${ }^{</em> 12}$ Dhruv Agarwal ${ }^{<em> 3}$<br>Bhavana Dalvi Mishra ${ }^{</em> 1}$ Abhijeetsingh Meena ${ }^{2}$ Aryan Prakhar ${ }^{2}$ Tirth Vora ${ }^{2}$<br>Tushar Khot ${ }^{1}$ Ashish Sabharwal ${ }^{1}$ Peter Clark ${ }^{1}$<br>${ }^{1}$ Allen Institute for AI ${ }^{2}$ OpenLocus ${ }^{3}$ University of Massachusetts Amherst<br>Website: https://github.com/allenai/discoverybench<br>https://huggingface.co/datasets/allenai/discoverybench<br>*equal contributions</p>
<h4>Abstract</h4>
<p>Can the rapid advances in code generation, function calling, and data analysis using large language models (LLMs) help automate the search and verification of hypotheses purely from a set of provided datasets? To evaluate this question, we present DiscoveryBench, the first comprehensive benchmark that formalizes the multi-step process of data-driven discovery. The benchmark is designed to systematically assess current model capabilities in discovery tasks and provide a useful resource for improving them. Our benchmark contains 264 tasks collected across 6 diverse domains, such as sociology and engineering, by manually deriving discovery workflows from published papers to approximate the real-world challenges faced by researchers, where each task is defined by a dataset, its metadata, and a discovery goal in natural language. We additionally provide 903 synthetic tasks to conduct controlled evaluations across task complexity. Furthermore, our structured formalism of data-driven discovery enables a facet-based evaluation that provides useful insights into different failure modes. We evaluate several popular LLM-based reasoning frameworks using both open and closed LLMs as baselines on DiscoveryBench and find that even the best system scores only $25 \%$. Our benchmark, thus, illustrates the challenges in autonomous data-driven discovery and serves as a valuable resource for the community to make progress.</p>
<h2>1 Introduction</h2>
<p>Knowledge discovery via the scientific process has been a catalyst for human progress for centuries but has, thus far, been a predominantly manual pursuit [16]. Recent breakthroughs in capabilities of large language models (LLMs) to reason and interface with the world using code [9, 40], external tools [41], and interactive agents [51, 32], however, now suggest the possibility of realizing a discovery system that is fully autonomous. Indeed, recent works [33] provide initial evidence for this paradigm within the setting of data-driven discovery, where both search and verification of hypotheses may be carried out using a dataset alone (i.e., after physical experiments and data collection ${ }^{1}$ ), but the extent of this ability remains unclear. We, therefore, aim to systematically evaluate the following question:</p>
<p>How good are current state-of-the-art LLMs at automated data-driven discovery?</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Each DiscoveryBench task consists of a goal and dataset(s) (left). Solving the task requires both statistical analysis and scientific semantic reasoning, e.g., deciding which analysis is appropriate for the domain, and mapping goal terms to column names (center). A faceted evaluation allows open-ended final answers to be rigorously evaluated (right).</p>
<p>Answering this question is hard, as data-driven discovery in the wild (real-world) is diverse across domains and subject areas, which in turn makes it difficult to build a robust evaluation framework to measure progress. We address this using a pragmatic formalization of data-driven discovery, namely the search for a relationship that may hold between variables in a context, where (importantly) the description of those facets may not be in the language of the dataset. A data-driven discovery task then has one of these components missing, e.g., "How did urban land use affect the invasion of introduced plants in Catalonia?". Importantly, this formalization allows for systematic, reproducible evaluation over a wide variety of real-world problems, by leveraging these facets (Fig 1, right).
Unlike prior datasets for statistical analysis [28] or AutoML [55, 15], DiscoveryBench tasks also require scientific semantic reasoning, for instance, deciding which of the many possible analysis techniques are appropriate for the domain (e.g., spatial autocorrelation for plant invasion, Fig 1 center), how to clean and/or normalize the data, and how to map goal terms to dataset terms (e.g., "land use" to "habitat type"). Task solutions typically requires a multistep workflow (Fig 1, center). In this way, DiscoveryBench is the first large-scale dataset to address the broader data-driven discovery pipeline, not just the statistical analysis component, and explore LLMs' capacity for this.
Given this framework, we created DiscoveryBench by manually extracting 264 discovery tasks, i.e., goal + dataset(s), from over 20 published papers, as well as creating real-world discovery workflows that solve each task. We additionally provide 903 synthetic tasks across 48 domains generated using LLMs to mimic the real-world discovery process. The synthetic benchmark allows us to conduct controlled model evaluations by varying task difficulty. Our contributions are thus:</p>
<ul>
<li>DiscoveryBench, the first comprehensive benchmark to formalize the multi-step process of data-driven hypothesis search and verification, covering many real-world discovery tasks plus additional synthetic tasks.</li>
<li>A pragmatic formalism for data-driven discovery, flexible enough to characterize many real-world tasks while constrained enough to allow for rigorous, reproducible evaluation.</li>
<li>A comprehensive evaluation across state-of-the-art LLM-based reasoning methods ("discovery agents"). We find performance peaks at $25 \%$, demonstrating the challenging nature of our task.</li>
</ul>
<p>These suggest that DiscoveryBench may be a valuable resource for helping make progress on autonomous, data-driven discovery.</p>
<h1>2 Related Work</h1>
<p>Automated data-driven discovery has been a long-standing dream of AI [33, 21]. Although there have been a range of data-driven discovery systems, from early ones that fit equations to idealized data, e.g., Bacon [23], to more modern ones handling complex real-world problems, e.g., AlphaFold [19], their associated datasets are task-specific and customized to a pre-built pipeline. In contrast, DiscoveryBench aims to be a general test over multiple tasks, including testing whether systems can design appropriate pipelines themselves.
A number of datasets and tools are available for AutoML, a related technology aimed at automating workflows for building optimal machine learning models [18, 55, 24]. AutoML tools include packages like Scikit [13], and embedded in cloud platforms such as Google Cloud Platform, Microsoft Azure,</p>
<p>and Amazon Web Services. However, associated datasets for AutoML are primarily used for training models, rather than for open-ended discovery tasks.</p>
<p>Similarly, there are several datasets that test statistical analysis in various fields, e.g., [42, 25, 50]. Software packages like Tableaux, SAS, and R also support users in that task. However, these datasets and tools are designed specifically for data analysis, while DISCOVERYBENCH aims to automate the broader pipeline including ideation, semantic reasoning, and pipeline design, where statistical analysis is just one component.</p>
<p>One recent dataset similar in spirit to ours is QRData [28]. QRData also explores LLM capabilities but targets statistical/causal analysis for well-defined (mainly) textbook questions that have unique, (mainly) numeric gold answers. In contrast, DISCOVERYBENCH has no prescribed boundaries on statistical techniques to apply, uses open-ended questions and answers, and complex tasks drawn from state-of-the-art published work.</p>
<h1>3 Formalization</h1>
<p>We begin by formalizing what we mean by a data-driven hypothesis and how the structure of a complex hypothesis may be viewed as a hypothesis semantic tree.</p>
<p>A data-driven hypothesis $h$ in $\mathcal{H}$ (the space of such hypotheses) is a declarative sentence about the state of the world whose truth value may be inferred from a given dataset $D$ using a verification procedure $\mathcal{V}_{D}: \mathcal{H} \rightarrow$ {supported, unsupported $}$, for instance, via statistical modeling.</p>
<p>Each hypothesis may further be expressed using a propositional formula $\phi$ over a set of subhypotheses $h_{i} \in \mathcal{H}$ using logical connectives, e.g., disjunctions and conjunctions, such that $h:=\phi\left(h_{1}, \ldots, h_{n}\right)$ and $\mathcal{V}<em D="D">{D}(h)=\phi\left(\mathcal{V}</em>}\left(h_{1}\right), \ldots, \mathcal{V<em n="n">{D}\left(h</em>$.}\right)\right)$. For instance, suppose $h$ is the hypothesis "for men younger than 20, popularity of product $A$ varies proportional to their age $\left(h_{1}\right)$, while there exists an inverse relationship for those older than $40\left(h_{2}\right)$ ", then $h$ can be expressed as the conjunction $h_{1} \wedge h_{2</p>
<p>Inspired by recent work of Thompson and Skau [47], we additionally introduce a structured formalism that breaks a hypothesis down into three hypothesis dimensions:</p>
<ul>
<li>Contexts $(c)$ : Boundary conditions that limit the scope of a hypothesis. E.g., "for men over the age of 30" or "in Asia and Europe" or unbounded/full dataset when not specified.</li>
<li>Variables $(v)$ : Known set of concepts that interact in a meaningful way under a given context to produce the hypothesis. E.g., gender, age, or income. Note that each hypothesis is associated with a target variable and a set of independent variables.</li>
<li>Relationships $(r)$ : Interactions between a given set of variables under a given context that produces the hypothesis. E.g., "quadratic relationship", "inversely proportional", or piecewise conditionals.</li>
</ul>
<p>With slight abuse of notation, we can now equivalently define hypothesis $h:=\psi(c, v, r)$, where $\psi(\cdot, \cdot, \cdot)$ returns the declarative sentence "under context $c$, variables $v$ have relationship $r$." For instance, for sub-hypothesis $h_{1}$ in our example above, $c_{1}:=$ "men younger than 20", $v_{1}:={$ gender, consumer_age, product_popularity}, and $r_{1}:=$ "popularity is proportional to age".</p>
<p>Hypothesis Semantic Tree. Observe that each independent variable in a hypothesis may itself be a target variable for a prior hypothesis. To emphasize this hierarchical nature, we introduce the concept of a hypothesis semantic tree whose nodes are variables (independent or derived) and whose sub-trees represent hypotheses, as follows. Consider a hypothesis $h$. A semantic hypothesis tree $\mathcal{T}<em h_prime="h^{\prime">{h}$ with $h$ as the primary hypothesis is a Markov tree whose root node is the target variable of $h$, each of whose leaf nodes is an independent variable that is not derived further, and each of whose internal nodes is the target variable of an intermediate hypothesis. In other words, each sub-tree $\mathcal{T}</em>$ with $v$ as the target variable. In particular, a sub-tree rooted
}}$ rooted at an internal node $v$ of $\mathcal{T}_{h}$ is itself a hypothesis semantic tree for a hypothesis $h^{\prime<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Hypothesis Semantic Tree</p>
<p>at $v$ and all its immediate children $C_{v}$ implicitly encodes $h^{\prime}$ as $\psi(c,{v} \cup C_{v}, r)$ where $r$ is the relationship between $v$ and $C_{v}$ under context $c$ as specified in $h^{\prime}$. More generally, $\mathcal{T}_{h}$ can encode many different hypotheses by choosing one node as the target variable and considering nodes at arbitrary descendant levels as independent variables.</p>
<p>For instance, in Fig 2, we show a semantic tree $\mathcal{T}<em 0="0">{h}$ with the following primary hypothesis ( $h$ ): "The visibility of a galaxy reduces when the blue spectrum dominates and the distance of the galaxy from Earth increases", where galaxy_visibility $\left(v</em>}\right)$ is the target variable with independent variables distance $\left(v_{1}\right)$ and galaxy_color $\left(v_{2}\right)$. Consider now the sub-tree rooted at $v_{2}$, which encodes the following intermediate hypothesis: "Visibility of blue light from galaxies increases with an increase in galaxy size and decrease in star density", where galaxy_color $\left(v_{2}\right)$ is the target variable with independent variables galaxy_size $\left(v_{3}\right)$ and galaxy_density $\left(v_{4}\right)$. Note further that due to there existing ancestor edges from $v_{3}$ and $v_{4}$ to $v_{0}$, $\mathcal{T<em 0="0">{h}$ also encodes the hypothesis: "The visibility of a galaxy reduces with distance from Earth combined with an increase in galaxy size and decrease in star density", where the target variable is $v</em>$.
(Task) Dataset. We formally define a dataset $D$ on which hypothesis search and verification is performed as a collection of tuples $\left{\mathbf{x}}$ and the independent variables are $v_{3}$ and $v_{4<em i="1">{i}\right}</em>}^{m}$ that supports multiple hypothesis semantic trees resulting in a semantic forest $\mathcal{F}:=\cup_{i} \mathcal{T<em i="i">{h^{(i)}}$, where each $\mathbf{x}</em>}$ is a row in the dataset and $x \in \mathbf{x<em i="i">{i}$ is an observation for a particular column. Further, $\mathbf{x}</em>$ with different degrees of observability of internal nodes, altering the difficulty of the discovery task.}$ may span only a subset of nodes in $\mathcal{F}$, i.e., not all nodes in $\mathcal{F}$ may be observed. Specifically, while roots and leaves are always observed, internal nodes (target variables for intermediate hypotheses) may be latent. Therefore, multiple versions of $D$ may be collected for $\mathcal{F</p>
<h1>4 DiscoveryBench</h1>
<p>We now introduce a novel benchmark, DiscoveryBench, for discovering data-driven hypotheses. In this benchmark, a data-driven discovery task is defined as follows: Given one or more task dataset(s) $D$ and a discovery goal $G$, derive a hypothesis $h=\psi(c, v, r)$ addressing $G$ with the highest specificity for the context $c$, variables $v$, and relationship $r$ supported by $D$. Optionally, a workflow of deriving such a hypothesis can be outputted to augment information already present in the hypothesis. DiscoveryBench has two components: DB-Real encompassing data-driven hypotheses and workflows derived from published scientific papers and DB-Synth capturing systemic variations in data-driven hypotheses and workflows obtained from synthetically generated datasets. We release our dataset under the ODC-BY license: https://github.com/allenai/discoverybench.</p>
<h3>4.1 DB-REAL: Collecting data-driven hypotheses in the wild</h3>
<p>Our goal is to replicate the scientific process undertaken by researchers to search for and validate a hypothesis from one or more datasets. We focus on six scientific domains where data-driven research is the cornerstone of scientific progress: sociology, biology, humanities, economics, engineering, and meta-science. Our data collection follows either a data-first or code-first approach.</p>
<p>For the data-first approach: 1) we filter papers based on open public datasets ( $D$ ) such as National Longitudinal Surveys (NLS), Global Biodiversity Information Facility (GBIF), and World Bank Open Data (WBOD) that have workflow details; 2) we then try to replicate these workflows in Python. For this data-first approach, replication took up to 90 person-hours per dataset, often ( $30 \%$ ) not resulting in success. This highlights building data-driven discovery benchmarks from real studies is not only challenging and time-consuming, but automating discovery can also be key for scientific progress and reproducibility.</p>
<p>The data-first approach by design is limited to well-known aforementioned public datasets. To improve diversity in domains, datasets ( $D$ ), and workflows, we also adopted a code-first approach to look beyond popular public datasets. In this approach, we 1) search for code repositories based on scientific papers with available datasets and 2) attempt to replicate them in Python with existing code or from scratch with interpretation of the associated paper. We looked at 785 data points in Zenodo, EU's Open Research Repository, with a filter for computational notebooks. Over 85\% of the repositories either had missing code, code that could not be easily translated to Python, or a</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Workflow categories in DB-REAL with representative examples.
proprietary/non-open dataset. We finalized a candidate list of 14 repositories, but in the end, 3 of them passed all our checks for their hypotheses to be included in the benchmark ${ }^{2}$.
Upon replication of the result or implementation of the full procedure as described in the paper, we include the (dataset $D$, hypothesis $h$, implementation workflow) tuple to the benchmark.
During the process, the implementation workflow may lead to other hypotheses that are not directly reported in the paper but can be supported by the data. We included them in DISCOVERYBENCH, which leads to a good mix of already reported science-worthy hypotheses as well as novel hypotheses grounded in datasets. This is particularly useful as our goal is to evaluate LLMs' ability to solve a discovery task that is realistic but never reported before.
Finally, the task datasets are supplemented with a dataset description, natural language descriptions of the columns, and additional background knowledge related to the domain or the datasets. Some of our tasks, for instance, archaeology, require domain knowledge to derive a particular hypothesis.
Inferring task difficulty. Using the hypothesis semantic tree defined in Section 3, we say that the difficulty of a discovery task is proportional to the path length from an observed node to the target hypothesis node in the tree. However, knowing the tree structure from a task dataset alone is impractical due to incomplete a priori information about unobserved intermediate nodes and edges between observed nodes. To infer task difficulty, we, therefore, approximate the path length between the target and leaf nodes using the length of the implementation workflow required to derive a target hypothesis. Specifically, for each step in the workflow, we add 1 to the discovery path length. In some cases, we derive two tasks: easy and hard from the same hypothesis, where for easy, we provide the derived variables as observed variables in the dataset (e.g., BMI), and for hard, it would require deriving intermediate variables (BMI from height and weight) to reach the target. Additionally, given the view of a task dataset as encoding the union of multiple semantic trees rooted at different hypotheses, i.e., a semantic forest $\mathcal{F}$, we further posit that task difficulty increases as the number of trees in the forest $(|\mathcal{F}|)$ increases. Intuitively, discovery becomes harder as the hypothesis search space increases. In practice, this setting is observed when a task requires access to multiple datasets.
Forming discovery goals. By definition, each hypothesis can be fully specified by the declarative sentence as $h:=\psi(c, v, r)$. To systematically construct the discovery goals for the task, we first mask one of each dimension, context $c$, variable $v$, relationship $r$, and generate a discovery goal to</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>identify the masked information given the rest of the hypothesis and the task dataset(s). For instance, for a target hypothesis, "The effect of socioeconomic status on college degree completion is higher for females (0.4995) than males (0.4467)", we form a discovery goal as "How does socioeconomic status impact on college degree completion for females compared to males?" seeking the relationship $r$ to be discovered from the dataset(s) given the relevant variables $v$ and context $c$. Additionally, we ensure each discovery goal leads to only one answer, i.e., the target hypothesis.</p>
<h1>4.1.1 Features of DB-REAL benchmark</h1>
<p>DiscoveryBench incorporates a broad landscape of data-driven discovery. With over 500 instances of data preparation activities such as cleaning, deduplication, and integration, captures the complexity of real-world data preprocessing for discovery. Tasks also demand a spectrum of statistical methods, from statistical tests to mixture models, and include domain-specific approaches in econometric and ecological modeling, as reflected in the Fig 33.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Train</th>
<th style="text-align: right;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># tasks</td>
<td style="text-align: center;">25</td>
<td style="text-align: right;">239</td>
</tr>
<tr>
<td style="text-align: left;"># unique hypotheses</td>
<td style="text-align: center;">14</td>
<td style="text-align: right;">144</td>
</tr>
<tr>
<td style="text-align: left;"># tasks need $&gt;1$ dataset</td>
<td style="text-align: center;">4</td>
<td style="text-align: right;">110</td>
</tr>
<tr>
<td style="text-align: left;"># domains</td>
<td style="text-align: center;">3</td>
<td style="text-align: right;">6</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics for DB-REAL</p>
<p>Table 1 shows the diversity of tasks both in train and test split for DB-REAL. Most importantly, the benchmark incorporates $114(4+110)$ tasks that require more than one related datasets to be analyzed, with a maximum of 6 datasets for a task. Each workflow within the dataset can be viewed as a composition of unit actions-such as code generation for statistical tests-that LLMs excel at, showing how our tasks require the chaining of such atomic actions to address complex scenarios for data-driven discovery. We measure the complexity of these workflows by quantifying the number of unit actions involved, referring to this as the workflow length, whose distribution can be seen in Fig 5.</p>
<h3>4.2 DB-Synth: Generating data-driven hypotheses using LLMs</h3>
<p>To scale data collection, we next introduce a supplementary benchmark, which is synthetically constructed to enable controlled model evaluations. Our goal is to reverse-engineer the process of hypothesis discovery to synthesize datasets and discovery tasks of varying difficulty that require analysis workflows similar to those in the real-world benchmark. Our approach leverages the broad pre-trained knowledge of LLMs in four stages:
Domain sampling: First, we prompt the model to generate a list of diverse topics or domains along with their natural language descriptions. E.g., "Ancient architecture" $\rightarrow$ "Related to historic buildings, architectural marvels, and ancient construction techniques".
Semantic tree construction: For each domain, we then build a semantic tree $\mathcal{T}<em h="h">{h}$, recursively deriving nodes starting from a primary hypothesis $h$. Specifically, we prompt the model with the domain and a sampled real-world workflow (e.g., "within-cluster analysis") to generate a hypothesis and its target variable. Setting the target variable as root, we then derive child nodes by generating the independent variables required to verify $h$ using $V(\cdot)$. We operationalize this by generating a column name and description for each child node (along with a data type and range) and a pandas expression ${ }^{4}$ [49] over only independent variables in $\mathcal{T}</em>}$ such that its execution results in the target variable. We repeat this with each leaf in $\mathcal{T<em i="i">{h}$ as the root of a new semantic sub-tree, generating intermediate hypotheses and a new set of variables until the desired height of $\mathcal{T}$ is reached. ${ }^{5}$ We also generate a set of distractor columns disjoint from nodes in $\mathcal{T}$, thus resulting in a synthetic semantic forest $\mathcal{F}$.
Data generation: We then construct a task dataset $D:=\left{\mathbf{x}</em>\right}<em i="i">{i=1}^{m}$ by generating synthetic data in a bottom-up manner (i.e., from leaves to root) for each node in $\mathcal{F}$. Starting with various sampling strategies for leaf nodes (see more in Sec D), for each subsequent level in $\mathcal{F}$, we create new columns for nodes by simply executing their pandas expressions. Finally, to mimic real-world challenges in data collection, we probabilistically perturb each instance $x \in \mathbf{x}</em>$.}$ by adding noise or dropping values to create missing data ${ }^{6}$. Note that at this stage, $D$ contains a column for each node in $\mathcal{F</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Task generation: For each internal node $h$ in $\mathcal{F}$, we now create multiple task datasets $D_{h}^{(l)}$ from $D$, varying the difficulty of the discovery task based on the path length $l$ between $h$ and the observed independent variables in $\mathcal{F}$. Finally, we follow the same strategy for goal formulation as DB-REAL. We generate 903 tasks over 48 diverse domains and assign them to train, dev, and test sets using a 60/20/20 split, where each task is additionally tagged with a difficulty level from 1-4. While we evaluate our agents on the test, the training set can serve as supervised data for improving models.</p>
<h1>4.3 Evaluation</h1>
<p>We evaluate task performance by measuring the alignment of the predicted and gold hypotheses in natural language. ${ }^{7}$ We take inspiration from recent works in LLM benchmarking [43, 54, 52, 14, 26, 27] and design a model-based evaluation strategy using gpt-4-preview-0125 as the evaluator, conditioned on our structured formalism of data-driven hypotheses.
Recall the propositional form $h:=\phi\left(h_{1}, \ldots, h_{n}\right)$ of a hypothesis $h$ that decomposes it into subhypotheses. We first use our GPT-4 based evaluator to independently decompose the gold $\left(h^{g}\right)$ and predicted $\left(h^{p}\right)$ hypotheses into their respective sub-hypotheses $\left{h_{i}^{g}\right}<em j="j">{i=1}^{n}$ and $\left{h</em>\right}}^{p<em k="k">{j=1}^{m}$, asking it to also identify, for each sub-hypothesis $h</em>$, its context, variables, and relationship dimensions (prompt in Listing 1). Given this structured representation of the gold and predicted hypotheses, we then compute a hypothesis match score (HMS), which measures the degree to which two hypotheses align on each dimension, as follows.
To compute HMS, we match each predicted sub-hypothesis $h_{j}^{p}$ with a gold sub-hypothesis $h_{i}^{g}$ when their contexts are judged as equivalent by our GPT-4 based evaluator (prompt in Listing 2). ${ }^{8}$ Let $M$ denote this set of context-matched pairs of predicted and gold sub-hypotheses. At this point, treating each sub-hypothesis context as a single unit, we can compute an F1 score, $\operatorname{ctx}<em _mathrm_F="\mathrm{F">{\mathrm{F} 1}$, capturing how aligned the $n$ contexts of sub-hypothesis of $h^{g}$ with the $m$ contexts of sub-hypotheses of $h^{p}$. Then, for each matched pair of sub-hypotheses, we measure how well the variables and relations align, using an F1 score for the variables $\left(\operatorname{var}</em>} 1}\right)$ and an accuracy score for the relation $\left(\mathrm{rel<em _mathrm_F="\mathrm{F">{\mathrm{acc}}\right)$. Specifically, for each sub-hypothesis pair in $M$, we extract the set of interacting variables in the gold and predicted sub-hypotheses using the GPT-4 based evaluator (prompt in Listing 3). We compute the alignment between these two sets of variables as an F1 score, $\operatorname{var}</em>} 1}$, similar to how $\operatorname{ctx<em _mathrm_acc="\mathrm{acc">{\mathrm{F} 1}$ was computed. For relationships, we compute relationship accuracy with reference to the relationship between the gold variables $\left(\mathrm{rel}</em>\right)$ based on evaluator judgments using the following scoring heuristic: 100 if there is an exact match of the relation, 50 when the predicted relationship is broader than the gold relationship but encompasses it, and 0 otherwise (prompt in Listing 4). Finally, we compute HMS $\in[0,100]$ as the average alignment of the variable and relationship dimensions over context-matched sub-hypotheses, weighted by the overall context alignment:}</p>
<p>$$
\operatorname{HMS}\left(h^{p}, h^{g}\right)=\operatorname{ctx}<em i="1">{\mathrm{F} 1}\left(h^{p}, h^{g}\right) \times \frac{1}{|M|} \sum</em>}^{|M|}\left(\operatorname{var<em i="i">{\mathrm{F} 1}\left(h</em>}^{p}, h_{i}^{g}\right) \times \operatorname{rel<em i="i">{\mathrm{acc}}\left(h</em>\right)\right)
$$}^{p}, h_{i}^{g</p>
<h2>5 Experiments</h2>
<h3>5.1 Discovery Agents</h3>
<p>We benchmark state-of-the-art LLM-based few-shot reasoning methods as discovery agents with two closed models, GPT-4o and GPT-4-0125-preview (GPT-4p), and one open, Llama-3-70B, model powering the reasoning methods. A discovery agent takes the task description, paths to the task dataset(s) $D$, metadata about the datasets (description, column descriptions), and the goal, $G$, to produce a natural language (NL) hypothesis specified by context, variables, and relationship.</p>
<ul>
<li>CodeGen generates the entire code at one go to solve the task, where we provide a demonstration of a solution code in the context. After code execution and based on the result, it generates the NL hypothesis and summarizes the workflow.</li>
<li>ReAct [51] solves the task by generating thought and subsequent codes in a multi-turn fashion.</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GPT-4o</th>
<th style="text-align: center;">GPT-4p</th>
<th style="text-align: center;">Llama-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DB-REAL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">NoDataGuess</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">11.5</td>
</tr>
<tr>
<td style="text-align: left;">CodeGen</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">12.1</td>
</tr>
<tr>
<td style="text-align: left;">React</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">13.5</td>
</tr>
<tr>
<td style="text-align: left;">DataVoyager</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">11.5</td>
</tr>
<tr>
<td style="text-align: left;">Reflexion (Oracle)</td>
<td style="text-align: center;">$\mathbf{2 4 . 5}$</td>
<td style="text-align: center;">$\mathbf{1 9 . 5}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">DB-SYNTH</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">CodeGen</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">10.9</td>
</tr>
<tr>
<td style="text-align: left;">React</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">12.0</td>
</tr>
<tr>
<td style="text-align: left;">DataVoyager</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">11.7</td>
</tr>
<tr>
<td style="text-align: left;">Reflexion (Oracle)</td>
<td style="text-align: center;">$\mathbf{1 5 . 7}$</td>
<td style="text-align: center;">$\mathbf{1 2 . 9}$</td>
<td style="text-align: center;">$\mathbf{2 3 . 2}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (Left) Hypothesis Matching Scores (HMS) across agent-LLM pairs in DB-REAL and DB-SYnTH. (Right) Scatter plot for $\mathrm{ctx}<em _mathrm_F="\mathrm{F">{\mathrm{F} 1}$ and average $\operatorname{var}</em>$, showing accurate contexts increases the probability of predicting variables and relations accurately. Scores are for the best model on DB-REAL and only include data points ( $44.2 \%$ ) where both scores are non-zero.} 1} \times \operatorname{rel}_{\mathrm{acc}</p>
<ul>
<li>DataVoyager is a multi-component data-driven discovery agent from [33]. It has four components, planner, code generator, data analysis, and critic, that orchestrate the discovery process.</li>
<li>Reflexion (Oracle) [44] is an extension of CodeGen agent, where at the end of one trial, we provide the "oracle" HMS score as an evaluation signal, and it generates a reflection to improve (when $\mathrm{HMS}&lt;1$ ) in the next trial till it solves the task, or maximum trials (3) are reached.</li>
<li>NoDataGuess guesses the hypothesis (in DB-REAL) just from the dataset description and the goal without accessing the datasets where we measure LLM's memorization of already published works.</li>
</ul>
<h1>5.2 Main Results</h1>
<p>Fig 4(left) shows that overall performance for all framework-LLM pairs is low for both DB-REAL and DB-SYnTH, highlighting the challenging nature of the task and the benchmark. Most importantly, effective reasoning prompts such as React and planning with a self-critic (DataVoyager) do not help improve the simple CodeGen agent. But with oracle feedback, Reflexion (Oracle) significantly improves over CodeGen (base) performance. Analysis reveals that almost all non-reflexion agents solve the easiest (in terms of workflow category and length) instances from the benchmark. GPT-4o refuses to hallucinate in the NoDataGuess baseline, whereas surprisingly Llama-3 performs similarly in both data and no-data modes. We additionally observe that the models' performance in DB-REAL and DB-SYNTH are similar, indicating our synthetic benchmark captures complexities of the real workflow but provides a systematic way to analyze the models' performance.</p>
<h3>5.3 Analysis</h3>
<p>Context is important. Fig 4(right) shows the trends of the $\mathrm{ctx}<em _mathrm_F="\mathrm{F">{\mathrm{F} 1}$ and combined $\operatorname{var}</em>$. A positive trend signifies that to predict variables and relationships accurately, precise and accurate context prediction is necessary. However, correct identification of context is an important first step, although it does not guarantee success.} 1} \times \operatorname{rel}_{\mathrm{acc}</p>
<p>Workflow complexity barrier. Almost all agents struggle more with tasks involving complex statistical techniques, complex data preparation methods, or domain-specific models. The top three workflow categories where the best non-oracle model was highly performant are correlation analysis (55\%), data selection (18\%), and summary statistics (18\%), whereas the lowest three workflow categories are spatial analysis ( $0 \%$ ), pollen dating ( $0 \%$ ), and ecological modeling ( $0 \%$ ).</p>
<p>Domain knowledge dependency. To check if additional domain helps agents perform better, we collect targeted domain knowledge for the archaeology-related tasks that needed significant domain knowledge during data collection. When added as additional hints, we find that DataVoyager's (GPT-4p) performance jumps from $9.9 \%$ (w/ domain knowledge) to $17.5 \%$ (w/o domain knowledge).</p>
<p>Performance across domains and goal types. Fig 5(a) depicts that biology ( $0 \%$ ) and engineering (7\%) perform the worst due to their higher dependence on advanced statistical methods, while economics (25\%) and sociology (23\%) perform better. Additionally, Fig 5(b) shows goals related to discovering a relationship given context and variables are more easily solved than the other two types</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Best non-oracle agent's performance (HMS) (a) across domains, (b) for goal types (dimension to be discovered), and (c) for different workflow lengths. In (c) workflow length categories for DB-REAL are s: $&lt;10$, m: $&gt;10$, $&lt;20$, l: $&gt;20$. For DB-SYNTH, it is the semantic tree height.</p>
<p>of goals, finding context and variables. This is explained by the complexity of the hypothesis search, which is broader for finding the right context or a set of variables given a fixed relationship, whereas finding the relationship given context and variables is easier.</p>
<p><strong>Impact of workflow length.</strong> Inherently, the difficulty of the tasks is measured by the gold workflow length (DB-REAL) or the height of the semantic tree (DB-SYNTH). Figure 5(c) shows a decreasing trend in performance as workflow length (hence, complexity) increases. The performance drops significantly even for medium-length workflows, highlighting current agents' limitations.</p>
<h2>6 Conclusion</h2>
<p>We present DISCOVERYBENCH, the first data-driven discovery benchmark consisting of 264 discovery tasks that capture real scientific workflows extracted from published works. We supplement this with 903 structurally generated synthetic tasks, tailored to evaluate discovery agents at various levels of difficulty. We benchmark state-of-the-art reasoning frameworks with the most advanced LLMs, but the best agent's performance only peaks at 25% underscoring the challenging nature of the task and the benchmark. We hope our timely contribution can increase interest and efforts in making progress on reliable and reproducible autonomous scientific discovery using large generative models.</p>
<h2>References</h2>
<ul>
<li>[1] K. L. Alexander, C. Riordan, J. Fennessey, and A. M. Pallas. Social background, academic resources, and college graduation: Recent evidence from the national longitudinal survey. <em>American Journal of Education</em>, 90(4):315–333, 1982.</li>
<li>[2] A. P. S. Alves, M. Kalinowski, G. Giray, D. Mendez, N. Lavesson, K. Azevedo, H. Villamizar, T. Escovedo, H. Lopes, S. Biffl, et al. Status quo and problems of requirements engineering for machine learning: Results from an international survey. In <em>International Conference on Product-Focused Software Process Improvement</em>, pages 159–174. Springer, 2023.</li>
<li>[3] R. Apel and G. Sweeten. The impact of incarceration on employment during the transition to adulthood. <em>Social problems</em>, 57(3):448–479, 2010.</li>
<li>[4] E. N. Appiah. The effect of education expenditure on per capita gdp in developing countries. <em>International Journal of Economics and Finance</em>, 9(10):136–144, 2017.</li>
<li>[5] J. Brinkmann. Copper output, demand for wood and energy expenditure–evaluating economic aspects of bronze age metallurgy. <em>How's life</em>, pages 11–34, 2019.</li>
<li>[6] J. P. Brozio, J. Müller, M. Furholt, W. Kirleis, S. Dreibrodt, I. Feeser, W. Dörfler, M. Weinelt, H. Raese, and A. Bock. Monuments and economies: What drove their variability in the middle-holocene neolithic? <em>The Holocene</em>, 29(10):1558–1571, 2019.</li>
<li>[7] J. P. Brozio, J. Kneisel, S. Schaefer-Di Maida, J. Laabs, I. Feeser, A. Ribeiro, and S. Schultrich. Patterns of socio-economic cultural transformations in neolithic and bronze age societies in the central northern european plain: Human-environmental interaction concerning bourdieu's forms</li>
</ul>
<p>of capital. In Perspectives on Socio-environmental Transformations in Ancient Europe, pages 105-142. Springer, 2024.
[8] F. O. Cerezer, C. S. Dambros, M. T. Coelho, F. A. Cassemiro, E. Barreto, J. S. Albert, R. O. Wüest, and C. H. Graham. Accelerated body size evolution in upland environments is correlated with recent speciation in south american freshwater fishes. Nature Communications, 14(1):6070, 2023.
[9] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[10] M. Dal Corso, W. Kirleis, J. Kneisel, N. Taylor, M. Wieckowska-Lüth, and M. Zanon. How's Life? Living Conditions in the 2nd and 1st Millennia BCE. Sidestone Press, 2019.
[11] C. Dougherty. Numeracy, literacy and earnings: Evidence from the national longitudinal survey of youth. Economics of education review, 22(5):511-521, 2003.
[12] I. Feeser, W. Dörfler, J. Kneisel, M. Hinz, and S. Dreibrodt. Human impact and population dynamics in the neolithic and bronze age: Multi-proxy evidence from north-western central europe. The Holocene, 29(10):1596-1606, 2019.
[13] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and F. Hutter. Efficient and robust automated machine learning. In NeurIPS, 2015.
[14] J. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate as you desire. ArXiv, abs/2302.04166, 2023. URL https://api.semanticscholar.org/CorpusID:256662188.
[15] P. Gijsbers, M. L. P. Bueno, S. Coors, E. LeDell, S. Poirier, J. Thomas, B. Bischl, and J. Vanschoren. Amlb: an automl benchmark. ArXiv, abs/2207.12560, 2022. URL https: //api.semanticscholar.org/CorpusID:251066648.
[16] D. J. Glass and N. Hall. A brief history of the hypothesis. Cell, 134(3):378-381, 2008.
[17] R. Heyard and L. Held. Meta-regression to explain shrinkage and heterogeneity in large-scale replication projects. Technical report, Center for Open Science, 2024.
[18] H. Jin, F. Chollet, Q. Song, and X. Hu. Autokeras: An automl library for deep learning. J. Mach. Learn. Res., 24:6:1-6:6, 2023.
[19] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583-589, 2021.
[20] E. Kaiser and W. Schier. Time and Materiality: Periodization and Regional Chronologies at the Transition from Bronze to Iron Age in Eurasia (1200-600 BCE). Edited by Elke Kaiser and Wolfram Schier. Verlag Marie Leidorf GmbH, 2021.
[21] H. Kitano. Artificial intelligence to win the nobel prize and beyond: Creating the engine for scientific discovery. AI magazine, 37(1):39-49, 2016.
[22] J. Kneisel. Chronology and transformation. the transition from bronze to iron age in northern europe. Time and materiality: Periodization and regional chronologies at the transition from Bronze to Iron Age in Eurasia (1200-600 BCE), pages 237-263, 2021.
[23] P. Langley. Data-driven discovery of physical laws. Cogn. Sci., 5:31-54, 1981. URL https : //api.semanticscholar.org/CorpusID:39694251.
[24] E. LeDell and S. Poirier. H2O AutoML: Scalable automatic machine learning. In Proceedings of the AutoML Workshop at ICML, 2020.
[25] M. Y. Li, E. B. Fox, and N. D. Goodman. Automated statistical model discovery with language models. ArXiv, abs/2402.17879, 2024. URL https://api.semanticscholar.org/ CorpusID:268041863.
[26] X. Li, T. Zhang, Y. Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/ tatsu-lab/alpaca_eval, 2023.
[27] B. Y. Lin, K. Chandu, F. Brahman, Y. Deng, A. Ravichander, V. Pyatkin, R. L. Bras, and Y. Choi. Wildbench: Benchmarking language models with challenging tasks from real users in the wild, 2024. URL https://huggingface.co/spaces/allenai/WildBench.</p>
<p>[28] X. Liu, Z. Wu, X. Wu, P. Lu, K.-W. Chang, and Y. Feng. Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data. arXiv preprint arXiv:2402.17644, 2024.
[29] L. Lorenz. Kommunikationsstrukturen mittelneolithischer Gesellschaften im nordmitteleuropäischen Tiefland. Verlag Dr. Rudolf Habelt GmbH, in Kommission, 2018.
[30] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, and P. Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=S37hDerQLB.
[31] S.-D. Maida et al. Unter hügeln: Bronzezeitliche transformationsprozesse in schleswig-holstein am beispiel des fundplatzes von mang de bargen (bornhöved, kr. segeberg) band 1, 2023.
[32] B. P. Majumder, B. D. Mishra, P. Jansen, O. Tafjord, N. Tandon, L. Zhang, C. Callison-Burch, and P. Clark. CLIN: A continually learning language agent for rapid task adaptation and generalization. arXiv preprint arXiv:2310.10134, 2023.
[33] B. P. Majumder, H. Surana, D. Agarwal, S. Hazra, A. Sabharwal, and P. Clark. Data-driven discovery with large generative models. ICML, 2024.
[34] G. I. P. Ottaviano, G. Peri, and G. C. Wright. Immigration, offshoring, and american jobs. American Economic Review, 103(5):1925-1959, 2013.
[35] L. C. Pal. Impact of education on economic development. Khazanah Pendidikan Islam, 5(1): $10-19,2023$.
[36] A. Palmisano, A. Bevan, A. Kabelindde, N. Roberts, and S. Shennan. Long-term demographic trends in prehistoric italy: Climate impacts and regionalised socio-ecological trajectories. Journal of world prehistory, 34(3):381-432, 2021.
[37] E. W. Parkinson, T. R. McLaughlin, C. Esposito, S. Stoddart, and C. Malone. Radiocarbon dated trends and central mediterranean prehistory. Journal of world prehistory, 34:317-379, 2021.
[38] N. Rambeli, D. A. A. Marikan, J. M. Podivinsky, R. Amiruddin, and I. Ismail. The dynamic impact of government expenditure in education on economic growth. International Journal of Business and Society, 22(3):1487-1507, 2021.
[39] M. Riera, J. Pino, L. Sáez, P. Aymerich, and Y. Melero. Effect of introduction pathways on the invasion success of non-native plants along environmental gradients. Biological Invasions, pages $1-20,2024$.
[40] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.
[41] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.
[42] Z. Shao, F. Wang, Y. Xu, W. Wei, C. Yu, Z. Zhang, D. Yao, G. Jin, X. Cao, G. Cong, C. S. Jensen, and X. Cheng. Exploring progress in multivariate time series forecasting: Comprehensive benchmarking and heterogeneity analysis. ArXiv, abs/2310.06119, 2023.
[43] S. Shashidhar, A. Chinta, V. Sahai, Z. Wang, and H. Ji. Democratizing llms: An exploration of cost-performance trade-offs in self-refined open-source models. ArXiv, abs/2310.07611, 2023. URL https://api.semanticscholar.org/CorpusID:263834891.
[44] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning. In NeurIPS, 2023. URL https: //api.semanticscholar.org/CorpusID:258833055.
[45] P. K. Smith, B. Bogin, and D. Bishai. Are time preference and body mass index associated?: Evidence from the national longitudinal survey of youth. Economics \&amp; Human Biology, 3(2): $259-270,2005$.
[46] C. Sommerfeld. Gerätegeld Sichel: studien zur monetären Struktur bronzezeitlicher Horte im nördlichen Mitteleuropa, volume 19. Walter de Gruyter, 2013.</p>
<p>[47] W. H. Thompson and S. Skau. On the scope of scientific hypotheses. Royal Society Open Science, 10(8):230607, 2023.
[48] H. Weatherly, K. Lopez, and C. Tierra. The impact of education on gdp per capita. 2022.
[49] Wes McKinney. Data Structures for Statistical Computing in Python. In Stéfan van der Walt and Jarrod Millman, editors, Proceedings of the 9th Python in Science Conference, pages 56 61, 2010. doi: 10.25080/Majora-92bf1922-00a.
[50] Z. Yang, X. Liu, T. Li, D. Wu, J. Wang, Y. Zhao, and H. Han. A systematic literature review of methods and datasets for anomaly-based network intrusion detection. Comput. Secur., 116: 102675, 2022.
[51] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. In ICLR, 2023. URL https://openreview.net/ forum?id=WE_vluYUL-X.
[52] Z. Yuan, J. Liu, Q. Zi, M. Liu, X. Peng, and Y. Lou. Evaluating instruction-tuned large language models on code comprehension and generation. ArXiv, abs/2308.01240, 2023. URL https://api.semanticscholar.org/CorpusID:260379087.
[53] K. Zaw, D. Hamilton, and W. Darity. Race, wealth and incarceration: Results from the national longitudinal survey of youth. Race and Social Problems, 8:103-115, 2016.
[54] Z. Zeng, J. Yu, T. Gao, Y. Meng, T. Goyal, and D. Chen. Evaluating large language models at evaluating instruction following. ArXiv, abs/2310.07641, 2023. URL https: //api.semanticscholar.org/CorpusID:263834884.
[55] S. Zhang, C. Gong, L. Wu, X. Liu, and M. Zhou. AutoML-GPT: Automatic machine learning with GPT. ArXiv, abs/2305.02499, 2023.</p>
<h1>A FAQs</h1>
<ol>
<li>Dataset or Benchmark: Is this a dataset or a benchmark? A benchmark</li>
<li>Benchmark: For benchmarks, the supplementary materials must ensure that all results are easily reproducible (i.e., all necessary datasets, code, and evaluation procedures must be accessible and documented)
Datasets: DISCOVERYBENCH is released at https://github.com/allenai/ discoverybench/tree/main/discoverybench.
Code (Baseline Models): Code for Discovery Agents are provided in the repository, at: https://github.com/allenai/discoverybench/tree/main/agents. A CLI is available to run the discovery agents on the benchmark.
Evaluation Procedures: Please follow our main paper for the details of our evaluation process. The code to run eval on a single instance of our benchmark is provided at: https:// github.com/allenai/discoverybench/tree/main/eval. A CLI and some example scripts have been provided as well.</li>
<li>Accessibility: The following are accessibility items on the submission checklist:</li>
</ol>
<p>Links to access the benchmark: The link to access the benchmark is provided in the main submission (https://github.com/allenai/discoverybench/tree/main/ discoverybench).
Any data should use open and widely used formats. Simulation environments should explain how they can be used: Our data are stored in widely accessible standard formats (e.g., JSON, CSV), with the structure described in Appendix F.</p>
<p>Long-term preservation. Code and data are provided on GitHub. All aspects will be publicly available for a long term.
Explicit Licence: Our benchmark is licensed using ODC-BY and the associated code is licensed with APACHE 2.0, as included in the GitHub repository.
Structured Metadata for a dataset: Our dataset is also available as the HuggingFace dataset: https://huggingface.co/datasets/allenai/discoverybench. Structured Metadata will be available once we finalize our work after addressing the reviewers' comments, if any.
A persistent dereferenceable identifier (e.g., a code repository such as GitHub): The repository for our benchmark is: https://github.com/allenai/discoverybench.</p>
<h1>B Limitations</h1>
<p>We currently filtered domains and tasks that required forecasting, simulation, or very specific modeling (species distribution, infection spread, astrophysics equations for exoplanets) in the benchmark as they were very time-consuming to replicate as well as discover hypotheses. As a result, we discarded more papers focused on natural and physical sciences compared to social sciences, which we plan to include in future benchmarks.
We currently do not tackle the challenge of understanding and processing massive datasets, such as the 8.92 petabytes data from the Cancer Genome Atlas (https://portal.gdc.cancer.gov) or the extensive brain data from the Allen Institute (https://alleninstitute.org/division/ brain-science). While the potential to discover new insights from such vast data volumes is significant, ensuring these findings are robust and not subject to $p$-hacking remains unaddressed by our current methods.
We currently do not handle multi-modal data and complex pipelines, such as those needed for analyzing satellite and other geospatial data relevant to climate science and astronomy data. This would involve multiple stages of data processing, the use of various tools, and managing workflow complexities, for example, analyzing thousands of species patterns combined with satellite data to study habitats. So we do not incorporate workflows like those of EarthRanger (https://www. earthranger.com).
Ethical Considerations There could be many potential societal consequences of systems tuned on our proposed benchmark since it involves using LLMs, such as policy misuse, legal ramifications, and false discovery. On the positive side, our proposed benchmark can advance the rate of discovery, leading to an improved standard of living and social well-being.</p>
<h2>C Data collection for DB-REAL</h2>
<p>For data-first approach, replication took 15 to 40 person-hours for each NLS-related paper and up to 90 person-hours for the GBIF dataset, where specialized domain knowledge and tools led to higher complexity. All papers replicated in the NLS dataset were included, while less than half of the papers in specialized datasets like GBIF and WBOD were added to DISCOVERYBENCH.
Citation/Repositories for DB-REAL: List of scientific works from where we have replicated our gold workflows and hypotheses:</p>
<ol>
<li>Sociology: $[53,3,1,45,11]$</li>
<li>Biology: $[8,39]$</li>
<li>Economics: $[35,4,48,38,34]$</li>
<li>Engineering: [2]</li>
<li>Meta-science: [17]</li>
<li>Humanities: $[7,6,29,31,46,12,37,22,20,5,10,36,37]$</li>
</ol>
<p>All assets come under CC license or open licenses.</p>
<h2>D Data Generation for DB-SYnTH</h2>
<p>For leaves, we use different sampling strategies based on the data type. Specifically, for categorical nodes, we sample instances with replacement from the range of allowed values, whereas for numeric, we first select a distribution (e.g., normal) and its parameters based on the specified range and then perform sampling. For each subsequent level in $\mathcal{F}$, we create new columns for nodes by simply executing their pandas expressions ${ }^{9}$. To recover from any execution errors, we additionally use a self-refine [30] approach to generate new pandas expressions guided by the execution error logs. Finally, to mimic real-world challenges in data collection, we probabilistically perturb each instance</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$x \in \mathbf{x}_{i}$ by adding noise or dropping values to create missing data ${ }^{10}$. After generation, $D$ contains a column for each node in $\mathcal{F}$.</p>
<h1>E Datasheets</h1>
<h2>E. 1 Motivation</h2>
<ul>
<li>For what purpose was the dataset created? DISCOVERYBENCH is created to help assess large language models' (LLMs) ability to automate the search and verification of hypotheses purely from a set of provided datasets.</li>
<li>Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? Authors belong to the Allen Institute for AI, OpenLocus, and the University of Massachusetts Amherst. The data collection is part of research efforts conducted by the Allen Institute for AI.</li>
<li>Who funded the creation of the dataset? Allen Institute for AI.</li>
</ul>
<h2>E. 2 Collection Process</h2>
<ul>
<li>How was the data associated with each instance acquired? Our goal is to replicate the scientific process undertaken by researchers to search for and validate a hypothesis from one or more datasets. We focus on six scientific domains where data-driven research is the cornerstone of scientific progress: sociology, biology, humanities, economics, engineering, and meta-science. Our data collection follows either a data-first or code-first approach. Each instance has been manually implemented and verified by the authors for solvability.</li>
</ul>
<h2>E. 3 Uses</h2>
<ul>
<li>Has the dataset been used for any tasks already? We use this benchmark to evaluate LLM's ability to search and verify hypotheses purely from a set of datasets.</li>
<li>Are there tasks for which the dataset should not be used? We do not expect the community members to use this data to train models that can aggravate $p$-hacking.</li>
</ul>
<h2>E. 4 Distribution and Maintainance</h2>
<ul>
<li>How will the dataset will be distributed? We distribute this benchmark via our GitHub repository: https://github.com/allenai/discoverybench and https: //huggingface.co/datasets/allenai/discoverybench.</li>
<li>How can the owner/curator/manager of the dataset be contacted? For any benchmark-related queries, please contact: bodhisattwam@allenai.org. For any coderelated discussions, please raise an issue in GitHub: https://github.com/allenai/ discoverybench.</li>
</ul>
<h2>F Composition of DISCOVERYBENCH</h2>
<h2>F. 1 Metadata structure</h2>
<ul>
<li>id: An identifier for the metadata.</li>
<li>domain: The broad field of study or area of research.</li>
<li>workflow_tags: A set of keywords summarizing the main processes or techniques used in the replication implementation. They provide an overview of the methodological approach and facilitating the identification of relevant analytical techniques.</li>
</ul>
<h2>- domain_knowledge:</h2>
<ul>
<li>Contextual information or insights related to the dataset, explaining how certain behaviors or variables can be interpreted within the field of study.</li>
</ul>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>It helps open avenues to think in directions that LLM might not have considered otherwise, broadening the understanding of the field.</li>
<li>datasets: Contains detailed information about the datasets used, including:</li>
<li>name: The name or filename of the dataset.</li>
<li>description: A summary of the dataset's contents and the type of data it includes.</li>
<li>max_depth: The maximum hierarchical level of nested data structures within the dataset, indicating the complexity of the data.</li>
<li>columns: Detailed descriptions of each column in the dataset, including:</li>
<li>name: The column's name or header.</li>
<li>description: Explanation of the data contained in the column and its significance.</li>
<li>depth: The hierarchical level of the column within the dataset, indicating its structural position.</li>
<li>hypotheses: Statements or predictions being tested, divided into:</li>
<li>main: Primary hypotheses that are central to the discovery task.</li>
<li>workflow: A step-by-step description of the replication process followed to validate the hypotheses, outlining the methods and procedures used from data preparation to final analysis. Some of the workflows and sub-workflows are high-level and thus the same for different queries as they follow the same implementation leading to a range of hypotheses.</li>
<li>queries: Goals related to each hypothesis, each including:</li>
<li>qid: A unique identifier for the goal for a given true/gold hypothesis.</li>
<li>difficulty: Categorization of the difficulty. Structurally defined for DB-SYNTH using the semantic tree definition.</li>
<li>true_hypothesis: The hypothesis being tested through the goal. This defines the primary statement or prediction under investigation.</li>
<li>relevant_cols: Columns from the dataset that are relevant to answering the query, indicating the specific data points that can be used in the analysis. Only appears for DB-SYNTH.</li>
<li>target_col: The column being predicted or the dependent variable in the analysis. Only appears for DB-SYNTH.</li>
<li>question_type: The type of question being asked categorizing the nature of the inquiry.</li>
<li>question: The discovery goal.</li>
</ul>
<h1>F. 2 Directory structure for DB-REAL</h1>
<p>There may be more than one query per metadata. The train split contains 14 metadata files and 25 queries. The test split contains 144 metadata files and 239 queries. Metadata folders with the same prefixes use the same underlying dataset with either a subset or a preprocessed version. When dealing with a full dataset (i.e., nls_raw), the task becomes substantially harder due to the data preparation required.</p>
<div class="codehilite"><pre><span></span><code><span class="o">|-</span><span class="n">test</span>
<span class="o">|---</span><span class="n">archaeology</span>
<span class="o">|---</span><span class="n">introduction_pathways_non</span><span class="o">-</span><span class="n">native_plants</span>
<span class="o">|---</span><span class="n">meta_regression</span>
<span class="o">|---</span><span class="n">meta_regression_raw</span>
<span class="o">|---</span><span class="n">nls_incarceration</span>
<span class="o">|---</span><span class="n">nls_raw</span>
<span class="o">|---</span><span class="n">nls_ses</span>
<span class="o">|---</span><span class="n">requirements_engineering_for_ML_enabled_systems</span>
<span class="o">|---</span><span class="n">worldbank_education_gdp</span>
<span class="o">|---</span><span class="n">worldbank_education_gdp_indicators</span>
<span class="o">|-</span><span class="n">train</span>
<span class="o">|---</span><span class="n">evolution_freshwater_fish</span>
<span class="o">|---</span><span class="n">immigration_offshoring_effect_on_employment</span>
<span class="o">|---</span><span class="n">nls_bmi</span>
<span class="o">|---</span><span class="n">nls_bmi_raw</span>
</code></pre></div>

<h1>F. 3 Directory structure for DB-SYnTH</h1>
<p>There is one query per metadata. The train split contains 551 metadata files (queries), the dev split contains 153 metadata files (queries), and the test split contains 200 metadata files (queries).</p>
<div class="codehilite"><pre><span></span><code>    |-test
    |---ancient-languages_*_*
    |---artificial-ecosystems_*_*
    |---astronomy_*_*
    |---board-games_*_*
    |---coding-competitions_*_*
    |---digital-artistry_*_*
    |---futuristic-technology_*_*
    |---impressionist-art_*_*
    |---machine-learning_*_*
    |---molecular-gastronomy_*_*
    |---neuroscience_*_*
    |---philosophical-debates_*_*
    |---robotics_*_*
    |-train
    |---adventure-travel_*_*
    |---ancient-architecture_*_*
    |---ancient-astronomy_*_*
    |---aviation_*_*
    |---biodiversity-conservation_*_*
    |---cryptic-puzzles_*_*
    |---cryptocurrency_*_*
    |---culinary-arts_*_*
    |---cybersecurity_*_*
    |---environmental-activism_*_*
    |---fashion-design_*_*
    |---fine-arts_*_*
    |---literary-classics_*_*
    |---marine-biology_*_*
    |---marine-conservation_*_*
    |---medieval-literature_*_*
    |---musical-therapy_*_*
    |---photography_*_*
    |---robotic-explorers_*_*
    |---solar-power_*_*
    |---space-tourism_*_*
    |---steampunk-culture_*_*
    |---theater-productions_*_*
    |---underwater-archaeology_*_*
    |---urban-gardening_*_*
    |---vintage-automobiles_*_*
    |---virtual-reality_*_*
</code></pre></div>

<h2>G Discovery Agent</h2>
<p>The command discovery_agent.py is used with various options to customize its behavior for discovery tasks. Below are the options explained:</p>
<ul>
<li>Usage: discovery_agent.py [OPTIONS] QUERY - Executes the discovery agent with specified options.</li>
</ul>
<h2>- Options:</h2>
<ul>
<li>
<p>-agent_type [coder|react]: Specifies the type of agent to use for discovery. The default type is coder. Options include coder for code-related tasks and react for reactive tasks.</p>
</li>
<li>
<p>-model_name TEXT: Sets the model to be used. The default is gpt-4o. Available models include gpt-4-turbo, llama-3-70b-chat, claude-3-opus, and gemini-pro. An exhaustive list is available in config/model_config.json.</p>
</li>
<li>-api_config TEXT: Path to the API configuration file. The default path is config/api_config.json.</li>
<li>-log_file TEXT: Specifies the path to the log file where operations details are stored.</li>
<li>-metadata_path TEXT: Path to the metadata file. This option is required.</li>
<li>-metadata_type [real|synth]: Specifies the type of metadata, where real stands for actual metadata and synth for synthetic. This option is required.</li>
<li>-add_domain_knowledge: Includes domain-specific knowledge in the query processing.</li>
<li>-add_workflow_tags: Includes workflow tags in the query to enhance context.</li>
<li>-help: Displays the help message and exits, showing all available command options.</li>
</ul>
<h1>H Evaluation</h1>
<p>Explain about evaluation in a line and then explain the CLI usage here.
The command discovery_eval.py is used to evaluate the outputs generated by the discovery agent. Below are the detailed descriptions of the command options:</p>
<ul>
<li>Usage: discovery_eval.py [OPTIONS] QUERY - Executes the evaluation agent with specified options and a query.</li>
</ul>
<h2>- Options:</h2>
<ul>
<li>-gold_hypo TEXT: Specifies the gold standard hypothesis for comparison. This field is required.</li>
<li>-gold_workflow TEXT: Specifies the gold standard workflow to be used as a reference during evaluation.</li>
<li>-pred_hypo TEXT: Specifies the predicted hypothesis generated by the discovery agent. This field is required.</li>
<li>-pred_workflow TEXT: Specifies the predicted workflow generated by the discovery agent.</li>
<li>-metadata_path TEXT: Specifies the path to the metadata file that is utilized during evaluation. This field is required.</li>
<li>-metadata_type [real|synth]: Determines the type of metadata used in the evaluation, where real indicates actual metadata and synth indicates synthetic metadata. This field is required.</li>
<li>-eval_output_path TEXT: Specifies where the evaluation results should be saved.</li>
<li>-help: Displays the help message and exits, detailing all available command options.</li>
</ul>
<h2>I Experiments</h2>
<p>For GPT-based models, we use OpenAI API (https://platform.openai.com/docs/ models), and for Llama3, we used Together API (https://docs.together.ai/docs/ inference-models)</p>
<h2>J Evaluator Prompts</h2>
<p>We provide below the exact prompts used for our GPT-4 based evaluation of the generated hypothesis against the gold hypothesis.</p>
<p>Listing 1 Decomposition Prompt to obtain sub-hypotheses from a hypothesis.</p>
<div class="codehilite"><pre><span></span><code><span class="n">decomposition_prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">    Given a set of dataset columns, a ground-truth hypothesis, and the</span>
<span class="s2">    analysis workflow used, your task is to extract the set of sub-hypotheses</span>
<span class="s2">    that are present in the hypothesis such that each sub-hypothesis covers a</span>
<span class="s2">    separate context, is self-sufficient, and operates on a coherent set of 3</span>
<span class="s2">    dimensions: Context, Variables, and Relations.</span>
<span class="s2">    Here are the definitions for these dimensions:</span>
<span class="s2">    - Contexts: Boundary conditions that limit the scope of a sub-hypothesis.</span>
<span class="s2">    E.g., &quot;for men over the age of 30&quot;, &quot;in Asia and Europe&quot;, or &quot;None&quot; if</span>
<span class="s2">    there is no boundary condition specified.</span>
<span class="s2">    - Variables: Known concepts that interact in a meaningful way under a</span>
<span class="s2">    given context to produce the sub-hypothesis. E.g., gender, age, income, or</span>
<span class="s2">    &quot;None&quot; if there is no interacting variable.</span>
<span class="s2">    - Relations: Interactions between a given set of variables under a given</span>
<span class="s2">    context to produce the sub-hypothesis. E.g., &quot;quadratic relationship&quot;,</span>
<span class="s2">    &quot;inversely proportional&quot;, piecewise conditionals, or &quot;None&quot; if there is no</span>
<span class="s2">    interacting relationship.</span>
<span class="s2">    Make sure to only use the information present in the hypothesis and the</span>
<span class="s2">    workflow. Do not add any new information.</span>
<span class="s2">    If no sub-hypotheses can be extracted, return an empty list.</span>
<span class="s2">    Here is the metadata for the task:</span>
<span class="s2">    ```json</span>
<span class="s2">    {{</span>
<span class="s2">        &quot;datasets&quot;: {dataset_metadata},</span>
<span class="s2">        &quot;hypothesis&quot;: &quot;{hypothesis}&quot;,</span>
<span class="s2">        &quot;workflow&quot;: &quot;{workflow}&quot;</span>
<span class="s2">    }}</span>
<span class="s2">    ```</span>

<span class="s2">    Return your answer as a JSON object in the following format:</span>
<span class="s2">    ```json</span>
<span class="s2">    {{</span>
<span class="s2">    &quot;sub_hypo&quot;: [</span>
<span class="s2">        {{</span>
<span class="s2">            &quot;text&quot;: the sub-hypothesis in natural language,</span>
<span class="s2">            &quot;context&quot;: a short text description of the context of the</span>
<span class="s2">            sub-hypothesis,</span>
<span class="s2">            &quot;variables&quot;: a list of columns involved in the sub-hypothesis,</span>
<span class="s2">            &quot;relations&quot;: a short text description of the relationship between</span>
<span class="s2">            the variables of the sub-hypothesis,</span>
<span class="s2">            &quot;explanation&quot;: a short text explanation for the breakdown of the</span>
<span class="s2">            sub-hypothesis</span>
<span class="s2">        }},</span>
<span class="s2">    ...</span>
<span class="s2">    ]</span>
<span class="s2">    }}</span>
</code></pre></div>

<p>Listing 2 Matching prompt to match contexts of two sub-hypotheses.</p>
<div class="codehilite"><pre><span></span><code>matching_prompt = f&quot;&quot;&quot;
    Given a gold hypothesis, a gold context, a predicted hypothesis, and a
    predicted context, your task is
    to determine if the predicted context semantically matches the
    ground-truth context.
    Here is the definition for Context: Boundary conditions that limit the
    scope of a sub-hypothesis. E.g., &quot;for men over the age of 30&quot;, &quot;in Asia
    and Europe&quot;, or &quot;None&quot; if there is no boundary condition specified.
    If the predicted context matches the gold context, return true, otherwise
    return false.
    Here is the metadata for the task:
    ```json
    {{
        &quot;gold_hypothesis&quot;: &quot;{gold_hypotheis}&quot;,
        &quot;gold_context&quot;: &quot;{gold_context}&quot;,
        &quot;predicted_hypothesis&quot;: &quot;{pred_hypothesis}&quot;,
        &quot;predicted_context&quot;: &quot;{pred_context}&quot;
    }}
    ```
    Return your answer as a JSON object in the following format:
    ```json
    {{
        &quot;match&quot;: true or false
    }}
</code></pre></div>

<p>Listing 3 Prompt for variable alignment between two sub-hypotheses.</p>
<div class="codehilite"><pre><span></span><code><span class="n">main_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    You are going to compare two natural-language hypotheses HypoA and HypoB</span>
<span class="s2">    accompanied with optional workflows: WorkflowA for HypoA and WorkflowB for</span>
<span class="s2">    HypoB.</span>
<span class="s2">    Both the hypotheses answer the natural language query &quot;QUERY&quot; over the</span>
<span class="s2">    dataset(s) described by dataset description(s) and column description(s)</span>
<span class="s2">    below.</span>
<span class="s2">    Compare HypoA and HypoB in terms of three aspects: Contexts, Variables,</span>
<span class="s2">    and Relations.</span>
<span class="s2">    E.g., for the hypothesis &quot;From 1995 to 2009, the number of sandhill cranes</span>
<span class="s2">    around the tundra (Indigilka River) surged by an astounding ~10X&quot;:</span>
<span class="s2">    * Contexts refer to the stratification of the data under which the given</span>
<span class="s2">    hypothesis is True. E.g., &quot;For all women&quot;, &quot;From 1995 to 2009&quot;.</span>
<span class="s2">    * Variables refer to the set of variables (either dependent or independent)</span>
<span class="s2">    that are mentioned in the hypothesis. E.g., number of sandhill cranes,</span>
<span class="s2">    location.</span>
<span class="s2">    * Relations refer to the form of relation between the variables. E.g.,</span>
<span class="s2">    &quot;surged by ~10x&quot;.</span>
<span class="s2">    Answer the following questions for a given pair of hypotheses, HypoA and</span>
<span class="s2">    HypoB, along with an explanation grounded on the QUERY and the DATASET(S).</span>
<span class="s2">    Here is the metadata for the task:</span>
<span class="s2">    ~~~json</span>
<span class="s2">    {{</span>
<span class="s2">    &quot;datasets&quot;: {datasets_json},</span>
<span class="s2">    &quot;query&quot;: {query},</span>
<span class="s2">    &quot;HypoA&quot;: {gold_hypo},</span>
<span class="s2">    &quot;WorkflowA&quot;: {gold_workflow},</span>
<span class="s2">    &quot;HypoB&quot;: {gen_hypo},</span>
<span class="s2">    &quot;WorkflowB&quot;: {gen_workflow}</span>
<span class="s2">    }}</span>
<span class="s2">    ~~~</span>
<span class="s2">    {variable_question}&quot;&quot;&quot;</span>
<span class="n">variable_question</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">    Question: For both HypoA and HypoB, what are the different variables found</span>
<span class="s2">    in the hypotheses? </span><span class="se">\</span>
<span class="s2">    Return your answer as a JSON object in the following format:</span>
<span class="s2">    ~~~json</span>
<span class="s2">    {{</span>
<span class="s2">    &quot;sizeA&quot;: num of variables used in HypoA</span>
<span class="s2">    &quot;sizeB&quot;: num of variables used in HypoB</span>
<span class="s2">    &quot;intersection&quot;: num of variables common in HypoA and HypoB. Use *fuzzy</span>
<span class="s2">    matching* to determine intersection, accounting for paraphrases or</span>
<span class="s2">    slightly different surface forms</span>
<span class="s2">    &quot;explanation&quot;: a short text explanation about the variables</span>
<span class="s2">    }}</span>
<span class="s2">    Answer:&quot;&quot;&quot;</span>
</code></pre></div>

<p>Listing 4 Prompt for relationship alignment between two sub-hypotheses.</p>
<div class="codehilite"><pre><span></span><code><span class="n">main_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    You are going to compare two natural-language hypotheses HypoA and HypoB</span>
<span class="s2">    accompanied with optional workflows: WorkflowA for HypoA and WorkflowB for</span>
<span class="s2">    HypoB.</span>
<span class="s2">    Both the hypotheses answer the natural language query &quot;QUERY&quot; over the</span>
<span class="s2">    dataset(s) described by dataset description(s) and column description(s)</span>
<span class="s2">    below.</span>
<span class="s2">    Compare HypoA and HypoB in terms of three aspects: Contexts, Variables,</span>
<span class="s2">    and Relations.</span>
<span class="s2">    E.g., for the hypothesis &quot;From 1995 to 2009, the number of sandhill cranes</span>
<span class="s2">    around the tundra (Indigilka River) surged by an astounding ~10X&quot;:</span>
<span class="s2">    * Contexts refer to the stratification of the data under which the given</span>
<span class="s2">    hypothesis is True. E.g., &quot;For all women&quot;, &quot;From 1995 to 2009&quot;.</span>
<span class="s2">    * Variables refer to the set of variables (either dependent or independent)</span>
<span class="s2">    that are mentioned in the hypothesis. E.g., number of sandhill cranes,</span>
<span class="s2">    location.</span>
<span class="s2">    * Relations refer to the form of relation between the variables. E.g.,</span>
<span class="s2">    &quot;surged by ~10x&quot;.</span>
<span class="s2">    Answer the following questions for a given pair of hypotheses, HypoA and</span>
<span class="s2">    HypoB, along with an explanation grounded on the QUERY and the DATASET(S).</span>
<span class="s2">    Here is the metadata for the task:</span>
<span class="s2">    ~~json</span>
<span class="s2">    {{</span>
<span class="s2">    &quot;datasets&quot;: {datasets_json},</span>
<span class="s2">    &quot;query&quot;: {query},</span>
<span class="s2">    &quot;HypoA&quot;: {gold_hypo},</span>
<span class="s2">    &quot;WorkflowA&quot;: {gold_workflow},</span>
<span class="s2">    &quot;HypoB&quot;: {gen_hypo},</span>
<span class="s2">    &quot;WorkflowB&quot;: {gen_workflow}</span>
<span class="s2">    }}</span>
<span class="s2">    ~</span>
<span class="s2">    {variable_question}&quot;&quot;&quot;</span>
<span class="n">dimension_question</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Question: Does HypoB exhibit the same relation as HypoA?</span>
<span class="s2">    Compare using the following example hierarchy of relationships (based on</span>
<span class="s2">    specificity): </span><span class="se">\</span>
<span class="s2">    &quot;there exists a relationship&quot; &gt; &quot;positive relationship&quot; &gt; &quot;positive AND</span>
<span class="s2">    (linear OR quadratic)&quot; &gt; &quot;positive AND linear.&quot;</span>
<span class="s2">    Options: A) very similar B) similar but general than HypoA C) different</span>
<span class="s2">    Return your answer as a JSON object in the following format:</span>
<span class="s2">    ~~json</span>
<span class="s2">    {{</span>
<span class="s2">    &quot;answer&quot;: one of the options from A) very similar B) similar but general</span>
<span class="s2">    than HypoA C) different</span>
<span class="s2">    &quot;explanation&quot;: a short text explanation about the relationship comparison</span>
<span class="s2">    }}</span>
<span class="s2">    Answer:&quot;&quot;&quot;</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ Each value is noised independently; therefore, each row has sufficient true data useful for discovery.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
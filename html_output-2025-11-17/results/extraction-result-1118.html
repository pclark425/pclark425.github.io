<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1118 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1118</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1118</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-250451641</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2009.08111v4.pdf" target="_blank">Reward Maximisation through Discrete Active Inference</a></p>
                <p><strong>Paper Abstract:</strong> Active inference is a probabilistic framework for modelling the behaviour of biological and artificial agents, which derives from the principle of minimising free energy. In recent years, this framework has successfully been applied to a variety of situations where the goal was to maximise reward, offering comparable and sometimes superior performance to alternative approaches. In this paper, we clarify the connection between reward maximisation and active inference by demonstrating how and when active inference agents perform actions that are optimal for maximising reward. Precisely, we show the conditions under which active inference produces the optimal solution to the Bellman equation--a formulation that underlies several approaches to model-based reinforcement learning and control. On partially observed Markov decision processes, the standard active inference scheme can produce Bellman optimal actions for planning horizons of 1, but not beyond. In contrast, a recently developed recursive active inference scheme (sophisticated inference) can produce Bellman optimal actions on any finite temporal horizon. We append the analysis with a discussion of the broader relationship between active inference and reinforcement learning.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1118.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1118.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standard Active Inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard Active Inference (softmax over negative expected free energy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A planning-as-inference scheme that assigns probabilities to action sequences via a softmax over negative expected free energy G(a|s_t)=D_KL[Q(s|a,s_t)||C(s)], then selects actions by sampling/choosing the most probable first action; integrates perception (variational/Bayesian state-estimation) with planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Standard active inference agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent embodies a generative model (MDP or POMDP), performs perception as (variational) Bayesian inference to obtain Q(s|a,observations), computes expected free energy G for candidate action sequences, and chooses actions according to Q(a|s_t) ∝ exp(-G(a|s_t)). Key components: categorical generative model over discrete states, variational inference for partial observability, expected free energy objective, softmax decision rule.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>information-seeking via expected information gain / expected free energy minimization (risk+ambiguity), i.e., intrinsic motivation / information gain maximization</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts action selection by computing posterior predictive distributions Q(s|a, s_t) for candidate action sequences and scoring them with expected free energy which decomposes into a risk term (alignment with a preference distribution C) and an ambiguity (expected outcome entropy) term; actions that reduce expected surprise and/or ambiguity are favoured. Uses current beliefs (posterior over states and learned parameters) to evaluate and select next actions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Finite-horizon MDPs and POMDPs (as studied in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Discrete-time, discrete state and action spaces; can be fully observed (MDP) or partially observed (POMDP) with observation likelihoods P(o|s); transition probabilities assumed known in the analysis, but learning extensions discussed (unknown transitions/likelihoods learned via Dirichlet updates).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Generic finite horizon: |S| finite, |A| finite, horizon T (planning horizon) arbitrary; no specific numeric sizes provided in the paper (complexity scales exponentially with T when enumerating action sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Theoretically Bellman-optimal for planning horizon T = 1 in the zero-temperature limit (β→+∞) and when state-estimation is exact; no empirical numeric performance metrics reported in this paper (analytical result only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balanced intrinsically via the decomposition of expected free energy into risk (exploitation: aligning future states with preferences) and ambiguity/information-seeking (exploration: preferring actions that reduce expected outcome entropy); randomness can be injected by sampling from the posterior over action sequences rather than greedy selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Backward induction (dynamic programming), model-based RL formulations, Thompson sampling (mentioned), Bayes-adaptive RL, maximum-entropy RL / Soft Actor-Critic (mentioned in discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Standard active inference (softmax over -G) selects Bellman-optimal actions only for one-step planning horizons (T=1) under exact state-estimation and preferences that concentrate mass on reward-maximising states (zero-temperature β→+∞); for longer horizons it generally does not coincide with backward induction because it scores all future courses of action rather than recursively considering future actions that themselves minimise expected free energy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not Bellman-optimal for planning horizons >1 even with known transitions unless modified; relies on exact state-estimation for the theoretical optimality claim (performance degrades with approximate/inexact inference); computationally expensive due to enumeration of action sequences (combinatorial explosion with horizon and branching factor).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1118.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1118.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sophisticated Active Inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sophisticated (Recursive) Active Inference (sophisticated inference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recursive planning-as-inference scheme that defines expected free energy recursively: G(a_tau|s_tau)=immediate G + E_{Q(a_{tau+1},s_{tau+1}|...)}[G(a_{tau+1}|s_{tau+1})], selecting actions that minimise this recursive G, effectively implementing backward induction within the expected free energy framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Sophisticated active inference agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent uses the same generative model and inference machinery as standard active inference but evaluates actions via a recursively defined expected free energy that assumes future actions will themselves minimise expected free energy; planning proceeds backward from the terminal time, yielding state-conditional action choices analogous to backward induction. Key components: generative model, (variational) state-estimation, recursive G computation, policy extraction via argmin G at each state.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>recursive expected free energy minimization (planning by recursive inference akin to dynamic programming / backward induction)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts actions by computing, for each candidate action at time τ, the immediate risk/ambiguity plus the expected G obtainable when subsequently taking actions that themselves minimise G (i.e., expectations taken under Q that conditions on choosing future G-minimising actions). Uses the current posterior over states and the model transitions to propagate value-like G backward through time.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Finite-horizon MDPs and POMDPs (with known transitions in theoretical analysis); partial observability accommodated via variational inference.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Discrete finite state and action spaces, finite temporal horizon T, can be partially observable with observation model P(o|s); theoretical results assume known dynamics and exact state-estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Arbitrary finite horizon T and finite |S|, |A|; complexity of planning grows with horizon and branching factor; paper does not report specific numeric sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Theoretically Bellman-optimal on any finite temporal horizon in the zero-temperature limit (β→+∞) and with exact state-estimation; i.e., under those assumptions, it coincides with backward induction and therefore yields reward-maximising policies. No empirical numeric metrics provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration emerges from the ambiguity term in G and preference-weighting; when multiple reward-maximising trajectories exist, the scheme selects ones that maximise entropy of future states (keeping options open) and minimise expected ambiguity (choosing actions leading to informative observations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Backward induction (identical in the β→∞, exact-inference limit), standard active inference, model-based RL/backward dynamic programming methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Recursive ('sophisticated') active inference implements backward induction in the expected free energy formulation and is Bellman-optimal on any finite horizon given exact inference and preference specification; when multiple reward-maximising sequences exist, it prefers those that maximise entropy over future states and reduce ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>In practical implementations the recursive scheme is computationally demanding; the specific sophisticated active inference implementation in cited work does not generally perform exact inference, so real-world performance depends on the accuracy of approximations and inference algorithms, and may degrade when inference is inexact or when scaling to large state/action/horizon spaces.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1118.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1118.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active Inference on POMDPs (with reward learning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Inference applied to Partially Observable Markov Decision Processes with reward/outcome learning via Dirichlet updates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of active inference to POMDPs where perception is variational Bayesian inference over latent states given observations, planning minimises expected free energy (risk + ambiguity), and reward/likelihoods can be learned by updating Dirichlet priors over observation likelihood matrices (A) or transition matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Active inference POMDP agent with Dirichlet parameter learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent maintains a generative POMDP (transition matrix, observation likelihood A, preference distribution C), performs variational inference to approximate P(s|a,observations), plans by minimising expected free energy incorporating ambiguity, and updates parameters (e.g., Dirichlet priors over A or transitions) by accumulating expected counts across episodes (a ← a + Σ_t o_t ⊗ Q(s_t|o_0:T)).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian adaptive experimental design via variational state-estimation combined with information-seeking terms in expected free energy; parameter learning via Bayesian (Dirichlet) updates</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts both action selection and internal model parameters: actions are chosen to minimise expected free energy (thus balancing reward-seeking and information gain), while parameters of the generative model (likelihoods or transitions) are updated after observations by accumulating Dirichlet counts weighted by posterior state beliefs, improving future state-estimation and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Partially Observable MDPs (POMDPs) with unknown reward/likelihood parameters in the learning scenario</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable, stochastic transitions and observations, discrete state/action/observation spaces; may include uncertainty over mapping from states to rewarding observations (sparse/ambiguous reward possible depending on setup).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Finite discrete spaces; no explicit numeric sizes provided; complexity determined by |S|, |A|, |O| and horizon T; learning requires accumulating counts over episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Theoretical guarantee: given exact variational minimisation (exact inference), minimising expected free energy in the β→+∞ limit yields action sequences that maximise expected reward; practical performance depends on accuracy of inference and learning—no numeric empirical performance reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not quantified in this paper; sample efficiency depends on accuracy of posterior inference and the rate at which Dirichlet counts accumulate, and is noted to be sensitive to inference approximations (e.g., severe mean-field approximations can harm learning).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Built into expected free energy: ambiguity term encourages information-seeking exploration to reduce state/parameter uncertainty, risk term encourages exploitation towards preferred (rewarding) outcomes; preference priors (C) and the structure of G shape the tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Bayesian model-based RL with Thompson sampling (discussed and compared qualitatively / in referenced empirical studies), Bayes-adaptive RL, model-free RL approaches noted for contrast.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Analytically shown that, in POMDPs with known transitions and exact inference, minimising expected free energy recovers reward-maximising actions in the β→+∞ limit; with learning, reward/likelihood mappings can be learned by Dirichlet accumulation rules, but success depends on the fidelity of state-estimation (inaccurate inference can mis-assign counts and impair learning).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Learning the true likelihood (or reward mapping) can fail when state inference is inaccurate (e.g., due to overly crude mean-field approximations), causing 'jumping to conclusions' and incorrect accumulation of Dirichlet parameters; scalability and computational tractability for large POMDPs remain challenging.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1118.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1118.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Monte-Carlo Tree Search (MCTS) for Active Inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte-Carlo Tree Search used to search the decision tree in active inference planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A selective decision-tree search algorithm (MCTS) proposed as a computationally efficient approach to approximate active inference planning by exploring high-value branches of the action tree rather than enumerating all action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Active inference agent with MCTS planning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Conceptual combination: active inference agent that uses Monte-Carlo tree search to selectively sample and evaluate action sequences under the expected free energy objective, thereby approximating exhaustive planning; components include generative model, posterior predictive simulation, and MCTS search/backpropagation to estimate G-values of nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>adaptive tree search (MCTS) guided by expected free energy estimates—selective sampling/amortised planning</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Rather than evaluating all action sequences, the planner grows and samples a search tree, using simulation rollouts and estimates of expected free energy to expand promising branches and prune others, thus allocating computational budget adaptively to informative/plausible trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Large POMDP/MDP decision trees where exhaustive enumeration is impractical (general mention; no specific benchmark used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable or fully observable sequential decision problems with large branching factor and horizon where naive enumeration is intractable; stochastic transitions and observations possible.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>High branch factor and long horizon typical of problems needing MCTS; no numeric sizes provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Mentioned as improving tractability and enabling selective deep planning, with references to empirical and methodological work (e.g., Champion et al., Fountas et al.); no numeric performance figures in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>MCTS inherently balances exploration/exploitation in tree search (e.g., via UCT-like criteria), combined here with G-based scoring to prioritise branches that minimise expected free energy (thus combining information-seeking with reward-seeking in the search criterion).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Full enumeration of action sequences (exact planning), amortised planning via neural networks, hierarchical abstraction methods (mentioned as alternative scalability solutions).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Posited as a practical approach to make active inference planning tractable in large decision trees; cited alongside other scalability approaches (hierarchical models, amortised planning). No formal or empirical evaluation provided in this paper itself.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No direct empirical evaluation in this paper; limitations are inherited from MCTS (sample complexity, variance in rollouts) and depend on fidelity of the generative model and rollout policy used during search.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1118.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1118.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thompson Sampling / Bayesian Model-Based RL (comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thompson Sampling (posterior sampling) and Bayesian model-based reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of Bayesian adaptive RL methods where agents sample a model or policy from a posterior distribution and act according to the sampled hypothesis (Thompson sampling); cited in the paper as behaviorally and formally related to active inference under certain preference/observation formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayesian model-based RL agent using Thompson sampling</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent maintains a posterior over environment models or parameters, samples a model/policy from that posterior (Thompson sampling), plans/acts according to the sampled model, and updates the posterior with observed data. In referenced comparisons, such agents can show similar exploration-exploitation patterns to active inference agents when preferences are defined over outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Thompson sampling / posterior sampling (Bayesian adaptive experimental design)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by sampling plausible models from the posterior and selecting actions optimal under the sampled model, thereby implicitly balancing exploration and exploitation according to posterior uncertainty; updated after each episode/observation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Bayes-adaptive MDPs / POMDPs (general mention; compared in referenced literature)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown dynamics or reward mappings initially; partially observable settings are possible; stochastic transitions and observations; uncertainty over model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies by application; no numeric details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitatively reported in referenced empirical work (Sajid et al. 2021a) to exhibit similar behaviour to active inference agents when preferences are over outcomes; no numeric metrics provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balanced via posterior uncertainty: more uncertain regions produce diverse sampled models leading to exploratory actions; as uncertainty reduces, samples concentrate and behaviour becomes exploitative.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Active inference (directly compared qualitatively/empirically in referenced works), Bayes-adaptive RL methods, information-theoretic exploration bonuses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Paper notes that Bayesian model-based RL with Thompson sampling can produce behaviour similar to active inference under certain formulations (e.g., preferences over outcomes), and that both approaches can select policies that maximise information gain when reward is absent. The current paper does not present new experiments on Thompson sampling but references prior empirical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not evaluated empirically in this paper; discussion highlights that empirical differences between these approaches depend on model specification, inference accuracy, and how preferences/rewards are encoded.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sophisticated Inference <em>(Rating: 2)</em></li>
                <li>Deep active inference agents using Monte-Carlo methods <em>(Rating: 2)</em></li>
                <li>An empirical evaluation of active inference in multi-armed bandits <em>(Rating: 2)</em></li>
                <li>Active Inference: Demystified and Compared <em>(Rating: 2)</em></li>
                <li>Branching Time Active Inference: Empirical study and complexity class analysis <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1118",
    "paper_id": "paper-250451641",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Standard Active Inference",
            "name_full": "Standard Active Inference (softmax over negative expected free energy)",
            "brief_description": "A planning-as-inference scheme that assigns probabilities to action sequences via a softmax over negative expected free energy G(a|s_t)=D_KL[Q(s|a,s_t)||C(s)], then selects actions by sampling/choosing the most probable first action; integrates perception (variational/Bayesian state-estimation) with planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Standard active inference agent",
            "agent_description": "Agent embodies a generative model (MDP or POMDP), performs perception as (variational) Bayesian inference to obtain Q(s|a,observations), computes expected free energy G for candidate action sequences, and chooses actions according to Q(a|s_t) ∝ exp(-G(a|s_t)). Key components: categorical generative model over discrete states, variational inference for partial observability, expected free energy objective, softmax decision rule.",
            "adaptive_design_method": "information-seeking via expected information gain / expected free energy minimization (risk+ambiguity), i.e., intrinsic motivation / information gain maximization",
            "adaptation_strategy_description": "Adapts action selection by computing posterior predictive distributions Q(s|a, s_t) for candidate action sequences and scoring them with expected free energy which decomposes into a risk term (alignment with a preference distribution C) and an ambiguity (expected outcome entropy) term; actions that reduce expected surprise and/or ambiguity are favoured. Uses current beliefs (posterior over states and learned parameters) to evaluate and select next actions.",
            "environment_name": "Finite-horizon MDPs and POMDPs (as studied in the paper)",
            "environment_characteristics": "Discrete-time, discrete state and action spaces; can be fully observed (MDP) or partially observed (POMDP) with observation likelihoods P(o|s); transition probabilities assumed known in the analysis, but learning extensions discussed (unknown transitions/likelihoods learned via Dirichlet updates).",
            "environment_complexity": "Generic finite horizon: |S| finite, |A| finite, horizon T (planning horizon) arbitrary; no specific numeric sizes provided in the paper (complexity scales exponentially with T when enumerating action sequences).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Theoretically Bellman-optimal for planning horizon T = 1 in the zero-temperature limit (β→+∞) and when state-estimation is exact; no empirical numeric performance metrics reported in this paper (analytical result only).",
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": "Balanced intrinsically via the decomposition of expected free energy into risk (exploitation: aligning future states with preferences) and ambiguity/information-seeking (exploration: preferring actions that reduce expected outcome entropy); randomness can be injected by sampling from the posterior over action sequences rather than greedy selection.",
            "comparison_methods": "Backward induction (dynamic programming), model-based RL formulations, Thompson sampling (mentioned), Bayes-adaptive RL, maximum-entropy RL / Soft Actor-Critic (mentioned in discussion).",
            "key_results": "Standard active inference (softmax over -G) selects Bellman-optimal actions only for one-step planning horizons (T=1) under exact state-estimation and preferences that concentrate mass on reward-maximising states (zero-temperature β→+∞); for longer horizons it generally does not coincide with backward induction because it scores all future courses of action rather than recursively considering future actions that themselves minimise expected free energy.",
            "limitations_or_failures": "Not Bellman-optimal for planning horizons &gt;1 even with known transitions unless modified; relies on exact state-estimation for the theoretical optimality claim (performance degrades with approximate/inexact inference); computationally expensive due to enumeration of action sequences (combinatorial explosion with horizon and branching factor).",
            "uuid": "e1118.0"
        },
        {
            "name_short": "Sophisticated Active Inference",
            "name_full": "Sophisticated (Recursive) Active Inference (sophisticated inference)",
            "brief_description": "A recursive planning-as-inference scheme that defines expected free energy recursively: G(a_tau|s_tau)=immediate G + E_{Q(a_{tau+1},s_{tau+1}|...)}[G(a_{tau+1}|s_{tau+1})], selecting actions that minimise this recursive G, effectively implementing backward induction within the expected free energy framework.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Sophisticated active inference agent",
            "agent_description": "Agent uses the same generative model and inference machinery as standard active inference but evaluates actions via a recursively defined expected free energy that assumes future actions will themselves minimise expected free energy; planning proceeds backward from the terminal time, yielding state-conditional action choices analogous to backward induction. Key components: generative model, (variational) state-estimation, recursive G computation, policy extraction via argmin G at each state.",
            "adaptive_design_method": "recursive expected free energy minimization (planning by recursive inference akin to dynamic programming / backward induction)",
            "adaptation_strategy_description": "Adapts actions by computing, for each candidate action at time τ, the immediate risk/ambiguity plus the expected G obtainable when subsequently taking actions that themselves minimise G (i.e., expectations taken under Q that conditions on choosing future G-minimising actions). Uses the current posterior over states and the model transitions to propagate value-like G backward through time.",
            "environment_name": "Finite-horizon MDPs and POMDPs (with known transitions in theoretical analysis); partial observability accommodated via variational inference.",
            "environment_characteristics": "Discrete finite state and action spaces, finite temporal horizon T, can be partially observable with observation model P(o|s); theoretical results assume known dynamics and exact state-estimation.",
            "environment_complexity": "Arbitrary finite horizon T and finite |S|, |A|; complexity of planning grows with horizon and branching factor; paper does not report specific numeric sizes.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Theoretically Bellman-optimal on any finite temporal horizon in the zero-temperature limit (β→+∞) and with exact state-estimation; i.e., under those assumptions, it coincides with backward induction and therefore yields reward-maximising policies. No empirical numeric metrics provided in this paper.",
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": "Exploration emerges from the ambiguity term in G and preference-weighting; when multiple reward-maximising trajectories exist, the scheme selects ones that maximise entropy of future states (keeping options open) and minimise expected ambiguity (choosing actions leading to informative observations).",
            "comparison_methods": "Backward induction (identical in the β→∞, exact-inference limit), standard active inference, model-based RL/backward dynamic programming methods.",
            "key_results": "Recursive ('sophisticated') active inference implements backward induction in the expected free energy formulation and is Bellman-optimal on any finite horizon given exact inference and preference specification; when multiple reward-maximising sequences exist, it prefers those that maximise entropy over future states and reduce ambiguity.",
            "limitations_or_failures": "In practical implementations the recursive scheme is computationally demanding; the specific sophisticated active inference implementation in cited work does not generally perform exact inference, so real-world performance depends on the accuracy of approximations and inference algorithms, and may degrade when inference is inexact or when scaling to large state/action/horizon spaces.",
            "uuid": "e1118.1"
        },
        {
            "name_short": "Active Inference on POMDPs (with reward learning)",
            "name_full": "Active Inference applied to Partially Observable Markov Decision Processes with reward/outcome learning via Dirichlet updates",
            "brief_description": "An extension of active inference to POMDPs where perception is variational Bayesian inference over latent states given observations, planning minimises expected free energy (risk + ambiguity), and reward/likelihoods can be learned by updating Dirichlet priors over observation likelihood matrices (A) or transition matrices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Active inference POMDP agent with Dirichlet parameter learning",
            "agent_description": "Agent maintains a generative POMDP (transition matrix, observation likelihood A, preference distribution C), performs variational inference to approximate P(s|a,observations), plans by minimising expected free energy incorporating ambiguity, and updates parameters (e.g., Dirichlet priors over A or transitions) by accumulating expected counts across episodes (a ← a + Σ_t o_t ⊗ Q(s_t|o_0:T)).",
            "adaptive_design_method": "Bayesian adaptive experimental design via variational state-estimation combined with information-seeking terms in expected free energy; parameter learning via Bayesian (Dirichlet) updates",
            "adaptation_strategy_description": "Adapts both action selection and internal model parameters: actions are chosen to minimise expected free energy (thus balancing reward-seeking and information gain), while parameters of the generative model (likelihoods or transitions) are updated after observations by accumulating Dirichlet counts weighted by posterior state beliefs, improving future state-estimation and planning.",
            "environment_name": "Partially Observable MDPs (POMDPs) with unknown reward/likelihood parameters in the learning scenario",
            "environment_characteristics": "Partially observable, stochastic transitions and observations, discrete state/action/observation spaces; may include uncertainty over mapping from states to rewarding observations (sparse/ambiguous reward possible depending on setup).",
            "environment_complexity": "Finite discrete spaces; no explicit numeric sizes provided; complexity determined by |S|, |A|, |O| and horizon T; learning requires accumulating counts over episodes.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Theoretical guarantee: given exact variational minimisation (exact inference), minimising expected free energy in the β→+∞ limit yields action sequences that maximise expected reward; practical performance depends on accuracy of inference and learning—no numeric empirical performance reported in this paper.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Not quantified in this paper; sample efficiency depends on accuracy of posterior inference and the rate at which Dirichlet counts accumulate, and is noted to be sensitive to inference approximations (e.g., severe mean-field approximations can harm learning).",
            "exploration_exploitation_tradeoff": "Built into expected free energy: ambiguity term encourages information-seeking exploration to reduce state/parameter uncertainty, risk term encourages exploitation towards preferred (rewarding) outcomes; preference priors (C) and the structure of G shape the tradeoff.",
            "comparison_methods": "Bayesian model-based RL with Thompson sampling (discussed and compared qualitatively / in referenced empirical studies), Bayes-adaptive RL, model-free RL approaches noted for contrast.",
            "key_results": "Analytically shown that, in POMDPs with known transitions and exact inference, minimising expected free energy recovers reward-maximising actions in the β→+∞ limit; with learning, reward/likelihood mappings can be learned by Dirichlet accumulation rules, but success depends on the fidelity of state-estimation (inaccurate inference can mis-assign counts and impair learning).",
            "limitations_or_failures": "Learning the true likelihood (or reward mapping) can fail when state inference is inaccurate (e.g., due to overly crude mean-field approximations), causing 'jumping to conclusions' and incorrect accumulation of Dirichlet parameters; scalability and computational tractability for large POMDPs remain challenging.",
            "uuid": "e1118.2"
        },
        {
            "name_short": "Monte-Carlo Tree Search (MCTS) for Active Inference",
            "name_full": "Monte-Carlo Tree Search used to search the decision tree in active inference planning",
            "brief_description": "A selective decision-tree search algorithm (MCTS) proposed as a computationally efficient approach to approximate active inference planning by exploring high-value branches of the action tree rather than enumerating all action sequences.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Active inference agent with MCTS planning",
            "agent_description": "Conceptual combination: active inference agent that uses Monte-Carlo tree search to selectively sample and evaluate action sequences under the expected free energy objective, thereby approximating exhaustive planning; components include generative model, posterior predictive simulation, and MCTS search/backpropagation to estimate G-values of nodes.",
            "adaptive_design_method": "adaptive tree search (MCTS) guided by expected free energy estimates—selective sampling/amortised planning",
            "adaptation_strategy_description": "Rather than evaluating all action sequences, the planner grows and samples a search tree, using simulation rollouts and estimates of expected free energy to expand promising branches and prune others, thus allocating computational budget adaptively to informative/plausible trajectories.",
            "environment_name": "Large POMDP/MDP decision trees where exhaustive enumeration is impractical (general mention; no specific benchmark used in this paper)",
            "environment_characteristics": "Partially observable or fully observable sequential decision problems with large branching factor and horizon where naive enumeration is intractable; stochastic transitions and observations possible.",
            "environment_complexity": "High branch factor and long horizon typical of problems needing MCTS; no numeric sizes provided in this paper.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Mentioned as improving tractability and enabling selective deep planning, with references to empirical and methodological work (e.g., Champion et al., Fountas et al.); no numeric performance figures in this paper.",
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": "MCTS inherently balances exploration/exploitation in tree search (e.g., via UCT-like criteria), combined here with G-based scoring to prioritise branches that minimise expected free energy (thus combining information-seeking with reward-seeking in the search criterion).",
            "comparison_methods": "Full enumeration of action sequences (exact planning), amortised planning via neural networks, hierarchical abstraction methods (mentioned as alternative scalability solutions).",
            "key_results": "Posited as a practical approach to make active inference planning tractable in large decision trees; cited alongside other scalability approaches (hierarchical models, amortised planning). No formal or empirical evaluation provided in this paper itself.",
            "limitations_or_failures": "No direct empirical evaluation in this paper; limitations are inherited from MCTS (sample complexity, variance in rollouts) and depend on fidelity of the generative model and rollout policy used during search.",
            "uuid": "e1118.3"
        },
        {
            "name_short": "Thompson Sampling / Bayesian Model-Based RL (comparison)",
            "name_full": "Thompson Sampling (posterior sampling) and Bayesian model-based reinforcement learning",
            "brief_description": "A family of Bayesian adaptive RL methods where agents sample a model or policy from a posterior distribution and act according to the sampled hypothesis (Thompson sampling); cited in the paper as behaviorally and formally related to active inference under certain preference/observation formulations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Bayesian model-based RL agent using Thompson sampling",
            "agent_description": "Agent maintains a posterior over environment models or parameters, samples a model/policy from that posterior (Thompson sampling), plans/acts according to the sampled model, and updates the posterior with observed data. In referenced comparisons, such agents can show similar exploration-exploitation patterns to active inference agents when preferences are defined over outcomes.",
            "adaptive_design_method": "Thompson sampling / posterior sampling (Bayesian adaptive experimental design)",
            "adaptation_strategy_description": "Adapts by sampling plausible models from the posterior and selecting actions optimal under the sampled model, thereby implicitly balancing exploration and exploitation according to posterior uncertainty; updated after each episode/observation.",
            "environment_name": "Bayes-adaptive MDPs / POMDPs (general mention; compared in referenced literature)",
            "environment_characteristics": "Unknown dynamics or reward mappings initially; partially observable settings are possible; stochastic transitions and observations; uncertainty over model parameters.",
            "environment_complexity": "Varies by application; no numeric details in this paper.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitatively reported in referenced empirical work (Sajid et al. 2021a) to exhibit similar behaviour to active inference agents when preferences are over outcomes; no numeric metrics provided in this paper.",
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": "Balanced via posterior uncertainty: more uncertain regions produce diverse sampled models leading to exploratory actions; as uncertainty reduces, samples concentrate and behaviour becomes exploitative.",
            "comparison_methods": "Active inference (directly compared qualitatively/empirically in referenced works), Bayes-adaptive RL methods, information-theoretic exploration bonuses.",
            "key_results": "Paper notes that Bayesian model-based RL with Thompson sampling can produce behaviour similar to active inference under certain formulations (e.g., preferences over outcomes), and that both approaches can select policies that maximise information gain when reward is absent. The current paper does not present new experiments on Thompson sampling but references prior empirical comparisons.",
            "limitations_or_failures": "Not evaluated empirically in this paper; discussion highlights that empirical differences between these approaches depend on model specification, inference accuracy, and how preferences/rewards are encoded.",
            "uuid": "e1118.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sophisticated Inference",
            "rating": 2,
            "sanitized_title": "sophisticated_inference"
        },
        {
            "paper_title": "Deep active inference agents using Monte-Carlo methods",
            "rating": 2,
            "sanitized_title": "deep_active_inference_agents_using_montecarlo_methods"
        },
        {
            "paper_title": "An empirical evaluation of active inference in multi-armed bandits",
            "rating": 2,
            "sanitized_title": "an_empirical_evaluation_of_active_inference_in_multiarmed_bandits"
        },
        {
            "paper_title": "Active Inference: Demystified and Compared",
            "rating": 2,
            "sanitized_title": "active_inference_demystified_and_compared"
        },
        {
            "paper_title": "Branching Time Active Inference: Empirical study and complexity class analysis",
            "rating": 2,
            "sanitized_title": "branching_time_active_inference_empirical_study_and_complexity_class_analysis"
        }
    ],
    "cost": 0.02077175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reward Maximisation through Discrete Active Inference</p>
<p>Lancelot Da Costa l.da-costa@imperial.ac.uk 
Department of Mathematics
Laureate Institute for Brain Research Tulsa
Wellcome Centre for Human Neuroimaging University College London London
Imperial College London London
SW7 2AZ, WC1N 3AR, 74136OKUK, UK, United States</p>
<p>Noor Sajid noor.sajid.18@ucl.ac.uk 
Department of Mathematics
Laureate Institute for Brain Research Tulsa
Wellcome Centre for Human Neuroimaging University College London London
Imperial College London London
SW7 2AZ, WC1N 3AR, 74136OKUK, UK, United States</p>
<p>Thomas Parr thomas.parr.12@ucl.ac.uk 
Department of Mathematics
Laureate Institute for Brain Research Tulsa
Wellcome Centre for Human Neuroimaging University College London London
Imperial College London London
SW7 2AZ, WC1N 3AR, 74136OKUK, UK, United States</p>
<p>Karl Friston k.friston@ucl.ac.uk 
Department of Mathematics
Laureate Institute for Brain Research Tulsa
Wellcome Centre for Human Neuroimaging University College London London
Imperial College London London
SW7 2AZ, WC1N 3AR, 74136OKUK, UK, United States</p>
<p>Ryan Smith rsmith@laureateinstitute.org 
Department of Mathematics
Laureate Institute for Brain Research Tulsa
Wellcome Centre for Human Neuroimaging University College London London
Imperial College London London
SW7 2AZ, WC1N 3AR, 74136OKUK, UK, United States</p>
<p>Reward Maximisation through Discrete Active Inference
Generative modelcontrol as inferencedynamic programmingBellman optimalitymodel-based reinforcement learningdiscrete-time stochastic optimal controlBayesian inferenceMarkov decision process
Active inference is a probabilistic framework for modelling the behaviour of biological and artificial agents, which derives from the principle of minimising free energy. In recent years, this framework has successfully been applied to a variety of situations where the goal was to maximise reward, offering comparable and sometimes superior performance to alternative approaches. In this paper, we clarify the connection between reward maximisation and active inference by demonstrating how and when active inference agents perform actions that are optimal for maximising reward. Precisely, we show the conditions under which active inference produces the optimal solution to the Bellman equation-a formulation that underlies several approaches to model-based reinforcement learning and control. On partially observed Markov decision processes, the standard active inference scheme can produce Bellman optimal actions for planning horizons of 1, but not beyond. In contrast, a recently developed recursive active inference scheme (sophisticated inference) can produce Bellman optimal actions on any finite temporal horizon. We append the analysis with a discussion of the broader relationship between active inference and reinforcement learning.</p>
<p>Introduction</p>
<p>Active inference</p>
<p>Active inference is a normative framework for modelling intelligent behaviour in biological and artificial agents. It simulates behaviour by numerically integrating equations of motion thought to describe the behaviour of biological systems, a description based on the free energy principle (Barp et al., 2022;Friston, 2010;Ramstead et al., 2022). Active inference comprises a collection of algorithms for modelling perception, learning, and decision-making in the context of both continuous and discrete state spaces (Barp et al., 2022;Buckley et al., 2017;Da Costa et al., 2020;Friston et al., , 2017c. Briefly, building active inference agents entails: 1) equipping the agent with a (generative) model of the environment, 2) fitting the model to observations through approximate Bayesian inference by minimising variational free energy (i.e., optimising an evidence lower bound (Beal, 2003;Bishop, 2006;Blei et al., 2017;Jordan et al., 1998)) and 3) selecting actions that minimise expected free energy, a quantity that that can be decomposed into risk (i.e., the divergence between predicted and preferred paths) and ambiguity, leading to context-specific combinations of exploratory and exploitative behaviour (Millidge, 2021;. This framework has been used to simulate and explain intelligent behaviour in neuroscience (Adams et al., 2013;Parr, 2019;, psychology and psychiatry (Smith et al., 2020a(Smith et al., ,b,d, 2021a(Smith et al., ,b,c,d, 2022b, machine learning (Çatal et al., 2020;Fountas et al., 2020;Mazzaglia et al., 2021;Millidge, 2020;Tschantz et al., 2019 and robotics (Çatal et al., 2021;Oliver et al., 2021;Pezzato et al., 2020;Pio-Lopez et al., 2016;Sancaktar et al., 2020;Schneider et al., 2022).</p>
<p>Reward maximisation through active inference?</p>
<p>In contrast, the traditional approaches towards simulating and explaining intelligent behaviourstochastic optimal control (Bellman, 1957;Bertsekas and Shreve, 1996) and reinforcement learning (RL; Barto and Sutton (1992))-derive from the normative principle of executing actions to maximise reward scoring the utility afforded by each state of the world. This idea dates back to expected utility theory (Von Neumann and Morgenstern, 1944), an economic model of rational choice behaviour, which also underwrites game theory (Von Neumann and Morgenstern, 1944) and decision theory (Berger, 1985;Dayan and Daw, 2008). Several empirical studies have shown that active inference can successfully perform tasks that involve collecting reward, often (but not always) showing comparative or superior performance to RL (Cullen et al., 2018;Marković et al., 2021;Mazzaglia et al., 2021;Millidge, 2020;Paul et al., 2021;Sajid et al., 2021a;Smith et al., 2020dSmith et al., , 2021bSmith et al., ,c, 2022bvan der Himst and Lanillos, 2020), and marked improvements when interacting with volatile environments (Marković et al., 2021;Sajid et al., 2021a). Given the prevalence and historical pedigree of reward maximisation, we ask:</p>
<p>How and when do active inference agents execute actions that are optimal with respect to reward maximisation?</p>
<p>Organisation of paper</p>
<p>In this paper, we explain (and prove) how and when active inference agents exhibit (Bellman) optimal reward maximising behaviour.</p>
<p>For this, we start by restricting ourselves to the simplest problem: maximising reward on a finite horizon Markov decision process (MDP) with known transition probabilities-a sequential decisionmaking task with complete information. In this setting, we review the backward induction algorithm from dynamic programming, which forms the workhorse of many optimal control and model-based RL algorithms. This algorithm furnishes a Bellman optimal state-action mapping, which means that it provides provably optimal decisions from the point of view of reward maximisation (Section 2).</p>
<p>We then introduce active inference on finite horizon MDPs (Section 3)-a scheme consisting of perception as inference followed by planning as inference, which selects actions so that future states best align with preferred states.</p>
<p>In Section 4, we show how and when active inference maximises reward in MDPs. Specifically, when the preferred distribution is a (uniform mixture of) Dirac distribution(s) over reward maximising trajectories, selecting action sequences according to active inference maximises reward (Section 4.1). Yet, active inference agents, in their standard implementation, can select actions that maximise reward only when planning one step ahead (Section 4.2). It takes a recursive, sophisticated form of active inference to select actions that maximise reward-in the sense of a Bellman optimal state-action mapping-on any finite time-horizon (Section 4.3).</p>
<p>In Section 5, we introduce active inference on partially observable Markov decision processes with known transition probabilities-a sequential decision-making task where states need to be inferred from observations-and explain how the results from the MDP setting generalise to this setting.</p>
<p>Our findings are summarised in Section 7. All of our analyses assume that the agent knows the environmental dynamics (i.e., transition probabilities) and reward function. In Appendix A, we discuss how active inference agents can learn their world model and rewarding states when these are initially unknown-and the broader relationship between active inference and RL. Figure 1: Finite horizon Markov decision process. This is a Markov decision process pictured as a Bayesian network (Jordan et al., 1998;Pearl, 1998). A finite horizon MDP comprises a finite sequence of states, indexed in time. The transition from one state to the next state depends on action. As such, for any given action sequence, the dynamics of the MDP form a Markov chain on state-space. In this fully observed setting, actions can be selected under a state-action policy, Π, indicated with a dashed line: this is a probabilistic mapping from state-space and time to actions.</p>
<p>Reward maximisation on finite horizon MDPs</p>
<p>In this section, we consider the problem of reward maximisation in Markov decision processes (MDPs) with known transition probabilities.</p>
<p>Basic definitions</p>
<p>MDPs are a class of models specifying environmental dynamics widely used in dynamic programming, model-based RL, and more broadly in engineering and artificial intelligence (Barto and Sutton, 1992;Stone, 2019). They are used to simulate sequential decision-making tasks with the objective of maximising a reward or utility function. An MDP specifies environmental dynamics unfolding in discrete space and time under the actions pursued by an agent.</p>
<p>Definition 1 (Finite horizon MDP). A finite horizon MDP comprises the following collection of data:</p>
<p>• S a finite set of states.</p>
<p>• T = {0, ..., T } a finite set which stands for discrete time. T is the temporal horizon (a.k.a. planning horizon).</p>
<p>• A is a finite set of actions.</p>
<p>• P (s t = s | s t−1 = s, a t−1 = a) is the probability that action a ∈ A in state s ∈ S at time t − 1 will lead to state s ∈ S at time t. s t are random variables over S that correspond to the state being occupied at time t = 0, ..., T .</p>
<p>• P (s 0 = s) specifies the probability of being at state s ∈ S at the start of the trial.</p>
<p>• R(s) is the finite reward received by the agent when at state s ∈ S.</p>
<p>The dynamics afforded by a finite horizon MDP (see Figure 1) can be written globally as a probability distribution over state trajectories s 0:T := (s 0 , . . . , s T ), given a sequence of actions a 0:T −1 := (a 0 , . . . , a T −1 ), which factorises as follows:
P (s 0:T | a 0:T −1 ) = P (s 0 ) T τ =1 P (s τ | s τ −1 , a τ −1 ).
Remark 2 (On the definition of reward). More generally, the reward function can be taken to be dependent on the previous action and previous state: R a (s | s) is the reward received after transitioning from state s to state s , due to action a ( Barto and Sutton, 1992;Stone, 2019). However, given an MDP with such a reward function, we can recover our simplified setting by defining a new MDP where the new states comprise the previous action, previous state, and current state in the original MDP. By inspection, the resulting reward function on the new MDP depends only on the current state (i.e., R(s)).</p>
<p>Remark 3 (Admissible actions). In general, it is possible that only some actions can be taken at each state. In this case, one defines A s to be the finite set of (allowable) actions from state s ∈ S. All forthcoming results concerning MDPs can be extended to this setting.</p>
<p>To formalise what it means to choose actions in each state, we introduce the notion of a stateaction policy.</p>
<p>Definition 4 (State-action policy). A state-action policy Π is a probability distribution over actions, that depends on the state that the agent occupies, and time. Explicitly,
Π : A × S × T → [0, 1] (a, s, t) → Π(a | s, t) ∀(s, t) ∈ S × T : a∈A Π(a | s, t) = 1.
When s t = s, we will write Π(a | s t ) := Π(a | s, t). Note that the action at the temporal horizon T is redundant, as no further reward can be reaped from the environment. Therefore, one often specifies state-action policies only up to time T − 1, as Π :
A × S × {0, . . . , T − 1} → [0, 1].
The state-action policy-as defined here-can be regarded as a generalisation of a deterministic state-action policy that assigns the probability of 1 to an available action and 0 otherwise.</p>
<p>Remark 5 (Conflicting terminologies: policy in active inference). In active inference, a policy is defined as a sequence of actions indexed in time 1 . To avoid terminological confusion, we use action sequences to denote policies under active inference.</p>
<p>At time t, the goal is to select an action that maximises future cumulative reward:
R(s t+1:T ) := T τ =t+1 R(s τ ).
Specifically, this entails following a state-action policy Π that maximises the state-value function:
v Π (s, t) := E Π [R(s t+1:T ) | s t = s]
for any (s, t) ∈ S × T. The state-value function scores the expected cumulative reward if the agent pursues state-action policy Π from the state s t = s. When the state s t = s is clear from context, we will often write v Π (s t ) := v Π (s, t). Loosely speaking, we will call the expected reward the return.</p>
<p>Remark 6 (Notation E Π ). Whilst standard in RL (Barto and Sutton, 1992;Stone, 2019), the notation E Π [R(s t+1:T ) | s t = s] can be confusing. It denotes the expected reward, under the transition probabilities of the MDP and a state-action policy Π E P (s t+1:T |a t:T −1 ,st=s)Π(a t:T −1 |s t+1:T −1 ,st=s) [R(s t+1:T )].</p>
<p>It is important to keep this correspondence in mind, as we will use both notations depending on context.</p>
<ol>
<li>These are analogous to temporally extended actions or options introduced under the options framework in RL (Stolle and Precup, 2002).</li>
</ol>
<p>Remark 7 (Temporal discounting). In infinite horizon MDPs (i.e., when T is infinite), RL often seeks to maximise the discounted sum of rewards
v Π (s, t) := E Π ∞ τ =t γ τ −t R(s τ +1 ) | s t = s ,
for a given temporal discounting term γ ∈ (0, 1) (Barto and Sutton, 1992;Bertsekas and Shreve, 1996;Kaelbling et al., 1998). In fact, temporal discounting is added to ensure that the infinite sum of future rewards converges to a finite value (Kaelbling et al., 1998). In finite horizon MDPs temporal discounting is not necessary so we set γ = 1 (c.f., (Schmidhuber, 2006(Schmidhuber, , 2010).</p>
<p>To find the best state-action policies, we would like to rank them in terms of their return. We introduce a partial ordering such that a state-action policy is better than another if it yields a higher return in any situation:
Π ≥ Π ⇐⇒ ∀(s, t) ∈ S × T : v Π (s, t) ≥ v Π (s, t).
Similarly, a state-action policy Π is strictly better than another Π if it yields strictly higher returns:
Π &gt; Π ⇐⇒ Π ≥ Π and ∃(s, t) ∈ S × T : v Π (s, t) &gt; v Π (s, t).</p>
<p>Bellman optimal state-action policies</p>
<p>A state-action policy is Bellman optimal if it is better than all alternatives.</p>
<p>Definition 8 (Bellman optimality). A state-action policy Π * is Bellman optimal if and only if it is better than all other state-action policies:
Π * ≥ Π, ∀Π.
In other words, it maximises the state-value function v Π (s, t) for any state s at time t.</p>
<p>It is important to verify that this concept is not vacuous.</p>
<p>Proposition 9 (Existence of Bellman optimal state-action policies). Given a finite horizon MDP as specified in Definition 1, there exists a Bellman optimal state-action policy Π * .</p>
<p>A proof can be found in Appendix B.1. Note that uniqueness of the Bellman optimal state-action policy is not implied by Proposition 9; indeed, multiple Bellman optimal state-action policies may exist (Bertsekas and Shreve, 1996;Puterman, 2014). Now that we know that Bellman optimal state-action policies exist, we can characterise them as a return-maximising action followed by a Bellman optimal state-action policy.</p>
<p>Proposition 10 (Characterisation of Bellman optimal state-action policies). For a state-action policy Π, the following are equivalent:</p>
<ol>
<li>
<p>Π is Bellman optimal.</p>
</li>
<li>
<p>Π is both (a) Bellman optimal when restricted to {1, . . . , T }. In other words, ∀ state-action policy Π and (s,
t) ∈ S × {1, . . . T } v Π (s, t) ≥ v Π (s, t).
(b) At time 0, Π selects actions that maximise return:
Π(a | s, 0) &gt; 0 ⇐⇒ a ∈ arg max a∈A E Π [R(s 1:T ) | s 0 = s, a 0 = a], ∀s ∈ S.(1)
A proof can be found in Appendix B.2. Note that this characterisation offers a recursive way to construct Bellman optimal state-action policies by successively selecting the best action, as specified by (1), starting from T and inducting backwards (Puterman, 2014).</p>
</li>
</ol>
<p>Backward induction</p>
<p>Proposition 10 suggests a straightforward recursive algorithm to construct Bellman optimal stateaction policies known as backward induction (Puterman, 2014). Backward induction has a long history. It was developed by the German mathematician Zermelo in 1913 to prove that chess has Bellman optimal strategies (Zermelo, 1913). In stochastic control, backward induction is one of the main methods for solving the Bellman equation (Adda and Cooper, 2003;Miranda and Fackler, 2002;Sargent, 2000). In game theory, the same method is used to compute sub-game perfect equilibria in sequential games (Fudenberg and Tirole, 1991).</p>
<p>Backward induction entails planning backwards in time, from a goal state at the end of a problem, by recursively determining the sequence of actions that enables reaching the goal. It proceeds by first considering the last time at which a decision might be made and choosing what to do in any situation at that time in order to get to the goal state. Using this information, one can then determine what to do at the second-to-last decision time. This process continues backwards until one has determined the best action for every possible situation or state at every point in time.</p>
<p>Proposition 11 (Backward induction: construction of Bellman optimal state-action policies). Backward induction
Π(a | s, T − 1) &gt; 0 ⇐⇒ a ∈ arg max a∈A E[R(s T ) | s T −1 = s, a T −1 = a], ∀s ∈ S Π(a | s, T − 2) &gt; 0 ⇐⇒ a ∈ arg max a∈A E Π [R(s T −1:T ) | s T −2 = s, a T −2 = a], ∀s ∈ S . . . Π(a | s, 0) &gt; 0 ⇐⇒ a ∈ arg max a∈A E Π [R(s 1:T ) | s 0 = s, a 0 = a], ∀s ∈ S (2)
defines a Bellman optimal state-action policy Π. Furthermore, this characterisation is complete: all Bellman optimal state-action policies satisfy the backward induction relation (2).</p>
<p>A proof can be found in Appendix B.3.</p>
<p>Example 1 (Intuition for backward induction). To give a concrete example of this kind of planning, backward induction (2) would consider the actions below in the following order:</p>
<ol>
<li>Desired goal: I would like to go to the grocery store, 2. Intermediate action: I need to drive to the store, 3. Current best action: I should put my shoes on.</li>
</ol>
<p>Proposition 11 tells us that to be optimal with respect to reward maximisation, one must plan like backward induction. This will be central to our analysis of reward maximisation in active inference.</p>
<p>Active inference on finite horizon MDPs</p>
<p>We now turn to introducing active inference agents on finite horizon MDPs with known transition probabilities. We assume that the agent's generative model of its environment is given by the previously defined finite horizon MDP (see Definition 1). We do not consider the case where the transitions have to be learned but comment on it in the Appendix A.2 (see also (Da Costa et al., 2020;).</p>
<p>In what follows, we fix a time t ≥ 0 and suppose that the agent has been in states s 0 , . . . , s t . To ease notation, we let s := s t+1:T , a := a t:T be the future states and future actions. We define Q to be the predictive distribution, which encodes the predicted future states and actions given that the agent is in state s t Q( s, a | s t ) :=
T −1 τ =t Q(s τ +1 | a τ , s τ )Q(a τ | s τ ).</p>
<p>Perception as inference</p>
<p>In active inference, perception entails inferences about future, past, and current states given observations and a sequence of actions. When states are partially observed, this is done through variational Bayesian inference by minimising a free energy functional (a.k.a. an evidence bound (Beal, 2003;Bishop, 2006;Blei et al., 2017;Wainwright and Jordan, 2007)).</p>
<p>In the MDP setting, past and current states are known, so it is only necessary to infer future states given the current state and action sequence P ( s | a, s t ). These posterior distributions P ( s | a, s t ) can be computed exactly in virtue of the fact that the transition probabilities of the MDP are known; hence variational inference becomes exact Bayesian inference.
Q( s | a, s t ) := P ( s | a, s t ) = T −1 τ =t P (s τ +1 | s τ , a τ ).
(3)</p>
<p>Planning as inference</p>
<p>Now that the agent has inferred future states given alternative action sequences, we must assess these alternative plans by examining the resulting state trajectories. The objective that active inference agents optimise-in order to select the best possible actions-is the expected free energy (Barp et al., 2022;Da Costa et al., 2020;. Under active inference, agents minimise expected free energy in order to maintain themselves distributed according to a target distribution C over the state-space S encoding the agent's preferences.</p>
<p>Definition 12 (Expected free energy on MDPs). On MDPs, the expected free energy of an action sequence a starting from s t is defined as (Barp et al., 2022):
G( a | s t ) = D KL <a href="4">Q( s | a, s t ) | C( s)</a>
Therefore, minimising expected free energy corresponds to making the distribution over predicted states close to the distribution C that encodes prior preferences. Note that the expected free energy in partially observed MDPs comprises an additional ambiguity term (see Section 5), which is dropped here as there is no ambiguity about observed states.</p>
<p>Since the expected free energy assesses the goodness of inferred future states under a course of action, we can refer to planning as inference (Attias, 2003;Botvinick and Toussaint, 2012). The expected free energy may be rewritten as
G( a | s t ) = E Q( s| a,st) [− log C( s)] Expected surprise − H[Q( s | a, s t )]
Entropy of future states (5) Hence, minimising expected free energy minimises the expected surprise of states 2 according to C and maximises the entropy of Bayesian beliefs over future states (a maximum entropy principle (Jaynes, 1957a), which is sometimes cast as keeping options open (Klyubin et al., 2008)).</p>
<p>Remark 13 (Numerical tractability). The expected free energy is straightforward to compute using linear algebra. Given an action sequence a, C( s) and Q( s | a, s t ) are categorical distributions over S T −t . Let their parameters be c, s a ∈ [0, 1] |S|(T −1) , where | · | denotes the cardinality of a set. Then the expected free energy reads
G( a | s t ) = s T a (log s a − log c).(6)
Notwithstanding, (6) is expensive to evaluate repeatedly when all possible action sequences are considered. In practice, one can adopt a temporal mean field approximation over future states :
Q( s | a, s t ) ≈ T τ =t+1 Q(s τ | a, s t ),
which yields the simplified expression
G( a | s t ) ≈ T τ =t+1 D KL [Q(s τ | a, s t ) | C(s τ )].(7)
Expression (7) is much easier to handle: for each action sequence a, 1) one evaluates the summands sequentially τ = t + 1, . . . , T , and 2) if and when the sum up to τ becomes significantly higher than the lowest expected free energy encountered during planning, G( a | s t ) is set to an arbitrarily high value. Setting G( a | s t ) to a high value is equivalent to pruning away unlikely trajectories. This bears some similarity to decision tree pruning procedures used in RL (Huys et al., 2012). It finesses exploration of the decision-tree in full depth and provides an Occam's window for selecting action sequences.</p>
<p>Complementary approaches can help make planning tractable. For example, hierarchical generative models factorise decisions into multiple levels. By abstracting information at a higher-level, lower-levels entertain fewer actions )-which reduces the depth of the decision tree by orders of magnitude. Another approach is to use algorithms that search the decision-tree selectively, such as Monte-Carlo tree search (Champion et al., 2021a,b;Fountas et al., 2020;Maisto et al., 2021;Silver et al., 2016), and amortising planning using artificial neural networks (i.e., learning to plan) (Çatal et al., 2020;Fountas et al., 2020;Sajid et al., 2021b).</p>
<p>Reward maximisation on MDPs through active inference</p>
<p>Here, we show how active inference solves the reward maximisation problem.</p>
<p>Reward maximisation as reaching preferences</p>
<p>From the definition of expected free energy (4), active inference on MDPs can be thought of as reaching and remaining at a target distribution C over state-space.</p>
<p>The idea that underwrites this section is that when the stationary distribution has all of its mass on reward maximising states, the agent will maximise reward. To illustrate this, we define a preference distribution C β , β &gt; 0 over state-space S, such that preferred states are rewarding states 3
C β (σ) := exp βR(σ) ς∈S exp βR(ς) ∝ exp(βR(σ)), ∀σ ∈ S ⇐⇒ − log C β (σ) = −βR(σ) − c(β), ∀σ ∈ S, for some c(β) ∈ R constant w.r.t σ.
The (inverse temperature) parameter β &gt; 0 scores how motivated the agent is to occupy reward maximising states. Note that states s ∈ S that maximise the reward R(s) maximise C β (s) and minimise − log C β (s) for any β &gt; 0.</p>
<p>Using the additive property of the reward function, we can extend C β to a probability distribution over trajectories σ := (σ 1 , . . . , σ T ) ∈ S T . Specifically, C β scores to what extent a trajectory is preferred over another trajectory:
C β ( σ) := exp βR( σ) ς∈S T exp βR( ς) = T τ =1 exp βR(σ τ ) ς∈S exp βR(ς) = T τ =1 C β (σ τ ), ∀ σ ∈ S T ⇐⇒ − log C β ( σ) = −βR( σ) − c (β) = − T τ =1 βR(σ τ ) − c (β), ∀ σ ∈ S T ,(8)where c (β) := c(β)T ∈ R is constant w.r.t σ.
When the preferences are defined in this way, the zero-temperature limit β → +∞ is the case where the preferences C β are non-zero only for states or trajectories that maximise reward. In this case, lim β→+∞ C β is a uniform mixture of Dirac distributions over reward maximising trajectories:
lim β→+∞ C β ∝ s∈I T −t Dirac s I := arg max s∈S R(s).(9)
This is because, for a reward maximising state σ, exp(βR(σ)) will converge to +∞ more quickly than exp(βR(σ )) for a non-reward maximising state σ . Since C β is constrained to be normalised to 1 (as it is a probability distribution), C β (σ ) β→+∞ − −−−− → 0. Hence, in the limit β → +∞, C β is non-zero (and uniform) only on reward maximising states.</p>
<p>We now show how reaching preferred states can be formulated as reward maximisation:</p>
<p>Lemma 14. The sequence of actions that minimises expected free energy also maximises expected reward in the zero temperature limit β → +∞ (9):
lim β→+∞ arg min a G( a | s t ) ⊆ arg max a E Q( s| a,st) [R( s)]
Furthermore, of those action sequences that maximise expected reward, the expected free energy minimisers will be those that maximise the entropy of future states H[Q( s | a, s t )].</p>
<p>A proof can be found in Appendix B.4. In the zero temperature limit β → +∞, minimising expected free energy corresponds to choosing the action sequence a such that Q( s | a, s t ) has most mass on reward maximising states or trajectories (see Figure 2). Of those reward maximising candidates, the minimiser of expected free energy maximises the entropy of future states H[Q( s | a, s t )], thus leaving options open.</p>
<p>Reward maximisation on MDPs with a temporal horizon of 1</p>
<p>In this section, we first consider the case of a single-step decision problem (i.e., a temporal horizon of T = 1) and demonstrate how the standard active inference scheme maximises reward on this problem in the limit β → +∞. This will act as an important building block for when we subsequently consider more general multi-step decision problems.</p>
<p>The standard decision-making procedure in active inference consists of assigning each action sequence with a probability given by the softmax of the negative expected free energy (Barp et al., 2022;Da Costa et al., 2020;Friston et al., 2017a)
Q( a | s t ) ∝ exp(−G( a | s t )).
Agents then select the most likely action under this distribution
a t ∈ arg max a∈A Q(a | s t ) = arg max a∈A a Q(a | a)Q( a | s t ) = arg max a∈A a Q(a | a) exp(−G( a | s t )) = arg max a∈A a ( a) t =a exp(−G( a | s t )).</p>
<p>Figure 2:</p>
<p>Reaching preferences and the zero temperature limit. We illustrate how active inference selects actions such that Q( s | a, s t ) most closely matches the preference distribution C β (top-right). In this example, the discrete state-space is a discretisation of a continuous interval in R, and the preferences and predictive distributions over states have a Gaussian shape. The predictive distribution Q is assumed to have a fixed variance with respect to action sequences, such that the only parameter that can be optimised by action selection is its mean. Crucially, in the zero temperature limit (9), lim β→+∞ C β becomes a Dirac distribution over the reward maximising state (bottom). Thus, minimising expected free energy corresponds to selecting the action such that the predicted states assign most mass to the reward maximising state (bottom-right). Q * := Q( s | a * , s t ) denotes the predictive distribution over states given the action sequence that minimises expected free energy a * = arg min a G( a | s t ).</p>
<p>In summary, this scheme selects the first action within action sequences that, on average, maximise their exponentiated negative expected free energies. As a corollary, if the first action is in a sequence with a very low expected free energy, this adds an exponentially large contribution to the selection of this particular action. We summarise this scheme in Table 1.  (Barp et al., 2022).
Process Computation Perceptual inference Q( s | a, s t ) = P ( s | a, s t ) = T −1 τ =t P (s τ +1 | s τ , a τ ) Planning as inference G( a | s t ) = D KL [Q( s | a, s t ) | C( s)] Decision-making Q( a | s t ) ∝ exp(−G( a | s t )) Action selection a t ∈ arg max a∈A [Q(a t = a | s t ) = a Q(a t = a | a)Q( a | s t )]
Theorem 15. In MDPs with known transition probabilities and in the zero temperature limit β → +∞ (9), the scheme of Table 1 a t ∈ lim
β→+∞ arg max a∈A a ( a) t =a exp(−G( a | s t )), G( a | s t ) = D KL <a href="10">Q( s | a, s t ) | C β ( s)</a>
is Bellman optimal for the temporal horizon T = 1.</p>
<p>A proof can be found in Appendix B.5. Importantly, the standard active inference scheme (10) falls short in terms of Bellman optimality on planning horizons greater than one; this rests upon the fact that it does not coincide with backward induction. Recall that backward induction offers a complete description of Bellman optimal state-action policies (Proposition 11). In contrast, active inference plans by adding weighted expected free energies of each possible future course of action. In other words, unlike backward induction, it considers future courses of action beyond the subset that will subsequently minimise expected free energy, given subsequently encountered states.</p>
<p>Reward maximisation on MDPs with finite temporal horizons</p>
<p>To achieve Bellman optimality on finite temporal horizons, we turn to the expected free energy of an action given future actions that also minimise expected free energy. To do this we can write the expected free energy recursively, as the immediate expected free energy, plus the expected free energy that one would obtain by subsequently selecting actions that minimise expected free energy . The resulting scheme consists of minimising an expected free energy defined recursively, from the last time step to the current timestep. In finite horizon MDPs, this reads
G(a T −1 | s T −1 ) = D KL [Q(s T | a T −1 , s T −1 ) | C β (s T )] G(a τ | s τ ) = D KL [Q(s τ +1 | a τ , s τ ) | C β (s τ +1 )] + E Q(aτ+1,sτ+1|aτ ,sτ ) [G(a τ +1 | s τ +1 )], τ = t, . . . , T − 2,
where, at each time-step, actions are chosen to minimise expected free energy
Q(a τ +1 | s τ +1 ) &gt; 0 ⇐⇒ a τ +1 ∈ arg min a∈A G(a | s τ +1 ).(11)
To make sense of this formulation, we unravel the recursion
G(a t | s t ) = D KL [Q(s t+1 | a t , s t ) | C β (s t+1 )] + E Q(at+1,st+1|at,st) [G(a t+1 | s t+1 )] = D KL [Q(s t+1 | a t , s t ) | C β (s t+1 )] + E Q(at+1,st+1|at,st) [D KL [Q(s t+2 | a t+1 , s t+1 ) | C β (s t+2 )]] + E Q(at+1:t+2,st+1:t+2|at,st) [G(a t+2 | s t+2 )] = . . . = E Q( a, s|at,st) T −1 τ =t D KL [Q(s τ +1 | a τ , s τ ) | C β (s τ +1 )] = E Q( a, s|at,st) D KL [Q( s | a, s t ) | C β ( s)],(12)
which shows that this expression is exactly the expected free energy under action a t , if one is to pursue future actions that minimise expected free energy (11). We summarise this 'sophisticated inference' scheme in Table 2.  .
Process Computation Perceptual inference Q(s τ +1 | a τ , s τ ) = P (s τ +1 | a τ , s τ ) Planning as inference G(a τ | s τ ) = D KL [Q(s τ +1 | a τ , s τ ) | C β (s τ +1 )] . . . . . . + E Q(aτ+1,sτ+1|aτ ,sτ ) [G(a τ +1 | s τ +1 )] Decision-making Q(a τ | s τ ) &gt; 0 ⇐⇒ a τ ∈ arg min a∈A G(a | s τ ) Action selection a t ∼ Q(a t | s t )
The crucial improvement over the standard active inference scheme (Table 1) is that planning is now performed based on subsequent counterfactual actions that minimise expected free energy, as opposed to considering all future courses of action. Translating this into the language of state-action policies yields ∀s ∈ S
a T −1 (s) ∈ arg min a∈A G(a | s T −1 = s) a T −2 (s) ∈ arg min a∈A G(a | s T −2 = s) . . . a 1 (s) ∈ arg min a∈A G(a | s 1 = s) a 0 (s) ∈ arg min a∈A G(a | s 0 ).(13)
Equation (13) is strikingly similar to the backward induction algorithm (Proposition 11), and indeed we recover backward induction in the limit β → +∞.</p>
<p>Theorem 16 (Backward induction as active inference). In MDPs with known transition probabilities, and in the zero temperature limit β → +∞ (9), the scheme of Table 2 Q
(a τ | s τ ) &gt; 0 ⇐⇒ a t ∈ lim β→+∞ arg min a∈A G(a | s τ ) G(a τ | s τ ) = D KL [Q(s τ +1 | a τ , s τ ) | C β (s τ +1 )] + E Q(aτ+1,sτ+1|aτ ,sτ ) <a href="14">G(a τ +1 | s τ +1 )</a>
is Bellman optimal on any finite temporal horizon as it coincides with the backward induction algorithm from Proposition 11. Furthermore, if there are multiple actions that maximise future reward, those that are selected by active inference also maximise the entropy of future states H[Q( s | a, a, s 0 )].</p>
<p>Note that maximising the entropy of future states keeps the agent's options open (Klyubin et al., 2008) in the sense of committing the least to a specified sequence of states. A proof of Theorem 16 can be found in Appendix B.6.</p>
<p>Generalisation to POMDPs</p>
<p>Partially observable Markov decision processes (POMDPs) generalise MDPs in that the agent observes a modality o t , which carries incomplete information about the current state s t , as opposed to the current state itself.</p>
<p>Definition 17 (Finite horizon POMDP). A finite horizon POMDP is an MDP (see Definition 1) with the following additional data:</p>
<p>• O a finite set of observations.</p>
<p>• P (o t = o | s t = s) is the probability that the state s ∈ S at time t will lead to the observation o ∈ O at time t. o t are random variables over O that correspond to the observation being sampled at time t = 0, ..., T .</p>
<p>Active inference on finite horizon POMDPs</p>
<p>We briefly introduce active inference agents on finite horizon POMDPs with known transition probabilities (for more details, see (Da Costa et al., 2020;Smith et al., 2022a)). We assume that the agent's generative model of its environment is given by the previously defined POMDP (Definition 17) 4 .</p>
<p>4.</p>
<p>We do not consider the case where the model parameters have to be learned but comment on it in Appendix A.2 (details in (Da Costa et al., 2020;).</p>
<p>Let s := s 0:T , a := a 0:T −1 be all states and actions (past, present, and future), letõ := o 0:t be the observations available up to time t, and o := o t+1:T be the future observations. The agent has a predictive distribution over states given actions
Q( s | a,õ) := T −1 τ =0 Q(s τ +1 | a τ , s τ ,õ).
that is continuously updated following new observations.</p>
<p>Perception as inference</p>
<p>In active inference, perception entails inferences about (past, present, and future) states given observations and a sequence of actions. When states are partially observed, the posterior distribution P ( s | a,õ) is intractable to compute directly. Thus, one approximates it by optimising a variational free energy functional F a (a.k.a. an evidence bound (Beal, 2003;Bishop, 2006;Blei et al., 2017;Wainwright and Jordan, 2007)) over a space of probability distributions Q(· | a,õ) called the variational family
P ( s | a,õ) = arg min Q F a [Q( s | a,õ)] = arg min Q D KL [Q( s | a,õ) | P ( s | a,õ)] F a [Q( s | a,õ)] := E Q( s| a,õ) [log Q( s | a,õ) − log P (õ, s | a)].(15)
Here, P (õ, s | a) is the POMDP, which is supplied to the agent, and P ( s | a,õ). When the free energy minimum (15) is reached, the inference is exact
Q( s | a,õ) = P ( s | a,õ).(16)
For numerical tractability, the variational family may be constrained to a parametric family of distributions, in which case equality is not guaranteed
Q( s | a,õ) ≈ P ( s | a,õ).(17)</p>
<p>Planning as inference</p>
<p>The objective that active inference minimises in order the select the best possible courses of action is the expected free energy (Barp et al., 2022;Da Costa et al., 2020;. In POMDPs, the expected free energy reads (Barp et al., 2022)
G( a |õ) = D KL [Q( s | a,õ) | C β ( s)] Risk + E Q( s| a,õ) H[P ( o | s)] Ambiguity .
The expected free energy on POMDPs is the expected free energy on MDPs plus an extra term called ambiguity. This ambiguity term accommodates the uncertainty implicit in partially observed problems. The reason that this resulting functional is called expected free energy is because it comprises a relative entropy (risk) and expected energy (ambiguity). The expected free energy objective subsumes several decision-making objectives that predominate in statistics, machine learning, and psychology, which confers it with several useful properties when simulating behaviour (see Figure 3 for details).</p>
<p>Maximising reward on POMDPs</p>
<p>Crucially, our reward maximisation results translate to the POMDP case. To make this explicit, we rehearse Lemma 14 in the context of POMDPs. Figure 3: Active inference. The top panels illustrate the perception-action loop in active inference, in terms of minimisation of variational and expected free energy. The lower panels illustrate how expected free energy relates to several descriptions of behaviour that predominate in the psychological, machine learning, and economics. These descriptions are disclosed when one removes particular terms from the objective. For example, if we ignore extrinsic value, we are left with intrinsic value, variously known as expected information gain (Lindley, 1956;MacKay, 2003). This underwrites intrinsic motivation in machine learning and robotics (Barto et al., 2013;Deci and Ryan, 1985;Oudeyer and Kaplan, 2007) and expected Bayesian surprise in visual search (Itti and Baldi, 2009;Sun et al., 2011) and the organisation of our visual apparatus (Barlow, 1961(Barlow, , 1974Linsker, 1990;Optican and Richmond, 1987). In the absence of ambiguity, we are left with minimising risk, which corresponds to aligning predicted states to preferred states. This leads to risk averse decisions in behavioural economics (Kahneman and Tversky, 1979) and formulations of control as inference in engineering such as KL control (van den Broek et al., 2010). If we then remove intrinsic value, we are left with expected utility in economics (Von Neumann and Morgenstern, 1944) that underwrites RL and behavioural psychology (Barto and Sutton, 1992). Bayesian formulations of maximising expected utility under uncertainty are also the basis of Bayesian decision theory (Berger, 1985). Finally, if we only consider a fully observed environment with no preferences, minimising expected free energy corresponds to a maximum entropy principle over future states (Jaynes, 1957a,b). Note that here C(o) denotes the preferences over observations derived from the preferences over states. These are related by P (o | s)C(s) = P (s | o)C(o).</p>
<p>Proposition 18 (Reward maximisation on POMDPs). In POMDPs with known transition probabilities, provided that the free energy minimum is reached (16), the sequence of actions that minimises expected free energy also maximises expected reward in the zero temperature limit β → +∞ (9):
lim β→+∞ arg min a G( a |õ) ⊆ arg max a E Q( s| a,õ) [R( s)].
Furthermore, of those action sequences that maximise expected reward, the expected free energy minimisers will be those that maximise the entropy of future states minus the (expected) entropy of outcomes given states H
[Q( s | a,õ)] − E Q( s|at,õ) H[P ( o | s)]].
From Proposition 18 we see that if there are multiple maximise reward action sequences, those that are selected maximise
H[Q( s | a,õ)] Entropy of future states − E Q( s|at,õ) [H[P ( o | s)]]
Entropy of observations given future states .</p>
<p>In other words, they least commit to a prespecified sequence of future states and ensure that their expected observations are maximally informative of states. Of course, when inferences are inexact, the extent to which Proposition 18 holds depends upon the accuracy of the approximation (17). A proof of Proposition 18 can be found in Appendix B.7.</p>
<p>The schemes of Table 1 &amp; 2 exist in the POMDP setting, (e.g., (Barp et al., 2022) and (Friston et al., 2021), respectively). Thus, in POMDPs with known transition probabilities, provided that inferences are exact (16) and in the zero temperature limit β → +∞ (9), standard active inference (Barp et al., 2022) maximises reward on temporal horizons of 1 but not beyond, and a recursive scheme such as sophisticated active inference  maximises reward on finite temporal horizons. Note that, for computational tractability, the sophisticated active inference scheme presented in  does not generally perform exact inference; thus, the extent to which it will maximise reward in practice will depend upon the accuracy of its inferences. Nevertheless, our results indicate that sophisticated active inference will vastly outperform standard active inference in most reward maximisation tasks.</p>
<p>Discussion</p>
<p>In this paper, we have examined a specific notion of optimality; namely, Bellman optimality; defined as selecting actions to maximise future expected rewards. We demonstrated how and when active inference is Bellman optimal on finite horizon POMDPs with known transition probabilities and reward function.</p>
<p>These results highlight important relationships between active inference, stochastic control, and RL, as well as conditions under which they would and would not be expected to behave similarly (e.g., environments with multiple reward-maximising trajectories, those affording ambiguous observations, etc.). We refer the reader to Appendix A for a broader discussion of the relationship between active inference and reinforcement learning.</p>
<p>Decision-making beyond reward maximisation</p>
<p>More broadly, it is important to ask if reward maximisation is the right objective underwriting intelligent decision-making? This is an important question for decision neuroscience. That is, do humans optimise a reward signal, expected free energy, or other planning objectives. This can be addressed by comparing the evidence for these competing hypotheses based on empirical data (e.g., see (Smith et al., 2020d(Smith et al., , 2021b(Smith et al., ,c, 2022b). Current empirical evidence suggests that humans are not purely reward-maximising agents: they also engage in both random and directed exploration (Daw et al., 2006;Gershman, 2018;Mirza et al., 2018;Schulz and Gershman, 2019;Wilson et al., 2014Wilson et al., , 2021Xu et al., 2021) and keep their options open (Schwartenbeck et al., 2015b). As we have illustrated, active inference implements a clear form of directed exploration through minimising expected free energy. Although not covered in detail here, active inference can also accommodate random exploration by sampling actions from the posterior belief over action sequences, as opposed to selecting the most likely action as presented in Tables 1 and 2.</p>
<p>Note that behavioural evidence favouring models that do not solely maximise reward within reward maximisation tasks-i.e., where "maximise reward" is the explicit instruction-is not a contradiction. Rather, gathering information about the environment (exploration) generally helps to reap more reward in the long run, as opposed to greedily maximising reward based on imperfect knowledge (Cullen et al., 2018;Sajid et al., 2021a). This observation is not new and many approaches to simulating adaptive agents employed today differ significantly from their reward maximising antecedents (Appendix A.3).</p>
<p>Learning</p>
<p>When the transition probabilities or reward function are unknown to the agent, the problem becomes one of reinforcement learning (RL) (Shoham et al., 2003) as opposed to stochastic control. Although we did not explicitly consider it above, this scenario can be accommodated by active inference by simply equipping the generative model with a prior, and updating the model via variational Bayesian inference to best fit observed data. Depending on the specific learning problem and generative model structure, this can involve updating the transition probabilities and/or the target distribution C. In POMDPs it can also involve updating the probabilities of observations under each state. We refer to Appendix A.2 for discussion of reward learning through active inference and connections to representative RL approaches, and (Da Costa et al., 2020; for learning transition probabilities through active inference.</p>
<p>Scaling active inference</p>
<p>When comparing RL and active inference approaches generally, one outstanding issue for active inference is whether it can be scaled up to solve the more complex problems currently handled by RL in machine learning contexts (Çatal et al., 2020, 2021Fountas et al., 2020;Mazzaglia et al., 2021;Millidge, 2020;Tschantz et al., 2019). This is an area of active research.</p>
<p>One important issue along these lines is that planning ahead by evaluating all or many possible sequences of actions is computationally prohibitive in many applications. Three complementary solutions that have emerged are: 1) employing hierarchical generative models that factorise decisions into multiple levels and reduce the size of the decision tree by orders of magnitude (Çatal et al., 2021;, 2) efficiently searching the decision tree using algorithms like Monte Carlo tree search (Champion et al., 2021a,b;Fountas et al., 2020;Maisto et al., 2021;Silver et al., 2016), and 3) amortising planning using artificial neural networks (Çatal et al., 2020;Fountas et al., 2020;Sajid et al., 2021b).</p>
<p>Another issue rests upon learning the generative model. Active inference may readily learn the parameters of a generative model; however, more work needs to be done on devising algorithms for learning the structure of generative models themselves (Friston et al., 2017b;Smith et al., 2020c). This is an important research problem in generative modelling, called Bayesian model selection or structure learning (Gershman and Niv, 2010;Tervo et al., 2016).</p>
<p>Note that these issues are not unique to active inference. Model-based RL algorithms deal with the same combinatorial explosion when evaluating decision trees, which is one primary motivation for developing efficient model-free RL algorithms. However, other heuristics have also been developed for efficiently searching and pruning decision trees in model-based RL, e.g., (Huys et al., 2012;Lally et al., 2017). Furthermore, model-based RL suffers the same limitation regarding learning generative model structure. Yet, RL may have much to offer active inference in terms of efficient implementation and the identification of methods to scale to more complex applications (Fountas et al., 2020;Mazzaglia et al., 2021).</p>
<p>Conclusion</p>
<p>In summary, we have shown that under the specification that the active inference agent prefers maximising reward (9): 1. On finite horizon POMDPs with known transition probabilities, the objective optimised for action selection in active inference (i.e., expected free energy) produces reward maximising action sequences when state-estimation is exact. When there are multiple reward maximising candidates, this selects those sequences that maximise the entropy of future states-thereby keeping options open-and that minimise the ambiguity of future observations so that they are are maximally informative. More generally, the extent to which action sequences will be reward maximising will depend on the accuracy of state-estimation.</p>
<ol>
<li>
<p>The standard active inference scheme (e.g., (Barp et al., 2022)) produces Bellman optimal actions for planning horizons of 1 when state-estimation is exact, but not beyond.</p>
</li>
<li>
<p>A sophisticated active inference scheme (e.g., ) produces Bellman optimal actions on any finite planning horizon when state-estimation is exact. Furthermore, this scheme generalises the well-known backward induction algorithm from dynamic programming to partially observed environments. Note that, for computational efficiency, the scheme presented in  does not generally perform exact state-estimation; thus, the extent to which it will maximise reward in practice will depend upon the accuracy of its inferences. Nevertheless, it is clear from our results that sophisticated active inference will vastly outperform standard active inference in most reward maximisation tasks.</p>
</li>
</ol>
<p>Note that, for computational tractability, the sophisticated active inference scheme presented in  does not generally perform exact inference; thus, the extent to which it will maximise reward in practice will depend upon the accuracy of its inferences. Nevertheless, it is clear from these results that sophisticated active inference will vastly outperform standard active inference in most reward maximisation tasks.</p>
<p>Author contributions</p>
<p>LD: conceptualisation, proofs, writing -first draft, review and editing. NS, TP, KF, RS: conceptualisation, writing -review and editing. strengths and limitations. This makes RL difficult to characterise as a whole. Thankfully, many approaches to model-based RL and control can be traced back to approximating the optimal solution to the Bellman equation (Bellman and Dreyfus, 2015;Bertsekas and Shreve, 1996) (although this may become computationally intractable in high-dimensions (Barto and Sutton, 1992)). Our results showed how and when decisions under active inference and such RL approaches are similar.</p>
<p>This appendix discusses how active inference and RL relate and differ more generally. Their relationship has become increasingly important to understand, as a growing body of research has begun to 1) compare the performance of active inference and RL models in simulated environments (Cullen et al., 2018;Millidge, 2020;Sajid et al., 2021a), 2) apply active inference to model human behaviour on reward learning tasks (Smith et al., 2020d(Smith et al., , 2021b(Smith et al., ,c, 2022b, and 3) consider the complementary predictions and interpretations they each offer in computational neuroscience, psychology, and psychiatry (Cullen et al., 2018;Huys et al., 2012;Schwartenbeck et al., 2015a.</p>
<p>A.1 Main differences between active inference and reinforcement learning</p>
<p>Philosophy. Active inference and RL differ profoundly in their philosophy. RL derives from the normative principle of maximising reward (Barto and Sutton, 1992), while active inference describes systems that maintain their structural integrity over time (Barp et al., 2022;. Despite this difference, there are many practical similarities between these frameworks. For example, recall that behaviour in active inference is completely determined by the agent's preferences, determined as priors in their generative model. Crucially, log priors can be interpreted as reward functions and vice-versa, which is how behaviour under RL and active inference can be related.</p>
<p>Model based and model free. Active inference agents always embody a generative (i.e., forward) model of their environment, while RL comprises both model-based algorithms and simpler modelfree algorithms. In brief, 'model-free' means that agents learn a reward-maximising state-action mapping, based on updating cached state-action pair values, through initially random actions that do not consider future state transitions. In contrast, model-based RL algorithms attempt to extend stochastic control approaches by learning the dynamics and reward function from data. Recall that stochastic control calls on strategies that evaluate different actions on a carefully handcrafted forward model of dynamics (i.e., known transition probabilities) to finally execute the reward-maximising action. Under this terminology, all active inference agents are model-based.</p>
<p>Modelling exploration. Exploratory behaviour-which can improve reward maximisation in the long run-is implemented differently in the two approaches. In most cases, RL implements a simple form of exploration by incorporating randomness in decision-making (Tokic and Palm, 2011;Wilson et al., 2014), where the level of randomness may or may not change over time as a function of uncertainty. In other cases, RL incorporates ad-hoc information bonuses in the reward function or other decision-making objectives to build in directed exploratory drives (e.g., upper confidence bound algorithms or Thompson sampling). In contrast, directed exploration emerges naturally within active inference through interactions between the risk and ambiguity terms in the expected free energy (Da Costa et al., 2020;. This addresses the explore-exploit dilemma and confers the agent with artificial curiosity (Friston et al., 2017b;Schmidhuber, 2010;Still and Precup, 2012), as opposed to the need to add ad-hoc information bonus terms (Tokic and Palm, 2011). We expand on this relationship further in Appendix A.3.</p>
<p>Control and learning as inference. Active inference integrates state-estimation, learning, decisionmaking, and motor control under the single objective of minimising free energy (Da Costa et al., 2020). In fact, active inference extends previous work on the duality between inference and control (Kappen et al., 2012;Rawlik et al., 2013;Todorov, 2008;Toussaint, 2009) to solve motor control problems via approximate inference (i.e., planning as inference) (Attias, 2003;Botvinick and Toussaint, 2012;Friston et al., 2012Friston et al., , 2009Millidge et al., 2020b). Therefore, some of the closest RL methods to active inference are control as inference, also known as maximum entropy RL (Levine, 2018b;Millidge et al., 2020b;Ziebart, 2010), though one major difference is in the choice of decisionmaking objective. Loosely speaking, these aforementioned methods minimise the risk term of the expected free energy, while active inference also minimises ambiguity.</p>
<p>Useful features of active inference.</p>
<ol>
<li>
<p>Active inference allows great flexibility and transparency when modelling behaviour. It affords explainable decision-making as a mixture of information-and reward-seeking policies that are explicitly encoded (and evaluated in terms of expected free energy) in the generative model as priors, which are specified by the user (Da Costa et al., 2022a). As we have seen, the kind of behaviour that can be produced includes the optimal solution to the Bellman equation.</p>
</li>
<li>
<p>Active inference accommodates deep hierarchical generative models combining both discrete and continuous state-spaces (Friston et al., 2017c.</p>
</li>
<li>
<p>The expected free energy objective optimised during planning subsumes many approaches used to describe and simulate decision-making in the physical, engineering, and life sciences, affording it various interesting properties as an objective (Figure 3 and ). For example, exploratory and exploitative behaviour are canonically integrated, which finesses the need for manually incorporating ad-hoc exploration bonuses in the reward function (Da Costa et al., 2022b).</p>
</li>
<li>
<p>Active inference goes beyond state-action policies that predominate in traditional RL to sequential policy optimisation. In sequential policy optimisation, one relaxes the assumption that the same action is optimal given a particular state-and acknowledges that the sequential order of actions may matter. This is similar to the linearly-solvable MDP formulation presented by (Todorov, 2007(Todorov, , 2009, where transition probabilities directly determine actions, and an optimal policy specifies transitions that minimise some divergence cost. This way of approaching policies is perhaps most apparent in terms of exploration. Put simply, it is clearly better to explore and then exploit than the converse. Because expected free energy is a functional of beliefs, exploration becomes an integral part of decision-making-in contrast with traditional RL approaches that try to optimise a reward function of states. In other words, active inference agents will explore until enough uncertainty is resolved for reward maximising, goal-seeking imperatives to start to predominate.</p>
</li>
</ol>
<p>Such advantages should motivate future research to better characterise the environments in which these properties offer useful advantages-such as where performance benefits from learning and planning at multiple temporal scales, and from the ability to select policies that resolve both state and parameter uncertainty.</p>
<p>A.2 Reward learning</p>
<p>Given the focus on relating active inference to the objective of maximising reward, it is worth briefly illustrating how active inference can learn the reward function from data and its potential connections to representative RL approaches. One common approach for active inference to learn a reward function (Smith et al., 2020d(Smith et al., , 2022b is to set preferences over observations rather than states, which corresponds to assuming that inferences over states given outcomes are accurate
D KL [Q ( s | a,õ) | C ( s)] Risk (states) = D KL [Q ( o | a,õ) | C ( o)] Risk (outcomes) + E Q( o| a,õ) [D KL [Q ( s | o,õ, a) | P ( s | o)]] ≈0 ≈ D KL [Q ( o | a,õ) | C ( o)] Risk (outcomes) ,
i.e., equality holds whenever the free energy minimum is reached (16). Then one sets the preference distribution such that the observations designated as rewards are most preferred. In the zero temperature limit (9), preferences only assign mass to reward-maximising observations. When formulated in this way, the reward signal is treated as sensory data, as opposed to a separate signal from the environment. When one sets allowable actions (controllable state transitions) to be fully deterministic such that the selection of each action will transition the agent to a given state with certainty, the emerging dynamics are such that the agent chooses actions to resolve uncertainty about the probability of observing reward under each state. Thus, learning the reward probabilities of available actions amounts to learning the likelihood matrix P ( o | s) := o t · As t , where A is a stochastic matrix. This is done by setting a prior a over A, i.e., a matrix of non-negative components, the columns of which are Dirichlet priors over the columns of A. The agent then learns by accumulating Dirichlet parameters. Explicitly, at the end of a trial or episode, one sets (Da Costa et al., 2020; 
a ← a + T τ =0 o τ ⊗ Q(s τ | o 0:T )(18)
In (18), Q(s τ | o 0:T ) is seen as a vector of probabilities over the state-space S, corresponding to the probability of having been in one or another state at time the τ after having gathered observations throughout the trial. This rule simply amounts to counting observed state-outcome pairs, which is equivalent to state-reward pairs when the observation modalities correspond to reward. One should not conflate this approach with the update rule consisting of accumulating stateobservation counts in the likelihood matrix
A ← A + T τ =0 o τ ⊗ Q(s τ | o 0:T )(19)
and then normalising its columns to sum to one when computing probabilities. The latter simply approximates the likelihood matrix A by accumulating the number of observed state-outcome pairs. This is distinct from the approach outlined above, which encodes uncertainty over the matrix A, as a probability distribution over possible distributions P (o t | s t ). The agent is initially very unconfident about A, which means that it doesn't place high probability mass on any specification of P (o t | s t ). This uncertainty is gradually resolved by observing state-observation (or state-reward) pairs. Computationally, it is a general fact of Dirichlet priors that an increase in elements of a causes the entropy of P (o t | s t ) to decrease. As the terms added in (18) are always positive, one choice of distribution P (o t | s t )-which best matches available data and prior beliefs-is ultimately singled out. In other words, the likelihood mapping is learned. The update rule consisting of accumulating state-observation counts in the likelihood matrix (19) (i.e., not incorporating Dirichlet priors) bears some similarity to off-policy learning algorithms such as Q-learning. In Q-learning, the objective is to find the best action given the current observed state. For this, the Q-learning agent accumulates values for state-action pairs with repeated observation of rewarding/punishing action outcomes-much like state-observation counts. This allows it to learn the Q-value function that defines a reward maximising policy.</p>
<p>As always in partially observed environments, we cannot guarantee that the true likelihood mapping will be learned in practice. Please see (Smith et al., 2019) for examples where, although not in an explicit reward-learning context, learning the likelihood can be more or less successful in different situations. Learning the true likelihood fails when the inference over states is inaccurate, such as when using too severe of a mean-field approximation to the free energy (Blei et al., 2017;Tanaka, 1999), which causes the agent to misinfer states and thereby accumulate Dirichlet parameters in the wrong locations. Intuitively, this amounts to jumping to conclusions too quickly.</p>
<p>Remark 19. If so desired, reward learning in active inference can also be equivalently formulated as learning transition probabilities P (s t+1 | s t , a t ). In this alternative setup (as exemplified in (Sales et al., 2019)), mappings between reward states and reward outcomes in A are set as identity matrices, and the agent instead learns the probability of transitioning to states that deterministically generate preferred (rewarding) observations given the choice of each action sequence. The transition probabilities under each action are learned in a similar fashion as above (18), by accumulating counts on a Dirichlet prior over P (s t+1 | s t , a t ). See (Da Costa et al., 2020, Appendix) for details.</p>
<p>Given the model-based Bayesian formulation of active inference, more direct links can be made between the active inference approach to reward learning described above and other Bayesian modelbased RL approaches. For such links to be realised, the Bayesian RL agent would be required to have a prior over a prior (e.g., a prior over the reward function prior or transition function prior). One way to implicitly incorporate this is through Thompson sampling (Ghavamzadeh et al., 2016;Van Roy, 2014, 2016;Russo et al., 2017). While not the focus of this paper, future work could further examine the links between reward learning in active inference and model-based Bayesian RL schemes.</p>
<p>A.3 Solving the exploration-exploitation dilemma</p>
<p>An important distinction between active inference and reinforcement learning schemes is how they solve the exploration-exploitation dilemma.</p>
<p>The exploration-exploitation dilemma (Berger-Tal et al., 2014) arises whenever an agent has incomplete information about its environment, such as when the environment is partially observed, or the generative model has to be learned. The dilemma is then about deciding whether to execute actions aiming to collect reward based on imperfect information about the environment, or to execute actions aiming to gather more information-allowing the agent to reap more reward in the future. Intuitively, it is always best to explore and then exploit, but optimising this trade-off can be difficult.</p>
<p>Active inference balances exploration and exploitation through minimising the risk and ambiguity inherent in the minimisation of expected free energy. This balance is context-sensitive and can be adjusted by modifying the agent's preferences (Da Costa et al., 2022a). In turn, the expected free energy is obtained from a description of agency in biological systems derived from physics (Barp et al., 2022;.</p>
<p>Modern RL algorithms integrate exploratory and exploitative behaviour in many different ways. One option is curiosity-driven rewards to encourage exploration. Maximum entropy RL and controlas-inference make decisions by minimising a KL divergence to the target distribution (Eysenbach and Levine, 2019;Haarnoja et al., 2017Haarnoja et al., , 2018Levine, 2018a;Todorov, 2008;Ziebart et al., 2008), which combines reward maximisation with maximum entropy over states. This is similar to active inference on MDPs (Millidge et al., 2020b). Similarly, the model-free Soft Actor-Critic (Haarnoja et al., 2018) algorithm maximises both expected reward and entropy. This outperforms other stateof-the-art algorithms in continuous control environments and has been shown to be more sample efficient than its reward-maximising counterparts (Haarnoja et al., 2018). Hyper Zintgraf et al. (2021) proposes reward maximisation alongside minimising uncertainty over both external states and model parameters. Bayes-adaptive RL (Guez et al., 2013a,b;Ross et al., 2008;Zintgraf et al., 2020) provides policies that balance exploration and exploitation with the aim of maximising reward. Thompson sampling provides a way to balance exploiting current knowledge to maximise immediate performance and accumulating new information to improve future performance (Russo et al., 2017). This reduces to optimising dual objectives, reward maximisation and information gain, similar to active inference on POMDPs. Empirically, Sajid et al. (2021a) demonstrated that an active inference agent and a Bayesian model-based RL agent using Thompson sampling exhibit similar behaviour when preferences are defined over outcomes. They also highlighted that, when completely removing the reward signal from the environment, the two agents both select policies that maximise some sort of information gain.</p>
<p>In general, the way each of these approaches to the exploration-exploitation dilemma differ in theory and in practice remains largely unexplored.</p>
<p>Appendix B. Proofs</p>
<p>B.1 Proof of Proposition 9</p>
<p>Note that a Bellman optimal state-action policy Π * is a maximal element according to the partial ordering ≤. Existence thus consists of a simple application of Zorn's lemma. Zorn's lemma states that if any increasing chain
Π 1 ≤ Π 2 ≤ Π 3 ≤ . . .(20)
has an upper bound that is a state-action policy, then there is a maximal element Π * . Given the chain (20), we construct an upper bound. We enumerate A × S × T by (α 1 , σ 1 , t 1 ), . . . , (α N , σ N , t N ). Then the state-action policy sequence Π n (α 1 | σ 1 , t 1 ), n = 1, 2, 3, . . . is bounded within [0, 1]. By the Bolzano-Weierstrass theorem, there exists a subsequence Π n k (α 1 | σ 1 , t 1 ), k = 1, 2, 3, . . . that converges. Similarly, Π n k (α 2 | σ 2 , t 2 ) is also a bounded sequence, and by Bolzano-Weierstrass it has a subsequence Π n k j (a 2 | σ 2 , t 2 ) that converges. We repeatedly take subsequences until N . To ease notation, call the resulting subsequence Π m , m = 1, 2, 3, . . .</p>
<p>With this, we defineΠ = lim m→∞ Π m . It is straightforward to see thatΠ is a state-action policy:
Π(α | σ, t) = lim m→∞ Π m (α | σ, t) ∈ [0, 1], ∀(α, σ, t) ∈ A × S × T, α∈AΠ (α | σ, t) = lim m→∞ α∈A Π m (α | σ, t) = 1, ∀(σ, t) ∈ S × T.
To show thatΠ is an upper bound, take any Π in the original chain of state-action policies (20). Then by the definition of an increasing subsequence, there exists an index M ∈ N such that ∀k ≥ M : Π k ≥ Π. Since limits commute with finite sums, we have vΠ(s, t) = lim m→∞ v Πm (s, t) ≥ v Π k (s, t) ≥ v Π (s, t) for any (s, t) ∈ S × T. Thus, by Zorn's lemma there exists a Bellman optimal state-action policy Π * .</p>
<p>B.2 Proof of Proposition 10 1) ⇒ 2) : We only need to show assertion (b). By contradiction, suppose that ∃(s, α) ∈ S × A such that Π(α | s, 0) &gt; 0 and
E Π [R(s 1:T ) | s 0 = s, a 0 = α] &lt; max a∈A E Π [R(s 1:T ) | s 0 = s, a 0 = a].
We let α be the Bellman optimal action at state s and time 0 defined as
α := arg max a∈A E Π [R(s 1:T ) | s 0 = s, a 0 = a].
Then, we let Π be the same state-action policy as Π except that Π (· | s, 0) assigns α deterministically. Then, = v Π (s, 0). So Π is not Bellman optimal, which is a contradiction.</p>
<p>1) ⇐ 2) : We only need to show that Π maximises v Π (s, 0), ∀s ∈ S. By contradiction, there exists a state-action policy Π and a state s ∈ S such that v Π (s, 0) &lt; v Π (s, 0)
⇐⇒ a∈A E Π [R(s 1:T ) | s 0 = s, a 0 = a]Π(a | s, 0) &lt; a∈A E Π [R(s 1:T ) | s 0 = s, a 0 = a]Π (a | s, 0).
By (a) the left hand side equals
max a∈A E Π [R(s 1:T ) | s 0 = s, a 0 = a].
Unpacking the expression on the right-hand side:
a∈A E Π [R(s 1:T ) | s 0 = s, a 0 = a]Π (a | s, 0) = a∈A σ∈S E Π [R(s 1:T ) | s 1 = σ]P (s 1 = σ | s 0 = s, a 0 = a)Π (a | s, 0) = a∈A σ∈S {E Π [R(s 2:T ) | s 1 = σ] + R(σ)} P (s 1 = σ | s 0 = s, a 0 = a)Π (a | s, 0) = a∈A σ∈S {v Π (σ, 1) + R(σ)] P (s 1 = σ | s 0 = s, a 0 = a)Π (a | s, 0)(21)
Since Π is Bellman optimal when restricted to {1, . . . , T } we have v Π (σ, 1) ≤ v Π (σ, 1), ∀σ ∈ S. Therefore, which is a contradiction.</p>
<p>B.3 Proof of Proposition 11</p>
<p>• We first prove that state-action policies Π defined as in (2) are Bellman optimal by induction on T .</p>
<p>T = 1 : Π(a | s, 0) &gt; 0 ⇐⇒ a ∈ arg max a E[R(s 1 ) | s 0 = s, a 0 = a], ∀s ∈ S is a Bellman optimal state-action policy as it maximises the total reward possible in the MDP.</p>
<p>Let T &gt; 1 be finite and suppose that the Proposition holds for MDPs with a temporal horizon of T − 1. This means that Π(a | s, T − 1) &gt; 0 ⇐⇒ a ∈ arg max a E[R(s T ) | s T −1 = s, a T −1 = a], ∀s ∈ S Π(a | s, T − 2) &gt; 0 ⇐⇒ a ∈ arg max a E Π [R(s T −1:T ) | s T −2 = s, a T −2 = a], ∀s ∈ S . . .</p>
<p>Π(a | s, 1) &gt; 0 ⇐⇒ a ∈ arg max a E Π [R(s 2:T ) | s 1 = s, a 1 = a], ∀s ∈ S is a Bellman optimal state-action policy on the MDP restricted to times 1 to T . Therefore, since Π(a | s, 0) &gt; 0 ⇐⇒ a ∈ arg max a E Π [R(s 1:T ) | s 0 = s, a 0 = a], ∀s ∈ S Proposition 10 allows us to deduce that Π is Bellman optimal.</p>
<p>• We now show that any Bellman optimal state-action policy satisfies the backward induction algorithm (2).</p>
<p>Suppose by contradiction that there exists a state-action policy Π that is Bellman optimal but does not satisfy (2). Say, ∃(a, s, t) ∈ A × S × T, t &lt; T , such that Π(a | s, t) &gt; 0 and a / ∈ arg max α∈A E Π [R(s t+1:T ) | s t = s, a t = α].</p>
<p>This implies Letã ∈ arg max α E Π [R(s t+1:T ) | s t = s, a t = α]. LetΠ be a state-action policy such that Π(· | s, t) assignsã ∈ A deterministically, and such thatΠ = Π otherwise. Then we can contradict the Bellman optimality of Π as follows The inclusion follows from the fact that, as β → +∞, a minimiser of the expected free energy has to maximise E Q( s| a,st) [R( s)]. Among such action sequences, the expected free energy minimisers are those that maximise the entropy of future states H[Q( s | a, s t )].</p>
<p>B.5 Proof of Theorem 15</p>
<p>When T = 1 the only action is a 0 . We fix an arbitrary initial state s 0 = s ∈ S. By Proposition 10, a Bellman optimal state-action policy is fully characterised by an action a * 0 that maximises immediate reward a * 0 ∈ arg max a∈A E[R(s 1 ) | s 0 = s, a 0 = a].</p>
<p>Recall that by Remark 6, this expectation stands for return under the transition probabilities of the MDP a * 0 ∈ arg max a∈A E P (s1|a0=a,s0=s) [R(s 1 )].</p>
<p>Since transition probabilities are assumed to be known ( which concludes the proof.</p>
<p>B.6 Proof of Theorem 16</p>
<p>We prove this result by induction on the temporal horizon T of the MDP. The proof of the Theorem when T = 1 can be seen from the proof of Theorem 15. Now suppose that T &gt; 1 is finite and that the Theorem holds for MDPs with a temporal horizon of T − 1.</p>
<p>Our induction hypothesis says that Q(a τ | s τ ), as defined in (14), is a Bellman optimal stateaction policy on the MDP restricted to times τ = 1, . . . , T . Therefore, by Proposition 10, we only need to show that the action a 0 selected under active inference satisfies  (12)).</p>
<p>Therefore, an action a 0 selected under active inference is a Bellman optimal state-action policy on finite temporal horizons. Furthermore, the inclusion follows from the fact that if there are multiple actions that maximise expected reward, that which is selected under active inference maximises the entropy of beliefs about future states.</p>
<p>B.7 Proof of Proposition 18</p>
<p>Unpacking the zero temperature limit The inclusion follows from the fact that as β → +∞ a minimiser of the expected free energy has first and foremost to maximise E Q( s| a,õ) [R( s)]. Among such action sequences, the expected free energy minimisers are those that maximise the entropy of (beliefs about) future states H[Q( s | a,õ)] and resolve ambiguity about future outcomes by minimising E Q( s| a,õ) H[P ( o | s)].</p>
<p>E
Π [R(s 1:T ) | s 0 = s, a 0 = a]Π(a | s, 0) &lt; max a∈A E Π [R(s 1:T ) | s 0 = s, a 0 = a] = E Π [R(s 1:T ) | s 0 = s, a 0 = α ]Π (α | s, 0) = a∈A E Π [R(s 1:T ) | s 0 = s, a 0 = a]Π (a | s, 0)</p>
<p>a∈A σ∈S {v Π (σ, 1) + R(σ)] P (s 1 = σ | s 0 = s, a 0 = a)Π (a | s, 0) ≤ a∈A σ∈S {v Π (σ, 1) + R(σ)] P (s 1 = σ | s 0 = s, a 0 = a)Π (a | s, 0). Repeating the steps above (21), but in reverse order, yields a∈A E Π [R(s 1:T ) | s 0 = s, a 0 = a]Π (a | s, 0) ≤ a∈A E Π [R(s 1:T ) | s 0 = s, a 0 = a]Π (a | s, 0) However, a∈A E Π [R(s 1:T ) | s 0 = s, a 0 = a]Π (a | s, 0) &lt; max a∈A E Π [R(s 1:T ) | s 0 = s, a 0 = a]</p>
<p>E
Π [R(s t+1:T ) | s t = s, a t = a] &lt; max α∈A E Π [R(s t+1:T ) | s t = s, a t = α].</p>
<p>vD−−E
Π (s, t) = E Π [R(s t+1:T ) | s t = s] = α∈A E Π [R(s t+1:T ) | s t = s, a t = α]Π(α | s, t) &lt; max α∈A E Π [R(s t+1:T ) | s t = s, a t = α] = E Π [R(s t+1:T ) | s t = s, a t =ã] = EΠ[R(s t+1:T ) | s t = s, a t =ã] = α∈A EΠ[R(s t+1:T ) | s t = s, a t = α]Π(α | s, t) KL [Q( s | a, s t ) | C β ( H[Q( s | a, s t )] + E Q( s| a,st) [− log C β ( H[Q( s | a, s t )] − βE Q( s| a,st) [R( s | a, s t )] + βE Q( s| a,st) [R( Q( s| a,st) [R( s)]</p>
<p>E
Q(s1|a0=a,s0=s) [R(s 1 )],</p>
<p>EEEE
a 0 ∈ arg max a∈A E Q [R( s) | s 0 , a 0 = a]. P ( s|a 1:T ,a0=a,s0)Q( a|s 1:T ) [R( s)] (by Remark 6) = arg max a∈A E Q( s, a|a0=a,s0) [R( s)] (as the transitions are known) Q( s, a|a0=a,s0) [βR( s)] − H[Q( s | a, a 0 = a, s Q( s, a|a0=a,s0) [− log C β ( s)] − H[Q( s | a, a 0 = a, s 0 )] Q( s, a|a0=a,s0) D KL [Q( s | a, a 0 = a, s 0 ) | C β ( 0 = a | s 0 ) (by</p>
<p>D−−
KL [Q( s | a,õ) | C β ( s)] + E Q( s| a,õ) H[P ( o | s)] H[Q( s | a,õ)] + E Q( s| a,õ) [− log C β ( s)] + E Q(s| a,õ) H[P ( o | s)] H[Q( s | a,õ)] − βE Q( s| a,õ) [R( s)] + E Q( s| a,õ) H[P ( o | s)]</p>
<p>Table 1 :
1Standard active inference scheme on finite horizon MDPs</p>
<p>Table 2 :
2Sophisticated active inference scheme on finite horizon MDPs
. The surprise (a.k.a. self information or surprisal) of states − log C( s) is information theoretic nomenclature(Stone, 2015) that scores the extent to which an observation is unusual under C. It does not imply that the agent experiences surprise in a subjective or declarative sense.
. Note the connection with statistical mechanics: β is an inverse temperature parameter, −R is a potential function and C β is the corresponding Gibbs distribution (Pavliotis, 2014;Rahme and Adams, 2019).
AcknowledgementsThe authors thank Dimitrije Markovic and Quentin Huys for providing helpful feedback during the preparation of the manuscript.Funding informationAppendix A. Active inference and reinforcement learningThis paper considered how active inference can solve the stochastic control problem. In this appendix, we discuss the broader relationship between active inference and RL.Loosely speaking, RL is the field of methodologies and algorithms that learn reward-maximising actions from data and seek to maximise reward in the long run. Because RL is a data-driven field, algorithms are selected based on how well they perform on benchmark problems. This has produced a plethora of diverse algorithms, many designed to solve specific problems, each with their own
The Computational Anatomy of Psychosis. Rick A Adams, Klaas Enno Stephan, Harriet R Brown, Christopher D Frith, Karl J Friston, 10.3389/fpsyt.2013.00047Frontiers in Psychiatry. 4Rick A. Adams, Klaas Enno Stephan, Harriet R. Brown, Christopher D. Frith, and Karl J. Friston. The Computational Anatomy of Psychosis. Frontiers in Psychiatry, 4, 2013. ISSN 1664-0640. doi: 10.3389/fpsyt.2013.00047.</p>
<p>Dynamic Economics Quantitative Methods and Applications. Jerome Adda, Russell W Cooper, MIT PressJerome Adda and Russell W. Cooper. Dynamic Economics Quantitative Methods and Applications. MIT Press, 2003.</p>
<p>Planning by Probabilistic Inference. Hagai Attias, 9th Int. Workshop on Artificial Intelligence and Statistics. 8Hagai Attias. Planning by Probabilistic Inference. In 9th Int. Workshop on Artificial Intelligence and Statistics, page 8, 2003.</p>
<p>Possible Principles Underlying the Transformations of Sensory Messages. H B Barlow, 978-0-262-31421-3The MIT PressH. B. Barlow. Possible Principles Underlying the Transformations of Sensory Messages. The MIT Press, 1961. ISBN 978-0-262-31421-3.</p>
<p>Inductive Inference, Coding, Perception, and Language. H B Barlow, 10.1068/p030123Perception. 32H B Barlow. Inductive Inference, Coding, Perception, and Language. Perception, 3(2):123-134, June 1974. ISSN 0301-0066. doi: 10.1068/p030123.</p>
<p>Geometric Methods for Sampling, Optimisation, Inference and Adaptive Agents. Alessandro Barp, Lancelot Da Costa, Guilherme França, Karl Friston, Mark Girolami, Michael I Jordan, Grigorios A Pavliotis, 978-0-323-91345-4Geometry and Statistics, number 46 in Handbook of Statistics. Academic PressAlessandro Barp, Lancelot Da Costa, Guilherme França, Karl Friston, Mark Girolami, Michael I. Jordan, and Grigorios A. Pavliotis. Geometric Methods for Sampling, Optimisation, Inference and Adaptive Agents. In Geometry and Statistics, number 46 in Handbook of Statistics. Academic Press, 2022. ISBN 978-0-323-91345-4.</p>
<p>Reinforcement Learning: An Introduction. Andrew Barto, Richard Sutton, Andrew Barto and Richard Sutton. Reinforcement Learning: An Introduction. 1992.</p>
<p>Novelty or Surprise? Frontiers in Psychology, 4. Andrew Barto, Marco Mirolli, Gianluca Baldassarre, 10.3389/fpsyg.2013.00907Andrew Barto, Marco Mirolli, and Gianluca Baldassarre. Novelty or Surprise? Frontiers in Psy- chology, 4, 2013. ISSN 1664-1078. doi: 10.3389/fpsyg.2013.00907.</p>
<p>Variational Algorithms for Approximate Bayesian Inference. Matthew James Beal, University of LondonPhD thesisMatthew James Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, University of London, 2003.</p>
<p>Dynamic Programming. Richard E Bellman, 978-0-691-14668-3Princeton University PressPrinceton, NJ, USRichard E. Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, US, 1957. ISBN 978-0-691-14668-3.</p>
<p>Applied Dynamic Programming. Richard E Bellman, Stuart E Dreyfus, 978-1-4008-7465-1Princeton University PressRichard E. Bellman and Stuart E. Dreyfus. Applied Dynamic Programming. Princeton University Press, December 2015. ISBN 978-1-4008-7465-1.</p>
<p>Statistical Decision Theory and Bayesian Analysis. James O Berger, 978-0-387-96098-2. doi: 10.1007/ 978-1-4757-4286-2Springer Series in Statistics. New YorkSpringer-Verlagsecond editionJames O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer Series in Statis- tics. Springer-Verlag, New York, second edition, 1985. ISBN 978-0-387-96098-2. doi: 10.1007/ 978-1-4757-4286-2.</p>
<p>The Exploration-Exploitation Dilemma: A Multidisciplinary Framework. Oded Berger-Tal, Jonathan Nathan, Ehud Meron, David Saltz, 10.1371/journal.pone.0095693PLOS ONE. 9495693Oded Berger-Tal, Jonathan Nathan, Ehud Meron, and David Saltz. The Exploration-Exploitation Dilemma: A Multidisciplinary Framework. PLOS ONE, 9(4):e95693, April 2014. ISSN 1932-6203. doi: 10.1371/journal.pone.0095693.</p>
<p>Stochastic Optimal Control: The Discrete Time Case. Dimitri P Bertsekas, Steven E Shreve, 978-1-886529-03-8Athena Scientific. Dimitri P. Bertsekas and Steven E. Shreve. Stochastic Optimal Control: The Discrete Time Case. Athena Scientific, 1996. ISBN 978-1-886529-03-8.</p>
<p>Christopher M Bishop, 978-0-387-31073-2Pattern Recognition and Machine Learning. Information Science and Statistics. New YorkSpringerChristopher M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statis- tics. Springer, New York, 2006. ISBN 978-0-387-31073-2.</p>
<p>Variational Inference: A Review for Statisticians. David M Blei, Alp Kucukelbir, Jon D Mcauliffe, 10.1080/01621459.2017.1285773Journal of the American Statistical Association. 112518David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational Inference: A Review for Statis- ticians. Journal of the American Statistical Association, 112(518):859-877, April 2017. ISSN 0162-1459, 1537-274X. doi: 10.1080/01621459.2017.1285773.</p>
<p>Planning as inference. Matthew Botvinick, Marc Toussaint, 10.1016/j.tics.2012.08.006Trends in Cognitive Sciences. 1610Matthew Botvinick and Marc Toussaint. Planning as inference. Trends in Cognitive Sciences, 16 (10):485-488, October 2012. ISSN 13646613. doi: 10.1016/j.tics.2012.08.006.</p>
<p>The free energy principle for action and perception: A mathematical review. Christopher L Buckley, Chang Sub Kim, Simon Mcgregor, Anil K Seth, 10.1016/j.jmp.2017.09.004Journal of Mathematical Psychology. 81Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K. Seth. The free energy principle for action and perception: A mathematical review. Journal of Mathematical Psychology, 81:55-79, December 2017. ISSN 00222496. doi: 10.1016/j.jmp.2017.09.004.</p>
<p>Learning Perception and Planning With Deep Active Inference. Ozan Çatal, Tim Verbelen, Johannes Nauta, Cedric De Boom, Bart Dhoedt, 10.1109/ICASSP40776.2020.9054364ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Ozan Çatal, Tim Verbelen, Johannes Nauta, Cedric De Boom, and Bart Dhoedt. Learning Per- ception and Planning With Deep Active Inference. In ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3952-3956, May 2020. doi: 10.1109/ICASSP40776.2020.9054364.</p>
<p>Robot navigation as hierarchical active inference. Ozan Çatal, Tim Verbelen, Toon Van De Maele, Bart Dhoedt, Adam Safron, 10.1016/j.neunet.2021.05.010Neural Networks. 142Ozan Çatal, Tim Verbelen, Toon Van de Maele, Bart Dhoedt, and Adam Safron. Robot navigation as hierarchical active inference. Neural Networks, 142:192-204, October 2021. ISSN 0893-6080. doi: 10.1016/j.neunet.2021.05.010.</p>
<p>Théophile Champion, Howard Bowman, Marek Grześ, arXiv:2111.11276Branching Time Active Inference: Empirical study and complexity class analysis. Théophile Champion, Howard Bowman, and Marek Grześ. Branching Time Active Inference: Em- pirical study and complexity class analysis. arXiv:2111.11276 [cs], November 2021a.</p>
<p>Théophile Champion, Lancelot Da Costa, Howard Bowman, Marek Grześ, arXiv:2111.11107Branching Time Active Inference: The theory and its generality. Théophile Champion, Lancelot Da Costa, Howard Bowman, and Marek Grześ. Branching Time Active Inference: The theory and its generality. arXiv:2111.11107 [cs], November 2021b.</p>
<p>Active Inference in OpenAI Gym: A Paradigm for Computational Investigations Into Psychiatric Illness. Maell Cullen, Ben Davey, Karl J Friston, Rosalyn J Moran, 10.1016/j.bpsc.2018.06.010Biological Psychiatry: Cognitive Neuroscience and Neuroimaging. 39Maell Cullen, Ben Davey, Karl J. Friston, and Rosalyn J. Moran. Active Inference in OpenAI Gym: A Paradigm for Computational Investigations Into Psychiatric Illness. Biological Psychiatry: Cognitive Neuroscience and Neuroimaging, 3(9):809-818, September 2018. ISSN 24519022. doi: 10.1016/j.bpsc.2018.06.010.</p>
<p>Active inference on discrete state-spaces: A synthesis. Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, Karl Friston, 10.1016/j.jmp.2020.102447Journal of Mathematical Psychology. 99102447Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl Friston. Active inference on discrete state-spaces: A synthesis. Journal of Mathematical Psychology, 99: 102447, December 2020. ISSN 0022-2496. doi: 10.1016/j.jmp.2020.102447.</p>
<p>. Lancelot Da Costa, Pablo Lanillos, Noor Sajid, Karl Friston, Shujhat Khan, 10.3390/e24030361How Active Inference Could Help Revolutionise Robotics. Entropy. 243361Lancelot Da Costa, Pablo Lanillos, Noor Sajid, Karl Friston, and Shujhat Khan. How Active Inference Could Help Revolutionise Robotics. Entropy, 24(3):361, March 2022a. ISSN 1099-4300. doi: 10.3390/e24030361.</p>
<p>Active Inference as a Model of Agency. Lancelot Da Costa, Samuel Tenka, Dominic Zhao, Noor Sajid, Workshop on RL as a Model of Agency. Lancelot Da Costa, Samuel Tenka, Dominic Zhao, and Noor Sajid. Active Inference as a Model of Agency. In Workshop on RL as a Model of Agency, 2022b.</p>
<p>Cortical substrates for exploratory decisions in humans. Nathaniel D Daw, John P O&apos;doherty, Peter Dayan, Ben Seymour, Raymond J Dolan, 10.1038/nature04766Nature. 4417095Nathaniel D. Daw, John P. O'Doherty, Peter Dayan, Ben Seymour, and Raymond J. Dolan. Cortical substrates for exploratory decisions in humans. Nature, 441(7095):876-879, June 2006. ISSN 1476- 4687. doi: 10.1038/nature04766.</p>
<p>Decision theory, reinforcement learning, and the brain. P Dayan, N D Daw, 10.3758/CABN.8.4.429Cognitive, Affective, &amp; Behavioral Neuroscience. 84P. Dayan and N. D. Daw. Decision theory, reinforcement learning, and the brain. Cognitive, Affective, &amp; Behavioral Neuroscience, 8(4):429-453, December 2008. ISSN 1530-7026, 1531-135X. doi: 10.3758/CABN.8.4.429.</p>
<p>Intrinsic Motivation and Self-Determination in Human Behavior. Edward Deci, Richard M Ryan, 10.1007/978-1-4899-2271-7Perspectives in Social Psychology. Springer US. Edward Deci and Richard M. Ryan. Intrinsic Motivation and Self-Determination in Human Behav- ior. Perspectives in Social Psychology. Springer US, New York, 1985. ISBN 978-0-306-42022-1. doi: 10.1007/978-1-4899-2271-7.</p>
<p>If maxent rl is the answer, what is the question?. Benjamin Eysenbach, Sergey Levine, arXiv:1910.01913arXiv preprintBenjamin Eysenbach and Sergey Levine. If maxent rl is the answer, what is the question? arXiv preprint arXiv:1910.01913, 2019.</p>
<p>Deep active inference agents using Monte-Carlo methods. Zafeirios Fountas, Noor Sajid, Pedro A M Mediano, Karl Friston, arXiv:2006.04176cs, q-bio, statZafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, and Karl Friston. Deep active inference agents using Monte-Carlo methods. arXiv:2006.04176 [cs, q-bio, stat], June 2020.</p>
<p>The free-energy principle: A unified brain theory?. Karl Friston, 10.1038/nrn2787Nature Reviews Neuroscience. 112Karl Friston. The free-energy principle: A unified brain theory? Nature Reviews Neuroscience, 11 (2):127-138, February 2010. ISSN 1471-003X, 1471-0048. doi: 10.1038/nrn2787.</p>
<p>Active inference and agency: Optimal control without cost functions. Karl Friston, Spyridon Samothrakis, Read Montague, 10.1007/s00422-012-0512-8Biological Cybernetics. 1068Karl Friston, Spyridon Samothrakis, and Read Montague. Active inference and agency: Optimal control without cost functions. Biological Cybernetics, 106(8):523-541, October 2012. ISSN 1432- 0770. doi: 10.1007/s00422-012-0512-8.</p>
<p>Active inference and learning. Karl Friston, Thomas Fitzgerald, Francesco Rigoli, Philipp Schwartenbeck, O&apos; John, Giovanni Doherty, Pezzulo, 10.1016/j.neubiorev.2016.06.022Neuroscience &amp; Biobehavioral Reviews. 68Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John O'Doherty, and Giovanni Pezzulo. Active inference and learning. Neuroscience &amp; Biobehavioral Reviews, 68: 862-879, September 2016. ISSN 01497634. doi: 10.1016/j.neubiorev.2016.06.022.</p>
<p>Active Inference: A Process Theory. Karl Friston, Thomas Fitzgerald, Francesco Rigoli, Philipp Schwartenbeck, Giovanni Pezzulo, 10.1162/NECO_a_00912Neural Computation. 291Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo. Active Inference: A Process Theory. Neural Computation, 29(1):1-49, January 2017a. ISSN 0899-7667, 1530-888X. doi: 10.1162/NECO_a_00912.</p>
<p>Sophisticated Inference. Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, Thomas Parr, 0899-7667. doi: 10.1162/ neco_a_01351Neural Computation. 333Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr. Sophisticated Inference. Neural Computation, 33(3):713-763, February 2021. ISSN 0899-7667. doi: 10.1162/ neco_a_01351.</p>
<p>The free energy principle made simpler but not too simple. Karl Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Kai Ueltzhöffer, Grigorios A Pavliotis, Thomas Parr, arXiv:2201.06387cond-mat, physics:nlin, physics:physics, q-bioKarl Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Kai Ueltzhöffer, Grigorios A. Pavliotis, and Thomas Parr. The free energy principle made simpler but not too simple. arXiv:2201.06387 [cond-mat, physics:nlin, physics:physics, q-bio], January 2022.</p>
<p>Reinforcement Learning or Active Inference. Karl J Friston, Jean Daunizeau, Stefan J Kiebel, 10.1371/journal.pone.0006421PLoS ONE. 476421Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. Reinforcement Learning or Active Inference? PLoS ONE, 4(7):e6421, July 2009. ISSN 1932-6203. doi: 10.1371/journal.pone.0006421.</p>
<p>Action and behavior: A freeenergy formulation. Karl J Friston, Jean Daunizeau, James Kilner, Stefan J Kiebel, 10.1007/s00422-010-0364-zBiological Cybernetics. 1023Karl J. Friston, Jean Daunizeau, James Kilner, and Stefan J. Kiebel. Action and behavior: A free- energy formulation. Biological Cybernetics, 102(3):227-260, March 2010. ISSN 1432-0770. doi: 10.1007/s00422-010-0364-z.</p>
<p>Active Inference, Curiosity and Insight. Karl J Friston, Marco Lin, Christopher D Frith, Giovanni Pezzulo, J Allan Hobson, Sasha Ondobaka, 10.1162/neco_a_00999Neural Computation. 2910Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson, and Sasha On- dobaka. Active Inference, Curiosity and Insight. Neural Computation, 29(10):2633-2683, October 2017b. ISSN 0899-7667, 1530-888X. doi: 10.1162/neco_a_00999.</p>
<p>The graphical brain: Belief propagation and active inference. Karl J Friston, Thomas Parr, Bert De Vries, 10.1162/NETN_a_00018Network Neuroscience. 14Karl J. Friston, Thomas Parr, and Bert de Vries. The graphical brain: Belief propagation and active inference. Network Neuroscience, 1(4):381-414, December 2017c. ISSN 2472-1751. doi: 10.1162/NETN_a_00018.</p>
<p>Deep temporal models and active inference. Karl J Friston, Richard Rosch, Thomas Parr, Cathy Price, Howard Bowman, 10.1016/j.neubiorev.2018.04.004Neuroscience &amp; Biobehavioral Reviews. 90Karl J. Friston, Richard Rosch, Thomas Parr, Cathy Price, and Howard Bowman. Deep temporal models and active inference. Neuroscience &amp; Biobehavioral Reviews, 90:486-501, July 2018. ISSN 01497634. doi: 10.1016/j.neubiorev.2018.04.004.</p>
<p>Game Theory. Drew Fudenberg, Jean Tirole, 978-0-262-06141-4MIT PressDrew Fudenberg and Jean Tirole. Game Theory. MIT Press, 1991. ISBN 978-0-262-06141-4.</p>
<p>Deconstructing the human algorithms for exploration. J Samuel, Gershman, 10.1016/j.cognition.2017.12.014Cognition. 173Samuel J. Gershman. Deconstructing the human algorithms for exploration. Cognition, 173:34-42, April 2018. ISSN 1873-7838. doi: 10.1016/j.cognition.2017.12.014.</p>
<p>Learning latent structure: Carving nature at its joints. J Samuel, Yael Gershman, Niv, 10.1016/j.conb.2010.02.008Current Opinion in Neurobiology. 202Samuel J. Gershman and Yael Niv. Learning latent structure: Carving nature at its joints. Current Opinion in Neurobiology, 20(2):251-256, April 2010. ISSN 1873-6882. doi: 10.1016/j.conb.2010. 02.008.</p>
<p>Bayesian reinforcement learning: A survey. Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, arXiv:1609.04436arXiv preprintMohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforcement learning: A survey. arXiv preprint arXiv:1609.04436, 2016.</p>
<p>Scalable and Efficient Bayes-Adaptive Reinforcement Learning Based on Monte-Carlo Tree Search. A Guez, D Silver, P Dayan, 10.1613/jair.4117Journal of Artificial Intelligence Research. 48A. Guez, D. Silver, and P. Dayan. Scalable and Efficient Bayes-Adaptive Reinforcement Learn- ing Based on Monte-Carlo Tree Search. Journal of Artificial Intelligence Research, 48:841-883, November 2013a. ISSN 1076-9757. doi: 10.1613/jair.4117.</p>
<p>Arthur Guez, David Silver, Peter Dayan, arXiv:1205.3109Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search. cs, statArthur Guez, David Silver, and Peter Dayan. Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search. arXiv:1205.3109 [cs, stat], December 2013b.</p>
<p>Reinforcement learning with deep energy-based policies. Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, Sergey Levine, arXiv:1702.08165arXiv preprintTuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.</p>
<p>Soft actor critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, abs / 1801.01290CoRRTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. CoRR, abs / 1801.01290, 2018. URL http://arxiv.org/abs/1801.01290.</p>
<p>Bonsai Trees in Your Head: How the Pavlovian System Sculpts Goal-Directed Choices by Pruning Decision Trees. J M Quentin, Neir Huys, Eshel, O&apos; Elizabeth, Luke Nions, Peter Sheridan, Jonathan P Dayan, Roiser, 10.1371/journal.pcbi.1002410PLoS Computational Biology. 831002410Quentin J. M. Huys, Neir Eshel, Elizabeth O'Nions, Luke Sheridan, Peter Dayan, and Jonathan P. Roiser. Bonsai Trees in Your Head: How the Pavlovian System Sculpts Goal-Directed Choices by Pruning Decision Trees. PLoS Computational Biology, 8(3):e1002410, March 2012. ISSN 1553-7358. doi: 10.1371/journal.pcbi.1002410.</p>
<p>Bayesian surprise attracts human attention. Laurent Itti, Pierre Baldi, 10.1016/j.visres.2008.09.007Vision research. 4910Laurent Itti and Pierre Baldi. Bayesian surprise attracts human attention. Vision research, 49(10): 1295-1306, May 2009. ISSN 0042-6989. doi: 10.1016/j.visres.2008.09.007.</p>
<p>Information Theory and Statistical Mechanics. E T Jaynes, 10.1103/PhysRev.106.620Physical Review. 1064E. T. Jaynes. Information Theory and Statistical Mechanics. Physical Review, 106(4):620-630, May 1957a. doi: 10.1103/PhysRev.106.620.</p>
<p>Information Theory and Statistical Mechanics. E T Jaynes, 10.1103/PhysRev.108.171II. Physical Review. 1082E. T. Jaynes. Information Theory and Statistical Mechanics. II. Physical Review, 108(2):171-190, October 1957b. doi: 10.1103/PhysRev.108.171.</p>
<p>An Introduction to Variational Methods for Graphical Models. Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, Lawrence K Saul, 10.1007/978-94-011-5014-9_5Learning in Graphical Models. Michael I. JordanNetherlands, DordrechtSpringerMichael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An Introduction to Variational Methods for Graphical Models. In Michael I. Jordan, editor, Learning in Graphical Models, pages 105-161. Springer Netherlands, Dordrecht, 1998. ISBN 978-94-010-6104-9 978-94- 011-5014-9. doi: 10.1007/978-94-011-5014-9_5.</p>
<p>Planning and acting in partially observable stochastic domains. Leslie Pack Kaelbling, Michael L Littman, Anthony R Cassandra, 10.1016/S0004-3702(98)00023-XArtificial Intelligence. 1011Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101(1):99-134, May 1998. ISSN 0004-3702. doi: 10.1016/S0004-3702(98)00023-X.</p>
<p>Prospect Theory: An Analysis of Decision under Risk. Daniel Kahneman, Amos Tversky, 10.2307/1914185Econometrica. 472Daniel Kahneman and Amos Tversky. Prospect Theory: An Analysis of Decision under Risk. Econometrica, 47(2):263-291, 1979. ISSN 0012-9682. doi: 10.2307/1914185.</p>
<p>Optimal control as a graphical model inference problem. Hilbert J Kappen, Vicenç Gómez, Manfred Opper, 10.1007/s10994-012-5278-7Machine Learning. 87Hilbert J. Kappen, Vicenç Gómez, and Manfred Opper. Optimal control as a graphical model inference problem. Machine Learning, 87(2):159-182, May 2012. ISSN 0885-6125, 1573-0565. doi: 10.1007/s10994-012-5278-7.</p>
<p>Keep Your Options Open: An Information-Based Driving Principle for Sensorimotor Systems. Alexander S Klyubin, Daniel Polani, Chrystopher L Nehaniv, 10.1371/journal.pone.0004018PLOS ONE. 3124018Alexander S. Klyubin, Daniel Polani, and Chrystopher L. Nehaniv. Keep Your Options Open: An Information-Based Driving Principle for Sensorimotor Systems. PLOS ONE, 3(12):e4018, December 2008. ISSN 1932-6203. doi: 10.1371/journal.pone.0004018.</p>
<p>The Neural Basis of Aversive Pavlovian Guidance during Planning. Níall Lally, Quentin J M Huys, Neir Eshel, Paul Faulkner, Peter Dayan, Jonathan P Roiser, 10.1523/JNEUROSCI.0085-17Journal of Neuroscience. 3742Níall Lally, Quentin J. M. Huys, Neir Eshel, Paul Faulkner, Peter Dayan, and Jonathan P. Roiser. The Neural Basis of Aversive Pavlovian Guidance during Planning. Journal of Neuroscience, 37 (42):10215-10229, October 2017. ISSN 0270-6474, 1529-2401. doi: 10.1523/JNEUROSCI.0085-17.</p>
<p>Robot self/other distinction: Active inference meets neural networks learning in a mirror. Pablo Lanillos, Jordi Pages, Gordon Cheng, European Conference on Artificial Intelligence. IOS pressPablo Lanillos, Jordi Pages, and Gordon Cheng. Robot self/other distinction: Active inference meets neural networks learning in a mirror. In European Conference on Artificial Intelligence. IOS press, April 2020.</p>
<p>Sergey Levine, arXiv:1805.00909Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprintSergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018a.</p>
<p>Sergey Levine, arXiv:1805.00909Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review. cs, statSergey Levine. Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review. arXiv:1805.00909 [cs, stat], May 2018b.</p>
<p>On a Measure of the Information Provided by an Experiment. D V Lindley, 0003-4851The Annals of Mathematical Statistics. 274D. V. Lindley. On a Measure of the Information Provided by an Experiment. The Annals of Mathematical Statistics, 27(4):986-1005, 1956. ISSN 0003-4851.</p>
<p>Perceptual Neural Organization: Some Approaches Based on Network Models and Information Theory. R Linsker, 10.1146/annurev.ne.13.030190.00135313Annual Review of NeuroscienceR Linsker. Perceptual Neural Organization: Some Approaches Based on Network Models and Information Theory. Annual Review of Neuroscience, 13(1):257-281, 1990. doi: 10.1146/annurev. ne.13.030190.001353.</p>
<p>Information Theory, Inference and Learning Algorithms. J C David, Mackay, 978-0-521-64298-9Cambridge University PressCambridge, UK; New Yorksixth printing 2007 edition editionDavid J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge University Press, Cambridge, UK ; New York, sixth printing 2007 edition edition, September 2003. ISBN 978-0-521-64298-9.</p>
<p>Active Tree Search in Large POMDPs. Domenico Maisto, Francesco Gregoretti, Karl Friston, Giovanni Pezzulo, arXiv:2103.13860cs, math, q-bioDomenico Maisto, Francesco Gregoretti, Karl Friston, and Giovanni Pezzulo. Active Tree Search in Large POMDPs. arXiv:2103.13860 [cs, math, q-bio], March 2021.</p>
<p>An empirical evaluation of active inference in multi-armed bandits. Dimitrije Marković, Hrvoje Stojić, Sarah Schwöbel, Stefan J Kiebel, 10.1016/j.neunet.2021.08.018Neural Networks. 144Dimitrije Marković, Hrvoje Stojić, Sarah Schwöbel, and Stefan J. Kiebel. An empirical evaluation of active inference in multi-armed bandits. Neural Networks, 144:229-246, December 2021. ISSN 0893-6080. doi: 10.1016/j.neunet.2021.08.018.</p>
<p>Contrastive Active Inference. Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Advances in Neural Information Processing Systems. Pietro Mazzaglia, Tim Verbelen, and Bart Dhoedt. Contrastive Active Inference. In Advances in Neural Information Processing Systems, May 2021.</p>
<p>Deep active inference as variational policy gradients. Beren Millidge, 10.1016/j.jmp.2020.102348Journal of Mathematical Psychology. 96102348Beren Millidge. Deep active inference as variational policy gradients. Journal of Mathematical Psychology, 96:102348, June 2020. ISSN 0022-2496. doi: 10.1016/j.jmp.2020.102348.</p>
<p>Beren Millidge, arXiv:2107.00140Applications of the Free Energy Principle to Machine Learning and Neuroscience. Beren Millidge. Applications of the Free Energy Principle to Machine Learning and Neuroscience. arXiv:2107.00140 [cs], June 2021.</p>
<p>Beren Millidge, Alexander Tschantz, Christopher L Buckley, arXiv:2004.08128Whence the Expected Free Energy. Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. Whence the Expected Free En- ergy? arXiv:2004.08128 [cs], April 2020a.</p>
<p>On the Relationship Between Active Inference and Control as Inference. Beren Millidge, Alexander Tschantz, Anil K Seth, Christopher L Buckley, 10.1007/978-3-030-64919-7_1Active Inference, Communications in Computer and Information Science. Tim Verbelen, Pablo Lanillos, Christopher L. Buckley, and Cedric De BoomChamSpringer International PublishingBeren Millidge, Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. On the Relationship Between Active Inference and Control as Inference. In Tim Verbelen, Pablo Lanillos, Christo- pher L. Buckley, and Cedric De Boom, editors, Active Inference, Communications in Computer and Information Science, pages 3-11, Cham, 2020b. Springer International Publishing. ISBN 978-3-030-64919-7. doi: 10.1007/978-3-030-64919-7_1.</p>
<p>Applied Computational Economics and Finance. Mario J Miranda, Paul L Fackler, 978-0-262-63309- 3The MIT PressCambridge, Mass. Londonnew ed edition editionMario J. Miranda and Paul L. Fackler. Applied Computational Economics and Finance. The MIT Press, Cambridge, Mass. London, new ed edition edition, September 2002. ISBN 978-0-262-63309- 3.</p>
<p>Human visual exploration reduces uncertainty about the sensed world. M , Berk Mirza, Rick A Adams, Christoph Mathys, Karl J Friston, 10.1371/journal.pone.0190429PLOS ONE. 131190429M. Berk Mirza, Rick A. Adams, Christoph Mathys, and Karl J. Friston. Human visual exploration reduces uncertainty about the sensed world. PLOS ONE, 13(1):e0190429, 2018. ISSN 1932-6203. doi: 10.1371/journal.pone.0190429.</p>
<p>An empirical study of active inference on a humanoid robot. Guillermo Oliver, Pablo Lanillos, Gordon Cheng, 10.1109/TCDS.2021.3049907IEEE Transactions on Cognitive and Developmental Systems. Guillermo Oliver, Pablo Lanillos, and Gordon Cheng. An empirical study of active inference on a humanoid robot. IEEE Transactions on Cognitive and Developmental Systems, pages 1-1, 2021. ISSN 2379-8939. doi: 10.1109/TCDS.2021.3049907.</p>
<p>Temporal encoding of two-dimensional patterns by single units in primate inferior temporal cortex. III. Information theoretic analysis. L M Optican, B J Richmond, 10.1152/jn.1987.57.1.162Journal of Neurophysiology. 571L. M. Optican and B. J. Richmond. Temporal encoding of two-dimensional patterns by single units in primate inferior temporal cortex. III. Information theoretic analysis. Journal of Neurophysiology, 57(1):162-178, January 1987. ISSN 0022-3077. doi: 10.1152/jn.1987.57.1.162.</p>
<p>What is Intrinsic Motivation? A Typology of Computational Approaches. Pierre- , Yves Oudeyer, Frederic Kaplan, 10.3389/neuro.12.006.2007Frontiers in Neurorobotics. 16Pierre-Yves Oudeyer and Frederic Kaplan. What is Intrinsic Motivation? A Typology of Com- putational Approaches. Frontiers in Neurorobotics, 1:6, November 2007. ISSN 1662-5218. doi: 10.3389/neuro.12.006.2007.</p>
<p>The Computational Neurology of Active Vision. Thomas Parr, LondonUniversity College LondonPhD thesisThomas Parr. The Computational Neurology of Active Vision. PhD thesis, University College London, London, 2019.</p>
<p>Neuronal message passing using Mean-field, Bethe, and Marginal approximations. Thomas Parr, Dimitrije Markovic, Stefan J Kiebel, Karl J Friston, 10.1038/s41598-018-38246-3Scientific Reports. 911889Thomas Parr, Dimitrije Markovic, Stefan J. Kiebel, and Karl J. Friston. Neuronal message passing using Mean-field, Bethe, and Marginal approximations. Scientific Reports, 9(1):1889, December 2019. ISSN 2045-2322. doi: 10.1038/s41598-018-38246-3.</p>
<p>The computational neurology of movement under active inference. Thomas Parr, Jakub Limanowski, Vishal Rawji, Karl Friston, 10.1093/brain/awab085Brain. Thomas Parr, Jakub Limanowski, Vishal Rawji, and Karl Friston. The computational neurology of movement under active inference. Brain, March 2021. ISSN 0006-8950. doi: 10.1093/brain/ awab085.</p>
<p>Active Inference: The Free Energy Principle in Mind, Brain, and Behavior. Thomas Parr, Giovanni Pezzulo, Karl J Friston, 978-0-262- 04535-3MIT PressCambridge, MA, USAThomas Parr, Giovanni Pezzulo, and Karl J. Friston. Active Inference: The Free Energy Principle in Mind, Brain, and Behavior. MIT Press, Cambridge, MA, USA, March 2022. ISBN 978-0-262- 04535-3.</p>
<p>Active Inference for Stochastic Control. Aswin Paul, Noor Sajid, Manoj Gopalkrishnan, Adeel Razi, arXiv:2108.12245Aswin Paul, Noor Sajid, Manoj Gopalkrishnan, and Adeel Razi. Active Inference for Stochastic Control. arXiv:2108.12245 [cs], August 2021.</p>
<p>A Grigorios, Pavliotis, 978-1-4939-1322-0Stochastic Processes and Applications: Diffusion Processes, the Fokker-Planck and Langevin Equations. Number volume. New YorkSpringer60Grigorios A. Pavliotis. Stochastic Processes and Applications: Diffusion Processes, the Fokker- Planck and Langevin Equations. Number volume 60 in Texts in Applied Mathematics. Springer, New York, 2014. ISBN 978-1-4939-1322-0.</p>
<p>Quantified Representation of Uncertainty and Imprecision, Handbook of Defeasible Reasoning and Uncertainty Management Systems. Judea Pearl, 10.1007/978-94-017-1735-9_12Philippe SmetsSpringerNetherlands, DordrechtGraphical Models for Probabilistic and Causal ReasoningJudea Pearl. Graphical Models for Probabilistic and Causal Reasoning. In Philippe Smets, editor, Quantified Representation of Uncertainty and Imprecision, Handbook of Defeasible Reasoning and Uncertainty Management Systems, pages 367-389. Springer Netherlands, Dordrecht, 1998. ISBN 978-94-017-1735-9. doi: 10.1007/978-94-017-1735-9_12.</p>
<p>A Novel Adaptive Controller for Robot Manipulators Based on Active Inference. Corrado Pezzato, Riccardo Ferrari, Carlos Hernández Corbato, 10.1109/LRA.2020.2974451IEEE Robotics and Automation Letters. 52Corrado Pezzato, Riccardo Ferrari, and Carlos Hernández Corbato. A Novel Adaptive Controller for Robot Manipulators Based on Active Inference. IEEE Robotics and Automation Letters, 5(2): 2973-2980, April 2020. ISSN 2377-3766. doi: 10.1109/LRA.2020.2974451.</p>
<p>Active inference and robot control: A case study. Léo Pio-Lopez, Ange Nizard, Karl Friston, Giovanni Pezzulo, 10.1098/rsif.2016.0616Journal of The Royal Society Interface. 1312220160616Léo Pio-Lopez, Ange Nizard, Karl Friston, and Giovanni Pezzulo. Active inference and robot control: A case study. Journal of The Royal Society Interface, 13(122):20160616, September 2016. doi: 10.1098/rsif.2016.0616.</p>
<p>Markov Decision Processes: Discrete Stochastic Dynamic Programming. Martin L Puterman, 978-1-118-62587-3John Wiley &amp; SonsMartin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &amp; Sons, August 2014. ISBN 978-1-118-62587-3.</p>
<p>Jad Rahme, Ryan P Adams, arXiv:1906.10228A Theoretical Connection Between Statistical Physics and Reinforcement Learning. cond-mat, statJad Rahme and Ryan P. Adams. A Theoretical Connection Between Statistical Physics and Rein- forcement Learning. arXiv:1906.10228 [cond-mat, stat], June 2019.</p>
<p>J D Maxwell, Ramstead, A R Dalton, Conor Sakthivadivel, Magnus Heins, Beren Koudahl, Lancelot Da Millidge, Brennan Costa, Karl J Klein, Friston, On Bayesian Mechanics: A Physics of and by Beliefs. Maxwell J. D. Ramstead, Dalton A. R. Sakthivadivel, Conor Heins, Magnus Koudahl, Beren Millidge, Lancelot Da Costa, Brennan Klein, and Karl J. Friston. On Bayesian Mechanics: A Physics of and by Beliefs, May 2022.</p>
<p>On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference. Konrad Rawlik, Marc Toussaint, Sethu Vijayakumar, Twenty-Third International Joint Conference on Artificial Intelligence. Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On Stochastic Optimal Control and Re- inforcement Learning by Approximate Inference. In Twenty-Third International Joint Conference on Artificial Intelligence, June 2013.</p>
<p>A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes. Stéphane Ross, Joelle Pineau, Brahim Chaib-Draa, Pierre Kreitmann, 42Stéphane Ross, Joelle Pineau, Brahim Chaib-draa, and Pierre Kreitmann. A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes. page 42.</p>
<p>Bayes-Adaptive POMDPs. Stephane Ross, Brahim Chaib-Draa, Joelle Pineau, Advances in Neural Information Processing Systems. J. C. Platt, D. Koller, Y. Singer, and S. T. RoweisCurran Associates, Inc20Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. Bayes-Adaptive POMDPs. In J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1225-1232. Curran Associates, Inc., 2008.</p>
<p>Learning to optimize via posterior sampling. Daniel Russo, Benjamin Van Roy, Mathematics of Operations Research. 394Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of Operations Research, 39(4):1221-1243, 2014.</p>
<p>An information-theoretic analysis of thompson sampling. Daniel Russo, Benjamin Van Roy, The Journal of Machine Learning Research. 171Daniel Russo and Benjamin Van Roy. An information-theoretic analysis of thompson sampling. The Journal of Machine Learning Research, 17(1):2442-2471, 2016.</p>
<p>Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, arXiv:1707.02038A tutorial on thompson sampling. arXiv preprintDaniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on thompson sampling. arXiv preprint arXiv:1707.02038, 2017.</p>
<p>Active Inference: Demystified and Compared. Noor Sajid, Philip J Ball, Thomas Parr, Karl J Friston, 0899-7667. doi: 10.1162/ neco_a_01357Neural Computation. 333Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active Inference: Demystified and Compared. Neural Computation, 33(3):674-712, January 2021a. ISSN 0899-7667. doi: 10.1162/ neco_a_01357.</p>
<p>Exploration and preference satisfaction trade-off in reward-free learning. Noor Sajid, Panagiotis Tigas, Alexey Zakharov, arXiv:2106.04316Zafeirios Fountas, and Karl Friston. cs, q-bioNoor Sajid, Panagiotis Tigas, Alexey Zakharov, Zafeirios Fountas, and Karl Friston. Exploration and preference satisfaction trade-off in reward-free learning. arXiv:2106.04316 [cs, q-bio], July 2021b.</p>
<p>A mixed generative model of auditory word repetition. Noor Sajid, Emma Holmes, Lancelot Da Costa, Cathy Price, Karl Friston, Noor Sajid, Emma Holmes, Lancelot Da Costa, Cathy Price, and Karl Friston. A mixed generative model of auditory word repetition, January 2022.</p>
<p>Locus Coeruleus tracking of prediction errors optimises cognitive flexibility: An Active Inference model. Anna C Sales, Karl J Friston, Matthew W Jones, Anthony E Pickering, Rosalyn J Moran, 10.1371/journal.pcbi.1006267PLOS Computational Biology. 1511006267Anna C. Sales, Karl J. Friston, Matthew W. Jones, Anthony E. Pickering, and Rosalyn J. Moran. Locus Coeruleus tracking of prediction errors optimises cognitive flexibility: An Active Inference model. PLOS Computational Biology, 15(1):e1006267, January 2019. ISSN 1553-7358. doi: 10.1371/journal.pcbi.1006267.</p>
<p>End-to-End Pixel-Based Deep Active Inference for Body Perception and Action. Cansu Sancaktar, Marcel Van Gerven, Pablo Lanillos, arXiv:2001.05847cs, q-bioCansu Sancaktar, Marcel van Gerven, and Pablo Lanillos. End-to-End Pixel-Based Deep Active Inference for Body Perception and Action. arXiv:2001.05847 [cs, q-bio], May 2020.</p>
<p>. R W H Sargent, 10.1016/S0377-0427(00)00418-0Optimal control. Journal of Computational and Applied Mathematics. 1241R. W. H. Sargent. Optimal control. Journal of Computational and Applied Mathematics, 124(1): 361-371, December 2000. ISSN 0377-0427. doi: 10.1016/S0377-0427(00)00418-0.</p>
<p>Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts. Jürgen Schmidhuber, 0954-0091. doi: 10.1080/ 09540090600768658Connection Science. 182Jürgen Schmidhuber. Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts. Connection Science, 18(2):173-187, June 2006. ISSN 0954-0091. doi: 10.1080/ 09540090600768658.</p>
<p>Jürgen Schmidhuber, 10.1109/TAMD.2010.2056368Formal Theory of Creativity, Fun, and Intrinsic Motivation. 2Jürgen Schmidhuber. Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990-2010). IEEE Transactions on Autonomous Mental Development, 2(3):230-247, September 2010. ISSN 1943-0604, 1943-0612. doi: 10.1109/TAMD.2010.2056368.</p>
<p>Active Inference for Robotic Manipulation. Tim Schneider, Boris Belousov, Hany Abdulsamad, Jan Peters, Tim Schneider, Boris Belousov, Hany Abdulsamad, and Jan Peters. Active Inference for Robotic Manipulation, June 2022.</p>
<p>The algorithmic architecture of exploration in the human brain. Eric Schulz, Samuel J Gershman, 10.1016/j.conb.2018.11.003Current Opinion in Neurobiology. 55Eric Schulz and Samuel J. Gershman. The algorithmic architecture of exploration in the human brain. Current Opinion in Neurobiology, 55:7-14, 2019. ISSN 1873-6882. doi: 10.1016/j.conb.2018.11.003.</p>
<p>The Dopaminergic Midbrain Encodes the Expected Certainty about Desired Outcomes. Philipp Schwartenbeck, H B Thomas, Christoph Fitzgerald, Ray Mathys, Karl Dolan, Friston, 1460-2199. doi: 10. 1093/cercor/bhu159Cerebral Cortex. 2510Philipp Schwartenbeck, Thomas H. B. FitzGerald, Christoph Mathys, Ray Dolan, and Karl Friston. The Dopaminergic Midbrain Encodes the Expected Certainty about Desired Outcomes. Cerebral Cortex (New York, N.Y.: 1991), 25(10):3434-3445, October 2015a. ISSN 1460-2199. doi: 10. 1093/cercor/bhu159.</p>
<p>Evidence for surprise minimization over value maximization in choice behavior. Philipp Schwartenbeck, H B Thomas, Christoph Fitzgerald, Ray Mathys, Martin Dolan, Karl Kronbichler, Friston, 10.1038/srep16575Scientific Reports. 516575Philipp Schwartenbeck, Thomas H. B. FitzGerald, Christoph Mathys, Ray Dolan, Martin Kron- bichler, and Karl Friston. Evidence for surprise minimization over value maximization in choice behavior. Scientific Reports, 5:16575, November 2015b. ISSN 2045-2322. doi: 10.1038/srep16575.</p>
<p>Computational mechanisms of curiosity and goal-directed exploration. eLife. Philipp Schwartenbeck, Johannes Passecker, U Tobias, Hauser, H B Thomas, Martin Fitzgerald, Karl J Kronbichler, Friston, 45Philipp Schwartenbeck, Johannes Passecker, Tobias U Hauser, Thomas HB FitzGerald, Martin Kron- bichler, and Karl J Friston. Computational mechanisms of curiosity and goal-directed exploration. eLife, page 45, 2019.</p>
<p>Multi-agent reinforcement learning: A critical survey. Yoav Shoham, Rob Powers, Trond Grenager, Technical reportYoav Shoham, Rob Powers, and Trond Grenager. Multi-agent reinforcement learning: A critical survey. Technical report, 2003.</p>
<p>Mastering the game of Go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, 10.1038/nature16961Thore Graepel, and Demis Hassabis. 529David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484-489, January 2016. ISSN 0028-0836, 1476-4687. doi: 10.1038/nature16961.</p>
<p>An active inference model of concept learning. bioRxiv, page 633677. Ryan Smith, Philipp Schwartenbeck, Thomas Parr, Karl J Friston, 10.1101/633677Ryan Smith, Philipp Schwartenbeck, Thomas Parr, and Karl J. Friston. An active inference model of concept learning. bioRxiv, page 633677, May 2019. doi: 10.1101/633677.</p>
<p>A Bayesian computational model reveals a failure to adapt interoceptive precision estimates across depression, anxiety, eating, and substance use disorders. Ryan Smith, Rayus Kuplicki, Justin Feinstein, Katherine L Forthman, Jennifer L Stewart ; Sahib, S Khalsa, 10.1371/journal.pcbi.1008484Martin P. Paulus, Tulsa 1000 Investigators, and. 161008484Ryan Smith, Rayus Kuplicki, Justin Feinstein, Katherine L. Forthman, Jennifer L. Stewart, Mar- tin P. Paulus, Tulsa 1000 Investigators, and Sahib S. Khalsa. A Bayesian computational model reveals a failure to adapt interoceptive precision estimates across depression, anxiety, eating, and substance use disorders. PLOS Computational Biology, 16(12):e1008484, December 2020a. ISSN 1553-7358. doi: 10.1371/journal.pcbi.1008484.</p>
<p>Confirmatory evidence that healthy individuals can adaptively adjust prior expectations and interoceptive precision estimates. Ryan Smith, Rayus Kuplicki, Adam Teed, Valerie Upshaw, Sahib S Khalsa, Ryan Smith, Rayus Kuplicki, Adam Teed, Valerie Upshaw, and Sahib S. Khalsa. Confirmatory evi- dence that healthy individuals can adaptively adjust prior expectations and interoceptive precision estimates, September 2020b.</p>
<p>An Active Inference Approach to Modeling Structure Learning: Concept Learning as an Example Case. Ryan Smith, Philipp Schwartenbeck, Thomas Parr, Karl J Friston, 10.3389/fncom.2020.00041Frontiers in Computational Neuroscience. 14Ryan Smith, Philipp Schwartenbeck, Thomas Parr, and Karl J. Friston. An Active Inference Ap- proach to Modeling Structure Learning: Concept Learning as an Example Case. Frontiers in Computational Neuroscience, 14, May 2020c. ISSN 1662-5188. doi: 10.3389/fncom.2020.00041.</p>
<p>Imprecise Action Selection in Substance Use Disorder: Evidence for Active Learning Impairments When Solving the Explore-Exploit Dilemma. Drug and alcohol dependence. Ryan Smith, Philipp Schwartenbeck, Jennifer L Stewart, Rayus Kuplicki, Hamed Ekhtiari, Martin P Paulus, 10.1016/j.drugalcdep.2020.108208215108208Ryan Smith, Philipp Schwartenbeck, Jennifer L. Stewart, Rayus Kuplicki, Hamed Ekhtiari, and Martin P. Paulus. Imprecise Action Selection in Substance Use Disorder: Evidence for Active Learning Impairments When Solving the Explore-Exploit Dilemma. Drug and alcohol dependence, 215:108208, October 2020d. ISSN 0376-8716. doi: 10.1016/j.drugalcdep.2020.108208.</p>
<p>An Active Inference Approach to Dissecting Reasons for Nonadherence to Antidepressants. Ryan Smith, S Sahib, Martin P Khalsa, Paulus, 10.1016/j.bpsc.2019.11.012Biological Psychiatry. Cognitive Neuroscience and Neuroimaging. 69Ryan Smith, Sahib S. Khalsa, and Martin P. Paulus. An Active Inference Approach to Dissecting Reasons for Nonadherence to Antidepressants. Biological Psychiatry. Cognitive Neuroscience and Neuroimaging, 6(9):919-934, September 2021a. ISSN 2451-9030. doi: 10.1016/j.bpsc.2019.11.012.</p>
<p>Greater decision uncertainty characterizes a transdiagnostic patient sample during approach-avoidance conflict: A computational modelling approach. Ryan Smith, Namik Kirlic, Jennifer L Stewart, James Touthang, Rayus Kuplicki, S Sahib, Justin Khalsa, Martin P Feinstein, Robin L Paulus, Aupperle, 10.1503/jpn.200032Journal of psychiatry &amp; neuroscience: JPN. 461Ryan Smith, Namik Kirlic, Jennifer L. Stewart, James Touthang, Rayus Kuplicki, Sahib S. Khalsa, Justin Feinstein, Martin P. Paulus, and Robin L. Aupperle. Greater decision uncertainty char- acterizes a transdiagnostic patient sample during approach-avoidance conflict: A computational modelling approach. Journal of psychiatry &amp; neuroscience: JPN, 46(1):E74-E87, January 2021b. ISSN 1488-2434. doi: 10.1503/jpn.200032.</p>
<p>Longterm stability of computational parameters during approach-avoidance conflict in a transdiagnostic psychiatric patient sample. Ryan Smith, Namik Kirlic, Jennifer L Stewart, James Touthang, Rayus Kuplicki, Timothy J Mcdermott, Samuel Taylor, Sahib S Khalsa, Martin P Paulus, Robin L Aupperle, 10.1038/s41598-021-91308-xScientific Reports. 11111783Ryan Smith, Namik Kirlic, Jennifer L. Stewart, James Touthang, Rayus Kuplicki, Timothy J. McDermott, Samuel Taylor, Sahib S. Khalsa, Martin P. Paulus, and Robin L. Aupperle. Long- term stability of computational parameters during approach-avoidance conflict in a transdiagnostic psychiatric patient sample. Scientific Reports, 11(1):11783, June 2021c. ISSN 2045-2322. doi: 10.1038/s41598-021-91308-x.</p>
<p>Gut inference: A computational modelling approach. Ryan Smith, Ahmad Mayeli, Samuel Taylor, Al Obada, Jessyca Zoubi, Sahib S Naegele, Khalsa, 10.1016/j.biopsycho.2021.108152September 2021d. 164108152Ryan Smith, Ahmad Mayeli, Samuel Taylor, Obada Al Zoubi, Jessyca Naegele, and Sahib S. Khalsa. Gut inference: A computational modelling approach. Biological Psychology, 164:108152, Septem- ber 2021d. ISSN 0301-0511. doi: 10.1016/j.biopsycho.2021.108152.</p>
<p>A step-by-step tutorial on active inference and its application to empirical data. Ryan Smith, Karl J Friston, Christopher J Whyte, 10.1016/j.jmp.2021.102632Journal of Mathematical Psychology. 107102632Ryan Smith, Karl J. Friston, and Christopher J. Whyte. A step-by-step tutorial on active inference and its application to empirical data. Journal of Mathematical Psychology, 107:102632, April 2022a. ISSN 0022-2496. doi: 10.1016/j.jmp.2021.102632.</p>
<p>Slower Learning Rates from Negative Outcomes in Substance Use Disorder over a 1-Year Period and Their Potential Predictive Utility. Ryan Smith, Samuel Taylor, Jennifer L Stewart, Salvador M Guinjoan, Maria Ironside, Namik Kirlic, Hamed Ekhtiari, Evan J White, Haixia Zheng, Rayus , Martin P Paulus, 10.5334/cpsy.85Tulsa 1000 Investigators. 6Ryan Smith, Samuel Taylor, Jennifer L. Stewart, Salvador M. Guinjoan, Maria Ironside, Namik Kirlic, Hamed Ekhtiari, Evan J. White, Haixia Zheng, Rayus Kuplicki, Tulsa 1000 Investigators, and Martin P. Paulus. Slower Learning Rates from Negative Outcomes in Substance Use Disorder over a 1-Year Period and Their Potential Predictive Utility. Computational Psychiatry, 6(1): 117-141, June 2022b. ISSN 2379-6227. doi: 10.5334/cpsy.85.</p>
<p>An information-theoretic approach to curiosity-driven reinforcement learning. Susanne Still, Doina Precup, 10.1007/s12064-011-0142-zTheory in Biosciences = Theorie in Den Biowissenschaften. 1313Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforcement learning. Theory in Biosciences = Theorie in Den Biowissenschaften, 131(3):139-148, September 2012. ISSN 1611-7530. doi: 10.1007/s12064-011-0142-z.</p>
<p>Learning options in reinforcement learning. Martin Stolle, Doina Precup, International Symposium on abstraction, reformulation, and approximation. SpringerMartin Stolle and Doina Precup. Learning options in reinforcement learning. In International Symposium on abstraction, reformulation, and approximation, pages 212-223. Springer, 2002.</p>
<p>Information Theory: A Tutorial Introduction. James V Stone, 978-0-9563728-5-7Sebtel PressEngland1st edition editionJames V. Stone. Information Theory: A Tutorial Introduction. Sebtel Press, England, 1st edition edition, February 2015. ISBN 978-0-9563728-5-7.</p>
<p>V James, Stone, Artificial Intelligence Engines: A Tutorial Introduction to the Mathematics of Deep Learning. James V Stone. Artificial Intelligence Engines: A Tutorial Introduction to the Mathematics of Deep Learning. 2019.</p>
<p>Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments. Yi Sun, Faustino Gomez, Juergen Schmidhuber, arXiv:1103.5708cs, statYi Sun, Faustino Gomez, and Juergen Schmidhuber. Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments. arXiv:1103.5708 [cs, stat], March 2011.</p>
<p>A Theory of Mean Field Approximation. Toshiyuki Tanaka, 10Toshiyuki Tanaka. A Theory of Mean Field Approximation. page 10, 1999.</p>
<p>Toward the neural implementation of structure learning. D , Gowanlock R Tervo, Joshua B Tenenbaum, Samuel J Gershman, 10.1016/j.conb.2016.01.014Current Opinion in Neurobiology. 37D. Gowanlock R. Tervo, Joshua B. Tenenbaum, and Samuel J. Gershman. Toward the neural implementation of structure learning. Current Opinion in Neurobiology, 37:99-105, April 2016. ISSN 1873-6882. doi: 10.1016/j.conb.2016.01.014.</p>
<p>Linearly-solvable markov decision problems. Emanuel Todorov, Advances in neural information processing systems. Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in neural information processing systems, pages 1369-1376, 2007.</p>
<p>General duality between optimal control and estimation. Emanuel Todorov, 10.1109/cdc.2008.473943847th IEEE Conference on Decision and Control. Emanuel Todorov. General duality between optimal control and estimation. 2008 47th IEEE Con- ference on Decision and Control, pages 4286-4292, 2008. doi: 10.1109/cdc.2008.4739438.</p>
<p>Efficient computation of optimal actions. Emanuel Todorov, Proceedings of the national academy of sciences. the national academy of sciences106Emanuel Todorov. Efficient computation of optimal actions. Proceedings of the national academy of sciences, 106(28):11478-11483, 2009.</p>
<p>Value-Difference Based Exploration: Adaptive Control between Epsilon-Greedy and Softmax. Michel Tokic, Günther Palm, 10.1007/978-3-642-24455-1_33KI 2011: Advances in Artificial Intelligence. Joscha Bach and Stefan EdelkampBerlin, HeidelbergSpringerMichel Tokic and Günther Palm. Value-Difference Based Exploration: Adaptive Control between Epsilon-Greedy and Softmax. In Joscha Bach and Stefan Edelkamp, editors, KI 2011: Advances in Artificial Intelligence, Lecture Notes in Computer Science, pages 335-346, Berlin, Heidelberg, 2011. Springer. ISBN 978-3-642-24455-1. doi: 10.1007/978-3-642-24455-1_33.</p>
<p>Robot trajectory optimization using approximate inference. Marc Toussaint, 10.1145/1553374.1553508Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09. the 26th Annual International Conference on Machine Learning, ICML '09Montreal, Quebec, CanadaAssociation for Computing MachineryMarc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09, pages 1049-1056, Montreal, Quebec, Canada, June 2009. Association for Computing Machinery. ISBN 978-1-60558- 516-1. doi: 10.1145/1553374.1553508.</p>
<p>Alexander Tschantz, Manuel Baltieri, Anil K Seth, Christopher L Buckley, arXiv:1911.10601Scaling active inference. cs, eess, math, statAlexander Tschantz, Manuel Baltieri, Anil K. Seth, and Christopher L. Buckley. Scaling active inference. arXiv:1911.10601 [cs, eess, math, stat], November 2019.</p>
<p>Reinforcement Learning through Active Inference. Alexander Tschantz, Beren Millidge, Anil K Seth, Christopher L Buckley, ICLR. Alexander Tschantz, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. Reinforcement Learning through Active Inference. In ICLR, February 2020a.</p>
<p>Learning action-oriented models through active inference. Alexander Tschantz, Anil K Seth, Christopher L Buckley, 10.1371/journal.pcbi.1007805PLOS Computational Biology. 1641007805Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. Learning action-oriented models through active inference. PLOS Computational Biology, 16(4):e1007805, April 2020b. ISSN 1553- 7358. doi: 10.1371/journal.pcbi.1007805.</p>
<p>Risk sensitive path integral control. Wim Bart Van Den Broek, Bert Wiegerinck, Kappen, UAIBart van den Broek, Wim Wiegerinck, and Bert Kappen. Risk sensitive path integral control. UAI, 2010.</p>
<p>Pablo Otto Van Der Himst, Lanillos, 10.1007/978-3-030-64919-7_8arXiv:2009.03622Deep Active Inference for Partially Observable MDPs. 1326cs, statOtto van der Himst and Pablo Lanillos. Deep Active Inference for Partially Observable MDPs. arXiv:2009.03622 [cs, stat], 1326:61-71, 2020. doi: 10.1007/978-3-030-64919-7_8.</p>
<p>Theory of Games and Economic Behavior. Theory of Games and Economic Behavior. J , Von Neumann, O Morgenstern, Princeton University PressPrinceton, NJ, USJ. Von Neumann and O. Morgenstern. Theory of Games and Economic Behavior. Theory of Games and Economic Behavior. Princeton University Press, Princeton, NJ, US, 1944.</p>
<p>Graphical Models, Exponential Families, and Variational Inference. Foundations and Trends® in Machine Learning. J Martin, Michael I Wainwright, Jordan, 10.1561/22000000011Martin J. Wainwright and Michael I. Jordan. Graphical Models, Exponential Families, and Vari- ational Inference. Foundations and Trends® in Machine Learning, 1(1-2):1-305, 2007. ISSN 1935-8237, 1935-8245. doi: 10.1561/2200000001.</p>
<p>Humans Use Directed and Random Exploration to Solve the Explore-Exploit Dilemma. Robert C Wilson, Andra Geana, John M White, Elliot A Ludvig, Jonathan D Cohen, 10.1037/a0038199Journal of experimental psychology. General. 1436Robert C. Wilson, Andra Geana, John M. White, Elliot A. Ludvig, and Jonathan D. Cohen. Hu- mans Use Directed and Random Exploration to Solve the Explore-Exploit Dilemma. Journal of experimental psychology. General, 143(6):2074-2081, December 2014. ISSN 0096-3445. doi: 10.1037/a0038199.</p>
<p>Balancing exploration and exploitation with information and randomization. Robert C Wilson, Elizabeth Bonawitz, Vincent D Costa, R Becket Ebitz, 10.1016/j.cobeha.2020.10.001Current Opinion in Behavioral Sciences. 38Robert C. Wilson, Elizabeth Bonawitz, Vincent D. Costa, and R. Becket Ebitz. Balancing ex- ploration and exploitation with information and randomization. Current Opinion in Behavioral Sciences, 38:49-56, 2021. ISSN 2352-1554. doi: 10.1016/j.cobeha.2020.10.001.</p>
<p>Novelty is not surprise: Human exploratory and adaptive behavior in sequential decision-making. A He, Alireza Xu, Modirshanechi, P Marco, Wulfram Lehmann, Michael H Gerstner, Herzog, PLOS Computational Biology. 1761009070He A Xu, Alireza Modirshanechi, Marco P Lehmann, Wulfram Gerstner, and Michael H Herzog. Novelty is not surprise: Human exploratory and adaptive behavior in sequential decision-making. PLOS Computational Biology, 17(6):e1009070, 2021.</p>
<p>über eine Anwendung der Mengenlehre auf die Theorie des Schachspiels. Ernst Zermelo, Ernst Zermelo. über eine Anwendung der Mengenlehre auf die Theorie des Schachspiels. 1913.</p>
<p>Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy. B Ziebart, PittsburghCarnegie Mellon UniversityPhD thesisB. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy. PhD thesis, Carnegie Mellon University, Pittsburgh, 2010.</p>
<p>Maximum entropy inverse reinforcement learning. D Brian, Andrew L Ziebart, Andrew Maas, Anind K Bagnell, Dey, Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. 2008.</p>
<p>VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via. Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, Shimon Whiteson, arXiv:1910.08348Meta-Learning. cs, statLuisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, and Shimon Whiteson. VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta- Learning. arXiv:1910.08348 [cs, stat], February 2020.</p>
<p>Exploration in approximate hyper-state space for meta reinforcement learning. M Luisa, Leo Zintgraf, Cong Feng, Maximilian Lu, Kristian Igl, Katja Hartikainen, Shimon Hofmann, Whiteson, International Conference on Machine Learning. PMLRLuisa M Zintgraf, Leo Feng, Cong Lu, Maximilian Igl, Kristian Hartikainen, Katja Hofmann, and Shimon Whiteson. Exploration in approximate hyper-state space for meta reinforcement learning. In International Conference on Machine Learning, pages 12991-13001. PMLR, 2021.</p>            </div>
        </div>

    </div>
</body>
</html>
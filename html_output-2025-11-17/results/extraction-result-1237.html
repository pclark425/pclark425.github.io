<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1237 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1237</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1237</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-266818451</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.02695v2.pdf" target="_blank">VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model</a></p>
                <p><strong>Paper Abstract:</strong> In the realm of household robotics, the Zero-Shot Object Navigation (ZSON) task empowers agents to adeptly traverse unfamiliar environments and locate objects from novel categories without prior explicit training. This paper introduces VoroNav, a novel semantic exploration framework that proposes the Reduced Voronoi Graph to extract exploratory paths and planning nodes from a semantic map constructed in real time. By harnessing topological and semantic information, VoroNav designs text-based descriptions of paths and images that are readily interpretable by a large language model (LLM). In particular, our approach presents a synergy of path and farsight descriptions to represent the environmental context, enabling LLM to apply commonsense reasoning to ascertain waypoints for navigation. Extensive evaluation on HM3D and HSSD validates VoroNav surpasses existing benchmarks in both success rate and exploration efficiency (absolute improvement: +2.8% Success and +3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally introduced metrics that evaluate obstacle avoidance proficiency and perceptual efficiency further corroborate the enhancements achieved by our method in ZSON planning. Project page: https://voro-nav.github.io</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1237.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1237.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VoroNav</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical, zero-shot object navigation framework that builds a Reduced Voronoi Graph (RVG) from an online semantic map, generates natural-language path and farsight descriptions, queries an LLM (GPT-3.5) for commonsense priors, and selects mid-term waypoints via a hierarchical reward combining exploration, efficiency and semantic scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HM3D; HSSD</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>High-fidelity indoor household/building 3D reconstructions (HM3D) and synthetic household scenes (HSSD) used for embodied household object-goal navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Sparse skeletonized topological graph (RVG) derived from GVD; nodes represent intersections and endpoints, edges are navigable medial-axis segments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Varies per scene; HM3D contains 20 reconstructions used in evaluation (2K validation episodes) and HSSD contains 40 scenes (1.2K episodes); node/edge counts not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VoroNav hierarchical planner (RVG + GPT-3.5 + local Fast Marching motion planner)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Modular architecture: (1) Semantic mapping to 2D multi-channel grid, (2) RVG extraction and classification of agent/neighbor/exploratory/ordinary nodes, (3) LLM-guided mid-term waypoint selection using path and farsight textual descriptions, (4) local shortest-path motion via Fast Marching. Decision rule maximizes W = P + C + L (exploration, efficiency, semantic).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>SPL (Success weighted by Path Length); also SEA (Success-weighted Explored Area) and SCA (Success-weighted Collision Avoidance) for planning evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>HM3D SPL 26.0; HSSD SPL 23.2 (reported in Table 2 for VoroNav)</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>HM3D 42.0%; HSSD 41.0% (reported in Table 2 for VoroNav)</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Graph-search / planning-based hierarchical policy that reasons over topological waypoints with memory (map + RVG) and semantic priors from an LLM</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Selecting intersections (RVG nodes) as decision points increases perceptual coverage per step and reduces proximity to obstacles, which improves obstacle-avoidance (higher SCA), increases observed area per path length (higher SEA), and yields better Success and SPL compared to frontier-based choices; the RVG structure supports efficient exploration by prioritizing nodes that lead to unexplored regions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Voronoi-based methods (RVG/VoroNav) outperform Frontier-based methods: higher Success and SPL, higher SCA and SEA. Voronoi waypoints (intersections) are less collision-prone and provide broader views, increasing chance of locating targets with fewer steps.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that make deliberative mid-term decisions at topological intersections and combine topological (exploration/efficiency) with semantic (LLM) rewards perform best; hierarchical reward prioritizing exploration then efficiency then semantics reduces backtracking and improves exploration efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1237.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1237.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RVG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reduced Voronoi Graph (RVG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sparsified topological graph extracted from the Generalized Voronoi Diagram (GVD) of the observed traversable area; nodes correspond to intersections or endpoints of the GVD and edges are medial-axis segments connecting nodes, used as waypoint candidates for mid-term planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HM3D; HSSD</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Topological representation of observed traversable floor-space in indoor household scenes, derived from the semantic/explored/obstacle map.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td>Endpoints of the GVD are retained as RVG nodes; exact count/proportion not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Sparse, skeleton-like connectivity capturing medial-axis routes and intersections; connectivity is pruned (merge proximate nodes, remove trivial forks) to emphasize informative intersections.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Used by VoroNav as the global topological planner interface</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RVG supplies agent with discrete neighbor nodes (agent node and its neighbor nodes) and exploratory nodes; the agent computes shortest paths along RVG edges to exploratory nodes and forms textual descriptions for LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Indirectly evaluated via SPL, SEA and SCA when RVG is used for waypoint selection</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Topology-aware planning that selects waypoints at intersections (graph-node based planning)</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>RVG nodes at intersections give richer observations and lead to more informative mid-term decisions; using RVG increases SEA and SCA relative to frontier-only planning, improving exploration efficiency and safety.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>RVG-based waypoint selection (Voronoi) yields higher SCA and SEA and better Success/SPL compared to frontier-based baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Treating intersections (RVG nodes) as decision points and structuring policy as sequential planning over graph nodes (choose neighbor node, follow local planner) is beneficial; exploratory nodes (single-edge nodes to unexplored areas) are flagged to bias exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1237.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1237.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GVD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generalized Voronoi Diagram (GVD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A medial-axis representation of free space formed by points equidistant to nearest obstacles; used here as the basis for skeletonizing traversable area before forming the RVG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HM3D; HSSD (as basis for RVG extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Geometric/topological skeleton of unoccupied regions of an observed 2D occupancy map in indoor scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Dense medial-axis lines prior to sparsification; connectivity reflects medial pathways equidistant from obstacles.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Intermediate geometric structure used to derive RVG nodes and edges after skeletonization and pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>GVD provides the geometric substrate whose intersections/branches correspond to informative vantage points; pruning to RVG improves usability for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Using medial-axis-derived nodes concentrates planning on central traversable corridors and intersections, reducing proximity to obstacles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1237.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1237.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Frontier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Frontier-based Exploration (Yamauchi-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classic exploration approach that selects nearest boundary points between explored and unexplored regions (frontiers) as mid-term goals for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HM3D; HSSD (baselines evaluated on same datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Exploration over indoor household maps by selecting frontier boundary coordinates; used as a baseline planning strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Implicit grid/frontier-based selection rather than an explicit sparse topological graph; tends to choose proximal unexplored boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Frontier planner (nearest unexplored boundary selection)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Selects nearest frontier point as mid-term goal repeatedly; does not explicitly prioritize informative intersections or integrate semantic LLM priors.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Success, SPL; planning evaluated via SCA and SEA</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>In reported experiments Frontier achieved lower SPL and Success than Voronoi-based methods; e.g., Frontier SCA HM3D 24.2 SEA 17.4 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported lower than Voronoi/VoroNav (example numbers in tables: Frontier Success values in various tables are lower than Voronoi methods; exact per-dataset success in Table 1/2).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Reactive frontier-selection strategy favors nearest unexplored area but can be suboptimal for target-directed search without semantic priors</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Frontier selection often places mid-term goals near obstacles or in suboptimal observation locations, yielding lower SEA (observed area per path length) and lower SCA (more collisions) compared to Voronoi-intersection choices.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Frontier underperforms Voronoi-based methods in Success, SPL, SCA and SEA; frontier choices are less informative per step and can produce inefficient back-and-forth movement.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Frontier policies benefit from being augmented with semantics/LLM priors or by switching decision points to intersection-centric graph nodes to reduce inefficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1237.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1237.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voronoi (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voronoi-based navigation baseline (no semantic reward)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that uses the RVG/Voronoi topology to select informative neighbor nodes as waypoints but omits LLM-provided semantic reward; evaluates purely topological planning benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HM3D; HSSD</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same indoor navigation environments; Voronoi baseline uses RVG to plan but without semantic LLM scores.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Sparse RVG connectivity emphasizing intersections.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voronoi waypoint planner (no semantic reward)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Chooses neighbor nodes based on topological signals (exploratory/efficiency) without LLM semantic scoring; local motion via Fast Marching.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Success, SPL, SCA, SEA</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>HM3D SPL 23.3; HSSD SPL 22.2 (reported in Table 2 for Voronoi baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>HM3D 38.7%; HSSD 40.3% (reported in Table 2 for Voronoi baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Topology-driven planning without semantic priors</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Demonstrates that topology alone (RVG intersections) improves perceptual efficiency and collision avoidance versus frontier-based planning, but adding semantic LLM scores further improves success/SPL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Voronoi baseline has higher SCA/SEA and better success/SPL than Frontier, but underperforms VoroNav which adds semantic LLM priors.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Topological selection of intersections is effective; LLM semantic scores add complementary gains by biasing toward areas likely to contain target objects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1237.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1237.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCA/SEA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCA (Success-weighted Collision Avoidance) and SEA (Success-weighted Explored Area)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two planning/evaluation metrics introduced to quantify (1) obstacle-avoidance tendency during successful runs (SCA) and (2) perceptual efficiency measured as observed area relative to path length during successful runs (SEA).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HM3D; HSSD</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Metrics applied to evaluate planners operating in indoor embodied navigation scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Evaluation metrics computed from episode statistics: SCA = (1/N) sum_i S_i (1 - eta * C_i / F_i) where C_i collisions, F_i forward steps; SEA = (1/N) sum_i S_i gamma * sqrt(A_i / L_i) where A_i explored area and L_i path length.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>SCA and SEA (complementary to Success and SPL)</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Reported (Table 3): HM3D SCA: Frontier 24.2, Voronoi 29.4, VoroNav 39.8; HM3D SEA: Frontier 17.4, Voronoi 17.9, VoroNav 20.9. HSSD SCA: Frontier 35.5, Voronoi 40.2, VoroNav 40.9; HSSD SEA: Frontier 16.5, Voronoi 18.6, VoroNav 19.3.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Higher SCA and SEA correlate with Voronoi-based waypoint choices (intersections) that keep agents away from obstacles and provide larger perceptual coverage per unit path length, which in turn correlates with higher Success and SPL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>SCA and SEA quantitatively show Voronoi/RVG methods outperform Frontier-based methods in obstacle avoidance and perceptual efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Designing planners that prioritize intersection-centric waypoints increases SCA and SEA; these metrics help diagnose whether topology choice (Voronoi vs Frontier) improves safety and perceptual efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1237.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1237.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 LLM guidance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 as LLM for commonsense reasoning in navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of GPT-3.5 to interpret textual path and farsight descriptions and output probability estimates that a given neighbor area contains the target object; these probabilities form the semantic reward vector L in waypoint selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HM3D; HSSD (used as reasoning module in VoroNav)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>LLM consumes textualized summaries of RVG paths and panoramic image captions corresponding to neighbor nodes in indoor scenes to provide semantic priors for object likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-guided planner (GPT-3.5 integrated into VoroNav)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM forms semantic reward vector L by estimating target likelihood per neighbor node from fused path and farsight textual descriptions; used in weighted decision rule W = P + C + L.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Contributes to SPL and Success via semantic reward; no standalone efficiency value</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Combines model-based topological planning with LLM-provided semantic priors for better target-directed exploration</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>LLM guidance is most effective when combined with topology-derived, information-rich observations (e.g., descriptions produced at intersections) because broader context improves LLM reasoning; the paper shows VoroNav (topology + LLM) outperforms topology-only and frontier+LLM variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>LLM-based semantic scoring yields larger gains when used with RVG-derived path/farsight descriptions than when used with frontier clusters or fixed-interval image-based decisions (e.g., Pixel-Nav, ESC).</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that integrate LLM semantic priors with topological exploration and explicit exploration/efficiency rewards outperform purely topological or purely LLM-driven approaches; LLM alone can make poor choices unless given broader scene descriptions from topology.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A frontier-based approach for autonomous exploration <em>(Rating: 2)</em></li>
                <li>Incremental reconstruction of generalized voronoi diagrams on grids <em>(Rating: 2)</em></li>
                <li>Improving autonomous exploration using reduced approximated generalized voronoi graphs <em>(Rating: 2)</em></li>
                <li>Revolt: Relational reasoning and voronoi local graph planning for target-driven navigation <em>(Rating: 2)</em></li>
                <li>Navigation with large language models: Semantic guesswork as a heuristic for planning <em>(Rating: 2)</em></li>
                <li>Zero experience required: Plug & play modular transfer learning for semantic visual navigation <em>(Rating: 1)</em></li>
                <li>Bridging zero-shot object navigation and foundation models through pixel-guided navigation skill <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1237",
    "paper_id": "paper-266818451",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "VoroNav",
            "name_full": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
            "brief_description": "A hierarchical, zero-shot object navigation framework that builds a Reduced Voronoi Graph (RVG) from an online semantic map, generates natural-language path and farsight descriptions, queries an LLM (GPT-3.5) for commonsense priors, and selects mid-term waypoints via a hierarchical reward combining exploration, efficiency and semantic scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "HM3D; HSSD",
            "environment_description": "High-fidelity indoor household/building 3D reconstructions (HM3D) and synthetic household scenes (HSSD) used for embodied household object-goal navigation tasks.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Sparse skeletonized topological graph (RVG) derived from GVD; nodes represent intersections and endpoints, edges are navigable medial-axis segments.",
            "environment_size": "Varies per scene; HM3D contains 20 reconstructions used in evaluation (2K validation episodes) and HSSD contains 40 scenes (1.2K episodes); node/edge counts not reported.",
            "agent_name": "VoroNav hierarchical planner (RVG + GPT-3.5 + local Fast Marching motion planner)",
            "agent_description": "Modular architecture: (1) Semantic mapping to 2D multi-channel grid, (2) RVG extraction and classification of agent/neighbor/exploratory/ordinary nodes, (3) LLM-guided mid-term waypoint selection using path and farsight textual descriptions, (4) local shortest-path motion via Fast Marching. Decision rule maximizes W = P + C + L (exploration, efficiency, semantic).",
            "exploration_efficiency_metric": "SPL (Success weighted by Path Length); also SEA (Success-weighted Explored Area) and SCA (Success-weighted Collision Avoidance) for planning evaluation",
            "exploration_efficiency_value": "HM3D SPL 26.0; HSSD SPL 23.2 (reported in Table 2 for VoroNav)",
            "success_rate": "HM3D 42.0%; HSSD 41.0% (reported in Table 2 for VoroNav)",
            "optimal_policy_type": "Graph-search / planning-based hierarchical policy that reasons over topological waypoints with memory (map + RVG) and semantic priors from an LLM",
            "topology_performance_relationship": "Selecting intersections (RVG nodes) as decision points increases perceptual coverage per step and reduces proximity to obstacles, which improves obstacle-avoidance (higher SCA), increases observed area per path length (higher SEA), and yields better Success and SPL compared to frontier-based choices; the RVG structure supports efficient exploration by prioritizing nodes that lead to unexplored regions.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Voronoi-based methods (RVG/VoroNav) outperform Frontier-based methods: higher Success and SPL, higher SCA and SEA. Voronoi waypoints (intersections) are less collision-prone and provide broader views, increasing chance of locating targets with fewer steps.",
            "policy_structure_findings": "Policies that make deliberative mid-term decisions at topological intersections and combine topological (exploration/efficiency) with semantic (LLM) rewards perform best; hierarchical reward prioritizing exploration then efficiency then semantics reduces backtracking and improves exploration efficiency.",
            "uuid": "e1237.0",
            "source_info": {
                "paper_title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "RVG",
            "name_full": "Reduced Voronoi Graph (RVG)",
            "brief_description": "A sparsified topological graph extracted from the Generalized Voronoi Diagram (GVD) of the observed traversable area; nodes correspond to intersections or endpoints of the GVD and edges are medial-axis segments connecting nodes, used as waypoint candidates for mid-term planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "HM3D; HSSD",
            "environment_description": "Topological representation of observed traversable floor-space in indoor household scenes, derived from the semantic/explored/obstacle map.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": "Endpoints of the GVD are retained as RVG nodes; exact count/proportion not reported.",
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Sparse, skeleton-like connectivity capturing medial-axis routes and intersections; connectivity is pruned (merge proximate nodes, remove trivial forks) to emphasize informative intersections.",
            "environment_size": null,
            "agent_name": "Used by VoroNav as the global topological planner interface",
            "agent_description": "RVG supplies agent with discrete neighbor nodes (agent node and its neighbor nodes) and exploratory nodes; the agent computes shortest paths along RVG edges to exploratory nodes and forms textual descriptions for LLM reasoning.",
            "exploration_efficiency_metric": "Indirectly evaluated via SPL, SEA and SCA when RVG is used for waypoint selection",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Topology-aware planning that selects waypoints at intersections (graph-node based planning)",
            "topology_performance_relationship": "RVG nodes at intersections give richer observations and lead to more informative mid-term decisions; using RVG increases SEA and SCA relative to frontier-only planning, improving exploration efficiency and safety.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "RVG-based waypoint selection (Voronoi) yields higher SCA and SEA and better Success/SPL compared to frontier-based baselines.",
            "policy_structure_findings": "Treating intersections (RVG nodes) as decision points and structuring policy as sequential planning over graph nodes (choose neighbor node, follow local planner) is beneficial; exploratory nodes (single-edge nodes to unexplored areas) are flagged to bias exploration.",
            "uuid": "e1237.1",
            "source_info": {
                "paper_title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GVD",
            "name_full": "Generalized Voronoi Diagram (GVD)",
            "brief_description": "A medial-axis representation of free space formed by points equidistant to nearest obstacles; used here as the basis for skeletonizing traversable area before forming the RVG.",
            "citation_title": "",
            "mention_or_use": "use",
            "environment_name": "HM3D; HSSD (as basis for RVG extraction)",
            "environment_description": "Geometric/topological skeleton of unoccupied regions of an observed 2D occupancy map in indoor scenes.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Dense medial-axis lines prior to sparsification; connectivity reflects medial pathways equidistant from obstacles.",
            "environment_size": null,
            "agent_name": null,
            "agent_description": "Intermediate geometric structure used to derive RVG nodes and edges after skeletonization and pruning.",
            "exploration_efficiency_metric": null,
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": null,
            "topology_performance_relationship": "GVD provides the geometric substrate whose intersections/branches correspond to informative vantage points; pruning to RVG improves usability for planning.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "",
            "policy_structure_findings": "Using medial-axis-derived nodes concentrates planning on central traversable corridors and intersections, reducing proximity to obstacles.",
            "uuid": "e1237.2",
            "source_info": {
                "paper_title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Frontier",
            "name_full": "Frontier-based Exploration (Yamauchi-style)",
            "brief_description": "A classic exploration approach that selects nearest boundary points between explored and unexplored regions (frontiers) as mid-term goals for exploration.",
            "citation_title": "",
            "mention_or_use": "use",
            "environment_name": "HM3D; HSSD (baselines evaluated on same datasets)",
            "environment_description": "Exploration over indoor household maps by selecting frontier boundary coordinates; used as a baseline planning strategy.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Implicit grid/frontier-based selection rather than an explicit sparse topological graph; tends to choose proximal unexplored boundaries.",
            "environment_size": null,
            "agent_name": "Frontier planner (nearest unexplored boundary selection)",
            "agent_description": "Selects nearest frontier point as mid-term goal repeatedly; does not explicitly prioritize informative intersections or integrate semantic LLM priors.",
            "exploration_efficiency_metric": "Success, SPL; planning evaluated via SCA and SEA",
            "exploration_efficiency_value": "In reported experiments Frontier achieved lower SPL and Success than Voronoi-based methods; e.g., Frontier SCA HM3D 24.2 SEA 17.4 (Table 3).",
            "success_rate": "Reported lower than Voronoi/VoroNav (example numbers in tables: Frontier Success values in various tables are lower than Voronoi methods; exact per-dataset success in Table 1/2).",
            "optimal_policy_type": "Reactive frontier-selection strategy favors nearest unexplored area but can be suboptimal for target-directed search without semantic priors",
            "topology_performance_relationship": "Frontier selection often places mid-term goals near obstacles or in suboptimal observation locations, yielding lower SEA (observed area per path length) and lower SCA (more collisions) compared to Voronoi-intersection choices.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Frontier underperforms Voronoi-based methods in Success, SPL, SCA and SEA; frontier choices are less informative per step and can produce inefficient back-and-forth movement.",
            "policy_structure_findings": "Frontier policies benefit from being augmented with semantics/LLM priors or by switching decision points to intersection-centric graph nodes to reduce inefficiency.",
            "uuid": "e1237.3",
            "source_info": {
                "paper_title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Voronoi (baseline)",
            "name_full": "Voronoi-based navigation baseline (no semantic reward)",
            "brief_description": "A baseline that uses the RVG/Voronoi topology to select informative neighbor nodes as waypoints but omits LLM-provided semantic reward; evaluates purely topological planning benefits.",
            "citation_title": "",
            "mention_or_use": "use",
            "environment_name": "HM3D; HSSD",
            "environment_description": "Same indoor navigation environments; Voronoi baseline uses RVG to plan but without semantic LLM scores.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Sparse RVG connectivity emphasizing intersections.",
            "environment_size": null,
            "agent_name": "Voronoi waypoint planner (no semantic reward)",
            "agent_description": "Chooses neighbor nodes based on topological signals (exploratory/efficiency) without LLM semantic scoring; local motion via Fast Marching.",
            "exploration_efficiency_metric": "Success, SPL, SCA, SEA",
            "exploration_efficiency_value": "HM3D SPL 23.3; HSSD SPL 22.2 (reported in Table 2 for Voronoi baseline)",
            "success_rate": "HM3D 38.7%; HSSD 40.3% (reported in Table 2 for Voronoi baseline)",
            "optimal_policy_type": "Topology-driven planning without semantic priors",
            "topology_performance_relationship": "Demonstrates that topology alone (RVG intersections) improves perceptual efficiency and collision avoidance versus frontier-based planning, but adding semantic LLM scores further improves success/SPL.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Voronoi baseline has higher SCA/SEA and better success/SPL than Frontier, but underperforms VoroNav which adds semantic LLM priors.",
            "policy_structure_findings": "Topological selection of intersections is effective; LLM semantic scores add complementary gains by biasing toward areas likely to contain target objects.",
            "uuid": "e1237.4",
            "source_info": {
                "paper_title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SCA/SEA",
            "name_full": "SCA (Success-weighted Collision Avoidance) and SEA (Success-weighted Explored Area)",
            "brief_description": "Two planning/evaluation metrics introduced to quantify (1) obstacle-avoidance tendency during successful runs (SCA) and (2) perceptual efficiency measured as observed area relative to path length during successful runs (SEA).",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "HM3D; HSSD",
            "environment_description": "Metrics applied to evaluate planners operating in indoor embodied navigation scenes.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "",
            "graph_connectivity": null,
            "environment_size": null,
            "agent_name": null,
            "agent_description": "Evaluation metrics computed from episode statistics: SCA = (1/N) sum_i S_i (1 - eta * C_i / F_i) where C_i collisions, F_i forward steps; SEA = (1/N) sum_i S_i gamma * sqrt(A_i / L_i) where A_i explored area and L_i path length.",
            "exploration_efficiency_metric": "SCA and SEA (complementary to Success and SPL)",
            "exploration_efficiency_value": "Reported (Table 3): HM3D SCA: Frontier 24.2, Voronoi 29.4, VoroNav 39.8; HM3D SEA: Frontier 17.4, Voronoi 17.9, VoroNav 20.9. HSSD SCA: Frontier 35.5, Voronoi 40.2, VoroNav 40.9; HSSD SEA: Frontier 16.5, Voronoi 18.6, VoroNav 19.3.",
            "success_rate": null,
            "optimal_policy_type": null,
            "topology_performance_relationship": "Higher SCA and SEA correlate with Voronoi-based waypoint choices (intersections) that keep agents away from obstacles and provide larger perceptual coverage per unit path length, which in turn correlates with higher Success and SPL.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "SCA and SEA quantitatively show Voronoi/RVG methods outperform Frontier-based methods in obstacle avoidance and perceptual efficiency.",
            "policy_structure_findings": "Designing planners that prioritize intersection-centric waypoints increases SCA and SEA; these metrics help diagnose whether topology choice (Voronoi vs Frontier) improves safety and perceptual efficiency.",
            "uuid": "e1237.5",
            "source_info": {
                "paper_title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-3.5 LLM guidance",
            "name_full": "GPT-3.5 as LLM for commonsense reasoning in navigation",
            "brief_description": "Use of GPT-3.5 to interpret textual path and farsight descriptions and output probability estimates that a given neighbor area contains the target object; these probabilities form the semantic reward vector L in waypoint selection.",
            "citation_title": "",
            "mention_or_use": "use",
            "environment_name": "HM3D; HSSD (used as reasoning module in VoroNav)",
            "environment_description": "LLM consumes textualized summaries of RVG paths and panoramic image captions corresponding to neighbor nodes in indoor scenes to provide semantic priors for object likelihood.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": null,
            "environment_size": null,
            "agent_name": "LLM-guided planner (GPT-3.5 integrated into VoroNav)",
            "agent_description": "LLM forms semantic reward vector L by estimating target likelihood per neighbor node from fused path and farsight textual descriptions; used in weighted decision rule W = P + C + L.",
            "exploration_efficiency_metric": "Contributes to SPL and Success via semantic reward; no standalone efficiency value",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Combines model-based topological planning with LLM-provided semantic priors for better target-directed exploration",
            "topology_performance_relationship": "LLM guidance is most effective when combined with topology-derived, information-rich observations (e.g., descriptions produced at intersections) because broader context improves LLM reasoning; the paper shows VoroNav (topology + LLM) outperforms topology-only and frontier+LLM variants.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "LLM-based semantic scoring yields larger gains when used with RVG-derived path/farsight descriptions than when used with frontier clusters or fixed-interval image-based decisions (e.g., Pixel-Nav, ESC).",
            "policy_structure_findings": "Policies that integrate LLM semantic priors with topological exploration and explicit exploration/efficiency rewards outperform purely topological or purely LLM-driven approaches; LLM alone can make poor choices unless given broader scene descriptions from topology.",
            "uuid": "e1237.6",
            "source_info": {
                "paper_title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A frontier-based approach for autonomous exploration",
            "rating": 2,
            "sanitized_title": "a_frontierbased_approach_for_autonomous_exploration"
        },
        {
            "paper_title": "Incremental reconstruction of generalized voronoi diagrams on grids",
            "rating": 2,
            "sanitized_title": "incremental_reconstruction_of_generalized_voronoi_diagrams_on_grids"
        },
        {
            "paper_title": "Improving autonomous exploration using reduced approximated generalized voronoi graphs",
            "rating": 2,
            "sanitized_title": "improving_autonomous_exploration_using_reduced_approximated_generalized_voronoi_graphs"
        },
        {
            "paper_title": "Revolt: Relational reasoning and voronoi local graph planning for target-driven navigation",
            "rating": 2,
            "sanitized_title": "revolt_relational_reasoning_and_voronoi_local_graph_planning_for_targetdriven_navigation"
        },
        {
            "paper_title": "Navigation with large language models: Semantic guesswork as a heuristic for planning",
            "rating": 2,
            "sanitized_title": "navigation_with_large_language_models_semantic_guesswork_as_a_heuristic_for_planning"
        },
        {
            "paper_title": "Zero experience required: Plug & play modular transfer learning for semantic visual navigation",
            "rating": 1,
            "sanitized_title": "zero_experience_required_plug_play_modular_transfer_learning_for_semantic_visual_navigation"
        },
        {
            "paper_title": "Bridging zero-shot object navigation and foundation models through pixel-guided navigation skill",
            "rating": 1,
            "sanitized_title": "bridging_zeroshot_object_navigation_and_foundation_models_through_pixelguided_navigation_skill"
        }
    ],
    "cost": 0.01674475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model
6 Feb 2024</p>
<p>Pengying Wu 
Peking University</p>
<p>Yao Mu 
The University of Hong Kong</p>
<p>OpenGVLab
Shanghai AI Laboratory</p>
<p>Bingxian Wu 
Peking University</p>
<p>Yi Hou 
Peking University</p>
<p>Ji Ma 
Peking University</p>
<p>Shanghang Zhang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#115;&#104;&#97;&#110;&#103;&#104;&#97;&#110;&#103;&#64;&#112;&#107;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#115;&#104;&#97;&#110;&#103;&#104;&#97;&#110;&#103;&#64;&#112;&#107;&#117;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a> 
Peking University</p>
<p>Chang Liu 
Peking University</p>
<p>1 1 2</p>
<p>VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model
6 Feb 2024F9443C17F5EF99DC335E7299C706B04AarXiv:2401.02695v2[cs.RO]
In the realm of household robotics, the Zero-Shot Object Navigation (ZSON) task empowers agents to adeptly traverse unfamiliar environments and locate objects from novel categories without prior explicit training.This paper introduces VoroNav, a novel semantic exploration framework that proposes the Reduced Voronoi Graph to extract exploratory paths and planning nodes from a semantic map constructed in real time.By harnessing topological and semantic information, VoroNav designs text-based descriptions of paths and images that are readily interpretable by a large language model (LLM).In particular, our approach presents a synergy of path and farsight descriptions to represent the environmental context, enabling LLM to apply commonsense reasoning to ascertain waypoints for navigation.Extensive evaluation on HM3D and HSSD validates VoroNav surpasses existing benchmarks in both success rate and exploration efficiency (absolute improvement: +2.8% Success and +3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD).Additionally introduced metrics that evaluate obstacle avoidance proficiency and perceptual efficiency further corroborate the enhancements achieved by our method in ZSON planning.</p>
<p>Introduction</p>
<p>Navigation capability holds great significance for household robots, enabling these machines to effectively reach designated areas and complete various subsequent tasks.Within this context, Zero-Shot Object Navigation (ZSON) demands  that an agent have the ability to move toward a target object of an unfamiliar category by leveraging scene reasoning, a capability essential for the performance of diverse complex tasks by household robots.The core of ZSON centers on making good use of general commonsense to steer agents for exploration with minimal movement cost and accurate localization of a novel target object.</p>
<p>Current ZSON methods can be categorized into two types: end-to-end, network-based navigation (Majumdar et al., 2022;Park et al., 2023;Zhao et al., 2023;Chen et al., 2023a;Gadre et al., 2023); and modular, map-based navigation (Zhou et al., 2023;Gadre et al., 2023;Chen et al., 2023b).The end-to-end methods use reinforcement or imitation learning for training policy networks, and are designed for mapping directly from RGB-D images to actions.However, the end-to-end model's output lacks interpretability and necessitates a substantial amount of training data, and exhibits serious inefficiency problems of back-and-forth redundant movement regarding the actual performance.</p>
<p>Map-based methods leverage maps to store historical topology information for planning purposes.Map-based navigation frameworks usually plan new waypoints either every predetermined number of steps or when the increment in map building reaches a specific threshold.However, the selected waypoints usually come short of optimal positions for decision-making.This occurs because the agent could arrive at an intersection with massive information and potentially uncover expansive unseen areas by just one more step from here, which can bring huge benefits to scene reasoning and task planning.Yet, such benefits may not be obtained using the traditional strategy of choosing waypoints, for they would not actively identify informative points as waypoints.</p>
<p>Just imagine that you are looking for an object, walking down a long corridor, and encountering the scene shown in Figure 1 (b).Would you be more inclined to halt at the intersection, take a moment to observe your surroundings, and then make a thoughtful decision after comparing the adjacent areas?Therefore, this study puts insight into the positive impact of making decisions at intersections in the field of navigation, and develops a Reduced Voronoi Graph (RVG) generation approach to distill intersection points and viable pathways from the real-time built map.Utilizing graph-structured RVG, we systematize the planning process as navigation subtasks across graph nodes.</p>
<p>Another significant issue faced by existing navigation algorithms is the integral representation of observed scenes for subsequent planning.When presented with RGB images, network-based approaches leverage semantic embeddings to identify novel object categories and utilize recurrent policy networks to directly predict optimal actions (Khandelwal et al., 2022); Conversely, map-based methods mostly employ an open-set detector to segment RGB images, which, in conjunction with depth data and pose information, are utilized to construct a semantic map.By interpreting the representation of the semantic map, the next subgoal point is selected (Chaplot et al., 2020).Each method, however, presents distinct limitations: network-based methods struggle with low exploration efficiency and constrained planning memory that is limited by implicit scene representation and network size, whereas map-based methods only build maps within the field of view of the depth camera, thus unable to integrate information beyond the depth sensing range to plan informed waypoints.To overcome these limitations, it is essential for the agent to fuse the observations of both maps and images, comprehensively understand both modalities, and make appropriate decisions.</p>
<p>To provide a direct response, we adopt the large language model (LLM) as a cognitive engine for spatial reasoning to understand various scenes.The custom-designed prompts are developed to effectively integrate observations of maps and images, considering the preference of LLM.Previous studies (Zhou et al., 2023;Yu et al., 2023b) have collected objects around frontier points to depict the scenes of the exploration areas, and then employed LLMs to infer probable locations of the target.While these foundational applications established a groundwork for guiding navigation with LLM, we reimagine these principles by imitating human exploratory behaviors.Specifically, human exploration typically involves scene description from two perspectives: the egocentric view and the scenes along traversable paths.Descriptions that align analogously with human cognition ensure that the resulting prompts are closer to human corpora (Beckner et al., 2009;Lai et al., 2018), and previous works (Brown et al., 2020;Naveed et al., 2023) show that LLM typically exhibits enhanced performance when dealing with natural language problems similar to the corpora.Starting from this standpoint, we formulate the prompt by generating descriptions of paths (scenes along traversable paths) and farsight images (egocentric view), thereby promoting LLM's understanding of the observed scenes.</p>
<p>The ZSON task requires the agent to find the target at the lowest path cost.The reasoning results of LLM can guide the agent in predicting the probable locations of the target, but struggle to handle the problem of exploration.So we design a hierarchical reward mechanism that combines the topological information of the map and the suggestions provided by LLM.This mechanism evaluates the exploration significance, path efficiency, and commonsense tendencies of each candidate waypoint on the RVG, thereby facilitating more informed decision-making.</p>
<p>Our contributions can be summarized as follows:</p>
<p> We introduce Voronoi-based scene graph generation for ZSON, designed to select waypoints that provide a wealth of observation data to facilitate subsequent planning processes. We design an innovative prompting strategy of scene representation that combines both path and farsight descriptions to provide holistic scene descriptions for LLM to analyze and evaluate. We propose a decision-making policy that necessitates deliberation among exploration, path efficiency, and commonsense tendencies to yield rational actions. We achieve state-of-the-art results on the ZSON task and outperform benchmark methods on representative datasets, i.e., HM3D (Ramakrishnan et al., 2021) and HSSD (Khanna et al., 2023).</p>
<p>Related Work</p>
<p>Zero-shot Object Navigation</p>
<p>In contrast to conventional object navigation, ZSON aims to locate objects of unfamiliar categories and attain high exploration efficiency.Image-based ZSON works (Majumdar et al., 2022;Al-Halah et al., 2022;Gadre et al., 2023) map the egocentric images and target object instructions to the embedding spaces, utilizing a trained policy network to predict subsequent actions.In contrast, map-based ZSON works mostly adopt hierarchical structures, integrated with zero-shot object detectors that identify target objects.These approaches make informed decisions by leveraging prior knowledge of object relationships (Chen et al., 2023b) or by employing large language models (Zhou et al., 2023;Yu et al., 2023b;Shah et al., 2023).</p>
<p>Scene Representation for Navigation</p>
<p>In the hierarchical framework of visual navigation, scene representation is used to process the received observation information into an explicit structure that can be directly utilized by subsequent decision-making.Frontier-based works (Ramakrishnan et al., 2022;Chen et al., 2023b;Gadre et al., 2023;Gervet et al., 2023) model semantic information into frontiers extracted by online grid maps to complete exploration with specific tasks.Graph-based works predict waypoints directly from RGB-D images (Krantz et al., 2020;2021;An et al., 2023) or from sparsified maps (Li et al., 2020;Kwon et al., 2023;Liu et al., 2023a) to represent the environment as topological maps, integrating geographic and semantic information into nodes for waypoint navigation.</p>
<p>LLM Guided Navigation</p>
<p>LLMs have become a new way of prior-knowledge reasoning in navigation due to its powerful information processing and generative capabilities.</p>
<p>VoroNav Approach</p>
<p>This section first introduces the task definition of ZSON (Section 3.1).Subsequently, the modules of the VoroNav framework are introduced.As shown in Figure 2, VoroNav constructs a semantic map in Semantic Mapping Module (Section 3.2), then determines the mid-term goal in Global Decision Module (Section 3.3), and finally plans local motion and selects a discrete action in Local Policy Module (Section 3.4).</p>
<p>Task Definition of ZSON</p>
<p>Traditional supervised object navigation relies on the knowledge or reward from the training data to predict the optimal action a t and is limited to navigating to targets within a closed set of known categories K.However, the ZSON task requires neither purposeful training nor closely linked prior knowledge for navigation toward a novel set of object types N .Initially, the agent is placed at a designated start point p 0 and is given the category G  N of the target to find.The agent's observation includes RGB-D images I t and the real-time pose p t in the environment E.An effective decision-making framework needs to be developed to leverage these observed data O t = {{p0, I0}, . . ., {pt, It}} to understand and deduce the environment, aiming to predict the likely position of the target object.The agent is required to explore the environment according to its planning module until it discovers the target, after which it should proceed toward the target.Success is achieved when the agent reaches a geodesic distance of less than 0.1 meters from the target and executes a "Stop" command.Conversely, the task is deemed failed if the agent either exceeds the maximum step count without finding the target or executes the "Stop" action at a distance greater than 0.1 meters from the target.</p>
<p>The Semantic Mapping Module</p>
<p>We maintain a 2D semantic map M t by processing RGB-D images {I 0 , . . ., I t } and poses {p 0 , . . ., p t }.This semantic map is structured as a (K + 2)  M  M grid, where M denotes the dimensions of the map's width and height, and (K + 2) indicates the total number of channels within the map.These channels comprise K categorical maps,  an obstacle map, and an explored map, which correspond to detected object regions, obstacle regions, and observed regions, respectively.Given the depth image and the agent's pose, 3D point clouds are generated.All point clouds near the floor are assigned to the explored map representing the feasible area to travel through, whereas those at other heights are mapped into the obstacle map.Meanwhile, we predict the category masks of the RGB image by Grounded-SAM (Liu et al., 2023b;Kirillov et al., 2023) and map the masks into 3D semantic point clouds using the depth information and the agent's pose.The 3D point clouds with K categorical information are correspondingly mapped to K categorical map channels.</p>
<p>The Global Decision Module</p>
<p>Graph Extraction.The Generalized Voronoi Diagram (GVD) of a map depicts a set of points that are equidistant from the two closest obstacle points, representing the medialaxis pathway of unoccupied space outside the obstacles of arbitrary shape.Let X  R 2 be the map space and  denote the area occupied by obstacles on the map.The point set V of GVD can be represented as follows:
V = {x  X \|i  = j  , d(x, i) = d(x, j) = f (x)} (1)
where  () represents any point within the obstacles , the function d(, ) denotes the Euclidean distance between two points, while f () signifies the positive Euclidean Signed Distance Field (ESDF), which is defined as follows:
f (x) = inf y d(x, y)(2)
where  indicates the boundary of the obstacles.</p>
<p>Given the obstacle and explored maps, we can obtain the GVD points from these maps and construct RVG G to represent the observed spaces1 .We then classify the RVG nodes into four categories based on the node positions: agent nodes, neighbor nodes, exploratory nodes, and ordinary nodes.Specifically, the node closest to the agent is designated as the agent node, representing the agent's current decision-making position; The nodes directly connected to the agent node are considered neighbor nodes for subsequent planning; The nodes adjacent to unexplored areas with a single connecting edge are classified as exploratory nodes.</p>
<p>All the other nodes are categorized as ordinary nodes.</p>
<p>Path Description.We generate navigable paths formed by RVG edges and create text descriptions that embody the scene along each path, as shown in Figure 3 (a).To be specific, given m exploratory nodes, we leverage the Wavefront Propagation method (Kalra et al., 2009) to obtain the shortest path P j from the agent node to the j th exploratory node on the GVD and compile all the paths into a set P = {P 1 , . . ., P m }, as shown in Figure 6 (c).To generate the semantic description of each path P j , we gather from the semantic map the occurrence of c objects {o j,1 , . . ., o j,c } along the exploratory path P j and the objects' central locations {l j,1 , . . ., l j,c }.Assuming paths within the set {P a , . . ., P b } all pass through the neighbor node N i , prompts of the form Template({Pa : Now, you have 3 areas to go: l Area 1: there is a bedroom with a couch, a bed and pillows, you will pass by a bedroom with bed, chair and refrigerator on the way.l Area 2: there is a meeting room with dining-table and chairs.l Area 3: there is a bathroom with a bathtub, you will pass by a bathroom with toilet and sink on the way.Please estimate the probability of couch appearing at these areas in [0,1].</p>
<p>Scene Description: LLM Response:</p>
<p>Area 1 : 0.9 Area 2 : 0.3 Area 3 : 0.1</p>
<p>Objects Along the Path: Path Description:</p>
<p>You are given the center point coordinates of objects that appear on a path: Bed (62,120); Chair (98,171); Refrigerator (117,122).You need to describe this path.</p>
<p>You will pass by a bedroom with bed, chair and refrigerator on the way.(oa,1, la,1), . ..} . . .{P b : (o b,1 , l b,1 ), . . .}) are generated for the neighbor node N i , by collecting and summarizing the semantic information along the paths in {P a , . . ., P b }.The function Template() processes the input data, converting it into the textual form and integrating it with predefined templates to create a format conducive to conversational interactions with LLM (refer to Figure 3 (a): Objects Along the Path).Afterward, to distill the fragmented and unstructured path information into a coherent format, we employ GPT-3.5 (Ouyang et al., 2022), which possesses robust comprehension and generative capabilities, for creating the scene descriptions D p i along paths that traverse each neighbor node N i (refer to Figure 3 (a): Path Description).Similarly, assuming there involve n neighbor nodes, we describe the scenes of the paths each neighbor node N i leads to and compile the path descriptions into a set D p = {D p 1 , . . ., D p n }.This process textualizes the scenarios the agent will encounter along possible navigable paths after reaching each neighbor node.</p>
<p>Farsight Description.Path description generation is the process of converting the semantic map into scene descriptions of path form; however, the semantic map is constrained by the depth camera's limited range, precluding the incorporation of map information beyond its scope.Consequently, semantic descriptions of RGB images of unexplored areas add crucial complementary context for robot navigation.As shown in Figure 4, at the onset of the ZSON task or upon reaching an RVG node, the agent executes a full rotation to capture panoramic images.We then determine the ray R i on the map that extends from the agent's current node (agent node) to each neighbor node N i .The RGB image Caption 2: There is a meeting room with dining-table and chairs .</p>
<p>Caption 3: There is a bathroom with a bathtub.</p>
<p>Caption 1: There is a bedroom with a couch, a bed and pillows.I k collected from the full rotation I t = {I t11 , . . ., I t } (a full rotation includes 12 turns), whose central Line of Sight (LoS) T k exhibits the least angular deviation from the ray R i , is identified as the one (I f i ) oriented towards the corresponding neighbor node N i .Let T t = {T t11 , . . ., T t } be the central LoS set of I t , the process of matching images with each neighbor node N i can be defined as follows:
arg min T k g(R i , T k ) s.t. T k  T t(3)
where the function g(, ) indicates the angle between two rays on the map.The BLIP model (Li et al., 2022) is then employed to generate descriptions
D f = {D f 1 , . . . , D f n } for those images {I f 1 , . . . , I f n } facing different neighbor nodes {N 1 , . . . , N n }.
Planning with LLM.We select the mid-term target points by considering three distinct factors: exploration objective, locomotion efficiency (traversed path length), and alignment with typical scene layouts.The rewards for exploration and efficiency are space reasoning results stemming from spatial topology.To encourage the agent to explore the environment, we design a binary exploration reward vector P to indicate if there exists an exploratory path from the agent node to each exploratory node that traverses through neighbor nodes.Considering exploration efficiency, we design an efficiency reward vector C and assess whether each neighbor node lies within the previously traversed area.</p>
<p>Semantic rewards are reasoning feedback grounded in empirical knowledge and commonsense.The Global Decision Module utilizes the commonsense reasoning capabilities of the large language model, GPT-3.5, to select the most promising goal node for finding or approaching the target object among all neighbor nodes.To this end, we combine the path and farsight descriptions of each neighbor node to generate a specially designed form of prompt that is amenable for GPT-3.5.This enables the LLM to more accu-rately estimate the probability of the target object's presence on each neighbor node by detailed prompts, as illustrated in Figure 3 (b).The probabilities given by the LLM's response are compiled into a semantic reward vector L and serve as varying levels of semantic incentive to navigate towards neighbor nodes.</p>
<p>When the agent simultaneously considers exploration, efficiency, and semantic aspects of decision-making, balancing the priority among these factors becomes challenging.To mitigate potential conflicts, we have implemented a hierarchical structure within the reward system.Assuming there are a total of n neighbor nodes, the cumulative reward vector W  R n is the sum of exploration reward vector P, efficiency reward vector C and semantic reward vector L. The next navigation point selection can be formulated as follows:
arg max S W T S s.t. S  E n (4)
where E n = {e 1 , e 2 , . . ., e n } is the standard orthogonal basis composed of n-dimensional coordinate vectors.The decision variable S = e i if i th neighbor node is selected for next navigation waypoint.The reward vectors are defined as follows:
               W = P + C + L, P = 2 ( 1 e 1 + . . . +  n e n ) , C =  1 e 1 + . . . +  n e n , 0  L  1,  i ,  i  {0, 1}, W, P, C, L  R n . (5)
where the i th dimensional component of L is the semantic score of the i th neighbor node provided by LLM within the interval (0, 1).The binary coefficient  i  {0, 1} stands for whether the i th neighbor node is traversed through by exploratory paths ( i = 1) or not ( i = 0), and  i  {0, 1} denotes whether the i th neighbor node is covered by historical trajectories ( i = 0) or not ( i = 1).We establish the hierarchy of priorities for each aspect by assigning different reward weights of the reward vectors as shown in Equation (5) (1 st : Exploration; 2 nd : Efficiency; 3 rd : Semantic).</p>
<p>We select the neighbor node that offers the highest cumulative reward as the next target waypoint for navigation.If the agent's vision model identifies the target object G while exploring, the Semantic Mapping module will map the target's point cloud onto the existing map M, enabling direct path planning toward the target's location.</p>
<p>The Local Policy Module</p>
<p>Given the agent's pose, obstacle map, and target point, we use the Fast Marching Method (Sethian, 1996) to find the shortest path from the current position to the target, which is composed of a sequence of discrete points in the map.</p>
<p>The nearest coordinate on this shortest path is selected as an immediate navigation objective for executing actions such as moving forward or turning.Once arriving at a Voronoi node, the agent will rotate and repeat the selection of the mid-term goal.</p>
<p>Experiments</p>
<p>In order to assess the navigation capability and exploration efficiency of VoroNav, we carry out extensive experiments on two representative datasets: HM3D and HSSD datasets.</p>
<p>Baselines and Metrics</p>
<p>Datasets.The HM3D dataset provides 20 high-fidelity reconstructions of entire buildings and contains 2K validation episodes for object navigation tasks.The HSSD dataset provides 40 high-quality synthetic scenes and contains 1.2K validation episodes for object navigation.</p>
<p>Metrics.We adopt Success rate (Success) and Success weighted by Path Length (SPL) as the evaluation metrics (Anderson et al., 2018), which are defined as follows:</p>
<p> Success represents the percentage of successful episodes to the total number of episodes. SPL quantifies the agent's mobility efficiency in goaloriented navigation by calculating the inverse ratio of the actual path length traversed to the optimal path length weighted by success rate.</p>
<p>Baselines.We conduct comparative evaluations of VoroNav and several representative baseline planners, including:</p>
<p> Random Exploration drives the robot to march to randomly sampled points in unexplored areas. Frontier (Yamauchi, 1997) is an exploration method that selects the nearest boundary points of unexplored areas and unoccupied areas as the mid-term goals. Voronoi corresponds to VoroNav but without considering the semantic reward. L3MVN (Yu et al., 2023b) is a region-oriented navigation method that leverages LLM to select the optimal midterm waypoint by evaluating regions clustered by the frontier points. Pixel-Nav (Cai et al., 2023) is image-based zero-shot navigation that analyzes panoramic images and utilizes LLM to determine optimal pixel for exploration. ESC (Zhou et al., 2023) is a pioneering method employing an LLM to determine the mid-term goal from the frontier points during exploration.</p>
<p>Note that Random Exploration, Frontier, and Voronoi methods only utilize topological information of the map for planning, while L3MVN, Pixel-Nav, and ESC additionally require semantic information.1, our approach outperforms the bestperforming competitor (+2.8% Success and +3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD).As expected, the Random Exploration method suffers from the blind exploration strategy, resulting in a high likelihood of targeting the wrong areas and walking back and forth during exploration.The Frontier method consistently pursues the closest unexplored boundary, resulting in a more rapid exploration compared to the Random Exploration.Yet, the Frontier method suffers from relatively low efficiency as it fails to prioritize rapidly locating the target at the perception level.The Voronoi method shares similarities with the Frontier method in search for the nearest unexplored point.However, Voronoi enhances the navigation process by proceeding to informative neighbor nodes along the RVG paths, thereby pursuing to uncover larger areas with very few steps.Both L3MVN and ESC adopt the frontier exploration strategy and leverage an LLM to select appropriate frontier points, whereas Pixel-Nav makes decisions after a fixed number of steps, utilizing LLMs to predict the direction with the highest probability of leading to the target, and subsequently employing an RGB-based policy to plan a route and navigate accordingly.These three semantic planning methods uniformly make decisions at predetermined intervals, which can lead to agents determining the mid-term goal in suboptimal positions with insufficient observations, thereby failing to fully unleash the reasoning power of LLM.Our VoroNav method further improves the navigation process while using scene descriptions of broader observations to assist LLM in reasoning and decision-making, thereby achieving better performance.</p>
<p>Figure 5 illustrates a successful episode of VoroNav navigating to the target with the help of the RVG and LLM.It visualizes the observations and the environments at four key global decisions in this episode, and the details of a representative LLM decision-making process.</p>
<p>ABLATION STUDY</p>
<p>To manifest the contribution of each module, we compare VoroNav with three ablation models on both HM3D and HSSD datasets.The Voronoi method keeps the same settings as in Section 4.1.The Voro-path method omits the farsight descriptions in VoroNav and depends entirely on the path descriptions, whereas the Voro-farsight method excludes the path descriptions in VoroNav and depends merely on the farsight descriptions for decision-making.As indicated in Table 2, both Voro-path and Voro-farsight show higher Success and SPL than Voronoi, indicating the benefits of integrating semantic information to augment navigation capabilities.Furthermore, VoroNav exhibits superior performance compared to all ablation models, demonstrating the positive outcomes of integrating both path and farsight descriptions to enhance the performance of LLM's reasoning.</p>
<p>PLANNING STUDY</p>
<p>To verify that Voronoi-based methods are more suitable for visual navigation compared to Frontier-based methods, we introduce two metrics to evaluate the planning effect of each method: the Success weighted by Collision Avoidance (SCA) and the Success weighted by Explored Area (SEA).</p>
<p>The metric SCA quantifies the proportion of non-collision steps to the total number of agent forward steps weighted by</p>
<p>Exploration Approaching</p>
<p>Now, you have 3 areas to go.l Area 1: there is a dining room with chairs and table , you will pass by chair and table on the way.l Area 2: there is a living room with sofa and clock and stairs, you will pass by sofa on the way.l Area 3: there is a kitchen with an oven and fridge.</p>
<p>Please estimate the probability of "bed" appearing at these areas in [0,1].success rate, manifesting the tendency of obstacle avoidance in navigation, which is defined as follows:
SCA = 1 N N i=1 S i 1   C i F i , (6)
where N is the total number of episodes, the binary variable S i  {0, 1} indicates whether the i th episode is successful (S i = 1) or not (S i = 0).The discount factor  is predetermined and varies across datasets.We set  = 0.1 in HM3D and  = 1 in HSSD.The variables C i and F i represent the number of collisions and forward steps in the i th episode.</p>
<p>The metric SEA measures the normalized ratio of the observed region to the path length weighted by success rate, denoting the efficiency of perceiving the surrounding environment, which is defined as follows:
SEA = 1 N N i=1 S i   A i L i ,(7)
where A i and L i signify the area of the explored region and path length in the i th episode, respectively.The discount factor  is also predetermined, and we set  = 0.002 in HM3D and  = 0.01 in HSSD.</p>
<p>As demonstrated in Table 3, we find that the SCA and SEA of Voronoi-based methods (Voronoi and VoroNav) significantly surpass those of Frontier-based methods (Frontier and L3MVN).The higher SCA score suggests that, throughout the exploration process, the mid-term goals of the Voronoibased methods are typically chosen at intersections within unoccupied regions, which are less likely to be in proximity to obstacles, thereby reducing the incidence of collisions compared to the frontier-based methods.Specifically, if the agent gets too close to obstacles during navigation, it is easy to encounter blind spots in perceiving the surrounding environment, which can lead to collisions with undetected obstacles that lie within these blind spots during subsequent movement.In contrast, our method involves less movement</p>
<p>Conclusion</p>
<p>We have presented the VoroNav framework that explores a novel form of graph representation for navigation space and substantially enhances ZSON by using a structured graphbased exploration strategy.Our approach circumvents the limitations of traditional end-to-end and map-based methods by generating informative waypoints and representing the environment with an innovative fusion of text information.The RVG generation module, together with the use of GPT-3.5 for decision-making, leads to more strategic navigation and efficient exploration.By making use of LLM and topologically structured scene graphs, VoroNav sets a new benchmark for ZSON and opens up new pathways for intelligent robotic systems to interact with environments.</p>
<p>Impact Statements</p>
<p>VoroNav, in addressing the challenges of autonomous navigation in robotics, has profound implications for the future of household robotics and AI-driven navigation systems.By leveraging the synergy of semantic mapping and LLMs, it promises enhanced efficiency and effectiveness in robotic navigation tasks, potentially transforming how machines interact with and understand their environments.</p>
<p>A. Method Details</p>
<p>A.1. RVG Generation</p>
<p>Given the obstacle and explored maps, we can obtain the unoccupied map by logically subtracting the obstacle map from the explored map, representing the traversable areas within the observed regions.We then preprocess the unoccupied map by using the morphology methods ( Van der Walt et al., 2014) to fill holes and smooth boundaries.To obtain the GVD, we extract a set of Voronoi points V by skeletonizing the unoccupied map (Figure 6 (a)).Subsequently, to manifest the connectivity and accessibility of the unoccupied map, the GVD can be processed into RVG G, a graph form with nodes V and edges E (Figure 6 (b)).The nodes correspond to GVD points that are either at intersections or on the endpoint of GVD, while the segments directly connecting two adjacent nodes are identified as edges.The raw graph is then preprocessed through operations such as merging proximate nodes and eliminating trivial forks.</p>
<p>A.2. Reward Roles</p>
<p>To illustrate the robustness of VoroNav in various challenging scenarios and the roles played by distinct reward vectors, we have visualized a worst-case navigation example and the details of cumulative rewards, as shown in Figure 7.For exploration purposes, only neighbor nodes traversed along the exploratory path indicate its heading toward unexplored areas, which is foremost for the exploration process (refer to Figure 7 (c)); Concerning efficiency, we rely on the agent's historical decisions, considering the current agent node as the optimal choice from previous decisions and discouraging turning back.If there is one or more extensions from the agent node leading to unexplored areas, the agent is inclined to continue its ongoing exploration (refer to Figure 7 (a)).Conversely, if no extension offers exploratory paths, it indicates that unexplored regions exist elsewhere, and historically traversed nodes will be revisited, prompting the agent to return to previously traveled paths (refer to Figure 7 (b)).Thus, the agent prioritizes exploration and efficiency from topological perspectives in the navigation.In cases where multiple nodes hold equivalent exploration and efficiency rewards, the agent will proceed to the neighbor node where the target object is more likely to be found, as indicated by the higher predicted semantic probability (refer to Figure 7 (a)).</p>
<p>A.3. Navigation Process</p>
<p>A complete process of a navigation episode is illustrated in Algorithm 1.The code snippet of the LookAround proce-dure, as presented in Algorithm 1, is further elaborated in Algorithm 2. At the beginning of each episode, the subgoal is initially empty.At each step, the agent updates the semantic map of its surroundings and the RVG accordingly (Line 4-8).If the agent detects the target object at any time, it will immediately plan a direct route to approach the object (Line 9-10).Conversely, if the target remains undetected, the agent performs a complete rotation to establish a preliminary RVG scene representation (Line 13, Algorithm 2).The agent then navigates to the closest RVG node (Line 14).Upon reaching the RVG node or the mid-term goal(Line 16), the agent rotates a full circle again (Line 17, Algorithm 2), derives the exploratory paths (Line 18) and surrounding images (Line 19), generates corresponding descriptions of paths (Line 20-21) and farsight (Line22) integrated with the respective neighbor nodes.A large language model is then employed to evaluate the fused descriptions of each neighbor node, obtaining semantic rewards based on the results of scene reasoning (Line 23).Concurrently, the agent acquires exploration rewards (Line 24) and efficiency rewards (Line 25) on the neighbor nodes by analyzing the layout of unexplored areas and historical trajectories.The neighbor node with the highest cumulative reward will be selected as the optimal mid-term goal point for exploration (Line 26).</p>
<p>Finally, low-level motion planning is utilized to devise a sequence of actions targeting the mid-term goal (Line 29).</p>
<p>If the agent reaches the vicinity of the identified target or takes actions surpassing the maximum number of steps, it will immediately issue a "Stop" action and the episode ends.</p>
<p>B. Experiment Details</p>
<p>B.1. Experiment Setup</p>
<p>Evaluations on HM3D follow settings outlined in the Habitat ObjectNav challenge 2022 (Yadav et al., 2022).Evaluations on HSSD adopt the same validation parameters utilized in (Khanna et al., 2023).In both datasets, the agent is a LoCoBot (Gupta et al., 2018) with a base radius of 0.18m, outfitted with an RGB-D camera mounted at a height of 0.88 meters and a pose sensor that provides accurate localization.</p>
<p>The camera has a 79  Horizontal Field of View (HFoV) and frame dimensions of 480  640 pixels.The agent's action space is {Stop, MoveForward, TurnLeft, TurnRight, LookUp, LookDown}, with a discrete movement increment of 0.25m and discrete rotations of 30  .The object goal categories in episodes include "bed", "chair", "sofa", "tv", "plant", and "toilet".
M t  Mapping(M t1 , O t , ObjectMasks) 7: V  Skeletonize(M t ) 8: G  (V, E)  Filter(M t , V) 9: if G exists</p>
<p>B.2. Failure Case Study</p>
<p>We have collected the reasons and frequencies of failures across all episodes, categorizing them into detection failures, planning failures, and exploration failures.Detection failure occurs when the agent mistakenly identifies non-target items as targets or overlooks the actual targets within the agent's field of view.Planning failure arises when the agent gets stuck or fails to navigate to the target location despite having accurately detected the target.Exploration failure is attributed to the situation where the agent has not encountered the target within the assigned maximum number of steps.As shown in Figure 8, we find that most failures are caused by incorrect detection in HM3D and by abnormal planning in HSSD.</p>
<p>B.3. Ground-truth Vision Experiments</p>
<p>To eliminate the impact of detection errors during navigation and analyze the planning and exploration effects of navigation methods, we uniformly replace the RGB images across all methods with ground-truth semantic images.As shown in Table 4, when semantic priors of reasoning and planning are absent, the Voronoi method exhibits consid-erably superior performance in terms of both Success and SPL compared to the Frontier method.This enhancement in performance proves that within the topological domain, the Voronoi planning approach is more suitable for goaloriented navigation than the Frontier exploration method.</p>
<p>With the guidance of LLM, VoroNav outperforms L3MVN by a large margin, further demonstrating that VoroNav's information processing and decision-making are more effective for robot navigation with lower step cost and a higher success rate compared to L3MVN.</p>
<p>B.4. Prompt Template</p>
<p>In Figure 9 and Figure 10, we present the specific prompt templates for path description generation and decision-making.The red font in the prompt templates refers to the parts that vary according to different scenarios.</p>
<p>You are given the objects and coordinates appearing along a path, you should guess the type of room the path passes through and describe them like "You will pass by {room type} with {objects}".The objects and coordinates are: [Objects and coordinates] You need to describe the scene along this path with a sentence.You have areas to go: [Area: farsight descriptions and path descriptions] Please estimate the probability of [Target] appearing at these areas for me in [0,1] and explain the reasons.</p>
<p>Reply with the JSON form :{{"area_number": $area_number, "probability": $probability,"reason": $reason}}  You will pass by a bedroom with a bed, chair, and TV."area_number": 1, "probability": 0.8, "reason": "The presence of a washer suggests that this area is likely to contain bathroom facilities, including a toilet."</p>
<p>"area_number": 2, "probability": 0.3, "reason": "While it is less likely for a toilet to be in this area as it is a living room, there is still a small chance that a toilet will be located nearby, perhaps in an adjacent bathroom."</p>
<p>"area_number": 3, "probability": 0.9, "reason": "The presence of a bedroom with a bed and dresser suggests that this area is likely to contain an attached or nearby bathroom, making it highly probable for a toilet to be present."</p>
<p>Area 1 : there is a room with a bed, a washer and a lamp; Area 2 : there is a picture of a living room with a flat screen tv, and you will pass by a living room with a couch and a TV; Area 3 : there is a view of a bedroom with a bed, dresser, and television, and you will pass by a bedroom with a TV and a bed;</p>
<p>Area 1 : there is a kitchen with a sink and a mirror; Area 2 : there is a table with a potted plant on it in a room; Area 3 : there is view of a living room with a couch, a table, and a television, and you will pass by a living room with a chair and a couch; Area 4 : there is a view of a kitchen with a black refrigerator and a black stove, and you will pass by a kitchen with a refrigerator;</p>
<p>"area_number": 1, "probability": 0, "reason": "Based on the description, this area includes a kitchen with a sink and a mirror, but there is no mention of a television."</p>
<p>"area_number": 2, "probability": 0, "reason": "There is a table with a potted plant in a room, but no mention of a television."</p>
<p>"area_number": 3, "probability": 1, "reason": "The description includes a living room with a couch, a table, and a television, indicating a high probability of a television appearing in this area."</p>
<p>"area_number": 4, "probability": 0.3, "reason": "The view includes a kitchen with a black refrigerator and a black stove, and there is a high likelihood of a television appearing in this area as well."</p>
<p>Area 1 : there is a living room with a brick wall and a large window; Area 2 : there is a living room with a dining table and chairs, and you will pass by a living room with a chair; Area 3 : there is a plant in a pot in front of a window; Area 4 : there is a room with a table and chairs and a picture on the wall;</p>
<p>"area_number": 1, "probability": 0.8, "reason": "The living room with a brick wall and a large window is a common area for a couch to be placed for seating and relaxation."</p>
<p>"area_number": 2, "probability": 0.9, "reason": "In a living room with a dining table and chairs, it is likely that there would also be a couch for additional seating and comfort."</p>
<p>"area_number": 3, "probability": 0.5, "reason": "The presence of a plant in a pot in front of a window suggests that there will not be enough space for a couch in this area."</p>
<p>"area_number": 4, "probability": 0.7, "reason": "A room with a table and chairs and a picture on the wall is a common area for a couch to be placed for relaxation and entertainment."</p>
<p>Find a TV Find a couch</p>
<p>Area 1 : there is a dining room with a table and chairs and a television; Area 2 : there is a plant in a vase on a table in a room; Area 3 : there is a living room with a brick wall and a dining table, and you will pass by a room with a chair"; "area_number": 1, "probability": 0.8, "reason": "A dining room with a television is likely to have a couch for seating and relaxation.""area_number": 2, "probability": 0.3, "reason": "The presence of a plant in a vase on a table suggests a decorative focus rather than seating, so the probability of a couch is lower.""area_number": 3, "probability": 0.7, "reason": "A living room with a dining table and a brick wall indicates a space designed for seating and socializing, making it likely to have a couch.""area_number": 1, "probability": 0.1, "reason": "The presence of a bedroom with a bed and a ceiling fan indicates that it is less likely to have a toilet in this area.""area_number": 2, "probability": 0.7, "reason": "The presence of a sink and a mirror in the hallway suggests that it is likely to lead to a bathroom with a sink.""area_number": 3, "probability": 0.3, "reason": "The room with a desk, chair, and a window does not typically indicate the presence of a toilet."</p>
<p>Find a couch Find a toilet</p>
<p>Area 1 : there is a bedroom with a bed and a ceiling fan, and you will pass by a bedroom with a bed; Area 2 : there is a hallway with a sink and a mirror in it, and you will pass by a bathroom with a sink; Area 3 : there is a room with a desk, chair, and a window; Area 1 : there is a chair and a table in a room with a painting on the wall; Area 2 there is a living room with a table and a fan; Area 3 : there is a bedroom with a bed, dresser, mirror and a television, and you will pass by a bedroom with a potted plant and a bed "area_number": 1, "probability": 0.1, "reason": "This area seems to be more of a common sitting area or a study room with a chair and table, and there is no mention of a bathroom or plumbing facilities.""area_number": 2, "probability": 0.05, "reason": "This area seems to be a living room with a table and a fan, and living rooms typically do not have toilet facilities.""area_number": 3, "probability": 0.4, "reason": "This area is a bedroom with a bed, dresser, mirror, and a television, and bedrooms are commonly adjacent to bathrooms.Additionally, the presence of a potted plant often suggests the proximity of a bathroom.Thus, the probability is relatively high."</p>
<p>Area 1 : there is a 3d image of a staircase in a room, and you will pass by a living room with potted plant, chair, and TV; Area 2 : there is a picture of a room with a toilet and a plant; Area 3 : there is a plant that is sitting in a vase on a table; Area 4 : there is a living room with a couch, chair, and a television, and you will pass by a lounge with couch and chair; "area_number": 1, "probability": 0, "reason": "The 3d image of a staircase does not typically include a toilet, and the adjacent living room would not likely have a toilet.""area_number": 2, "probability": 1, "reason": "The picture of a room with a toilet makes it highly probable for a toilet to appear in this area.""area_number": 3, "probability": 0, "reason": "A plant sitting in a vase on a table does not typically indicate the presence of a toilet in this area.""area_number": 4, "probability": 0, "reason": "The rendering of a living room and a lounge with a couch and chair do not typically include a toilet."</p>
<p>Find a toilet Find a toilet</p>
<p>Area 1 : there is a bed with two pillows and a picture on the wall; Area 2 : there is a bathroom with a toilet and a sink with a hole in the wall; Area 3 : there is a white closet with a mirror on the wall, and you will pass by a bedroom with a bed; "area_number": 1, "probability": 0.8, "reason": "It's common for bedrooms to have a chair or seating area for relaxation or reading, and the presence of a bed and pillows suggests that a chair will be present for comfort or utility.""area_number": 2, "probability": 0.2, "reason": "Bathrooms usually do not have chairs due to space constraints and moisture, and the presence of a toilet and sink with a hole in the wall does not indicate the presence of a chair.""area_number": 3, "probability": 0.6, "reason": "Closets are not typical places for chairs, but the presence of a mirror and bed suggests that it could be a dressing area where a chair will be present for dressing or putting on shoes."</p>
<p>Find a chair Find a toilet</p>
<p>Find a coffee-table.</p>
<p>Figure 1 .
1
Figure 1.Voronoi-based Navigation with LLM.Our model focuses on optimizing the decision-making process in ZSON.It enables the agent to pinpoint intersections rich in observation on the map by Voronoi sparsification, which act as navigation waypoints.The agent perceives the environment at intersections, collects scene information from nearby waypoints, and performs reasoning guided by LLM to ascertain the most plausible waypoint leading to the desired target.The five images presented in (a) depict the agent's corresponding perspectives as it faces five adjacent navigation waypoints at the intersection illustrated in (b), with the indices showing the correspondence.</p>
<p>"</p>
<p>Find a couch"  !: there is a livingroom with a bed  " : there is a bathroom with a bathtab.</p>
<p>Figure 2 .
2
Figure 2. Components of VoroNav.VoroNav includes three modules.Perceptual inputs include RGB-D images and real-time pose, while the output of the agent is "Action".The RGB-D and pose observation are processed by the Semantic Mapping Module (light blue module) to form a semantic map.The Global Decision Module (light yellow module) generates RVG, which is used to produce textual descriptions of surrounding neighbor nodes and exploratory paths.This module then employs an LLM to assist in selecting the promising neighbor node as a mid-term goal by inferring the fused prompt of scene descriptions.The Local Policy Module (light green module) plans the low-level actions of the agent to reach the target point.</p>
<p>Figure 3 .
3
Figure 3. Commonsense Reasoning with LLM.(a) LLM analyzes the objects and their coordinates that appear on the path and depicts the scene along the path.(b) LLM predicts the probability of the target object appearing in each area by comprehending the fused text descriptions of the scene.</p>
<p>Figure 4 .
4
Figure 4. Farsight Image Captioning.The agent selects all RGB images that capture the views of neighbor nodes and uses BLIP to generate captions of these images.</p>
<p>Figure 5 .
5
Figure 5. Simulation Experiments.Utilizing LLM, the agent explores efficiently, discovers the target with a minimal path cost, and finally navigates to the target object with success.In this figure, we visualize the RGB images and semantic maps of the four global decision instances, and the dialog box on the left exhibits the conversation between the agent and LLM in the first global decision process.</p>
<p>FilterFigure 6 .
6
Figure6.The Voronoi Processing Module.In (a), we skeletonize the areas that are not occupied by obstacles in the explored area and obtain the GVD (blue lines).In (b), the nodes and edges are extracted in GVD to form the RVG.The agent nodes (robot icon), neighbor nodes (red circles), ordinary nodes (green circles), and exploratory nodes (orange circles) are filtered by the location of the nodes.In (c), the exploratory paths (blue arrows) are generated by searching for the shortest paths on the GVD from the agent node to the exploratory nodes.</p>
<p>Figure 7 .
7
Figure 7.A Worst-case Example.This figure depicts the exploration process on RVG and the cumulative rewards of neighbor nodes when LLM alone makes undesirable decisions.(a) Three neighbor nodes are traversed by exploratory paths, one of which has been passed by the agent, and the agent compares LLM scores between the other two to make a choice.(b) Two nodes have not been passed by the agent but are not traversed by exploratory paths, and another node that the agent has passed by but with an exploratory path traversed is selected as a mid-term goal.(c) Three neighbor nodes have all been passed by the agent, but one of them leads to an unexplored area and is selected as a mid-term goal.(d) The agent finds the target object after exploration.</p>
<p>C</p>
<p> Efficiency(M t , O t , V ) 26:SubGoal  Decision(V, P, C, L)</p>
<p>Figure 8 .
8
Figure 8. Failure Cases Percentage.In the failure statistics on HM3D and HSSD datasets, our VoroNav experiences the fewest planning failures.</p>
<p>Figure 9 .
9
Figure 9. Prompt Template for Generating Path Description.</p>
<p>Figure 10 .
10
Figure 10.Prompt Template for LLM Decision.</p>
<p>Figure 11 .
11
Figure 11.Examples of Path Description Generation.</p>
<p>Figure 12 .
12
Figure 12.Examples of LLM Decision.</p>
<p>Figure 13 .
13
Figure 13.Examples of LLM Decision.</p>
<p>For example, Zhou et al. (2023)use LLM to predict the degree of correlation with the target object at the object level and the room level to infer where is the most likely location of the target object.Yu et al.
to the target object. Cai et al. (2023) cluster panoramicimages into scene nodes by LLM, use CoT of LLM to deter-mine whether exploration or exploitation, select the imagewith the highest likelihood of finding the target object, andnavigate accordingly based on the chosen image. Yu et al.(2023a) apply the decision-making of LLM for multi-robotcollaborative navigation, and the LLM centrally plans themid-term goal for each robot by extracting information suchas obstacles, frontiers, object coordinates, and robot statesfrom online maps.(2023b) generate clusters of unexplored areas by frontiers,and leverage LLM to infer the correlation between the tar-get object and the objects contained within each cluster tonavigate to the scene closer to the target object. Gadre et al.(2023) adopt LLM to provide prior information at the objectlevel to assist in target object localization. Shah et al. (2023)feed chain-of-thought(CoT) into LLM for navigation thatencourages exploration of areas with higher relevance whileconcurrently avoiding moving to areas that are unrelated</p>
<p>Table 1 .
1
Comprison with ZSON Baselines.Our proposed VoroNav outperforms the ZSON baselines on both HM3D and HSSD.To guarantee the zero-shot navigation capability of each method, we use Grounded-SAM to replace the vision modules of methods marked by an asteroid ( * ), which aligns with our model.
MethodPlannerTraining-freeLLMHM3DHSSDSuccess SPL Success SPLRandom Exploration  <em>Random-26.59.230.212.7Frontier (Yamauchi, 1997)  *  Voronoi  </em>Topological --33.7 38.715.3 23.336.0 40.317.7 22.2L3MVN (Yu et al., 2023b)  *GPT-235.216.538.419.4Pixel-Nav (Cai et al., 2023) ESC (Zhou et al., 2023)Semantic GPT-4 GPT-3.537.9 39.220.5 22.3----VoroNav (Ours)GPT-3.542.026.041.023.24.2. Results and Analysis4.2.1. COMPARISON WITH SOTA METHODSAs shown in Table</p>
<p>Table 2 .
2
Ablation Study.We compare VoroNav with three abla-
tion models: (1) Voronoi: Voronoi-based navigation without anysemantic guidance; (2) Voro-path: VoroNav that eliminates thefarsight descriptions; (3) Voro-farsight: VoroNav that eliminatesthe path descriptions.MethodHM3DHSSDSuccess SPL Success SPLVoronoi38.723.340.322.2Voro-path40.024.240.623.0Voro-farsight41.225.240.822.7VoroNav42.026.041.023.2</p>
<p>Table 3 .
3
Planning Study.We analyze the planning capability of different methods by comparing SCA and SEA.Our VoroNav method achieves the highest SCA and SEA scores, indicating advanced capability for obstacle avoidance and low-cost exploration.
MethodHM3DHSSDSCA SEA SCA SEAFrontier24.217.435.516.5Voronoi29.417.940.218.6L3MVN27.517.737.616.6VoroNav39.820.940.919.3near obstacles during exploration, allowing for wider unob-structed views and thus more comprehensive observations,resulting in safer navigation. Similarly, a higher SEA scoreindicates that the Voronoi-based methods favor intersectionsrich in information, enabling broader areas to be observedwith minimal movement. The enhanced perceptual rangeincreases the probability of the agent discovering the targetdirectly. Meanwhile, this strategy yields potentially morevaluable scene hints generated by comprehensive percep-tion to inform LLM's decision-making, thus heightening thechances of locating the target object.</p>
<p>Algorithm 1 Navigation Process of VoroNav 1: Input: Target object G 2: Initialize: Initial observation O 0   Initial semantic map M 0   Step Number t  1 SubGoal  None 3: while Episode is done do
4:O t  O t1 {p t , I t }5:ObjectMasks  GroundedSAM(I t )6:</p>
<p>in M t then NeighborImages  Select(I t , M t , V )
10:SubGoal  Location(M t , G)11:else12:if SubGoal is None then13:LookAround;14:SubGoal  Nearest(p t , V )15:end if16:if Agent reaches node in V then17:LookAround18:P  Search(p t , G)19:</p>
<p>Table 4 .
4
Ground-truth Semantics.We find that after replacing the Grounded-SAM with ground truth semantic segmentation, our VoroNav still achieves the best performance, revealing the superiority of our planning module.
MethodHM3DHSSDSuccess SPL Success SPLFrontier63.533.051.220.6Voronoi67.337.457.232.7L3MVN65.536.558.028.0VoroNav67.640.559.734.1HM3DHSSD100%7.27.966.6Failure Cases Percentage80% 60% 40% 20%43.9 26.4 29.746.6 24.2 29.244.2 23.6 32.249.2 22.3 28.528.9 63.928.3 63.832 6232.1 61.3 Exploration Planning Detection0%Frontier Voronoi L3MVN VoroNav Frontier Voronoi L3MVN VoroNav
The process for generating RVG is detailed in Appendix A.1</p>
<p>Zero experience required: Plug &amp; play modular transfer learning for semantic visual navigation. Z Al-Halah, S K Ramakrishnan, K Grauman, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>D An, H Wang, W Wang, Z Wang, Y Huang, K He, L Wang, Etpnav, arXiv:2304.03047Evolving topological planning for vision-language navigation in continuous environments. 2023arXiv preprint</p>
<p>On evaluation of embodied navigation agents. P Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, arXiv:1807.067572018arXiv preprint</p>
<p>Language is a complex adaptive system: Position paper. C Beckner, R Blythe, J Bybee, M H Christiansen, W Croft, N C Ellis, J Holland, J Ke, D Larsen-Freeman, T Schoenemann, Language Learning. 592009</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Bridging zero-shot object navigation and foundation models through pixel-guided navigation skill. W Cai, S Huang, G Cheng, Y Long, P Gao, C Sun, H Dong, arXiv:2309.103092023arXiv preprint</p>
<p>Object goal navigation using goal-oriented semantic exploration. D S Chaplot, D P Gandhi, A Gupta, R R Salakhutdinov, Advances in Neural Information Processing Systems. 202033</p>
<p>Zeroshot object searching using large-scale object relationship prior. H Chen, R Xu, S Cheng, P A Vela, D Xu, 2023a</p>
<p>How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers. J Chen, G Li, S Kumar, B Ghanem, F Yu, 2023b</p>
<p>Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation. S Y Gadre, M Wortsman, G Ilharco, L Schmidt, S Song, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Navigating to objects in the real world. T Gervet, S Chintala, D Batra, J Malik, D S Chaplot, Science Robotics. 87969912023</p>
<p>Robot learning in homes: Improving generalization and reducing dataset bias. Advances in neural information processing systems. A Gupta, A Murali, D P Gandhi, L Pinto, 201831</p>
<p>Incremental reconstruction of generalized voronoi diagrams on grids. N Kalra, D Ferguson, A Stentz, Robotics and Autonomous Systems. 5722009</p>
<p>Simple but effective: Clip embeddings for embodied ai. A Khandelwal, L Weihs, R Mottaghi, A Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for objectgoal navigation. M Khanna, Y Mao, H Jiang, S Haresh, B Shacklett, D Batra, A Clegg, E Undersander, A X Chang, M Savva, 2023</p>
<p>. A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, P Dollr, R Girshick, arXiv:2304.026432023Segment anything</p>
<p>Beyond the nav-graph: Vision-and-language navigation in continuous environments. J Krantz, E Wijmans, A Majumdar, D Batra, S Lee, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 202016</p>
<p>Waypoint models for instruction-guided navigation in continuous environments. J Krantz, A Gokaslan, D Batra, S Lee, O Maksymets, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Renderable neural radiance map for visual navigation. O Kwon, J Park, S Oh, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Language communities, corpora, and cognition. H.-L Lai, K Chui, W.-H Sah, S.-F Chung, C.-L Liu, Big Data in Computational Social Science and Humanities. 2018</p>
<p>Bootstrapping language-image pre-training for unified vision-language understanding and generation. J Li, D Li, C Xiong, S Hoi, Blip, International Conference on Machine Learning. PMLR2022</p>
<p>Improving autonomous exploration using reduced approximated generalized voronoi graphs. L Li, X Zuo, H Peng, F Yang, H Zhu, D Li, J Liu, F Su, Y Liang, G Zhou, Journal of Intelligent &amp; Robotic Systems. 992020</p>
<p>Revolt: Relational reasoning and voronoi local graph planning for target-driven navigation. J Liu, J Guo, Z Meng, J Xue, arXiv:2301.023822023aarXiv preprint</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. S Liu, Z Zeng, T Ren, F Li, H Zhang, J Yang, C Li, J Yang, H Su, J Zhu, arXiv:2303.054992023barXiv preprint</p>
<p>Zero-shot object-goal navigation using multimodal goal embeddings. A Majumdar, G Aggarwal, B S Devnani, J Hoffman, D Batra, Zson, Advances in Neural Information Processing Systems. A H Oh, A Agarwal, D Belgrave, K Cho, 2022</p>
<p>H Naveed, A U Khan, S Qiu, M Saqib, S Anwar, M Usman, N Barnes, A Mian, arXiv:2307.06435A comprehensive overview of large language models. 2023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, 2022</p>
<p>Zero-shot active visual search (zavis): Intelligent object search for robotic assistants. J Park, T Yoon, J Hong, Y Yu, M Pan, S Choi, 10.1109/ICRA48891.2023.101613452023 IEEE International Conference on Robotics and Automation (ICRA). 2004-2010, 2023</p>
<p>S K Ramakrishnan, A Gokaslan, E Wijmans, O Maksymets, A Clegg, J Turner, E Undersander, W Galuba, A Westbury, A X Chang, arXiv:2109.08238Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. 2021arXiv preprint</p>
<p>Potential functions for objectgoal navigation with interaction-free learning. S K Ramakrishnan, D S Chaplot, Z Al-Halah, J Malik, K Grauman, Poni, Computer Vision and Pattern Recognition (CVPR), 2022 IEEE Conference on. IEEE2022</p>
<p>A fast marching level set method for monotonically advancing fronts. J A Sethian, proceedings of the National Academy of Sciences. 9341996</p>
<p>Navigation with large language models: Semantic guesswork as a heuristic for planning. D Shah, M Equi, B Osinski, F Xia, B Ichter, S Levine, 7th Annual Conference on Robot Learning. 2023</p>
<p>S Van Der Walt, J L Schnberger, J Nunez-Iglesias, F Boulogne, J D Warner, N Yager, E Gouillart, T Yu, scikit-image: image processing in python. 20142e453</p>
<p>. K Yadav, S K Ramakrishnan, J Turner, A Gokaslan, O Maksymets, R Jain, R Ramrakhya, A X Chang, A Clegg, M Savva, E Undersander, D S Chaplot, D Batra, Habitat challenge. 2022</p>
<p>A frontier-based approach for autonomous exploration. B Yamauchi, Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97.'Towards New Computational. 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97.'Towards New Computational</p>            </div>
        </div>

    </div>
</body>
</html>
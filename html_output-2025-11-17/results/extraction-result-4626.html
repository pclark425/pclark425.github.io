<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4626 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4626</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4626</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-270562219</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.12288v3.pdf" target="_blank">An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown strong arithmetic reasoning capabilities when prompted with Chain-of-Thought (CoT) prompts. However, we have only a limited understanding of how they are processed by LLMs. To demystify it, prior work has primarily focused on ablating different components in the CoT prompt and empirically observing their resulting LLM performance change. Yet, the reason why these components are important to LLM reasoning is not explored. To fill this gap, in this work, we investigate ``neuron activation'' as a lens to provide a unified explanation to observations made by prior work. Specifically, we look into neurons within the feed-forward layers of LLMs that may have activated their arithmetic reasoning capabilities, using Llama2 as an example. To facilitate this investigation, we also propose an approach based on GPT-4 to automatically identify neurons that imply arithmetic reasoning. Our analyses revealed that the activation of reasoning neurons in the feed-forward layers of an LLM can explain the importance of various components in a CoT prompt, and future research can extend it for a more complete understanding.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4626.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4626.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FF-neuron subupdates</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feed-Forward Layer Neurons (sub-update components)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The feed-forward (FF) layer decomposes into dm static vectors v_{l,j} whose input-dependent scalar coefficients m_{l,ij} produce additive 'sub-updates' to token representations; projecting these sub-updates to vocabulary space often reveals human-interpretable concepts (e.g., '+' or 'and'). The paper hypothesizes these neurons as the locus of arithmetic/conceptual processing for multi-step arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B (primary analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (Llama2-7B) used in experiments; standard multi-layer transformer with feed-forward blocks parameterized by K_l and V_l matrices producing dm sub-updates f(K_l x) V_l per token; analysis focused on neurons v_{l,j} in FF layers and their activation coefficients m_{l,ij}.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Grade-school multi-step arithmetic (GSM8K) including addition, subtraction, multiplication, division and multi-step reasoning chains</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Arithmetic is mediated by activation of FF 'concept' neurons which produce interpretable sub-updates (projected to vocabulary) that represent arithmetic operators, numbers, logical connectors, etc.; attention provides traversal while FF neurons supply processing; persistent activation across nearby timesteps supports multi-step computation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Automated neuron discovery (project top promoted tokens per neuron, filter with seed tokens, GPT-4 annotation) found neurons mapping to arithmetic concepts; activation patterns (arithmetic neurons spike during equations/numbers; logic neurons at BOS); positive correlation between average reasoning-neuron coefficient and prompt accuracy; ablation (Gaussian noise added to discovered reasoning neurons) substantially reduced GSM8K accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Activation of reasoning neurons is necessary but not sufficient: CoT prompts with OOD/incorrect labels can show similar reasoning-neuron activations yet much worse accuracy; superficial token counts do not correlate with accuracy, indicating neuron activation alone misses inter-neuron interactions and attention/circuit effects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported Llama2-7B GSM8K few-shot CoT accuracies: CoT 16.83%, w/o Equation 12.58%, w/o Textual Explanation 13.41%, AddOnly 13.26%, MultOnly 13.13%, Incorrect Label 16.45%, OOD Label 7.58%. Zero-shot CoT prompt examples: 7.05%, 4.47%, 11.06%, 5.83% for four different prompts (plotted vs average coefficient).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Neuron discovery: stored top-T activated neurons across decoding steps (T=10 per step), projected to vocabulary (top-P tokens, P=20), filtered by seed tokens (F>=2), then annotated by GPT-4 (~1300 prompts). Intervention: random-noise ablation (add Gaussian noise to neuron vectors v_{l,j}) on discovered reasoning neurons reduced accuracy from 16.83% to 4.54%; corrupting same number of random neurons reduced accuracy to 11.37%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Neurons can be polysemantic (promote multiple concepts) and activations can be 'faked' by surface token sequences; activation analysis ignores interactions (attention, circuits), so it does not fully explain reasoning or in-context learning; similar activation patterns can occur with incorrect/OOD labels producing flawed reasoning; analysis limited to seven pre-defined concepts; results derived primarily from Llama2-7B.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Paper reports preliminary application to Llama3-8B where similar reasoning neurons were discoverable but model-level behavior differed (see separate entry). Prior work (Stolfo et al.) suggested attention enables information traversal while FF handles computation; Geva et al. showed FF sub-updates project to interpretable tokens — this paper extends those ideas to arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4626.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4626.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-7B arithmetic behavior</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama2-7B arithmetic reasoning (CoT analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical evaluation and mechanistic analysis of Llama2-7B on grade-school arithmetic (GSM8K), showing that Chain-of-Thought prompts activate FF neurons associated with arithmetic concepts and that these activations correlate with performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source decoder-only transformer (Llama2 family) with ~7 billion parameters; used with greedy decoding for analyses; feed-forward layers analyzed for neuron activations and projections to vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>GSM8K grade-school multi-step word problems requiring addition, subtraction, multiplication, division and stepwise reasoning (2–8 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Llama2-7B performs arithmetic reasoning via activation of FF layer concept neurons (operator and logic neurons) whose sub-updates accumulate across layers to build intermediate representations for next-token prediction; textual explanations and explicit equations increase activation of these neurons and thus improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Found 113 neurons in Llama2-7B associated with seven arithmetic-related concepts via automated discovery; activation-frequency and average coefficient analyses show more/stronger activations for CoT prompts with equations and textual explanations; positive correlation between average reasoning-neuron coefficient and prompt accuracy; targeted corruption of discovered neurons sharply degrades accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>CoT prompts with OOD labels show similar reasoning-neuron activation counts/coefficients but much lower accuracy, indicating neuron activation alone cannot guarantee correct computation; word-level counts of reasoning tokens do not correlate with performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GSM8K few-shot CoT (Llama2-7B): CoT baseline 16.83%; ablations: w/o Equation 12.58%, w/o Textual Explanation 13.41%, AddOnly 13.26%, MultOnly 13.13%, Incorrect Label 16.45%, OOD Label 7.58%. Neuron-ablation: no corruption 16.83% → corrupted reasoning neurons 4.54% → corrupted random neurons 11.37%.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Automated neuron discovery (top-T activations, projection to vocab, GPT-4 annotation); random-noise ablation to v_{l,j} (Gaussian) applied to discovered reasoning neurons and to random neurons for baseline; comparison of activated-neuron overlaps when labels changed (63.05% overlap for incorrect labels vs 14.91% for OOD labels in encoding steps where labels manipulated).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Low absolute accuracy on GSM8K (CoT 16.83%) indicates limited arithmetic competence at this scale; reasoning neuron activation necessary but not sufficient; polysemantic neurons, persistence but potential compounding errors across steps; analysis restricted to single model and pre-defined concept set.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared qualitatively with Llama3-8B (preliminary): both contain discoverable reasoning neurons, but Llama3-8B shows extreme sensitivity to small random-noise ablations; related prior findings (Stolfo et al.) align on FF involvement and head-mediated traversal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4626.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4626.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-8B fragility</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3-8B preliminary arithmetic neuron findings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Preliminary application of the neuron-discovery pipeline to Llama3-8B found reasoning neurons (≈112) but revealed that the model is extremely sensitive to small random-noise ablations, suggesting different robustness and circuit properties from Llama2-7B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Newer Llama3 family model with ~8 billion parameters (as cited); authors applied the same automated neuron-discovery pipeline to it and report qualitative differences in ablation sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same GSM8K/CoT style arithmetic prompts for neuron discovery and corruption experiments (preliminary).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Llama3-8B also contains FF neurons that project to arithmetic concepts, but its arithmetic behavior appears fragile: small perturbations to a modest number of neurons can throw activations off-distribution and catastrophically reduce performance, implying tighter coupling or distributional brittleness in internal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Discovery found ~112 neurons associated with the seven concepts; applying random-noise ablation to a small number (≈10–20) of random neurons caused accuracy to drop from 45.23% to <2.00%, demonstrating sensitivity of computations to neuron perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Extreme sensitivity to small random noise complicates causal interpretation: perturbations may move activations off-distribution producing artifactual failures rather than revealing specific causal circuits; no full ablation studies analogous to Llama2-7B reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported preliminary baseline accuracy 45.23% (context unspecified in-detail) which collapsed to <2.00% after small random-noise ablation to ~10–20 neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Applied the same GPT-4-guided neuron discovery; observed catastrophic failure under small random-noise ablation; authors note this may be due to off-distribution activations rather than direct evidence of causal necessity of specific neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>High fragility to small perturbations (possible off-distribution effects), limited analysis depth (preliminary), and lack of controlled interventions to isolate circuits — prevents straightforward generalization of mechanistic conclusions from Llama2-7B to Llama3-8B.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasts with Llama2-7B which showed modest, interpretable performance drops when reasoning neurons were corrupted; suggests model-specific circuit structure and robustness differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4626.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4626.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 neuron annotation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-assisted automated neuron annotation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical pipeline that uses GPT-4 to label candidate FF neurons by presenting their top projected vocabulary tokens and asking whether the neuron promotes a predefined concept; used to scale neuron concept annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (used as an annotator, not as arithmetic performer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large language model (OpenAI GPT-4) used interactively to judge whether a neuron's top promoted tokens correspond to a target concept; the paper used ~1300 GPT-4 queries for Llama2-7B neuron annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Not an arithmetic performer here; used to classify neurons into arithmetic-related concept categories (add, sub, mul, div, logical connectors, equals, calculation).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>GPT-4 can reliably interpret short lists of top-promoted tokens from a neuron's projection and decide whether a neuron represents an arithmetic-related concept, enabling automated, scalable neuron discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Authors validated GPT-4 results manually and found no objections; using GPT-4 with seed-token filtering (P=20, F=2) they discovered 113 neurons in Llama2-7B and 112 in Llama3-8B associated with the target concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Costs (∼1300 GPT-4 prompts) and reliance on seed-token filters; GPT-4 may misclassify polysemantic neurons or be sensitive to token translation/noise; filtering and thresholds (P, F) impose heuristic biases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Discovery process required ~1300 GPT-4 prompts for Llama2-7B; yielded 113 neurons for 7 concepts (Llama2-7B) and 112 for Llama3-8B.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Pipeline: collect top-T (T=10) activated neurons across K=20 examples, project neurons to vocabulary (top-P tokens), filter by seed tokens (F>=2), then query GPT-4 with the top promoted tokens asking Yes/No if neuron promotes the concept. Returned 'Yes'/'No' plus explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Annotation depends on the quality of projected top tokens and seed-token heuristics; GPT-4-based judgments may inherit GPT-4 biases; cost and number of prompts scale with model/layer/neuron count; does not substitute for causal circuit analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Analyzing encoded concepts in transformer language models <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Towards understanding chain-of-thought prompting: An empirical study of what matters <em>(Rating: 1)</em></li>
                <li>Complementary explanations for effective in-context learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4626",
    "paper_id": "paper-270562219",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "FF-neuron subupdates",
            "name_full": "Feed-Forward Layer Neurons (sub-update components)",
            "brief_description": "The feed-forward (FF) layer decomposes into dm static vectors v_{l,j} whose input-dependent scalar coefficients m_{l,ij} produce additive 'sub-updates' to token representations; projecting these sub-updates to vocabulary space often reveals human-interpretable concepts (e.g., '+' or 'and'). The paper hypothesizes these neurons as the locus of arithmetic/conceptual processing for multi-step arithmetic reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-7B (primary analysis)",
            "model_description": "Decoder-only transformer (Llama2-7B) used in experiments; standard multi-layer transformer with feed-forward blocks parameterized by K_l and V_l matrices producing dm sub-updates f(K_l x) V_l per token; analysis focused on neurons v_{l,j} in FF layers and their activation coefficients m_{l,ij}.",
            "arithmetic_task_type": "Grade-school multi-step arithmetic (GSM8K) including addition, subtraction, multiplication, division and multi-step reasoning chains",
            "mechanism_hypothesis": "Arithmetic is mediated by activation of FF 'concept' neurons which produce interpretable sub-updates (projected to vocabulary) that represent arithmetic operators, numbers, logical connectors, etc.; attention provides traversal while FF neurons supply processing; persistent activation across nearby timesteps supports multi-step computation.",
            "evidence_for_mechanism": "Automated neuron discovery (project top promoted tokens per neuron, filter with seed tokens, GPT-4 annotation) found neurons mapping to arithmetic concepts; activation patterns (arithmetic neurons spike during equations/numbers; logic neurons at BOS); positive correlation between average reasoning-neuron coefficient and prompt accuracy; ablation (Gaussian noise added to discovered reasoning neurons) substantially reduced GSM8K accuracy.",
            "evidence_against_mechanism": "Activation of reasoning neurons is necessary but not sufficient: CoT prompts with OOD/incorrect labels can show similar reasoning-neuron activations yet much worse accuracy; superficial token counts do not correlate with accuracy, indicating neuron activation alone misses inter-neuron interactions and attention/circuit effects.",
            "performance_metrics": "Reported Llama2-7B GSM8K few-shot CoT accuracies: CoT 16.83%, w/o Equation 12.58%, w/o Textual Explanation 13.41%, AddOnly 13.26%, MultOnly 13.13%, Incorrect Label 16.45%, OOD Label 7.58%. Zero-shot CoT prompt examples: 7.05%, 4.47%, 11.06%, 5.83% for four different prompts (plotted vs average coefficient).",
            "probing_or_intervention_results": "Neuron discovery: stored top-T activated neurons across decoding steps (T=10 per step), projected to vocabulary (top-P tokens, P=20), filtered by seed tokens (F&gt;=2), then annotated by GPT-4 (~1300 prompts). Intervention: random-noise ablation (add Gaussian noise to neuron vectors v_{l,j}) on discovered reasoning neurons reduced accuracy from 16.83% to 4.54%; corrupting same number of random neurons reduced accuracy to 11.37%.",
            "limitations_and_failure_modes": "Neurons can be polysemantic (promote multiple concepts) and activations can be 'faked' by surface token sequences; activation analysis ignores interactions (attention, circuits), so it does not fully explain reasoning or in-context learning; similar activation patterns can occur with incorrect/OOD labels producing flawed reasoning; analysis limited to seven pre-defined concepts; results derived primarily from Llama2-7B.",
            "comparison_to_other_models": "Paper reports preliminary application to Llama3-8B where similar reasoning neurons were discoverable but model-level behavior differed (see separate entry). Prior work (Stolfo et al.) suggested attention enables information traversal while FF handles computation; Geva et al. showed FF sub-updates project to interpretable tokens — this paper extends those ideas to arithmetic reasoning.",
            "uuid": "e4626.0",
            "source_info": {
                "paper_title": "An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama2-7B arithmetic behavior",
            "name_full": "Llama2-7B arithmetic reasoning (CoT analysis)",
            "brief_description": "Empirical evaluation and mechanistic analysis of Llama2-7B on grade-school arithmetic (GSM8K), showing that Chain-of-Thought prompts activate FF neurons associated with arithmetic concepts and that these activations correlate with performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama2-7B",
            "model_description": "Open-source decoder-only transformer (Llama2 family) with ~7 billion parameters; used with greedy decoding for analyses; feed-forward layers analyzed for neuron activations and projections to vocabulary.",
            "arithmetic_task_type": "GSM8K grade-school multi-step word problems requiring addition, subtraction, multiplication, division and stepwise reasoning (2–8 steps).",
            "mechanism_hypothesis": "Llama2-7B performs arithmetic reasoning via activation of FF layer concept neurons (operator and logic neurons) whose sub-updates accumulate across layers to build intermediate representations for next-token prediction; textual explanations and explicit equations increase activation of these neurons and thus improve performance.",
            "evidence_for_mechanism": "Found 113 neurons in Llama2-7B associated with seven arithmetic-related concepts via automated discovery; activation-frequency and average coefficient analyses show more/stronger activations for CoT prompts with equations and textual explanations; positive correlation between average reasoning-neuron coefficient and prompt accuracy; targeted corruption of discovered neurons sharply degrades accuracy.",
            "evidence_against_mechanism": "CoT prompts with OOD labels show similar reasoning-neuron activation counts/coefficients but much lower accuracy, indicating neuron activation alone cannot guarantee correct computation; word-level counts of reasoning tokens do not correlate with performance.",
            "performance_metrics": "GSM8K few-shot CoT (Llama2-7B): CoT baseline 16.83%; ablations: w/o Equation 12.58%, w/o Textual Explanation 13.41%, AddOnly 13.26%, MultOnly 13.13%, Incorrect Label 16.45%, OOD Label 7.58%. Neuron-ablation: no corruption 16.83% → corrupted reasoning neurons 4.54% → corrupted random neurons 11.37%.",
            "probing_or_intervention_results": "Automated neuron discovery (top-T activations, projection to vocab, GPT-4 annotation); random-noise ablation to v_{l,j} (Gaussian) applied to discovered reasoning neurons and to random neurons for baseline; comparison of activated-neuron overlaps when labels changed (63.05% overlap for incorrect labels vs 14.91% for OOD labels in encoding steps where labels manipulated).",
            "limitations_and_failure_modes": "Low absolute accuracy on GSM8K (CoT 16.83%) indicates limited arithmetic competence at this scale; reasoning neuron activation necessary but not sufficient; polysemantic neurons, persistence but potential compounding errors across steps; analysis restricted to single model and pre-defined concept set.",
            "comparison_to_other_models": "Compared qualitatively with Llama3-8B (preliminary): both contain discoverable reasoning neurons, but Llama3-8B shows extreme sensitivity to small random-noise ablations; related prior findings (Stolfo et al.) align on FF involvement and head-mediated traversal.",
            "uuid": "e4626.1",
            "source_info": {
                "paper_title": "An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama3-8B fragility",
            "name_full": "Llama3-8B preliminary arithmetic neuron findings",
            "brief_description": "Preliminary application of the neuron-discovery pipeline to Llama3-8B found reasoning neurons (≈112) but revealed that the model is extremely sensitive to small random-noise ablations, suggesting different robustness and circuit properties from Llama2-7B.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3-8B",
            "model_description": "Newer Llama3 family model with ~8 billion parameters (as cited); authors applied the same automated neuron-discovery pipeline to it and report qualitative differences in ablation sensitivity.",
            "arithmetic_task_type": "Same GSM8K/CoT style arithmetic prompts for neuron discovery and corruption experiments (preliminary).",
            "mechanism_hypothesis": "Llama3-8B also contains FF neurons that project to arithmetic concepts, but its arithmetic behavior appears fragile: small perturbations to a modest number of neurons can throw activations off-distribution and catastrophically reduce performance, implying tighter coupling or distributional brittleness in internal representations.",
            "evidence_for_mechanism": "Discovery found ~112 neurons associated with the seven concepts; applying random-noise ablation to a small number (≈10–20) of random neurons caused accuracy to drop from 45.23% to &lt;2.00%, demonstrating sensitivity of computations to neuron perturbations.",
            "evidence_against_mechanism": "Extreme sensitivity to small random noise complicates causal interpretation: perturbations may move activations off-distribution producing artifactual failures rather than revealing specific causal circuits; no full ablation studies analogous to Llama2-7B reported.",
            "performance_metrics": "Reported preliminary baseline accuracy 45.23% (context unspecified in-detail) which collapsed to &lt;2.00% after small random-noise ablation to ~10–20 neurons.",
            "probing_or_intervention_results": "Applied the same GPT-4-guided neuron discovery; observed catastrophic failure under small random-noise ablation; authors note this may be due to off-distribution activations rather than direct evidence of causal necessity of specific neurons.",
            "limitations_and_failure_modes": "High fragility to small perturbations (possible off-distribution effects), limited analysis depth (preliminary), and lack of controlled interventions to isolate circuits — prevents straightforward generalization of mechanistic conclusions from Llama2-7B to Llama3-8B.",
            "comparison_to_other_models": "Contrasts with Llama2-7B which showed modest, interpretable performance drops when reasoning neurons were corrupted; suggests model-specific circuit structure and robustness differences.",
            "uuid": "e4626.2",
            "source_info": {
                "paper_title": "An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4 neuron annotation",
            "name_full": "GPT-4-assisted automated neuron annotation",
            "brief_description": "A practical pipeline that uses GPT-4 to label candidate FF neurons by presenting their top projected vocabulary tokens and asking whether the neuron promotes a predefined concept; used to scale neuron concept annotation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (used as an annotator, not as arithmetic performer)",
            "model_description": "Large language model (OpenAI GPT-4) used interactively to judge whether a neuron's top promoted tokens correspond to a target concept; the paper used ~1300 GPT-4 queries for Llama2-7B neuron annotation.",
            "arithmetic_task_type": "Not an arithmetic performer here; used to classify neurons into arithmetic-related concept categories (add, sub, mul, div, logical connectors, equals, calculation).",
            "mechanism_hypothesis": "GPT-4 can reliably interpret short lists of top-promoted tokens from a neuron's projection and decide whether a neuron represents an arithmetic-related concept, enabling automated, scalable neuron discovery.",
            "evidence_for_mechanism": "Authors validated GPT-4 results manually and found no objections; using GPT-4 with seed-token filtering (P=20, F=2) they discovered 113 neurons in Llama2-7B and 112 in Llama3-8B associated with the target concepts.",
            "evidence_against_mechanism": "Costs (∼1300 GPT-4 prompts) and reliance on seed-token filters; GPT-4 may misclassify polysemantic neurons or be sensitive to token translation/noise; filtering and thresholds (P, F) impose heuristic biases.",
            "performance_metrics": "Discovery process required ~1300 GPT-4 prompts for Llama2-7B; yielded 113 neurons for 7 concepts (Llama2-7B) and 112 for Llama3-8B.",
            "probing_or_intervention_results": "Pipeline: collect top-T (T=10) activated neurons across K=20 examples, project neurons to vocabulary (top-P tokens), filter by seed tokens (F&gt;=2), then query GPT-4 with the top promoted tokens asking Yes/No if neuron promotes the concept. Returned 'Yes'/'No' plus explanation.",
            "limitations_and_failure_modes": "Annotation depends on the quality of projected top tokens and seed-token heuristics; GPT-4-based judgments may inherit GPT-4 biases; cost and number of prompts scale with model/layer/neuron count; does not substitute for causal circuit analysis.",
            "uuid": "e4626.3",
            "source_info": {
                "paper_title": "An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Analyzing encoded concepts in transformer language models",
            "rating": 2,
            "sanitized_title": "analyzing_encoded_concepts_in_transformer_language_models"
        },
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Towards understanding chain-of-thought prompting: An empirical study of what matters",
            "rating": 1,
            "sanitized_title": "towards_understanding_chainofthought_prompting_an_empirical_study_of_what_matters"
        },
        {
            "paper_title": "Complementary explanations for effective in-context learning",
            "rating": 1,
            "sanitized_title": "complementary_explanations_for_effective_incontext_learning"
        }
    ],
    "cost": 0.01333225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs</p>
<p>Daking Rai 
Ziyu Yao ziyuyao@gmu.edu 
Lawrence Chan 
Adria Garriga-Alonso 
Nicholas Goldowsky-Dill 
Ryan Greenblatt 
Jenny Nitishin- Skaya 
Ansh Radhakrishnan 
Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Mark Chen 
Heewoo Jun 
Lukasz Kaiser 
Matthias Plappert 
Jerry Tworek 
Jacob Hilton 
Reiichiro Nakano 
Nelson Elhage 
Tristan Hume 
Catherine Olsson 
Neel Nanda 
Tom Henighan 
Scott Johnston 
Sheer Elshowk 
Nicholas Joseph 
Nova Dassarma 
Ben Mann 
Danny Hernandez 
Amanda Askell 
Kamal Ndousse 
Andy Jones 
Dawn Drain drai2@gmu.edu 
Anna Chen 
Yun- Tao Bai 
Deep Ganguli 
Liane Lovitt 
Zac Hatfield- Dodds 
Jackson Kernion 
Tom Conerly 
Shauna Kravec 
Stanislav Fort 
Saurav Kadavath 
Josh Ja- Cobson 
Eli Tran-Johnson 
Jared Kaplan 
Jack Clark 
Tom Brown 
Sam Mccandlish 
Dario Amodei 
Christopher Olah 
Yuntao Bai 
Yao Fu 
Litu Ou 
Mingyu Chen 
Yuhao Wan 
Hao Peng 
Tushar 2023 Khot 
Mor Geva 
Avi Caciularu 
Kevin Wang 
Wes Gurnee 
ZifanTheo Horsley 
Carl Guo 
Tara Rezaei Kheirkhah 
Qinyi Sun 
Will Hathaway 
Matthew Pauly 
Kather- Ine Harvey 
Dmitrii Troitskii 
Yuntao Bai </p>
<p>Department of Computer Science
George Mason University
FairfaxVA</p>
<p>Department of Computer Science
George Mason University
FairfaxVA</p>
<p>An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs
245B04018DD8567FA85A589C63CD1A5A
Large language models (LLMs) have shown strong arithmetic reasoning capabilities when prompted with Chain-of-Thought (CoT) prompts.However, we have only a limited understanding of how they are processed by LLMs.To demystify it, prior work has primarily focused on ablating different components in the CoT prompt and empirically observing their resulting LLM performance change (Madaan and Yazdanbakhsh, 2022;Wang et al., 2023;Ye et al., 2023).Yet, the reason why these components are important to LLM reasoning is not explored.To fill this gap, in this work, we investigate "neuron activation" as a lens to provide a unified explanation to observations made by prior work.Specifically, we look into neurons within the feed-forward layers of LLMs that may have activated their arithmetic reasoning capabilities, using Llama2(Touvron et al., 2023)as an example.To facilitate this investigation, we also propose an approach based on GPT-4 to automatically identify neurons that imply arithmetic reasoning.Our analyses revealed that the activation of reasoning neurons in the feed-forward layers of an LLM can explain the importance of various components in a CoT prompt, and future research can extend it for a more complete understanding. 1</p>
<p>Introduction</p>
<p>Arithmetic reasoning is one of the emergent properties in large language models (LLMs), which is necessary for them to tackle tasks that require multiple steps to arrive at the correct answer.In recent years, Chain-of-Thought (CoT) has become a popular prompting strategy to elicit reasoning2 in LLMs (Wei et al., 2022).Despite its successes, there is little understanding of what makes it effective and how LLMs utilize it to facilitate reasoning.</p>
<p>To address this concern, a line of research has focused on decomposing the CoT prompt into various components and performing ablation studies on them to ascertain the significance of each component on the LLM reasoning performance (Madaan and Yazdanbakhsh, 2022;Wang et al., 2023;Ye et al., 2023).Although these studies have yielded several insightful observations on the effect of input on LLM's reasoning performance, they do not shed light on how these inputs are being processed internally by LLMs to perform reasoning.</p>
<p>On the other hand, there is a growing body of research in the field of mechanistic interpretability (Elhage et al., 2021;Wang et al., 2022a) that specifically examines the internals of LLMs to understand their mechanism.In this vein, Stolfo et al. (2023) studied the internal mechanism of LLMs to perform arithmetic calculation, suggesting that attention heads facilitate information traversal, while the feed-forward layer (FF) handles information processing to produce accurate answers for a given computation.However, Stolfo et al. (2023) only studied the mechanism for a single mathematical computation and doesn't study arithmetic reasoning in full scope.In parallel, some other research demonstrated that LLMs consist of neurons that can be associated with human-interpretable concepts, which play a crucial role in various capabilities of LLMs (Geva et al., 2022;Dai et al., 2021;Gurnee et al., 2024).Specifically, Geva et al. (2022) showed that neurons in the FF layer of a transformer model (Vaswani et al., 2017) form key-value pairs that facilitate next-token prediction by promoting concepts in the vocabulary space.However, none of the prior work has applied the intuition to understand LLM reasoning.</p>
<p>Motivating by the need to form a deeper understanding of how CoT prompts elicit reasoning in LLMs and observing the pivotal role of neurons arXiv:2406.12288v3[cs.AI] 2 Sep 2024 within the FF layers of LLMs (Geva et al., 2022), in this work, we propose to investigate the activation of FF neurons in LLMs as a lens to interpret their arithmetic reasoning capabilities.Particularly, we aim to use neuron activation to provide a unified explanation of observations that were only empirically made by prior work (Madaan and Yazdanbakhsh, 2022;Wang et al., 2023;Ye et al., 2023), as listed in Table 1.</p>
<p>To this end, we first propose an approach that leverages LLMs (e.g., GPT-4 (OpenAI, 2023)) to automatically search for neurons that are related to arithmetic reasoning (e.g., arithmetic addition, logical connections, etc.).Prior work trying to search for concept-relevant neurons relies on human analysis (Geva et al., 2022;Elhage et al., 2022).For example, Geva et al. (2022) proposed to manually examine a neuron's top promoted tokens and determine if the neuron promotes the given pre-defined concept or not.However, this manual approach becomes impractical for LLMs with a large number of layers and numerous neurons in each layer.Our approach instead decides whether a given neuron expresses a certain concept automatically by prompting the GPT-4 with its top promoted tokens and make a judgment on its represented concept.Our experimental results demonstrate the high effectiveness of utilizing GPT-4 for this purpose.Subsequently, we apply our proposed approach to identify FF neurons in Llama2-7B that promote several concepts relevant to arithmetic reasoning, listed in Table 3.</p>
<p>Leveraging the identified reasoning neurons, we performed a series of analyses on observations made by prior work (Madaan and Yazdanbakhsh, 2022;Wang et al., 2023;Ye et al., 2023), including the importance of textual explanation, equations, arithmetic diversity, and the negligible impact of incorrect labels in CoT prompts for elicitating reasoning in LLMs.Specifically, we analyzed the activation patterns of the identified reasoning neurons, such as their activation frequency and strength, to gain insights into these observations.Our results reveal that examining the activation of FF neurons in response to different CoT prompts can provide valuable insights into why certain CoT prompts are more effective in eliciting arithmetic reasoning capabilities in LLMs.We then conclude the paper with a discussion of future work that can complement the proposed neuron activation analysis with other approaches to form a more complete understanding of LLM reasoning.</p>
<p>Background and Related Work</p>
<p>Prior Work towards Understanding the CoT Reasoning of LLMs</p>
<p>Prior studies attempted to understand the arithmetic reasoning in LLMs by decomposing the Chainof-Thought (CoT) prompt into different semantic components and evaluating their importance via ablation studies.We present a summary of the major findings from prior work in Table 1.For example, to understand whether equations matter in the fewshot CoT prompt, Ye et al. (2023) experimented with a CoT variant where all equations (e.g., "21 -15 = 6") were eliminated and only the calculation results (e.g., "6") was presented.By observing the resulting LLM performance change, one can empirically gauge the importance of equations in a CoT prompt.While previous studies have highlighted the significance of various components (e.g., textual explanation, equations, etc.) within the CoT prompt, the underlying reason behind these observations remains unanswered.This thus motivates us to study the underlying inner mechanism that is responsible for LLM reasoning.</p>
<p>Interpreting Neurons of LLMs</p>
<p>Many prior interpretability works have studied neurons to understand the inner mechanism of LLMs and have led to the discovery of many interesting types of neurons such as knowledge neurons (Dai et al., 2021), skill neurons (Wang et al., 2022b), sentiment neurons (Radford et al., 2017), concept neurons (Geva et al., 2022), universal neurons (Gurnee et al., 2024), and many others related to linguistic and grammar features (Durrani et al., 2022;Sajjad et al., 2022).Furthermore, the activation patterns of these neurons have been found to significantly influence the behavior of LLMs (Geva et al., 2022).To discover the targeted neurons, probing is the most widely used approach, which involves training a simple classifier (probe) on the representations of neurons using a human-annotated dataset (Gurnee et al., 2023;Belinkov, 2022).Another popular approach specific to transformer-based LLMs is the projection of neuron representations to the vocabulary space, introduced by Geva et al. ( 2022), and has been widely adopted (Dar et al., 2022;Belrose et al., 2023;Ghandeharioun et al., 2024).However, to the best of our knowledge, none of the prior work has applied neuron activation to understand LLM reasoning.Our work draws inspiration from Geva et al. ( 2022) but extends it for a unified explanation of observations in CoT prompting.To this end, we also proposed an automatic approach based on GPT-4 for neuron discovery.Relevant to our work, Stolfo et al. (2023) have also attempted to understand arithmetic reasoning by interpreting their neuron behaviors.However, the majority of their study focused on coarser units such as the entire attention or FF layer.Furthermore, their investigation solely focused on how LLMs execute arithmetic calculations whereas the (multi-step) reasoning process is under-explored.Consider an auto-regressive transformer-based LLM consisting of L layers, which predicts the next token by projecting its last-layer hidden state onto a vocabulary V via an embedding matrix E ∈ R d×|V| , where d denotes the embedding size and |V| represents the vocabulary size.We denote the FF component in the l-th layer as F F l .Given a token sequence X = (x 1 , ..., x |X| ) as input, the representation of each token x i at layer l (denoted as x l i ∈ R d ) is updated by F F l as follows:</p>
<p>Concept
xl i = x l i + F F l (x l i )(1)
The updated representation xl i then goes through the multi-head self-attention at layer l, which results in x l+1 i for the next FF layer (i.e., F F l+1 ).With the residual connection (He et al., 2016), each FF update can be seen as producing additive updates to the token representation.</p>
<p>In transformers, each F F l is defined with two parameter matrices K l , V l ∈ R dm×d , where d m is the intermediate hidden dimension, and a nonlinearity function f :
F F l (x l i ) = f (K l x l i )V l (2)
Eqn 2 can further be decomposed as:
F F l (x l i ) = dm j=1 f (x l i • k l j )v l j = dm j=1 m l ij v l j (3)
where k l j ∈ R d and v l j ∈ R d are the j-th row of K l and V l , respectively, and
m l ij = f (x l i • k l j
) is a scalar representing the activation coefficient of v l j (i.e., the neuron).Geva et al. ( 2022) interpreted each term in this sum as a set of d m subupdates to the token representation.They also proposed to project this sub-update to the vocabulary by Ev l j .By analyzing the projected vocabulary tokens (typically tokens with top projection scores), they found that the sub-update often encodes human-interpretable concepts.It is important to note that every v l j is a static parameter that is input-independent, while the coefficient m l ij depends on the input token x i .</p>
<p>Observing their critical roles and leveraging their interpretability after projection, Geva et al. ( 2022) demonstrated the potential of encouraging nontoxic language by manipulating the coefficients of FF neurons in LLMs.This was achieved by identifying FF neurons representing non-toxic language concepts and then increasing their coefficients.Getting inspired by their findings, our work aims to explore: Can FF neuron activation be similarly used to interpret and even control LLM reasoning?It is important to note that "toxicity" and "reasoning" represent distinct extents of abstraction.While whether a sentence is toxic or not can be judged by superficial keyword searching, "reasoning" is more abstract and can encompass multiple aspects (e.g., logical induction, mathematical calculation, etc.), which thus presents a significant challenge.</p>
<p>Neuron Discovery using GPT-4</p>
<p>To facilitate the neuron analysis, we first propose an approach for discovering neurons that express concepts related to arithmetic reasoning.To achieve the same goal, Geva et al. ( 2022) manually examined the top-scoring vocabulary tokens projected by each neuron v l j and annotated its concept.However, this manual search approach can become impractical for LLMs with deep layers and numerous subupdates per layer.To overcome this inefficiency, we propose a method that leverages GPT-4 to automate the search process.</p>
<p>Our proposed approach involves two steps.First, for a given LLM, we store the T neurons v l j 's with the largest coefficient m l ij from each layer l and at each generation time step i, using a set of examples E that showcase the LLM's capability (i.e., arithmetic reasoning in our case) to provide the prompt.We only considered the top-T neurons to narrow our search to the most activated neurons.This returns a set of candidate neurons N .We present this step in Algorithm 1. for each decoding step i : 6:</p>
<p>for each layer l = 1, ..., L : 7: In the second step, we task GPT-4 to determine whether each neuron in N promotes a predefined concept C name (e.g., arithmetic addition).However, employing GPT-4 to classify all neurons in N still requires a large number of prompts and may incur significant costs.To address this issue, we propose to first filter out the irrelevant neurons by using a set of human-annotated "seed tokens" (denoted as S name ) that are likely to be associated with the given concept as per human intuition. 3or instance, when searching for neurons that promote arithmetic addition, relevant tokens may include "add", "addition", "sum", "+", and "plus".Although a neuron that promotes the given concept may not invariably promote all the tokens from the seed tokens, it is quite probable that it promotes at least some of them.Leveraging this insight, we filter out neurons that do not consist of at least a threshold of F seed tokens in their top-P promoted tokens V P , obtained by projecting the neuron to vocabulary space.Finally, we prompt GPT-4 to inquire whether a neuron from filtered N promotes a given concept or not, This step is described in Algorithm 2, and we include the prompt script in Appendix A.  ranked at top 10.The other alternative would be to define a threshold based on m l ij to determine its activation.However, coming up with an appropriate threshold poses a challenge, as the threshold value may vary across different layers or even among the individual neurons.Consequently, we opt to focus solely on neurons with the top 10 largest coefficients in our analysis.
{m l ij ′ } T j ′ =1 ← FindLargestT({m l ij } dm j=1 , T ) 8: N ← N ∪ {v l j |m l ij ∈ {m l ij ′ } T j ′ =1 } Algorithm 2</p>
<p>Experimental Setup</p>
<p>Dataset and Model Setup We conduct our experiment on the GSM8k dataset (Cobbe et al., 2021), which is widely used for evaluating the arithmetic reasoning capabilities of LLMs.It consists of diverse grade school math word problems and only requires basic arithmetic operations to solve, often involving problem-solving steps ranging from two to eight.We use Llama2-7B (Touvron et al., 2023) as our model to investigate the reasoning capabilities in LLMs.However, we believe that our findings apply to other transformer-based decoderonly LLMs as well.</p>
<p>Our experiments are based on the CoT prompts obtained from Fu et al. ( 2023), with a slight modification to ensure a consistent format in multistep reasoning which makes further analysis easier.Each CoT prompt consists of eight exemplars.For reproducibility purposes, we provide a complete list of our prompts in the Appendix I.</p>
<p>Before investigating the mechanism of LLM reasoning, we have conducted experiments to replicate and validate observations made by prior work (Table 1).For RQ4 and RQ6, different prior work adopted different ablation designs.We opted for the most suitable and fair design among them.The experimental results based on Llama2-7B are presented in Table 2, which present consistent observations as prior research.We refer readers to Appendix B for more details.Summary of Research Questions (RQs) Leveraging the lens of neuron activation, we aim to an-swer two sets of questions.The first set of questions (RQs 1-2) tries to understand the underlying mechanism of LLM reasoning where we initially find different neurons related to arithmetic reasoning and explore the importance of these discovered reasoning neurons for activating reasoning in LLMs.Built upon this foundational understanding of LLMs' reasoning mechanism, the second set of questions (RQs 3-6) attempts to provide a unified explanation of observations made by prior work.</p>
<p>Understanding the Mechanism of</p>
<p>Reasoning in LLMs 5.1 RQ1: Are there neurons or sub-updates related to the concept of "reasoning"?</p>
<p>To answer this question, we apply the proposed approach in Section 3 to automatically identify neurons implying a set of 7 concepts, including logical connectors, which plays a crucial role in deciding the reasoning direction, a set of four arithmetic operations (i.e., add, subtract, multiply, and division), and others (equals to and calculation), which are also important to arithmetic reasoning.Though they may not fully encompass arithmetic reasoning, these concepts are sufficient for an initial investigation of neuron activation.The seed token set S name for each concept, the identified neuron examples, and the expanded concept tokens found in the identified neurons, are presented in Table 3.The specific implementation details are included in Appendix C. We find a total of 113 neurons associated with the listed concept in Llama2-7B.We performed manual validation of the results and didn't find any objection.For instance, the "L21N7027" neuron, corresponding to the 21st layer and 7027-th row of V 21 , projects with high coefficients to tokens such as "+", "U+4e0e", "&amp;", "and", "U+acfc", "plus", "+", "AND", "U+3068", etc., and GPT-4 reasonably classified it as a neuron that promotes "Arithmetic Addition".Notably, we discovered neurons that group certain concepts using different language characters.For instance, the "L21N7027" neuron promotes tokens like "and" and "+" with their corresponding translation for Chinese (U+4e0e) and Japanese (U+3068).Additionally, we also found some neurons with somewhat polysemantic characteristics, where a single neuron promotes multiple concepts.For instance, "L27N10751" promotes tokens related to both addition (+, plus, +=, ..) and subtraction (-, minus, -+, ..).Table 3: List of concepts related to arithmetic reasoning along with their seed tokens and the count of discovered neurons in Llama2-7B.We also list the expanded tokens, promoted by the discovered neurons and their exemplar neurons.For some exemplar neurons, we also show its top-scored vocabulary tokens enclosed within braces.</p>
<p>Activation Pattern of Reasoning Neurons Our further investigation found out intriguing activation pattern of reasoning neurons throughout an LLM's reasoning process.For example, in Figure 2 of Appendix D, we showed that the logical connector neurons are often activated at the beginning of a generated sentence, whereas arithmetic neurons are mostly activated in response to arithmetic symbols and numbers.Once a neuron is activated, it remains activated for a few subsequent time steps.This persistence implies a lasting impact of activated neurons on text generation in its proximity.</p>
<p>RQ2: Are the discovered neurons important for eliciting the reasoning capability of LLMs?</p>
<p>To validate the importance of our discovered neurons and assess their faithfulness in promoting various reasoning concepts, we perform random noise ablation (Meng et al., 2022) of the discovered neurons.If these neurons are critical to LLM reasoning, the corrupted LLM should exhibit a decrease in performance.Specifically, for all reasoning neurons in FF, we added Gaussian noise to the neurons, changing Eqn 3 to F F l (x l i ) = dm j=1 m l ij (v l j + N oise).As a baseline, we also corrupted the same number of random neurons for comparison.Subsequently, we run the Llama2-7B with corrupted reasoning neurons and random neurons separately.We report the few-shot CoT performance of each LLM variant on the GSM8k test set in Table 4.</p>
<p>We observe a substantial performance decrease</p>
<p>LLM Variant Accuracy</p>
<p>No corruption 16.83% w/ corrupted reasoning neurons 4.54% w/ corrupted random neurons 11.37% of 12.29% when the discovered reasoning neurons are corrupted, in contrast to a decrease of only 5.47% observed when random neurons are corrupted.The results thus show the essential role of the discovered reasoning neurons in facilitating effective reasoning by LLMs, indicating their necessity for eliciting reasoning in LLMs. 4 In addition, the performance drop when corrupting random neurons implies that some of these neurons may also play an important role (e.g., for context understanding).As we will show in Table 5, these neurons reveal non-zero coefficients on average.</p>
<p>Correlation between the reasoning performance of LLMs and the activation of their reasoning neurons</p>
<p>Given that the identified neurons make critical contributions to an LLM's arithmetic reasoning, a natural question is: Does an LLM's reasoning perfor-  mance correlate positively with how their reasoning neurons are activated?To answer this question, we performed an experiment in the zero-shot CoT setting (Kojima et al., 2022).We specifically selected a zero-shot CoT setting for this analysis because it is unbiased due to the lack of demonstration.In our experiment, we select four zero-shot CoT prompts with varying levels of accuracy on the GSM8K test set, sourced from Yang et al. (2023).The prompts include "Let's think step by step", "Take a deep breath and work on this problem step-by-step", "Break this down", and "A little bit of arithmetic and a logical approach will help us quickly arrive at the solution to this problem".Their respective accuracies are 7.05%, 4.47%, 11.06%, and 5.83% in Llama2-7B.In Figure 1, we plot their accuracy along with the average coefficient of their reasoning neurons per time step during the output generation.</p>
<p>The result confirms our hypothesized positive correlation.It also reveals the potential of predicting an LLM's reasoning performance by examining the activation of their reasoning neurons, without needing human-annotated labels.We leave systematic explorations of this potential to the future.Additionally, we examine if the same correlation can be observed superficially at the word level, because, if the word-level statistics present the same correlation, it could be a more convenient approach of probing into an LLM's reasoning performance than neuron activation.To respond to this question, we similarly examine the correlation between the count of reasoning tokens in the LLM generation, using a combination of the human-annotated seed tokens and the GPT-4-extracted expanded tokens listed in Table 3, and their accuracy on the GSM8k test set.Our result is presented in Figure 1.Intriguingly, we observe no positive correlation between the two factors, which thus highlights the impor-tance of performing neuron-level analysis, as the latter offers direct insights into the functioning of LLMs that may not be visible from simply analyzing their superficial text generation.</p>
<p>Understanding Prior Observations via the Lens of Neuron Activation</p>
<p>In this section, we revisit the major findings from prior work and use the activation of FF neurons in an LLM to explain them.For each research question (RQ), our analysis will be based on how each CoT prompt variant triggers different neuron activation patterns.These observations are summarized in Table 5, where the number of total or unique activated neurons is counted across the encoding steps of each CoT prompt, and the reported coefficient is an average per neuron.To show a baseline, we also report the average coefficient of randomly sampled neurons.</p>
<p>6.1 RQ3: Why do equations matter?</p>
<p>Prior works (Wang et al., 2023;Ye et al., 2023;Madaan and Yazdanbakhsh, 2022) have shown that equations play an important role in eliciting reasoning in LLMs.Looking into its activation pattern, we found the CoT prompt without equations (denoted as "w/o Equation") activates fewer reasoning neurons overall, 842 activations, compared to the CoT prompt with equations (CoT), 1119 activations.Furthermore, we observed a decrease in both the number of activated neurons for individual concepts and their corresponding average coefficients across all categories.This shows that equations play an important role in activating the reasoning neurons which are deemed to be important for arithmetic reasoning.As a result, the presence of equations can help elicit arithmetic reasoning in LLMs.Interestingly, we also note that although there were no equations or arithmetic operators in the "w/o Equation" prompt, neurons associated with arithmetic operations (i.e., C add , C sub , C mul , C div ) were still activated.This indicates that even in the absence of explicit equations in the CoT prompt, LLMs are capable of recognizing the necessity of performing arithmetic operations, which explains the 12.58% retained accuracy in Table 2.</p>
<p>RQ4: Why do text explanations matter?</p>
<p>The importance of textual explanations as found in prior work (Wang et al., 2023;Ye et al., 2023;Madaan and Yazdanbakhsh, 2022) is also consis- Table 5: For each prompt variant, we present (count of activated neurons, count of unique activated neurons, average coefficient) for each concept or total.We also present the average coefficient of random neurons as a baseline.</p>
<p>tent with our observation.We found that the CoT prompt without textual explanations activates reasoning neurons fewer times, 783 activated neurons, compared to the CoT prompt with explanation, 1119 activated neurons.Specifically, we observe a significant decrease in the activation of neurons associated with logical connectors (C logic ) and a slight decrease in the activation of neurons associated with arithmetic operations (particularly C add ).This shows the utility of textual explanations not only in activating neurons associated with logical connectors, crucial for determining the reasoning direction but also in activating neurons associated with arithmetic operations.</p>
<p>6.3 RQ5: Why does arithmetic diversity in exemplars matter?Ye et al. (2023) showed that arithmetic diversity in exemplars is important for arithmetic reasoning, i.e.CoT prompts that consist of all arithmetic operations in their demonstrations yield better performance than the ones that do not.Our results in Table 5 indicate that the performance decline is likely caused by the bias introduced by the partial operators.We observe that the AddOnly prompt activates a higher number of C add neurons (651 vs 599) and C mul neurons (173 vs 98) when compared to CoT, but fewer C div neurons with a lower average coefficient.Similarly, we found that Mul-tOnly activates a significantly higher number of C mul neurons when compared to the CoT prompt (229 vs 98), but significantly fewer C add neurons (322 vs 599).This shows that although both Ad-dOnly and MultOnly activate the neurons related to arithmetic reasoning, they exhibit a bias toward emphasizing specific arithmetic operations, which explains their degraded performance.</p>
<p>6.4 RQ6: Why does incorrect reasoning or gold label not matter?</p>
<p>Prior work (Wang et al., 2023;Min et al., 2022) shows that incorrect labels in the few-shot exemplars do not matter, as long as the labels come from the same distribution.Consistent with our previous findings, we observed a similar reasoning neuron activation pattern for CoT prompts with correct and incorrect labels.However, despite a 9.25% decrease in accuracy for the "OOD Label" prompt, it still exhibited a similar reasoning neuron activation pattern compared to the patterns of CoT.</p>
<p>To understand this phenomenon, we conducted the second analysis.In the prior work, Geva et al. (2022) found that LLMs refresh their token representations by accumulating sub-updates (Section 2.3).Therefore, two CoT prompts with similar performance presumably should reveal similar subupdates per layer in the corresponding step, and vice versa.To validate it, we looked into the neuron activation for each prompt in the encoding steps where the labels were manipulated (e.g., the positions of "1" and "Dawson" in Table 1), as other input tokens are the same in all the three prompts.We then plotted the overlap of activated neurons per layer between CoT and "Incorrect Labels" or between CoT and "OOD Label" in Figure 4 of Appendix E. Note that here we consider all activated neurons, no matter if they are discovered as reasoning neurons or not.We observe a substantial overlap of 63.05% on average in the former case while merely 14.91% in the latter.The observation is consistent with our hypothesis, showing that the activation of FF neurons can be used to explain the performance of CoT prompting.</p>
<p>The two observations (i.e., inconsistent reasoning neuron's activation pattern based on Table 5 but consistent sub-update pattern based on the overlap analysis) thus imply that the activation of reasoning neurons are necessary but not sufficient to elicit reasoning in LLMs.In fact, our qualitative analysis showed that in the case of providing OOD labels, the LLM still engages in reasoning, and their reasoning paths are similar to those prompted by correct labels (see examples in Appendix F), which explains the activation of their reasoning neurons.However, this reasoning is biased by the use of OOD tokens as variables, leading to messy variable references and an increasing amount of incorrect reasoning as the reasoning proceeds.We include a further discussion in Limitations.</p>
<p>Conclusions</p>
<p>Our work is among the first in applying neuron activation analysis to understanding LLMs in arithmetic reasoning.Our results offer valuable insights into the role of neurons and their utility in understanding the internal mechanism of LLMs.We thus expect this work to pave the way for future research on LLM interpretability.</p>
<p>Limitations</p>
<p>Sufficiency vs Necessity We show that neuron activation is a necessary condition for LLM to elicit reasoning capability through random noise ablation study in RQ2.However, another crucial question to raise is, does neuron activation represent all about LLM reasoning?In other words, is an analysis of neuron activation sufficient to completely explain the LLM reasoning capability?As discussed in Section 6.4, although CoT with incorrect OOD labels has lower accuracy than CoT with correct labels (16.83% vs 7.58%), they show a similar number of reasoning neuron activations (1119 vs 1087) and similar average coefficients (2.51 vs 2.50).This indicates that the activation of reasoning neurons is necessary but not sufficient to elicit the reasoning ability of LLMs.Despite its efficacy in explaining RQs in this work, the analysis of neuron activation is inherently limited by its focus on analyzing neurons individually, without considering the interaction among neurons or other LLM components (e.g., attention modules); as a result, it may not be able to explain complicated model behaviors that result from the interactions among different components in an LLM.For instance, to understand in-context learning within CoT, analyzing neurons in isolation may prove insufficient.Instead, as explored by Olsson et al. (2022), it requires study-ing attention heads and their circuits, which are sub-networks of neurons.Despite this limitation, through our study, we show that analysis of neuron activation can play an important role.Therefore, future work should study it together with other approaches such as circuit analysis (Olsson et al., 2022;Wang et al., 2022a), top-down approach (Zou et al., 2023;Meng et al., 2022), etc., to provide a more complete picture of LLMs' inner mechanism for reasoning.</p>
<p>Limitations of pre-defined concepts Although we employ seven concepts introduced in Section 5.1 to study arithmetic reasoning in LLMs, they may not represent the full scope of arithmetic reasoning.Hence, our study is also limited to the scope of these seven concepts.Furthermore, the activation of these neurons may only indicate the appearance of these concepts during an LLM's reasoning process, but this can be easily "faked" (e.g., prompting an LLM to produce a sequence of concept tokens pretending to be performing reasoning).As a result, the coefficient of reasoning neurons as a metric is more helpful when the prompts to LLMs are valid.Thus, it is important to exercise caution when drawing conclusions from the analysis.</p>
<p>Generalization Our analysis in this paper was performed on only Llama2-7B.Therefore, there is a concern about whether the insights we observed generalize to other LLMs.To answer this question, we conducted a preliminary study based on Llama3-8B (Meta, 2024).In Table 7, we present the discovered reasoning neurons when employing our proposed neuron discovery algorithm in Section 3. From the results, we confirm that reasoning neurons do exist in various LLMs.However, during the RQ2 investigation, we have found that Llama3-8B behaved very differently to Llama2-7B.Specifically, it is highly sensitive to random noise ablation (Meng et al., 2022).Even adding a small noise to ablate a few random neurons (approximately 10-20) can drastically decrease its performance from 45.23% to less than 2.00%.We believe this sensitivity is due to the model activations being thrown off-distribution by the addition of the noise (Chan et al., 2022).This observation suggests that different approaches may be needed to evaluate the mechanism of different LLMs.More systematic studies should be conducted to investigate these limitations in the future.</p>
<p>A Prompt for Neuron Annotation with GPT-4</p>
<p>To implement the GPT4ConceptQuery function in Algorithm 2, we query GPT-4 using the following prompt: "A neuron in language model promotes the following set of words: w 1 , .., w P .Is this neuron promoting C name ?First, answer in Yes or No format and provide an explanation."The function returns "Yes" when GPT-4 considers the neuron (as represented by their projected vocabulary tokens) to represent the target concept C name .We additionally prompt GPT-4 to provide an explanation as it empirically motivates more precise results from GPT-4.</p>
<p>B Additional Details of Replicating Observations of Prior Work</p>
<p>Before investigating the mechanism of LLM reasoning, we first conduct experiments to replicate and validate observations made by prior work (Table 1).The experimental results based on Llama2-7B are presented in Table 2.We successfully replicated all the results of the prior work.</p>
<p>Although some research questions (RQs) were common in prior work, the experiment design could differ.In these cases, we opted for a more suitable or fair experiment design among them.Specifically, for RQ4, "Does textual explanation matter?",we follow the specification of Ye et al. (2023) instead of Madaan and Yazdanbakhsh (2022).Madaan and Yazdanbakhsh (2022) ablated the text and rewrites the multiple equations into a single equation to evaluate the importance of the text.We find it unfair to compare the importance of equations in the few-shot exemplar as single problem-solving steps rather than multiple steps.</p>
<p>In our experiments, we only remove text while retaining all the equations from our original CoT instead of restructuring them into singular equations.Similarly, for RQ6, "Does correct reasoning or gold label matter?",Ye et al. (2023) proposed to manipulate only the labels of the equation.On the other hand, Wang et al. (2023) proposed to manipulate other components such as operators and textual explanations as well.We follow the specification of Ye et al. (2023) instead of Wang et al. (2023) for its simplicity and ease of analysis.</p>
<p>C Additional Implementation Details for</p>
<p>Neuron Discovery (RQ1)</p>
<p>In Algorithm 1, we randomly select 20 examples from the GSM8k (Cobbe et al., 2021) test set as E and set K = 20.Additionally, we perform simple greedy decoding on Llama2-7B that consists of 7 billion parameters using a single NVIDIA A100 GPU for 6-7 hours to save the candidate neurons using Algorithm 1. Subsequently, we employ Algorithm 2 to identify associated neurons for each concept, with thresholds P = 20 and F = 2 where we prompt GPT-4 ∼ 1300 times to obtain the reasoning neurons listed in Table 3.</p>
<p>D Reasoning Neurons Activation Dynamics</p>
<p>To better understand the activation pattern of identified reasoning neurons in Section 5.1, we plot their activation throughout an LLM's reasoning text for a randomly selected example, as shown in Figure 3 and Figure 2. Our goal is to discern the activation sites of these reasoning neurons and utilize this information to understand the role of these reasoning neurons in each reasoning step or process.To this end, we first divide the LLM's reasoning text into four sections to simplify the observation -(1) Beginning of a sentence (BOS) (2) Equations ( 3) Numbers (4) Other texts.The activation showed a clear pattern of activation for both neurons related to arithmetic operations and logical connections.</p>
<p>In Figure 3, the heightened activation of arithmetic neurons, encompassing those involved in addition, subtraction, multiplication, and division, within equations is evident.Conversely, Figure 2 demonstrates increased activation of logical connection neurons at the beginning of sentences (BOS).These observations underscore the specific roles played by different neurons in the reasoning process.</p>
<p>E Neuron Activation Overlap between CoT with Correct Labels and Incorrect or OOD Labels</p>
<p>The neuron activation overlap between CoT prompt with correct labels and incorrect or OOD labels, discussed in Section 6.4 is shown in Figure 4.</p>
<p>F Example Predictions for CoT with OOD labels</p>
<p>We list the example prediction for CoT with OOD labels as discussed in Section 6.4 is listed in Ta-</p>
<p>G Implementation Details</p>
<p>We use models, Llama2-7B (Touvron et al., 2023) and GPT-4 (OpenAI, 2023), and the GSM8K dataset (Cobbe et al., 2021) that are consistent with their intended use.For each experiment in our analysis (RQs 2-6), we perform simple greedy decoding on Llama2-7B which consists of 7 billion parameters using a single NVIDIA A100 GPU for 6-7 hours.</p>
<p>H Neuron Discovery on Llama3-8B</p>
<p>We apply our neuron discovery approach, introduced in Section 3, to automatically discover neurons for a pre-defined set of seven concepts.We find a total of 112 neurons associated with the listed concept in Llama3-8B as listed in Table 7.</p>
<p>Figure 1 :
1
Figure 1: Correlation between prompt accuracy and the LLM's average coefficient on the discovered reasoning neurons (blue stars) and its average count of reasoning tokens (orange diamonds).</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: Activation pattern of logical connector neurons for a randomly sampled example.The horizontal dotted line represents the average coefficient of randomly sampled neurons for the same set of examples.</p>
<p>Figure 4 :
4
Figure4: Overlap of neuron activation when the LLM is prompted with CoT and "Incorrect Labels" or "OOD Labels".</p>
<p>Table 1 :
1
Let's think step by step.First there are 15 trees.Then there were 21 trees after some more were planted.So there must have been 21 -15 = 6 trees.The answer is 6. w/o Equation: Let's think step by step.First there are 15 trees.Then there were 21 trees after some more were planted.So there must have been 6 trees.The answer is 6.Summary of shared findings from prior works.Our reproduced results are shown in Table2.
Research Questions Examples in CoT PromptsPrior WorkFindingsDoes equation matter?w Equation: Wang et al. (2023);Yes(RQ3)Ye et al. (2023);Madaan andYazdanbakhsh(2022)Does textualw Textual Explanation: Let's think step by step. First Leah had 32 chocolates andWang et al. (2023);Yesexplanation matter?her sister had 42 chocolates. So in total they had 32 + 42 = 74 chocolates. Then theyYe et al. (2023);(RQ4)ate 35 chocolates. So there must be 74 -35 = 39 chocolates. The answer is 39.Madaan andw/o Textual Explanation: 32 + 42 = 74. 74 -35 = 39. The answer is 39.Yazdanbakhsh(2022)Does the diversity ofAddOnly: Let's think step by step. First there are 3 cars. Then 2 more cars arrive. SoYe et al. (2023)Yesarithmetic operatorsthere must be 3 + 2 = 5 cars. The answer is 5.matter? (RQ5)MultOnly: Let's think step by step. First a farmer has 5 cows. Then each cow has 4legs. So the cows have 5 x 4 = 20 legs in total. The answer is 20.Does incorrectCorrect Label: Let's think step by step. First there are 15 trees. Then there were 21Wang et al. (2023);Noreasoning or goldtrees after some more were planted. So there must have been 21 -15 = 6 trees. TheYe et al. (2023)label not matter?answer is 6.(RQ6)Incorrect Label: Let's think step by step. First there are 15 trees. Then there were 21trees after some more were planted. So there must have been 21 -15 = 1 trees. Theanswer is 1.OOD Label: Let's think step by step. First there are 15 trees. Then there were 21Wang et al. (2023);Yestrees after some more were planted. So there must have been 21 -15 = Dawson trees.Ye et al. (2023)The answer is Dawson.</p>
<p>Algorithm 1 Candidate Neuron Collection 1: Input: A set of examples E implying the capability, a filtering threshold T , the target LLM 2: Output: A set of candidate neurons N .3: Initialize N ← {} 4: for each example in E : 5:</p>
<p>Neuron Annotation via GPT-4 1: Input: Concept Cname, a set of seed tokens Sname, filtering thresholds P and F , embedding E of LLM, and candidate neuron set N .2: Output: A subset of neurons R ⊂ N representing concept Cname.3: Initialize R ← {} 4: for each neuron vn ∈ N :</p>
<p>5: VP = {w1, ..., wP } ← GetLargestP(Evn, P ) 6: if |VP ∩ Sname| ≥ F : 7: if GPT4ConceptQuery(VP , Cname) : 8: R ← R ∪ {vn}</p>
<p>Neuron Activation Following Geva et al. (2022), we consider a neuron being activated in a layer l at a time step i when the neuron's coefficient m l ij is
CoT PromptAccuracyCoT16.83%w/o Equation (RQ3)12.58%w/o Textual Explanation (RQ4)13.41%AddOnly (RQ5)13.26%MultOnly (RQ5)13.13%Incorrect Label (RQ6)16.45%OOD Label (RQ6)7.58%</p>
<p>Table 2 :
2
The accuracy of Llama2-7B on GSM8k test set based on different CoT prompts.</p>
<p>Table 4 :
4
Llama2-7B's performance before and after corruption of reasoning neurons vs random neurons.</p>
<p>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77</p>
<p>The source code for our implementation is available at https://github.com/Dakingrai/ neuron-analysis-cot-arithmetic-reasoning.
Our work focuses on "arithmetic reasoning". For ease of presentation, we use "reasoning" interchangeably with it.
Seed tokens are solely utilized to filter out irrelevant neurons. They are not included in the prompt for GPT-4 and do not influence the neuron annotation process.
We define a mechanism (e.g., reasoning neuron activation) to be a "necessity" for a model capability (e.g., arithmetic reasoning) if the absence of the mechanism results in the model's inability to demonstrate the capability. A relevant concept is "sufficiency", which is defined when the model's capability can be attributed to it(DeYoung et al., 2019;Tuan et al., 2021;Rai et al., 2023). We include further discussions in the Limitations section.
AcknowledgementsThis project was sponsored by the National Science Foundation (SHF2311468) and College of Computing and Engineering and the Department of Computer Science at George Mason University.This project was also supported by resources provided by the Office of Research Computing at George Mason University (https://orc.gmu.edu) and funded in part by grants from the National Science Foundation (2018631).We thank the anonymous reviewers and members at GMU NLP group for their feedback on this work.Buck Shlegeris, and Nate Thomas.2022.Causal scrubbing: A method for rigorously testing interpretability hypotheses.In AI Alignment Forum, pages 1828-1843.Ethics StatementWe do not anticipate any severe ethical issues from using the proposed approach.We use fully opensourced datasets and will open-source our results and dataset as well.On the other hand, we stress the positive impact of our work, as it contributes to interpreting the black box of LLMs.Forming a clear understanding of the inner mechanism of LLMs is crucial for their safe and trustworthy applications.With our investigation of neuron activation for understanding LLMs, we hope to inspire more researchers to extend the research of LLM interpretability.It is also our plan to connect neuron activation with the present societal concerns around LLM safety (e.g., analyzing an LLM's reasoning process and detecting potential vulnerabilities through their neuron activation patterns).Question: Janet's ducks lay 16 eggs per day.She eats three for breakfast every morning and bakes muffins for her friends every day with four.She sells the remainder at the farmers' market daily for $2 per fresh duck egg.How much in dollars does she make every day at the farmers' market?Let's think step by step First Janet's ducks lay 16 eggs per day.Then she eats 3 for breakfast every day.So she has 16 -3 = Dylan eggs left.Then she bakes muffins for her friends every day with 4. So she has Dylan -4 = Dylan eggs left.Then she sells the remainder at the farmers' market daily for $2 per fresh duck egg.So she has Dylan -4 = Dylan eggs left.So she makes $2 * Dylan = Dylan dollars.The answer is Dylan.Question: Two trains leave San Rafael at the same time.They begin traveling westward, both traveling for 80 miles.The next day, they travel northwards, covering 150 miles.What's the distance covered by each train in the two days?Let's think step by step First two trains leave San Rafael at the same time.Then they begin traveling westward, both traveling for 80 miles.So they travel 80 miles each.Then the next day, they travel northwards, covering 150 miles.So they travel 150 miles each.So they travel 80 + 150 = Dylan miles each.The answer is Dylan.7: List of concepts related to arithmetic reasoning along with their seed tokens and the count of discovered neurons in Llama3-8B.We also list the expanded tokens, promoted by the discovered neurons and their exemplar neurons.For some exemplar neurons, we also show its top-scored vocabulary tokens enclosed within braces.I CoT Prompts for ReproducibilityWe list all the CoT prompts used in our analysis, RQs 1-6.The CoT prompts (Correct) is listed in Question: There were nine computers in the server room.Five more computers were installed each day, from monday to thursday.How many computers are now in the server room?Let's think step by step First there were 9 computers.Then for each of 4 days, 5 more computers were added.So 5 * 4 = 20 computers were added.So there must be in total 9 + 20 = 29 computers.The answer is 29.Question: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of Wednesday?Let's think step by step First Michael started with 58 golf balls.Then he lost 23 on Tuesday.So he had 58 -23 = 35 golf balls.Then he lost 2 more on Wednesday.So he must have 35 -2 = 33 golf balls.The answer is 33.Question: Olivia has $23.She bought five bagels for $3 each.How much money does she have left?Let's think step by step First Olivia has 23 dollars.Then she bought five bagels for 3 dollars each.We know 5 bagels for 3 dollars each will be 5 * 3 = 15 dollars.So she has 23 -15 = 8 dollars left.The answer is 8.Table8: Full prompt for CoT prompting for arithmetic reasoning.Question: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?21 -15 = 6.The answer is 6.Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? 3 + 2 = 5.The answer is 5.  Question: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?Let's think step by step First there are 15 trees.Then there were 21 trees after some more were planted.So there must have been 6 trees.The answer is 6.Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?Let's think step by step First there are 3 cars.Then 2 more cars arrive.So there must be 5 cars.The answer is 5. Then she bought five bagels for 3 dollars each.We know 5 bagels for 3 dollars each will be 15 dollars.So she has 8 dollars left.The answer is 8. Question: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?Let's think step by step First there are 15 trees.Then there were 21 trees after some more were planted.So there must have been 21 -15 = 1 trees.The answer is 1.Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?Let's think step by step First there are 3 cars.Then 2 more cars arrive.So there must be 3 + 2 = 3 cars.The answer is 3. Then she bought five bagels for 3 dollars each.We know 5 bagels for 3 dollars each will be 5 * 3 = 23 dollars.So she has 23 -23 = 9 dollars left.The answer is 9. Question: There were nine computers in the server room.Five more computers were installed each day, from monday to thursday.How many computers are now in the server room?Let's think step by step First there were 9 computers.Then for each of 4 days, 5 more computers were added.So 5 * 4 = Mcgowan computers were added.So there must be in total 9 + Mcgowan = Damarion computers.The answer is Damarion.Question: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of Wednesday?Let's think step by step First Michael started with 58 golf balls.Then he lost 23 on Tuesday.So he had 58 -23 = Jemima golf balls.Then he lost 2 more on Wednesday.So he must have Jemima -2 = Astrid golf balls.The answer is Astrid.Question: Olivia has $23.She bought five bagels for $3 each.How much money does she have left?Let's think step by step First Olivia has 23 dollars.Then she bought five bagels for 3 dollars each.We know 5 bagels for 3 dollars each will be 5 * 3 = Gallagher dollars.So she has 23 -Gallagher = Baily dollars left.The answer is Baily.The answer is 320.Question: Christina has 3 snakes.1 snake is 2 feet long.Another snake is 16 inches long.The last snake is 10 inches long.How many inches are all of her snakes combined?Let's think step by step First Christina has 3 snakes.Then 1 snake is 2 feet long.We know 1 foot is 12 inches.So 2 feet is 12 + 12 = 24 inches.Then another snake is 16 inches long.Then the last snake is 10 inches long.So all of her snakes combined are 24 + 16 + 10 = 50 inches.The answer is 50.Question: Bush and Matt are brothers.Bush is younger than Matt by 3 years.This year Bush will be 12 years old.What will be Matt's age 10 years from now? Let's think step by step First Bush is younger than Matt by 3 years.We know Bush will be 12 years old this year.So Matt will be 12 + 3 = 15 years old this year.Then Matt's age 10 years from now will be 15 + 10 = 25 years old.The answer is 25.Question: Jeremy listened to five more songs yesterday than today.Yesterday, he listened to nine songs.How many songs did Jeremy listen to in two days?Let's think step by step First Jeremy listened to 9 songs yesterday.Then he listened to 5 more songs yesterday than today.So he listened to 9 + 5 = 14 songs today.So he listened to 9 + 14 = 23 songs in two days.The answer is 23.Question: Jar A has 28 marbles.Jar B has 12 more marbles than jar A. Jar C has as many marbles as jar B. How many marbles are there altogether?Let's think step by step First Jar A has 28 marbles.Question: In a jar that has 50 ants, the number of ants in the jar doubles each hour.How many ants will be in the jar after 5 hours?Let's think step by step First there are 50 ants in the jar.Then the number of ants in the jar doubles each hour.So there will be 50 x 2 = 100 ants in the jar after 1 hour.So there will be 100 x 2 = 200 ants in the jar after 2 hours.So there will be 200 x 2 = 400 ants in the jar after 3 hours.So there will be 400 x 2 = 800 ants in the jar after 4 hours.So there will be 800 x 2 = 1600 ants in the jar after 5 hours.The answer is 1600.Question: Mark loves to see shows in theaters.He decided to visit the theater at least once a week.One performance lasts 3 hours.The price of the ticket depends on the time spent in the theater and stands at $5 for each hour.How much will Mark spend on visits to the theater in 6 weeks?Let's think step by step First Mark decided to visit the theater at least once a week.Then one performance lasts 3 hours.We know the price of the ticket depends on the time spent in the theater and stands at 5 dollars for each hour.So the price of the ticket for one performance is 5 x 3 = 15 dollars.So Mark will spend 15 x 6 = 90 dollars on visits to the theater in 6 weeks.The answer is 90.Question: A sixty bulb watt uses 60 watts of power each day.If Allyn has 40 such bulbs in his house and pays an electricity bill of twenty cents per power watt used, calculate Allyn's total monthly expenses on electricity in June.Let's think step by step First a sixty bulb watt uses 60 watts of power each day.Then Allyn has 40 such bulbs in his house.So Allyn has 40 x 60 = 2400 watts of power each day.Then Allyn pays an electricity bill of twenty cents per power watt used.So Allyn pays 2400 x 0.2 = 480 dollars per day.So Allyn pays 480 x 30 = 14400 dollars per month.The answer is 14400.
Probing classifiers: Promises, shortcomings, and advances. Yonatan Belinkov, Computational Linguistics. 4812022</p>
<p>Eliciting latent predictions from transformers with the tuned lens. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev Mckinney, Stella Biderman, Jacob Steinhardt, arXiv:2303.081122023arXiv preprint</p>
<p>Openai, arXivGpt-4 technical report. 2023</p>
<p>Learning to generate reviews and discovering sentiment. Alec Radford, Rafal Jozefowicz, Ilya Sutskever, arXiv:1704.014442017arXiv preprint</p>
<p>Explaining large language model-based neural semantic parsers (student abstract). Daking Rai, Yilun Zhou, Bailin Wang, Ziyu Yao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Analyzing encoded concepts in transformer language models. Hassan Sajjad, Nadir Durrani, Fahim Dalvi, Firoj Alam, Abdul Rafae Khan, Jia Xu, arXiv:2206.132892022arXiv preprint</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Local explanation of dialogue response generation. Yi-Lin Tuan, Connor Pryor, Wenhu Chen, Lise Getoor, William Yang, Wang , Advances in Neural Information Processing Systems. 202134</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, 10.18653/v1/2023.acl-long.153Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, arXiv:2211.00593terpretability in the wild: a circuit for indirect object identification in gpt-2 small. 2022aarXiv preprint</p>
<p>Finding skill neurons in pre-trained transformer-based language models. Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, Juanzi Li, arXiv:2211.073492022barXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Denny Quoc V Le, Xinyun Zhou, Chen, arXiv:2309.03409Large language models as optimizers. 2023arXiv preprint</p>
<p>Complementary explanations for effective in-context learning. Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Veselin Stoyanov, Greg Durrett, Ramakanth Pasunuru, 10.18653/v1/2023.findings-acl.273Findings of the Association for Computational Linguistics: ACL 2023. Toronto, Canada2023Association for Computational Linguistics</p>
<p>Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, arXiv:2310.01405Representation engineering: A topdown approach to ai transparency. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2048 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2048</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2048</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-276741579</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.00735v1.pdf" target="_blank">LADDER: S ELF -I MPROVING LLM S T HROUGH R ECURSIVE P ROBLEM D ECOMPOSITION</a></p>
                <p><strong>Paper Abstract:</strong> ,</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2048.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2048.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LADDER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning through Autonomous Difficulty-Driven Example Recursion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum framework in which an LLM recursively generates progressively simpler variants of hard problems to form a difficulty gradient, then uses verified rewards and reinforcement learning (GRPO) to train itself on that curriculum, enabling large performance gains on mathematical integration tasks without human labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Llama 3.2 3B (experiments); Qwen2.5 7B (MIT experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td>3B; 7B</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>For each original problem the base LLM generates a tree of variants (easier or equivalent integrals). Variant generation uses a transformation library (mathematical transformations affecting difficulty), samples 3–5 transformations per variant, generates variants in batches of ~10 per prompt for diversity, cycles sampling temperature between 0.8 and 1.4, and rotates 'persona' prompts (e.g., 'think like Euler') to increase variety. Trees are generated recursively (depth capped at 2 for MIT experiments, 3 in Llama ablations) producing many easier sub-problems; the model is then trained with GRPO using a numerical verifier as the reward signal. The curriculum therefore generates tasks (simpler integrals/subgoals) conditioned on the original problem and its generated transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Mathematical integration (indefinite integrals; MIT Integration Bee problems)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Formal, highly structured mathematical domain with compositional techniques (substitution, partial fractions, nested/combined functions); small coefficient or structural changes can drastically change difficulty; tasks are verifiable numerically; not open-ended but combinatorially varied; requires symbolic reasoning and composition of multiple techniques; medium-to-long reasoning chains but bounded by calculus methods.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td>Diversity-promoting prompting: batch generation (10 per prompt) to increase variety, temperature cycling (0.8–1.4) for creative sampling, persona-based prompts to elicit different mathematical perspectives, and recursive branching to cover difficulty spectrum. Also explicit skewing of difficulty distribution (e.g., 70% easier / 30% equivalent for MIT experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Numerical solution verifier (multi-point adaptive quadrature, singularity handling, timeouts/filters), GRPO reinforcement learning, transformation library, quality-control filters to reject trivial/degenerate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>LADDER (RL with LLM-generated variants) yielded large gains: Llama 3.2 3B accuracy improved from pass@1 1% (base) / pass@10 2% to 82% on a 100-problem undergraduate integration test set after training on variants generated from 10 training problems (~500 variants per train problem, ~5000 variants total). On the 2025 MIT Integration Bee qualifying exam, LADDER applied to Qwen2.5 7B increased accuracy from a 7B baseline of 50% to 73%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>Compared to RL without variants (i.e., training only on the small curated training set), RL without variants failed: performance never exceeded 3% and collapsed to 0% within ~30 training steps. This indicates heuristics/manual small-set training performed poorly under identical hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>Base model baselines: Llama 3.2 3B pass@1 = 1%, pass@10 = 2% on the undergraduate test. Qwen2.5 7B baseline on MIT exam = 50% (pre-LADDER).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>Llama experiments: ~500 variants per training problem (10 train problems → ~5,000 variants); exact-match overlap with test set minimal (5 matches among 5,000). MIT experiments: ~9,000 variants generated (two-level recursion). In Llama variant generation, ~8% of variants were effectively unsolvable by the symbolic solver (timeout >5s).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Strong generalization reported: Llama 3B trained on only 10 problems plus variants generalized to 100 held-out test integrals achieving 82% accuracy. LADDER-trained Qwen2.5 7B generalized to 2025 MIT exam questions (73% accuracy), none of which appeared in the variant set.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported in monetary terms. Empirical costs: variant generation scaled to thousands (5k–9k) of integrals; verification uses multi-point adaptive quadrature with a 2s per-attempt timeout and up to three retries; training used GRPO with batch size up to 128 for MIT experiments. Authors note verification and unsolvable variants consumed significant compute but provide no wall-clock/time or dollar figures.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Variant generation quality control issues: some purportedly 'easier' variants became harder due to small perturbations (e.g., coefficients introducing complex roots); ~8% unsolvable variants in Llama runs; harder-than-parent variants consume verification compute and are ignored during training via zero reward but waste resources. LADDER struggles on integrals requiring synthesis of multiple techniques and novel substitution patterns; recursive depth beyond 2–3 may produce trivially simple or irrelevant variants. Reliance on a numerical verifier may allow subtle verification exploits if not carefully filtered.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>In the specialized domain of calculus/integration, the curriculum is highly effective: small (7B) models trained with LADDER outperform much larger models on the MIT Integration Bee benchmark (LADDER 73% vs GPT-4o 42% reported; note differences in verifiers/constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablations include: RL with variants (LADDER) vs RL without variants (training on only the original small curated set) under identical hyperparameters — only LADDER succeeded. Variant-generation ablations: batch size, temperature cycling, and persona prompting were found empirically important for variant diversity (no quantitative % given). Increasing variant corpus improved performance up to diminishing returns (~improvement slowed past 6,000 variants).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td>Experiments used 3B (Llama 3.2) and 7B (Qwen2.5) models. No systematic sweep across many sizes; authors report large gains for mid-sized models but do not present continuous scaling curves for curriculum generation quality vs model size.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>LLM-generated curricula (recursive variant trees) create a smooth difficulty gradient that is essential for successful RL: LADDER moved a 3B model from ~1–2% to 82% on undergraduate integrals and a 7B model from 50% to 73% on the MIT Integration Bee. RL without such a curriculum collapses or fails (≤3%). Variant diversity mechanisms (batch generation, temperature cycling, persona prompting, recursion) and a reliable verifier are key to effectiveness; scaling the number of variants increases performance but with diminishing returns.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2048.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2048.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TTRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-Time Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A test-time curriculum procedure that generates problem-specific variant trees at inference and runs short targeted RL (GRPO) on those variants to adapt the model to the particular test instance before answering, then rolls back the model for the next question.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated (per-test-instance)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Qwen2.5 7B (applied in MIT experiments); base model used for generation in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>For each difficult test question, TTRL generates a focused tree of variants (recursion depth typically 2, ~800 variants per unsolved test question in MIT experiments), then runs GRPO for a fixed small number of steps (100 RL steps reported) using the variant tree and numerical verification reward. If the tuned model produces a correct solution at any point it's counted as solved; after the procedure the model parameters are rolled back to pre-TTRL for the next test instance. The generation uses same transformation library, batch/temperature/persona diversity techniques, and quality-control filters as LADDER.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Mathematical integration (MIT Integration Bee qualifying exam)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Same as LADDER: formal mathematical domain with compositional solution techniques, high sensitivity to structural perturbations, and verifiability by numerical methods; test instances often cleverly constructed and out-of-distribution relative to standard training data.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td>Per-instance variant diversity via batch generation, temperature cycling, persona prompts and intentionally mixing easier and equivalent variants (authors used e.g., 70% easier / 30% equivalent) to encourage exploration of novel solution patterns relevant to that specific test problem.</td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Numerical verifier, GRPO RL, per-question variant generator, quality-control filters; rollback mechanism to revert model after per-question tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Applied after LADDER, TTRL increased Qwen2.5 7B performance on the 2025 MIT Integration Bee from 73% (LADDER alone) to 90% (LADDER + TTRL). TTRL solved an additional 3–4 problems out of 20 previously failed by LADDER; steps required per solved question ranged from ~3 to ~30 RL steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>TTRL applied to the base (pre-LADDER) model failed to solve any of the previously LADDER+TTRL-solved questions after 100 steps, indicating TTRL alone (without prior curriculum exposure) is ineffective under the tested compute budget.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>Not applicable beyond the base-model baseline: base model + TTRL (no prior LADDER) produced no solved problems in the attempted set.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>Per-problem TTRL used ~800 variants and produced solutions for some questions within 3–30 RL steps; no aggregate diversity counts beyond per-problem variant counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>TTRL is explicitly per-instance and primarily improves on the specific test problem; the authors observed that TTRL is only effective when applied to a model that previously saw variant-based training (LADDER) — indicating reliance on transfer/generalization from prior curriculum exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reported procedural costs: per-problem variant generation (~800 variants) and 100 RL steps per failed test question in MIT experiments; no wall-clock time or monetary cost provided. Authors note TTRL is a form of test-time compute scaling and is parallelizable across compute units.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>TTRL depends on prior LADDER-style exposure: when applied to base model without LADDER pretraining it failed to solve problems under identical compute budget. Some hard integrals remained unsolved even after LADDER+TTRL (two MIT questions remained incorrect). Computational cost per test instance can be high due to large variant sets and RL steps. Variant quality control and verifier limits remain sources of inefficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>Highly effective in the specialized integration domain when used on top of LADDER-trained models: enabled a 7B model to reach state-of-the-art performance (90%) on the MIT Integration Bee qualifying exam, surpassing larger models under the paper's evaluation conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Key ablation: TTRL applied to pre-LADDER base model (100 steps) -> no solved problems, demonstrating the necessity of prior LADDER training for TTRL effectiveness. No additional ablation quantifying number of TTRL steps or variant counts reported beyond reported settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td>TTRL experiments reported on a 7B model only; no systematic study of different model sizes for TTRL was provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Test-time LLM-generated curricula (TTRL) can substantially boost per-instance performance when combined with prior variant-based training (LADDER): +17 percentage points on the MIT exam (73%→90%) with ~800 variants and 100 RL steps per problem; TTRL alone on untrained base models did not produce gains under the tested compute budget.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2048.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2048.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Variant Generation (LLM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Variant Generation (transformation library, batch prompting, persona and temperature cycling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The concrete LLM-driven mechanism that synthesizes easier/equivalent problem variants using a transformation library, batched prompts, temperature cycling, and persona-based prompting, producing recursive variant trees used as curricula for RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Base LLM (Llama 3.2 3B in ablations; Qwen2.5 7B for MIT experiments and variant generation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td>3B; 7B</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Authors first build a transformation library of mathematical transformations (e.g., reduce exponent, simplify denominators, introduce nested functions). For each integral, they sample 3–5 transformations and prompt the model to produce batches (~10 per prompt) of variants, cycle sampling temperature between 0.8 and 1.4, and rotate persona prompts to increase diversity. Variants are generated recursively into trees (depth capped at 3 in Llama experiments, 2 in MIT setup). The policy then uses these generated tasks as RL training data with verifier-based rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Mathematical integration</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>See LADDER: formal, compositional, sensitive to small structural changes, verifiable numerically, not open-ended but rich combinatorial space of functions and techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td>Diversity techniques: batch generation to avoid repetition, temperature cycling for varied outputs, persona-based prompts to elicit different mathematical reasoning styles, and recursive branching to cover a range of difficulty levels. Authors empirically tuned fraction of easier vs equivalent variants (e.g., 70% easier / 30% equivalent for MIT).</td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Transformation library, numerical verifier, timeout/filters for unsolvable outputs, and downstream GRPO RL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Variant-generation scale metrics: Llama experiments used ~500 variants per training problem; MIT experiments generated ~9,000 variants overall; Llama variant set produced minimal overlap with test set (5/5,000). Empirical effect: more variants correlated with higher LADDER performance up to diminishing returns (improvement rate slowed beyond ~6,000 variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>Empirical comparisons show that without variant generation (i.e., RL on raw small training set) training collapses or fails (≤3%); this demonstrates superiority of generated curricular variants versus training only on small curated sets or naive RL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>See LADDER baselines: base pass@1 and pass@10 were 1% and 2% respectively on the undergraduate set.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>Batch generation and persona cycling increased diversity; concrete counts: ~5,000 Llama variants (500 per training problem), ~9,000 MIT variants, exact-match overlap with test set extremely low (5 matches among 5,000), ~8% unsolvable by symbolic solver in Llama runs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Variant corpus generated from 10 training problems enabled generalization to 100 unseen test integrals with 82% accuracy for Llama 3B; MIT variant corpora enabled generalization to the 2025 qualifying exam (73% LADDER for 7B).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Generation produced thousands of variants; verification required adaptive quadrature evaluations across 5 random sample intervals per candidate with 2s timeout per attempt and up to three retries — authors note significant compute from unsuccessful/harder-than-intended variants but provide no explicit cost figures.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Generation sometimes produced variants that were harder than the parent or unsolvable (approx 8% unsolvable); small coefficient/structural changes can unexpectedly increase complexity; naive generation without diversity constraints led to repetitive or low-quality variants; verification cost wasted on hard/unintended variants.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>Highly effective in integration domain; variant generation produced curriculum items that enabled mid-sized models to learn techniques and generalize across many unseen integrals.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Observed effects of generation choices: batch size matters (batch of 10 improved diversity; smaller batches repeated; larger batches degraded quality), temperature cycling and persona prompting crucial for higher diversity, recursion depth beyond 2–3 became less useful or produced trivial variants; increasing total variants improved performance with diminishing returns past ~6k variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td>Variant generation and its effects were tested with 3B and 7B models. No large-scale model-size sweep for generation quality was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Carefully engineered LLM-driven variant generation (transformation library + batch/temperature/persona/recursion) produces high-diversity curricula that enable RL to succeed where naive RL fails; quality-control and diversity mechanisms are essential to avoid wasted compute on unsolvable or harder-than-intended variants.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models can teach themselves to program better <em>(Rating: 2)</em></li>
                <li>Bootstrapping reasoning with reasoning <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Curriculum learning for reinforcement learning domains: A framework and survey <em>(Rating: 2)</em></li>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
                <li>Learning to reason with llms <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2048",
    "paper_id": "paper-276741579",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [
        {
            "name_short": "LADDER",
            "name_full": "Learning through Autonomous Difficulty-Driven Example Recursion",
            "brief_description": "A curriculum framework in which an LLM recursively generates progressively simpler variants of hard problems to form a difficulty gradient, then uses verified rewards and reinforcement learning (GRPO) to train itself on that curriculum, enabling large performance gains on mathematical integration tasks without human labeling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated",
            "llm_model_name": "Llama 3.2 3B (experiments); Qwen2.5 7B (MIT experiments)",
            "llm_model_size": "3B; 7B",
            "curriculum_description": "For each original problem the base LLM generates a tree of variants (easier or equivalent integrals). Variant generation uses a transformation library (mathematical transformations affecting difficulty), samples 3–5 transformations per variant, generates variants in batches of ~10 per prompt for diversity, cycles sampling temperature between 0.8 and 1.4, and rotates 'persona' prompts (e.g., 'think like Euler') to increase variety. Trees are generated recursively (depth capped at 2 for MIT experiments, 3 in Llama ablations) producing many easier sub-problems; the model is then trained with GRPO using a numerical verifier as the reward signal. The curriculum therefore generates tasks (simpler integrals/subgoals) conditioned on the original problem and its generated transformations.",
            "domain_name": "Mathematical integration (indefinite integrals; MIT Integration Bee problems)",
            "domain_characteristics": "Formal, highly structured mathematical domain with compositional techniques (substitution, partial fractions, nested/combined functions); small coefficient or structural changes can drastically change difficulty; tasks are verifiable numerically; not open-ended but combinatorially varied; requires symbolic reasoning and composition of multiple techniques; medium-to-long reasoning chains but bounded by calculus methods.",
            "state_conditioning": false,
            "state_conditioning_details": null,
            "novelty_mechanism": true,
            "novelty_mechanism_details": "Diversity-promoting prompting: batch generation (10 per prompt) to increase variety, temperature cycling (0.8–1.4) for creative sampling, persona-based prompts to elicit different mathematical perspectives, and recursive branching to cover difficulty spectrum. Also explicit skewing of difficulty distribution (e.g., 70% easier / 30% equivalent for MIT experiments).",
            "complementary_systems": "Numerical solution verifier (multi-point adaptive quadrature, singularity handling, timeouts/filters), GRPO reinforcement learning, transformation library, quality-control filters to reject trivial/degenerate outputs.",
            "performance_llm_curriculum": "LADDER (RL with LLM-generated variants) yielded large gains: Llama 3.2 3B accuracy improved from pass@1 1% (base) / pass@10 2% to 82% on a 100-problem undergraduate integration test set after training on variants generated from 10 training problems (~500 variants per train problem, ~5000 variants total). On the 2025 MIT Integration Bee qualifying exam, LADDER applied to Qwen2.5 7B increased accuracy from a 7B baseline of 50% to 73%.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": "Compared to RL without variants (i.e., training only on the small curated training set), RL without variants failed: performance never exceeded 3% and collapsed to 0% within ~30 training steps. This indicates heuristics/manual small-set training performed poorly under identical hyperparameters.",
            "performance_no_curriculum": "Base model baselines: Llama 3.2 3B pass@1 = 1%, pass@10 = 2% on the undergraduate test. Qwen2.5 7B baseline on MIT exam = 50% (pre-LADDER).",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "Llama experiments: ~500 variants per training problem (10 train problems → ~5,000 variants); exact-match overlap with test set minimal (5 matches among 5,000). MIT experiments: ~9,000 variants generated (two-level recursion). In Llama variant generation, ~8% of variants were effectively unsolvable by the symbolic solver (timeout &gt;5s).",
            "transfer_generalization_results": "Strong generalization reported: Llama 3B trained on only 10 problems plus variants generalized to 100 held-out test integrals achieving 82% accuracy. LADDER-trained Qwen2.5 7B generalized to 2025 MIT exam questions (73% accuracy), none of which appeared in the variant set.",
            "computational_cost": "Not reported in monetary terms. Empirical costs: variant generation scaled to thousands (5k–9k) of integrals; verification uses multi-point adaptive quadrature with a 2s per-attempt timeout and up to three retries; training used GRPO with batch size up to 128 for MIT experiments. Authors note verification and unsolvable variants consumed significant compute but provide no wall-clock/time or dollar figures.",
            "failure_modes_limitations": "Variant generation quality control issues: some purportedly 'easier' variants became harder due to small perturbations (e.g., coefficients introducing complex roots); ~8% unsolvable variants in Llama runs; harder-than-parent variants consume verification compute and are ignored during training via zero reward but waste resources. LADDER struggles on integrals requiring synthesis of multiple techniques and novel substitution patterns; recursive depth beyond 2–3 may produce trivially simple or irrelevant variants. Reliance on a numerical verifier may allow subtle verification exploits if not carefully filtered.",
            "long_horizon_performance": null,
            "specialized_domain_performance": "In the specialized domain of calculus/integration, the curriculum is highly effective: small (7B) models trained with LADDER outperform much larger models on the MIT Integration Bee benchmark (LADDER 73% vs GPT-4o 42% reported; note differences in verifiers/constraints).",
            "ablation_studies": "Ablations include: RL with variants (LADDER) vs RL without variants (training on only the original small curated set) under identical hyperparameters — only LADDER succeeded. Variant-generation ablations: batch size, temperature cycling, and persona prompting were found empirically important for variant diversity (no quantitative % given). Increasing variant corpus improved performance up to diminishing returns (~improvement slowed past 6,000 variants).",
            "model_size_scaling": "Experiments used 3B (Llama 3.2) and 7B (Qwen2.5) models. No systematic sweep across many sizes; authors report large gains for mid-sized models but do not present continuous scaling curves for curriculum generation quality vs model size.",
            "key_findings_curriculum_effectiveness": "LLM-generated curricula (recursive variant trees) create a smooth difficulty gradient that is essential for successful RL: LADDER moved a 3B model from ~1–2% to 82% on undergraduate integrals and a 7B model from 50% to 73% on the MIT Integration Bee. RL without such a curriculum collapses or fails (≤3%). Variant diversity mechanisms (batch generation, temperature cycling, persona prompting, recursion) and a reliable verifier are key to effectiveness; scaling the number of variants increases performance but with diminishing returns.",
            "uuid": "e2048.0"
        },
        {
            "name_short": "TTRL",
            "name_full": "Test-Time Reinforcement Learning",
            "brief_description": "A test-time curriculum procedure that generates problem-specific variant trees at inference and runs short targeted RL (GRPO) on those variants to adapt the model to the particular test instance before answering, then rolls back the model for the next question.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated (per-test-instance)",
            "llm_model_name": "Qwen2.5 7B (applied in MIT experiments); base model used for generation in ablations",
            "llm_model_size": "7B",
            "curriculum_description": "For each difficult test question, TTRL generates a focused tree of variants (recursion depth typically 2, ~800 variants per unsolved test question in MIT experiments), then runs GRPO for a fixed small number of steps (100 RL steps reported) using the variant tree and numerical verification reward. If the tuned model produces a correct solution at any point it's counted as solved; after the procedure the model parameters are rolled back to pre-TTRL for the next test instance. The generation uses same transformation library, batch/temperature/persona diversity techniques, and quality-control filters as LADDER.",
            "domain_name": "Mathematical integration (MIT Integration Bee qualifying exam)",
            "domain_characteristics": "Same as LADDER: formal mathematical domain with compositional solution techniques, high sensitivity to structural perturbations, and verifiability by numerical methods; test instances often cleverly constructed and out-of-distribution relative to standard training data.",
            "state_conditioning": false,
            "state_conditioning_details": null,
            "novelty_mechanism": true,
            "novelty_mechanism_details": "Per-instance variant diversity via batch generation, temperature cycling, persona prompts and intentionally mixing easier and equivalent variants (authors used e.g., 70% easier / 30% equivalent) to encourage exploration of novel solution patterns relevant to that specific test problem.",
            "complementary_systems": "Numerical verifier, GRPO RL, per-question variant generator, quality-control filters; rollback mechanism to revert model after per-question tuning.",
            "performance_llm_curriculum": "Applied after LADDER, TTRL increased Qwen2.5 7B performance on the 2025 MIT Integration Bee from 73% (LADDER alone) to 90% (LADDER + TTRL). TTRL solved an additional 3–4 problems out of 20 previously failed by LADDER; steps required per solved question ranged from ~3 to ~30 RL steps.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": "TTRL applied to the base (pre-LADDER) model failed to solve any of the previously LADDER+TTRL-solved questions after 100 steps, indicating TTRL alone (without prior curriculum exposure) is ineffective under the tested compute budget.",
            "performance_no_curriculum": "Not applicable beyond the base-model baseline: base model + TTRL (no prior LADDER) produced no solved problems in the attempted set.",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "Per-problem TTRL used ~800 variants and produced solutions for some questions within 3–30 RL steps; no aggregate diversity counts beyond per-problem variant counts reported.",
            "transfer_generalization_results": "TTRL is explicitly per-instance and primarily improves on the specific test problem; the authors observed that TTRL is only effective when applied to a model that previously saw variant-based training (LADDER) — indicating reliance on transfer/generalization from prior curriculum exposure.",
            "computational_cost": "Reported procedural costs: per-problem variant generation (~800 variants) and 100 RL steps per failed test question in MIT experiments; no wall-clock time or monetary cost provided. Authors note TTRL is a form of test-time compute scaling and is parallelizable across compute units.",
            "failure_modes_limitations": "TTRL depends on prior LADDER-style exposure: when applied to base model without LADDER pretraining it failed to solve problems under identical compute budget. Some hard integrals remained unsolved even after LADDER+TTRL (two MIT questions remained incorrect). Computational cost per test instance can be high due to large variant sets and RL steps. Variant quality control and verifier limits remain sources of inefficiency.",
            "long_horizon_performance": null,
            "specialized_domain_performance": "Highly effective in the specialized integration domain when used on top of LADDER-trained models: enabled a 7B model to reach state-of-the-art performance (90%) on the MIT Integration Bee qualifying exam, surpassing larger models under the paper's evaluation conditions.",
            "ablation_studies": "Key ablation: TTRL applied to pre-LADDER base model (100 steps) -&gt; no solved problems, demonstrating the necessity of prior LADDER training for TTRL effectiveness. No additional ablation quantifying number of TTRL steps or variant counts reported beyond reported settings.",
            "model_size_scaling": "TTRL experiments reported on a 7B model only; no systematic study of different model sizes for TTRL was provided.",
            "key_findings_curriculum_effectiveness": "Test-time LLM-generated curricula (TTRL) can substantially boost per-instance performance when combined with prior variant-based training (LADDER): +17 percentage points on the MIT exam (73%→90%) with ~800 variants and 100 RL steps per problem; TTRL alone on untrained base models did not produce gains under the tested compute budget.",
            "uuid": "e2048.1"
        },
        {
            "name_short": "Variant Generation (LLM-based)",
            "name_full": "LLM-based Variant Generation (transformation library, batch prompting, persona and temperature cycling)",
            "brief_description": "The concrete LLM-driven mechanism that synthesizes easier/equivalent problem variants using a transformation library, batched prompts, temperature cycling, and persona-based prompting, producing recursive variant trees used as curricula for RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated",
            "llm_model_name": "Base LLM (Llama 3.2 3B in ablations; Qwen2.5 7B for MIT experiments and variant generation)",
            "llm_model_size": "3B; 7B",
            "curriculum_description": "Authors first build a transformation library of mathematical transformations (e.g., reduce exponent, simplify denominators, introduce nested functions). For each integral, they sample 3–5 transformations and prompt the model to produce batches (~10 per prompt) of variants, cycle sampling temperature between 0.8 and 1.4, and rotate persona prompts to increase diversity. Variants are generated recursively into trees (depth capped at 3 in Llama experiments, 2 in MIT setup). The policy then uses these generated tasks as RL training data with verifier-based rewards.",
            "domain_name": "Mathematical integration",
            "domain_characteristics": "See LADDER: formal, compositional, sensitive to small structural changes, verifiable numerically, not open-ended but rich combinatorial space of functions and techniques.",
            "state_conditioning": false,
            "state_conditioning_details": null,
            "novelty_mechanism": true,
            "novelty_mechanism_details": "Diversity techniques: batch generation to avoid repetition, temperature cycling for varied outputs, persona-based prompts to elicit different mathematical reasoning styles, and recursive branching to cover a range of difficulty levels. Authors empirically tuned fraction of easier vs equivalent variants (e.g., 70% easier / 30% equivalent for MIT).",
            "complementary_systems": "Transformation library, numerical verifier, timeout/filters for unsolvable outputs, and downstream GRPO RL.",
            "performance_llm_curriculum": "Variant-generation scale metrics: Llama experiments used ~500 variants per training problem; MIT experiments generated ~9,000 variants overall; Llama variant set produced minimal overlap with test set (5/5,000). Empirical effect: more variants correlated with higher LADDER performance up to diminishing returns (improvement rate slowed beyond ~6,000 variants).",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": "Empirical comparisons show that without variant generation (i.e., RL on raw small training set) training collapses or fails (≤3%); this demonstrates superiority of generated curricular variants versus training only on small curated sets or naive RL.",
            "performance_no_curriculum": "See LADDER baselines: base pass@1 and pass@10 were 1% and 2% respectively on the undergraduate set.",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "Batch generation and persona cycling increased diversity; concrete counts: ~5,000 Llama variants (500 per training problem), ~9,000 MIT variants, exact-match overlap with test set extremely low (5 matches among 5,000), ~8% unsolvable by symbolic solver in Llama runs.",
            "transfer_generalization_results": "Variant corpus generated from 10 training problems enabled generalization to 100 unseen test integrals with 82% accuracy for Llama 3B; MIT variant corpora enabled generalization to the 2025 qualifying exam (73% LADDER for 7B).",
            "computational_cost": "Generation produced thousands of variants; verification required adaptive quadrature evaluations across 5 random sample intervals per candidate with 2s timeout per attempt and up to three retries — authors note significant compute from unsuccessful/harder-than-intended variants but provide no explicit cost figures.",
            "failure_modes_limitations": "Generation sometimes produced variants that were harder than the parent or unsolvable (approx 8% unsolvable); small coefficient/structural changes can unexpectedly increase complexity; naive generation without diversity constraints led to repetitive or low-quality variants; verification cost wasted on hard/unintended variants.",
            "long_horizon_performance": null,
            "specialized_domain_performance": "Highly effective in integration domain; variant generation produced curriculum items that enabled mid-sized models to learn techniques and generalize across many unseen integrals.",
            "ablation_studies": "Observed effects of generation choices: batch size matters (batch of 10 improved diversity; smaller batches repeated; larger batches degraded quality), temperature cycling and persona prompting crucial for higher diversity, recursion depth beyond 2–3 became less useful or produced trivial variants; increasing total variants improved performance with diminishing returns past ~6k variants.",
            "model_size_scaling": "Variant generation and its effects were tested with 3B and 7B models. No large-scale model-size sweep for generation quality was reported.",
            "key_findings_curriculum_effectiveness": "Carefully engineered LLM-driven variant generation (transformation library + batch/temperature/persona/recursion) produces high-diversity curricula that enable RL to succeed where naive RL fails; quality-control and diversity mechanisms are essential to avoid wasted compute on unsolvable or harder-than-intended variants.",
            "uuid": "e2048.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models can teach themselves to program better",
            "rating": 2
        },
        {
            "paper_title": "Bootstrapping reasoning with reasoning",
            "rating": 2
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2
        },
        {
            "paper_title": "Curriculum learning for reinforcement learning domains: A framework and survey",
            "rating": 2
        },
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1
        },
        {
            "paper_title": "Learning to reason with llms",
            "rating": 1
        }
    ],
    "cost": 0.01532625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LADDER: SELF-IMPROVING LLMS THROUGH RECURSIVE PROBLEM DECOMPOSITION
March 6, 2025</p>
<p>Toby Simonds 
Tufa Labs</p>
<p>Akira Yoshiyama 
Tufa Labs</p>
<p>LADDER: SELF-IMPROVING LLMS THROUGH RECURSIVE PROBLEM DECOMPOSITION
March 6, 2025D769F5E67930E3B1F8FBC2B255AA2641arXiv:2503.00735v3[cs.LG]
We introduce LADDER (Learning through Autonomous Difficulty-Driven Example Recursion), a framework which enables Large Language Models to autonomously improve their problem-solving capabilities through self-guided learning by recursively generating and solving progressively simpler variants of complex problems.Unlike prior approaches that require curated datasets or human feedback, LADDER leverages a model's own capabilities to generate easier question variants.We demonstrate LADDER's effectiveness on the subject of mathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on undergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to achieve 73% on the MIT Integration Bee qualifying examination.We also introduce TTRL (Test-Time Reinforcement Learning), where we perform reinforcement learning on variants of test problems at inference time.TTRL enables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of 90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's performance.These results show how self-directed strategic learning can achieve significant capability improvements without relying on architectural scaling or human supervision.</p>
<p>Introduction</p>
<p>Reinforcement Learning (RL) has emerged as a highly effective approach for training Large Language Models (LLMs), yet its success hinges critically on the availability of appropriate training tasks [5,9,10,12].A fundamental challenge lies in obtaining verifiable tasks that match the model's current capabilities.For RL to be effective, tasks must form a gradient of difficulties that allows for incremental learning progress [8].When tasks exceed the model's current abilities, the training process not only stalls but can lead to catastrophic collapse, resulting in degraded performance.This challenge is particularly acute in domains requiring complex reasoning, where the gap between simple and advanced tasks can be substantial [4,12].</p>
<p>We propose Learning through Autonomous Difficulty-Driven Example Recursion (LADDER), a framework that enables LLMs to autonomously improve their problem-solving capabilities through strategic self-guided learning.The key insight is that models can bootstrap their own learning by recursively generating and solving progressively simpler variants of complex problems.For each challenging problem, LADDER prompts the model to create multiple easier variants, forming a natural difficulty gradient.This process continues recursively, with each variant spawning simpler sub-variants, until reaching problems the model can reliably solve.The solutions to these simpler problems then provide stepping stones for tackling progressively harder variants.We demonstrate that this self-bootstrapping approach achieves dramatic improvements beyond what's possible through standard techniques like pass@k sampling -enabling models to reliably solve problems that were previously far beyond their capabilities.</p>
<p>Unlike previous approaches requiring carefully curated datasets or human feedback, LADDER leverages the model's existing capabilities to create a natural difficulty gradient, allowing for systematic improvement through reinforcement learning with verifiable rewards.The framework requires only a reliable verification mechanism -in our case, numerical integration for checking solutions.This enables the model to assess its own progress and guide its learning trajectory without human intervention.</p>
<p>We demonstrate LADDER's effectiveness on mathematical integration tasks, achieving remarkable improvements across multiple benchmarks.Using this approach, we improve a Llama 3B model's accuracy from 1% to 82% on undergraduatelevel integration problems.When applied to the challenging 2025 MIT Integration Bee examination, LADDER enables a 7B parameter model to achieve 73% accuracy, significantly outperforming much larger models, such as GPT-4o (42%), and typical human performance (15-30%).These results showcase how strategic problem decomposition and verified self-learning can achieve substantial capability improvements without relying on architectural scaling or human supervision.</p>
<p>Building on LADDER's self-improvement framework, we propose Test-Time Reinforcement Learning (TTRL), a novel approach that extends these principles to inference time.TTRL dynamically generates problem variants during test-time and applies reinforcement learning to refine the model's solutions, effectively creating a micro-learning process for each test instance.By leveraging the same verification mechanisms used in training, TTRL enables the model to further improve its performance.When applied to the 2025 MIT Integration Bee, TTRL boosts accuracy from 73% -with just LADDER -to 90%, demonstrating how scaling test-time compute through strategic problem decomposition can yield substantial performance improvements.We achieve state of the art accuracy, outperforming significantly larger models, such as OpenAI's o1.Thus, we make the following contributions:</p>
<p>• We propose a novel framework for autonomous model improvement through recursive problem decomposition and self-guided learning via reinforcement learning with GRPO.</p>
<p>• We develop a systematic method for generating and verifying problem variants that create natural difficulty gradients, requiring only numerical verification.</p>
<p>• We demonstrate significant empirical improvements on mathematical reasoning tasks, improving a Llama 3B model from 2% to 82% on undergraduate integration problems and achieving 73% accuracy on the MIT Integration Bee with a 7B model, matching SoTA performance</p>
<p>• We introduce Test-Time Reinforcement Learning (TTRL), a method for scaling compute at inference time through variant generation and reinforcement learning, boosting performance on the MIT Integration Bee from 73% to 90%.</p>
<p>Related Work</p>
<p>Self-Improvement and Automated Curriculum Generation in LLMs.Haluptzok et al. (2023) present a self-play setup where a code-focused LLM continually invents and solves new programming puzzles, checks solutions with a Python interpreter, and fine-tunes on correct results [3].We build upon the generate → solve → verify → learn cycle, adding a more explicit curricular element to guide the model's step-by-step improvement.</p>
<p>Recent advances such as STaR (Self-Taught Reasoner) have illustrated that LLMs can improve their reasoning skills by learning from their own generated chain-of-thoughts-effectively acting as both student and teacher [15].Step" [6] demonstrated that breaking down verification into explicit steps and allocating additional computation to each verification component can significantly improve performance.Similarly, Tree of Thoughts [14] showed how systematically exploring multiple reasoning paths during inference can lead to better problem-solving capabilities.These approaches typically focus on increasing the length and deliberation of model outputs, either through structured prompting, such as s1, or systematic exploration of solution spaces, such as self-consistency [7,12].Our work differs fundamentally from these approaches by introducing dynamic learning at test-time -rather than just increasing reasoning length or exploring multiple paths, TTRL enables the model to actually improve its capabilities through practice on related problems.This represents a shift from static inference-time techniques to dynamic adaptation through targeted learning.</p>
<p>Reinforcement Learning Approaches for LLM Self-Improvement.In recent years, reinforcement learning has emerged as a pivotal strategy for enabling language models to self-improve through iterative self-correction and feedback.Notably, OpenAI's o1 model uses reinforcement learning to refine its chain-of-thought reasoning-learning to "think" step by step [9].Similarly, DeepSeek's R1 model leverages a reinforcement learning framework that minimizes human supervision by generating and self-evaluating its own training data [2].</p>
<p>Importantly, recent work has shown that RL approaches may generalize much better than Supervised Fine-Tuning (SFT), which appears to memorize instead [1,13].RL approaches have also shown to be highly effective in generalizing to out-of-distribution tasks, particularly in competitive mathematics [13].These results strongly suggest that reinforcement learning offers a more promising path to self-improvement than supervised fine-tuning.</p>
<p>These innovations underscore a paradigm shift-from externally curated curricula and human-labeled data toward autonomous, feedback-driven self-improvement in language models.LADDER is a structured framework designed to enhance LLM problem-solving capabilities through recursive problem decomposition and reinforcement learning.We demonstrate the effectiveness of LADDER in the domain of mathematical integration.LADDER consists of the following components:</p>
<p>• Variant Generation: A structured method for generating trees of progressively simpler variants of complex problems, establishing a natural difficulty gradient.</p>
<p>• Solution Verification: A numerical integration method to verify the solution of an integral.</p>
<p>• Reinforcement Learning: The protocol used to train the base model on the variant trees.</p>
<p>LADDER is a two-step algorithm.First, the training set of integrals is collected and a tree of variants is generated for each integral as described in section 3.1.2.Second, we perform the reinforcement learning protocol described in section 3.1.4on a base model using the set of variant trees as the training set.The resulting model should then have significantly improved and generalized mathematical integration capabilities, and can then be prompted to solve new integrals.See Algorithm 1 for LADDER pseudocode.V LADDER is the set of variants generated for all questions in the train set of integrals and a i is the answer to the ith test question.</p>
<p>The number of variants N generated per question is a hyperparameter, which we vary for different experiments.We experiment with LADDER in sections 3.2.1 and 3.2.2.</p>
<p>Variant Generation</p>
<p>The variant generation process required careful design to ensure sufficient diversity and appropriate difficulty scaling.We found that naive approaches to variant generation -simply asking the model to generate easier versions -led to repetitive patterns and limited diversity.Through experimentation, we developed a multi-stage process that dramatically improved the quality and variety of generated variants.</p>
<p>Algorithm 1 LADDER 1: Q train , Q test ← train and test sets of integrals 2: N ← number of variants to generate 3: G(q, j) : generates the jth variant of integral q
4: V LADDER ← {v i,j ∈ G(Q train [i], j) | 1 ≤ i ≤ |Q train |, 1 ≤ j ≤ N } 5: π θLADDER ← GRPO(π θbase , V LADDER ) 6: for i ← 1, |Q test | do 7: a i ∼ π θLADDER (• | Q test [i]) 8: end for Algorithm 2 TTRL 1: Q test ← test set of integrals 2:
N ← number of variants to generate 3: G(q, j) : generates the jth variant of integral q 4: for i ← 1, |Q test | do 5:
V TTRL,i ← {v j ∈ G(Q test [i], j) | 1 ≤ j ≤ N } 6:
π θTTRL,i ← GRPO(π θbase , V TTRL,i )</p>
<p>7:
a i ∼ π θTTRL,i (• | Q test [i]) 8: end for
First, we used the base model to generate an extensive set of mathematical transformations that could be applied to integrals, categorized by their impact on problem difficulty.These included operations ranging from simple (e.g., reducing exponent values, simplifying denominators) to more complex (e.g., introducing nested functions, combining multiple function types).This transformation library served as a foundation for guiding variant generation.</p>
<p>Second, for each integral, we randomly sampled 3-5 transformations and provided them as explicit suggestions to the variant generation model.Critically, we found that generating variants in batches of 10 per prompt significantly improved diversity -smaller batch sizes led to more repetitive outputs, while larger batches reduced quality.To further enhance variation, we employed two key techniques:</p>
<ol>
<li>Temperature cycling: We dynamically varied the sampling temperature between 0.8 and 1.4 across prompts.</li>
</ol>
<p>This helped balance between creativity and mathematical validity.</p>
<p>Persona-based prompting:</p>
<p>We prompted the model to adopt different mathematical perspectives (e.g., "think like Euler focusing on series", "approach like Gauss looking for patterns").</p>
<p>The combination of batch generation, varied temperatures, and rotating personas proved crucial led to significantly more diverse variants.Without them the model would often converge to generating very similar and often repeating variants.</p>
<p>Third, to ensure sufficient coverage of the difficulty space, we applied this generation process recursively, generating a tree of variants, each a simpler integral than its parent.We found that this also helped improve variant diversity.This recursive approach helped build natural difficulty gradients -each problem could spawn multiple easier variants, which in turn could generate even simpler problems.We capped tree depth at three to maintain problem relevance.The following is an example of one root-to-leaf path in a variant tree:</p>
<p>Original problem:
x 2 + 1 x 4 + 2x 2 + 1 dx Level 1 variant: x 2 x 4 + 1 dx Level 2 variant: 1 x 2 + 1 dx Level 3 variant: 1 x 2 dx
Quality control presented a significant challenge in our variant generation process.While we prompted the model to generate "easier" or "equivalent" variants, the actual difficulty of the resulting integrals often deviated substantially from the intended level.Small perturbations in coefficients or function composition could transform seemingly simple integrals into much harder ones -for instance, changing a coefficient from 1 to 2 in a rational function could introduce complex roots that make the integral significantly more challenging.</p>
<p>For our Llama experiments, approximately 8% of generated variants were effectively unsolvable (i.e. it took a symbolic system more than 5 seconds to solve), and many more were substantially harder than their parent problems despite being intended as simpler variants.While these harder variants were naturally ignored during training through zero reward signals, they consumed significant computational resources in verification attempts and represented wasted generation capacity.Future work could explore methods to better constrain variant generation to maintain intended difficulty levels while preserving mathematical diversity.</p>
<p>We apply the above three-step variant generation method to generate the sets of variant integrals in both LADDER and TTRL.V LADDER is the set of variants for all questions in the train set Q train and V TTRL,i is the set of variants for the ith question in the test set, Q test [i].See Algorithms 1 and 2.</p>
<p>Solution Verification</p>
<p>Integral variant generation occurs at the beginning of LADDER.Throughout the process of performing RL in LADDER we must also perform solution verification.In order to do so, we developed a robust numerical verification framework that balanced accuracy with computational efficiency.The key challenge was ensuring reliable verification across different types of integrals while handling edge cases and potential numerical instabilities.</p>
<p>For each candidate solution to an integral, we employed a multi-point numerical comparison approach.We randomly sampled five points from the domain [-10, 10] and evaluated both the candidate solution and the original integral over small intervals of length 0.1 centered at these points.This interval length was chosen empirically to minimize numerical integration errors while maintaining sensitivity to local solution behavior.The use of multiple small intervals, rather than a single large one, helped detect both local and global errors in the solutions.</p>
<p>Our verification protocol included several key components:</p>
<p>• Singularity handling: When a sampled point was near a singularity (detected through rapid value changes or numerical overflow), we automatically resampled to a new point.This adaptive sampling ensured robust verification even for functions with challenging behavior.• Numerical precision: Solutions were deemed correct if their values exhibited a relative difference of 10 −2 or less compared to numerical integration results across all test intervals.We found empirically that this tolerance level effectively balanced false positives and negatives.• Timeout management: To handle computationally intensive integrals, we implemented a 2-second timeout for each verification attempt.If timeout occurred, new evaluation points were sampled, with up to three retries before marking the solution as unverifiable.• Edge case detection: Early iterations revealed that models could exploit verification weaknesses by outputting trivial solutions (like the integration symbol itself) or degenerate forms.We added specific filters to detect and reject such cases, ensuring solutions demonstrated genuine understanding.</p>
<p>All numerical integrations were performed using adaptive quadrature methods, with automatic precision adjustment based on the integrand's complexity.This approach provided reliable results even for oscillatory and highly nonlinear functions, while maintaining reasonable computational efficiency.</p>
<p>We note, however, that the numerical verifier may not be 100% accurate.We opted for a numerical approach rather than a symbolic one because many problems beyond integration can be numerically verified despite lacking a symbolic solver.</p>
<p>We apply the above solution verification in our RL Protocol in section 3.1.4and experiments in sections 3.2.1,3.2.2 and 3.2.3.In our MIT experiments, final benchmark results are verified directly against the official solutions.</p>
<p>Reinforcement Learning Protocol</p>
<p>We decided to employ Group Relative Policy Optimization (GRPO) in both LADDER and TTRL [11].GRPO does not use a separate critic model and instead estimates the baseline from group scores, improving efficiency and reducing memory.For each question q, GRPO samples a group of outputs o 1 , o 2 , ..., o G from the old policy π θ old and then optimizes the policy model π θ by maximizing the following objective:
J GRPO (θ) = E q ∼ P (Q), {o i } G i=1 ∼ π θ old (O | q) 1 G
where ϵ and β are hyperparameters, and A i is the advantage, computed using a group of rewards r 1 , r 2 , ..., r G corresponding to the outputs within each group:
A i = r i − mean {r 1 , r 2 , . . . , r G } std {r 1 , r 2 , . . . , r G }(3)
We adopted a simple, rule-based reward model.The reward model is kept simple in order to be straightforward to apply to other verifiable domains in the future.The reward model consists of two rewards:</p>
<p>• Accuracy reward: Using the solution verification method described in 3.1.3,we evaluate whether the response is correct or not.The model is required to provide the final answer in a specified format (i.e. in <ANSWER> </ANSWER> tags), enabling reliable rule-based verification of correctness.</p>
<p>• Format reward: In addition to the accuracy reward model, we employ a format reward that encourages the model to put its answer in <ANSWER> </ANSWER> tags.</p>
<p>We perform the RL runs with a KL divergence coefficient of 0.001.Training batch size and number of epochs differ among our experiments.</p>
<p>Test-Time Reinforcement Learning</p>
<p>Test-Time Reinforcement Learning (TTRL) extends our LADDER framework to inference time, enabling dynamic adaptation to challenging problems directly during testing.Upon encountering a difficult integration problem, TTRL generates a focused set of related variants and conducts targeted reinforcement learning specifically for that problem.By recursively decomposing the challenging problem into simpler variants and learning from this focused curriculum, TTRL allows the model to rapidly develop problem-specific expertise without hand-crafting variants or architectural modifications.This provides a novel approach to scaling compute at test-time -rather than simply increasing output sampling or model size, TTRL leverages compute to dynamically improve the model's problem-solving capabilities through focused practice on variants of the specific challenge at hand.</p>
<p>The same three components of LADDER are also used in Test-Time Reinforcement Learning (TTRL).For each question at test-time, there are two steps.First, we generate a tree of variants for the test question at hand.Second, we perform the reinforcement learning protocol described in section 3.1.4on a base model using the variant tree as the training set.The resulting model should then have significantly improved mathematical integration capabilities tuned to the test question at hand.The test question is then answered using the tuned model, and finally the model is rolled back to its original parameters for the next test question.See Algorithm 2 for TTRL pseudocode, where V TTRL,i is the set of variants generated for the ith test question and a i is the answer to the ith test question.</p>
<p>As in LADDER, the number of variants generated per question is a hyperparameter, which we vary for different experiments.We experiment with TTRL in section 3.2.3.</p>
<p>Experiments</p>
<p>Llama 3B Experiments</p>
<p>We first experiment with LADDER using a Llama 3.2 3B parameter model as our base architecture to allow us to more quickly run ablation experiments.To establish our evaluation dataset, we developed a comprehensive collection of 110 indefinite integration problems, combining questions sourced from university-level mathematics curricula with synthetically generated problems using GPT-4o.To ensure appropriate difficulty calibration, we benchmarked each problem to verify it was solvable by GPT-4o but beyond the capabilities of the base Llama 3.2 3B model.This was done purposefully to benchmark performance on problems that can still be solved by an LLM but are outside the scope of Llama 3.2 3B.</p>
<p>We randomly split this collection into a training set of 10 problems and a test set of 100 problems.we generated approximately 500 variants for each of our 10 training problems.To validate the diversity of our variant set, we performed exact matching against our test set and found minimal overlap (only 5 matches among 5,000 variants).The small training set was deliberately chosen to demonstrate our method's effectiveness in generating useful variants from limited examples.Approximately half of the problems were sourced from curriculum textbooks, with the remainder synthetically generated by GPT-4o.Notably, the synthetic problems were indistinguishable from curriculum-sourced ones.During training, models were prompted to express solutions in sympy algebraic notation without integration constants, ensuring consistent evaluation across different but mathematically equivalent expressions of the same solution.This standardization was crucial for reliable verification of solution correctness during the training process.</p>
<p>We performed two RL experiments with Llama 3.2 3B.The first experiment trained on only the 10 question training set (RL w/o variants).The second experiment trained on only variants generated from the 10 question training set (RL w/ variants, i.e.LADDER).Both runs used identical hyper parameters and were continued until performance plateaued.</p>
<p>MIT Integration Bee (LADDER)</p>
<p>Building off the findings from our Llama 3B experiemtns, we extended our methodology to the 2025 MIT Integration Bee qualifying examination, an annual competition that attracts both undergraduate and graduate students from MIT, with participants often having competitive mathematics backgrounds.The qualifying exam, which serves to select 16 finalists, contains 20 questions and has a typical qualifying threshold of 73%, though most participants score between 15-30%, with 50% to 73% out of 20 considered strong performance.Students are given 20 minutes to attempt the entire exam.</p>
<p>Using the DeepSeek-R1 distilled Qwen 2.5 7B model, we applied our variant generation approach to historical qualifying exams from 2010-2024.Our variant generation followed a two-level tree structure -first generating variants from each source problem, then generating additional variants from those first-level variants.We capped recursion at depth two, as preliminary experiments showed third-level variants became trivially simple and lost mathematical relevance to the original problems.With this approach we generated 9,000 variants.</p>
<p>Through experimentation with different difficulty distributions, we settled on prompting the model to generate 70% easier and 30% equivalent variants than their parent problems.While we initially attempted to include more challenging variants, we found this skew toward easier problems created more effective learning trajectories.Notably, many "equivalent" variants still introduced novel mathematical patterns while maintaining similar difficulty levels, contributing to the model's generalization capabilities.</p>
<p>We verified that none of the 2025 exam questions appeared in our variant set, though we note this precaution was primarily for methodological rigor rather than necessity, as our model never accessed the test set or solutions during training.We apply the same hyperparameters as in the 3B experiments, except for modifying the batch size to 128.</p>
<p>MIT Integration Bee (LADDER + TTRL)</p>
<p>After applying LADDER, there remain certain questions which the tuned model continues to fail to answer.For each of these questions we further apply TTRL.For each unsolved problem, we generate a tree of variants following the same process illustrated in Figure 1, but limited to two levels of depth and approximately 800 total variants.Using these problem-specific variants, we conduct 100 steps of reinforcement learning with identical RL parameters as in the MIT Integration Bee LADDER setup.The problem is considered "solved" if the model produces a solution at any point during this process, as verified by our numerical integration framework.</p>
<p>TTRL's approach of on-the-fly data synthesis means it can handle novel integrals better by expanding its training setfocused on that problem -at test time.This is especially important for challenges such as the MIT Integration Bee, where many integrals are cleverly constructed and might be out-of-distribution relative to standard calculus problems.</p>
<p>Further, TTRL provides a systematic approach to scaling performance through additional compute.Rather than simply increasing sampling or temperature parameters, TTRL enables active improvement of problem-solving capabilities through focused practice at test-time.</p>
<p>Results</p>
<p>B</p>
<p>Llama 3B Experiments</p>
<p>Our experiments evaluated whether generating easier variants could enable effective reinforcement learning in mathematical integration tasks on Llama 3.2 3B.We tested four conditions: base model with pass@1 samping, base model with pass@10 sampling, RL without variants (only on the training set), and LADDER (RL with variants from the training set).</p>
<p>In Figure 2, we observe a consistent and rapid improvement from step 0 to 250, with the model's performance quickly ascending before ultimately plateauing.This swift progression suggests an effective learning mechanism, characterized by substantial performance gains that stabilize around 82% accuracy as the model approaches its performance ceiling.</p>
<p>LADDER achieved 82% accuracy on the test set, significantly outperforming both the base model with pass@1 sampling (1%) and with pass@10 sampling (2%).RL without variants consistently failed, with performance never exceeding 3% before collapsing to 0% within 30 training steps, highlighting the necessity of a smooth difficulty gradient for successful training (see Figure 4).</p>
<p>Analysis of the remaining unsolved problems reveals nuanced challenges in the model's mathematical integration capabilities.The model demonstrates difficulty with integrals requiring multiple technique combinations and novel substitution patterns.These complex scenarios highlight limitations in the current approach, suggesting that while the variant generation successfully teaches individual techniques, synthesizing these methods for more intricate problems remains a challenge.</p>
<p>We observed a clear upward trend in performance as the number of generated variants increased, with no definitive plateau in the graph (see Figure 3).While the rate of improvement appears to slow beyond 6,000 variants, the data suggests that continued scaling may still yield further gains.This indicates that generating more variants could enhance performance, though with diminishing returns relative to earlier stages of training.Rather than identifying a strict optimal corpus size, our findings highlight a trade-off between computational cost and incremental improvements, suggesting that additional scaling may remain beneficial depending on resource availability.</p>
<p>The stark improvement from base model pass@10 (2%) to LADDER (82%) demonstrates genuine acquisition of mathematical abilities rather than merely improved output sampling.The consistent failure of RL without variants under identical hyperparameters confirms that success stems from the carefully constructed difficulty gradient rather than the RL algorithm itself, suggesting potential applications to other complex reasoning tasks where direct training proves ineffective.</p>
<p>MIT integration Bee (LADDER)</p>
<p>Applying our methodology to the MIT Integration Bee led to a significant improvement in model performance.LADDER improved the accuracy of the Deepseek-R1 Qwen 2.5 7B base model from a baseline of 50% to 73% on the 2025 qualifying examination, significantly outperforming other existing models, including GPT-4o (42%).See Figure 5.</p>
<p>For comparison, most human participants score between 3-6 points out of 20 (15-30%), with scores above 7-12 (35-73%) considered strong performances.With a 73% accuracy, our model meets the qualification threshold of 14/20.However, while LADDER achieved substantial gains, it still did not reach the performance of o1 (80%).</p>
<p>The consistent performance gap between our trained model and base models highlights the effectiveness of our variantbased recursive training approach in improving mathematical reasoning.Despite the relatively modest parameter count of 7B, our base model was able to significantly outperform larger models that did not undergo targeted recursive training.This suggests that structured self-improvement methodologies can lead to substantial gains in problem-solving ability without requiring massive increases in model size.</p>
<p>MIT Integration Bee (LADDER + TTRL)</p>
<p>After applying LADDER to the 7B base model, we further employed Test-Time Reinforcement Learning (TTRL) for 100 steps on questions that LADDER failed to answer correctly.Of the remaining incorrect responses, TTRL successfully solved 3-4 additional problems, increasing performance from 73% to 90% (Figure 6).This improvement places our model's performance well above the typical qualification threshold of 73% for the MIT Integration Bee, setting a new state-of-the-art for mid-sized LLMs on this benchmark.In Figure 6, we compare LADDER pass@100 with TTRL, demonstrating that TTRL is a more effective test-time scaling method than naively attempting a test question many times.TTRL's application of recursive problem decomposition on the test questions themselves allows the initial LADDER model to correctly solve some of the hardest questions on the MIT Integration Bee qualifying examination.TTRL enables our relatively small 7B LADDER model to outperform OpenAI's o1 model at pass@1.</p>
<p>The number of RL steps required to solve each test question previously incorrectly answered varied, ranging from as few as 3 steps to as many as 30.This variation likely reflects the complexity of each integral, with more complex problems requiring a greater number of solved variants to develop a useful learning trajectory.The questions that remained unsolved even after LADDER + TTRL are particularly complex for undergraduate students and are included in the appendix.</p>
<p>To assess the necessity of LADDER in TTRL's effectiveness, we also applied TTRL directly to the base model (pre-LADDER).After 100 steps, the base model remained unable to correctly answer any of the questions that LADDER + TTRL successfully solved.Performance gains from TTRL only occurred when applied to the LADDER-trained model, suggesting that the RL protocol in LADDER provides essential exposure to the distribution of integrals, enabling TTRL to later learn further.Future research could explore whether TTRL alone, with sufficiently high compute constraints, could achieve comparable results.</p>
<p>We note that while our LADDER+TTRL model surpasses o1's score on the MIT Integration Bee, this comparison should be taken as a general baseline rather than a strict head-to-head evaluation.Unlike our approach, o1 does not have access to a numerical checker, meaning it operates under different constraints.Our results highlight the effectiveness of self-improvement via recursive problem decomposition and reinforcement learning rather than suggesting a direct superiority over o1's approach.</p>
<p>Discussion</p>
<p>Our results demonstrate a potentially transformative approach to AI self-improvement through recursive problem decomposition and solution verification.The key insight is that models can direct their own learning trajectory by generating and solving progressively more challenging problems, requiring only a formal verifier rather than human guidance or curated datasets.This represents a significant shift from traditional supervised learning or RLHF approaches, suggesting a path toward more autonomous AI systems capable of extending their own capabilities.</p>
<p>The effectiveness of this self-directed learning is evident in our empirical results: improvements from 1% to 82% on undergraduate integration problems and from 50% to 73% on the MIT Integration Bee, achieved with LADDER, without human feedback or model scaling.With additional variant-based learning at test-time (TTRL), we achieve a state-of-the-art score of 90%.The 7B parameter model's ability to outperform much larger architectures suggests that improvements in AI capabilities may come not just from scaling compute or parameters, but from enabling models to structure their own learning progression.This methodology's recursive capability makes it particularly powerful.When faced with problems beyond their current abilities, models can autonomously decompose them into simpler variants, creating their own curriculum that adapts to their evolving capabilities.The implications extend beyond mathematical reasoning -suggesting a general framework for AI systems that can bootstrap more sophisticated capabilities from simpler ones, guided only by formal verification mechanisms rather than human oversight.</p>
<p>A Form of Test-Time Compute Scaling</p>
<p>Test-time compute scaling represents a promising new frontier in enhancing model capabilities beyond traditional approaches like architectural scaling or increased parameter counts.LADDER and TTRL introduce a fundamentally different paradigm: rather than relying on larger models or more extensive pre-training, we can achieve substantial performance improvements by enabling models to strategically practice and learn during inference.This approach is particularly powerful because it allows models to develop problem-specific expertise dynamically, adapting their capabilities to the exact challenges they encounter.</p>
<p>The dominant approach to test-time compute scaling today concentrates on increasing output token length, allowing models to engage in more extensive step-by-step reasoning.However, this approach faces fundamental challenges with extremely difficult problems that require extensive exploration.The sequential nature of token generation creates memory constraints and potential bottlenecks, as each token must be generated and processed in sequence while maintaining the entire chain of reasoning in context.</p>
<p>TTRL offers a fundamentally different approach that mirrors human learning strategies.Just as humans often need to study and practice similar problems before tackling a particularly challenging question, TTRL enables models to systematically explore and learn from related problems before attempting the target task.This approach is inherently more parallelizable than sequential token generation -variant problems can be generated and solved independently across multiple compute units, with their insights aggregated to improve performance on the original problem.While our current implementation explored variants sequentially, the framework naturally extends to distributed training approaches, potentially offering significant speedups through parallel exploration of the problem space.</p>
<p>This parallelizability presents a key advantage over traditional test-time scaling approaches.Where increasing output length faces fundamental sequential constraints, TTRL's variant generation and learning process can be distributed across multiple GPUs or machines.Each compute unit can independently explore different regions of the problem space, generating and solving variants while sharing insights through model updates.This mirrors distributed training approaches in the pre-training phase, but applied dynamically at test-time to specific challenging problems.</p>
<p>Extension to Other Verifiable Domains</p>
<p>While we demonstrated our approach using numerical integration, the underlying principle can extend to a broad range of formal reasoning tasks through appropriate verification tools.It is likely that any domain where question variants can be generated and which has a verifier-generator gap can leverage our approach.These domains share the critical property we identified in integration: a clear generator-verifier gap where solution verification is more straightforward than solution generation.This suggests our approach of iterative variant generation and verified learning could provide a general framework for improving formal reasoning capabilities in language models.Competitive programming is an example of another possible domain in which our method may be effective.An LLM could generate a solution to a programming problem and verify it with provided unit tests.In formal mathematics, tools like the Lean proof assistant offer another avenue for verification.Lean can rigorously verify mathematical proofs, providing a reliable signal for whether a model's reasoning is correct.</p>
<p>More generally, this approach has potential applications in planning and agent-based tasks, where models can teach themselves by successfully executing simpler tasks before building up to more complex challenges.This progressive learning approach, combined with reliable verification at each step, provides a robust framework developing advanced reasoning capabilities.</p>
<p>Future Work</p>
<p>A core challenge in optimizing LADDER lies in the precise calibration of variant difficulty and curriculum sequencing.While our current approach uses static parameters for variant generation, an adaptive strategy could dynamically adjust the difficulty gradient based on model performance.Rather than generating a fixed number of variants at predetermined difficulty levels, the framework could analyze solution success rates to determine optimal branching points and difficulty steps.This dynamic calibration would ensure each generated variant meaningfully contributes to the learning trajectory while avoiding redundant or inappropriately difficult examples that waste computational resources.The balance between exploration of novel problem patterns and exploitation of known solution strategies remains to be optimized.</p>
<p>Conclusion</p>
<p>We proposed LADDER, a framework for improving language models' problem-solving capabilities through recursive problem decomposition and reinforcement learning with verifiable rewards, as well as its extension to test-time, TTRL.By enabling models to generate and learn from progressively simpler variants of complex problems, LADDER demonstrates how strategic self-directed learning can achieve significant improvements without relying on architectural scaling or human supervision.The dramatic improvements in mathematical reasoning capabilities -from 1% to 82% on undergraduate integration problems and achieving 90% accuracy on the MIT Integration Bee through TTRLdemonstrate the effectiveness of this approach.</p>
<p>Beyond mathematical integration, our work builds upon three insights in general AI development: (1) the power of recursive problem decomposition for tackling complex tasks, (2) the importance of verifiable rewards for guiding self-improvement, and (3) the potential of scaling compute through strategic practice at test-time.The success of both LADDER during training and TTRL at inference time reinforces that focusing on how models interact with their environment is just as important as architectural innovations for advancing AI capabilities.</p>
<p>The principles demonstrated in this paper could likely extend to any domain with clear verification mechanisms, from program synthesis to formal theorem proving.By providing a framework for models to bootstrap their own capabilities through carefully constructed learning trajectories, we move closer to AI systems that can systematically extend their abilities into increasingly complex domains.</p>
<p>Figure 1 :
1
Figure 1: Example of variant generation for an integration problem.Each level represents progressively simpler variants of the original problem.The tree structure ensures each variant has exactly one parent, maintaining a clear progression of difficulty.</p>
<p>Figure 3 :
3
Figure 2: LADDER training progression.</p>
<p>Figure 4 :
4
Figure 4: Comparison of LLM scores by training approach.</p>
<p>C l a u d e -3 5 GFigure 5 :
55
Figure 5: Comparison of LLM scores on the 2025 MIT Integration Bee (7B).Average across 3 runs.</p>
<p>LADDER pass@ 1 LADDERFigure 6 :
16
Figure 6: TTRL score improvement on 2025 MIT Integration Bee.</p>
<p>Figure 7 :
7
Figure 7: Example of variant generation for twin prime problem.</p>
<p>Appendix: MIT Integration Bee Failed QuestionsAfter applying LADDER and TTRL, there were two questions on the 2025 MIT Integration Bee qualifying exam which continued to be answered incorrectly.The questions incorrectly answered are among the most complex questions on the exam, and for an undergraduate student are very difficult to solve.Question 12:
Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V Le, Sergey Levine, Yi Ma, Sft memorizes, rl generalizes: A comparative study of foundation model post-training. 2025</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , 2025</p>
<p>Language models can teach themselves to program better. Patrick Haluptzok, Matthew Bowers, Adam Tauman, Kalai , 2023</p>
<p>On reinforcement learning and distribution matching for fine-tuning language models with no catastrophic forgetting. Tomasz Korbak, Hady Elsahar, Germán Kruszewski, Marc Dymetman, 2022</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James, V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training. 2025</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023</p>
<p>Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, 2025</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, Peter Stone, 2020</p>
<p>Learning to reason with llms. Openai, 2024</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe, 2022</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, 2024</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, 2023</p>
<p>Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, Chong Luo, 2025</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 2023</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, 2022</p>            </div>
        </div>

    </div>
</body>
</html>
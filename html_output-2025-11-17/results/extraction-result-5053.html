<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5053 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5053</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5053</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-ec8afc75ec219f2a5f9ed9d7c9dde0720f69b5a2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ec8afc75ec219f2a5f9ed9d7c9dde0720f69b5a2" target="_blank">Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> Experimental results show that X-VLM effectively leverages the learned multi-grained alignments to many downstream vision language tasks and consistently outperforms state-of-the-art methods.</p>
                <p><strong>Paper Abstract:</strong> Most existing methods in vision language pre-training rely on object-centric features extracted through object detection and make fine-grained alignments between the extracted features and texts. It is challenging for these methods to learn relations among multiple objects. To this end, we propose a new method called X-VLM to perform `multi-grained vision language pre-training.' The key to learning multi-grained alignments is to locate visual concepts in the image given the associated texts, and in the meantime align the texts with the visual concepts, where the alignments are in multi-granularity. Experimental results show that X-VLM effectively leverages the learned multi-grained alignments to many downstream vision language tasks and consistently outperforms state-of-the-art methods.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5053.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5053.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>X-VLM / NLVR2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>X-VLM applied to NLVR2 (Natural Language for Visual Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>X-VLM is a multi-grained vision-language transformer introduced in this paper; it is fine-tuned and evaluated on NLVR2, a visual reasoning benchmark that requires understanding relations (including spatial relations) between two images and a natural language sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>X-VLM (multi-grained vision-language transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>X-VLM: image encoder based on Vision Transformer (initialized with Swin-Transformer_base), a text encoder (6 transformer layers initialized from BERT_base first six layers), and a cross-modal encoder (6 transformer layers initialized from BERT_base last six layers). Total ~215.6M parameters. Pre-trained under two settings (4M and 16M images) with mixed objectives: bounding-box prediction (L_bbox), contrastive learning (L_cl), matching (L_match), and masked language modeling (L_mlm). Uses patch-based vision tokens, aggregates patch features to form multi-grained visual concept representations, and fuses modalities via cross-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>NLVR2 (Natural Language for Visual Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>NLVR2 is a dataset/task where the model receives a natural language sentence and two images and must determine whether the sentence correctly describes the relationship between the two images; this requires compositional and spatial reasoning about objects and relations across the pair of images (e.g., left/right, relative positions, counts, presence/absence).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>X-VLM learns multi-grained alignments by locating visual concepts (objects/regions/image) given associated texts via bounding-box prediction (sigmoid-MLP on cross-modal [CLS]), and aligns text and visual concepts using contrastive learning, hard-negative matching prediction, and cross-modal masked language modeling. For NLVR2 specifically, X-VLM extends the cross-modal encoder to reason over two images and performs an additional one-epoch pre-training task mapping text to either image1, image2, or none to better learn cross-image relations before fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Empirical improvement on NLVR2 compared to strong baselines indicates the model captures relational/spatial information. Qualitative visualizations (Grad-CAM on cross-attention maps) show alignment of words to specific image regions; ablation studies demonstrate that region concepts and bounding-box prediction materially affect performance on NLVR2 (removing region concepts or bbox loss degrades scores). The paper reports an additional pre-training step explicitly designed to teach assignment of text to one of two images, encouraging cross-image relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>X-VLM(4M): NLVR2 dev = 84.16, test-P = 84.21. X-VLM(16M): NLVR2 dev = 84.41, test-P = 84.76. Ablation (80K steps variant in Table 4): full X-VLM variant NLVR2 test-P = 82.42; w/o bbox loss NLVR2 test-P = 81.49 (showing bbox loss contributes ~0.9 percentage points in that ablation setting).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No explicit symbolic puzzle (e.g., Sudoku) solving is attempted; X-VLM is a vision-language model, not a pure LLM for abstract symbolic spatial puzzles. Evidence of spatial reasoning is indirect (improved task scores, Grad-CAM visualizations) rather than demonstrations of explicit, interpretable internal spatial representations. Performance depends on availability of region/object annotations for multi-grained supervision; ablations show removing object/region concepts or bbox loss reduces performance. Pre-training data scale is moderate compared to very large web-scale models, so some scaling benefits may be unrealized.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>X-VLM outperforms prior strong V+L baselines on NLVR2: e.g., VinVL (pretrained with object-detections) had NLVR2 dev/test-P around 82.67/83.98, while X-VLM(4M) achieves 84.16/84.21 (absolute improvement ~0.86% averaged as reported). X-VLM also outperforms coarse-grained approaches (ALBEF, SOHO, METER-Swin) on NLVR2; ablations quantify the contribution of multi-grained components and bbox loss.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5053.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5053.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>X-VLM / RefCOCO+ (visual grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>X-VLM applied to RefCOCO+ (Referring Expression Comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>X-VLM is used to perform visual grounding (locating image region corresponding to a textual referring expression) by directly predicting bounding boxes and via Grad-CAM in weakly supervised settings, demonstrating spatial localization capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>X-VLM (multi-grained vision-language transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same X-VLM architecture as above (~216M params), trained with multi-grained objectives including explicit bounding-box regression (L_bbox) and IoU loss combined with L1. Uses cross-modal [CLS] embeddings to regress normalized box coordinates and leverages cross-attention maps (Grad-CAM) for weakly-supervised localization.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>RefCOCO+ (referring expression comprehension / visual grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Given a referring expression (text describing an object in an image), the task is to localize the corresponding object/region by predicting a bounding box. This requires precise spatial understanding (object attributes, relative positions, interactions) to disambiguate among similar objects.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>X-VLM predicts bounding boxes via an MLP on the cross-modal [CLS] output, normalized with sigmoid; training uses a combination of generalized IoU loss and L1 loss. For weakly-supervised settings, cross-attention Grad-CAM heatmaps are used to rank candidate proposals. Multi-grained concept representations (objects, regions, whole image) are aggregated from patch embeddings to allow object- and region-level alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Quantitative gains in supervised and weakly-supervised grounding, per-word Grad-CAM visualizations that align textual tokens (e.g., 'dog', 'holding', 'pulling') to localized image regions, and ablations showing bounding-box loss and region/object concepts materially improve grounding accuracy provide converging evidence of spatial/localization reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Supervised RefCOCO+ (X-VLM(4M)): val = 80.17, testA = 86.36, testB = 71.00. X-VLM(16M): val = 84.51, testA = 89.00, testB = 76.91. Weakly-supervised (X-VLM(4M)): val/testA/testB = 68.46 / 76.53 / 57.09 (paper reports large gains over ALBEF weakly-supervised baseline). Ablation: removing bbox loss or region/object concepts reduces grounding performance (see Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reliance on bounding-box annotations for supervised performance; weakly-supervised performance is lower (but improved over prior weak baselines). Visualizations and Grad-CAM provide qualitative evidence but do not fully explain failure modes. The model's spatial reasoning is demonstrated in image grounding but not tested on abstract spatial puzzle-solving (e.g., Sudoku, discrete spatial planning).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>X-VLM(4M) outperforms UNITER by ~4.5% absolute on RefCOCO+ and also outperforms MDETR (~1.1% absolute) while being a general V+L model; weakly-supervised X-VLM substantially outperforms ALBEF(14M) in the weakly-supervised setting (~10.5% average improvement as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts', 'publication_date_yy_mm': '2021-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A corpus for reasoning about natural language grounded in photographs <em>(Rating: 2)</em></li>
                <li>VinVL: Revisiting visual representations in vision-language models <em>(Rating: 2)</em></li>
                <li>Align before fuse: Vision and language representation learning with momentum distillation <em>(Rating: 1)</em></li>
                <li>SimVLM: Simple visual language model pretraining with weak supervision <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5053",
    "paper_id": "paper-ec8afc75ec219f2a5f9ed9d7c9dde0720f69b5a2",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "X-VLM / NLVR2",
            "name_full": "X-VLM applied to NLVR2 (Natural Language for Visual Reasoning)",
            "brief_description": "X-VLM is a multi-grained vision-language transformer introduced in this paper; it is fine-tuned and evaluated on NLVR2, a visual reasoning benchmark that requires understanding relations (including spatial relations) between two images and a natural language sentence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "X-VLM (multi-grained vision-language transformer)",
            "model_description": "X-VLM: image encoder based on Vision Transformer (initialized with Swin-Transformer_base), a text encoder (6 transformer layers initialized from BERT_base first six layers), and a cross-modal encoder (6 transformer layers initialized from BERT_base last six layers). Total ~215.6M parameters. Pre-trained under two settings (4M and 16M images) with mixed objectives: bounding-box prediction (L_bbox), contrastive learning (L_cl), matching (L_match), and masked language modeling (L_mlm). Uses patch-based vision tokens, aggregates patch features to form multi-grained visual concept representations, and fuses modalities via cross-attention.",
            "puzzle_name": "NLVR2 (Natural Language for Visual Reasoning)",
            "puzzle_description": "NLVR2 is a dataset/task where the model receives a natural language sentence and two images and must determine whether the sentence correctly describes the relationship between the two images; this requires compositional and spatial reasoning about objects and relations across the pair of images (e.g., left/right, relative positions, counts, presence/absence).",
            "mechanism_or_strategy": "X-VLM learns multi-grained alignments by locating visual concepts (objects/regions/image) given associated texts via bounding-box prediction (sigmoid-MLP on cross-modal [CLS]), and aligns text and visual concepts using contrastive learning, hard-negative matching prediction, and cross-modal masked language modeling. For NLVR2 specifically, X-VLM extends the cross-modal encoder to reason over two images and performs an additional one-epoch pre-training task mapping text to either image1, image2, or none to better learn cross-image relations before fine-tuning.",
            "evidence_of_spatial_reasoning": "Empirical improvement on NLVR2 compared to strong baselines indicates the model captures relational/spatial information. Qualitative visualizations (Grad-CAM on cross-attention maps) show alignment of words to specific image regions; ablation studies demonstrate that region concepts and bounding-box prediction materially affect performance on NLVR2 (removing region concepts or bbox loss degrades scores). The paper reports an additional pre-training step explicitly designed to teach assignment of text to one of two images, encouraging cross-image relational reasoning.",
            "performance_metrics": "X-VLM(4M): NLVR2 dev = 84.16, test-P = 84.21. X-VLM(16M): NLVR2 dev = 84.41, test-P = 84.76. Ablation (80K steps variant in Table 4): full X-VLM variant NLVR2 test-P = 82.42; w/o bbox loss NLVR2 test-P = 81.49 (showing bbox loss contributes ~0.9 percentage points in that ablation setting).",
            "limitations_or_failure_cases": "No explicit symbolic puzzle (e.g., Sudoku) solving is attempted; X-VLM is a vision-language model, not a pure LLM for abstract symbolic spatial puzzles. Evidence of spatial reasoning is indirect (improved task scores, Grad-CAM visualizations) rather than demonstrations of explicit, interpretable internal spatial representations. Performance depends on availability of region/object annotations for multi-grained supervision; ablations show removing object/region concepts or bbox loss reduces performance. Pre-training data scale is moderate compared to very large web-scale models, so some scaling benefits may be unrealized.",
            "comparison_baseline": "X-VLM outperforms prior strong V+L baselines on NLVR2: e.g., VinVL (pretrained with object-detections) had NLVR2 dev/test-P around 82.67/83.98, while X-VLM(4M) achieves 84.16/84.21 (absolute improvement ~0.86% averaged as reported). X-VLM also outperforms coarse-grained approaches (ALBEF, SOHO, METER-Swin) on NLVR2; ablations quantify the contribution of multi-grained components and bbox loss.",
            "uuid": "e5053.0",
            "source_info": {
                "paper_title": "Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts",
                "publication_date_yy_mm": "2021-11"
            }
        },
        {
            "name_short": "X-VLM / RefCOCO+ (visual grounding)",
            "name_full": "X-VLM applied to RefCOCO+ (Referring Expression Comprehension)",
            "brief_description": "X-VLM is used to perform visual grounding (locating image region corresponding to a textual referring expression) by directly predicting bounding boxes and via Grad-CAM in weakly supervised settings, demonstrating spatial localization capability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "X-VLM (multi-grained vision-language transformer)",
            "model_description": "Same X-VLM architecture as above (~216M params), trained with multi-grained objectives including explicit bounding-box regression (L_bbox) and IoU loss combined with L1. Uses cross-modal [CLS] embeddings to regress normalized box coordinates and leverages cross-attention maps (Grad-CAM) for weakly-supervised localization.",
            "puzzle_name": "RefCOCO+ (referring expression comprehension / visual grounding)",
            "puzzle_description": "Given a referring expression (text describing an object in an image), the task is to localize the corresponding object/region by predicting a bounding box. This requires precise spatial understanding (object attributes, relative positions, interactions) to disambiguate among similar objects.",
            "mechanism_or_strategy": "X-VLM predicts bounding boxes via an MLP on the cross-modal [CLS] output, normalized with sigmoid; training uses a combination of generalized IoU loss and L1 loss. For weakly-supervised settings, cross-attention Grad-CAM heatmaps are used to rank candidate proposals. Multi-grained concept representations (objects, regions, whole image) are aggregated from patch embeddings to allow object- and region-level alignment.",
            "evidence_of_spatial_reasoning": "Quantitative gains in supervised and weakly-supervised grounding, per-word Grad-CAM visualizations that align textual tokens (e.g., 'dog', 'holding', 'pulling') to localized image regions, and ablations showing bounding-box loss and region/object concepts materially improve grounding accuracy provide converging evidence of spatial/localization reasoning.",
            "performance_metrics": "Supervised RefCOCO+ (X-VLM(4M)): val = 80.17, testA = 86.36, testB = 71.00. X-VLM(16M): val = 84.51, testA = 89.00, testB = 76.91. Weakly-supervised (X-VLM(4M)): val/testA/testB = 68.46 / 76.53 / 57.09 (paper reports large gains over ALBEF weakly-supervised baseline). Ablation: removing bbox loss or region/object concepts reduces grounding performance (see Table 4).",
            "limitations_or_failure_cases": "Reliance on bounding-box annotations for supervised performance; weakly-supervised performance is lower (but improved over prior weak baselines). Visualizations and Grad-CAM provide qualitative evidence but do not fully explain failure modes. The model's spatial reasoning is demonstrated in image grounding but not tested on abstract spatial puzzle-solving (e.g., Sudoku, discrete spatial planning).",
            "comparison_baseline": "X-VLM(4M) outperforms UNITER by ~4.5% absolute on RefCOCO+ and also outperforms MDETR (~1.1% absolute) while being a general V+L model; weakly-supervised X-VLM substantially outperforms ALBEF(14M) in the weakly-supervised setting (~10.5% average improvement as reported).",
            "uuid": "e5053.1",
            "source_info": {
                "paper_title": "Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts",
                "publication_date_yy_mm": "2021-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A corpus for reasoning about natural language grounded in photographs",
            "rating": 2
        },
        {
            "paper_title": "VinVL: Revisiting visual representations in vision-language models",
            "rating": 2
        },
        {
            "paper_title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "rating": 1
        },
        {
            "paper_title": "SimVLM: Simple visual language model pretraining with weak supervision",
            "rating": 1
        }
    ],
    "cost": 0.012432249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts</h1>
<p>Yan Zeng ${ }^{1}$ Xinsong Zhang ${ }^{1}$ Hang Li ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Most existing methods in vision language pretraining rely on object-centric features extracted through object detection and make fine-grained alignments between the extracted features and texts. It is challenging for these methods to learn relations among multiple objects. To this end, we propose a new method called X-VLM ${ }^{1}$ to perform 'multi-grained vision language pre-training.' The key to learning multi-grained alignments is to locate visual concepts in the image given the associated texts, and in the meantime align the texts with the visual concepts, where the alignments are in multi-granularity. Experimental results show that X-VLM effectively leverages the learned multi-grained alignments to many downstream vision language tasks and consistently outperforms state-of-the-art methods.</p>
<h2>1. Introduction</h2>
<p>Vision language pre-training aims to learn vision language alignments from a large number of image-text pairs. A pretrained Vision Language Model (VLM) fine-tuned with a small amount of labeled data has shown the state-of-the-art performances in many Vision Language ( $\mathrm{V}+\mathrm{L}$ ) tasks such as visual question answering and image-text retrieval.</p>
<p>Existing methods learning vision language alignments fall into two approaches as shown in Figure 1 (a, b). Most of them detect objects in the image and align the text with fine-grained (object-centric) features. They either utilize pre-trained object detectors (Tan \&amp; Bansal, 2019; Lu et al., 2019; Li et al., 2019; 2020a; Chen et al., 2020; Li et al., 2020b; Gan et al., 2020) or conduct object detection on-thefly in the pre-training process (Su et al., 2020; Xu et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. A comparison of (a) the existing methods relying on object detection, (b) the methods aligning the texts with the whole image, and (c) our approach.</p>
<p>2021a). The other methods do not rely on object detection and only learn alignments between the texts and coarsegrained (overall) features of the image (Huang et al., 2020; 2021; Kim et al., 2021; Li et al., 2021a).</p>
<p>Both the fine-grained and coarse-grained approaches have drawbacks. Object detection identifies all possible objects in the image, and some of them might not be relevant to the text. Object-centric features cannot easily represent relations among multiple objects, e.g. " man crossing the street". Moreover, it is challenging to pre-define the categories of objects suitable for downstream tasks. On the other hand, the coarse-grained approaches cannot effectively learn fine-grained alignments between vision and language, e.g. object-level, which has shown to be critical for some downstream tasks such as visual reasoning, visual grounding, and image captioning.</p>
<p>Ideally, we want a VLM to learn multi-grained alignments between vision and language in pre-training, which are not restricted to object-level or image-level, and leverage the learned alignments to downstream V+L tasks. Unfortunately, existing methods cannot satisfactorily handle multi-</p>
<p>grained alignments between vision and language.
In this paper, we propose performing multi-grained vision language pre-training by aligning text descriptions with the corresponding visual concepts in images. Taking Figure 1 as an example, we have the following data for training: 1) the image caption describing the whole image; 2) region annotations such as "man wearing backpack" each of which has been related to a region in the image, while previous approaches roughly align the region descriptions with the whole image; 3) object labels such as "backpack" which are utilized by previous methods to train object detectors. We re-formulate the data, so that an image may have multiple bounding boxes, and a text ${ }^{2}$ is directly associated with the visual concept in each box. The 'visual concept' (Krishna et al., 2017; Zhang et al., 2021; Changpinyo et al., 2021) may be an object, a region, or the image itself, as the example in Figure 1 (c). By doing so, our approach learns unlimited visual concepts associated with diverse text descriptions, which are also not restricted to object-level or image-level.</p>
<p>Our multi-grained model, denoted as X-VLM, consists of an image encoder that produces representations of visual concepts (including the image itself) in an image, a text encoder, and a cross-modal encoder that conducts crossattention between the vision features and language features to learn vision language alignments. The key to learning multi-grained alignments is to optimize X-VLM by: 1) locating visual concepts in the image given associated texts by a combination of box regression loss and intersection over union loss; 2) in the meantime aligning the texts with the visual concepts, e.g. by a contrastive loss, a matching loss, and a masked language modeling loss, where the alignments are in multi-granularity, as illustrated in Figure 1 (c). In fine-tuning and inference, X-VLM can leverage the learned multi-grained alignments to perform the downstream V+L tasks without bounding box annotations in the input images.</p>
<p>We demonstrate the effectiveness of our approach on various downstream tasks. On image-text retrieval, X-VLM learning multi-grained vision language alignments outperforms VinVL (Zhang et al., 2021) which is based on objectcentric features, achieving an absolute gain of $4.65 \%$ in terms of R@1 score on MSCOCO. X-VLM also outperforms ALIGN (Jia et al., 2021), ALBEF (Li et al., 2021a), and METER (Dou et al., 2021) by a large margin even though they are pre-trained on more data or have more parameters. On visual reasoning tasks, X-VLM achieves absolute improvements of $0.79 \%$ on VQA and $1.06 \%$ on NLVR2 compared to VinVL (Zhang et al., 2021), with a much faster inference speed. X-VLM also outperforms SimVLM $_{\text {base }}$ (Wang et al., 2021) pre-trained with 1.8B in-house data, especially on NLVR2 by $2.4 \%$. On visual</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>grounding (RefCOCO+), X-VLM achieves absolute improvements of $4.5 \%$ compared to UNITER (Chen et al., 2020) and $1.1 \%$ compared to MDETR (Kamath et al., 2021) which is specialized for grounding tasks. X-VLM also has comparable performance with SimVLM $_{\text {base }}$ in the image caption generation task.</p>
<p>The contributions of this work are as follows:</p>
<ul>
<li>We propose performing multi-grained vision language pre-training to handle the alignments between texts and visual concepts.</li>
<li>We propose to optimize the model (X-VLM) by locating visual concepts in the image given the associated texts and in the meantime aligning the texts with the visual concepts, where the alignments are in multigranularity.</li>
<li>We empirically verify that our approach effectively leverages the learned multi-grained alignments in finetuning. X-VLM consistently outperforms existing state-of-the-art methods on many downstream V+L tasks.</li>
</ul>
<h2>2. Related Work</h2>
<p>The existing work on vision language pre-training typically falls into two categories: fine-grained and coarse-grained.</p>
<p>Most existing methods belong to the fine-grained approach, which relies on object detection (Tan \&amp; Bansal, 2019; Lu et al., 2019; Li et al., 2019; 2020a; Chen et al., 2020; Li et al., 2020b; Gan et al., 2020; Li et al., 2021b). An object detector first identifies all regions that probably contain an object, then conducts object classification on each region. An image is then represented by dozens of object-centric features of the identified regions. Object detectors, such as Faster R-CNN (Ren et al., 2015), Bottom-Up and Top-Down Attention (BUTD) (Anderson et al., 2018), are trained on image annotations of common objects, e.g. COCO (Lin et al., 2014) (110K images) and Visual Genome (Krishna et al., 2017) (100K), and can be utilized. VinVL (Zhang et al., 2021) has, for example, achieved SoTA performances on many V+L tasks by utilizing a powerful object detector pre-trained with a large collection of image annotations ( 2.5 M images). The challenge with the approach is that object-centric features cannot represent relations among multiple objects in multiple regions. Furthermore, it is not easy to define the categories of objects in advance that are useful for downstream V+L tasks.</p>
<p>The coarse-grained approach builds VLMs by extracting and encoding overall image features with convolutional network (Jiang et al., 2020; Huang et al., 2020; 2021) or vision</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Pre-training model architecture and objectives of X-VLM. As shown on the left side, we extract features from the subset of patches from the vision transformer to represent images/regions/objects ( $I$ and $V^{1-3}$ ), which are then paired with corresponding text features ( $T$ and $T^{1-3}$ ) for contrastive learning, matching, and MLM. Meanwhile, the image ( $I$ ) is paired with different textual descriptions ( $T$ and $T^{1-3}$ ) for bounding box prediction to locate visual concepts in the image.
transformer (Kim et al., 2021; Li et al., 2021a). The performances are usually not as good as the fine-grained approach. Though object-centric features are only related to certain objects, learning fine-grained alignments, e.g. object-level, has shown to be critical for some downstream tasks such as visual reasoning and visual grounding. To cope with the problem, SOHO (Huang et al., 2021) employs online clustering on image features to obtain more comprehensive representations, ViLT (Kim et al., 2021) uses a more advanced vision transformer, i.e. Swin-Transformer (Liu et al., 2021b) for image encoding, and ALBEF (Li et al., 2021a) exploits contrastive learning and momentum distillation in learning of image-text alignments. However, the improvements still cannot close the gap with the fine-grained approach.</p>
<p>Recently, there emerge some methods managing to learn both object-level and image-level alignments. However, these approaches still rely on object detectors and thus suffer from the aforementioned problems. For example, VLBERT (Su et al., 2020) incorporates Faster R-CNN into pretraining. E2E-VLP (Xu et al., 2021a) adds an end-to-end object detection module (i.e. DETR (Carion et al., 2020)). Uni-EDEN (Li et al., 2022) uses Faster R-CNN as the vision backbone. KD-VLP (Liu et al., 2021a) relies on external object detectors to perform object knowledge distillation. In contrast, X-VLM does not rely on object detection. Besides, X-VLM learns multi-grained vision language alignments,
which are not restricted to object-level or image-level. Also, unlike Uni-EDEN, which aligns objects to language by object classification and aligns images to language by caption generation, X-VLM learns visual concepts in different granularities in a unified way. We will show the effectiveness of X-VLM in the experiments.</p>
<h2>3. Method</h2>
<h3>3.1. Overview</h3>
<p>X-VLM consists of an image encoder ( $I_{\text {trans }}$ ), a text encoder ( $T_{\text {trans }}$ ), and a cross-modal encoder ( $X_{\text {trans }}$ ). All encoders are based on Transformer (Vaswani et al., 2017). The crossmodal encoder fuses the vision features with the language features through cross-attention at each layer.</p>
<p>We re-formulate the widely used pre-training datasets (see Section 4.1) so that an image may have multiple bounding boxes, and each of them is associated with a text that describes an object or a region, denoted as $\left(I, T,\left{\left(V^{j}, T^{j}\right)\right}^{N}\right)$. Note that some images do not have associated texts, i.e., $T$ is NaN , and some images do not have bounding boxes, i.e., $N=0$. Here, $V^{j}$ is an object or region in the bounding box $\boldsymbol{b}^{j}=(c x, c y, w, h)$ represented by the normalized center coordinates, width, and height of the box. When the image itself represents a visual concept, $\boldsymbol{b}=(0.5,0.5,1,1)$. Figure 2 illustrates the architecture and</p>
<p>pre-training objectives of X-VLM.</p>
<h3>3.2. Vision Encoding</h3>
<p>The image encoder efficiently produces multi-grained visual concept representations in an image. The encoder is based on vision transformer (Dosovitskiy et al., 2020). It first splits an image into non-overlapping patches and linearly embeds all patches. Then, these patches are passed into the transformer layers, yielding $\left{\boldsymbol{v}<em N_I="N^{I">{1}, \ldots, \boldsymbol{v}</em>=49$.}}\right}$. For an image of resolution of 224 x 224 and patch size of 32 x 32 , we have $N^{I</p>
<p>We assume that $\boldsymbol{v}<em i="i">{p</em>}}$ encodes the information of the corresponding patch $p_{i}$. Therefore, we represent a visual concept $V^{j}$ (object, region, or the image) that corresponds to a set of patches by aggregating information among the patches as shown in Figure 2. Specifically, we reshape the patch features while keeping their position information, denoted as $\left{\boldsymbol{v<em 1="1">{p</em>}^{i}}, \ldots, \boldsymbol{v<em M="M">{p</em>$, and prepend it.}^{i}}\right} .\left{p_{1}^{j}, \ldots, p_{M}^{j}\right}$ are patches of $V^{j}$. We also calculate the average of the features to represent the whole visual concept, denoted as $\boldsymbol{v}_{\mathrm{cls}}^{j</p>
<p>The image encoder then creates $N+1$ concept representations in different granularities, represented as $I_{\text {trans }}\left(V^{j}\right)=$ $\left{\boldsymbol{v}<em p__1="p_{1">{\mathrm{cls}}^{j}, \boldsymbol{v}</em>}^{i}}, \ldots, \boldsymbol{v<em M="M">{p</em>\right)$ denote the image representation in which all patch features are utilized. In the following section, we will describe how the representations are utilized in the learning of multi-grained alignments.}^{i}}\right}, j \in[0, N]$. We let $I_{\text {trans }}\left(V^{0</p>
<h3>3.3. Cross-Modal Modeling</h3>
<p>As shown in Figure 2, we optimize X-VLM by locating visual concepts in the image given the corresponding texts and in the meantime aligning the texts and visual concepts, where the alignments are in multi-granularity.</p>
<p>Bounding Box Prediction We let the model predict the bounding box $\boldsymbol{b}^{j}$ of visual concept $V^{j}$ given the image representation and the text representation, where $\boldsymbol{b}^{j}=$ $(c x, c y, w, h)$. By locating different visual concepts in the same image, we expect that the model better learns finegrained vision language alignments. The bounding box is predicted by:</p>
<p>$$
\tilde{\boldsymbol{b}}^{j}\left(I, T^{j}\right)=\operatorname{Sigmoid}\left(\operatorname{MLP}\left(\boldsymbol{x}_{\mathrm{cls}}^{j}\right)\right)
$$</p>
<p>where Sigmoid is for normalization, MLP denotes multilayer perceptron, and $\boldsymbol{x}_{\text {cls }}^{j}$ is the output [CLS] embedding of the cross-modal encoder given $I$ and $T^{j}$.</p>
<p>For bounding box prediction, $\ell_{1}$ is the most commonly-used loss. However, it has different scales for small and large boxes, even if their relative errors are similar. To mitigate this issue, we use a linear combination of the $\ell_{1}$ loss and the generalized Intersection over Union (IoU) loss (Rezatofighi
et al., 2019), which is scale-invariant. The overall loss is defined as:</p>
<p>$$
\mathcal{L}<em _left_V_j="\left(V^{j">{\mathrm{bbox}}=\mathbb{E}</em>}, T^{j}\right) \sim I ; I \sim D}\left[\mathcal{L<em j="j">{\mathrm{iou}}\left(\boldsymbol{b}</em>}, \hat{\boldsymbol{b}<em j="j">{j}\right)+\left|\boldsymbol{b}</em>}-\hat{\boldsymbol{b}<em 1="1">{j}\right|</em>\right]
$$</p>
<p>Meanwhile, we align texts and visual concepts by three objectives which are widely used in vision language pretraining (Chen et al., 2020; Radford et al., 2021; Li et al., 2021a). We extend the objectives to incorporate multigrained visual concepts in the images.</p>
<p>Contrastive Learning We predict (visual concept, text) pairs, denoted $(V, T)$, from in-batch negatives. Note that visual concepts include objects, regions, and images. Similar to Radford et al. (2021), we randomly sample a mini-batch of $N$ pairs, and calculate the in-batch vision-to-text similarity and text-to-vision similarity.</p>
<p>Given a pair $(V, T), T$ is the positive example for $V$, and we treat the other $(N-1)$ texts within the mini-batch as negative examples. We define cosine similarity $s(V, T)=$ $g_{v}\left(\boldsymbol{v}<em w="w">{\mathrm{cls}}\right)^{\top} g</em>}\left(\boldsymbol{w<em _mathrm_cls="\mathrm{cls">{\mathrm{cls}}\right) . \boldsymbol{w}</em>$ are transformations that map the [CLS] embeddings to normalized lower-dimensional representations. Then, we calculate the in-batch vision-totext similarity as:}}$ is the output [CLS] embedding of the text encoder. $g_{v}$ and $g_{w</p>
<p>$$
p^{\mathrm{v} 2 \mathrm{t}}(V)=\frac{\exp (s(V, T) / \tau)}{\sum_{i=1}^{N} \exp \left(s\left(V, T^{i}\right) / \tau\right)}
$$</p>
<p>Similarly, the text-to-vision similarity is:</p>
<p>$$
p^{\mathrm{t} 2 \mathrm{v}}(T)=\frac{\exp (s(V, T) / \tau)}{\sum_{i=1}^{N} \exp \left(s\left(V^{i}, T\right) / \tau\right)}
$$</p>
<p>where $\tau$ is a learnable temperature parameter. Let $\boldsymbol{y}^{\mathrm{v} 2 \mathrm{t}}(V)$ and $\boldsymbol{y}^{\mathrm{t} 2 \mathrm{v}}(T)$ denote the ground-truth one-hot similarity, in which only the positive pair has the probability of one. The contrastive loss is defined as the cross-entropy H between $\boldsymbol{p}$ and $\boldsymbol{y}$ :</p>
<p>$$
\begin{aligned}
\mathcal{L}<em D="D" T="T" V_="V," _sim="\sim">{\mathrm{cl}}=\frac{1}{2} \mathbb{E}</em> \
&amp; \left.+\mathrm{H}\left(\boldsymbol{y}^{\mathrm{t} 2 \mathrm{v}}(T), \boldsymbol{p}^{\mathrm{t} 2 \mathrm{v}}(T)\right)\right]
\end{aligned}
$$} &amp; {\left[\mathrm{H}\left(\boldsymbol{y}^{\mathrm{v} 2 \mathrm{t}}(V), \boldsymbol{p}^{\mathrm{v} 2 \mathrm{t}}(V)\right)\right.</p>
<p>Matching Prediction We determine whether a pair of visual concept and text is matched. For each visual concept in a mini-batch, we sample an in-batch hard negative text by following $p^{\mathrm{v} 2 \mathrm{t}}(V)$ in Equation 3. Texts that are more relevant to the concept are more likely to be sampled. We also sample one hard negative visual concept for each text. We use $\boldsymbol{x}_{\mathrm{cls}}$, the output [CLS] embedding of the cross-modal encoder, to predict the matching probability $p^{\text {match }}$, and the loss is:</p>
<p>$$
\mathcal{L}<em D="D" T="T" V_="V," _sim="\sim">{\text {match }}=\mathbb{E}</em>(V, T)\right)
$$} \mathrm{H}\left(\boldsymbol{y}^{\text {match }}, \boldsymbol{p}^{\text {match }</p>
<p>where $\boldsymbol{y}^{\text {match }}$ is a 2-dimensional one-hot vector representing the ground-truth label.</p>
<p>Masked Language Modeling We predict the masked words in the text based on the visual concept. We randomly mask out the input tokens with a probability of $25 \%$, and the replacements are $10 \%$ random tokens, $10 \%$ unchanged, and $80 \%$ [MASK]. We use the cross-modal encoder's outputs, and append a linear layer followed by softmax for prediction. Let $\hat{T}$ denote a masked text, and $\boldsymbol{p}^{j}(V, \hat{T})$ denote the predicted probability of the masked token $t_{j}$. We minimize the cross-entropy loss:</p>
<p>$$
\mathcal{L}<em t__j="t_{j">{\mathrm{mlm}}=\mathbb{E}</em>)\right)
$$} \sim \hat{T} ;(V, \hat{T}) \sim D} \mathrm{H}\left(\boldsymbol{y}^{j}, \boldsymbol{p}^{j}(V, \hat{T</p>
<p>where $\boldsymbol{y}^{j}$ is a one-hot distribution in which the ground-truth token $t_{j}$ has the probability of one.</p>
<p>Finally, the pre-training objective of X-VLM is defined as:</p>
<p>$$
\mathcal{L}=\mathcal{L}<em _mathrm_cl="\mathrm{cl">{\text {bbox }}+\mathcal{L}</em>}}+\mathcal{L<em _mathrm_mlm="\mathrm{mlm">{\text {match }}+\mathcal{L}</em>
$$}</p>
<h2>4. Experiment</h2>
<h3>4.1. Pre-training Datasets</h3>
<p>Table 1. Statistics of the pre-training datasets. See Appendix A. 1 for detailed statistics of object and region annotations.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"># Images</th>
<th style="text-align: center;"># Captions</th>
<th style="text-align: center;"># Ann</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">4 M</td>
<td style="text-align: center;">COCO</td>
<td style="text-align: center;">0.11 M</td>
<td style="text-align: center;">0.55 M</td>
<td style="text-align: center;">0.45 M</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VG</td>
<td style="text-align: center;">0.10 M</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5.7 M</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SBU</td>
<td style="text-align: center;">0.86 M</td>
<td style="text-align: center;">0.86 M</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CC-3M</td>
<td style="text-align: center;">2.9 M</td>
<td style="text-align: center;">2.9 M</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">16 M</td>
<td style="text-align: center;">4 M</td>
<td style="text-align: center;">4.0 M</td>
<td style="text-align: center;">5.1 M</td>
<td style="text-align: center;">6.2 M</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Objects365</td>
<td style="text-align: center;">0.58 M</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2.0 M</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OpenImages</td>
<td style="text-align: center;">1.7 M</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.2 M</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CC-12M</td>
<td style="text-align: center;">11.1 M</td>
<td style="text-align: center;">11.1 M</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>We compare X-VLM with existing approaches at two settings, as listed in Table 1. We refer to them as the 4 M setting and 16 M setting respectively. Following UNITER (Chen et al., 2020) and other existing work, we prepare our pretraining data using two in-domain datasets, COCO (Lin et al., 2014) and Visual Genome (VG) (Krishna et al., 2017), and two out-of-domain datasets, SBU Captions (Ordonez et al., 2011) and Conceptual Captions (CC) (Sharma et al., 2018).</p>
<p>In the 4 M setting, we utilize image annotations only from COCO and VG, which contain 2.5 M object annotations and 3.7 M region annotations. Note that BUTD, the most widely used object detector, is trained on the same set of object annotations. The existing methods of only learning imagetext alignments also utilize the region annotations of VG under the assumption that region descriptions can describe the whole images. In contrast, we take the object labels
as text descriptions of objects, and re-formulate the image annotations so that an image has multiple boxes and each box is associated with a text. The text describes the visual concept in the box, which can be an object, a region, or the image itself.</p>
<p>In the 16 M setting, we exploit a much noisier Conceptual 12 M dataset (CC-12M) (Changpinyo et al., 2021) following ALBEF (Li et al., 2021a). We additionally exploit Objects365 (Shao et al., 2019) and OpenImages (Kuznetsova et al., 2018) following VinVL (Zhang et al., 2021).</p>
<p>Since most downstream V+L tasks are built on top of COCO and VG, we exclude all images that also appear in the validation and test sets of downstream tasks to avoid information leak. We also exclude all co-occurring Flickr30K (Plummer et al., 2015) images via URL matching, because COCO and VG are from Flickr, and there are some overlaps.</p>
<h3>4.2. Implementation Details</h3>
<p>The image encoder of X-VLM is vision transformer (Dosovitskiy et al., 2020), which is initialized with Swin $\operatorname{Transformer}<em _base="{base" _text="\text">{\text {base }}$ (Liu et al., 2021b). The text encoder and the cross-modal encoder consist of six transformer layers respectively. The text encoder is initialized using the first six layers of BERT ${ }</em>$ (Devlin et al., 2019), and the crossmodal encoder is initialized using the last six layers. In total, X-VLM has 215.6 M parameters for pre-training.}</p>
<p>X-VLM takes images of resolution of $224 \times 224$ as input. For text input, we set the maximum number of tokens to 30. During fine-tuning, we increase the image resolution to $384 \times 384$ and interpolate the positional embeddings of image patches following Dosovitskiy et al. (2020).</p>
<p>We apply mixed precision for pre-training. In the 4 M setting, we train the model for 200 K steps on 8 NVIDIA A100 GPUs and the batch size is set to 1024 , which tasks $\sim 3.5$ days. In the 16 M setting, we train the model on 24 GPUs with a batch size of 3072 . We sample the data by making half of the images in a batch containing bounding box annotations. We use the AdamW (Loshchilov \&amp; Hutter, 2019) optimizer with a weight decay of 0.02 . The learning rate is warmed-up to $1 e^{-4}$ from $1 e^{-5}$ in the first 2500 steps and decayed to $1 e^{-5}$ following a linear schedule.</p>
<h3>4.3. Downstream Tasks</h3>
<p>We adapt X-VLM to five downstream V+L tasks. We follow the settings in the previous work on fine-tuning (see Appendix A.2). Note that we have cleaned the pre-training datasets to avoid data leaks since downstream V+L tasks have overlaps in images with COCO and Visual Genome.</p>
<p>Image-Text Retrieval There are two subtasks: text retrieval (TR) and image retrieval (IR). We evaluate X-VLM on</p>
<p>Table 2: Image-text retrieval results on MSCOCO and Flickr30K datasets. IR: Image Retrieval and TR: Text Retrieval. We compute Recall@K with K = 1, 5, 10, as the evaluation metric. Zero-shot retrieval results are given in Appendix A.3.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th># Params</th>
<th># Pre-train Images</th>
<th>MSCOCO (5K test set)</th>
<th></th>
<th></th>
<th>Flickr30K (1K test set)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>TR</td>
<td>IR</td>
<td></td>
<td>TR</td>
<td>IR</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>R@1/R@5/R@10</td>
<td>R@1/R@5/R@10</td>
<td></td>
<td>R@1/R@5/R@10</td>
<td>R@1/R@5/R@10</td>
<td></td>
</tr>
<tr>
<td>UNITER_{large}</td>
<td>300M</td>
<td>4M</td>
<td>65.7 / 88.6 / 93.8</td>
<td>52.9 / 79.9 / 88.0</td>
<td></td>
<td>87.3 / 98.0 / 99.2</td>
<td>75.6 / 94.1 / 96.8</td>
<td></td>
</tr>
<tr>
<td>METER-Swin</td>
<td>380M</td>
<td>4M</td>
<td>73.0 / 92.0 / 96.3</td>
<td>54.9 / 81.4 / 89.3</td>
<td></td>
<td>92.4 / 99.0 / 99.5</td>
<td>79.0 / 95.6 / 98.0</td>
<td></td>
</tr>
<tr>
<td>ALBEF</td>
<td>210M</td>
<td>4M</td>
<td>73.1 / 91.4 / 96.0</td>
<td>56.8 / 81.5 / 89.2</td>
<td></td>
<td>94.3 / 99.4 / 99.8</td>
<td>82.8 / 96.7 / 98.4</td>
<td></td>
</tr>
<tr>
<td>METER-CLIP</td>
<td>380M</td>
<td>4M</td>
<td>76.2 / 93.2 / 96.8</td>
<td>57.1 / 82.7 / 90.1</td>
<td></td>
<td>94.3 / 99.6 / 99.9</td>
<td>82.2 / 96.3 / 98.4</td>
<td></td>
</tr>
<tr>
<td>VinVL_{large}</td>
<td>550M</td>
<td>5.6M</td>
<td>75.4 / 92.9 / 96.2</td>
<td>58.8 / 83.5 / 90.3</td>
<td></td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>ALIGN</td>
<td>490M</td>
<td>1.8B</td>
<td>77.0 / 93.5 / 96.9</td>
<td>59.9 / 83.3 / 89.8</td>
<td></td>
<td>95.3 / 99.8 / 100.0</td>
<td>84.9 / 97.4 / 98.6</td>
<td></td>
</tr>
<tr>
<td>ALBEF</td>
<td>210M</td>
<td>14M</td>
<td>77.6 / 94.3 / 97.2</td>
<td>60.7 / 84.3 / 90.5</td>
<td></td>
<td>95.9 / 99.8 / 100.0</td>
<td>85.6 / 97.5 / 98.9</td>
<td></td>
</tr>
<tr>
<td>X-VLM</td>
<td>216M</td>
<td>4M</td>
<td>80.4 / 95.5 / 98.2</td>
<td>63.1 / 85.7 / 91.6</td>
<td></td>
<td>96.8 / 99.8 / 100.0</td>
<td>86.1 / 97.4 / 98.7</td>
<td></td>
</tr>
<tr>
<td>X-VLM</td>
<td>216M</td>
<td>16M</td>
<td>81.2 / 95.6 / 98.2</td>
<td>63.4 / 85.8 / 91.5</td>
<td></td>
<td>97.1 / 100.0 / 100.0</td>
<td>86.9 / 97.3 / 98.7</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 3: Results on downstream V+L tasks, including visual reasoning (VQA and NLVR2), visual grounding (RefCOCO+), and image caption generation (COCO Caption). RefCOCO+ scores with * are evaluated in the weakly-supervised setting. COCO Captioning scores with + are models optimized with CIDEr for the second stage of fine-tuning.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>VQA</th>
<th></th>
<th>NLVR2</th>
<th></th>
<th>RefCOCO+</th>
<th></th>
<th></th>
<th>COCO Caption</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>test-dev</td>
<td>test-std</td>
<td>dev</td>
<td>test-P</td>
<td>val^{d}</td>
<td>testA^{d}</td>
<td>testB^{d}</td>
<td>BLEU@4</td>
<td>CIDEr</td>
</tr>
<tr>
<td>ViLBERT</td>
<td>70.55</td>
<td>70.92</td>
<td>-</td>
<td>-</td>
<td>72.34</td>
<td>78.52</td>
<td>62.61</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>VL-BERT</td>
<td>71.16</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>72.59</td>
<td>78.57</td>
<td>62.30</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>VILLA</td>
<td>73.59</td>
<td>73.67</td>
<td>78.39</td>
<td>79.30</td>
<td>76.05</td>
<td>81.65</td>
<td>65.70</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>SOHO</td>
<td>73.25</td>
<td>73.47</td>
<td>76.37</td>
<td>77.32</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>E2E-VLP</td>
<td>73.25</td>
<td>73.67</td>
<td>77.25</td>
<td>77.96</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>36.2</td>
<td>117.3</td>
</tr>
<tr>
<td>KD-VLP</td>
<td>74.20</td>
<td>74.31</td>
<td>77.36</td>
<td>77.78</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>UNITER_{large}</td>
<td>73.82</td>
<td>74.02</td>
<td>79.12</td>
<td>79.98</td>
<td>75.90</td>
<td>81.45</td>
<td>66.70</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>ALBEF(4M)</td>
<td>74.54</td>
<td>74.70</td>
<td>80.24</td>
<td>80.50</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>ALBEF(14M)</td>
<td>75.84</td>
<td>76.04</td>
<td>82.55</td>
<td>83.14</td>
<td>58.46^{*}</td>
<td>65.89^{*}</td>
<td>46.25^{*}</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>METER-Swin</td>
<td>76.43</td>
<td>76.42</td>
<td>82.23</td>
<td>82.47</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>VinVL_{large}(5.6M)</td>
<td>76.52</td>
<td>76.60</td>
<td>82.67</td>
<td>83.98</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>41.0^{+}</td>
<td>140.9^{+}</td>
</tr>
<tr>
<td>METER-CLIP</td>
<td>77.68</td>
<td>77.64</td>
<td>82.33</td>
<td>83.05</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>SimVLM_{base}(1.8B)</td>
<td>77.87</td>
<td>78.14</td>
<td>81.72</td>
<td>81.77</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>39.0</td>
<td>134.8</td>
</tr>
<tr>
<td>X-VLM(4M)</td>
<td>78.07</td>
<td>78.09</td>
<td>84.16</td>
<td>84.21</td>
<td>80.17</td>
<td>86.36</td>
<td>71.00</td>
<td>39.8 / 41.3^{+}</td>
<td>133.1 / 140.8^{+}</td>
</tr>
<tr>
<td>X-VLM(16M)</td>
<td>78.22</td>
<td>78.37</td>
<td>84.41</td>
<td>84.76</td>
<td>84.51</td>
<td>89.00</td>
<td>76.91</td>
<td>39.9 / 41.0^{+}</td>
<td>134.0 / 140.3^{+}</td>
</tr>
</tbody>
</table>
<p>MSCOCO and Flickr30K (Plummer et al., 2015) datasets. We adopt the widely used Karpathy split (Karpathy &amp; Li, 2015) for both datasets. We optimize $\mathcal{L}<em _mathrm_match="\mathrm{match">{\mathrm{cl}}$ and $\mathcal{L}</em>(I, T)$ for ranking. Following ALBEF, $k$ is set to 256 for MSCOCO and 128 for Flickr30K.}}$ and fine-tune the model for 10 epochs. In inference, we first compute $s(I, T)$ for all images and texts, and then take the top-$k$ candidates and calculate $\boldsymbol{p}^{\text{match}</p>
<p>Visual Question Answering (Goyal et al., 2017) It requires the model to predict an answer given an image and a question. Following the previous work (Cho et al., 2021; Li et al., 2021a), we use a six-layer Transformer decoder to generate answers based on the outputs of the cross-modal encoder. We fine-tune the model for 10 epochs. During inference, we constrain the decoder to only generate from the 3,129 candidate answers to make a fair comparison with existing methods.</p>
<p>Natural Language for Visual Reasoning (NLVR2 (Suhr et al., 2019)) The task lets the model determine whether a text describes the relations between two images. Following ALBEF, we extend the cross-modal encoder to enable reasoning over two images, and perform an additional pre-training step for one epoch using the 4M images: given two images and a text, the model assigns the text to either the first image, the second image, or none of them. Then, we fine-tune the model for 10 epochs.</p>
<p>Visual Grounding The task (RefCOCO+ (Yu et al., 2016)) aims to locate the region in an image that corresponds to a specific text description. Previous approaches formulate grounding as a ranking task by relying on the region proposals provided by pre-trained object detectors (Lu et al., 2019; Su et al., 2020; Chen et al., 2020; Gan et al., 2020). In contrast, X-VLM is able to directly predict the bounding boxes of the target regions given images and text descriptions. We also evaluate X-VLM on a weakly-supervised</p>
<p>setting, proposed by ALBEF, in which case only image-text pairs are available, and thus we fine-tune X-VLM using $\mathcal{L}<em _match="{match" _text="\text">{\mathrm{cl}}$ and $\mathcal{L}</em>$;}</p>
<p>Image Captioning The task requires a model to generate textual descriptions of input images. We evaluate X-VLM on the COCO Captioning dataset (Chen et al., 2015). We report BLEU-4 and CIDEr scores on the Karparthy test split. To apply X-VLM for captioning, we do not need to add a decoder. Instead, we simply adapt X-VLM to a multi-modal decoder. Specifically, we train X-VLM with language modeling loss for one epoch on 4M data. Then, we fine-tune it on the COCO Captioning dataset. Additionally, following VinVL, we also report the results after applying CIDEr optimization (Rennie et al., 2017) for the second stage of fine-tuning, which are denoted with ${}^{+}$.</p>
<h3>4.4 Results on Image-Text Retrieval</h3>
<p>Table 2 compares X-VLM with SoTA approaches on MSCOCO and Flickr30K, which are based on either objectcentric features (i.e. UNITER and VinVL) or overall image features (i.e. ALIGN, METER, and ALBEF). ALIGN (Jia et al., 2021) is a dual-encoder model similar to CLIP (Radford et al., 2021) specially for image-text retrieval tasks, which is trained on in-house 1.8B image-text pairs. Other VLMs, including our approach, for more general purposes, have a cross-modal encoder and thus use the output of the cross-modal encoder for ranking.</p>
<p>Even though existing approaches either have more parameters or more training data, X-VLM under the 4M setting outperforms all the previous methods by a large margin, achieving new SoTA results. Specifically, X-VLM(4M) which learns multi-grained vision language alignments outperforms VinVL which is based on object-centric features. In contrast, ALBEF which learns only image-text alignments outperforms VinVL only when increasing the training data to 14M. Compared to METER-Swin (Dou et al., 2021) which also uses Swin Transformer as the image encoder, X-VLM has better performance. Furthermore, even though X-VLM(4M) has already achieved very high performance on the image-text retrieval tasks, we still obtain improvements on R@1 when increasing the training instances to 16M. Additionally, Appendix A. 3 shows that when increasing the training data to 16M, X-VLM obtains substantial improvements on zero-shot image-text retrieval. Moreover, X-VLM also outperforms ALIGN on zero-shot MSCOCO by a large margin.</p>
<p>Additionally, METER provides an empirical study of VLMs and shows that the vision backbone (or parameter initialization) is important for the model performance. From Swin Transformer to CLIP-ViT, METER improves significantly on both retrieval and VQA (Table 2 and 3). We also have some preliminary observations and leave detailed studies of different backbones of X-VLM for future work.</p>
<h3>4.5 Results on Visual Reasoning</h3>
<p>Table 3 shows experimental results on visual reasoning (VQA and NLVR ${ }^{2}$ ). First, though ALBEF(14M) outperforms VinVL on image-text retrieval, the coarse-grained approaches such as SOHO, METER-Swin, and ALBEF, all have worse performances than VinVL in visual reasoning tasks, except that METER-CLIP and SimVLM outperform VinVL on VQA. Besides, VinVL also substantially outperforms previous methods that rely on object detectors to learn both object-level and image-level alignments, such as E2E-VLP and KD-VLP.</p>
<p>Nevertheless, X-VLM(4M) with moderate model size and pre-trained on fewer instances outperforms VinVL. Specifically, X-VLM(4M) achieves absolute improvements of $1.52 \%$ on VQA and $0.86 \%$ on NLVR2 (average on metrics) over VinVL. Meanwhile, as reported in Li et al. (2021a), X-VLM, which encodes images without an object detection process, enjoys $\sim 10$ times faster inference speed than VinVL. The results indicate that our approach of X-VLM is both effective and efficient. X-VLM also outperforms SimVLM $_{\text {base }}$ which is pre-trained on in-house 1.8B data, especially on NLVR2.</p>
<h3>4.6 Results on Visual Grounding</h3>
<p>Table 3 reports the performance of X-VLM on RefCOCO+. X-VLM(4M) achieves absolute improvements of $4.5 \%$ compared to UNITER. As aforementioned, previous approaches formulate grounding as a ranking task by relying on the region proposals provided by object detectors. In contrast, X-VLM is able to directly predict the target boxes, which is much simpler and more efficient. Furthermore, X-VLM for general V+L purposes outperforms MDETR (Kamath et al., 2021) specialized for visual grounding tasks. X-VLM(4M) using the same set of image annotations achieves absolute improvements of $1.1 \%$ (average on metrics), compared to MDETR.</p>
<p>We also evaluate X-VLM in the weakly-supervised setting, proposed by ALBEF. X-VLM(4M) obtains 68.46/76.53/57.09 for val $^{d} /$ testA $^{d} /$ testB $^{d}$ respectively, achieving an absolute improvement of $10.5 \%$ (average on metrics) compared to ALBEF(14M). When increasing pretraining images to 16M, X-VLM obtains 77.26/84.11/67.13.</p>
<p>Figure 3 provides a few examples of images from the test set of RefCOCO+. For the supervised setting, we show the bounding boxes predicted by X-VLM given the text descriptions. For the weakly-supervised setting, following ALBEF, we provide the Grad-CAM visualization, which uses the cross-attention maps in the fourth layer of the crossmodal encoder. The visualization examples show that X-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Grad-CAM visualization and bounding box prediction on unseen images. X-VLM predicts correct regions even though the textual descriptions only differ in a single word. X-VLM can also align each word in the text to the corresponding image region. Appendix A. 4 gives more examples, showing X-VLM's superior ability of multi-grained vision language alignments.</p>
<p>Table 4. Ablation study results. Models w/o object and w/o region are ablated variants where the model is training without concepts of object and region respectively. Model w/o bbox loss is the variant where bounding box prediction is ablated. Model w/o all represents that all the above components are ablated.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Meta-Sum</th>
<th style="text-align: center;">MSCOCO</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Flickr30K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">VQA <br> test-dev</th>
<th style="text-align: center;">$\mathrm{NLVR}^{2}$ <br> test-P</th>
<th style="text-align: center;">RefCOCO+ <br> testA $^{d}$</th>
<th style="text-align: center;">testB $^{d}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">X-VLM</td>
<td style="text-align: center;">$\mathbf{6 0 5 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 8}$</td>
<td style="text-align: center;">$\mathbf{6 0 . 6}$</td>
<td style="text-align: center;">$\mathbf{9 6 . 0}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 1}$</td>
<td style="text-align: center;">76.20</td>
<td style="text-align: center;">82.42</td>
<td style="text-align: center;">72.07</td>
<td style="text-align: center;">54.84</td>
</tr>
<tr>
<td style="text-align: left;">w/o object</td>
<td style="text-align: center;">603.5</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">75.87</td>
<td style="text-align: center;">82.10</td>
<td style="text-align: center;">$\mathbf{7 3 . 3 7}$</td>
<td style="text-align: center;">$\mathbf{5 5 . 6 9}$</td>
</tr>
<tr>
<td style="text-align: left;">w/o region</td>
<td style="text-align: center;">596.0</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">$\mathbf{9 6 . 0}$</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">75.84</td>
<td style="text-align: center;">82.20</td>
<td style="text-align: center;">70.73</td>
<td style="text-align: center;">50.60</td>
</tr>
<tr>
<td style="text-align: left;">w/o bbox loss</td>
<td style="text-align: center;">594.9</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">$\mathbf{7 6 . 7 7}$</td>
<td style="text-align: center;">81.49</td>
<td style="text-align: center;">69.32</td>
<td style="text-align: center;">50.38</td>
</tr>
<tr>
<td style="text-align: left;">w/o all</td>
<td style="text-align: center;">580.6</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">74.90</td>
<td style="text-align: center;">80.70</td>
<td style="text-align: center;">67.79</td>
<td style="text-align: center;">46.43</td>
</tr>
</tbody>
</table>
<p>VLM has a strong ability of cross-modal understanding. It successfully predicts the correct regions in images, even though the text descriptions only differ in a single word. Furthermore, X-VLM can align each word in the text to the corresponding image region. We provide more examples in Appendix A.4, showing X-VLM's superior performance in vision language alignment.</p>
<h3>4.7. Results on Image Captioning</h3>
<p>We show that X-VLM, usually considered as an "encoderonly" model, has comparable performance with SoTA generative methods on image caption generation, as indicated in Table 3. Specifically, X-VLM pre-trained on 16M instances performs similarly to SimVLM which uses not only 1.8B in-house image-text pairs but also a large-scale text corpus.</p>
<p>Besides, we observe that CIDEr optimization largely boosts the CIDEr scores. X-VLM in moderate model size also has comparable performance to VinVL $_{\text {large }}$.</p>
<h3>4.8. Ablation Study</h3>
<p>We also conduct an in-depth ablation study to investigate the role of different components in the X-VLM, as shown in Table 4. All compared model variants are trained on 4 M images for 80 K steps with a batch size of 3072 to ensure a fair comparison. We use Recall@1 as an evaluation measure in the retrieval tasks and Meta-Sum as a general measure. We report RefCOCO+ evaluation results in the weakly-supervised setting.</p>
<p>First, we evaluate the effectiveness of visual concepts in different granularities, i.e. w/o object and w/o region. The</p>
<p>results show that training without either of them hurts the performance, demonstrating the necessity of learning multigrained alignments. Besides, we can observe that w/o region makes the performance drop more drastically than w/o object. Furthermore, the ablation study shows that bounding box prediction is a critical component of X-VLM, as w/o bbox loss leads to the lowest Meta-Sum. We also report the results of 'w/o all' where all the above components are ablated. Though in the 4M setting, only 210K images have dense annotations, X-VLM can leverage the data to learn multi-grained vision language alignment and substantially improve the performances in the downstream V+L tasks (Meta-Sum from 580.6 to 605.2).</p>
<h2>5. Conclusion and Discussion</h2>
<p>In this paper, we have proposed X-VLM, a strong and efficient approach to perform multi-grained vision language pre-training. Training of the model is driven by locating visual concepts in the image given the associated texts and aligning texts with relevant visual concepts, where the alignments are in multi-granularity. We have pre-trained X-VLM with 4 M and 16 M images, which are of moderate size. Also, X-VLM only consists of 216 M parameters. These choices are made because we want to make our experiments as "green" (Schwartz et al., 2020; Xu et al., 2021b) as possible and be accessible to a larger group of people. Experiments on downstream V+L tasks, including image-text retrieval, visual reasoning, visual grounding, and image caption generation have shown that X-VLM outperforms the existing methods which could be larger and/or pre-trained on more data. As suggested by the comparison between X-VLM(4M) and X-VLM(16M), adding more pre-training datasets will probably lead to further performance improvements. As for applications, X-VLM has shown better performance in understanding fine-grained vision language alignments. For example, it can generate image captions probably having more object details, which makes it a better choice to help people with disability in vision to understand images. On the other hand, X-VLM in moderate model size is also easier to deploy.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Wangchunshu Zhou, Wenguan Huang, and Xiu-jun Li at ByteDance for their generous assistance in data collection and insightful comments in technical discussions.</p>
<h2>References</h2>
<p>Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., and Zhang, L. Bottom-up and top-down attention for image captioning and visual question answering.
In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 6077-6086. IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.00636.</p>
<p>Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. End-to-end object detection with transformers. In European Conference on Computer Vision, pp. 213-229. Springer, 2020.</p>
<p>Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3558-3568, 2021.</p>
<p>Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Dollr, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. URL https:// arxiv.org/abs/1504.00325.</p>
<p>Chen, Y.-C., Li, L., Yu, L., El Kholy, A., Ahmed, F., Gan, Z., Cheng, Y., and Liu, J. Uniter: Universal image-text representation learning. In European conference on computer vision, pp. 104-120. Springer, 2020.</p>
<p>Cho, J., Lei, J., Tan, H., and Bansal, M. Unifying vision-andlanguage tasks via text generation. In International Conference on Machine Learning, pp. 1931-1942. PMLR, 2021.</p>
<p>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.</p>
<p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.</p>
<p>Dou, Z.-Y., Xu, Y., Gan, Z., Wang, J., Wang, S., Wang, L., Zhu, C., Liu, Z., Zeng, M., et al. An empirical study of training end-to-end vision-and-language transformers. arXiv preprint arXiv:2111.02387, 2021. URL https: //arxiv.org/abs/2111.02387.</p>
<p>Gan, Z., Chen, Y., Li, L., Zhu, C., Cheng, Y., and Liu, J. Large-scale adversarial training for vision-and-language</p>
<p>representation learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.</p>
<p>Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 6325-6334. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.670. URL https://doi.org/10.1109/CVPR.2017.670.</p>
<p>Huang, Z., Zeng, Z., Liu, B., Fu, D., and Fu, J. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849, 2020. URL https://arxiv.org/abs/2004.00849.</p>
<p>Huang, Z., Zeng, Z., Huang, Y., Liu, B., Fu, D., and Fu, J. Seeing out of the box: End-to-end pre-training for vision-language representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12976-12985, 2021.</p>
<p>Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904-4916. PMLR, 2021.</p>
<p>Jiang, H., Misra, I., Rohrbach, M., Learned-Miller, E., and Chen, X. In defense of grid features for visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1026710276, 2020.</p>
<p>Kamath, A., Singh, M., LeCun, Y., Synnaeve, G., Misra, I., and Carion, N. Mdetr-modulated detection for end-toend multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1780-1790, 2021.</p>
<p>Karpathy, A. and Li, F. Deep visual-semantic alignments for generating image descriptions. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pp. 31283137. IEEE Computer Society, 2015. doi: 10.1109/ CVPR.2015.7298932. URL https://doi.org/10. 1109/CVPR.2015.7298932.</p>
<p>Kim, W., Son, B., and Kim, I. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pp. 5583-5594. PMLR, 2021.</p>
<p>Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32-73, 2017.</p>
<p>Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. arXiv preprint arXiv:1811.00982, 2018. URL https://arxiv. org/abs/1811.00982.</p>
<p>Li, G., Duan, N., Fang, Y., Gong, M., and Jiang, D. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. In The ThirtyFourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 11336-11344. AAAI Press, 2020a. URL https://aaai.org/ojs/index. php/AAAI/article/view/6795.</p>
<p>Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., and Hoi, S. C. H. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34, 2021a.</p>
<p>Li, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang, K.-W. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019. URL https://arxiv. org/abs/1908.03557.</p>
<p>Li, W., Gao, C., Niu, G., Xiao, X., Liu, H., Liu, J., Wu, H., and Wang, H. UNIMO: Towards unified-modal understanding and generation via cross-modal contrastive learning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 2592-2607, Online, 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.202. URL https : //aclanthology.org/2021.acl-long. 202.</p>
<p>Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F., et al. Oscar: Objectsemantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pp. 121137. Springer, 2020b.</p>
<p>Li, Y., Fan, J., Pan, Y., Yao, T., Lin, W., and Mei, T. Uni-eden: Universal encoder-decoder network by multigranular vision-language pre-training. ACM Trans. Multim. Comput. Commun. Appl., 18(2):48:1-48:16, 2022. doi: 10.1145/3473140. URL https://doi.org/10. $1145 / 3473140$.</p>
<p>Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollr, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740-755. Springer, 2014.</p>
<p>Liu, Y., Wu, C., Tseng, S.-y., Lal, V., He, X., and Duan, N. Kd-vlp: Improving end-to-end vision-and-language pretraining with object knowledge distillation. arXiv preprint arXiv:2109.10504, 2021a. URL https:// arxiv.org/abs/2109.10504.</p>
<p>Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021b. URL https://arxiv. org/abs/2103.14030.</p>
<p>Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https: //openreview.net/forum?id=Bkg6RiCqY7.</p>
<p>Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d'Alch-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 13-23, 2019.</p>
<p>Ordonez, V., Kulkarni, G., and Berg, T. L. Im2text: Describing images using 1 million captioned photographs. In Shawe-Taylor, J., Zemel, R. S., Bartlett, P. L., Pereira, F. C. N., and Weinberger, K. Q. (eds.), Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain, pp. 1143-1151, 2011.</p>
<p>Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C., Hockenmaier, J., and Lazebnik, S. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 2641-2649. IEEE Computer Society, 2015. doi: 10.1109/ICCV.2015.303. URL https://doi.org/10.1109/ICCV.2015.303.</p>
<p>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 87488763. PMLR, 2021. URL http://proceedings. mlr.press/v139/radford21a.html.</p>
<p>Ren, S., He, K., Girshick, R. B., and Sun, J. Faster R-CNN: towards real-time object detection with region proposal networks. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 91-99, 2015.</p>
<p>Rennie, S. J., Marcheret, E., Mroueh, Y., Ross, J., and Goel, V. Self-critical sequence training for image captioning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 1179-1195. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.131. URL https://doi.org/ 10.1109/CVPR.2017.131.</p>
<p>Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I. D., and Savarese, S. Generalized intersection over union: A metric and a loss for bounding box regression. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 658-666. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00075.</p>
<p>Schwartz, R., Dodge, J., Smith, N. A., and Etzioni, O. Green ai. Communications of the ACM, 63(12):54-63, 2020.</p>
<p>Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D. Grad-cam: Visual explanations from deep networks via gradient-based localization. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pp. 618-626. IEEE Computer Society, 2017. doi: 10.1109/ICCV.2017.74. URL https://doi.org/ 10.1109/ICCV.2017.74.</p>
<p>Shao, S., Li, Z., Zhang, T., Peng, C., Yu, G., Zhang, X., Li, J., and Sun, J. Objects365: A large-scale, high-quality dataset for object detection. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pp. 8429-8438. IEEE, 2019. doi: 10.1109/ICCV.2019.00852. URL https://doi.org/10.1109/ICCV.2019. 00852 .</p>
<p>Sharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 25562565, Melbourne, Australia, 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1238. URL https://aclanthology.org/P18-1238.</p>
<p>Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., and Dai, J. VL-BERT: pre-training of generic visual-linguistic representations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https: //openreview.net/forum?id=SygXPaEYvH.</p>
<p>Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., and Artzi, Y. A corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6418-6428, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/ P19-1644. URL https://aclanthology.org/ P19-1644.</p>
<p>Tan, H. and Bansal, M. LXMERT: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5100-5111, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1514. URL https:// aclanthology.org/D19-1514.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998-6008, 2017.</p>
<p>Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., and Cao, Y. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021. URL https://arxiv. org/abs/2108.10904.</p>
<p>Xu, H., Yan, M., Li, C., Bi, B., Huang, S., Xiao, W., and Huang, F. E2E-VLP: End-to-end vision-language pretraining enhanced by visual learning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1:</p>
<p>Long Papers), pp. 503-513, Online, 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021. acl-long.42. URL https://aclanthology.org/ 2021.acl-long. 42.</p>
<p>Xu, J., Zhou, W., Fu, Z., Zhou, H., and Li, L. A survey on green deep learning. arXiv preprint arXiv:2111.05193, 2021b.</p>
<p>Yu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L. Modeling context in referring expressions. In European Conference on Computer Vision, pp. 69-85. Springer, 2016.</p>
<p>Yu, L., Lin, Z., Shen, X., Yang, J., Lu, X., Bansal, M., and Berg, T. L. Mattnet: Modular attention network for referring expression comprehension. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 1307-1315. IEEE Computer Society, 2018. doi: 10. 1109/CVPR.2018.00142.</p>
<p>Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L., Choi, Y., and Gao, J. Vinvl: Revisiting visual representations in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5579-5588, 2021.</p>
<h1>A. Appendix</h1>
<h2>A.1. Statistics of Object and Region Annotations</h2>
<p>Table 5. Statistics of annotations used in the pre-training.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;"># Images</th>
<th style="text-align: left;"># Captions</th>
<th style="text-align: left;"># Objects</th>
<th style="text-align: left;"># Regions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">COCO</td>
<td style="text-align: left;">0.11 M</td>
<td style="text-align: left;">0.55 M</td>
<td style="text-align: left;">0.45 M</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">VG</td>
<td style="text-align: left;">0.10 M</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">2.0 M</td>
<td style="text-align: left;">3.7 M</td>
</tr>
<tr>
<td style="text-align: left;">Objects365</td>
<td style="text-align: left;">0.58 M</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">2.0 M</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">OpenImages</td>
<td style="text-align: left;">1.7 M</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">4.2 M</td>
<td style="text-align: left;">-</td>
</tr>
</tbody>
</table>
<p>Table 5 gives statistics of object and region annotations of each dataset. Only the Visual Genome dataset contains region annotations. Besides, the OpenImages dataset offers some relationship annotations, indicating pairs of objects in particular relations (e.g. "woman playing guitar", "beer on table"), object properties (e.g. "table is wooden"), and human actions (e.g. "woman is jumping"), which can also be viewed as region annotations.</p>
<p>Note that we filtered out some samples because of: 1) invalid annotations (e.g. negative values for bounding boxes or boxes being outside of the images); 2) boxes being too small $(&lt;1 \%)$; 3) highly overlapped textual descriptions of regions ( $&gt;$ $75 \%$ ), etc. After pre-processing, we keep: for example, COCO objects 446,873 (from 859,999), VG objects 2,043,927 (from 3,802,349), VG regions 3,699,598 (from 5,402,953).</p>
<h2>A.2. Implementation Details of Downstream Tasks</h2>
<p>We follow the settings in existing methods for fine-tuning. We describe how we implement fine-tuning on the downstream $\mathrm{V}+\mathrm{L}$ tasks, and we also provide our fine-tuning scripts for more details. Note that we have cleaned our pre-training datasets to avoid data leaks since downstream V+L tasks have overlaps in images with COCO and Visual Genome.</p>
<p>Image-Text Retrieval We evaluate X-VLM on MSCOCO and Flickr30K (Plummer et al., 2015) benchmarks. We adopt the widely used Karpathy split (Karpathy \&amp; Li, 2015) for both datasets. We optimize $\mathcal{L}<em _match="{match" _text="\text">{\mathrm{cl}}$ and $\mathcal{L}</em>(I, T)$ for ranking. Following ALBEF, $k$ is set to 256 for MSCOCO and 128 for Flickr30K.}}$ for fine-tuning. Since there are multiple ground-truth texts associated with each image in the datasets, we change the ground-truth similarity of contrastive learning, $\boldsymbol{y}^{i 2 t}(I)$ and $\boldsymbol{y}^{12 v}(T)$, to consider multiple positives, where each positive example has a probability of $\frac{1}{# \text { positives }}$. We fine-tune the model for 10 epochs. During inference, we first compute $s(I, T)$ for all images and texts. Then we take the top- $k$ candidates and pass them into the cross-modal encoder to calculate $\boldsymbol{p}^{\text {match }</p>
<p>Visual Question Answering (VQA (Goyal et al., 2017)) Following existing methods (Tan \&amp; Bansal, 2019; Chen et al., 2020; Li et al., 2021a), we use both train and validation sets for training, and include additional question-answer pairs from Visual Genome. The VQA model contains a 6-layer transformer-based decoder to generate answers based on the outputs of the cross-modal encoder following previous work (Cho et al., 2021; Li et al., 2021a). The decoder is initialized using the pre-trained weights from the cross-modal encoder. Then, the model is fine-tuned by optimizing the auto-regressive loss for 10 epochs. During inference, we constrain the decoder to only generate from the 3,129 candidate answers ${ }^{3}$ to make a fair comparison with existing methods.</p>
<p>Natural Language for Visual Reasoning (NLVR2 (Suhr et al., 2019)) Since the task asks the model to distinguish whether a text describes a pair of images, we follow ALBEF to extend the cross-modal encoder to enable reasoning over two images. We also perform an additional pre-training step for 1 epoch using the 4 M images: given a pair of images and a text, the model needs to assign the text to either the first image, the second image, or none of them. Then, we fine-tune the model for 10 epochs.</p>
<p>Visual Grounding The task aims to locate the region in an image that corresponds to a specific text description (Ref-COCO+ (Yu et al., 2016)). We evaluate our approach in both supervised and weakly-supervised settings. The latter is proposed by ALBEF. In the supervised setting with bounding box annotations, we perform an additional pre-training step for one epoch using $\mathcal{L}<em _mathrm_cl="\mathrm{cl">{\text {bbox }}$ only. Then, we fine-tune the model for 10 epochs. In the weakly-supervised setting where only image-text pairs are available, we fine-tune the model using $\mathcal{L}</em>$ for 5 epochs. During inference, following ALBEF, we apply Grad-CAM (Selvaraju et al., 2017) to acquire heatmaps and use them to rank the detected proposals}}$ and $\mathcal{L}_{\text {match }</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>provided by (Yu et al., 2018).
Image Captioning The task requires a model to generate textual descriptions of input images. We evaluate X-VLM on the COCO Captioning dataset (Chen et al., 2015). We report BLEU-4 and CIDEr scores on the Karparthy test split. To apply X-VLM for captioning, we do not need to add a decoder. Instead, we simply adapt X-VLM to a multi-modal decoder. Specifically, we train X-VLM with language modeling loss for one epoch on 4M data. Then, we fine-tune it on the COCO Captioning dataset with naive cross-entropy loss for five epochs. Additionally, following VinVL, we also report the results after applying CIDEr optimization (Rennie et al., 2017) for the second stage of fine-tuning which takes another five epochs.</p>
<h1>A.3. Zero-Shot Image-Text Retrieval Results</h1>
<p>Table 6. Zero-shot results on MSCOCO and Flickr30K datasets. IR: Image Retrieval and TR: Text Retrieval.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;"># Params</th>
<th style="text-align: center;"># Pre-train <br> Images</th>
<th style="text-align: center;">MSCOCO (5K test set)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Flickr30K (1K test set)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TR</td>
<td style="text-align: center;">IR</td>
<td style="text-align: center;">TR</td>
<td style="text-align: center;">IR</td>
</tr>
<tr>
<td style="text-align: center;">CLIP</td>
<td style="text-align: center;">$\sim 100 \mathrm{M}$</td>
<td style="text-align: center;">400 M</td>
<td style="text-align: center;">R@1/R@5/R@10</td>
<td style="text-align: center;">R@1/R@5/R@10</td>
<td style="text-align: center;">R@1/R@5/R@10</td>
<td style="text-align: center;">R@1/R@5/R@10</td>
</tr>
<tr>
<td style="text-align: center;">ALIGN</td>
<td style="text-align: center;">490M</td>
<td style="text-align: center;">1.8B</td>
<td style="text-align: center;">58.4 / 81.5 / 88.1</td>
<td style="text-align: center;">37.8 / 62.4 / 72.2</td>
<td style="text-align: center;">88.0 / 98.7 / 99.4</td>
<td style="text-align: center;">68.7 / 90.6 / 95.2</td>
</tr>
<tr>
<td style="text-align: center;">X-VLM</td>
<td style="text-align: center;">216M</td>
<td style="text-align: center;">4 M</td>
<td style="text-align: center;">70.8 / 92.1 / 96.5</td>
<td style="text-align: center;">55.6 / 82.7 / 90.0</td>
<td style="text-align: center;">85.3 / 97.8 / 99.6</td>
<td style="text-align: center;">71.9 / 93.3 / 96.4</td>
</tr>
<tr>
<td style="text-align: center;">X-VLM</td>
<td style="text-align: center;">216M</td>
<td style="text-align: center;">16M</td>
<td style="text-align: center;">71.6 / 93.1 / 97.0</td>
<td style="text-align: center;">56.1 / 83.0 / 89.8</td>
<td style="text-align: center;">87.7 / 98.6 / 99.6</td>
<td style="text-align: center;">74.9 / 94.4 / 97.1</td>
</tr>
</tbody>
</table>
<p>Table 6 shows zero-shot image-text retrieval results and compares X-VLM with the dual encoder SoTAs (CLIP and ALIGN) which are pre-trained using only the retrieval objective. We can observe that though X-VLM is pre-trained using the combination of different objectives, it still has very competitive results on zero-shot retrieval tasks.</p>
<h2>A.4. Case Study</h2>
<p>Figure 4 and 5 provide visualizations of some images from the test set of RefCOCO+. We show the bounding boxes predicted by X-VLM given the text descriptions. For the weakly-supervised setting, we provide the Grad-CAM visualization which uses the cross-attention maps in the fourth layer of the cross-modal encoder. We can observe that in both settings X-VLM can predict correct regions even though the textual descriptions only differ in a single word. X-VLM can also align each word in the text to the corresponding image region, showing X-VLM's superior ability of multi-grained vision language alignments.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Locating visual concepts in unseen images given text descriptions. Since Grad-CAM gives visualizations each corresponds to an individual word, we only show the visualization of the subject word, e.g. "dog" for "brown dog".</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Bounding box prediction and per-word visualization on unseen images. It shows that X-VLM can also align concepts like "pulling" and "holding" to the corresponding regions in the images.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ There is a NULL answer. Thus, the actual number of candidate answers is 3,128 .&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>